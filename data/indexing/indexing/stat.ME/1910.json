[{"id": "1910.00006", "submitter": "Behnaz Pirzamanbein PhD", "authors": "Behnaz Pirzamanbein", "title": "Spatial methods and their applications to environmental and climate data", "comments": "Book (this report/review of methodology and applications was written\n  as an introductory paper for PhD study in Lund University)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental and climate processes are often distributed over large\nspace-time domains. Their complexity and the amount of available data make\nmodelling and analysis a challenging task. Statistical modelling of environment\nand climate data can have several different motivations including\ninterpretation or characterisation of the data. Results from statistical\nanalysis are often used as a integral part of larger environmental studies.\nSpatial statistics is an active and modern statistical field, concerned with\nthe quantitative analysis of spatial data; their dependencies and\nuncertainties. Spatio-temporal statistics extends spatial statistics through\nthe addition of time to the, two or three, spatial dimensions. The focus of\nthis introductory paper is to provide an overview of spatial methods and their\napplication to environmental and climate data. This paper also gives an\noverview of several important topics including large data sets and\nnon-stationary covariance structures. Further, it is discussed how Bayesian\nhierarchical models can provide a flexible way of constructing models.\nHierarchical models may seem to be a good solution, but they have challenges of\ntheir own such as, parameter estimation. Finally, the application of\nspatio-temporal models to the LANDCLIM data (LAND cover - CLIMate interactions\nin NW Europe during the Holocene) will be discussed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 11:31:11 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Pirzamanbein", "Behnaz", ""]]}, {"id": "1910.00103", "submitter": "Lin Zhang", "authors": "Lin Zhang, Andrew DiLernia, Karina Quevedo, Jazmin Camchong, Kelvin\n  Lim, Wei Pan1", "title": "A random covariance model for bi-level graphical modeling with\n  application to resting-state fMRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a novel problem, bi-level graphical modeling, in which\nmultiple individual graphical models can be considered as variants of a common\ngroup-level graphical model and inference of both the group- and\nindividual-level graphical models are of interest. Such problem arises from\nmany applications including multi-subject neuroimaging and genomics data\nanalysis. We propose a novel and efficient statistical method, the random\ncovariance model, to learn the group- and individual-level graphical models\nsimultaneously. The proposed method can be nicely interpreted as a random\ncovariance model that mimics the random effects model for mean structures in\nlinear regression. It accounts for similarity between individual graphical\nmodels, identifies group-level connections that are shared by individuals in\nthe group, and at the same time infers multiple individual-level networks.\nCompared to existing multiple graphical modeling methods that only focus on\nindividual-level networks, our model learns the group-level structure\nunderlying the multiple individual networks and enjoys computational efficiency\nthat is particularly attractive for practical use. We further define a measure\nof degrees-of-freedom for the complexity of the model that can be used for\nmodel selection. We demonstrate the asymptotic properties of the method and\nshow its finite sample performance through simulation studies. Finally, we\napply the proposed method to our motivating clinical data, a multi-subject\nresting-state functional magnetic resonance imaging (fMRI) dataset collected\nfrom schizophrenia patients.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 20:55:08 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhang", "Lin", ""], ["DiLernia", "Andrew", ""], ["Quevedo", "Karina", ""], ["Camchong", "Jazmin", ""], ["Lim", "Kelvin", ""], ["Pan1", "Wei", ""]]}, {"id": "1910.00131", "submitter": "Dominik Liebl", "authors": "Dominik Liebl and Matthew Reimherr", "title": "Fast and Fair Simultaneous Confidence Bands for Functional Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying uncertainty using confidence regions is a central goal of\nstatistical inference. Despite this, methodologies for confidence bands in\nFunctional Data Analysis are underdeveloped compared to estimation and\nhypothesis testing. This work represents a major leap forward in this area by\npresenting a new methodology for constructing simultaneous confidence bands for\nfunctional parameter estimates. These bands possess a number of striking\nqualities: (1) they have a nearly closed-form expression, (2) they give nearly\nexact coverage, (3) they have a finite sample correction, (4) they do not\nrequire an estimate of the full covariance of the parameter estimate, and (5)\nthey can be constructed adaptively according to a desired criteria. One option\nfor choosing bands we find especially interesting is the concept of fair bands\nwhich allows us to do fair (or equitable) inference over subintervals and could\nbe especially useful in longitudinal studies over long time scales. Our bands\nare constructed by integrating and extending tools from Random Field Theory, an\narea that has yet to overlap with Functional Data Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 22:11:54 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 12:46:14 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Liebl", "Dominik", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1910.00282", "submitter": "Nik Lomax Dr", "authors": "Nik Lomax, Nick Malleson and Le-Minh Kieu", "title": "Point Pattern Processes and Models", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a substantial increase in the availability of\ndatasets which contain information about the location and timing of an event or\ngroup of events and the application of methods to analyse spatio-temporal\ndatasets spans many disciplines. This chapter defines and provides an overview\nof tools for analysing spatial and temporal point patterns and processes, where\ndiscrete events occur at random across space or over time respectively. It also\nintroduces the concept of spatial-temporal point patterns and methods of\nanalysis for data where events occur in both space and time. We discuss models,\nmethods and tools for analysing point processes.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:57:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Lomax", "Nik", ""], ["Malleson", "Nick", ""], ["Kieu", "Le-Minh", ""]]}, {"id": "1910.00423", "submitter": "Keith Levin", "authors": "Keith Levin, Fred Roosta, Minh Tang, Michael W. Mahoney, Carey E.\n  Priebe", "title": "Limit theorems for out-of-sample extensions of the adjacency and\n  Laplacian spectral embeddings", "comments": "Portions of this work originally appeared in ICML2018 as\n  \"Out-of-sample extension of graph adjacency spectral embedding\" (accompanying\n  technical report available at arXiv:1802.06307). This work extends the\n  results of that earlier paper to a second graph embedding technique called\n  the Laplacian spectral embedding and presents additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embeddings, a class of dimensionality reduction techniques designed for\nrelational data, have proven useful in exploring and modeling network\nstructure. Most dimensionality reduction methods allow out-of-sample\nextensions, by which an embedding can be applied to observations not present in\nthe training set. Applied to graphs, the out-of-sample extension problem\nconcerns how to compute the embedding of a vertex that is added to the graph\nafter an embedding has already been computed. In this paper, we consider the\nout-of-sample extension problem for two graph embedding procedures: the\nadjacency spectral embedding and the Laplacian spectral embedding. In both\ncases, we prove that when the underlying graph is generated according to a\nlatent space model called the random dot product graph, which includes the\npopular stochastic block model as a special case, an out-of-sample extension\nbased on a least-squares objective obeys a central limit theorem about the true\nlatent position of the out-of-sample vertex. In addition, we prove a\nconcentration inequality for the out-of-sample extension of the adjacency\nspectral embedding based on a maximum-likelihood objective. Our results also\nyield a convenient framework in which to analyze trade-offs between estimation\naccuracy and computational expense, which we explore briefly.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 04:02:10 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Levin", "Keith", ""], ["Roosta", "Fred", ""], ["Tang", "Minh", ""], ["Mahoney", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1910.00533", "submitter": "Ryan Martin", "authors": "Joyce Cahoon and Ryan Martin", "title": "Generalized inferential models for meta-analyses based on few studies", "comments": "19 pages, 5 figures, 1 table. Comments welcome at\n  https://www.researchers.one/article/2019-09-25", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis based on only a few studies remains a challenging problem, as\nan accurate estimate of the between-study variance is apparently needed, but\nhard to attain, within this setting. Here we offer a new approach, based on the\ngeneralized inferential model framework, whose success lays in marginalizing\nout the between-study variance, so that an accurate estimate is not essential.\nWe show theoretically that the proposed solution is at least approximately\nvalid, with numerical results suggesting it is, in fact, nearly exact. We also\ndemonstrate that the proposed solution outperforms existing methods across a\nwide range of scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 16:35:16 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 22:45:44 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Cahoon", "Joyce", ""], ["Martin", "Ryan", ""]]}, {"id": "1910.00585", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Non-algorithmic theory of randomness", "comments": "15 pages", "journal-ref": "Fields of Logic and Computation III: Esseays Dedicated to Yuri\n  Gurevich on the Occasion of his 80th Birthday. Lecture Notes in Computer\n  Science 12180, pages 323-340 (2020)", "doi": "10.1007/978-3-030-48006-6", "report-no": "25", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an alternative language for expressing results of the\nalgorithmic theory of randomness. The language is more precise in that it does\nnot involve unspecified additive or multiplicative constants, making\nmathematical results, in principle, applicable in practice. Our main testing\nground for the proposed language is the problem of defining Bernoulli\nsequences, which was of great interest to Andrei Kolmogorov and his students.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:44:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "1910.00641", "submitter": "Martin Huber", "authors": "Martin Huber", "title": "An introduction to flexible methods for policy evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter covers different approaches to policy evaluation for assessing\nthe causal effect of a treatment or intervention on an outcome of interest. As\nan introduction to causal inference, the discussion starts with the\nexperimental evaluation of a randomized treatment. It then reviews evaluation\nmethods based on selection on observables (assuming a quasi-random treatment\ngiven observed covariates), instrumental variables (inducing a quasi-random\nshift in the treatment), difference-in-differences and changes-in-changes\n(exploiting changes in outcomes over time), as well as regression\ndiscontinuities and kinks (using changes in the treatment assignment at some\nthreshold of a running variable). The chapter discusses methods particularly\nsuited for data with many observations for a flexible (i.e. semi- or\nnonparametric) modeling of treatment effects, and/or many (i.e. high\ndimensional) observed covariates by applying machine learning to select and\ncontrol for covariates in a data-driven way. This is not only useful for\ntackling confounding by controlling for instance for factors jointly affecting\nthe treatment and the outcome, but also for learning effect heterogeneities\nacross subgroups defined upon observable covariates and optimally targeting\nthose groups for which the treatment is most effective.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 19:59:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Huber", "Martin", ""]]}, {"id": "1910.00745", "submitter": "Lucy Gao", "authors": "Lucy L. Gao and Julie Zhou", "title": "Minimax D-optimal designs for multivariate regression models with\n  multi-factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-response regression models, the error covariance matrix is never\nknown in practice. Thus, there is a need for optimal designs which are robust\nagainst possible misspecification of the error covariance matrix. In this\npaper, we approximate the error covariance matrix with a neighbourhood of\ncovariance matrices, in order to define minimax D-optimal designs which are\nrobust against small departures from an assumed error covariance matrix. It is\nwell known that the optimization problems associated with robust designs are\nnon-convex, which makes it challenging to construct robust designs analytically\nor numerically, even for one-response regression models. We show that the\nobjective function for the minimax D-optimal design is a difference of two\nconvex functions. This leads us to develop a flexible algorithm for computing\nminimax D-optimal designs, which can be applied to any multi-response model\nwith a discrete design space. We also derive several theoretical results for\nminimax D-optimal designs, including scale invariance and reflection symmetry.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 01:57:53 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Gao", "Lucy L.", ""], ["Zhou", "Julie", ""]]}, {"id": "1910.00812", "submitter": "Shintaro Hashimoto", "authors": "Shintaro Hashimoto and Shonosuke Sugasawa", "title": "Robust Bayesian Regression with Synthetic Posterior", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": "10.3390/e22060661", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although linear regression models are fundamental tools in statistical\nscience, the estimation results can be sensitive to outliers. While several\nrobust methods have been proposed in frequentist frameworks, statistical\ninference is not necessarily straightforward. We here propose a Bayesian\napproach to robust inference on linear regression models using synthetic\nposterior distributions based on $\\gamma$-divergence, which enables us to\nnaturally assess the uncertainty of the estimation through the posterior\ndistribution. We also consider the use of shrinkage priors for the regression\ncoefficients to carry out robust Bayesian variable selection and estimation\nsimultaneously. We develop an efficient posterior computation algorithm by\nadopting the Bayesian bootstrap within Gibbs sampling. The performance of the\nproposed method is illustrated through simulation studies and applications to\nfamous datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 07:52:51 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 10:39:07 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hashimoto", "Shintaro", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "1910.01162", "submitter": "Kyunghee Han", "authors": "Kyunghee Han, Pamela A. Shaw, Thomas Lumley", "title": "Combining multiple imputation with raking of weights: An efficient and\n  robust approach in the setting of nearly-true models", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation provides us with efficient estimators in model-based\nmethods for handling missing data under the true model. It is also\nwell-understood that design-based estimators are robust methods that do not\nrequire accurately modeling the missing data; however, they can be inefficient.\nIn any applied setting, it is difficult to know whether a missing data model\nmay be good enough to win the bias-efficiency trade-off. Raking of weights is\none approach that relies on constructing an auxiliary variable from data\nobserved on the full cohort, which is then used to adjust the weights for the\nusual Horvitz-Thompson estimator. Computing the optimally efficient raking\nestimator requires evaluating the expectation of the efficient score given the\nfull cohort data, which is generally infeasible. We demonstrate multiple\nimputation (MI) as a practical method to compute a raking estimator that will\nbe optimal. We compare this estimator to common parametric and semi-parametric\nestimators, including standard multiple imputation. We show that while\nestimators, such as the semi-parametric maximum likelihood and MI estimator,\nobtain optimal performance under the true model, the proposed raking estimator\nutilizing MI maintains a better robustness-efficiency trade-off even under mild\nmodel misspecification. We also show that the standard raking estimator,\nwithout MI, is often competitive with the optimal raking estimator. We\ndemonstrate these properties through several numerical examples and provide a\ntheoretical discussion of conditions for asymptotically superior relative\nefficiency of the proposed raking estimator.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 18:50:10 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 23:26:42 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Han", "Kyunghee", ""], ["Shaw", "Pamela A.", ""], ["Lumley", "Thomas", ""]]}, {"id": "1910.01396", "submitter": "Subhro Ghosh", "authors": "Subhro Ghosh and Sanjay Chaudhuri", "title": "Maximum Likelihood under constraints: Degeneracies and Random Critical\n  Points", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of semi-parametric maximum likelihood under\nconstraints on summary statistics. Such a procedure results in a discrete\nprobability distribution that maximises the likelihood among all such\ndistributions under the specified constraints (called estimating equations),\nand is an approximation to the underlying population distribution. The study of\nsuch empirical likelihood originates from the seminal work of Owen. We\ninvestigate this procedure in the setting of mis-specified (or biased)\nestimating equations, i.e. when the null hypothesis is not true. We establish\nthat the behaviour of the optimal distribution under such mis-specification\ndiffer markedly from their properties under the null, i.e. when the estimating\nequations are unbiased and correctly specified. This is manifested by certain\ndegeneracies in the optimal distribution which define the likelihood. Such\ndegeneracies are not observed under the null. Furthermore, we establish an\nanomalous behaviour of the log-likelihood based Wilks statistic, which, unlike\nunder the null, does not exhibit a chi-squared limit. In the Bayesian setting,\nwe rigorously establish the posterior consistency of procedures based on these\nideas, where instead of a parametric likelihood, an empirical likelihood is\nused to define the posterior distribution. In particular, we show that this\nposterior, as a random probability measure, rapidly converges to the delta\nmeasure at the true parameter value. A novel feature of our approach is the\ninvestigation of critical points of random functions in the context of such\nempirical likelihood. In particular, we obtain the location and the mass of the\ndegenerate optimal weights as the leading and sub-leading terms in a canonical\nexpansion of a particular critical point of a random function that is naturally\nassociated with the model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 10:50:50 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:59:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ghosh", "Subhro", ""], ["Chaudhuri", "Sanjay", ""]]}, {"id": "1910.01547", "submitter": "Teo Deveney", "authors": "Teo Deveney, Eike Mueller, Tony Shardlow", "title": "A deep surrogate approach to efficient Bayesian inversion in PDE and\n  integral equation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a deep learning approach to efficiently perform Bayesian\ninference in partial differential equation (PDE) and integral equation models\nover potentially high-dimensional parameter spaces. The contributions of this\npaper are two-fold; the first is the introduction of a neural network approach\nto approximating the solutions of Fredholm and Volterra integral equations of\nthe first and second kind. The second is the development of a new, efficient\ndeep learning-based method for Bayesian inversion applied to problems that can\nbe described by PDEs or integral equations. To achieve this we introduce a\nsurrogate model, and demonstrate how this allows efficient sampling from a\nBayesian posterior distribution in which the likelihood depends on the\nsolutions of PDEs or integral equations. Our method relies on the direct\napproximation of parametric solutions by neural networks, without need of\ntraditional numerical solves. This deep learning approach allows the accurate\nand efficient approximation of parametric solutions in significantly higher\ndimensions than is possible using classical discretisation schemes. Since the\napproximated solutions can be cheaply evaluated, the solutions of Bayesian\ninverse problems over large parameter spaces are efficient using Markov chain\nMonte Carlo. We demonstrate the performance of our method using two real-world\nexamples; these include Bayesian inference in the PDE and integral equation\ncase for an example from electrochemistry, and Bayesian inference of a\nfunction-valued heat-transfer parameter with applications in aviation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:12:02 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 16:57:23 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 17:12:46 GMT"}, {"version": "v4", "created": "Thu, 25 Mar 2021 13:18:52 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Deveney", "Teo", ""], ["Mueller", "Eike", ""], ["Shardlow", "Tony", ""]]}, {"id": "1910.01569", "submitter": "Kamiar Radnosrati", "authors": "Kamiar Radnosrati and Gustaf Hendeby and Fredrik Gustafsson", "title": "Exploring Positive Noise in Estimation Theory", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2999204", "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of a deterministic quantity observed in non-Gaussian additive\nnoise is explored via order statistics approach. More specifically, we study\nthe estimation problem when measurement noises either have positive supports or\nfollow a mixture of normal and uniform distribution. This is a problem of great\ninterest specially in cellular positioning systems where the wireless signal is\nprone to multiple sources of noises which generally have a positive support.\nMultiple noise distributions are investigated and, if possible, minimum\nvariance unbiased (MVU) estimators are derived. In case of uniform, exponential\nand Rayleigh noise distributions, unbiased estimators without any knowledge of\nthe hyper parameters of the noise distributions are also given. For each noise\ndistribution, the proposed order statistic-based estimator's performance, in\nterms of mean squared error, is compared to the best linear unbiased estimator\n(BLUE), as a function of sample size, in a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:14:57 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 17:23:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Radnosrati", "Kamiar", ""], ["Hendeby", "Gustaf", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1910.01623", "submitter": "Martin Slawski", "authors": "Martin Slawski, Guoqing Diao, Emanuel Ben-David", "title": "A Pseudo-Likelihood Approach to Linear Regression with Partially\n  Shuffled Data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been significant interest in linear regression in the\nsituation where predictors and responses are not observed in matching pairs\ncorresponding to the same statistical unit as a consequence of separate data\ncollection and uncertainty in data integration. Mismatched pairs can\nconsiderably impact the model fit and disrupt the estimation of regression\nparameters. In this paper, we present a method to adjust for such mismatches\nunder ``partial shuffling\" in which a sufficiently large fraction of\n(predictors, response)-pairs are observed in their correct correspondence. The\nproposed approach is based on a pseudo-likelihood in which each term takes the\nform of a two-component mixture density. Expectation-Maximization schemes are\nproposed for optimization, which (i) scale favorably in the number of samples,\nand (ii) achieve excellent statistical performance relative to an oracle that\nhas access to the correct pairings as certified by simulations and case\nstudies. In particular, the proposed approach can tolerate considerably larger\nfraction of mismatches than existing approaches, and enables estimation of the\nnoise level as well as the fraction of mismatches. Inference for the resulting\nestimator (standard errors, confidence intervals) can be based on established\ntheory for composite likelihood estimation. Along the way, we also propose a\nstatistical test for the presence of mismatches and establish its consistency\nunder suitable conditions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 17:43:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Slawski", "Martin", ""], ["Diao", "Guoqing", ""], ["Ben-David", "Emanuel", ""]]}, {"id": "1910.01692", "submitter": "Sonja Petrovic", "authors": "Elizabeth Gross and Vishesh Karwa and Sonja Petrovi\\'c", "title": "Algebraic statistics, tables, and networks: The Fienberg advantage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stephen Fienberg's affinity for contingency table problems and reinterpreting\nmodels with a fresh look gave rise to a new approach for hypothesis testing of\nnetwork models that are linear exponential families. We outline his vision and\ninfluence in this fundamental problem, as well as generalizations to\nmultigraphs and hypergraphs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:10:56 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Gross", "Elizabeth", ""], ["Karwa", "Vishesh", ""], ["Petrovi\u0107", "Sonja", ""]]}, {"id": "1910.01734", "submitter": "Jinchi Lv", "authors": "Jianqing Fan, Yingying Fan, Xiao Han, Jinchi Lv", "title": "SIMPLE: Statistical Inference on Membership Profiles in Large Networks", "comments": "60 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data is prevalent in many contemporary big data applications in which\na common interest is to unveil important latent links between different pairs\nof nodes. Yet a simple fundamental question of how to precisely quantify the\nstatistical uncertainty associated with the identification of latent links\nstill remains largely unexplored. In this paper, we propose the method of\nstatistical inference on membership profiles in large networks (SIMPLE) in the\nsetting of degree-corrected mixed membership model, where the null hypothesis\nassumes that the pair of nodes share the same profile of community memberships.\nIn the simpler case of no degree heterogeneity, the model reduces to the mixed\nmembership model for which an alternative more robust test is also proposed.\nBoth tests are of the Hotelling-type statistics based on the rows of empirical\neigenvectors or their ratios, whose asymptotic covariance matrices are very\nchallenging to derive and estimate. Nevertheless, their analytical expressions\nare unveiled and the unknown covariance matrices are consistently estimated.\nUnder some mild regularity conditions, we establish the exact limiting\ndistributions of the two forms of SIMPLE test statistics under the null\nhypothesis and contiguous alternative hypothesis. They are the chi-square\ndistributions and the noncentral chi-square distributions, respectively, with\ndegrees of freedom depending on whether the degrees are corrected or not. We\nalso address the important issue of estimating the unknown number of\ncommunities and establish the asymptotic properties of the associated test\nstatistics. The advantages and practical utility of our new procedures in terms\nof both size and power are demonstrated through several simulation examples and\nreal network applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:01:39 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Fan", "Jianqing", ""], ["Fan", "Yingying", ""], ["Han", "Xiao", ""], ["Lv", "Jinchi", ""]]}, {"id": "1910.01754", "submitter": "Jialei Chen", "authors": "Jialei Chen, Simon Mak, V. Roshan Joseph and Chuck Zhang", "title": "Function-on-function kriging, with applications to 3D printing of aortic\n  tissues", "comments": null, "journal-ref": "Technometrics,2020", "doi": "10.1080/00401706.2020.1801255", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D-printed medical prototypes, which use synthetic metamaterials to mimic\nbiological tissue, are becoming increasingly important in urgent surgical\napplications. However, the mimicking of tissue mechanical properties via\n3D-printed metamaterial can be difficult and time-consuming, due to the\nfunctional nature of both inputs (metamaterial structure) and outputs\n(mechanical response curve). To deal with this, we propose a novel\nfunction-on-function kriging model for efficient emulation and tissue-mimicking\noptimization. For functional inputs, a key novelty of our model is the\nspectral-distance (SpeD) correlation function, which captures important\nspectral differences between two functional inputs. Dependencies for functional\noutputs are then modeled via a co-kriging framework. We further adopt shrinkage\npriors on both the input spectra and the output co-kriging covariance matrix,\nwhich allows the emulator to learn and incorporate important physics (e.g.,\ndominant input frequencies, output curve properties). Finally, we demonstrate\nthe effectiveness of the proposed SpeD emulator in a real-world study on\nmimicking human aortic tissue, and show that it can provide quicker and more\naccurate tissue-mimicking performance compared to existing methods in the\nmedical literature.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:53:28 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 02:53:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Jialei", ""], ["Mak", "Simon", ""], ["Joseph", "V. Roshan", ""], ["Zhang", "Chuck", ""]]}, {"id": "1910.01793", "submitter": "Yingbo Li", "authors": "Yingbo Li, Robert Cezeaux, Di Yu", "title": "Automating Data Monitoring: Detecting Structural Breaks in Time Series\n  Data Using Bayesian Minimum Description Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern business modeling and analytics, data monitoring plays a critical\nrole. Nowadays, sophisticated models often rely on hundreds or even thousands\nof input variables. Over time, structural changes such as abrupt level shifts\nor trend slope changes may occur among some of these variables, likely due to\nchanges in economy or government policies. As a part of data monitoring, it is\nimportant to identify these changepoints, in terms of which variables exhibit\nsuch changes, and what time locations do the changepoints occur. Being alerted\nabout the changepoints can help modelers decide if models need modification or\nrebuilds, while ignoring them may increase risks of model degrading. Simple\nprocess control rules often flag too many false alarms because regular seasonal\nfluctuations or steady upward or downward trends usually trigger alerts. To\nreduce potential false alarms, we create a novel statistical method based on\nthe Bayesian Minimum Description Length (BMDL) framework to perform multiple\nchange-point detection. Our method is capable of detecting all structural\nbreaks occurred in the past, and automatically handling data with or without\nseasonality and/or autocorrelation. It is implemented with computation\nalgorithms such as Markov chain Monte Carlo (MCMC), and can be applied to all\nvariables in parallel. As an explainable anomaly detection tool, our\nchangepoint detection method not only triggers alerts, but provides useful\ninformation about the structural breaks, such as the times of changepoints, and\nestimation of mean levels and linear slopes before and after the changepoints.\nThis makes future business analysis and evaluation on the structural breaks\neasier.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 04:03:02 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Li", "Yingbo", ""], ["Cezeaux", "Robert", ""], ["Yu", "Di", ""]]}, {"id": "1910.01799", "submitter": "Matt Wand", "authors": "Marianne Menictas, Gioia Di Credico and Matt P. Wand", "title": "Streamlined Variational Inference for Linear Mixed Models with Crossed\n  Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive streamlined mean field variational Bayes algorithms for fitting\nlinear mixed models with crossed random effects. In the most general situation,\nwhere the dimensions of the crossed groups are arbitrarily large, streamlining\nis hindered by lack of sparseness in the underlying least squares system.\nBecause of this fact we also consider a hierarchy of relaxations of the mean\nfield product restriction. The least stringent product restriction delivers a\nhigh degree of inferential accuracy. However, this accuracy must be mitigated\nagainst its higher storage and computing demands. Faster sparse storage and\ncomputing alternatives are also provided, but come with the price of diminished\ninferential accuracy. This article provides full algorithmic details of three\nvariational inference strategies, presents detailed empirical results on their\npros and cons and, thus, guides the users on their choice of variational\ninference approach depending on the problem size and computing resources.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 04:49:49 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 22:27:08 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Menictas", "Marianne", ""], ["Di Credico", "Gioia", ""], ["Wand", "Matt P.", ""]]}, {"id": "1910.01931", "submitter": "Marianna Pensky", "authors": "Majid Noroozi, Marianna Pensky and Ramchandra Rimal", "title": "Sparse Popularity Adjusted Stochastic Block Model", "comments": "4 figures. arXiv admin note: text overlap with arXiv:1902.00431", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we study a sparse stochastic network enabled with a\nblock structure. The popular Stochastic Block Model (SBM) and the Degree\nCorrected Block Model (DCBM) address sparsity by placing an upper bound on the\nmaximum probability of connections between any pair of nodes. As a result,\nsparsity describes only the behavior of network as a whole, without\ndistinguishing between the block-dependent sparsity patterns. To the best of\nour knowledge, the recently introduced Popularity Adjusted Block Model (PABM)\nis the only block model that allows to introduce a {\\it structural sparsity}\nwhere some probabilities of connections are identically equal to zero while the\nrest of them remain above a certain threshold. The latter presents a more\nnuanced view of the network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 14:20:05 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:06:34 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Noroozi", "Majid", ""], ["Pensky", "Marianna", ""], ["Rimal", "Ramchandra", ""]]}, {"id": "1910.02087", "submitter": "Allison Meisner", "authors": "Allison Meisner, Marco Carone, Margaret S. Pepe, and Kathleen F. Kerr", "title": "Combining Biomarkers by Maximizing the True Positive Rate for a Fixed\n  False Positive Rate", "comments": "37 pages (including appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomarkers abound in many areas of clinical research, and often investigators\nare interested in combining them for diagnosis, prognosis, or screening. In\nmany applications, the true positive rate for a biomarker combination at a\nprespecified, clinically acceptable false positive rate is the most relevant\nmeasure of predictive capacity. We propose a distribution-free method for\nconstructing biomarker combinations by maximizing the true positive rate while\nconstraining the false positive rate. Theoretical results demonstrate desirable\nproperties of biomarker combinations produced by the new method. In\nsimulations, the biomarker combination provided by our method demonstrated\nimproved operating characteristics in a variety of scenarios when compared with\nalternative methods for constructing biomarker combinations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:02:23 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Meisner", "Allison", ""], ["Carone", "Marco", ""], ["Pepe", "Margaret S.", ""], ["Kerr", "Kathleen F.", ""]]}, {"id": "1910.02105", "submitter": "Allison Meisner", "authors": "Allison Meisner, Chirag R. Parikh, and Kathleen F. Kerr", "title": "Developing Biomarker Combinations in Multicenter Studies via Direct\n  Maximization and Penalization", "comments": "31 pages (including appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a study of acute kidney injury, we consider the setting of\nbiomarker studies involving patients at multiple centers where the goal is to\ndevelop a biomarker combination for diagnosis, prognosis, or screening. As\nbiomarker studies become larger, this type of data structure will be\nencountered more frequently. In the presence of multiple centers, one way to\nassess the predictive capacity of a given combination is to consider the\ncenter-adjusted AUC (aAUC), a summary of the ability of the combination to\ndiscriminate between cases and controls in each center. Rather than using a\ngeneral method, such as logistic regression, to construct the biomarker\ncombination, we propose directly maximizing the aAUC. Furthermore, it may be\ndesirable to have a biomarker combination with similar performance across\ncenters. To that end, we allow for penalization of the variability in the\ncenter-specific AUCs. We demonstrate desirable asymptotic properties of the\nresulting combinations. Simulations provide small-sample evidence that\nmaximizing the aAUC can lead to combinations with improved performance. We also\nuse simulated data to illustrate the utility of constructing combinations by\nmaximizing the aAUC while penalizing variability. Finally, we apply these\nmethods to data from the study of acute kidney injury.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 18:48:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Meisner", "Allison", ""], ["Parikh", "Chirag R.", ""], ["Kerr", "Kathleen F.", ""]]}, {"id": "1910.02170", "submitter": "Evan Rosenman", "authors": "Evan Rosenman and Karthik Rajkumar", "title": "Optimized Partial Identification Bounds for Regression Discontinuity\n  Designs with Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity (RD) design is one of the most popular\nquasi-experimental methods for applied causal inference. In practice, the\nmethod is quite sensitive to the assumption that individuals cannot control\ntheir value of a \"running variable\" that determines treatment status precisely.\nIf individuals are able to precisely manipulate their scores, then point\nidentification is lost. We propose a procedure for obtaining partial\nidentification bounds in the case of a discrete running variable where\nmanipulation is present. Our method relies on two stages: first, we derive the\ndistribution of non-manipulators under several assumptions about the data.\nSecond, we obtain bounds on the causal effect via a sequential convex\nprogramming approach. We also propose methods for tightening the partial\nidentification bounds using an auxiliary covariate, and derive confidence\nintervals via the bootstrap. We demonstrate the utility of our method on a\nsimulated dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 23:32:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rosenman", "Evan", ""], ["Rajkumar", "Karthik", ""]]}, {"id": "1910.02348", "submitter": "Hyebin Song", "authors": "Hyebin Song, Ran Dai, Garvesh Raskutti, Rina Foygel Barber", "title": "Convex and Non-convex Approaches for Statistical Inference with\n  Class-Conditional Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of estimation and testing in logistic regression with\nclass-conditional noise in the observed labels, which has an important\nimplication in the Positive-Unlabeled (PU) learning setting. With the key\nobservation that the label noise problem belongs to a special sub-class of\ngeneralized linear models (GLM), we discuss convex and non-convex approaches\nthat address this problem. A non-convex approach based on the maximum\nlikelihood estimation produces an estimator with several optimal properties,\nbut a convex approach has an obvious advantage in optimization. We demonstrate\nthat in the low-dimensional setting, both estimators are consistent and\nasymptotically normal, where the asymptotic variance of the non-convex\nestimator is smaller than the convex counterpart. We also quantify the\nefficiency gap which provides insight into when the two methods are comparable.\nIn the high-dimensional setting, we show that both estimation procedures\nachieve $\\ell_2$-consistency at the minimax optimal $\\sqrt{s\\log p/n}$ rates\nunder mild conditions. Finally, we propose an inference procedure using a\nde-biasing approach. We validate our theoretical findings through simulations\nand a real-data example.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 00:42:54 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:35:53 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Song", "Hyebin", ""], ["Dai", "Ran", ""], ["Raskutti", "Garvesh", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1910.02386", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar and Prashant Jha and Mohammad Arshad Rahman and\n  Joydeep Dutta", "title": "A New Graphical Device and Related Tests for the Shape of Non-parametric\n  Regression Function", "comments": "There were errors in mathematical proofs of Theorem 1 and related\n  lemmas. Major revisions were needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a non-parametric regression model $y = m(x) + \\epsilon$ and\npropose a novel graphical device to check whether the $r$-th ($r \\geqslant 1$)\nderivative of the regression function $m(x)$ is positive or otherwise. Since\nthe shape of the regression function can be completely characterized by its\nderivatives, the graphical device can correctly identify the shape of the\nregression function. The proposed device includes the check for monotonicity\nand convexity of the function as special cases. We also present an example to\nelucidate the practical utility of the graphical device. In addition, we employ\nthe graphical device to formulate a class of test statistics and derive its\nasymptotic distribution. The tests are exhibited in various simulated and real\ndata examples.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 06:44:42 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 07:28:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Rahman", "Mohammad Arshad", ""], ["Dutta", "Joydeep", ""]]}, {"id": "1910.02396", "submitter": "Xueheng Shi", "authors": "Xueheng Shi, Colin Gallagher", "title": "Estimating Unknown Cycles in Geophysical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples of cyclic (periodic) behavior in geophysical data abound. In many\ncases the primary period is known, such as in daily measurements of rain,\ntemperature, and sea level. However, many time series of measurements contain\ncycles of unknown or varying length. We consider the problem of estimating the\nunknown period in a time series.We review the basic methods, compare their\nperformance through a simulation studyusing observed sea level data, apply them\nto an astronomical data set, and discuss generalizations of the methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 08:33:03 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shi", "Xueheng", ""], ["Gallagher", "Colin", ""]]}, {"id": "1910.02406", "submitter": "Fabio Demarqui", "authors": "Fabio N. Demarqui, Vinicius D. Mayrink", "title": "A fully likelihood-based approach to model survival data with crossing\n  survival curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional hazards (PH), proportional odds (PO) and accelerated failure\ntime (AFT) models have been widely used to deal with survival data in different\nfields of knowledge. Despite their popularity, such models are not suitable to\nhandle survival data with crossing survival curves. Yang and Prentice (2005)\nproposed a semiparametric two-sample approach, denoted here as the YP model,\nallowing the analysis of crossing survival curves and including the PH and PO\nconfigurations as particular cases. In a general regression setting, the\npresent work proposes a fully likelihood-based approach to fit the YP model.\nThe main idea is to model the baseline hazard via the piecewise exponential\n(PE) distribution. The approach shares the flexibility of the semiparametric\nmodels and the tractability of the parametric representations. An extensive\nsimulation study is developed to evaluate the performance of the proposed\nmodel. In addition, we demonstrate how useful is the new method through the\nanalysis of survival times related to patients enrolled in a cancer clinical\ntrial. The simulation results indicate that our model performs well for\nmoderate sample sizes in the general regression setting. A superior performance\nis also observed with respect to the original YP model designed for the\ntwo-sample scenario.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 09:54:44 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Demarqui", "Fabio N.", ""], ["Mayrink", "Vinicius D.", ""]]}, {"id": "1910.02471", "submitter": "Leo Duan", "authors": "Leo L Duan, George Michailidis, Mingzhou Ding", "title": "Spiked Laplacian Graphs: Bayesian Community Detection in Heterogeneous\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network data analysis, it is becoming common to work with a collection of\ngraphs that exhibit \\emph{heterogeneity}. For example, neuroimaging data from\npatient cohorts are increasingly available. A critical analytical task is to\nidentify communities, and graph Laplacian-based methods are routinely used.\nHowever, these methods are currently limited to a single network and do not\nprovide measures of uncertainty on the community assignment. In this work, we\npropose a probabilistic network model called the ``Spiked Laplacian Graph''\nthat considers each network as an invertible transform of the Laplacian, with\nits eigenvalues modeled by a modified spiked structure. This effectively\nreduces the number of parameters in the eigenvectors, and their sign patterns\nallow efficient estimation of the community structure. Further, the posterior\ndistribution of the eigenvectors provides uncertainty quantification for the\ncommunity estimates. Subsequently, we introduce a Bayesian non-parametric\napproach to address the issue of heterogeneity in a collection of graphs.\nTheoretical results are established on the posterior consistency of the\nprocedure and provide insights on the trade-off between model resolution and\naccuracy. We illustrate the performance of the methodology on synthetic data\nsets, as well as a neuroscience study related to brain activity in working\nmemory.\n  Keywords: Hierarchical Community Detection, Isoperimetric Constant,\nMixed-Effect Eigendecomposition, Normalized Graph Cut, Stiefel Manifold\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 16:27:09 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 21:04:49 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Duan", "Leo L", ""], ["Michailidis", "George", ""], ["Ding", "Mingzhou", ""]]}, {"id": "1910.02505", "submitter": "Philip Versteeg", "authors": "Philip Versteeg, Joris M. Mooij", "title": "Boosting Local Causal Discovery in High-Dimensional Expression Data", "comments": "Accepted at BIBM / CABB 2019", "journal-ref": "2019 IEEE Intl. Conf. Bioinf. and Biomed. (BIBM 2019) pp.\n  2599-2604", "doi": "10.1109/BIBM47256.2019.8983232", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of Local Causal Discovery (LCD), a simple and\nefficient constraint-based method for causal discovery, in predicting causal\neffects in large-scale gene expression data. We construct practical estimators\nspecific to the high-dimensional regime. Inspired by the ICP algorithm, we use\nan optional preselection method and two different statistical tests.\nEmpirically, the resulting LCD estimator is seen to closely approach the\naccuracy of ICP, the state-of-the-art method, while it is algorithmically\nsimpler and computationally more efficient.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:16:23 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 16:19:35 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Versteeg", "Philip", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1910.02506", "submitter": "Subharup Guha", "authors": "Subharup Guha, Rex Jung and David Dunson", "title": "Predicting Phenotypes from Brain Connection Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the problem of predicting a response variable based\non a network-valued predictor. Our particular motivation is developing\ninterpretable and accurate predictive models for cognitive traits and\nneuro-psychiatric disorders based on an individual's brain connection network\n(connectome). Current methods focus on reducing the complex and\nhigh-dimensional brain network into a low-dimensional set of pre-specified\nfeatures prior to applying standard predictive algorithms. Such methods are\nsensitive to feature choice and inevitably discard information. We instead\npropose a nonparametric Bayes class of models that utilize information from the\nentire adjacency matrix defining connections among brain regions in adaptively\ndefining flexible predictive algorithms, while maintaining interpretability.\nThe proposed Bayesian Connectomics (BaCon) model class utilizes\nPoisson-Dirichlet processes to detect a lower-dimensional, bidirectional\n(covariate, subject) pattern in the adjacency matrix. The small n, large p\nproblem is transformed into a \"small n, small q\" problem, facilitating an\neffective stochastic search of the predictors. A spike-and-slab prior for the\ncluster predictors strikes a balance between regression model parsimony and\nflexibility, resulting in improved inferences and test case predictions. We\ndescribe basic properties of the BaCon model class and develop efficient\nalgorithms for posterior computation. The resulting methods are shown to\noutperform existing approaches in simulations and applied to a creative\nreasoning data set.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 19:19:05 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 16:23:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Guha", "Subharup", ""], ["Jung", "Rex", ""], ["Dunson", "David", ""]]}, {"id": "1910.02539", "submitter": "Lucy Gao", "authors": "Pengqi Liu, Lucy Gao, and Julie Zhou", "title": "R-optimal designs for multi-response regression models with\n  multi-factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate R-optimal designs for multi-response regression models with\nmulti-factors, where the random errors in these models are correlated. Several\ntheoretical results are derived for Roptimal designs, including scale\ninvariance, reflection symmetry, line and plane symmetry, and dependence on the\ncovariance matrix of the errors. All the results can be applied to linear and\nnonlinear models. In addition, an efficient algorithm based on an interior\npoint method is developed for finding R-optimal designs on discrete design\nspaces. The algorithm is very flexible, and can be applied to any\nmulti-response regression model.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 21:52:16 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Liu", "Pengqi", ""], ["Gao", "Lucy", ""], ["Zhou", "Julie", ""]]}, {"id": "1910.02542", "submitter": "Dhaker Hamza", "authors": "Hamza Dhaker, El Hadji Deme and Salah El-Adlouni", "title": "On Inference of Overlapping Coefficients in Two Inverse Lomax\n  Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Overlapping coefficient is a direct measure of similarity between two\ndistributions which is recently becoming very useful. This paper investigates\nestimation for some well-known measures of overlap, namely Matusita's measure\n$\\rho$, Weitzman's measure $\\Delta$ and $\\Lambda$ based on Kullback-Leibler.\nTwo estimation methods considered in this study are point estimation and\nBayesian approach. Two Inverse Lomax populations with different shape\nparameters are considered. The bias and mean square error properties of the\nestimators are studied through a simulation study and a real data example.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 22:21:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dhaker", "Hamza", ""], ["Deme", "El Hadji", ""], ["El-Adlouni", "Salah", ""]]}, {"id": "1910.02566", "submitter": "Purvasha Chakravarti", "authors": "Purvasha Chakravarti, Sivaraman Balakrishnan and Larry Wasserman", "title": "Gaussian Mixture Clustering Using Relative Tests of Fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider clustering based on significance tests for Gaussian Mixture\nModels (GMMs). Our starting point is the SigClust method developed by Liu et\nal. (2008), which introduces a test based on the k-means objective (with k = 2)\nto decide whether the data should be split into two clusters. When applied\nrecursively, this test yields a method for hierarchical clustering that is\nequipped with a significance guarantee. We study the limiting distribution and\npower of this approach in some examples and show that there are large regions\nof the parameter space where the power is low. We then introduce a new test\nbased on the idea of relative fit. Unlike prior work, we test for whether a\nmixture of Gaussians provides a better fit relative to a single Gaussian,\nwithout assuming that either model is correct. The proposed test has a simple\ncritical value and provides provable error control. One version of our test\nprovides exact, finite sample control of the type I error. We show how our\ntests can be used for hierarchical clustering as well as in a sequential manner\nfor model selection. We conclude with an extensive simulation study and a\ncluster analysis of a gene expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 00:51:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chakravarti", "Purvasha", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1910.02597", "submitter": "Zhigen Zhao", "authors": "Zhigen Zhao", "title": "Where to find needles in a haystack?", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many existing methods in multiple comparison, one starts with either\nFisher's p-values or the local fdr scores. The former one, with a usual\ndefinition as the tail probability exceeding the observed test statistic under\nthe null distribution, fails to use the information from the alternative\nhypothesis and the targeted region of signals could be completely wrong\nespecially when the likelihood ratio function is not monotone. The local fdr\nbased approaches, usually relying on the density functions, are optimal\noracally. However, the targeted region of the signals of the data-driven\nversion is problematic because of the slow convergence of the non-parametric\ndensity estimation especially on the boundaries. In this paper, we propose a\nnew method: Cdf and Local fdr Assisted multiple Testing method (CLAT), which is\noptimal for cases when the p-values based method are not. Additionally, the\ndata-driven version only relies on the estimation of the cumulative\ndistribution function and converges to the oracle version quickly. Both\nsimulations and real data analysis demonstrate the superior performance of the\nproposed method than the existing ones. Furthermore, the computation is\ninstantaneous based on a novel algorithm and is scalable to the large data set.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 03:55:54 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 01:12:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhao", "Zhigen", ""]]}, {"id": "1910.02722", "submitter": "Bernhard Spangl", "authors": "Bernhard Spangl, Norbert Kaiblinger, Peter Ruckdeschel, Dieter Rasch", "title": "Minimal sample size in balanced ANOVA models of crossed, nested, and\n  mixed classifications", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2021.1938126", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider balanced one-, two- and three-way ANOVA models to test the\nhypothesis that the fixed factor A has no effect. The other factors are fixed\nor random. We determine the noncentrality parameter for the exact F-test,\ndescribe its minimal value by a sharp lower bound, and thus we can guarantee\nthe worst case power for the F-test. These results allow us to compute the\nminimal sample size. We also provide a structural result for the minimum sample\nsize, proving a conjecture on the optimal experimental design.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 11:02:36 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 12:53:03 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Spangl", "Bernhard", ""], ["Kaiblinger", "Norbert", ""], ["Ruckdeschel", "Peter", ""], ["Rasch", "Dieter", ""]]}, {"id": "1910.02829", "submitter": "Ansgar Steland", "authors": "Yuan-Tsung Chang, Ansgar Steland", "title": "High-Confident Nonparametric Fixed-Width Uncertainty Intervals and\n  Applications to Projected High-Dimensional Data and Common Mean Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two-stage procedures to construct fixed-width confidence\nintervals are studied to quantify uncertainty. It is shown that the validity of\nthe random central limit theorem (RCLT) accompanied by a consistent and\nasymptotically unbiased estimator of the asymptotic variance already guarantees\nconsistency and first as well as second order efficiency of the two-stage\nprocedures. This holds under the common asymptotics where the length of the\nconfidence interval tends to $0$ as well as under the novel proposed\nhigh-confident asymptotics where the confidence level tends to $1$. The\napproach is motivated by and applicable to data analysis from distributed big\ndata with non-negligible costs of data queries. The following problems are\ndiscussed: Fixed-width intervals for a the mean, for a projection when\nobserving high-dimensional data and for the common mean when using nonlinear\ncommon mean estimators under order constraints. The procedures are investigated\nby simulations and illustrated by a real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 14:43:30 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chang", "Yuan-Tsung", ""], ["Steland", "Ansgar", ""]]}, {"id": "1910.02859", "submitter": "Paul McNicholas", "authors": "Nikola Pocuca, Michael P.B. Gallaugher, Katharine M. Clark and Paul D.\n  McNicholas", "title": "Assessing and Visualizing Matrix Variate Normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for assessing the matrix variate normality of three-way data is\ndeveloped. The framework comprises a visual method and a goodness of fit test\nbased on the Mahalanobis squared distance (MSD). The MSD of multivariate and\nmatrix variate normal estimators, respectively, are used as an assessment tool\nfor matrix variate normality. Specifically, these are used in the form of a\ndistance-distance (DD) plot as a graphical method for visualizing matrix\nvariate normality. In addition, we employ the popular Kolmogorov-Smirnov\ngoodness of fit test in the context of assessing matrix variate normality for\nthree-way data. Finally, an appropriate simulation study spanning a large range\nof dimensions and data sizes shows that for various settings, the test proves\nitself highly robust.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:38:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Pocuca", "Nikola", ""], ["Gallaugher", "Michael P. B.", ""], ["Clark", "Katharine M.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1910.02866", "submitter": "Dehan Kong", "authors": "Mark Koudstaal, Dengdeng Yu, Dehan Kong and Fang Yao", "title": "Nonparametric principal subspace regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific applications, multivariate observations often come in tandem\nwith temporal or spatial covariates, with which the underlying signals vary\nsmoothly. The standard approaches such as principal component analysis and\nfactor analysis neglect the smoothness of the data, while multivariate linear\nor nonparametric regression fail to leverage the correlation information among\nmultivariate response variables. We propose a novel approach named\nnonparametric principal subspace regression to overcome these issues. By\ndecoupling the model discrepancy, a simple and general two-step framework is\nintroduced, which leaves much flexibility in choice of model fitting. We\nestablish theoretical property of the general framework, and offer\nimplementation procedures that fulfill requirements and enjoy the theoretical\nguarantee. We demonstrate the favorable finite-sample performance of the\nproposed method through simulations and a real data application from an\nelectroencephalogram study.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:45:55 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 17:56:55 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 05:16:31 GMT"}, {"version": "v4", "created": "Sat, 12 Oct 2019 15:24:28 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Koudstaal", "Mark", ""], ["Yu", "Dengdeng", ""], ["Kong", "Dehan", ""], ["Yao", "Fang", ""]]}, {"id": "1910.02958", "submitter": "Robert Feldmann", "authors": "R. Feldmann", "title": "LEO-Py: Estimating likelihoods for correlated, censored, and uncertain\n  data with given marginal distributions", "comments": "21 pages, 8 figures, 2 tables, to appear in Astronomy and Computing,\n  LEO-Py is available at github.com/rfeldmann/leopy", "journal-ref": null, "doi": "10.1016/j.ascom.2019.100331", "report-no": null, "categories": "astro-ph.IM astro-ph.GA stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data with uncertain, missing, censored, and correlated values are commonplace\nin many research fields including astronomy. Unfortunately, such data are often\ntreated in an ad hoc way in the astronomical literature potentially resulting\nin inconsistent parameter estimates. Furthermore, in a realistic setting, the\nvariables of interest or their errors may have non-normal distributions which\ncomplicates the modeling. I present a novel approach to compute the likelihood\nfunction for such data sets. This approach employs Gaussian copulas to decouple\nthe correlation structure of variables and their marginal distributions\nresulting in a flexible method to compute likelihood functions of data in the\npresence of measurement uncertainty, censoring, and missing data. I demonstrate\nits use by determining the slope and intrinsic scatter of the star forming\nsequence of nearby galaxies from observational data. The outlined algorithm is\nimplemented as the flexible, easy-to-use, open-source Python package LEO-Py.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:00:00 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Feldmann", "R.", ""]]}, {"id": "1910.02986", "submitter": "Emily C Hector", "authors": "Emily C. Hector and Peter X.-K. Song", "title": "A Distributed and Integrated Method of Moments for High-Dimensional\n  Correlated Data Analysis", "comments": "35 pages, 5 figures, 3 tables", "journal-ref": "Journal of the American Statistical Association (2020) 1-14", "doi": "10.1080/01621459.2020.1736082", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is motivated by a regression analysis of electroencephalography\n(EEG) neuroimaging data with high-dimensional correlated responses with\nmulti-level nested correlations. We develop a divide-and-conquer procedure\nimplemented in a fully distributed and parallelized computational scheme for\nstatistical estimation and inference of regression parameters. Despite\nsignificant efforts in the literature, the computational bottleneck associated\nwith high-dimensional likelihoods prevents the scalability of existing methods.\nThe proposed method addresses this challenge by dividing responses into\nsubvectors to be analyzed separately and in parallel on a distributed platform\nusing pairwise composite likelihood. Theoretical challenges related to\ncombining results from dependent data are overcome in a statistically efficient\nway using a meta-estimator derived from Hansen's generalized method of moments.\nWe provide a rigorous theoretical framework for efficient estimation,\ninference, and goodness-of-fit tests. We develop an R package for ease of\nimplementation. We illustrate our method's performance with simulations and the\nanalysis of the EEG data, and find that iron deficiency is significantly\nassociated with two auditory recognition memory related potentials in the left\nparietal-occipital region of the brain.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:12:53 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:12:03 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 19:16:01 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hector", "Emily C.", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "1910.03021", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Isaac Lavine, Amy H. Herring and David B. Dunson", "title": "Perturbed factor analysis: Accounting for group differences in exposure\n  profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate group differences in phthalate exposure\nprofiles using NHANES data. Phthalates are a family of industrial chemicals\nused in plastics and as solvents. There is increasing evidence of adverse\nhealth effects of exposure to phthalates on reproduction and neuro-development,\nand concern about racial disparities in exposure. We would like to identify a\nsingle set of low-dimensional factors summarizing exposure to different\nchemicals, while allowing differences across groups. Improving on current\nmulti-group additive factor models, we propose a class of Perturbed Factor\nAnalysis (PFA) models that assume a common factor structure after perturbing\nthe data via multiplication by a group-specific matrix. Bayesian inference\nalgorithms are defined using a matrix normal hierarchical model for the\nperturbation matrices. The resulting model is just as flexible as current\napproaches in allowing arbitrarily large differences across groups but has\nsubstantial advantages that we illustrate in simulation studies. Applying PFA\nto NHANES data, we learn common factors summarizing exposures to phthalates,\nwhile showing clear differences across groups.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:08:26 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 14:25:21 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 05:08:52 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Roy", "Arkaprava", ""], ["Lavine", "Isaac", ""], ["Herring", "Amy H.", ""], ["Dunson", "David B.", ""]]}, {"id": "1910.03109", "submitter": "Serena Ng", "authors": "Kashif Yousuf and Serena Ng", "title": "Boosting High Dimensional Predictive Regressions with Time Varying\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional predictive regressions are useful in wide range of\napplications. However, the theory is mainly developed assuming that the model\nis stationary with time invariant parameters. This is at odds with the\nprevalent evidence for parameter instability in economic time series, but\ntheories for parameter instability are mainly developed for models with a small\nnumber of covariates. In this paper, we present two $L_2$ boosting algorithms\nfor estimating high dimensional models in which the coefficients are modeled as\nfunctions evolving smoothly over time and the predictors are locally\nstationary. The first method uses componentwise local constant estimators as\nbase learner, while the second relies on componentwise local linear estimators.\nWe establish consistency of both methods, and address the practical issues of\nchoosing the bandwidth for the base learners and the number of boosting\niterations. In an extensive application to macroeconomic forecasting with many\npotential predictors, we find that the benefits to modeling time variation are\nsubstantial and they increase with the forecast horizon. Furthermore, the\ntiming of the benefits suggests that the Great Moderation is associated with\nsubstantial instability in the conditional mean of various economic series.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:58:27 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Yousuf", "Kashif", ""], ["Ng", "Serena", ""]]}, {"id": "1910.03120", "submitter": "Lulu Kang", "authors": "Jiuhai Chen, Lulu Kang, Guang Lin", "title": "Gaussian Process Assisted Active Learning of Physical Laws", "comments": "27 pages, 5 figures, 10 tables", "journal-ref": "Technometrics 2020", "doi": "10.1080/00401706.2020.1817790", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas of science and engineering, discovering the governing\ndifferential equations from the noisy experimental data is an essential\nchallenge. It is also a critical step in understanding the physical phenomena\nand prediction of the future behaviors of the systems. However, in many cases,\nit is expensive or time-consuming to collect experimental data. This article\nprovides an active learning approach to estimate the unknown differential\nequations accurately with reduced experimental data size. We propose an\nadaptive design criterion combining the D-optimality and the maximin\nspace-filling criterion. In contrast to active learning for other regression\nmodels, the D-optimality here requires the unknown solution of the differential\nequations and derivatives of the solution. We estimate the Gaussian process\n(GP) regression models from the available experimental data and use them as the\nsurrogates of these unknown solution functions. The derivatives of the\nestimated GP models are derived and used to substitute the derivatives of the\nsolution. Variable selection-based regression methods are used to learn the\ndifferential equations from the experimental data. Through multiple case\nstudies, we demonstrate the proposed approach outperforms the D-optimality and\nthe maximin space-filling design alone in terms of model accuracy and data\neconomy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 22:48:19 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 19:54:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chen", "Jiuhai", ""], ["Kang", "Lulu", ""], ["Lin", "Guang", ""]]}, {"id": "1910.03134", "submitter": "Javier Zapata", "authors": "Javier Zapata, Sang-Yun Oh, Alexander Petersen", "title": "Partial Separability and Functional Graphical Models for Multivariate\n  Gaussian Processes", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The covariance structure of multivariate functional data can be highly\ncomplex, especially if the multivariate dimension is large, making extension of\nstatistical methods for standard multivariate data to the functional data\nsetting quite challenging. For example, Gaussian graphical models have recently\nbeen extended to the setting of multivariate functional data by applying\nmultivariate methods to the coefficients of truncated basis expansions.\nHowever, a key difficulty compared to multivariate data is that the covariance\noperator is compact, and thus not invertible. The methodology in this paper\naddresses the general problem of covariance modeling for multivariate\nfunctional data, and functional Gaussian graphical models in particular. As a\nfirst step, a new notion of separability for multivariate functional data is\nproposed, termed partial separability, leading to a novel Karhunen-Lo\\`eve-type\nexpansion for such data. Next, the partial separability structure is shown to\nbe particularly useful in order to provide a well-defined Gaussian graphical\nmodel that can be identified with a sequence of finite-dimensional graphical\nmodels, each of fixed dimension. This motivates a simple and efficient\nestimation procedure through application of the joint graphical lasso.\nEmpirical performance of the method for graphical model estimation is assessed\nthrough simulation and analysis of functional brain connectivity during a motor\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 23:42:21 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 18:58:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 03:16:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zapata", "Javier", ""], ["Oh", "Sang-Yun", ""], ["Petersen", "Alexander", ""]]}, {"id": "1910.03266", "submitter": "Giovanni Maria Merola", "authors": "Giovanni Maria Merola", "title": "SIMPCA: A framework for rotating and sparsifying principal components", "comments": "Accepted for publication by Journal of Applied Statistics on Oct 2,\n  2019", "journal-ref": null, "doi": "10.1080/02664763.2019.1676404", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithmic framework for computing sparse components from\nrotated principal components. This methodology, called SIMPCA, is useful to\nreplace the unreliable practice of ignoring small coefficients of rotated\ncomponents when interpreting them. The algorithm computes genuinely sparse\ncomponents by projecting rotated principal components onto subsets of\nvariables. The so simplified components are highly correlated with the\ncorresponding components. By choosing different simplification strategies\ndifferent sparse solutions can be obtained which can be used to compare\nalternative interpretations of the principal components. We give some examples\nof how effective simplified solutions can be achieved with SIMPCA using some\npublicly available data sets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 08:17:38 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Merola", "Giovanni Maria", ""]]}, {"id": "1910.03304", "submitter": "Ottmar Cronie", "authors": "Ottmar Cronie, Mehdi Moradi, Jorge Mateu", "title": "Inhomogeneous higher-order summary statistics for linear network point\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of intensity reweighted moment pseudostationary point\nprocesses on linear networks. Based on arbitrary general regular linear network\ndistances, we propose geometrically corrected versions of different\nhigher-order summary statistics, including the inhomogeneous empty space\nfunction, the inhomogeneous nearest neighbour distance distribution function\nand the inhomogeneous $J$-function. We also discuss their non-parametric\nestimators. Through a simulation study, considering models with different types\nof spatial interaction, we study the performance of our proposed summary\nstatistics. Finally, we make use of our methodology to analyse two datasets:\nmotor vehicle traffic accidents and spider data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 09:49:39 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Cronie", "Ottmar", ""], ["Moradi", "Mehdi", ""], ["Mateu", "Jorge", ""]]}, {"id": "1910.03481", "submitter": "Carlo Albert", "authors": "David Machac, Peter Reichert, J\\\"org Rieckermann, Dario Del Giudice,\n  Carlo Albert", "title": "Accelerating Bayesian inference in hydrological modeling with a\n  mechanistic emulator", "comments": null, "journal-ref": "Env. Mod. and Soft. 109, 66-79, 2018", "doi": "10.1016/j.envsoft.2018.07.016", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in many fields of dynamic modeling, the long runtime of hydrological\nmodels hinders Bayesian inference of model parameters from data. By replacing a\nmodel with an approximation of its output as a function of input and/or\nparameters, emulation allows us to complete this task by trading-off accuracy\nfor speed. We combine (i) the use of a mechanistic emulator, (ii)\nlow-discrepancy sampling of the parameter space, and (iii) iterative refinement\nof the design data set, to perform Bayesian inference with a very small design\ndata set constructed with 128 model runs in a parameter space of up to eight\ndimensions. In our didactic example we use a model implemented with the\nhydrological simulator SWMM that allows us to compare our inference results\nagainst those derived with the full model. This comparison demonstrates that\niterative improvements lead to reasonable results with a very small design data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 13:43:28 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Machac", "David", ""], ["Reichert", "Peter", ""], ["Rieckermann", "J\u00f6rg", ""], ["Del Giudice", "Dario", ""], ["Albert", "Carlo", ""]]}, {"id": "1910.03531", "submitter": "Daniel Scharfstein", "authors": "Yi Lu, Daniel O. Scharfstein, Maria M. Brooks, Kevin Quach, Edward H.\n  Kennedy", "title": "Causal Inference for Comprehensive Cohort Studies", "comments": "34 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a comprehensive cohort study of two competing treatments (say, A and B),\nclinically eligible individuals are first asked to enroll in a randomized trial\nand, if they refuse, are then asked to enroll in a parallel observational study\nin which they can choose treatment according to their own preference. We\nconsider estimation of two estimands: (1) comprehensive cohort causal effect --\nthe difference in mean potential outcomes had all patients in the comprehensive\ncohort received treatment A vs. treatment B and (2) randomized trial causal\neffect -- the difference in mean potential outcomes had all patients enrolled\nin the randomized trial received treatment A vs. treatment B. For each\nestimand, we consider inference under various sets of unconfoundedness\nassumptions and construct semiparametric efficient and robust estimators. These\nestimators depend on nuisance functions, which we estimate, for illustrative\npurposes, using generalized additive models. Using the theory of sample\nsplitting, we establish the asymptotic properties of our proposed estimators.\nWe also illustrate our methodology using data from the Bypass Angioplasty\nRevascularization Investigation (BARI) randomized trial and observational\nregistry to evaluate the effect of percutaneous transluminal coronary balloon\nangioplasty versus coronary artery bypass grafting on 5-year mortality. To\nevaluate the finite sample performance of our estimators, we use the BARI\ndataset as the basis of a realistic simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:42:45 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Lu", "Yi", ""], ["Scharfstein", "Daniel O.", ""], ["Brooks", "Maria M.", ""], ["Quach", "Kevin", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1910.03536", "submitter": "Michael Hudgens", "authors": "Sujatro Chakladar, Michael G. Hudgens, M. Elizabeth Halloran, John D.\n  Clemens, Mohammad Ali, Michael E. Emch", "title": "Inverse Probability Weighted Estimators of Vaccine Effects Accommodating\n  Partial Interference and Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating population-level effects of a vaccine is challenging because there\nmay be interference, i.e., the outcome of one individual may depend on the\nvaccination status of another individual. Partial interference occurs when\nindividuals can be partitioned into groups such that interference occurs only\nwithin groups. In the absence of interference, inverse probability weighted\n(IPW) estimators are commonly used to draw inference about causal effects of an\nexposure or treatment. Tchetgen Tchetgen and VanderWeele (2012) proposed a\nmodified IPW estimator for causal effects in the presence of partial\ninterference. Motivated by a cholera vaccine study in Bangladesh, this paper\nconsiders an extension of the Tchetgen Tchetgen and VanderWeele IPW estimator\nto the setting where the outcome is subject to right censoring using inverse\nprobability of censoring weights (IPCW). Censoring weights are estimated using\nproportional hazards frailty models. The large sample properties of the IPCW\nestimators are derived, and simulation studies are presented demonstrating the\nestimators' performance in finite samples. The methods are then used to analyze\ndata from the cholera vaccine study.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:46:32 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Chakladar", "Sujatro", ""], ["Hudgens", "Michael G.", ""], ["Halloran", "M. Elizabeth", ""], ["Clemens", "John D.", ""], ["Ali", "Mohammad", ""], ["Emch", "Michael E.", ""]]}, {"id": "1910.03577", "submitter": "Jin Ming", "authors": "Suprateek Kundu, Jin Ming, and Jennifer Stevens", "title": "Dynamic Brain Functional Networks Guided By Anatomical Knowledge", "comments": "45 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the potential of dynamic brain networks as a neuroimaging\nbiomarkers for mental illnesses is being increasingly recognized. However,\nthere are several unmet challenges in developing such biomarkers, including the\nneed for methods to model rapidly changing network states. In one of the first\nsuch efforts, we develop a novel approach for computing dynamic brain\nfunctional connectivity (FC), that is guided by brain structural connectivity\n(SC) computed from diffusion tensor imaging (DTI) data. The proposed approach\ninvolving dynamic Gaussian graphical models decomposes the time course into\nnon-overlapping state phases determined by change points, each having a\ndistinct network. We develop an optimization algorithm to implement the method\nsuch that the estimation of both the change points and the state-phase specific\nnetworks are fully data driven and unsupervised, and guided by SC information.\nThe approach is scalable to large dimensions and extensive simulations\nillustrate its clear advantages over existing methods in terms of network\nestimation accuracy and detecting dynamic network changes. An application of\nthe method to a posttraumatic stress disorder (PTSD) study reveals important\ndynamic resting state connections in regions of the brain previously implicated\nin PTSD. We also illustrate that the dynamic networks computed under the\nproposed method are able to better predict psychological resilience among\ntrauma exposed individuals compared to existing dynamic and stationary\nconnectivity approaches, which highlights its potential as a neuroimaging\nbiomarker.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 21:48:27 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kundu", "Suprateek", ""], ["Ming", "Jin", ""], ["Stevens", "Jennifer", ""]]}, {"id": "1910.03627", "submitter": "Guo Yu", "authors": "Guo Yu and Daniela Witten and Jacob Bien", "title": "Controlling Costs: Feature Selection on a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional framework for feature selection treats all features as\ncosting the same amount. However, in reality, a scientist often has\nconsiderable discretion regarding what variables to measure, and the decision\ninvolves a tradeoff between model accuracy and cost (where cost can refer to\nmoney, time, difficulty, or intrusiveness). In particular, unnecessarily\nincluding an expensive feature in a model is worse than unnecessarily including\na cheap feature. We propose a procedure, which we call cheap knockoffs, for\nperforming feature selection in a cost-conscious manner. The key idea behind\nour method is to force higher cost features to compete with more knockoffs than\ncheaper features. We derive an upper bound on the weighted false discovery\nproportion associated with this procedure, which corresponds to the fraction of\nthe feature cost that is wasted on unimportant features. We prove that this\nbound holds simultaneously with high probability over a path of selected\nvariable sets of increasing size. A user may thus select a set of features\nbased, for example, on the overall budget, while knowing that no more than a\nparticular fraction of feature cost is wasted. We investigate, through\nsimulation and a biomedical application, the practical importance of\nincorporating cost considerations into the feature selection process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:26:43 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:31:53 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yu", "Guo", ""], ["Witten", "Daniela", ""], ["Bien", "Jacob", ""]]}, {"id": "1910.03675", "submitter": "Kayla Kilpatrick", "authors": "Kayla W. Kilpatrick, Michael G. Hudgens, M. Elizabeth Halloran", "title": "Estimands and Inference in Cluster-Randomized Vaccine Trials", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster-randomized trials are often conducted to assess vaccine effects.\nDefining estimands of interest before conducting a trial is integral to the\nalignment between a study's objectives and the data to be collected and\nanalyzed. This paper considers estimands and estimators for overall, indirect,\nand total vaccine effects in trials where clusters of individuals are\nrandomized to vaccine or control. The scenario is considered where individuals\nself-select whether to participate in the trial and the outcome of interest is\nmeasured on all individuals in each cluster. Unlike the overall, indirect, and\ntotal effects, the direct effect of vaccination is shown in general not to be\nestimable without further assumptions, such as no unmeasured confounding. An\nillustrative example motivated by a cluster-randomized typhoid vaccine trial is\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 20:33:15 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Kilpatrick", "Kayla W.", ""], ["Hudgens", "Michael G.", ""], ["Halloran", "M. Elizabeth", ""]]}, {"id": "1910.03709", "submitter": "Sophie Berube", "authors": "Sophie B\\'erub\\'e, Abhirup Datta, Qingfeng Li, Chenguang Wang, Thomas\n  A. Louis", "title": "Percentile-Based Residuals for Model Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residuals are a key component of diagnosing model fit. The usual practice is\nto compute standardized residuals using expected values and standard deviations\nof the observed data, then use these values to detect outliers and assess model\nfit. Approximate normality of these residuals is key for this process to have\ngood properties, but in many modeling contexts, especially for complex,\nmulti-level models, normality may not hold. In these cases outlier detection\nand model diagnostics aren't properly calibrated. Alternatively, as we\ndemonstrate, residuals computed from the percentile location of a datum's value\nin its full predictive distribution lead to well calibrated evaluations of\nmodel fit. We generalize an approach described by Dunn and Smyth (1996) and\nevaluate properties mathematically, via case-studies and by simulation. In\naddition, we show that the standard residuals can be calibrated to mimic the\npercentile approach, but that this extra step is avoided by directly using\npercentile-based residuals. For both the percentile-based residuals and the\ncalibrated standard residuals, the use of full predictive distributions with\nthe appropriate location, spread and shape is necessary for valid assessments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 22:55:36 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["B\u00e9rub\u00e9", "Sophie", ""], ["Datta", "Abhirup", ""], ["Li", "Qingfeng", ""], ["Wang", "Chenguang", ""], ["Louis", "Thomas A.", ""]]}, {"id": "1910.03766", "submitter": "Wei Xie", "authors": "Wei Xie, Cheng Li, Yuefeng Wu, Pu Zhang", "title": "A Bayesian Nonparametric Framework for Uncertainty Quantification in\n  Simulation", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When we use simulation to assess the performance of stochastic systems, the\ninput models used to drive simulation experiments are often estimated from\nfinite real-world data. There exist both input and simulation estimation\nuncertainty in the system performance estimates. Without strong prior\ninformation on the input models and the system mean response surface, in this\npaper, we propose a Bayesian nonparametric framework to quantify the impact\nfrom both sources of uncertainty. Specifically, nonparametric input models are\nintroduced to faithfully capture the important features of the real-world data,\nsuch as heterogeneity, multi-modality and skewness. Bayesian posteriors of\nflexible input models characterize the input uncertainty, which automatically\naccounts for both model selection and parameter value uncertainty. Then, the\ninput uncertainty is propagated to outputs by using direct simulation. Thus,\nunder very general conditions, our framework delivers an empirical credible\ninterval accounting for both input and simulation uncertainties. A variance\ndecomposition is further developed to quantify the relative contributions from\nboth sources of uncertainty. Our approach is supported by rigorous theoretical\nand empirical study.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 03:08:33 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Xie", "Wei", ""], ["Li", "Cheng", ""], ["Wu", "Yuefeng", ""], ["Zhang", "Pu", ""]]}, {"id": "1910.03772", "submitter": "Xin Ma", "authors": "Xin Ma, Suprateek Kundu, Jennifer Stevens", "title": "Semi-parametric Bayes Regression with Network Valued Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing recognition of the role of brain networks as\nneuroimaging biomarkers in mental health and psychiatric studies. Our focus is\nposttraumatic stress disorder (PTSD), where the brain network interacts with\nenvironmental exposures in complex ways to drive the disease progression.\nExisting linear models seeking to characterize the relation between the\nclinical phenotype and the entire edge set in the brain network may be overly\nsimplistic and often involve inflated number of parameters leading to\ncomputational burden and inaccurate estimation. In one of the first such\nefforts, we develop a novel two stage Bayesian framework to find a\nnode-specific lower dimensional representation for the network using a latent\nscale approach in the first stage, and then use a flexible Gaussian process\nregression framework for prediction involving the latent scales and other\nsupplementary covariates in the second stage. The proposed approach relaxes\nlinearity assumptions, addresses the curse of dimensionality and is scalable to\nhigh dimensional networks while maintaining interpretability at the node level\nof the network. Extensive simulations and results from our motivating PTSD\napplication show a distinct advantage of the proposed approach over competing\nlinear and non-linear approaches in terms of prediction and coverage.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 03:26:29 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ma", "Xin", ""], ["Kundu", "Suprateek", ""], ["Stevens", "Jennifer", ""]]}, {"id": "1910.03779", "submitter": "Juntao Wang Mr", "authors": "Juntao Wang, Yang Liu, Yiling Chen", "title": "Forecast Aggregation via Peer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is a popular paradigm for soliciting forecasts on future\nevents. As people may have different forecasts, how to aggregate solicited\nforecasts into a single accurate prediction remains to be an important\nchallenge, especially when no historical accuracy information is available for\nidentifying experts. In this paper, we borrow ideas from the peer prediction\nliterature and assess the prediction accuracy of participants using solely the\ncollected forecasts. This approach leverages the correlations among peer\nreports to cross-validate each participant's forecasts and allows us to assign\na \"peer assessment score (PAS)\" for each agent as a proxy for the agent's\nprediction accuracy. We identify several empirically effective methods to\ngenerate PAS and propose an aggregation framework that uses PAS to identify\nexperts and to boost existing aggregators' prediction accuracy. We evaluate our\nmethods over 14 real-world datasets and show that i) PAS generated from peer\nprediction methods can approximately reflect the prediction accuracy of agents,\nand ii) our aggregation framework demonstrates consistent and significant\nimprovement in the prediction accuracy over existing aggregators for both\nbinary and multi-choice questions under three popular accuracy measures: Brier\nscore (mean square error), log score (cross-entropy loss) and AUC-ROC.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:07:13 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 00:27:10 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 19:39:40 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 05:03:42 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 07:28:11 GMT"}, {"version": "v6", "created": "Tue, 27 Apr 2021 21:11:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Juntao", ""], ["Liu", "Yang", ""], ["Chen", "Yiling", ""]]}, {"id": "1910.03832", "submitter": "Wojciech Zieli\\'nski", "authors": "Zofia Zieli\\'nska-Kolasi\\'nska, Wojciech Zieli\\'nski", "title": "A New Confidence Interval for Odds Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of interval estimation of the odds ratio. An\nasymptotic confidence interval is widely applied in medical research.\nUnfortunately that confidence interval has a poor coverage probability: it is\nsignificantly smaller than the nominal confidence level. In this paper a new\nconfidence interval is proposed. The construction needs only information on\nsample sizes and sample odds ratio. The coverage probability of the proposed\nconfidence interval is at least the nominal confidence level.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:12:16 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 17:10:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Zieli\u0144ska-Kolasi\u0144ska", "Zofia", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1910.04085", "submitter": "Guillaume Staerman", "authors": "Guillaume Staerman, Pavlo Mozharovskyi and Stephan Cl\\'emen\\c{c}on", "title": "The Area of the Convex Hull of Sampled Curves: a Robust Functional\n  Statistical Depth Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ubiquity of sensors in the IoT era, statistical observations are\nbecoming increasingly available in the form of massive (multivariate)\ntime-series. Formulated as unsupervised anomaly detection tasks, an abundance\nof applications like aviation safety management, the health monitoring of\ncomplex infrastructures or fraud detection can now rely on such functional\ndata, acquired and stored with an ever finer granularity. The concept of\nstatistical depth, which reflects centrality of an arbitrary observation w.r.t.\na statistical population may play a crucial role in this regard, anomalies\ncorresponding to observations with 'small' depth. Supported by sound\ntheoretical and computational developments in the recent decades, it has proven\nto be extremely useful, in particular in functional spaces. However, most\napproaches documented in the literature consist in evaluating independently the\ncentrality of each point forming the time series and consequently exhibit a\ncertain insensitivity to possible shape changes. In this paper, we propose a\nnovel notion of functional depth based on the area of the convex hull of\nsampled curves, capturing gradual departures from centrality, even beyond the\nenvelope of the data, in a natural fashion. We discuss practical relevance of\ncommonly imposed axioms on functional depths and investigate which of them are\nsatisfied by the notion of depth we promote here. Estimation and computational\nissues are also addressed and various numerical experiments provide empirical\nevidence of the relevance of the approach proposed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:13 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 07:14:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Staerman", "Guillaume", ""], ["Mozharovskyi", "Pavlo", ""], ["Cl\u00e9men\u00e7on", "Stephan", ""]]}, {"id": "1910.04086", "submitter": "David Ginsbourger", "authors": "Poompol Buathong, David Ginsbourger, Tipaluck Krityakierne", "title": "Kernels over Sets of Finite Sets using RKHS Embeddings, with Application\n  to Bayesian (Combinatorial) Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on kernel methods for set-valued inputs and their application to\nBayesian set optimization, notably combinatorial optimization. We investigate\ntwo classes of set kernels that both rely on Reproducing Kernel Hilbert Space\nembeddings, namely the ``Double Sum'' (DS) kernels recently considered in\nBayesian set optimization, and a class introduced here called ``Deep\nEmbedding'' (DE) kernels that essentially consists in applying a radial kernel\non Hilbert space on top of the canonical distance induced by another kernel\nsuch as a DS kernel. We establish in particular that while DS kernels typically\nsuffer from a lack of strict positive definiteness, vast subclasses of DE\nkernels built upon DS kernels do possess this property, enabling in turn\ncombinatorial optimization without requiring to introduce a jitter parameter.\nProofs of theoretical results about considered kernels are complemented by a\nfew practicalities regarding hyperparameter fitting. We furthermore demonstrate\nthe applicability of our approach in prediction and optimization tasks, relying\nboth on toy examples and on two test cases from mechanical engineering and\nhydrogeology, respectively. Experimental results highlight the applicability\nand compared merits of the considered approaches while opening new perspectives\nin prediction and sequential design with set inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:06:38 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:55:58 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Buathong", "Poompol", ""], ["Ginsbourger", "David", ""], ["Krityakierne", "Tipaluck", ""]]}, {"id": "1910.04102", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Miko{\\l}aj Kasprzak, Trevor Campbell, Tamara\n  Broderick", "title": "Validated Variational Inference via Practical Posterior Error Bounds", "comments": "A python package for carrying out our validated variational inference\n  workflow -- including doing black-box variational inference and computing the\n  bounds we develop in this paper -- is available at\n  https://github.com/jhuggins/viabel. The same repository also contains code\n  for reproducing all of our experiments", "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational inference has become an increasingly attractive fast alternative\nto Markov chain Monte Carlo methods for approximate Bayesian inference.\nHowever, a major obstacle to the widespread use of variational methods is the\nlack of post-hoc accuracy measures that are both theoretically justified and\ncomputationally efficient. In this paper, we provide rigorous bounds on the\nerror of posterior mean and uncertainty estimates that arise from\nfull-distribution approximations, as in variational inference. Our bounds are\nwidely applicable, as they require only that the approximating and exact\nposteriors have polynomial moments. Our bounds are also computationally\nefficient for variational inference because they require only standard values\nfrom variational objectives, straightforward analytic calculations, and simple\nMonte Carlo estimates. We show that our analysis naturally leads to a new and\nimproved workflow for validated variational inference. Finally, we demonstrate\nthe utility of our proposed workflow and error bounds on a robust regression\nproblem and on a real-data example with a widely used multilevel hierarchical\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:29:21 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 14:44:19 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:04:06 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 04:06:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1910.04109", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Daniel Malinsky, Ilya Shpitser", "title": "Optimal Training of Fair Predictive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been sustained interest in modifying prediction algorithms\nto satisfy fairness constraints. These constraints are typically complex\nnonlinear functionals of the observed data distribution. Focusing on the causal\nconstraints proposed by Nabi and Shpitser (2018), we introduce new theoretical\nresults and optimization techniques to make model training easier and more\naccurate. Specifically, we show how to reparameterize the observed data\nlikelihood such that fairness constraints correspond directly to parameters\nthat appear in the likelihood, transforming a complex constrained optimization\nobjective into a simple optimization problem with box constraints. We also\nexploit methods from empirical likelihood theory in statistics to improve\npredictive performance, without requiring parametric models for\nhigh-dimensional feature vectors.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 16:44:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:28:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nabi", "Razieh", ""], ["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1910.04221", "submitter": "Fan Bu", "authors": "Fan Bu, Allison E. Aiello, Jason Xu, and Alexander Volfovsky", "title": "Likelihood-based Inference for Partially Observed Epidemics on Dynamic\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model and an inference scheme for epidemic processes\non dynamic, adaptive contact networks. Network evolution is formulated as a\nlink-Markovian process, which is then coupled to an individual-level stochastic\nSIR model, in order to describe the interplay between epidemic dynamics on a\nnetwork and network link changes. A Markov chain Monte Carlo framework is\ndeveloped for likelihood-based inference from partial epidemic observations,\nwith a novel data augmentation algorithm specifically designed to deal with\nmissing individual recovery times under the dynamic network setting. Through a\nseries of simulation experiments, we demonstrate the validity and flexibility\nof the model as well as the efficacy and efficiency of the data augmentation\ninference scheme. The model is also applied to a recent real-world dataset on\ninfluenza-like-illness transmission with high-resolution social contact\ntracking records.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 19:48:15 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 21:09:35 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Bu", "Fan", ""], ["Aiello", "Allison E.", ""], ["Xu", "Jason", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1910.04291", "submitter": "Sean Jewell", "authors": "Sean Jewell, Paul Fearnhead, and Daniela Witten", "title": "Testing for a Change in Mean After Changepoint Detection", "comments": "Main text: 28 pages, 5 figures. Supplementary Materials: 15 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many methods are available to detect structural changes in a time\nseries, few procedures are available to quantify the uncertainty of these\nestimates post-detection. In this work, we fill this gap by proposing a new\nframework to test the null hypothesis that there is no change in mean around an\nestimated changepoint. We further show that it is possible to efficiently carry\nout this framework in the case of changepoints estimated by binary segmentation\nand its variants, $\\ell_{0}$ segmentation, or the fused lasso. Our setup allows\nus to condition on much less information than existing approaches, which yields\nhigher powered tests. We apply our proposals in a simulation study and on a\ndataset of chromosomal guanine-cytosine content. These approaches are freely\navailable in the R package ChangepointInference at\nhttps://jewellsean.github.io/changepoint-inference/.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 22:58:38 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:36:12 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 03:53:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jewell", "Sean", ""], ["Fearnhead", "Paul", ""], ["Witten", "Daniela", ""]]}, {"id": "1910.04302", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Francisco J. R. Ruiz, David M. Blei, Michalis K.\n  Titsias", "title": "Prescribed Generative Adversarial Networks", "comments": "Code for this paper can be found at\n  https://github.com/adjidieng/PresGANs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a powerful approach to\nunsupervised learning. They have achieved state-of-the-art performance in the\nimage domain. However, GANs are limited in two ways. They often learn\ndistributions with low support---a phenomenon known as mode collapse---and they\ndo not guarantee the existence of a probability density, which makes evaluating\ngeneralization using predictive log-likelihood impossible. In this paper, we\ndevelop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs\nadd noise to the output of a density network and optimize an\nentropy-regularized adversarial loss. The added noise renders tractable\napproximations of the predictive log-likelihood and stabilizes the training\nprocedure. The entropy regularizer encourages PresGANs to capture all the modes\nof the data distribution. Fitting PresGANs involves computing the intractable\ngradients of the entropy regularization term; PresGANs sidestep this\nintractability using unbiased stochastic estimates. We evaluate PresGANs on\nseveral datasets and found they mitigate mode collapse and generate samples\nwith high perceptual quality. We further found that PresGANs reduce the gap in\nperformance in terms of predictive log-likelihood between traditional GANs and\nvariational autoencoders (VAEs).\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 23:40:05 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Dieng", "Adji B.", ""], ["Ruiz", "Francisco J. R.", ""], ["Blei", "David M.", ""], ["Titsias", "Michalis K.", ""]]}, {"id": "1910.04333", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Efficient Estimation for Random Dot Product Graphs via a One-step\n  Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a one-step procedure to estimate the latent positions in random\ndot product graphs efficiently. Unlike the classical spectral-based methods\nsuch as the adjacency and Laplacian spectral embedding, the proposed one-step\nprocedure takes advantage of both the low-rank structure of the expected\nadjacency matrix and the Bernoulli likelihood information of the sampling model\nsimultaneously. We show that for each vertex, the corresponding row of the\none-step estimator converges to a multivariate normal distribution after proper\nscaling and centering up to an orthogonal transformation, with an efficient\ncovariance matrix. The initial estimator for the one-step procedure needs to\nsatisfy the so-called approximate linearization property. The one-step\nestimator improves the commonly-adopted spectral embedding methods in the\nfollowing sense: Globally for all vertices, it yields an asymptotic sum of\nsquares error no greater than those of the spectral methods, and locally for\neach vertex, the asymptotic covariance matrix of the corresponding row of the\none-step estimator dominates those of the spectral embeddings in spectra. The\nusefulness of the proposed one-step procedure is demonstrated via numerical\nexamples and the analysis of a real-world Wikipedia graph dataset.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 02:25:45 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 00:53:31 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1910.04375", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Estimating Transfer Entropy via Copula Entropy", "comments": "17 pages, 5 figures. with new experiments, discussion, and section on\n  related research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery is a fundamental problem in statistics and has wide\napplications in different fields. Transfer Entropy (TE) is a important notion\ndefined for measuring causality, which is essentially conditional Mutual\nInformation (MI). Copula Entropy (CE) is a theory on measurement of statistical\nindependence and is equivalent to MI. In this paper, we prove that TE can be\nrepresented with only CE and then propose a non-parametric method for\nestimating TE via CE. The proposed method was applied to analyze the Beijing\nPM2.5 data in the experiments. Experimental results show that the proposed\nmethod can infer causality relationships from data effectively and hence help\nto understand the data better.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 05:49:03 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 00:39:33 GMT"}, {"version": "v3", "created": "Sat, 6 Mar 2021 09:39:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "1910.04475", "submitter": "Fabio Demarqui", "authors": "Fabio N. Demarqui, Vinicius D. Mayrink, Sujit K. Ghosh", "title": "An Unified Semiparametric Approach to Model Lifetime Data with Crossing\n  Survival Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional hazards (PH), proportional odds (PO) and accelerated failure\ntime (AFT) models have been widely used in different applications of survival\nanalysis. Despite their popularity, these models are not suitable to handle\nlifetime data with crossing survival curves. In 2005, Yang and Prentice\nproposed a semiparametric two-sample strategy (YP model), including the PH and\nPO frameworks as particular cases, to deal with this type of data. Assuming a\ngeneral regression setting, the present paper proposes an unified approach to\nfit the YP model by employing Bernstein polynomials to manage the baseline\nhazard and odds under both the frequentist and Bayesian frameworks. The use of\nthe Bernstein polynomials has some advantages: it allows for uniform\napproximation of the baseline distribution, it leads to closed-form expressions\nfor all baseline functions, it simplifies the inference procedure, and the\npresence of a continuous survival function allows a more accurate estimation of\nthe crossing survival time. Extensive simulation studies are carried out to\nevaluate the behavior of the models. The analysis of a clinical trial data set,\nrelated to non-small-cell lung cancer, is also developed as an illustration.\nOur findings indicate that assuming the usual PH model, ignoring the existing\ncrossing survival feature in the real data, is a serious mistake with\nimplications for those patients in the initial stage of treatment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 10:47:35 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Demarqui", "Fabio N.", ""], ["Mayrink", "Vinicius D.", ""], ["Ghosh", "Sujit K.", ""]]}, {"id": "1910.04625", "submitter": "Lauren Beesley", "authors": "Lauren Beesley and Jeremy M G Taylor", "title": "A stacked approach for chained equations multiple imputation\n  incorporating the substantive model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation by chained equations (MICE) has emerged as a popular\napproach for handling missing data. A central challenge for applying MICE is\ndetermining how to incorporate outcome information into covariate imputation\nmodels, particularly for complicated outcomes. Often, we have a particular\nanalysis model in mind, and we would like to ensure congeniality between the\nimputation and analysis models.\n  We propose a novel strategy for directly incorporating the analysis model\ninto the handling of missing data. In our proposed approach, multiple\nimputations of missing covariates are obtained without using outcome\ninformation. We then utilize the strategy of imputation stacking, where\nmultiple imputations are stacked on top of each other to create a large\ndataset. The analysis model is then incorporated through weights. Instead of\napplying multiple imputation combining rules, we obtain parameter estimates by\nfitting a weighted version of the analysis model on the stacked dataset. We\npropose a novel estimator for obtaining standard errors for this stacked and\nweighted analysis. Our estimator is based on the observed data information\nprinciple in Louis (1982) and can be applied for analyzing stacked multiple\nimputations more generally. Our approach for analyzing stacked multiple\nimputations is the first well-motivated method that can be easily applied for a\nwide variety of standard analysis models and missing data settings.\n  In simulations, the proposed strategy produced unbiased parameter estimates\nwhen the analysis model was correctly specified. We developed an R package,\nStackImpute, allowing this imputation approach to be easily implemented for\nmany standard analysis models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:00:35 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Beesley", "Lauren", ""], ["Taylor", "Jeremy M G", ""]]}, {"id": "1910.04672", "submitter": "Alexander Buchholz", "authors": "Alexander Buchholz, Daniel Ahfock, Sylvia Richardson", "title": "Distributed Computation for Marginal Likelihood based Model Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general method for distributed Bayesian model choice, using the\nmarginal likelihood, where each worker has access only to non-overlapping\nsubsets of the data. Our approach approximates the model evidence for the full\ndata set through Monte Carlo sampling from the posterior on every subset\ngenerating a model evidence per subset. The model evidences per worker are then\nconsistently combined using a novel approach which corrects for the splitting\nusing summary statistics of the generated samples. This divide-and-conquer\napproach allows Bayesian model choice in the large data setting, exploiting all\navailable information but limiting communication between workers. Our work\nthereby complements the work on consensus Monte Carlo (Scott et al., 2016) by\nexplicitly enabling model choice. In addition, we show how the suggested\napproach can be extended to model choice within a reversible jump setting that\nexplores multiple feature combinations within one run.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:23:34 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 15:58:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Buchholz", "Alexander", ""], ["Ahfock", "Daniel", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1910.04679", "submitter": "Rachel Lynne Wilkerson", "authors": "Rachel L. Wilkerson and Jim Q. Smith", "title": "Bayesian Diagnostics for Chain Event Graphs", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chain event graphs have been established as a practical Bayesian graphical\ntool. While bespoke diagnostics have been developed for Bayesian Networks, they\nhave not yet been defined for the statistical class of Chain Event Graph\nmodels. Mirroring the methodology of prequential diagnostics for Bayesian\nNetworks, in this paper we design a number of new Bayesian diagnostics for this\nnew class. These can be used to check whether a selected model--presumably the\nbest within the class--captures most of the salient features of the observed\nprocess. These are designed to check the continued validity of a selected model\nas data about a population is collected. A previous study of childhood illness\nin New Zealand illustrates the efficacy of these diagnostics. A second example\non radicalisation is used as a more expressive example.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:34:52 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Wilkerson", "Rachel L.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1910.04883", "submitter": "Serena Ng", "authors": "Evan Munro and Serena Ng", "title": "Latent Dirichlet Analysis of Categorical Survey Responses", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2020.1802285", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beliefs are important determinants of an individual's choices and economic\noutcomes, so understanding how they comove and differ across individuals is of\nconsiderable interest. Researchers often rely on surveys that report individual\nbeliefs as qualitative data. We propose using a Bayesian hierarchical latent\nclass model to analyze the comovements and observed heterogeneity in\ncategorical survey responses. We show that the statistical model corresponds to\nan economic structural model of information acquisition, which guides\ninterpretation and estimation of the model parameters. An algorithm based on\nstochastic optimization is proposed to estimate a model for repeated surveys\nwhen responses follow a dynamic structure and conjugate priors are not\nappropriate. Guidance on selecting the number of belief types is also provided.\nTwo examples are considered. The first shows that there is information in the\nMichigan survey responses beyond the consumer sentiment index that is\nofficially published. The second shows that belief types constructed from\nsurvey responses can be used in a subsequent analysis to estimate heterogeneous\nreturns to education.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 21:27:01 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 12:47:35 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 20:28:06 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Munro", "Evan", ""], ["Ng", "Serena", ""]]}, {"id": "1910.04900", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Aaditya Ramdas", "title": "Online control of the familywise error rate", "comments": "Submitted to Biostatistics; added the real data example; renewed some\n  of the proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological research often involves testing a growing number of null\nhypotheses as new data is accumulated over time. We study the problem of online\ncontrol of the familywise error rate (FWER), that is testing an apriori\nunbounded sequence of hypotheses (p-values) one by one over time without\nknowing the future, such that with high probability there are no false\ndiscoveries in the entire sequence. This paper unifies algorithmic concepts\ndeveloped for offline (single batch) FWER control and online false discovery\nrate control to develop novel online FWER control methods. Though many offline\nFWER methods (e.g. Bonferroni, fallback procedures and Sidak's method) can\ntrivially be extended to the online setting, our main contribution is the\ndesign of new, powerful, adaptive online algorithms that control the FWER when\nthe p-values are independent or locally dependent in time. Our experiments\ndemonstrate substantial gains in power, that are also formally proved in a\nGaussian sequence model. Multiple testing, FWER control, online setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 22:50:03 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:21:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tian", "Jinjin", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1910.04968", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Daniel L. Jiang, Aaditya Ramdas, Michael I. Jordan", "title": "The Power of Batching in Multiple Hypothesis Testing", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important partition of algorithms for controlling the false discovery\nrate (FDR) in multiple testing is into offline and online algorithms. The first\ngenerally achieve significantly higher power of discovery, while the latter\nallow making decisions sequentially as well as adaptively formulating\nhypotheses based on past observations. Using existing methodology, it is\nunclear how one could trade off the benefits of these two broad families of\nalgorithms, all the while preserving their formal FDR guarantees. To this end,\nwe introduce $\\text{Batch}_{\\text{BH}}$ and $\\text{Batch}_{\\text{St-BH}}$,\nalgorithms for controlling the FDR when a possibly infinite sequence of batches\nof hypotheses is tested by repeated application of one of the most widely used\noffline algorithms, the Benjamini-Hochberg (BH) method or Storey's improvement\nof the BH method. We show that our algorithms interpolate between existing\nonline and offline methodology, thus trading off the best of both worlds.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:35:20 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 05:38:58 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 00:11:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zrnic", "Tijana", ""], ["Jiang", "Daniel L.", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1910.05037", "submitter": "Andi Wang", "authors": "Andi Q. Wang, Murray Pollock, Gareth O. Roberts, David Steinsaltz", "title": "Regeneration-enriched Markov processes with application to Monte Carlo", "comments": "v3: 43 pages, 3 figures. Some clarifications added. Accepted version,\n  to appear in Ann. Appl. Probab", "journal-ref": "The Annals of Applied Probability, 2021, Vol. 31, No. 2, 703-735", "doi": "10.1214/20-AAP1602", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of Markov processes that combine local dynamics, arising\nfrom a fixed Markov process, with regenerations arising at a state-dependent\nrate. We give conditions under which such processes possess a given target\ndistribution as their invariant measures, thus making them amenable for use\nwithin Monte Carlo methodologies. Since the regeneration mechanism can\ncompensate the choice of local dynamics, while retaining the same invariant\ndistribution, great flexibility can be achieved in selecting local dynamics,\nand the mathematical analysis is simplified. We give straightforward conditions\nfor the process to possess a central limit theorem, and additional conditions\nfor uniform ergodicity and for a coupling from the past construction to hold,\nenabling exact sampling from the invariant distribution. We further consider\nand analyse a natural approximation of the process which may arise in the\npractical simulation of some classes of continuous-time dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 09:12:26 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 13:18:54 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 15:12:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Andi Q.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""], ["Steinsaltz", "David", ""]]}, {"id": "1910.05083", "submitter": "Kohei Yoshikawa", "authors": "Kohei Yoshikawa, Shuichi Kawano", "title": "Sparse Reduced-Rank Regression for Simultaneous Rank and Variable\n  Selection via Manifold Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing a reduced-rank regression model whose\ncoefficient parameter is represented as a singular value decomposition with\nsparse singular vectors. The traditional estimation procedure for the\ncoefficient parameter often fails when the true rank of the parameter is high.\nTo overcome this issue, we develop an estimation algorithm with rank and\nvariable selection via sparse regularization and manifold optimization, which\nenables us to obtain an accurate estimation of the coefficient parameter even\nif the true rank of the coefficient parameter is high. Using sparse\nregularization, we can also select an optimal value of the rank. We conduct\nMonte Carlo experiments and real data analysis to illustrate the effectiveness\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:22:21 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 05:38:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Yoshikawa", "Kohei", ""], ["Kawano", "Shuichi", ""]]}, {"id": "1910.05098", "submitter": "Elahe Ghalebi", "authors": "Elahe Ghalebi, Hamidreza Mahyar, Radu Grosu, Graham W. Taylor, Sinead\n  A. Williamson", "title": "A Nonparametric Bayesian Model for Sparse Dynamic Multigraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the availability and importance of temporal interaction data--such as\nemail communication--increases, it becomes increasingly important to understand\nthe underlying structure that underpins these interactions. Often these\ninteractions form a multigraph, where we might have multiple interactions\nbetween two entities. Such multigraphs tend to be sparse yet structured, and\ntheir distribution often evolves over time. Existing statistical models with\ninterpretable parameters can capture some, but not all, of these properties. We\npropose a dynamic nonparametric model for interaction multigraphs that combines\nthe sparsity of edge-exchangeable multigraphs with dynamic clustering patterns\nthat tend to reinforce recent behavioral patterns. We show that our method\nyields improved held-out likelihood over stationary variants, and impressive\npredictive performance against a range of state-of-the-art dynamic graph\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 12:07:54 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 23:01:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ghalebi", "Elahe", ""], ["Mahyar", "Hamidreza", ""], ["Grosu", "Radu", ""], ["Taylor", "Graham W.", ""], ["Williamson", "Sinead A.", ""]]}, {"id": "1910.05108", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "Differentially Private Survival Function Estimation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival function estimation is used in many disciplines, but it is most\ncommon in medical analytics in the form of the Kaplan-Meier estimator.\nSensitive data (patient records) is used in the estimation without any explicit\ncontrol on the information leakage, which is a significant privacy concern. We\npropose a first differentially private estimator of the survival function and\nshow that it can be easily extended to provide differentially private\nconfidence intervals and test statistics without spending any extra privacy\nbudget. We further provide extensions for differentially private estimation of\nthe competing risk cumulative incidence function, Nelson-Aalen's estimator for\nthe hazard function, etc. Using eleven real-life clinical datasets, we provide\nempirical evidence that our proposed method provides good utility while\nsimultaneously providing strong privacy guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 15:15:40 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 21:02:49 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1910.05206", "submitter": "Victor Coscrato", "authors": "Victor Coscrato, Marco Henrique de Almeida In\\'acio, Tiago Botari,\n  Rafael Izbicki", "title": "NLS: an accurate and yet easy-to-interpret regression method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important feature of successful supervised machine learning applications\nis to be able to explain the predictions given by the regression or\nclassification model being used. However, most state-of-the-art models that\nhave good predictive power lead to predictions that are hard to interpret.\nThus, several model-agnostic interpreters have been developed recently as a way\nof explaining black-box classifiers. In practice, using these methods is a slow\nprocess because a novel fitting is required for each new testing instance, and\nseveral non-trivial choices must be made. We develop NLS (neural local\nsmoother), a method that is complex enough to give good predictions, and yet\ngives solutions that are easy to be interpreted without the need of using a\nseparate interpreter. The key idea is to use a neural network that imposes a\nlocal linear shape to the output layer. We show that NLS leads to predictive\npower that is comparable to state-of-the-art machine learning models, and yet\nis easier to interpret.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 14:15:52 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Coscrato", "Victor", ""], ["In\u00e1cio", "Marco Henrique de Almeida", ""], ["Botari", "Tiago", ""], ["Izbicki", "Rafael", ""]]}, {"id": "1910.05364", "submitter": "Oscar Fontanelli", "authors": "Oscar Fontanelli, Pedro Miramontes, Ricardo Mansilla, Germinal Cocho,\n  Wentian Li", "title": "Beta Rank Function: A Smooth Double-Pareto-Like Distribution", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Beta Rank Function (BRF) $x(u) =A(1-u)^b/u^a$, where $u$ is the\nnormalized and continuous rank of an observation $x$, has wide applications in\nfitting real-world data from social science to biological phenomena. The\nunderlying probability density function (pdf) $f_X(x)$ does not usually have a\nclosed expression except for specific parameter values. We show however that it\nis approximately a unimodal skewed and asymmetric two-sided power law/double\nPareto/log-Laplacian distribution. The BRF pdf has simple properties when the\nindependent variable is log-transformed: $f_{Z=\\log(X)}(z)$ . At the peak it\nmakes a smooth turn and it does not diverge, lacking the sharp angle observed\nin the double Pareto or Laplace distribution. The peak position of $f_Z(z)$ is\n$z_0=\\log A+(a-b)\\log(\\sqrt{a}+\\sqrt{b})-(a\\log(a)-b\\log(b))/2 $; the\nprobability is partitioned by the peak to the proportion of\n$\\sqrt{b}/(\\sqrt{a}+\\sqrt{b})$ (left) and $\\sqrt{a}/(\\sqrt{a}+\\sqrt{b})$\n(right); the functional form near the peak is controlled by the cubic term in\nthe Taylor expansion when $a\\ne b$; the mean of $Z$ is $E[Z]=\\log A+a-b$; the\ndecay on left and right sides of the peak is approximately exponential with\nforms $e^{\\frac{z-\\log A}{b} }/b$ and $e^{ -\\frac{z-\\log A}{a}}/a$. These\nresults are confirmed by numerical simulations. Properties of $f_X(x)$ without\nlog-transforming the variable are much more complex, though the approximate\ndouble Pareto behavior, $(x/A)^{1/b}/(bx)$ (for $x<A$) and $(x/A)^{-1/a}/(ax)$\n(for $x > A$) is simple. Our results elucidate the relationship between BRF and\nlog-normal distributions when $a=b$ and explain why the BRF is ubiquitous and\nversatile. Based on the pdf, we suggest a quick way to elucidate if a real data\nset follows a one-sided power-law, a log-normal, a two-sided power-law or a\nBRF. We illustrate our results with two examples: urban populations and\nfinancial returns.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:28:37 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Fontanelli", "Oscar", ""], ["Miramontes", "Pedro", ""], ["Mansilla", "Ricardo", ""], ["Cocho", "Germinal", ""], ["Li", "Wentian", ""]]}, {"id": "1910.05438", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn, Ilya Shpitser, and Eric J. Tchetgen Tchetgen", "title": "Comment on \"Blessings of Multiple Causes\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (This comment has been updated to respond to Wang and Blei's rejoinder\n[arXiv:1910.07320].)\n  The premise of the deconfounder method proposed in \"Blessings of Multiple\nCauses\" by Wang and Blei [arXiv:1805.06826], namely that a variable that\nrenders multiple causes conditionally independent also controls for unmeasured\nmulti-cause confounding, is incorrect. This can be seen by noting that no fact\nabout the observed data alone can be informative about ignorability, since\nignorability is compatible with any observed data distribution. Methods to\ncontrol for unmeasured confounding may be valid with additional assumptions in\nspecific settings, but they cannot, in general, provide a checkable approach to\ncausal inference, and they do not, in general, require weaker assumptions than\nthe assumptions that are commonly used for causal inference. While this is\noutside the scope of this comment, we note that much recent work on applying\nideas from latent variable modeling to causal inference problems suffers from\nsimilar issues.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 23:19:45 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:08:22 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 16:20:09 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1910.05452", "submitter": "Jialei Chen", "authors": "Jialei Chen, Simon Mak, V. Roshan Joseph, Chuck Zhang", "title": "Adaptive design for Gaussian process regression under censoring", "comments": null, "journal-ref": "Annals of Applied Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key objective in engineering problems is to predict an unknown experimental\nsurface over an input domain. In complex physical experiments, this may be\nhampered by response censoring, which results in a significant loss of\ninformation. For such problems, experimental design is paramount for maximizing\npredictive power using a small number of expensive experimental runs. To tackle\nthis, we propose a novel adaptive design method, called the integrated censored\nmean-squared error (ICMSE) method. The ICMSE method first estimates the\nposterior probability of a new observation being censored, then adaptively\nchooses design points that minimize predictive uncertainty under censoring.\nAdopting a Gaussian process regression model with product correlation function,\nthe proposed ICMSE criterion is easy to evaluate, which allows for efficient\ndesign optimization. We demonstrate the effectiveness of the ICMSE design in\ntwo real-world applications on surgical planning and wafer manufacturing.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 00:53:39 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 19:59:45 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Jialei", ""], ["Mak", "Simon", ""], ["Joseph", "V. Roshan", ""], ["Zhang", "Chuck", ""]]}, {"id": "1910.05473", "submitter": "Jiali Wang", "authors": "Jiali Wang, Anton Westveld, Bronwyn Loong, Alan Welsh", "title": "An Imputation model by Dirichlet Process Mixture of Elliptical Copulas\n  for Data of Mixed Type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula-based methods provide a flexible approach to build missing data\nimputation models of multivariate data of mixed types. However, the choice of\ncopula function is an open question. We consider a Bayesian nonparametric\napproach by using an infinite mixture of elliptical copulas induced by a\nDirichlet process mixture to build a flexible copula function. A slice sampling\nalgorithm is used to sample from the infinite dimensional parameter space. We\nextend the work on prior parallel tempering used in finite mixture models to\nthe Dirichlet process mixture model to overcome the mixing issue in multimodal\ndistributions. Using simulations, we demonstrate that the infinite mixture\ncopula model provides a better overall fit compared to their single component\ncounterparts, and performs better at capturing tail dependence features of the\ndata. Simulations further show that our proposed model achieves more accurate\nimputation especially for continuous variables and better inferential results\nin some analytic models. The proposed model is applied to a medical data set of\nacute stroke patients in Australia.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 03:10:41 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Wang", "Jiali", ""], ["Westveld", "Anton", ""], ["Loong", "Bronwyn", ""], ["Welsh", "Alan", ""]]}, {"id": "1910.05575", "submitter": "Rafael Izbicki", "authors": "Rafael Izbicki, Gilson T. Shimizu, Rafael B. Stern", "title": "Flexible distribution-free conditional predictive bands using density\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal methods create prediction bands that control average coverage under\nno assumptions besides i.i.d. data. Besides average coverage, one might also\ndesire to control conditional coverage, that is, coverage for every new testing\npoint. However, without strong assumptions, conditional coverage is\nunachievable. Given this limitation, the literature has focused on methods with\nasymptotical conditional coverage. In order to obtain this property, these\nmethods require strong conditions on the dependence between the target variable\nand the features. We introduce two conformal methods based on conditional\ndensity estimators that do not depend on this type of assumption to obtain\nasymptotic conditional coverage: Dist-split and CD-split. While Dist-split\nasymptotically obtains optimal intervals, which are easier to interpret than\ngeneral regions, CD-split obtains optimal size regions, which are smaller than\nintervals. CD-split also obtains local coverage by creating a data-driven\npartition of the feature space that scales to high-dimensional settings and by\ngenerating prediction bands locally on the partition elements. In a wide\nvariety of simulated scenarios, our methods have a better control of\nconditional coverage and have smaller length than previously proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 15:23:13 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 15:28:21 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Izbicki", "Rafael", ""], ["Shimizu", "Gilson T.", ""], ["Stern", "Rafael B.", ""]]}, {"id": "1910.05615", "submitter": "Peter Rousseeuw", "authors": "Bart De Ketelaere, Mia Hubert, Jakob Raymaekers, Peter J. Rousseeuw,\n  Iwein Vranckx", "title": "Real-time outlier detection for large datasets by RT-DetMCD", "comments": null, "journal-ref": "Chemometrics and Intelligent Laboratory Systems, 2020, Volume 199", "doi": "10.1016/j.chemolab.2020.103957", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern industrial machines can generate gigabytes of data in seconds,\nfrequently pushing the boundaries of available computing power. Together with\nthe time criticality of industrial processing this presents a challenging\nproblem for any data analytics procedure. We focus on the deterministic minimum\ncovariance determinant method (DetMCD), which detects outliers by fitting a\nrobust covariance matrix. We construct a much faster version of DetMCD by\nreplacing its initial estimators by two new methods and incorporating\nupdate-based concentration steps. The computation time is reduced further by\nparallel computing, with a novel robust aggregation method to combine the\nresults from the threads. The speed and accuracy of the proposed real-time\nDetMCD method (RT-DetMCD) are illustrated by simulation and a real industrial\napplication to food sorting.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:17:23 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 21:54:42 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["De Ketelaere", "Bart", ""], ["Hubert", "Mia", ""], ["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Vranckx", "Iwein", ""]]}, {"id": "1910.05620", "submitter": "Sepideh Mosaferi", "authors": "Sepideh Mosaferi and Hamidreza Navvabpour", "title": "A Recommendation for Net Undercount Estimation in Iran Population and\n  Dwelling Censuses", "comments": "19 pages. Originally published in the Journal of Statistical Research\n  of Iran", "journal-ref": "J. Statist. Res. Iran (2011): 8, 229-251", "doi": "10.18869/acadpub.jsri.8.2.229", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Census counts are subject to different types of nonsampling errors. One of\nthese main errors is coverage error. Undercount and overcount are two types of\ncoverage error. Undercount usually occurs more than the other, thus net\nundercount estimation is important. There are various methods for estimating\nthe coverage error in censuses. One of these methods is dual system (DS) that\nusually uses data from the census and a post-enumeration survey (PES). In this\npaper, the coverage error and necessity of its evaluation, PES design and DS\nmethod are explained. Then PES associated approaches and their effects on DS\nestimation are illustrated and these approaches are compared. Finally, we\nexplain the Statistical Center of Iran method of estimating net undercount in\nIran 2006 population and dwelling census and a suggestion will be given for\nimproving net undercount estimation in population and dwelling censuses of\nIran.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:30:53 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mosaferi", "Sepideh", ""], ["Navvabpour", "Hamidreza", ""]]}, {"id": "1910.05676", "submitter": "Zifeng Zhao", "authors": "Peng Shi and Zifeng Zhao", "title": "Regression for Copula-linked Compound Distributions with Applications in\n  Modeling Aggregate Insurance Claims", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In actuarial research, a task of particular interest and importance is to\npredict the loss cost for individual risks so that informative decisions are\nmade in various insurance operations such as underwriting, ratemaking, and\ncapital management. The loss cost is typically viewed to follow a compound\ndistribution where the summation of the severity variables is stopped by the\nfrequency variable. A challenging issue in modeling such outcome is to\naccommodate the potential dependence between the number of claims and the size\nof each individual claim. In this article, we introduce a novel regression\nframework for compound distributions that uses a copula to accommodate the\nassociation between the frequency and the severity variables, and thus allows\nfor arbitrary dependence between the two components. We further show that the\nnew model is very flexible and is easily modified to account for incomplete\ndata due to censoring or truncation. The flexibility of the proposed model is\nillustrated using both simulated and real data sets. In the analysis of\ngranular claims data from property insurance, we find substantive negative\nrelationship between the number and the size of insurance claims. In addition,\nwe demonstrate that ignoring the frequency-severity association could lead to\nbiased decision-making in insurance operations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 03:37:59 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Shi", "Peng", ""], ["Zhao", "Zifeng", ""]]}, {"id": "1910.05701", "submitter": "Zheng Gao", "authors": "Zheng Gao", "title": "Five Shades of Grey: Phase Transitions in High-dimensional Multiple\n  Testing", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by marginal screenings of categorical variables, and study\nhigh-dimensional multiple testing problems where test statistics have\napproximate chi-square distributions. We characterize four new phase\ntransitions in high-dimensional chi-square models, and derive the signal sizes\nnecessary and sufficient for statistical procedures to simultaneously control\nfalse discovery (in terms of family-wise error rate or false discovery rate)\nand missed detection (in terms of family-wise non-discovery rate or false\nnon-discovery rate) in large dimensions. Remarkably, degrees of freedom in the\nchi-square distributions do not affect the boundaries in all four phase\ntransitions. Several well-known procedures are shown to attain these\nboundaries. Two new phase transitions are also identified in the Gaussian\nlocation model under one-sided alternatives.\n  We then elucidate on the nature of signal sizes in association tests by\ncharacterizing its relationship with marginal frequencies, odds ratio, and\nsample sizes in $2\\times2$ contingency tables. This allows us to illustrate an\ninteresting manifestation of the phase transition phenomena in genome-wide\nassociation studies (GWAS). We also show, perhaps surprisingly, that given\ntotal sample sizes, balanced designs in such association studies rarely deliver\noptimal power.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 06:50:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Gao", "Zheng", ""]]}, {"id": "1910.05800", "submitter": "Tianchen Qian", "authors": "Tianchen Qian, Michael Rosenblum, Huitong Qiu", "title": "Improving Precision through Adjustment for Prognostic Variables in Group\n  Sequential Trial Designs: Impact of Baseline Variables, Short-Term Outcomes,\n  and Treatment Effect Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In randomized trials, appropriately adjusting for baseline variables and\nshort-term outcomes can lead to increased precision and reduced sample size. We\nexamine the impact of such adjustment in group sequential designs, i.e.,\ndesigns with preplanned interim analyses where enrollment may be stopped early\nfor efficacy or futility. We address the following questions: how much\nprecision gain can be obtained by appropriately adjusting for baseline\nvariables and a short-term outcome? How is this precision gain impacted by\nfactors such as the proportion of pipeline participants (those who enrolled but\nhaven't yet had their primary outcomes measured) and treatment effect\nheterogeneity? What is the resulting impact on power and average sample size in\na group sequential design? We derive an asymptotic formula that decomposes the\noverall precision gain from adjusting for baseline variables and a short-term\noutcome into contributions from factors mentioned above, for efficient\nestimators in the model that only assumes randomization and independent\ncensoring. We use our formula to approximate the precision gain from a targeted\nminimum loss-based estimator applied to data from a completed trial of a new\nsurgical intervention for stroke. Our formula implies that (for an efficient\nestimator) adjusting for a prognostic baseline variable leads to at least as\nmuch asymptotic precision gain as adjusting for an equally prognostic\nshort-term outcome. In many cases, such as our stroke trial application, the\nformer leads to substantially greater precision gains than the latter. In our\nsimulation study, we show how precision gains from adjustment can be converted\ninto sample size reductions (even when there is no treatment effect).\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 18:12:47 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 15:57:02 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Qian", "Tianchen", ""], ["Rosenblum", "Michael", ""], ["Qiu", "Huitong", ""]]}, {"id": "1910.05840", "submitter": "Sepideh Mosaferi", "authors": "Sepideh Mosaferi", "title": "Empirical and Constrained Empirical Bayes Variance Estimation Under A\n  One Unit Per Stratum Sample Design", "comments": "16 pages, 2 Figures. This paper was published as a Proceeding in the\n  Survey Research Methods Section, JSM 2015, American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single primary sampling unit (PSU) per stratum design is a popular design\nfor estimating the parameter of interest. Although, the point estimator of the\ndesign is unbiased and efficient, an unbiased variance estimator does not\nexist. A common practice to solve this is to collapse or combine the two\nadjacent strata, but the attained estimator of variance is not design-unbiased,\nand the bias increases as the population means of the collapsed strata become\nmore variant. Therefore, the one PSU per stratum design with collapsed stratum\nvariance estimator might not be a good choice, and some statisticians prefer a\ndesign in which two PSUs per stratum are selected. In this paper, we first\ncompare a one PSU per stratum design to a two PSUs per stratum design. Then, we\npropose an empirical Bayes estimator for the variance of one PSU per stratum\ndesign, where it over-shrinks towards the prior mean. To protect against this,\nwe investigate the potential of a constrained empirical Bayes estimator.\nThrough a simulation study, we show that the empirical Bayes and constrained\nempirical Bayes estimators outperform the classical collapsed one in terms of\nempirical relative mean squared error.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 21:42:31 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Mosaferi", "Sepideh", ""]]}, {"id": "1910.05845", "submitter": "Wei Xie", "authors": "Qiong Zhang, Bo Wang, Wei Xie", "title": "A Pooled Quantile Estimator for Parallel Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantile is an important risk measure quantifying the stochastic system\nrandom behaviors. This paper studies a pooled quantile estimator, which is the\nsample quantile of detailed simulation outputs after directly pooling\nindependent sample paths together. We derive the asymptotic representation of\nthe pooled quantile estimator and further prove its normality. By comparing\nwith the classical quantile estimator used in stochastic simulation, both\ntheoretical and empirical studies demonstrate the advantages of the proposal\nunder the context of parallel simulation.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:18:29 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Zhang", "Qiong", ""], ["Wang", "Bo", ""], ["Xie", "Wei", ""]]}, {"id": "1910.05847", "submitter": "Rui Meng", "authors": "Rui Meng, Soper Braden, Jan Nygard, Mari Nygrad, Herbert Lee", "title": "Hierarchical Hidden Markov Jump Processes for Cancer Screening Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov jump processes are an attractive approach for modeling clinical\ndisease progression data because they are explainable and capable of handling\nboth irregularly sampled and noisy data. Most applications in this context\nconsider time-homogeneous models due to their relative computational\nsimplicity. However, the time homogeneous assumption is too strong to\naccurately model the natural history of many diseases. Moreover, the population\nat risk is not homogeneous either, since disease exposure and susceptibility\ncan vary considerably. In this paper, we propose a piece-wise stationary\ntransition matrix to explain the heterogeneity in time. We propose a\nhierarchical structure for the heterogeneity in population, where prior\ninformation is considered to deal with unbalanced data. Moreover, an efficient,\nscalable EM algorithm is proposed for inference. We demonstrate the feasibility\nand superiority of our model on a cervical cancer screening dataset from the\nCancer Registry of Norway. Experiments show that our model outperforms\nstate-of-the-art recurrent neural network models in terms of prediction\naccuracy and significantly outperforms a standard hidden Markov jump process in\ngenerating Kaplan-Meier estimators.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:23:34 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Braden", "Soper", ""], ["Nygard", "Jan", ""], ["Nygrad", "Mari", ""], ["Lee", "Herbert", ""]]}, {"id": "1910.05851", "submitter": "Rui Meng", "authors": "Rui Meng, Braden Soper, Herbert Lee, Vincent X. Liu, John D. Greene,\n  Priyadip Ray", "title": "Nonstationary Multivariate Gaussian Processes for Electronic Health\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose multivariate nonstationary Gaussian processes for jointly modeling\nmultiple clinical variables, where the key parameters, length-scales, standard\ndeviations and the correlations between the observed output, are all time\ndependent. We perform posterior inference via Hamiltonian Monte Carlo (HMC). We\nalso provide methods for obtaining computationally efficient gradient-based\nmaximum a posteriori (MAP) estimates. We validate our model on synthetic data\nas well as on electronic health records (EHR) data from Kaiser Permanente (KP).\nWe show that the proposed model provides better predictive performance over a\nstationary model as well as uncovers interesting latent correlation processes\nacross vitals which are potentially predictive of patient risk.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 22:37:08 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Meng", "Rui", ""], ["Soper", "Braden", ""], ["Lee", "Herbert", ""], ["Liu", "Vincent X.", ""], ["Greene", "John D.", ""], ["Ray", "Priyadip", ""]]}, {"id": "1910.06008", "submitter": "Alan Huang", "authors": "Alan Huang, Andy Sang Il Kim", "title": "Bayesian generalized linear model for over and under dispersed counts", "comments": "10 pages, 2 figures, 1 table; 2 page supplement. Accepted October\n  2019", "journal-ref": null, "doi": "10.1080/03610926.2019.1682162", "report-no": "Accepted October 2019", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models that can handle both over and under dispersed counts are rare\nin the literature, perhaps because full probability distributions for dispersed\ncounts are rather difficult to construct. This note takes a first look at\nBayesian Conway-Maxwell-Poisson generalized linear models that can handle both\nover and under dispersion yet retain the parsimony and interpretability of\nclassical count regression models. The focus is on providing an explicit\ndemonstration of Bayesian regression inferences for dispersed counts via a\nMetropolis-Hastings algorithm. We illustrate the approach on two data analysis\nexamples and demonstrate some favourable frequentist properties via a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:38:51 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:59:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Huang", "Alan", ""], ["Kim", "Andy Sang Il", ""]]}, {"id": "1910.06121", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Aki Vehtari, Pekka Marttinen", "title": "Batch simulations and uncertainty quantification in Gaussian process\n  surrogate approximate Bayesian computation", "comments": "Minor improvements and clarifications to the text over the previous\n  version. 20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational efficiency of approximate Bayesian computation (ABC) has\nbeen improved by using surrogate models such as Gaussian processes (GP). In one\nsuch promising framework the discrepancy between the simulated and observed\ndata is modelled with a GP which is further used to form a model-based\nestimator for the intractable posterior. In this article we improve this\napproach in several ways. We develop batch-sequential Bayesian experimental\ndesign strategies to parallellise the expensive simulations. In earlier work\nonly sequential strategies have been used. Current surrogate-based ABC methods\nalso do not fully account the uncertainty due to the limited budget of\nsimulations as they output only a point estimate of the ABC posterior. We\npropose a numerical method to fully quantify the uncertainty in, for example,\nABC posterior moments. We also provide some new analysis on the GP modelling\nassumptions in the resulting improved framework called Bayesian ABC and discuss\nits connection to Bayesian quadrature (BQ) and Bayesian optimisation (BO).\nExperiments with toy and real-world simulation models demonstrate advantages of\nthe proposed techniques.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 13:01:32 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 12:35:02 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 12:46:52 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1910.06239", "submitter": "Matthias Kirchler", "authors": "Matthias Kirchler, Shahryar Khorasani, Marius Kloft, Christoph Lippert", "title": "Two-sample Testing Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample testing procedure based on learned deep neural\nnetwork representations. To this end, we define two test statistics that\nperform an asymptotic location test on data samples mapped onto a hidden layer.\nThe tests are consistent and asymptotically control the type-1 error rate.\nTheir test statistics can be evaluated in linear time (in the sample size).\nSuitable data representations are obtained in a data-driven way, by solving a\nsupervised or unsupervised transfer-learning task on an auxiliary (potentially\ndistinct) data set. If no auxiliary data is available, we split the data into\ntwo chunks: one for learning representations and one for computing the test\nstatistic. In experiments on audio samples, natural images and\nthree-dimensional neuroimaging data our tests yield significant decreases in\ntype-2 error rate (up to 35 percentage points) compared to state-of-the-art\ntwo-sample tests such as kernel-methods and classifier two-sample tests.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:16:58 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 16:01:53 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Kirchler", "Matthias", ""], ["Khorasani", "Shahryar", ""], ["Kloft", "Marius", ""], ["Lippert", "Christoph", ""]]}, {"id": "1910.06386", "submitter": "Arun Kuchibhotla", "authors": "Arun K. Kuchibhotla, Lawrence D. Brown, Andreas Buja and Junhui Cai", "title": "All of Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares linear regression is one of the oldest and widely used data\nanalysis tools. Although the theoretical analysis of the ordinary least squares\n(OLS) estimator is as old, several fundamental questions are yet to be\nanswered. Suppose regression observations\n$(X_1,Y_1),\\ldots,(X_n,Y_n)\\in\\mathbb{R}^d\\times\\mathbb{R}$ (not necessarily\nindependent) are available. Some of the questions we deal with are as follows:\nunder what conditions, does the OLS estimator converge and what is the limit?\nWhat happens if the dimension is allowed to grow with $n$? What happens if the\nobservations are dependent with dependence possibly strengthening with $n$? How\nto do statistical inference under these kinds of misspecification? What happens\nto the OLS estimator under variable selection? How to do inference under\nmisspecification and variable selection?\n  We answer all the questions raised above with one simple deterministic\ninequality which holds for any set of observations and any sample size. This\nimplies that all our results are a finite sample (non-asymptotic) in nature. In\nthe end, one only needs to bound certain random quantities under specific\nsettings of interest to get concrete rates and we derive these bounds for the\ncase of independent observations. In particular, the problem of inference after\nvariable selection is studied, for the first time, when $d$, the number of\ncovariates increases (almost exponentially) with sample size $n$. We provide\ncomments on the ``right'' statistic to consider for inference under variable\nselection and efficient computation of quantiles.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 19:21:46 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Brown", "Lawrence D.", ""], ["Buja", "Andreas", ""], ["Cai", "Junhui", ""]]}, {"id": "1910.06390", "submitter": "Eric Nyarko Mr.", "authors": "Eric Nyarko", "title": "Optimal Paired Comparison Block Designs", "comments": "42 pages technical report on paired comparison block designs under\n  the main effects model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For paired comparison experiments involving competing options described by\ntwo-level attributes several different methods of constructing designs having\nblock paired observations under the main effects model are presented. These\ndesigns are compared to alternative designs available in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 19:35:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Nyarko", "Eric", ""]]}, {"id": "1910.06443", "submitter": "Ruth Keogh", "authors": "Ruth H. Keogh and Jonathan W. Bartlett", "title": "Measurement error as a missing data problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on measurement error in covariates in regression\nanalyses in which the aim is to estimate the association between one or more\ncovariates and an outcome, adjusting for confounding. Error in covariate\nmeasurements, if ignored, results in biased estimates of parameters\nrepresenting the associations of interest. Studies with variables measured with\nerror can be considered as studies in which the true variable is missing, for\neither some or all study participants. We make the link between measurement\nerror and missing data and describe methods for correcting for bias due to\ncovariate measurement error with reference to this link, including regression\ncalibration (conditional mean imputation), maximum likelihood and Bayesian\nmethods, and multiple imputation. The methods are illustrated using data from\nthe Third National Health and Nutrition Examination Survey (NHANES III) to\ninvestigate the association between the error-prone covariate systolic blood\npressure and the hazard of death due to cardiovascular disease, adjusted for\nseveral other variables including those subject to missing data. We use\nmultiple imputation and Bayesian approaches that can address both measurement\nerror and missing data simultaneously. Example data and R code are provided in\nsupplementary materials.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:03:42 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Keogh", "Ruth H.", ""], ["Bartlett", "Jonathan W.", ""]]}, {"id": "1910.06449", "submitter": "David Cheng", "authors": "David Cheng, Rajeev Ayyagari, James Signorovitch", "title": "The Statistical Performance of Matching-Adjusted Indirect Comparisons", "comments": "36 pages, 3 figures; Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect comparisons of treatment-specific outcomes across separate studies\noften inform decision-making in the absence of head-to-head randomized\ncomparisons. Differences in baseline characteristics between study populations\nmay introduce confounding bias in such comparisons. Matching-adjusted indirect\ncomparison (MAIC) (Signorovitch et al., 2010) has been used to adjust for\ndifferences in observed baseline covariates when the individual patient-level\ndata (IPD) are available for only one study and aggregate data (AGD) are\navailable for the other study. The approach weights outcomes from the IPD using\nestimates of trial selection odds that balance baseline covariates between the\nIPD and AGD. With the increasing use of MAIC, there is a need for formal\nassessments of its statistical properties. In this paper we formulate\nidentification assumptions for causal estimands that justify MAIC estimators.\nWe then examine large sample properties and evaluate strategies for estimating\nstandard errors without the full IPD from both studies. The finite-sample bias\nof MAIC and the performance of confidence intervals based on different standard\nerror estimators are evaluated through simulations. The method is illustrated\nthrough an example comparing placebo arm and natural history outcomes in\nDuchenne muscular dystrophy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:14:46 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 04:47:08 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Cheng", "David", ""], ["Ayyagari", "Rajeev", ""], ["Signorovitch", "James", ""]]}, {"id": "1910.06512", "submitter": "John Page", "authors": "John Paige, Geir-Arne Fuglstad, Andrea Riebler, Jon Wakefield", "title": "Design- and Model-Based Approaches to Small-Area Estimation in a Low and\n  Middle Income Country Context: Comparisons and Recommendations", "comments": "Main text: 35 pages, 5 figures, 2 tables. Supplementary materials: 63\n  pages, 5 figures, 21 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for rigorous and timely health and demographic summaries has\nprovided the impetus for an explosion in geographic studies, with a common\napproach being the production of pixel-level maps, particularly in low and\nmiddle income countries. In this context, household surveys are a major source\nof data, usually with a two-stage cluster design with stratification by region\nand urbanicity. Accurate estimates are of crucial interest for precision public\nhealth policy interventions, but many current studies take a cavalier approach\nto acknowledging the sampling design, while presenting results at a fine\ngeographic scale. In this paper we investigate the extent to which accounting\nfor sample design can affect predictions at the aggregate level, which is\nusually the target of inference. We describe a simulation study in which\nrealistic sampling frames are created for Kenya, based on population and\ndemographic information, with a survey design that mimics a Demographic Health\nSurvey (DHS). We compare the predictive performance of various commonly-used\nmodels. We also describe a cluster level model with a discrete spatial\nsmoothing prior that has not been previously used, but provides reliable\ninference. We find that including stratification and cluster level random\neffects can improve predictive performance. Spatially smoothed direct\n(weighted) estimates were robust to priors and survey design. Continuous\nspatial models performed well in the presence of fine scale variation; however,\nthese models require the most \"hand holding\". Subsequently, we examine how the\nmodels perform on real data; specifically we model the prevalence of secondary\neducation for women aged 20-29 using data from the 2014 Kenya DHS.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:48:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Paige", "John", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Wakefield", "Jon", ""]]}, {"id": "1910.06538", "submitter": "Ick Hoon Jin", "authors": "Chang Che and Ick Hoon Jin and Zhiyong Zhang", "title": "Network Mediation Analysis Using Model-based Eigenvalue Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new two-stage network mediation method based on the use\nof a latent network approach -- model-based eigenvalue decomposition -- for\nanalyzing social network data with nodal covariates. In the decomposition stage\nof the observed network, no assumption on the metric of the latent space\nstructure is required. In the mediation stage, the most important eigenvectors\nof a network are used as mediators. This method further offers an innovative\nway for controlling for the conditional covariates and it only considers the\ninformation left in the network. We demonstrate this approach in a detailed\ntutorial R code provided for four separate cases -- unconditional and\nconditional model-based eigenvalue decompositions for either a continuous\noutcome or a binary outcome -- to show its applicability to empirical network\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:41:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Che", "Chang", ""], ["Jin", "Ick Hoon", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1910.06539", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou and Jacob Hinkle and M. Todd Young and David\n  Womble", "title": "Challenges in Markov chain Monte Carlo for Bayesian neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods have not been broadly adopted in\nBayesian neural networks (BNNs). This paper initially reviews the main\nchallenges in sampling from the parameter posterior of a neural network via\nMCMC. Such challenges culminate to lack of convergence to the parameter\nposterior. Nevertheless, this paper shows that a non-converged Markov chain,\ngenerated via MCMC sampling from the parameter space of a neural network, can\nyield via Bayesian marginalization a valuable predictive posterior of the\noutput of the neural network. Classification examples based on multilayer\nperceptrons showcase highly accurate predictive posteriors. The postulate of\nlimited scope for MCMC developments in BNNs is partially valid; an\nasymptotically exact parameter posterior seems less plausible, yet an accurate\npredictive posterior is a tenable research avenue.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 05:43:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 18:02:21 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 03:37:55 GMT"}, {"version": "v4", "created": "Tue, 23 Feb 2021 10:39:02 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Hinkle", "Jacob", ""], ["Young", "M. Todd", ""], ["Womble", "David", ""]]}, {"id": "1910.06596", "submitter": "Alex Lenkoski", "authors": "Alex Lenkoski and Fredrik Lohne Aanes", "title": "Sovereign Risk Indices and Bayesian Theory Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In economic applications, model averaging has found principal use examining\nthe validity of various theories related to observed heterogeneity in outcomes\nsuch as growth, development, and trade.Though often easy to articulate, these\ntheories are imperfectly captured quantitatively. A number of different proxies\nare often collected for a given theory and the uneven nature of this collection\nrequires care when employing model averaging. Furthermore, if valid, these\ntheories ought to be relevant outside of any single narrowly focused outcome\nequation. We propose a methodology which treats theories as represented by\nlatent indices, these latent processes controlled by model averaging on the\nproxy level. To achieve generalizability of the theory index our framework\nassumes a collection of outcome equations. We accommodate a flexible set of\ngeneralized additive models, enabling non-Gaussian outcomes to be included.\nFurthermore, selection of relevant theories also occurs on the outcome level,\nallowing for theories to be differentially valid. Our focus is on creating a\nset of theory-based indices directed at understanding a country's potential\nrisk of macroeconomic collapse. These Sovereign Risk Indices are calibrated\nacross a set of different \"collapse\" criteria, including default on sovereign\ndebt, heightened potential for high unemployment or inflation and dramatic\nswings in foreign exchange values. The goal of this exercise is to render a\nportable set of country/year theory indices which can find more general use in\nthe research community.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 08:52:36 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lenkoski", "Alex", ""], ["Aanes", "Fredrik Lohne", ""]]}, {"id": "1910.06667", "submitter": "Matthew Denwood", "authors": "Matthew Denwood, Giles Innocent, Jamie Prentice, Louise Matthews,\n  Stuart Reid, Christian Pipper, Bruno Levecke, Ray Kaplan, Andrew Kotze,\n  Jennifer Keiser, Marta Palmeirim, Iain McKendrick", "title": "A hypothesis testing framework for the ratio of means of two negative\n  binomial distributions: classifying the efficacy of anthelmintic treatment\n  against intestinal parasites", "comments": "34 pages including 2 main figures, 4 supplementary figures, and\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-dispersed count data typically pose a challenge to analysis using\nstandard statistical methods, particularly when evaluating the efficacy of an\nintervention through the observed effect on the mean. We outline a novel\nstatistical method for analysing such data, along with a statistically coherent\nframework within which the observed efficacy is assigned one of four easily\ninterpretable classifications relative to a target efficacy: \"adequate\",\n\"reduced\", \"borderline\" or \"inconclusive\". We illustrate our approach by\nanalysing the anthelmintic efficacy of mebendazole using a dataset of egg\nreduction rates relating to three intestinal parasites from a treatment arm of\na randomised controlled trial involving 91 children on Pemba Island, Tanzania.\nNumerical validation of the type I error rates of the novel method indicate\nthat it performs as well as the best existing computationally-simple method,\nbut with the additional advantage of providing valid inference in the case of\nan observed efficacy of 100%. The framework and statistical analysis method\npresented also allow the required sample size of a prospective study to be\ndetermined via simulation. Both the framework and method presented have high\npotential utility within medical parasitology, as well as other fields where\nover-dispersed count datasets are commonplace. In order to facilitate the use\nof these methods within the wider medical community, user interfaces for both\nstudy planning and analysis of existing datasets are freely provided along with\nour open-source code via: http://www.fecrt.com/framework\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 11:51:45 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Denwood", "Matthew", ""], ["Innocent", "Giles", ""], ["Prentice", "Jamie", ""], ["Matthews", "Louise", ""], ["Reid", "Stuart", ""], ["Pipper", "Christian", ""], ["Levecke", "Bruno", ""], ["Kaplan", "Ray", ""], ["Kotze", "Andrew", ""], ["Keiser", "Jennifer", ""], ["Palmeirim", "Marta", ""], ["McKendrick", "Iain", ""]]}, {"id": "1910.06724", "submitter": "H{\\aa}vard Kvamme", "authors": "H{\\aa}vard Kvamme and {\\O}rnulf Borgan", "title": "Continuous and Discrete-Time Survival Prediction with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of discrete-time survival methods for continuous-time survival\nprediction is considered. For this purpose, a scheme for discretization of\ncontinuous-time data is proposed by considering the quantiles of the estimated\nevent-time distribution, and, for smaller data sets, it is found to be\npreferable over the commonly used equidistant scheme. Furthermore, two\ninterpolation schemes for continuous-time survival estimates are explored, both\nof which are shown to yield improved performance compared to the discrete-time\nestimates. The survival methods considered are based on the likelihood for\nright-censored survival data, and parameterize either the probability mass\nfunction (PMF) or the discrete-time hazard rate, both with neural networks.\nThrough simulations and study of real-world data, the hazard rate\nparametrization is found to perform slightly better than the parametrization of\nthe PMF. Inspired by these investigations, a continuous-time method is proposed\nby assuming that the continuous-time hazard rate is piecewise constant. The\nmethod, named PC-Hazard, is found to be highly competitive with the\naforementioned methods in addition to other methods for survival prediction\nfound in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 13:23:19 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kvamme", "H\u00e5vard", ""], ["Borgan", "\u00d8rnulf", ""]]}, {"id": "1910.06897", "submitter": "Philip White", "authors": "Philip A. White and Alan E. Gelfand", "title": "Generalized Evolutionary Point Processes: Model Specifications and Model\n  Comparison", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability (2020+)", "doi": "10.1007/s11009-020-09797-8", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized evolutionary point processes offer a class of point process\nmodels that allows for either excitation or inhibition based upon the history\nof the process. In this regard, we propose modeling which comprises\ngeneralization of the nonlinear Hawkes process. Working within a Bayesian\nframework, model fitting is implemented through Markov chain Monte Carlo. This\nentails discussion of computation of the likelihood for such point patterns.\nFurthermore, for this class of models, we discuss strategies for model\ncomparison. Using simulation, we illustrate how well we can distinguish these\nmodels from point pattern specifications with conditionally independent event\ntimes, e.g., Poisson processes. Specifically, we demonstrate that these models\ncan correctly identify true relationships (i.e., excitation or\ninhibition/control). Then, we consider a novel extension of the log Gaussian\nCox process that incorporates evolutionary behavior and illustrate that our\nmodel comparison approach prefers the evolutionary log Gaussian Cox process\ncompared to simpler models. We also examine a real dataset consisting of\nviolent crime events from the 11th police district in Chicago from the year\n2018. This data exhibits strong daily seasonality and changes across the year.\nAfter we account for these data attributes, we find significant but mild\nself-excitation, implying that event occurrence increases the intensity of\nfuture events.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 16:12:13 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip A.", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1910.06939", "submitter": "Benjamin Lengerich", "authors": "Benjamin Lengerich, Bryon Aragam, Eric P. Xing", "title": "Learning Sample-Specific Models with Low-Rank Personalized Regression", "comments": "Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications of machine learning (ML) deal with increasingly\nheterogeneous datasets comprised of data collected from overlapping latent\nsubpopulations. As a result, traditional models trained over large datasets may\nfail to recognize highly predictive localized effects in favour of weakly\npredictive global patterns. This is a problem because localized effects are\ncritical to developing individualized policies and treatment plans in\napplications ranging from precision medicine to advertising. To address this\nchallenge, we propose to estimate sample-specific models that tailor inference\nand prediction at the individual level. In contrast to classical ML models that\nestimate a single, complex model (or only a few complex models), our approach\nproduces a model personalized to each sample. These sample-specific models can\nbe studied to understand subgroup dynamics that go beyond coarse-grained class\nlabels. Crucially, our approach does not assume that relationships between\nsamples (e.g. a similarity network) are known a priori. Instead, we use\nunmodeled covariates to learn a latent distance metric over the samples. We\napply this approach to financial, biomedical, and electoral data as well as\nsimulated data and show that sample-specific models provide fine-grained\ninterpretations of complicated phenomena without sacrificing predictive\naccuracy compared to state-of-the-art models such as deep neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:24:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Lengerich", "Benjamin", ""], ["Aragam", "Bryon", ""], ["Xing", "Eric P.", ""]]}, {"id": "1910.06964", "submitter": "Charles Gray", "authors": "Charles T. Gray", "title": "\\texttt{code::proof}: Prepare for \\emph{most} weather conditions", "comments": "This manuscript was presented by invitation at The Research School on\n  Statistics and Data Science 2019 (RSSDS2019)\n  [https://sites.google.com/view/rssds2019/home] and will be published with the\n  workshop proceedings in Springer Communications in Computer and Information\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational tools for data analysis are being released daily on\nrepositories such as the Comprehensive R Archive Network. How we integrate\nthese tools to solve a problem in research is increasingly complex and\nrequiring frequent updates. To mitigate these \\emph{Kafkaesque} computational\nchallenges in research, this manuscript proposes \\emph{toolchain walkthrough},\nan opinionated documentation of a scientific workflow. As a practical\ncomplement to our proof-based argument~(Gray and Marwick, arXiv, 2019) for\nreproducible data analysis, here we focus on the practicality of setting up a\nreproducible research compendia, with unit tests, as a measure of\n\\texttt{code::proof}, confidence in computational algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 22:13:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Gray", "Charles T.", ""]]}, {"id": "1910.06991", "submitter": "Zhichao Jiang", "authors": "Kosuke Imai and Zhichao Jiang", "title": "Discussion of \"The Blessings of Multiple Causes\" by Wang and Blei", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This commentary has two goals. We first critically review the deconfounder\nmethod and point out its advantages and limitations. We then briefly consider\nthree possible ways to address some of the limitations of the deconfounder\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 18:21:45 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Imai", "Kosuke", ""], ["Jiang", "Zhichao", ""]]}, {"id": "1910.07017", "submitter": "James Normington", "authors": "James Normington, Eric F. Lock, Thomas A. Murray, Caroline S. Carlin", "title": "Bayesian variable selection in hierarchical difference-in-differences\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular method for estimating a causal treatment effect with observational\ndata is the difference-in-differences (DiD) model. In this work, we consider an\nextension of the classical DiD setting to the hierarchical context in which\ndata cannot be matched at the most granular level (e.g., individual-level\ndifferences are unobservable). We propose a Bayesian hierarchical\ndifference-in-differences (HDiD) model which estimates the treatment effect by\nregressing the treatment on a latent variable representing the mean change in\ngroup-level outcome. We present theoretical and empirical results showing that\nan HDiD model that fails to adjust for a particular class of confounding\nvariables, or confounding with the baseline (pre-treatment) outcomes, biases\nthe treatment effect estimate. We propose and implement various approaches to\nperform variable selection using a structured Bayesian spike-and-slab model in\nthe HDiD context. Our proposed methods leverage the temporal structure within\nthe DiD context to select those covariates that lead to unbiased and efficient\nestimation of the causal treatment effect. We evaluate the methods' properties\nthrough theoretical results and simulation, and we use them to assess the\nimpact of primary care redesign of clinics in Minnesota on the management of\ndiabetes outcomes from 2008 to 2017.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 19:38:58 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Normington", "James", ""], ["Lock", "Eric F.", ""], ["Murray", "Thomas A.", ""], ["Carlin", "Caroline S.", ""]]}, {"id": "1910.07074", "submitter": "Paul Parker", "authors": "Paul A. Parker, Scott H. Holan and Ryan Janicki", "title": "Conjugate Bayesian Unit-level Modeling of Count Data Under Informative\n  Sampling Designs", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.267", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit-level models for survey data offer many advantages over their area-level\ncounterparts, such as potential for more precise estimates and a natural\nbenchmarking property. However two main challenges occur in this context:\naccounting for an informative survey design and handling non-Gaussian data\ntypes. The pseudo-likelihood approach is one solution to the former, and\nconjugate multivariate distribution theory offers a solution to the latter. By\ncombining these approaches, we attain a unit-level model for count data that\naccounts for informative sampling designs and includes fully Bayesian model\nuncertainty propagation. Importantly, conjugate full conditional distributions\nhold under the pseudo-likelihood, yielding an extremely computationally\nefficient approach. Our method is illustrated via an empirical simulation study\nusing count data from the American Community Survey public-use microdata\nsample.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 22:03:51 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""], ["Janicki", "Ryan", ""]]}, {"id": "1910.07084", "submitter": "Johannes Bracher", "authors": "Johannes Bracher", "title": "An extended note on the multibin logarithmic score used in the FluSight\n  competitions", "comments": "This note elaborates on a letter published in PNAS: Bracher, J: On\n  the multibin logarithmic score used in the FluSight competitions. PNAS first\n  published September 26, 2019 https://doi.org/10.1073/pnas.1912147116", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the Centers for Disease Control and Prevention (CDC) have\norganized FluSight influenza forecasting competitions. To evaluate the\nparticipants' forecasts a multibin logarithmic score has been created, which is\na non-standard variant of the established logarithmic score. Unlike the\noriginal log score, the multibin version is not proper and may thus encourage\ndishonest forecasting. We explore the practical consequences this may have,\nusing forecasts from the 2016/17 FluSight competition for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 15:56:31 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bracher", "Johannes", ""]]}, {"id": "1910.07121", "submitter": "Cheng Lee", "authors": "C.K. Lee", "title": "Sampling by Reversing The Landmarking Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Variations of the commonly applied landmark sampling are presented. These\nsamplings are \"forward\" landmarking such that each stack of data is created by\nfirst selecting landmarks and then including the subsequent observations of the\nselected landmarks. Unlike these forward landmarking samplings, a \"backward\" or\n\"reverse\" landmarking is proposed with a flexible \"progressive\" weighting on\nselecting different types of events and non-events. The backward landmark\nsample is compared with those forward landmark samples with a real world\nmortgage data. Results show that the backward landmark sample has smaller\nsampling errors than of those forward landmark samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 01:22:01 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Lee", "C. K.", ""]]}, {"id": "1910.07155", "submitter": "Tianbo Chen", "authors": "Tianbo Chen, Ying Sun, Ta-Hsin Li", "title": "A Semi-Parametric Estimation Method for the Quantile Spectrum with an\n  Application to Earthquake Classification Using Convolutional Neural Network", "comments": "24 pages, 3 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new estimation method is introduced for the quantile\nspectrum, which uses a parametric form of the autoregressive (AR) spectrum\ncoupled with nonparametric smoothing. The method begins with quantile\nperiodograms which are constructed by trigonometric quantile regression at\ndifferent quantile levels, to represent the serial dependence of time series at\nvarious quantiles. At each quantile level, we approximate the quantile spectrum\nby a function in the form of an ordinary AR spectrum. In this model, we first\ncompute what we call the quantile autocovariance function (QACF) by the inverse\nFourier transformation of the quantile periodogram at each quantile level.\nThen, we solve the Yule-Walker equations formed by the QACF to obtain the\nquantile partial autocorrelation function (QPACF) and the scale parameter.\nFinally, we smooth QPACF and the scale parameter across the quantile levels\nusing a nonparametric smoother, convert the smoothed QPACF to AR coefficients,\nand obtain the AR spectral density function. Numerical results show that the\nproposed method outperforms other conventional smoothing techniques. We take\nadvantage of the two-dimensional property of the estimators and train a\nconvolutional neural network (CNN) to classify smoothed quantile periodogram of\nearthquake data and achieve a higher accuracy than a similar classifier using\nordinary periodograms.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 03:42:16 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Chen", "Tianbo", ""], ["Sun", "Ying", ""], ["Li", "Ta-Hsin", ""]]}, {"id": "1910.07244", "submitter": "Johannes Bracher", "authors": "Johannes Bracher", "title": "A new INARMA(1, 1) model with Poisson marginals", "comments": "This is a pre-print (submitted version before peer review) of a\n  contribution in Steland, A., Rafajlowicz, E., Okhrin, O. (Eds.): Stochastic\n  Models, Statistics and Their Applications, p. 323-333, published by Springer\n  Nature Switzerland, 2019. The final authenticated version is available at\n  https://doi.org/10.1007/978-3-030-28665-1_24", "journal-ref": "In: Steland, A., Rafajlowicz, E., Okhrin, O. (Eds.): Stochastic\n  Models, Statistics and Their Applications, p. 323-333, Springer Nature\n  Switzerland, 2019", "doi": "10.1007/978-3-030-28665-1_24", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest an INARMA(1, 1) model with Poisson marginals which extends the\nINAR(1) in a similar way as the INGARCH(1, 1) does for the INARCH(1) model. The\nnew model is equivalent to a binomially thinned INAR(1) process. This allows us\nto obtain some of its stochastic properties and use inference methods for\nhidden Markov models. The model is compared to various other models in two case\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:38:41 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Bracher", "Johannes", ""]]}, {"id": "1910.07325", "submitter": "Florian Ziel", "authors": "Florian Ziel, Kevin Berk", "title": "Multivariate Forecasting Evaluation: On Sensitive and Strictly Proper\n  Scoring Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, probabilistic forecasting is an emerging topic, which is why\nthere is a growing need of suitable methods for the evaluation of multivariate\npredictions. We analyze the sensitivity of the most common scoring rules,\nespecially regarding quality of the forecasted dependency structures.\nAdditionally, we propose scoring rules based on the copula, which uniquely\ndescribes the dependency structure for every probability distribution with\ncontinuous marginal distributions. Efficient estimation of the considered\nscoring rules and evaluation methods such as the Diebold-Mariano test are\ndiscussed. In detailed simulation studies, we compare the performance of the\nrenowned scoring rules and the ones we propose. Besides extended synthetic\nstudies based on recently published results we also consider a real data\nexample. We find that the energy score, which is probably the most widely used\nmultivariate scoring rule, performs comparably well in detecting forecast\nerrors, also regarding dependencies. This contradicts other studies. The\nresults also show that a proposed copula score provides very strong distinction\nbetween models with correct and incorrect dependency structure. We close with a\ncomprehensive discussion on the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 12:57:00 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""], ["Berk", "Kevin", ""]]}, {"id": "1910.07393", "submitter": "Zachary Fisher", "authors": "Zachary F. Fisher and Kenneth A. Bollen", "title": "An Instrumental Variable Estimator for Mixed Indicators: Analytic\n  Derivatives and Alternative Parameterizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methodological development of the Model-implied Instrumental Variable (MIIV)\nestimation framework has proved fruitful over the last three decades. Major\nmilestones include Bollen's (1996) original development of the MIIV estimator\nand its robustness properties for continuous endogenous variable SEMs, the\nextension of the MIIV estimator to ordered categorical endogenous variables\n(Bollen \\& Maydeu-Olivares, 2007), and the introduction of a Generalized Method\nof Moments (GMM) estimator (Bollen, Kolenikov \\& Bauldry, 2014). This paper\nfurthers these developments by making several unique contributions not present\nin the prior literature: (1) we use matrix calculus to derive the analytic\nderivatives of the PIV estimator, (2) we extend the PIV estimator to apply to\nany mixture of binary, ordinal, and continuous variables, (3) we generalize the\nPIV model to include intercepts and means, (4) we devise a method to input\nknown threshold values for ordinal observed variables, and (5) we enable a\ngeneral parameterization that permits the estimation of means, variances, and\ncovariances of the underlying variables to use as input into a SEM analysis\nwith PIV. An empirical example illustrates a mixture of continuous variables\nand ordinal variables with fixed thresholds. We also include a simulation study\nto compare the performance of this novel estimator to WLSMV.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:59:56 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 15:32:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Fisher", "Zachary F.", ""], ["Bollen", "Kenneth A.", ""]]}, {"id": "1910.07438", "submitter": "Glen McGee", "authors": "Glen McGee, Marianthi-Anna Kioumourtzoglou, Marc G. Weisskopf,\n  Sebastien Haneuse and Brent A. Coull", "title": "On the Interplay Between Exposure Misclassification and Informative\n  Cluster Size", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12430", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the impact of exposure misclassification when cluster\nsize is potentially informative (i.e., related to outcomes) and when\nmisclassification is differential by cluster size. First, we show that\nmisclassification in an exposure related to cluster size can induce\ninformativeness when cluster size would otherwise be non-informative. Second,\nwe show that misclassification that is differential by informative cluster size\ncan not only attenuate estimates of exposure effects but even inflate or\nreverse the sign of estimates. To correct for bias in estimating marginal\nparameters, we propose two frameworks: (i) an observed likelihood approach for\njoint marginalized models of cluster size and outcomes and (ii) an expected\nestimating equations approach. Although we focus on estimating marginal\nparameters, a corollary is that the observed likelihood approach permits valid\ninference for conditional parameters as well. Using data from the Nurses Health\nStudy II, we compare the results of the proposed correction methods when\napplied to motivating data on the multigenerational effect of in-utero\ndiethylstilbestrol exposure on attention-deficit/hyperactivity disorder in\n106,198 children of 47,450 nurses.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 15:57:40 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["McGee", "Glen", ""], ["Kioumourtzoglou", "Marianthi-Anna", ""], ["Weisskopf", "Marc G.", ""], ["Haneuse", "Sebastien", ""], ["Coull", "Brent A.", ""]]}, {"id": "1910.07447", "submitter": "Amanda Luby", "authors": "Amanda Luby, Anjali Mazumder, Brian Junker", "title": "Psychometric Analysis of Forensic Examiner Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic science often involves the comparison of crime-scene evidence to a\nknown-source sample to determine if the evidence and the reference sample came\nfrom the same source. Even as forensic analysis tools become increasingly\nobjective and automated, final source identifications are often left to\nindividual examiners' interpretation of the evidence. Each source\nidentification relies on judgements about the features and quality of the\ncrime-scene evidence that may vary from one examiner to the next. The current\napproach to characterizing uncertainty in examiners' decision-making has\nlargely centered around the calculation of error rates aggregated across\nexaminers and identification tasks, without taking into account these\nvariations in behavior. We propose a new approach using IRT and IRT-like models\nto account for differences among examiners and additionally account for the\nvarying difficulty among source identification tasks. In particular, we survey\nsome recent advances (Luby, 2019a) in the application of Bayesian psychometric\nmodels, including simple Rasch models as well as more elaborate decision tree\nmodels, to fingerprint examiner behavior.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 16:08:21 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Luby", "Amanda", ""], ["Mazumder", "Anjali", ""], ["Junker", "Brian", ""]]}, {"id": "1910.07689", "submitter": "Zheng Fang", "authors": "Zheng Fang, Juwon Seo", "title": "A Projection Framework for Testing Shape Restrictions That Form Convex\n  Cones", "comments": "A previous version of this paper was circulated under the title \"A\n  General Framework for Inference on Shape Restrictions.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a uniformly valid and asymptotically nonconservative test\nbased on projection for a class of shape restrictions. The key insight we\nexploit is that these restrictions form convex cones, a simple and yet elegant\nstructure that has been barely harnessed in the literature. Based on a\nmonotonicity property afforded by such a geometric structure, we construct a\nbootstrap procedure that, unlike many studies in nonstandard settings,\ndispenses with estimation of local parameter spaces, and the critical values\nare obtained in a way as simple as computing the test statistic. Moreover, by\nappealing to strong approximations, our framework accommodates nonparametric\nregression models as well as distributional/density-related and structural\nsettings. Since the test entails a tuning parameter (due to the nonstandard\nnature of the problem), we propose a data-driven choice and prove its validity.\nMonte Carlo simulations confirm that our test works well.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 03:00:42 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 02:52:22 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 00:19:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fang", "Zheng", ""], ["Seo", "Juwon", ""]]}, {"id": "1910.07712", "submitter": "Jie Peng", "authors": "Jilei Yang and Jie Peng", "title": "Estimating Spatially-Smoothed Fiber Orientation Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion-weighted magnetic resonance imaging (D-MRI) is an in-vivo and\nnon-invasive imaging technology to probe anatomical architectures of biological\nsamples. The anatomy of white matter fiber tracts in the brain can be revealed\nto help understanding of the connectivity patterns among different brain\nregions. In this paper, we propose a novel Nearest-neighbor Adaptive Regression\nModel (NARM) for adaptive estimation of the fiber orientation distribution\n(FOD) function based on D-MRI data, where spatial homogeneity is used to\nimprove FOD estimation by incorporating neighborhood information. Specifically,\nwe formulate the FOD estimation problem as a weighted linear regression\nproblem, where the weights are chosen to account for spatial proximity and\npotential heterogeneity due to different fiber configurations. The weights are\nadaptively updated and a stopping rule based on nearest neighbor distance is\ndesigned to prevent over-smoothing. NARM is further extended to accommodate\nD-MRI data with multiple bvalues.\n  Comprehensive simulation results demonstrate that NARM leads to satisfactory\nFOD reconstructions and performs better than voxel-wise estimation as well as\ncompeting smoothing methods. By applying NARM to real 3T D-MRI datasets, we\ndemonstrate the effectiveness of NARM in recovering more realistic crossing\nfiber patterns and producing more coherent fiber tracking results, establishing\nthe practical value of NARM for analyzing D-MRI data and providing reliable\ninformation on brain structural connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:13:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Yang", "Jilei", ""], ["Peng", "Jie", ""]]}, {"id": "1910.07912", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Rafael Frongillo, Jana Hlavinov\\'a, Birgit Rudloff", "title": "Forecast Evaluation of Quantiles, Prediction Intervals, and other\n  Set-Valued Functionals", "comments": "46 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1907.01306", "journal-ref": "lectronic Journal of Statistics, Volume 15, Number 1 (2021),\n  1034-1084", "doi": "10.1214/21-EJS1808", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a theoretical framework of elicitability and identifiability of\nset-valued functionals, such as quantiles, prediction intervals, and systemic\nrisk measures. A functional is elicitable if it is the unique minimiser of an\nexpected scoring function, and identifiable if it is the unique zero of an\nexpected identification function; both notions are essential for forecast\nranking and validation, and $M$- and $Z$-estimation. Our framework\ndistinguishes between exhaustive forecasts, being set-valued and aiming at\ncorrectly specifying the entire functional, and selective forecasts, content\nwith solely specifying a single point in the correct functional. We establish a\nmutual exclusivity result: A set-valued functional can be either selectively\nelicitable or exhaustively elicitable or not elicitable at all. Notably, since\nquantiles are well known to be selectively elicitable, they fail to be\nexhaustively elicitable. We further show that the class of prediction intervals\nand Vorob'ev quantiles turn out to be exhaustively elicitable and selectively\nidentifiable. In particular, we provide a mixture representation of elementary\nexhaustive scores, leading the way to Murphy diagrams. We give possibility and\nimpossibility results for the shortest prediction interval and prediction\nintervals specified by an endpoint or a midpoint. We end with a comprehensive\nliterature review on common practice in forecast evaluation of set-valued\nfunctionals.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:23:48 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 19:05:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Fissler", "Tobias", ""], ["Frongillo", "Rafael", ""], ["Hlavinov\u00e1", "Jana", ""], ["Rudloff", "Birgit", ""]]}, {"id": "1910.07942", "submitter": "Topi Paananen", "authors": "Topi Paananen, Michael Riis Andersen, Aki Vehtari", "title": "Uncertainty-aware Sensitivity Analysis Using R\\'enyi Divergences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nonlinear supervised learning models, assessing the importance of\npredictor variables or their interactions is not straightforward because it can\nvary in the domain of the variables. Importance can be assessed locally with\nsensitivity analysis using general methods that rely on the model's predictions\nor their derivatives. In this work, we extend derivative based sensitivity\nanalysis to a Bayesian setting by differentiating the R\\'enyi divergence of a\nmodel's predictive distribution. By utilising the predictive distribution\ninstead of a point prediction, the model uncertainty is taken into account in a\nprincipled way. Our empirical results on simulated and real data sets\ndemonstrate accurate and reliable identification of important variables and\ninteraction effects compared to alternative methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 14:33:49 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 09:48:17 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Paananen", "Topi", ""], ["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""]]}, {"id": "1910.07965", "submitter": "Szymon Urbas", "authors": "Szymon Urbas, Chris Sherlock and Paul Metcalfe", "title": "Interim recruitment prediction for multi-centre clinical trials", "comments": "36 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for monitoring, modelling, and predicting\nthe recruitment to multi-centre clinical trials. The work is motivated by\noverly optimistic and narrow prediction intervals produced by existing\ntime-homogeneous recruitment models for multi-centre recruitment. We first\npresent two tests for detection of decay in recruitment rates, together with a\npower study. We then introduce a model based on the inhomogeneous Poisson\nprocess with monotonically decaying intensity, motivated by recruitment trends\nobserved in oncology trials. The general form of the model permits adaptation\nto any parametric curve-shape. A general method for constructing sensible\nparameter priors is provided and Bayesian model averaging is used for making\npredictions which account for the uncertainty in both the parameters and the\nmodel. The validity of the method and its robustness to misspecification are\ntested using simulated datasets. The new methodology is then applied to\noncology trial data, where we make interim accrual predictions, comparing them\nto those obtained by existing methods, and indicate where unexpected changes in\nthe accrual pattern occur.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 15:18:08 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 09:26:08 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 14:27:03 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 10:21:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Urbas", "Szymon", ""], ["Sherlock", "Chris", ""], ["Metcalfe", "Paul", ""]]}, {"id": "1910.08003", "submitter": "Samuel Jackson PhD", "authors": "Samuel E. Jackson and David C. Woods", "title": "Bayes Linear Emulation of Simulator Networks", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computational science allows complex scientific processes to be\ndescribed by mathematical models implemented in computer codes, or simulators.\nWhen these simulators are computationally expensive, it is common to\napproximate them using statistical emulators constructed from computer\nexperiments. Often, the overarching system of interest is best modelled via a\nchain, series or network of simulators, with inputs to some simulators arising\nas outputs from other simulators. Motivated by an epidemiological simulator\nchain to model the airborne dispersion of an infectious disease, we develop and\nassess novel methods for linking statistical emulators of the component\nsimulators within a network. Our methods, Uncertain Input Sampling and\nUncertain Input Bayes Linear Emulation, are developed within a Bayes linear\nframework and exploit the simpler structure that is typically observed for\ncomponent simulators. They explicitly account for simulator input uncertainty\ninduced by links in the network. We demonstrate the advantages of these methods\ncompared to use of a single emulator of the composite simulator network for a\nvariety of examples, including the motivating epidemiological application. In\nall cases, significant gains were attained in terms of both predictive accuracy\nand computational expense.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 16:21:11 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:56:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Jackson", "Samuel E.", ""], ["Woods", "David C.", ""]]}, {"id": "1910.08042", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour", "title": "Comment: Reflections on the Deconfounder", "comments": "Comment to appear in JASA discussion of \"The Blessings of Multiple\n  Causes.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this comment (set to appear in a formal discussion in JASA) is to\ndraw out some conclusions from an extended back-and-forth I have had with Wang\nand Blei regarding the deconfounder method proposed in \"The Blessings of\nMultiple Causes\" [arXiv:1805.06826]. I will make three points here. First, in\nmy role as the critic in this conversation, I will summarize some arguments\nabout the lack of causal identification in the bulk of settings where the\n\"informal\" message of the paper suggests that the deconfounder could be used.\nThis is a point that is discussed at length in D'Amour 2019 [arXiv:1902.10286],\nwhich motivated the results concerning causal identification in Theorems 6--8\nof \"Blessings\". Second, I will argue that adding parametric assumptions to the\nworking model in order to obtain identification of causal parameters (a\nstrategy followed in Theorem 6 and in the experimental examples) is a risky\nstrategy, and should only be done when extremely strong prior information is\navailable. Finally, I will consider the implications of the nonparametric\nidentification results provided for a narrow, but non-trivial, set of causal\nestimands in Theorems 7 and 8. I will highlight that these results may be even\nmore interesting from the perspective of detecting causal identification from\nobserved data, under relatively weak assumptions about confounders.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:10:30 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["D'Amour", "Alexander", ""]]}, {"id": "1910.08063", "submitter": "Georgios Karagiannis", "authors": "Bledar A. Konomi and Georgios Karagiannis", "title": "Bayesian analysis of multifidelity computer models with local features\n  and non-nested experimental designs: Application to the WRF model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-fidelity Bayesian emulator for the analysis of the Weather\nResearch and Forecasting (WRF) model when the available simulations are not\ngenerated based on hierarchically nested experimental design. The proposed\nprocedure, called Augmented Bayesian Treed Co-Kriging, extends the scope of\nco-kriging in two major ways. We introduce a binary treed partition latent\nprocess in the multifidelity setting to account for non-stationary and\npotential discontinuities in the model outputs at different fidelity levels.\nMoreover, we introduce an efficient imputation mechanism which allows the\npractical implementation of co-kriging when the experimental design is\nnon-hierarchically nested by enabling the specification of semi-conjugate\npriors. Our imputation strategy allows the design of an efficient RJ-MCMC\nimplementation that involves collapsed blocks and direct simulation from\nconditional distributions. We develop the Monte Carlo recursive emulator which\nprovides a Monte Carlo proxy for the full predictive distribution of the model\noutput at each fidelity level, in a computationally feasible manner. The\nperformance of our method is demonstrated on a benchmark example, and compared\nagainst existing methods. The proposed method is used for the analysis of a\nlarge-scale climate modeling application which involves the WRF model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 17:51:32 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Konomi", "Bledar A.", ""], ["Karagiannis", "Georgios", ""]]}, {"id": "1910.08105", "submitter": "Matteo Fontana", "authors": "Ilia Nouretdinov, James Gammerman, Matteo Fontana, Daljit Rehal", "title": "Multi-level conformal clustering: A distribution-free technique for\n  clustering and anomaly detection", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.07.114", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a clustering technique called \\textit{multi-level\nconformal clustering (MLCC)}. The technique is hierarchical in nature because\nit can be performed at multiple significance levels which yields greater\ninsight into the data than performing it at just one level. We describe the\ntheoretical underpinnings of MLCC, compare and contrast it with the\nhierarchical clustering algorithm, and then apply it to real world datasets to\nassess its performance. There are several advantages to using MLCC over more\nclassical clustering techniques: Once a significance level has been set, MLCC\nis able to automatically select the number of clusters. Furthermore, thanks to\nthe conformal prediction framework the resulting clustering model has a clear\nstatistical meaning without any assumptions about the distribution of the data.\nThis statistical robustness also allows us to perform clustering and anomaly\ndetection simultaneously. Moreover, due to the flexibility of the conformal\nprediction framework, our algorithm can be used on top of many other machine\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:28:56 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 20:03:28 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nouretdinov", "Ilia", ""], ["Gammerman", "James", ""], ["Fontana", "Matteo", ""], ["Rehal", "Daljit", ""]]}, {"id": "1910.08107", "submitter": "Bowen Gang", "authors": "Luella Fu, Bowen Gang, Gareth M. James and Wenguang Sun", "title": "Heterocedasticity-Adjusted Ranking and Thresholding for Large-Scale\n  Multiple Testing", "comments": "55 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardization has been a widely adopted practice in multiple testing, for\nit takes into account the variability in sampling and makes the test statistics\ncomparable across different study units. However, despite conventional wisdom\nto the contrary, we show that there can be a significant loss in information\nfrom basing hypothesis tests on standardized statistics rather than the full\ndata. We develop a new class of heteroscedasticity--adjusted ranking and\nthresholding (HART) rules that aim to improve existing methods by\nsimultaneously exploiting commonalities and adjusting heterogeneities among the\nstudy units. The main idea of HART is to bypass standardization by directly\nincorporating both the summary statistic and its variance into the testing\nprocedure. A key message is that the variance structure of the alternative\ndistribution, which is subsumed under standardized statistics, is highly\ninformative and can be exploited to achieve higher power. The proposed HART\nprocedure is shown to be asymptotically valid and optimal for false discovery\nrate (FDR) control. Our simulation results demonstrate that HART achieves\nsubstantial power gain over existing methods at the same FDR level. We\nillustrate the implementation through a microarray analysis of myeloma.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:37:07 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 01:03:33 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Fu", "Luella", ""], ["Gang", "Bowen", ""], ["James", "Gareth M.", ""], ["Sun", "Wenguang", ""]]}, {"id": "1910.08415", "submitter": "David Abramian", "authors": "David Abramian, Per Sid\\'en, Hans Knutsson, Mattias Villani, Anders\n  Eklund", "title": "Anatomically informed Bayesian spatial priors for fMRI analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.IV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing Bayesian spatial priors for functional magnetic resonance imaging\n(fMRI) data correspond to stationary isotropic smoothing filters that may\noversmooth at anatomical boundaries. We propose two anatomically informed\nBayesian spatial models for fMRI data with local smoothing in each voxel based\non a tensor field estimated from a T1-weighted anatomical image. We show that\nour anatomically informed Bayesian spatial models results in posterior\nprobability maps that follow the anatomical structure.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:35:18 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Abramian", "David", ""], ["Sid\u00e9n", "Per", ""], ["Knutsson", "Hans", ""], ["Villani", "Mattias", ""], ["Eklund", "Anders", ""]]}, {"id": "1910.08468", "submitter": "Nadia Perot", "authors": "Nadia Perot (LEMS, SESI), Alexandre Le Cocguen, Dominique Carr\\'e\n  (LabSIC), Herv\\'e Lamotte (CEA-DEN), Anne Duhart-Barone, Ingmar Pointeau\n  (LMTE)", "title": "Sampling strategy and statistical analysis for radioactive waste\n  characterization", "comments": null, "journal-ref": "Nuclear Engineering and Design, Elsevier, In press", "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the methodology we have developed to define a sampling\nstrategy adapted to operational constraints in order to characterize the\ndihydrogen flow rate of 2714 nuclear waste drums produced by radiolysis\nreaction of organic mixed with \\&alpha;-emitters. The objective was to perform\nfew but relevant measurements. Thus, a sample of only 38 drums has been\nselected to be measured. Statistical analysis of drum measurement data of\ndihydrogen rate provided an estimation of the mean and the upper bound of the\nphysical quantity of interest which gave a good convergence with global\nmeasurements from the ventilation system of the facility. Thereafter,\nperforming a factorial data analysis has demonstrated the representativeness of\nthe measurement data set and the sampling strategy assumption validity.\nMoreover, it provided information that has been used for a regression analysis\nto develop a linear prediction model of dihydrogen flow rate production for the\nwaste drum characterization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:26:51 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Perot", "Nadia", "", "LEMS, SESI"], ["Cocguen", "Alexandre Le", "", "LabSIC"], ["Carr\u00e9", "Dominique", "", "LabSIC"], ["Lamotte", "Herv\u00e9", "", "CEA-DEN"], ["Duhart-Barone", "Anne", "", "LMTE"], ["Pointeau", "Ingmar", "", "LMTE"]]}, {"id": "1910.08527", "submitter": "Ignavier Ng", "authors": "Ignavier Ng, Zhuangyan Fang, Shengyu Zhu, Zhitang Chen, Jun Wang", "title": "Masked Gradient-Based Causal Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning causal structures from\nobservational data. We reformulate the Structural Equation Model (SEM) in an\naugmented form with a binary graph adjacency matrix and show that, if the\noriginal SEM is identifiable, then this augmented form can be identified up to\nsuper-graphs of the true causal graph under mild conditions. Three methods are\nfurther provided to remove spurious edges to recover the true graph. We next\nutilize the augmented form to develop a masked structure learning method that\ncan be efficiently trained using gradient-based optimization methods, by\nleveraging a smooth characterization on acyclicity and the Gumbel-Softmax\napproach to approximate the binary adjacency matrix. It is found that the\nobtained entries are typically near zero or one, and can be easily thresholded\nto identify the edges. We conduct experiments on synthetic and real datasets to\nvalidate the effectiveness of the proposed method and show that the method can\nreadily include different smooth functions to model causal relationships.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 17:46:44 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 05:47:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Ng", "Ignavier", ""], ["Fang", "Zhuangyan", ""], ["Zhu", "Shengyu", ""], ["Chen", "Zhitang", ""], ["Wang", "Jun", ""]]}, {"id": "1910.08597", "submitter": "Matteo Sordello", "authors": "Matteo Sordello, Hangfeng He and Weijie Su", "title": "Robust Learning Rate Selection for Stochastic Optimization via Splitting\n  Diagnostic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes SplitSGD, a new dynamic learning rate schedule for\nstochastic optimization. This method decreases the learning rate for better\nadaptation to the local geometry of the objective function whenever a\nstationary phase is detected, that is, the iterates are likely to bounce at\naround a vicinity of a local minimum. The detection is performed by splitting\nthe single thread into two and using the inner product of the gradients from\nthe two threads as a measure of stationarity. Owing to this simple yet provably\nvalid stationarity detection, SplitSGD is easy-to-implement and essentially\ndoes not incur additional computational cost than standard SGD. Through a\nseries of extensive experiments, we show that this method is appropriate for\nboth convex problems and training (non-convex) neural networks, with\nperformance compared favorably to other stochastic optimization methods.\nImportantly, this method is observed to be very robust with a set of default\nparameters for a wide range of problems and, moreover, yields better\ngeneralization performance than other adaptive gradient methods such as Adam.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:38:53 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 13:59:57 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 17:30:27 GMT"}, {"version": "v4", "created": "Sun, 7 Jun 2020 16:38:13 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sordello", "Matteo", ""], ["He", "Hangfeng", ""], ["Su", "Weijie", ""]]}, {"id": "1910.08599", "submitter": "Bruno Santos", "authors": "Bruno Santos and Thomas Kneib", "title": "Noncrossing structured additive multiple-output Bayesian quantile\n  regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantile regression models are a powerful tool for studying different points\nof the conditional distribution of univariate response variables. Their\nmultivariate counterpart extension though is not straightforward, starting with\nthe definition of multivariate quantiles. We propose here a flexible Bayesian\nquantile regression model when the response variable is multivariate, where we\nare able to define a structured additive framework for all predictor variables.\nWe build on previous ideas considering a directional approach to define the\nquantiles of a response variable with multiple-outputs and we define\nnoncrossing quantiles in every directional quantile model. We define a Markov\nChain Monte Carlo (MCMC) procedure for model estimation, where the noncrossing\nproperty is obtained considering a Gaussian process design to model the\ncorrelation between several quantile regression models. We illustrate the\nresults of these models using two data sets: one on dimensions of inequality in\nthe population, such as income and health; the second on scores of students in\nthe Brazilian High School National Exam, considering three dimensions for the\nresponse variable.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:39:21 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Santos", "Bruno", ""], ["Kneib", "Thomas", ""]]}, {"id": "1910.08644", "submitter": "Fatima Batool", "authors": "Fatima Batool", "title": "Initialization methods for optimum average silhouette width clustering", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A unified clustering approach that can estimate number of clusters and\nproduce clustering against this number simultaneously is proposed. Average\nsilhouette width (ASW) is a widely used standard cluster quality index. A\ndistance based objective function that optimizes ASW for clustering is defined.\nThe proposed algorithm named as OSil, only, needs data observations as an input\nwithout any prior knowledge of the number of clusters. This work is about\nthorough investigation of the proposed methodology, its usefulness and\nlimitations. A vast spectrum of clustering structures were generated, and\nseveral well-known clustering methods including partitioning, hierarchical,\ndensity based, and spatial methods were consider as the competitor of the\nproposed methodology. Simulation reveals that OSil algorithm has shown superior\nperformance in terms of clustering quality than all clustering methods included\nin the study. OSil can find well separated, compact clusters and have shown\nbetter performance for the estimation of number of clusters as compared to\nseveral methods. Apart from the proposal of the new methodology and it's\ninvestigation the paper offers a systematic analysis on the estimation of\ncluster indices, some of which never appeared together in comparative\nsimulation setup before. The study offers many insightful findings useful for\nthe selection of the clustering methods and indices for clustering quality\nassessment.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:11:12 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 18:16:22 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 20:28:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Batool", "Fatima", ""]]}, {"id": "1910.08664", "submitter": "Chengan Du", "authors": "Chengan Du, Shu-Xia Li, Zhenqiu Lin and Haiqun Lin", "title": "Latent Variable Model for Multivariate Data with Measure-specific Sample\n  Weights and Its Application in Hospital Compare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a single factor model with measure-specific sample weights for\nmultivariate data with multiple observed indicators clustered within a higher\nlevel subject. The factor is therefore a latent variable shared by multiple\nindicators within a same subject and the sample weights are different across\ndifferent indicators and different subjects. Even after integrating out the\nlatent variable, the likelihood of the data cannot be written as the sum of\nweighted likelihood of each subject because a subject has different sample\nweights respectively for its multiple indicators. In addition, the number of\navailable indicators varies across subjects. We derive a pseudo likelihood for\nthe latent variable model with measure-specific weights. We investigate various\nstatistical properties of the latent variable model with measure-specific\nsample weights and its connection to the traditional factor analysis. We found\nthat the latent variable model provides consistent estimates for its variances\nwhen the measure-specific sample weights are properly re-scaled. Two estimation\nprocedures are developed - EM algorithm for the pseudo likelihood and\nmarginalization of the pseudo likelihood by directly integrating out the latent\nvariable to obtain the parameter estimates. This approach is illustrated by the\nanalysis of publicly reported hospitals with indicators and sample weights.\nNumerical studies are conducted to investigate the influence of weights and\ntheir sample distribution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 23:59:41 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Du", "Chengan", ""], ["Li", "Shu-Xia", ""], ["Lin", "Zhenqiu", ""], ["Lin", "Haiqun", ""]]}, {"id": "1910.08750", "submitter": "Aditi Kathpalia", "authors": "Aditi Kathpalia and Nithin Nagaraj", "title": "Measuring Causality: The Science of Cause and Effect", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining and measuring cause-effect relationships is fundamental to most\nscientific studies of natural phenomena. The notion of causation is distinctly\ndifferent from correlation which only looks at association of trends or\npatterns in measurements. In this article, we review different notions of\ncausality and focus especially on measuring causality from time series data.\nCausality testing finds numerous applications in diverse disciplines such as\nneuroscience, econometrics, climatology, physics and artificial intelligence.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 11:15:24 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Kathpalia", "Aditi", ""], ["Nagaraj", "Nithin", ""]]}, {"id": "1910.08769", "submitter": "Holger Dette", "authors": "Holger Dette, Kathrin M\\\"ollenhoff, Frank Bretz", "title": "Equivalence tests for binary efficacy-toxicity responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trials often aim to compare a new drug with a reference treatment in\nterms of efficacy and/or toxicity depending on covariates such as, for example,\nthe dose level of the drug. Equivalence of these treatments can be claimed if\nthe difference in average outcome is below a certain threshold over the\ncovariate range. In this paper we assume that the efficacy and toxicity of the\ntreatments are measured as binary outcome variables and we address two\nproblems. First, we develop a new test procedure for the assessment of\nequivalence of two treatments over the entire covariate range for a single\nbinary endpoint. Our approach is based on a parametric bootstrap, which\ngenerates data under the constraint that the distance between the curves is\nequal to the pre-specified equivalence threshold. Second, we address\nequivalence for bivariate binary (correlated) outcomes by extending the\nprevious approach for a univariate response. For this purpose we use a\n2-dimensional Gumbel model for binary efficacy-toxicity responses. We\ninvestigate the operating characteristics of the proposed approaches by means\nof a simulation study and present a case study as an illustration.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 13:30:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dette", "Holger", ""], ["M\u00f6llenhoff", "Kathrin", ""], ["Bretz", "Frank", ""]]}, {"id": "1910.08846", "submitter": "Samuel Jackson PhD", "authors": "Samuel E. Jackson and Ian Vernon", "title": "Efficient Emulation of Computer Models Utilising Multiple Known\n  Boundaries of Differing Dimensions", "comments": "41 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emulation has been successfully applied across a wide variety of scientific\ndisciplines for efficiently analysing computationally intensive models. We\ndevelop known boundary emulation strategies which utilise the fact that, for\nmany computer models, there exist hyperplanes in the input parameter space for\nwhich the model output can be evaluated far more efficiently, whether this be\nanalytically or just significantly faster using a more efficient and simpler\nnumerical solver. The information contained on these known hyperplanes, or\nboundaries, can be incorporated into the emulation process via analytical\nupdate, thus involving no additional computational cost. In this article, we\nshow that such analytical updates are available for multiple boundaries of\nvarious dimensions. We subsequently demonstrate which configurations of\nboundaries such analytical updates are available for, in particular by\npresenting a set of conditions that such a set of boundaries must satisfy. We\ndemonstrate the powerful computational advantages of the known boundary\nemulation techniques developed on both an illustrative low-dimensional\nsimulated example and a scientifically relevant and high-dimensional systems\nbiology model of hormonal crosstalk in the roots of an Arabidopsis plant.\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 21:29:56 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 09:01:41 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Jackson", "Samuel E.", ""], ["Vernon", "Ian", ""]]}, {"id": "1910.08877", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Targeted Estimation of Heterogeneous Treatment Effect in Observational\n  Survival Analysis", "comments": null, "journal-ref": "j.jbi.2020.103474", "doi": "10.1016/j.jbi.2020.103474", "report-no": null, "categories": "stat.ME q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The aim of clinical effectiveness research using repositories of electronic\nhealth records is to identify what health interventions 'work best' in\nreal-world settings. Since there are several reasons why the net benefit of\nintervention may differ across patients, current comparative effectiveness\nliterature focuses on investigating heterogeneous treatment effect and\npredicting whether an individual might benefit from an intervention. The\nmajority of this literature has concentrated on the estimation of the effect of\ntreatment on binary outcomes. However, many medical interventions are evaluated\nin terms of their effect on future events, which are subject to loss to\nfollow-up. In this study, we describe a framework for the estimation of\nheterogeneous treatment effect in terms of differences in time-to-event\n(survival) probabilities. We divide the problem into three phases: (1)\nestimation of treatment effect conditioned on unique sets of the covariate\nvector; (2) identification of features important for heterogeneity using an\nensemble of non-parametric variable importance methods; and (3) estimation of\ntreatment effect on the reference classes defined by the previously selected\nfeatures, using one-step Targeted Maximum Likelihood Estimation. We conducted a\nseries of simulation studies and found that this method performs well when\neither sample size or event rate is high enough and the number of covariates\ncontributing to the effect heterogeneity is moderate. An application of this\nmethod to a clinical case study was conducted by estimating the effect of oral\nanticoagulants on newly diagnosed non-valvular atrial fibrillation patients\nusing data from the UK Clinical Practice Research Datalink.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 02:27:45 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 22:31:03 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "1910.08892", "submitter": "Ying Jin", "authors": "Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, Jian Guo", "title": "Bayesian Symbolic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability is crucial for machine learning in many scenarios such as\nquantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a\nclassic interpretable machine learning method by bridging X and Y using\nmathematical expressions composed of some basic functions. However, the search\nspace of all possible expressions grows exponentially with the length of the\nexpression, making it infeasible for enumeration. Genetic programming (GP) has\nbeen traditionally and commonly used in SR to search for the optimal solution,\nbut it suffers from several limitations, e.g. the difficulty in incorporating\nprior knowledge; overly-complicated output expression and reduced\ninterpretability etc. To address these issues, we propose a new method to fit\nSR under a Bayesian framework. Firstly, Bayesian model can naturally\nincorporate prior knowledge (e.g., preference of basis functions, operators and\nraw features) to improve the efficiency of fitting SR. Secondly, to improve\ninterpretability of expressions in SR, we aim to capture concise but\ninformative signals. To this end, we assume the expected signal has an additive\nstructure, i.e., a linear combination of several concise expressions, whose\ncomplexity is controlled by a well-designed prior distribution. In our setup,\neach expression is characterized by a symbolic tree, and the proposed SR model\ncould be solved by sampling symbolic trees from the posterior distribution\nusing an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared\nwith GP, the proposed BSR(Bayesian Symbolic Regression) method saves computer\nmemory with no need to keep an updated 'genome pool'. Numerical experiments\nshow that, compared with GP, the solutions of BSR are closer to the ground\ntruth and the expressions are more concise. Meanwhile we find the solution of\nBSR is robust to hyper-parameter specifications such as the number of trees.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 04:28:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 06:22:19 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 02:11:21 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Jin", "Ying", ""], ["Fu", "Weilin", ""], ["Kang", "Jian", ""], ["Guo", "Jiadong", ""], ["Guo", "Jian", ""]]}, {"id": "1910.08904", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "$hv$-Block Cross Validation is not a BIBD: a Note on the Paper by Jeff\n  Racine (2000)", "comments": "Technique report. 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note corrects a mistake in the paper \"consistent cross-validatory\nmodel-selection for dependent data: $hv$-block cross-validation\" by Racine\n(2000). In his paper, he implied that the therein proposed $hv$-block\ncross-validation is consistent in the sense of Shao (1993). To get this\nintuition, he relied on the speculation that $hv$-block is a balanced\nincomplete block design (BIBD). This note demonstrates that this is not the\ncase, and thus the theoretical consistency of $hv$-block remains an open\nquestion. In addition, I also provide a Python program counting the number of\noccurrences of each sample and each pair of samples.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 05:27:10 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1910.08936", "submitter": "Adil Yazigi", "authors": "Adil Yazigi, Antti Penttinen, Anna-Kaisa Ylitalo, Matti Maltamo,\n  Petteri Packalen, Lauri Meht\\\"atalo", "title": "Sequential Spatial Point Process Models for Spatio-Temporal Point\n  Processes: A Self-Interactive Model with Application to Forest Tree Data", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the spatial dynamics of a forest stand by using a special class of\nspatio-temporal point processes, the sequential spatial point process, where\nthe spatial dimension is parameterized and the time component is atomic. The\nsequential spatial point processes differ from spatial point processes in the\nsense that the realizations are ordered sequences of spatial locations and the\norder of points allows us to approximate the spatial evolutionary dynamics of\nthe process. This feature shall be useful to interpret the long-term dependence\nand the memory formed by the spatial history of the process. As an\nillustration, the sequence can represent the tree locations ordered with\nrespect to time, or to some given quantitative marks such as tree diameters. We\nderive a parametric sequential spatial point process model that is expressed in\nterms of self-interaction of the spatial points, and then the\nmaximum-likelihood-based inference is tractable. As an application, we apply\nthe model obtained to forest dataset collected from the Kiihtelysvaara site in\nEastern Finland. Potential applications in remote sensing of forests are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 09:36:45 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yazigi", "Adil", ""], ["Penttinen", "Antti", ""], ["Ylitalo", "Anna-Kaisa", ""], ["Maltamo", "Matti", ""], ["Packalen", "Petteri", ""], ["Meht\u00e4talo", "Lauri", ""]]}, {"id": "1910.08997", "submitter": "Trambak Banerjee", "authors": "Trambak Banerjee, Qiang Liu, Gourab Mukherjee and Wenguang Sun", "title": "A General Framework for Empirical Bayes Estimation in Discrete Linear\n  Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a Nonparametric Empirical Bayes (NEB) framework for compound\nestimation in the discrete linear exponential family, which includes a wide\nclass of discrete distributions frequently arising from modern big data\napplications. We propose to directly estimate the Bayes shrinkage factor in the\ngeneralized Robbins' formula via solving a scalable convex program, which is\ncarefully developed based on a RKHS representation of the Stein's discrepancy\nmeasure. The new NEB estimation framework is flexible for incorporating various\nstructural constraints into the data driven rule, and provides a unified\napproach to compound estimation with both regular and scaled squared error\nlosses. We develop theory to show that the class of NEB estimators enjoys\nstrong asymptotic properties. Comprehensive simulation studies as well as\nanalyses of real data examples are carried out to demonstrate the superiority\nof the NEB estimator over competing methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 14:47:48 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Banerjee", "Trambak", ""], ["Liu", "Qiang", ""], ["Mukherjee", "Gourab", ""], ["Sun", "Wenguang", ""]]}, {"id": "1910.09007", "submitter": "Chandler Squires", "authors": "Chandler Squires, Yuhao Wang, Caroline Uhler", "title": "Permutation-Based Causal Structure Learning with Unknown Intervention\n  Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating causal DAG models from a mix of\nobservational and interventional data, when the intervention targets are\npartially or completely unknown. This problem is highly relevant for example in\ngenomics, since gene knockout technologies are known to have off-target\neffects. We characterize the interventional Markov equivalence class of DAGs\nthat can be identified from interventional data with unknown intervention\ntargets. In addition, we propose a provably consistent algorithm for learning\nthe interventional Markov equivalence class from such data. The proposed\nalgorithm greedily searches over the space of permutations to minimize a novel\nscore function. The algorithm is nonparametric, which is particularly important\nfor applications to genomics, where the relationships between variables are\noften non-linear and the distribution non-Gaussian. We demonstrate the\nperformance of our algorithm on synthetic and biological datasets. Links to an\nimplementation of our algorithm and to a reproducible code base for our\nexperiments can be found at https://uhlerlab.github.io/causaldag/utigsp.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:02:27 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 19:37:08 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Squires", "Chandler", ""], ["Wang", "Yuhao", ""], ["Uhler", "Caroline", ""]]}, {"id": "1910.09062", "submitter": "Nicole Pashley", "authors": "Nicole E. Pashley", "title": "Note on the Delta Method for Finite Population Inference with\n  Applications to Causal Inference", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The delta method creates more general inference results when coupled with\ncentral limit theorem results for the finite population. This opens up a range\nof new estimators for which we can find finite population asymptotic\nproperties. We focus on the use of this method to derive asymptotic\ndistributional results and variance expressions for causal estimators. We\nillustrate the use of the method by obtaining a finite population asymptotic\ndistribution for a causal ratio estimator.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 20:49:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 18:38:49 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pashley", "Nicole E.", ""]]}, {"id": "1910.09092", "submitter": "Michael Lingzhi Li", "authors": "Dimitris Bertsimas, Michael Lingzhi Li", "title": "Fast Exact Matrix Completion: A Unified Optimization Framework for\n  Matrix Completion", "comments": null, "journal-ref": "Journal of Machine Learning Research 21 (2020) 1-43", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the problem of matrix completion with and without side\ninformation as a non-convex optimization problem. We design fastImpute based on\nnon-convex gradient descent and show it converges to a global minimum that is\nguaranteed to recover closely the underlying matrix while it scales to matrices\nof sizes beyond $10^5 \\times 10^5$. We report experiments on both synthetic and\nreal-world datasets that show fastImpute is competitive in both the accuracy of\nthe matrix recovered and the time needed across all cases. Furthermore, when a\nhigh number of entries are missing, fastImpute is over $75\\%$ lower in MAPE and\n$15$ times faster than current state-of-the-art matrix completion methods in\nboth the case with side information and without.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 00:37:06 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 22:23:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "1910.09189", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock and Geoffrey J. McLachlan", "title": "An Apparent Paradox: A Classifier Trained from a Partially Classified\n  Sample May Have Smaller Expected Error Rate Than That If the Sample Were\n  Completely Classified", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interest in using semi-supervised learning to form\na classifier. As is well known, the (Fisher) information in an unclassified\nfeature with unknown class label is less (considerably less for weakly\nseparated classes) than that of a classified feature which has known class\nlabel. Hence assuming that the labels of the unclassified features are randomly\nmissing or their missing-label mechanism is simply ignored, the expected error\nrate of a classifier formed from a partially classified sample is greater than\nthat if the sample were completely classified. We propose to treat the labels\nof the unclassified features as missing data and to introduce a framework for\ntheir missingness in situations where these labels are not randomly missing. An\nexamination of several partially classified data sets in the literature\nsuggests that the unclassified features are not occurring at random but rather\ntend to be concentrated in regions of relatively high entropy in the feature\nspace. Here in the context of two normal classes with a common covariance\nmatrix we consider the situation where the missingness of the labels of the\nunclassified features can be modelled by a logistic model in which the\nprobability of a missing label for a feature depends on its entropy. Rather\nparadoxically, we show that the classifier so formed from the partially\nclassified sample may have smaller expected error rate that that if the sample\nwere completely classified.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:47:36 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 03:31:21 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Ahfock", "Daniel", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1910.09211", "submitter": "Tchilabalo Abozou Kpanzou", "authors": "Gane Samb Lo, Tchilabalo Abozou Kpanzou and Cheikh Mohamed Haidara", "title": "Statistical tests for the Pseudo-Lindley distribution and applications", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-Lindley distribution was introduced as a useful generalization of\nthe Lindley distribution in Zeghdoudi and Nedjar (2016) who showed interesting\nproperties of their new laws and efficiencies in modeling data in Reliability\nand Survival Analysis. In this paper we study the estimators of the pair of\nparameters and determine their asymptotic law from which a chi-square law is\nderived. From both asymptotic laws, statistical tests are built. Simulation\nstudies on the tests conclude to their efficiency for data sizes generally used\nin Reliability. R codes related to statistical analysis on that law are given\nin an appropriate archive repository code paper in Arxiv.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 08:57:12 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Lo", "Gane Samb", ""], ["Kpanzou", "Tchilabalo Abozou", ""], ["Haidara", "Cheikh Mohamed", ""]]}, {"id": "1910.09219", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn", "title": "Marginally Interpretable Linear Transformation Models for Clustered\n  Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustered observations are ubiquitous in controlled and observational studies\nand arise naturally in multicenter trials or longitudinal surveys. I present\ntwo novel models for the analysis of clustered observations where the marginal\ndistributions are described by a linear transformation model and the\ncorrelations by a joint multivariate normal distribution. Both models provide\nanalytic formulae for the marginal distributions, one of which features\ndirectly interpretable parameters. Owing to the richness of transformation\nmodels, the techniques are applicable to any type of response variable,\nincluding bounded, skewed, binary, ordinal, or survival responses. I present\nre-analyses of five applications from different domains, including models for\nnon-normal and discrete responses, and explain how specific models for the\nestimation of marginal distributions can be defined within this novel modelling\nframework and how the results can be interpreted in a marginal way.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:08:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hothorn", "Torsten", ""]]}, {"id": "1910.09227", "submitter": "Alisa Kirichenko", "authors": "Rianne de Heide and Alisa Kirichenko and Nishant Mehta and Peter\n  Gr\\\"unwald", "title": "Safe-Bayesian Generalized Linear Regression", "comments": "Final version. Accepted to AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalized Bayesian inference under misspecification, i.e. when the\nmodel is 'wrong but useful'. Generalized Bayes equips the likelihood with a\nlearning rate $\\eta$. We show that for generalized linear models (GLMs),\n$\\eta$-generalized Bayes concentrates around the best approximation of the\ntruth within the model for specific $\\eta \\neq 1$, even under severely\nmisspecified noise, as long as the tails of the true distribution are\nexponential. We derive MCMC samplers for generalized Bayesian lasso and\nlogistic regression and give examples of both simulated and real-world data in\nwhich generalized Bayes substantially outperforms standard Bayes.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:32:26 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 09:20:14 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 21:52:38 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["de Heide", "Rianne", ""], ["Kirichenko", "Alisa", ""], ["Mehta", "Nishant", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "1910.09394", "submitter": "Mehmet A. S\\\"uzen PhD", "authors": "Mehmet S\\\"uzen and Alper Yegenoglu", "title": "Generalised learning of time-series: Ornstein-Uhlenbeck processes", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In machine learning, statistics, econometrics and statistical physics\ncross-validation (CV) is used as a standard approach in quantifying the\ngeneralization performance of a statistical model. In practice, direct usage of\nCV is avoided for time-series due to several issues. A direct application of CV\nin time-series leads to the loss of serial correlations, a requirement of\npreserving any non-stationarity and the prediction of the past data using\nfuture data. In this work, we propose a meta-algorithm called reconstructive\ncross-validation (rCV ) that avoids all these issues. At first, k folds are\nformed with non-overlapping randomly selected subsets of the original\ntime-series. Then, we generate k new partial time-series by removing data\npoints from a given fold: every new partial time-series have missing points at\nrandom from a different entire fold. A suitable imputation or a smoothing\ntechnique is used to reconstruct k time-series. We call these reconstructions\nsecondary models. Thereafter, we build the primary k time-series models using\nnew time-series coming from the secondary models. The performance of the\nprimary models is evaluated simultaneously by computing the deviations from the\noriginally removed data points and out-of-sample (OSS) data. These amounts to\nreconstruction and prediction errors. If the secondary models use a technique\nthat retains the data points exactly, such as Gaussian process regression,\nthere will be no errors present on the data points that are not removed. By\nthis procedure serial correlations are retained, any non-stationarity is\npreserved within models and there will be no prediction of past data using the\nfuture data points. The cross-validation in time-series model can be practised\nwith rCV. Moreover, we can build time-series learning curves by repeating rCV\nprocedure with different k values.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:16:05 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 17:23:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["S\u00fczen", "Mehmet", ""], ["Yegenoglu", "Alper", ""]]}, {"id": "1910.09408", "submitter": "Sibo Cheng", "authors": "Sibo Cheng (EDF R&D PERICLES, LIMSI), Jean-Philippe Argaud (EDF R&D\n  PERICLES), Bertrand Iooss (EDF R&D PRISME, IMT), Didier Lucor (LIMSI),\n  Ang\\'elique Pon\\c{c}ot (EDF R&D PERICLES)", "title": "Background Error Covariance Iterative Updating with Invariant\n  Observation Measures for Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to leverage the information embedded in the background state and\nobservations, covariance matrices modelling is a pivotal point in data\nassimilation algorithms. These matrices are often estimated from an ensemble of\nobservations or forecast differences. Nevertheless, for many industrial\napplications the modelling still remains empirical based on some form of\nexpertise and physical constraints enforcement in the absence of historical\nobservations or predictions. We have developed two novel robust adaptive\nassimilation methods named CUTE (Covariance Updating iTerativE) and PUB\n(Partially Updating BLUE). These two non-parametric methods are based on\ndifferent optimization objectives, both capable of sequentially adapting\nbackground error covariance matrices in order to improve assimilation results\nunder the assumption of a good knowledge of the observation error covariances.\nWe have compared these two methods with the standard approach using a\nmisspecified background matrix in a shallow water twin experiments framework\nwith a linear observation operator. Numerical experiments have shown that the\nproposed methods bear a real advantage both in terms of posterior error\ncorrelation identification and assimilation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:35:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Cheng", "Sibo", "", "EDF R&D PERICLES, LIMSI"], ["Argaud", "Jean-Philippe", "", "EDF R&D\n  PERICLES"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT"], ["Lucor", "Didier", "", "LIMSI"], ["Pon\u00e7ot", "Ang\u00e9lique", "", "EDF R&D PERICLES"]]}, {"id": "1910.09499", "submitter": "Miaoyan Wang", "authors": "Zhuoyan Xu, Jiaxin Hu, and Miaoyan Wang", "title": "Generalized tensor regression with covariates on multiple modes", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of tensor-response regression given covariates on\nmultiple modes. Such data problems arise frequently in applications such as\nneuroimaging, network analysis, and spatial-temporal modeling. We propose a new\nfamily of tensor response regression models that incorporate covariates, and\nestablish the theoretical accuracy guarantees. Unlike earlier methods, our\nestimation allows high-dimensionality in both the tensor response and the\ncovariate matrices on multiple modes. An efficient alternating updating\nalgorithm is further developed. Our proposal handles a broad range of data\ntypes, including continuous, count, and binary observations. Through simulation\nand applications to two real datasets, we demonstrate the outperformance of our\napproach over the state-of-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 16:43:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Zhuoyan", ""], ["Hu", "Jiaxin", ""], ["Wang", "Miaoyan", ""]]}, {"id": "1910.09628", "submitter": "Jiarui Lu", "authors": "Jiarui Lu and Hongzhe Li", "title": "Hypothesis Testing in High-Dimensional Instrumental Variables Regression\n  with an Application to Genomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression and phenotype association can be affected by potential\nunmeasured confounders from multiple sources, leading to biased estimates of\nthe associations. Since genetic variants largely explain gene expression\nvariations, they can be used as instruments in studying the association between\ngene expressions and phenotype in the framework of high dimensional\ninstrumental variable (IV) regression. However, because the dimensions of both\ngenetic variants and gene expressions are often larger than the sample size,\nstatistical inferences such as hypothesis testing for such high dimensional IV\nmodels are not trivial and have not been investigated in literature. The\nproblem is more challenging since the instrumental variables (e.g., genetic\nvariants) have to be selected among a large set of genetic variants. This paper\nconsiders the problem of hypothesis testing for sparse IV regression models and\npresents methods for testing single regression coefficient and multiple testing\nof multiple coefficients, where the test statistic for each single coefficient\nis constructed based on an inverse regression. A multiple testing procedure is\ndeveloped for selecting variables and is shown to control the false discovery\nrate. Simulations are conducted to evaluate the performance of our proposed\nmethods. These methods are illustrated by an analysis of a yeast dataset in\norder to identify genes that are associated with growth in the presence of\nhydrogen peroxide.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 19:47:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Lu", "Jiarui", ""], ["Li", "Hongzhe", ""]]}, {"id": "1910.09648", "submitter": "Max Little", "authors": "Max A. Little, Reham Badawy", "title": "Causal bootstrapping", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To draw scientifically meaningful conclusions and build reliable models of\nquantitative phenomena, cause and effect must be taken into consideration\n(either implicitly or explicitly). This is particularly challenging when the\nmeasurements are not from controlled experimental (interventional) settings,\nsince cause and effect can be obscured by spurious, indirect influences. Modern\npredictive techniques from machine learning are capable of capturing\nhigh-dimensional, nonlinear relationships between variables while relying on\nfew parametric or probabilistic model assumptions. However, since these\ntechniques are associational, applied to observational data they are prone to\npicking up spurious influences from non-experimental (observational) data,\nmaking their predictions unreliable. Techniques from causal inference, such as\nprobabilistic causal diagrams and do-calculus, provide powerful (nonparametric)\ntools for drawing causal inferences from such observational data. However,\nthese techniques are often incompatible with modern, nonparametric machine\nlearning algorithms since they typically require explicit probabilistic models.\nHere, we develop causal bootstrapping for augmenting classical nonparametric\nbootstrap resampling with information on the causal relationship between\nvariables. This makes it possible to resample observational data such that, if\nit is possible to identify an interventional relationship from that data, new\ndata representing that relationship can be simulated from the original\nobservational data. In this way, we can use modern machine learning algorithms\nunaltered to make statistically powerful, yet causally-robust, predictions. We\ndevelop several causal bootstrapping algorithms for drawing interventional\ninferences from observational data, for classification and regression problems,\nand demonstrate, using synthetic and real-world examples, the value of this\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:52:56 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 20:51:57 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 00:23:44 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Little", "Max A.", ""], ["Badawy", "Reham", ""]]}, {"id": "1910.09679", "submitter": "Cian Naik", "authors": "Cian Naik, Fran\\c{c}ois Caron and Judith Rousseau", "title": "Sparse Networks with Core-Periphery Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical model for graphs with a core-periphery structure. To\ndo this we define a precise notion of what it means for a graph to have this\nstructure, based on the sparsity properties of the subgraphs of core and\nperiphery nodes. We present a class of sparse graphs with such properties, and\nprovide methods to simulate from this class, and to perform posterior\ninference. We demonstrate that our model can detect core-periphery structure in\nsimulated and real-world networks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 22:21:59 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Naik", "Cian", ""], ["Caron", "Fran\u00e7ois", ""], ["Rousseau", "Judith", ""]]}, {"id": "1910.09695", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Christeen Wijethunga", "title": "Confidence intervals centred on bootstrap smoothed estimators: an\n  impossibility result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Kabaila and Wijethunga assessed the performance of a confidence\ninterval centred on a bootstrap smoothed estimator, with width proportional to\nan estimator of Efron's delta method approximation to the standard deviation of\nthis estimator. They used a testbed situation consisting of two nested linear\nregression models, with error variance assumed known, and model selection using\na preliminary hypothesis test. This assessment was in terms of coverage and\nscaled expected length, where the scaling is with respect to the expected\nlength of the usual confidence interval with the same minimum coverage\nprobability. They found that this confidence interval has scaled expected\nlength that (a) has a maximum value that may be much greater than 1 and (b) is\ngreater than a number slightly less than 1 when the simpler model is correct.\nWe therefore ask the following question. For a confidence interval, centred on\nthe bootstrap smoothed estimator, does there exist a formula for its data-based\nwidth such that, in this testbed situation, it has the desired minimum coverage\nand scaled expected length that (a) has a maximum value that is not too much\nlarger than 1 and (b) is substantially less than 1 when the simpler model is\ncorrect? Using a recent decision-theoretic performance bound due to Kabaila and\nKong, it is shown that the answer to this question is `no' for a wide range of\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:43:08 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kabaila", "Paul", ""], ["Wijethunga", "Christeen", ""]]}, {"id": "1910.09699", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Zhengwu Zhang, David B. Dunson", "title": "Soft Tensor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Statistical methods relating tensor predictors to scalar outcomes in a\nregression model generally vectorize the tensor predictor and estimate the\ncoefficients of its entries employing some form of regularization, use\nsummaries of the tensor covariate, or use a low dimensional approximation of\nthe coefficient tensor. However, low rank approximations of the coefficient\ntensor can suffer if the true rank is not small. We propose a tensor regression\nframework which assumes a soft version of the parallel factors (PARAFAC)\napproximation. In contrast to classic PARAFAC, where each entry of the\ncoefficient tensor is the sum of products of row-specific contributions across\nthe tensor modes, the soft tensor regression (Softer) framework allows the\nrow-specific contributions to vary around an overall mean. We follow a Bayesian\napproach to inference, and show that softening the PARAFAC increases model\nflexibility, leads to improved estimation of coefficient tensors, more accurate\nidentification of important predictor entries, and more precise predictions,\neven for a low approximation rank. From a theoretical perspective, we show that\nemploying Softer leads to a weakly consistent posterior distribution of the\ncoefficient tensor, irrespective of the true or approximation tensor rank, a\nresult that is not true when employing the classic PARAFAC for tensor\nregression. In the context of our motivating application, we adapt Softer to\nsymmetric and semi-symmetric tensor predictors and analyze the relationship\nbetween brain network characteristics and human traits.soft\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 23:56:58 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 08:35:25 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David B.", ""]]}, {"id": "1910.09701", "submitter": "Boxin Zhao", "authors": "Boxin Zhao, Y. Samuel Wang, Mladen Kolar", "title": "Direct Estimation of Differential Functional Graphical Models", "comments": "21 pages, 3 figures, to be published in NeurIPS 2019; added link to\n  code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the difference between two functional\nundirected graphical models with shared structures. In many applications, data\nare naturally regarded as high-dimensional random function vectors rather than\nmultivariate scalars. For example, electroencephalography (EEG) data are more\nappropriately treated as functions of time. In these problems, not only can the\nnumber of functions measured per sample be large, but each function is itself\nan infinite dimensional object, making estimation of model parameters\nchallenging. We develop a method that directly estimates the difference of\ngraphs, avoiding separate estimation of each graph, and show it is consistent\nin certain high-dimensional settings. We illustrate finite sample properties of\nour method through simulation studies. Finally, we apply our method to EEG data\nto uncover differences in functional brain connectivity between alcoholics and\ncontrol subjects.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 00:05:44 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 16:53:38 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhao", "Boxin", ""], ["Wang", "Y. Samuel", ""], ["Kolar", "Mladen", ""]]}, {"id": "1910.09851", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov", "title": "Orthogonal variance decomposition based feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing feature selection methods fail to properly account for interactions\nbetween features when evaluating feature subsets. In this paper, we attempt to\nremedy this issue by using orthogonal variance decomposition to evaluate\nfeatures. The orthogonality of the decomposition allows us to directly\ncalculate the total contribution of a feature to the output variance. Thus we\nobtain an efficient algorithm for feature evaluation which takes into account\ninteractions among features. Numerical experiments demonstrate that our method\naccurately identifies relevant features and improves the accuracy of numerical\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 09:18:39 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Kamalov", "Firuz", ""]]}, {"id": "1910.10078", "submitter": "Brook Luers", "authors": "Brook Luers and Min Qian and Inbal Nahum-Shani and Connie Kasari and\n  Daniel Almirall", "title": "Linear Mixed Models for Comparing Dynamic Treatment Regimens on a\n  Longitudinal Outcome in Sequentially Randomized Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic treatment regimen (DTR) is a pre-specified sequence of decision\nrules which maps baseline or time-varying measurements on an individual to a\nrecommended intervention or set of interventions. Sequential multiple\nassignment randomized trials (SMARTs) represent an important data collection\ntool for informing the construction of effective DTRs. A common primary aim in\na SMART is the marginal mean comparison between two or more of the DTRs\nembedded in the trial. This manuscript develops a mixed effects modeling and\nestimation approach for these primary aim comparisons based on a continuous,\nlongitudinal outcome. The method is illustrated using data from a SMART in\nautism research.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:17:54 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Luers", "Brook", ""], ["Qian", "Min", ""], ["Nahum-Shani", "Inbal", ""], ["Kasari", "Connie", ""], ["Almirall", "Daniel", ""]]}, {"id": "1910.10102", "submitter": "Tianying Wang", "authors": "Tianying Wang, Iuliana Ionita-Laza and Ying Wei", "title": "Integrated Quantile RAnk Test (iQRAT) for gene-level associations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-based testing is a commonly employed strategy in many genetic\nassociation studies. Gene-trait associations can be complex due to underlying\npopulation heterogeneity, gene-environment interactions, and various other\nreasons. Existing gene-based tests, such as Burden and Sequence Kernel\nAssociation Tests (SKAT), are based on detecting differences in a single\nsummary statistic, such as the mean or the variance, and may miss or\nunderestimate higher-order associations that could be scientifically\ninteresting. In this paper, we propose a new family of gene-level association\ntests which integrate quantile rank score processes to better accommodate\ncomplex associations. The resulting test statistics have multiple advantages:\n(1) they are almost as efficient as the best existing tests when the\nassociations are homogeneous across quantile levels, and have improved\nefficiency for complex and heterogeneous associations, (2) they provide useful\ninsights on risk stratification, (3) the test statistics are distribution-free,\nand could hence accommodate a wide range of underlying distributions, and (4)\nthey are computationally efficient. We established the asymptotic properties of\nthe proposed tests under the null and alternative hypothesis and conducted\nlarge scale simulation studies to investigate their finite sample performance.\nWe applied the proposed tests to the Metabochip data to identify genetic\nassociations with lipid traits and compared the results with those of the\nBurden and SKAT tests.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:41:15 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 23:59:28 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 08:56:22 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wang", "Tianying", ""], ["Ionita-Laza", "Iuliana", ""], ["Wei", "Ying", ""]]}, {"id": "1910.10133", "submitter": "Arthur Charpentier", "authors": "Charpentier, Arthur and Mussard, Stephane and Tea Ouraga", "title": "Principal Component Analysis: A Generalized Gini Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A principal component analysis based on the generalized Gini correlation\nindex is proposed (Gini PCA). The Gini PCA generalizes the standard PCA based\non the variance. It is shown, in the Gaussian case, that the standard PCA is\nequivalent to the Gini PCA. It is also proven that the dimensionality reduction\nbased on the generalized Gini correlation matrix, that relies on city-block\ndistances, is robust to outliers. Monte Carlo simulations and an application on\ncars data (with outliers) show the robustness of the Gini PCA and provide\ndifferent interpretations of the results compared with the variance PCA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 17:32:33 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Charpentier", "", ""], ["Arthur", "", ""], ["Mussard", "", ""], ["Stephane", "", ""], ["Ouraga", "Tea", ""]]}, {"id": "1910.10233", "submitter": "Flavio Goncalves", "authors": "Fl\\'avio B. Gon\\c{c}alves and Juliane Venturelli and Rosangela H.\n  Loschi", "title": "Flexible Bayesian modelling in dichotomous item response theory using\n  mixtures of skewed item curves", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Item Response Theory (IRT) models for dichotomous responses are based on\nprobit or logit link functions which assume a symmetric relationship between\nthe responses and the latent traits of individuals submitted to a test. Such an\nassumption restricts the use of such models to situations in which all items\nhave symmetric behavior. Similar constraint is imposed by the asymmetric models\nproposed in the literature as it is required that all items have an asymmetric\nbehavior. Such assumptions are inappropriate for great part of the tests which,\nin general, are composed by both symmetric and asymmetric items. Furthermore, a\nstraightforward extension of the existing models in the literature of would\nrequire a prior selection of the items' symmetry/asymmetry status. This paper\nproposes a Bayesian IRT model that accounts for symmetric and asymmetric items\nin a flexible though parsimonious way. That is achieved by assigning a\npoint-mass mixture prior to the skewness parameter of the item, allowing for an\nanalysis under the model selection or model averaging approaches. Asymmetric\nitem curves are design through the centred skew normal distribution which has a\nparticularly appealing parametrisation in terms of parameter interpretation and\ncomputational efficiency. An efficient MCMC algorithm is proposed to perform\nBayesian inference and its performance is investigated in some simulated\nexamples. Finally, the proposed methodology is applied to a data set from a\nlarge scale educational exam in Brazil.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 21:13:17 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 15:45:26 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B.", ""], ["Venturelli", "Juliane", ""], ["Loschi", "Rosangela H.", ""]]}, {"id": "1910.10426", "submitter": "Linas Petkevicius", "authors": "Vilijandas Bagdonavicius, Linas Petkevicius", "title": "Multiple outlier detection tests for parametric models", "comments": null, "journal-ref": "Mathematics 8 (2020) 2156", "doi": "10.3390/math8122156", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple multiple outlier identification method for parametric\nlocation-scale and shape-scale models when the number of possible outliers is\nnot specified. The method is based on a result giving asymptotic properties of\nextreme z-scores. Robust estimators of model parameters are used defining\nz-scores. An extensive simulation study was done for comparing of the proposed\nmethod with existing methods. For the normal family, the method is compared\nwith the well known Davies-Gather, Rosner's, Hawking's and Bolshev's multiple\noutlier identification methods. The choice of an upper limit for the number of\npossible outliers in case of Rosner's test application is discussed. For other\nfamilies, the proposed method is compared with a method generalizing\nGather-Davies method. In most situations, the new method has the highest\noutlier identification power in terms of masking and swamping values. We also\ncreated R package outliersTests for proposed test.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:23:21 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bagdonavicius", "Vilijandas", ""], ["Petkevicius", "Linas", ""]]}, {"id": "1910.10443", "submitter": "Lifeng Ye", "authors": "Maria De Iorio, Stefano Favaro, Alessandra Guglielmi, and Lifeng Ye", "title": "Bayesian nonparametric temporal dynamic clustering via autoregressive\n  Dirichlet priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of dynamic clustering, where cluster\nmemberships may change over time and clusters may split and merge over time,\nthus creating new clusters and destroying existing ones. We propose a Bayesian\nnonparametric approach to dynamic clustering via mixture modeling. Our approach\nrelies on a novel time-dependent nonparametric prior defined by combining: i) a\ncopula-based transformation of a Gaussian autoregressive process; ii) the\nstick-breaking construction of the Dirichlet process. Posterior inference is\nperformed through a particle Markov chain Monte Carlo algorithm which is\nsimple, computationally efficient and scalable to massive datasets. Advantages\nof the proposed approach include flexibility in applications, ease of\ncomputations and interpretability. We present an application of our dynamic\nBayesian nonparametric mixture model to the study the temporal dynamics of\ngender stereotypes in adjectives and occupations in the 20th and 21st centuries\nin the United States. Moreover, to highlight the flexibility of our model we\npresent additional applications to time-dependent data with covariates and with\nspatial structure.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 10:15:26 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["De Iorio", "Maria", ""], ["Favaro", "Stefano", ""], ["Guglielmi", "Alessandra", ""], ["Ye", "Lifeng", ""]]}, {"id": "1910.10484", "submitter": "Carl Nordlund", "authors": "Carl Nordlund", "title": "Direct blockmodeling of valued and binary networks: a\n  dichotomization-free approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.socnet.2019.10.004", "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing open problem with direct blockmodeling is that it is\nexplicitly intended for binary, not valued, networks. The underlying dilemma is\nhow empirical valued blocks can be compared with ideal binary blocks, an\nintrinsic problem in the direct approach where partitions are solely determined\nthrough such comparisons. Addressing this dilemma, valued networks have either\nbeen dichotomized into binary versions, or novel types of ideal valued blocks\nhave been introduced. Both these workarounds are problematic in terms of\ninterpretability, unwanted data reduction, and the often arbitrary setting of\nmodel parameters.\n  This paper proposes a direct blockmodeling approach that effectively bypasses\nthe dilemma with blockmodeling of valued networks. By introducing an adaptive\nweighted correlation-based criteria function, the proposed approach is directly\napplicable to both binary and valued networks, without any form of\ndichotomization or transformation of the valued (or binary) data at any point\nin the analysis, while still using the conventional set of ideal binary blocks\nfrom structural, regular and generalized blockmodeling.\n  The approach is demonstrated by structural, regular and generalized\nblockmodeling applications of six classical binary and valued networks. Finding\nfeasible and intuitive optimal solutions in both the binary and valued\nexamples, the approach is proposed not only as a practical,\ndichotomization-free heuristic for blockmodeling of valued networks but also,\nthrough its additional benefits, as an alternative to the conventional direct\napproach to blockmodeling.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:30:32 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Nordlund", "Carl", ""]]}, {"id": "1910.10512", "submitter": "Saint-Clair Chabert-Liddell", "authors": "Saint-Clair Chabert-Liddell and Pierre Barbillon and Sophie Donnet and\n  Emmanuel Lazega", "title": "A Stochastic Block Model Approach for the Analysis of Multilevel\n  Networks: an Application to the Sociology of Organizations", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2021.107179", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multilevel network is defined as the junction of two interaction networks,\none level representing the interactions between individuals and the other the\ninteractions between organizations. The levels are linked by an affiliation\nrelationship, each individual belonging to a unique organization. A new\nStochastic Block Model is proposed as a unified probalistic framework tailored\nfor multilevel networks. This model contains latent blocks accounting for\nheterogeneity in the patterns of connection within each level and introducing\ndependencies between the levels. The sought connection patterns are not\nspecified a priori which makes this approach flexible. Variational methods are\nused for the model inference and an Integrated Classified Likelihood criterion\nis developed for choosing the number of blocks and also for deciding whether\nthe two levels are dependent or not. A comprehensive simulation study exhibits\nthe benefit of considering this approach, illustrates the robustness of the\nclustering and highlights the reliability of the criterion used for model\nselection. This approach is applied on a sociological dataset collected during\na television program trade fair, the inter-organizational level being the\neconomic network between companies and the inter-individual level being the\ninformal network between their representatives. It brings a synthetic\nrepresentation of the two networks unraveling their intertwined structure and\nconfirms the coopetition at stake.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 12:08:41 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:28:16 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 14:33:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chabert-Liddell", "Saint-Clair", ""], ["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""], ["Lazega", "Emmanuel", ""]]}, {"id": "1910.10562", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Arun K. Kuchibhotla, Aaditya K. Ramdas", "title": "Nested conformal prediction and quantile out-of-bag ensemble methods", "comments": "38 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a popular tool for providing valid prediction sets\nfor classification and regression problems, without relying on any\ndistributional assumptions on the data. While the traditional description of\nconformal prediction starts with a nonconformity score, we provide an alternate\n(but equivalent) view that starts with a sequence of nested sets and calibrates\nthem to find a valid prediction set. The nested framework subsumes all\nnonconformity scores, including recent proposals based on quantile regression\nand density estimation. While these ideas were originally derived based on\nsample splitting, our framework seamlessly extends them to other aggregation\nschemes like cross-conformal, jackknife+ and out-of-bag methods. We use the\nframework to derive a new algorithm (QOOB, pronounced cube) that combines four\nideas: quantile regression, cross-conformalization, ensemble methods and\nout-of-bag predictions. We develop a computationally efficient implementation\nof cross-conformal, that is also used by QOOB. In a detailed numerical\ninvestigation, QOOB performs either the best or close to the best on all\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 13:44:38 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:26:16 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 03:29:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Kuchibhotla", "Arun K.", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "1910.10589", "submitter": "Guilherme Pumi", "authors": "Taiane Schaedler Prass and Guilherme Pumi", "title": "On the behavior of the DFA and DCCA in trend-stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop the asymptotic theory of the Detrended Fluctuation\nAnalysis (DFA) and Detrended Cross-Correlation Analysis (DCCA) for\ntrend-stationary stochastic processes without any assumption on the specific\nform of the underlying distribution. All results are presented and derived\nunder the general framework of potentially overlapping boxes for the polynomial\nfit. We prove the stationarity of the DFA and DCCA, viewed as stochastic\nprocesses, obtain closed forms for moments up to second order, including the\ncovariance structure for DFA and DCCA and a miscellany of law of large number\nrelated results. Our results generalize and improve several results presented\nin the literature. To verify the behavior of our theoretical results in small\nsamples, we present a Monte Carlo simulation study and an empirical application\nto econometric time series.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:43:03 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 20:53:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Prass", "Taiane Schaedler", ""], ["Pumi", "Guilherme", ""]]}, {"id": "1910.10624", "submitter": "Imke Mayer", "authors": "Imke Mayer, Erik Sverdrup, Tobias Gauss, Jean-Denis Moyer, Stefan\n  Wager and Julie Josse", "title": "Doubly robust treatment effect estimation with missing attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing attributes are ubiquitous in causal inference, as they are in most\napplied statistical work. In this paper, we consider various sets of\nassumptions under which causal inference is possible despite missing attributes\nand discuss corresponding approaches to average treatment effect estimation,\nincluding generalized propensity score methods and multiple imputation. Across\nan extensive simulation study, we show that no single method systematically\nout-performs others. We find, however, that doubly robust modifications of\nstandard methods for average treatment effect estimation with missing data\nrepeatedly perform better than their non-doubly robust baselines; for example,\ndoubly robust generalized propensity score methods beat inverse-weighting with\nthe generalized propensity score. This finding is reinforced in an analysis of\nan observations study on the effect on mortality of tranexamic acid\nadministration among patients with traumatic brain injury in the context of\ncritical care management. Here, doubly robust estimators recover confidence\nintervals that are consistent with evidence from randomized trials, whereas\nnon-doubly robust estimators do not.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 15:46:43 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 14:03:21 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mayer", "Imke", ""], ["Sverdrup", "Erik", ""], ["Gauss", "Tobias", ""], ["Moyer", "Jean-Denis", ""], ["Wager", "Stefan", ""], ["Josse", "Julie", ""]]}, {"id": "1910.10791", "submitter": "Wei Deng", "authors": "Wei Deng, Xiao Zhang, Faming Liang, Guang Lin", "title": "An Adaptive Empirical Bayesian Method for Sparse Deep Learning", "comments": "Accepted by NeurIPS 2019; Update the assumption on the regularity of\n  Poisson equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive empirical Bayesian method for sparse deep\nlearning, where the sparsity is ensured via a class of self-adaptive\nspike-and-slab priors. The proposed method works by alternatively sampling from\nan adaptive hierarchical posterior distribution using stochastic gradient\nMarkov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters\nusing stochastic approximation (SA). We further prove the convergence of the\nproposed method to the asymptotically correct distribution under mild\nconditions. Empirical applications of the proposed method lead to the\nstate-of-the-art performance on MNIST and Fashion MNIST with shallow\nconvolutional neural networks and the state-of-the-art compression performance\non CIFAR10 with Residual Networks. The proposed method also improves resistance\nto adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:05:57 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 18:57:23 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Deng", "Wei", ""], ["Zhang", "Xiao", ""], ["Liang", "Faming", ""], ["Lin", "Guang", ""]]}, {"id": "1910.10862", "submitter": "David Puelz", "authors": "David Puelz, Guillaume Basse, Avi Feller, Panos Toulis", "title": "A Graph-Theoretic Approach to Randomization Tests of Causal Effects\n  Under General Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference exists when a unit's outcome depends on another unit's treatment\nassignment. For example, intensive policing on one street could have a\nspillover effect on neighboring streets. Classical randomization tests\ntypically break down in this setting because many null hypotheses of interest\nare no longer sharp under interference. A promising alternative is to instead\nconstruct a conditional randomization test on a subset of units and assignments\nfor which a given null hypothesis is sharp. Finding these subsets is\nchallenging, however, and existing methods are limited to special cases or have\nlimited power. In this paper, we propose valid and easy-to-implement\nrandomization tests for a general class of null hypotheses under arbitrary\ninterference between units. Our key idea is to represent the hypothesis of\ninterest as a bipartite graph between units and assignments, and to find an\nappropriate biclique of this graph. Importantly, the null hypothesis is sharp\nwithin this biclique, enabling conditional randomization-based tests. We also\nconnect the size of the biclique to statistical power. Moreover, we can apply\noff-the-shelf graph clustering methods to find such bicliques efficiently and\nat scale. We illustrate our approach in settings with clustered interference\nand show advantages over methods designed specifically for that setting. We\nthen apply our method to a large-scale policing experiment in Medellin,\nColombia, where interference has a spatial structure.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:55:57 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:50:24 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 16:02:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Puelz", "David", ""], ["Basse", "Guillaume", ""], ["Feller", "Avi", ""], ["Toulis", "Panos", ""]]}, {"id": "1910.10898", "submitter": "Abdul-Nasah Soale", "authors": "Abdul-Nasah Soale, Yuexiao Dong", "title": "On expectile-assisted inverse regression estimation for sufficient\n  dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moment-based sufficient dimension reduction methods such as sliced inverse\nregression may not work well in the presence of heteroscedasticity. We propose\nto first estimate the expectiles through kernel expectile regression, and then\ncarry out dimension reduction based on random projections of the regression\nexpectiles. Several popular inverse regression methods in the literature are\nextended under this general framework. The proposed expectile-assisted methods\noutperform existing moment-based dimension reduction methods in both numerical\nstudies and an analysis of the Big Mac data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 03:22:18 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:13:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Soale", "Abdul-Nasah", ""], ["Dong", "Yuexiao", ""]]}, {"id": "1910.10993", "submitter": "Behnaz Pirzamanbein PhD", "authors": "Behnaz Pirzamanbein and Johan Lindstr\\\"om", "title": "Reconstruction of Past Human land-use from Pollen Data and Anthropogenic\n  land-cover Changes Scenarios", "comments": "5 Human land-use maps of Europe (1900 CE, 1725 CE, 1425 CE, 1000 BCE\n  and, 4000 BCE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate maps of past land cover and human land-use are necessary when\nstudying the impact of anthropogenic land-cover changes on climate. Ideally the\nmaps of past land cover would be separated into naturally occurring vegetation\nand human induced changes, allowing us to quantify the effect of human land-use\non past climate. Here we investigate the possibility of combining regional,\nfossil pollen based, land-cover reconstructions with, population based,\nestimates of past human land-use. By merging these two datasets and\ninterpolating the pollen based land-cover reconstructions we aim at obtaining\nmaps that provide both past natural land-cover and the anthropogenic land-cover\nchanges. We develop a Bayesian hierarchical model to handle the complex data,\nusing a latent Gaussian Markov random fields (GMRF) for the interpolation.\nEstimation of the model is based on a block updated Markov chain Monte Carlo\n(MCMC) algorithm. The sparse precision matrix of the GMRF together with an\nadaptive Metropolis adjusted Langevin step allows for fast inference.\nUncertainties in the land-use predictions are computed from the MCMC posterior\nsamples. The model uses the pollen based observations to reconstruct three\ncomposition of land cover; Coniferous forest, Broadleaved forest and\nUnforested/Open land. The unforested land is then further decomposed into\nnatural and human induced openness by inclusion of the estimates of past human\nland-use. The model is applied to five time periods - centred around 1900 CE,\n1725 CE, 1425 CE, 1000 and, 4000 BCE over Europe. The results suggest pollen\nbased observations can be used to recover past human land-use by adjusting the\npopulation based anthropogenic land-cover changes estimates.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:32:49 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pirzamanbein", "Behnaz", ""], ["Lindstr\u00f6m", "Johan", ""]]}, {"id": "1910.11044", "submitter": "Josue Orellana", "authors": "Natalie Klein, Josue Orellana, Scott Brincat, Earl K. Miller, and\n  Robert E. Kass", "title": "Torus Graphs for Multivariate Phase Coupling Analysis", "comments": "N.K. and J.O. contributed equally to this work. Peer reviewed\n  version, in press at The Annals of Applied Statistics. 10 main Figures,\n  supplementary text appended with 11 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angular measurements are often modeled as circular random variables, where\nthere are natural circular analogues of moments, including correlation. Because\na product of circles is a torus, a d-dimensional vector of circular random\nvariables lies on a d-dimensional torus. For such vectors we present here a\nclass of graphical models, which we call torus graphs, based on the full\nexponential family with pairwise interactions. The topological distinction\nbetween a torus and Euclidean space has several important consequences.\n  Our development was motivated by the problem of identifying phase coupling\namong oscillatory signals recorded from multiple electrodes in the brain:\noscillatory phases across electrodes might tend to advance or recede together,\nindicating coordination across brain areas. The data analyzed here consisted of\n24 phase angles measured repeatedly across 840 experimental trials\n(replications) during a memory task, where the electrodes were in 4 distinct\nbrain regions, all known to be active while memories are being stored or\nretrieved. In realistic numerical simulations, we found that a standard\npairwise assessment, known as phase locking value, is unable to describe\nmultivariate phase interactions, but that torus graphs can accurately identify\nconditional associations. Torus graphs generalize several more restrictive\napproaches that have appeared in various scientific literatures, and produced\nintuitive results in the data we analyzed. Torus graphs thus unify multivariate\nanalysis of circular data and present fertile territory for future research.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 12:10:15 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Klein", "Natalie", ""], ["Orellana", "Josue", ""], ["Brincat", "Scott", ""], ["Miller", "Earl K.", ""], ["Kass", "Robert E.", ""]]}, {"id": "1910.11219", "submitter": "Onur Teymur", "authors": "Onur Teymur and Sarah Filippi", "title": "A Bayesian nonparametric test for conditional independence", "comments": null, "journal-ref": "Foundations of Data Science (2020) 2(2):155-172", "doi": "10.3934/fods.2020009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a Bayesian nonparametric method for quantifying the\nrelative evidence in a dataset in favour of the dependence or independence of\ntwo variables conditional on a third. The approach uses Polya tree priors on\nspaces of conditional probability densities, accounting for uncertainty in the\nform of the underlying distributions in a nonparametric way. The Bayesian\nperspective provides an inherently symmetric probability measure of conditional\ndependence or independence, a feature particularly advantageous in causal\ndiscovery and not employed in existing procedures of this type.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:23:49 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:33:53 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Teymur", "Onur", ""], ["Filippi", "Sarah", ""]]}, {"id": "1910.11251", "submitter": "James Hare", "authors": "James Z. Hare, Cesar Uribe, Lance Kaplan, Ali Jadbabaie", "title": "Non-Bayesian Social Learning with Gaussian Uncertain Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Bayesian social learning theory provides a framework for distributed\ninference of a group of agents interacting over a social network by\nsequentially communicating and updating beliefs about the unknown state of the\nworld through likelihood updates from their observations. Typically, likelihood\nmodels are assumed known precisely. However, in many situations the models are\ngenerated from sparse training data due to lack of data availability, high cost\nof collection/calibration, limits within the communications network, and/or the\nhigh dynamics of the operational environment. Recently, social learning theory\nwas extended to handle those model uncertainties for categorical models. In\nthis paper, we introduce the theory of Gaussian uncertain models and study the\nproperties of the beliefs generated by the network of agents. We show that even\nwith finite amounts of training data, non-Bayesian social learning can be\nachieved and all agents in the network will converge to a consensus belief that\nprovably identifies the best estimate for the state of the world given the set\nof prior information.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:51:06 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hare", "James Z.", ""], ["Uribe", "Cesar", ""], ["Kaplan", "Lance", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1910.11258", "submitter": "Xin Wang", "authors": "Xin Wang", "title": "Clustering of longitudinal curves via a penalized method and EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm is proposed for clustering longitudinal curves. The mean\ncurves and the principal component functions are modeled using B-spline. The\nclusters of the mean curves are identified through a concave pairwise fusion\nmethod. The EM algorithm and the alternating direction method of multiplier\nalgorithm are combined to estimate the group structure, mean functions and the\nprincipal components simultaneously. The proposed method also allows to\nincorporate the prior neighborhood information to have more meaningful groups\nby adding pairwise weights in the pairwise penalties. In the simulation study,\nthe performance of the proposed method is compared to two existing clustering\nmethods in terms of the accuracy for estimating the number of subgroups and\nmean functions. The results suggest that ignoring covariance structure will\nhave a great effect on the performance of estimating the number of groups and\nestimating accuracy. The effect of including pairwise weights is also explored\nin a spatial lattice setting to take consideration of the spatial information.\nThe results show that incorporating spatial weights will improve the\nperformance. An example is used to illustrate the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:58:41 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 03:40:27 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Xin", ""]]}, {"id": "1910.11397", "submitter": "Stephen Lauer", "authors": "Stephen A. Lauer, Nicholas G. Reich, Laura B. Balzer", "title": "The covariate-adjusted residual estimator and its use in both randomized\n  trials and observational settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We often seek to estimate the causal effect of an exposure on a particular\noutcome in both randomized and observational settings. One such estimation\nmethod is the covariate-adjusted residuals estimator, which was designed for\nindividually or cluster randomized trials. In this manuscript, we study the\nproperties of this estimator and develop a new estimator that utilizes both\ncovariate adjustment and inverse probability weighting We support our\ntheoretical results with a simulation study and an application in an infectious\ndisease setting. The covariate-adjusted residuals estimator is an efficient and\nunbiased estimator of the average treatment effect in randomized trials;\nhowever, it is not guaranteed to be unbiased in observational studies. Our\nnovel estimator, the covariate-adjusted residuals estimator with inverse\nprobability weighting, is unbiased in randomized and observational settings,\nunder a reasonable set of assumptions. Furthermore, when these assumptions\nhold, it provides efficiency gains over inverse probability weighting in\nobservational studies. The covariate-adjusted residuals estimator is valid for\nuse in randomized trials, but should not be used in observational studies. The\ncovariate-adjusted residuals estimator with inverse probability weighting\nprovides an efficient alternative for use in randomized and observational\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 19:51:37 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Lauer", "Stephen A.", ""], ["Reich", "Nicholas G.", ""], ["Balzer", "Laura B.", ""]]}, {"id": "1910.11445", "submitter": "Fan Yin", "authors": "Fan Yin, Weining Shen, Carter T. Butts", "title": "Finite Mixtures of ERGMs for Modeling Ensembles of Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of networks arise in many scientific fields, but there are few\nstatistical tools for inferring their generative processes, particularly in the\npresence of both dyadic dependence and cross-graph heterogeneity. To fill in\nthis gap, we propose characterizing network ensembles via finite mixtures of\nexponential family random graph models, a framework for parametric statistical\nmodeling of graphs that has been successful in explicitly modeling the complex\nstochastic processes that govern the structure of edges in a network. Our\nproposed modeling framework can also be used for applications such as\nmodel-based clustering of ensembles of networks and density estimation for\ncomplex graph distributions. We develop a Metropolis-within-Gibbs algorithm to\nconduct fully Bayesian inference and adapt a version of deviance information\ncriterion for missing data models to choose the number of latent heterogeneous\ngenerative mechanisms. Simulation studies show that the proposed procedure can\nrecover the true number of latent heterogeneous generative processes and\ncorresponding parameters. We demonstrate the utility of the proposed approach\nusing an ensemble of political co-voting networks among U.S. Senators.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 22:37:10 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 05:07:52 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 04:14:20 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Yin", "Fan", ""], ["Shen", "Weining", ""], ["Butts", "Carter T.", ""]]}, {"id": "1910.11479", "submitter": "Haim Bar", "authors": "Haim Y. Bar and James G. Booth and Martin T. Wells", "title": "Quantile Regression Modelling via Location and Scale Mixtures of Normal\n  Distributions", "comments": "a newer version of the paper which focuses on large-P variable\n  selection is available in arXiv:2104.08595", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the estimating equations for quantile regression can be solved\nusing a simple EM algorithm in which the M-step is computed via weighted least\nsquares, with weights computed at the E-step as the expectation of independent\ngeneralized inverse-Gaussian variables. We compute the variance-covariance\nmatrix for the quantile regression coefficients using a kernel density\nestimator that results in more stable standard errors than those produced by\nexisting software. A natural modification of the EM algorithm that involves\nfitting a linear mixed model at the M-step extends the methodology to mixed\neffects quantile regression models. In this case, the fitting method can be\njustified as a generalized alternating minimization algorithm. Obtaining\nquantile regression estimates via the weighted least squares method enables\nmodel diagnostic techniques similar to the ones used in the linear regression\nsetting. The computational approach is compared with existing software using\nsimulated data, and the methodology is illustrated with several case studies.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 01:15:31 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:13:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim Y.", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1910.11537", "submitter": "Fei Li", "authors": "Fei Li, Xuewei Wang, Qiang Lin, Zhenghui Hu", "title": "Unified model selection approach based on minimum description length\n  principle in Granger causality analysis", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2987033", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality analysis (GCA) provides a powerful tool for uncovering the\npatterns of brain connectivity mechanism using neuroimaging techniques.\nConventional GCA applies two different mathematical theories in a two-stage\nscheme: (1) the Bayesian information criterion (BIC) or Akaike information\ncriterion (AIC) for the regression model orders associated with endogenous and\nexogenous information; (2) F-statistics for determining the causal effects of\nexogenous variables. While specifying endogenous and exogenous effects are\nessentially the same model selection problem, this could produce different\nbenchmarks in the two stages and therefore degrade the performance of GCA. In\nthis course, we present a unified model selection approach based on the minimum\ndescription length (MDL) principle for GCA in the context of the general\nregression model paradigm. Compared with conventional methods, our approach\nemphasize that a single mathematical theory should be held throughout the GCA\nprocess. Under this framework, all candidate models within the model space\nmight be compared freely in the context of the code length, without the need\nfor an intermediate model. We illustrate its advantages over conventional\ntwo-stage GCA approach in a 3-node network and a 5-node network synthetic\nexperiments. The unified model selection approach is capable of identifying the\nactual connectivity while avoiding the false influences of noise. More\nimportantly, the proposed approach obtained more consistent results in a\nchallenge fMRI dataset for causality investigation, mental calculation network\nunder visual and auditory stimulus, respectively. The proposed approach has\npotential to accommodate other Granger causality representations in other\nfunction space. The comparison between different GC representations in\ndifferent function spaces can also be naturally deal with in the framework.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 05:42:23 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 16:12:10 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Li", "Fei", ""], ["Wang", "Xuewei", ""], ["Lin", "Qiang", ""], ["Hu", "Zhenghui", ""]]}, {"id": "1910.11575", "submitter": "Pierre Neuvial", "authors": "Gilles Blanchard (LMO), Pierre Neuvial (IMT), Etienne Roquain (LPSM\n  UMR 8001)", "title": "On agnostic post hoc approaches to false positive control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is a book chapter which gives a partial survey on post hoc\napproaches to false positive control.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 08:51:23 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Blanchard", "Gilles", "", "LMO"], ["Neuvial", "Pierre", "", "IMT"], ["Roquain", "Etienne", "", "LPSM\n  UMR 8001"]]}, {"id": "1910.11720", "submitter": "Stefan Engblom", "authors": "Stefan Engblom and Robin Eriksson and Stefan Widgren", "title": "Bayesian epidemiological modeling over high-resolution network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical epidemiological models have a broad use, including both\nqualitative and quantitative applications. With the increasing availability of\ndata, large-scale quantitative disease spread models can nowadays be\nformulated. Such models have a great potential, e.g., in risk assessments in\npublic health. Their main challenge is model parameterization given\nsurveillance data, a problem which often limits their practical usage.\n  We offer a solution to this problem by developing a Bayesian methodology\nsuitable to epidemiological models driven by network data. The greatest\ndifficulty in obtaining a concentrated parameter posterior is the quality of\nsurveillance data; disease measurements are often scarce and carry little\ninformation about the parameters. The often overlooked problem of the model's\nidentifiability therefore needs to be addressed, and we do so using a hierarchy\nof increasingly realistic known truth experiments.\n  Our proposed Bayesian approach performs convincingly across all our synthetic\ntests. From pathogen measurements of shiga toxin-producing Escherichia coli\nO157 in Swedish cattle, we are able to produce an accurate statistical model of\nfirst-principles confronted with data. Within this model we explore the\npotential of a Bayesian public health framework by assessing the efficiency of\ndisease detection and -intervention scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:36:08 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 12:06:22 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Engblom", "Stefan", ""], ["Eriksson", "Robin", ""], ["Widgren", "Stefan", ""]]}, {"id": "1910.11972", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Kernel Optimal Orthogonality Weighting: A Balancing Approach to\n  Estimating Effects of Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific questions require estimating the effects of continuous\ntreatments. Outcome modeling and weighted regression based on the generalized\npropensity score are the most commonly used methods to evaluate continuous\neffects. However, these techniques may be sensitive to model misspecification,\nextreme weights or both. In this paper, we propose Kernel Optimal Orthogonality\nWeighting (KOOW), a convex optimization-based method, for estimating the\neffects of continuous treatments. KOOW finds weights that minimize the\nworst-case penalized functional covariance between the continuous treatment and\nthe confounders. By minimizing this quantity, KOOW successfully provides\nweights that orthogonalize confounders and the continuous treatment, thus\nproviding optimal covariate balance, while controlling for extreme weights. We\nvaluate its comparative performance in a simulation study. Using data from the\nWomen's Health Initiative observational study, we apply KOOW to evaluate the\neffect of red meat consumption on blood pressure.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 01:03:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1910.11985", "submitter": "Tianchen Xu", "authors": "Tianchen Xu, Ryan T. Demmer, Gen Li", "title": "Zero-inflated Poisson Factor Model with Application to Microbiome\n  Absolute Abundance Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction of high-dimensional microbiome data facilitates\nsubsequent analysis such as regression and clustering. Most existing reduction\nmethods cannot fully accommodate the special features of the data such as\ncount-valued and excessive zero reads. We propose a zero-inflated Poisson\nfactor analysis (ZIPFA) model in this article. The model assumes that\nmicrobiome absolute abundance data follow zero-inflated Poisson distributions\nwith library size as offset and Poisson rates negatively related to the\ninflated zero occurrences. The latent parameters of the model form a low-rank\nmatrix consisting of interpretable loadings and low-dimensional scores which\ncan be used for further analyses. We develop an efficient and robust\nexpectation-maximization (EM) algorithm for parameter estimation. We\ndemonstrate the efficacy of the proposed method using comprehensive simulation\nstudies. The application to the Oral Infections, Glucose Intolerance and\nInsulin Resistance Study (ORIGINS) provides valuable insights into the relation\nbetween subgingival microbiome and periodontal disease.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 03:19:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xu", "Tianchen", ""], ["Demmer", "Ryan T.", ""], ["Li", "Gen", ""]]}, {"id": "1910.11991", "submitter": "Prosenjit Kundu", "authors": "Prosenjit Kundu and Nilanjan Chatterjee", "title": "Analysis of Two-Phase Studies using Generalized Method of Moments", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-phase design can reduce the cost of epidemiological studies by limiting\nthe ascertainment of expensive covariates or/and exposures to an efficiently\nselected subset (phase-II) of a larger (phase-I) study. Efficient analysis of\nthe resulting dataset combining disparate information from phase-I and\nphase-II, however, can be complex. Most of the existing methods including\nsemiparametric maximum-likelihood estimator, require the information in phase-I\nto be summarized into a fixed number of strata. In this paper, we describe a\nnovel method for analysis of two-phase studies where information from phase-I\nis summarized by parameters associated with a reduced logistic regression model\nof the disease outcome on available covariates. We then setup estimating\nequations for parameters associated with the desired extended logistic\nregression model, based on information on the reduced model parameters from\nphase-I and complete data available at phase-II after accounting for non-random\nsampling design at phase-II. We use the generalized method of moments to solve\noverly identified estimating equations and develop the resulting asymptotic\ntheory for the proposed estimator. Simulation studies show that the use of\nreduced parametric models, as opposed to summarizing data into strata, can lead\nto more efficient utilization of phase-I data. An application of the proposed\nmethod is illustrated using the US National Wilms Tumor study data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 04:04:38 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 18:22:16 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kundu", "Prosenjit", ""], ["Chatterjee", "Nilanjan", ""]]}, {"id": "1910.12046", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao and Hernando Ombao", "title": "Shape-Preserving Prediction for Stationary Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a novel method for prediction of stationary functional\ntime series, for trajectories sharing a similar pattern with phase variability.\nExisting prediction methodologies for functional time series only consider\namplitude variability. To overcome this limitation, we develop a prediction\nmethod that incorporates phase variability. One major advantage of our proposed\nmethod is the ability to preserve pattern by treating functional trajectories\nas shape objects defined in a quotient space with respect to time warping and\njointly modeling and estimating amplitude and phase variability. Moreover, the\nmethod does not involve unnatural transformations and can be easily implemented\nusing existing software. The asymptotic properties of the least squares\nestimator are studied. The effectiveness of the proposed method is illustrated\nin simulation study and real data analysis on annual ocean surface\ntemperatures. It is shown that prediction by the proposed SP (shape-preserving)\nmethod captures the common pattern better than the existing prediction method,\nwhile providing competitive prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 11:02:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jiao", "Shuhao", ""], ["Ombao", "Hernando", ""]]}, {"id": "1910.12049", "submitter": "Juhyun Park", "authors": "Juhyun Park and Nicolas J-B. Brunel", "title": "Mean curvature and mean shape for multivariate functional data under\n  Frenet-Serret framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of curves has been routinely dealt with using tools from\nfunctional data analysis. However its extension to multi-dimensional curves\nposes a new challenge due to its inherent geometric features that are difficult\nto capture with the classical approaches that rely on linear approximations. We\npropose a new framework for functional data as multidimensional curves that\nallows us to extract geometrical features from noisy data. We define a mean\nthrough measuring shape variation of the curves. The notion of shape has been\nused in functional data analysis somewhat intuitively to find a common pattern\nin one dimensional curves. As a generalization, we directly utilize a geometric\nrepresentation of the curves through the Frenet-Serret ordinary differential\nequations and introduce a new definition of mean curvature and mean shape\nthrough the mean ordinary differential equation. We formulate the estimation\nproblem in a penalized regression and develop an efficient algorithm. We\ndemonstrate our approach with both simulated data and a real data example.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 11:20:58 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Park", "Juhyun", ""], ["Brunel", "Nicolas J-B.", ""]]}, {"id": "1910.12090", "submitter": "Belhal Karimi", "authors": "Belhal Karimi and Marc Lavielle", "title": "Efficient Metropolis-Hastings Sampling for Nonlinear Mixed Effects\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate samples of the random effects from their conditional\ndistributions is fundamental for inference in mixed effects models. Random walk\nMetropolis is widely used to conduct such sampling, but such a method can\nconverge slowly for medium dimension problems, or when the joint structure of\nthe distributions to sample is complex. We propose a Metropolis Hastings (MH)\nalgorithm based on a multidimensional Gaussian proposal that takes into account\nthe joint conditional distribution of the random effects and does not require\nany tuning, in contrast with more sophisticated samplers such as the Metropolis\nAdjusted Langevin Algorithm or the No-U-Turn Sampler that involve costly tuning\nruns or intensive computation. Indeed, this distribution is automatically\nobtained thanks to a Laplace approximation of the original model. We show that\nsuch approximation is equivalent to linearizing the model in the case of\ncontinuous data. Numerical experiments based on real data highlight the very\ngood performances of the proposed method for continuous data model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 15:50:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Lavielle", "Marc", ""]]}, {"id": "1910.12126", "submitter": "Zeda Li", "authors": "Zeda Li, Ori Rosen, Fabio Ferrarelli, and Robert T. Krafty", "title": "Adaptive Bayesian Spectral Analysis of High-dimensional Nonstationary\n  Time Series", "comments": "7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a nonparametric approach to spectral analysis of a\nhigh-dimensional multivariate nonstationary time series. The procedure is based\non a novel frequency-domain factor model that provides a flexible yet\nparsimonious representation of spectral matrices from a large number of\nsimultaneously observed time series. Real and imaginary parts of the factor\nloading matrices are modeled independently using a prior that is formulated\nfrom the tensor product of penalized splines and multiplicative gamma process\nshrinkage priors, allowing for infinitely many factors with loadings\nincreasingly shrunk towards zero as the column index increases. Formulated in a\nfully Bayesian framework, the time series is adaptively partitioned into\napproximately stationary segments, where both the number and location of\npartition points are assumed unknown. Stochastic approximation Monte Carlo\n(SAMC) techniques are used to accommodate the unknown number of segments, and a\nconditional Whittle likelihood-based Gibbs sampler is developed for efficient\nsampling within segments. By averaging over the distribution of partitions, the\nproposed method can approximate both abrupt and slowly varying changes in\nspectral matrices. Performance of the proposed model is evaluated by extensive\nsimulations and demonstrated through the analysis of high-density\nelectroencephalography.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 19:59:45 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Zeda", ""], ["Rosen", "Ori", ""], ["Ferrarelli", "Fabio", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1910.12128", "submitter": "Subhadeep Paul", "authors": "Selena Shuo Wang, Subhadeep Paul, Paul De Boeck", "title": "Joint Latent Space Model for Social Networks with Multivariate\n  Attributes", "comments": "A previous version of this paper (version 1) used a different\n  application problem and dataset, and also had a slightly different title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application problems in social, behavioral, and economic sciences,\nresearchers often have data on a social network among a group of individuals\nalong with high dimensional multivariate measurements for each individual. To\nanalyze such networked data structures, we propose a joint Attribute and Person\nLatent Space Model (APLSM) that summarizes information from the social network\nand the multiple attribute measurements in a person-attribute joint latent\nspace. We develop a Variational Bayesian Expectation-Maximization estimation\nalgorithm to estimate the posterior distribution of the attribute and person\nlocations in the joint latent space. This methodology allows for effective\nintegration, informative visualization, and prediction of social networks and\nhigh dimensional attribute measurements. Using APLSM, we explore the inner\nworkings of the French financial elites based on their social networks and\ntheir career, political views, and social status. We observe a division in the\nsocial circles of the French elites in accordance with the differences in their\nindividual characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 20:12:09 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:11:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Wang", "Selena Shuo", ""], ["Paul", "Subhadeep", ""], ["De Boeck", "Paul", ""]]}, {"id": "1910.12208", "submitter": "Miaomiao Wang", "authors": "Miaomiao Wang, Xinyu Zhang, Alan T.K. Wan, Guohua Zou", "title": "On the asymptotic distribution of model averaging based on information\n  criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed AIC (S-AIC) and Smoothed BIC (S-BIC) are very widely used in model\naveraging and are very easily to implement. Especially, the optimal model\naveraging method MMA and JMA have only been well developed in linear models.\nOnly by modifying, they can be applied to other models. But S-AIC and S-BIC can\nbe used in all situations where AIC and BIC can be calculated. In this paper,\nwe study the asymptotic behavior of two commonly used model averaging\nestimators, the S-AIC and S-BIC estimators, under the standard asymptotic with\ngeneral fixed parameter setup. In addition, the resulting coverage probability\nin Buckland et al. (1997) is not studied accurately, but it is claimed that it\nwill be close to the intended. Our derivation make it possible to study\naccurately. Besides, we also prove that the confidence interval construction\nmethod in Hjort and Claeskens (2003) still works in linear regression with\nnormal distribution error. Both the simulation and applied example support our\ntheory conclusion.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 08:40:44 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Miaomiao", ""], ["Zhang", "Xinyu", ""], ["Wan", "Alan T. K.", ""], ["Zou", "Guohua", ""]]}, {"id": "1910.12209", "submitter": "Miaomiao Wang", "authors": "Miaomiao Wang, Guohua Zou", "title": "Jackknife Model Averaging for Composite Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model averaging considers the model uncertainty and is an alternative to\nmodel selection. In this paper, we propose a frequentist model averaging\nestimator for composite quantile regressions. In recent years, research on\nthese topics has been added as a separate method, but no study has investigated\nthem in combination. We apply a delete-one cross-validation method to estimate\nthe model weights, and prove that the jackknife model averaging estimator is\nasymptotically optimal in terms of minimizing out-of-sample composite final\nprediction error. Simulations are conducted to demonstrate the good finite\nsample properties of our estimator and compare it with commonly used model\nselection and averaging methods. The proposed method is applied to the analysis\nof the stock returns data and the wage data and performs well.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 08:47:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Miaomiao", ""], ["Zou", "Guohua", ""]]}, {"id": "1910.12210", "submitter": "Miaomiao Wang", "authors": "Miaomiao Wang, Guohua Zou", "title": "An outlier-robust model averaging approach by Mallows-type criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model averaging is an alternative to model selection for dealing with model\nuncertainty, which is widely used and very valuable. However, most of the\nexisting model averaging methods are proposed based on the least squares loss\nfunction, which could be very sensitive to the presence of outliers in the\ndata. In this paper, we propose an outlier-robust model averaging approach by\nMallows-type criterion. The key idea is to develop weight choice criteria by\nminimising an estimator of the expected prediction error for the function being\nconvex with an unique minimum, and twice differentiable in expectation, rather\nthan the expected squared error. The robust loss functions, such as least\nabsolute deviation and Huber's function, reduce the effects of large residuals\nand poor samples. Simulation study and real data analysis are conducted to\ndemonstrate the finite-sample performance of our estimators and compare them\nwith other model selection and averaging methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 08:55:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Miaomiao", ""], ["Zou", "Guohua", ""]]}, {"id": "1910.12222", "submitter": "Belhal Karimi", "authors": "Belhal Karimi, Marc Lavielle, Eric Moulines", "title": "f-SAEM: A fast Stochastic Approximation of the EM algorithm for\n  nonlinear mixed effects models", "comments": "Accepted in CSDA 2020, 35 pages. arXiv admin note: text overlap with\n  arXiv:1910.12090", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate samples of the random effects from their conditional\ndistributions is fundamental for inference in mixed effects models. Random walk\nMetropolis is widely used to perform such sampling, but this method is known to\nconverge slowly for medium dimensional problems, or when the joint structure of\nthe distributions to sample is spatially heterogeneous. The main contribution\nconsists of an independent Metropolis-Hastings (MH) algorithm based on a\nmultidimensional Gaussian proposal that takes into account the joint\nconditional distribution of the random effects and does not require any tuning.\nIndeed, this distribution is automatically obtained thanks to a Laplace\napproximation of the incomplete data model. Such approximation is shown to be\nequivalent to linearizing the structural model in the case of continuous data.\nNumerical experiments based on simulated and real data illustrate the\nperformance of the proposed methods. For fitting nonlinear mixed effects\nmodels, the suggested MH algorithm is efficiently combined with a stochastic\napproximation version of the EM algorithm for maximum likelihood estimation of\nthe global parameters.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 09:56:54 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Lavielle", "Marc", ""], ["Moulines", "Eric", ""]]}, {"id": "1910.12267", "submitter": "Andreas Kryger Jensen", "authors": "Andreas Kryger Jensen and Theis Lange", "title": "A novel high-power test for continuous outcomes truncated by death", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient reported outcomes including quality of life (QoL) assessments are\nincreasingly being included as either primary or secondary outcomes in\nrandomized controlled trials. While making the outcomes more relevant for\npatients it entails a challenge in cases where death or a similar event makes\nthe outcome of interest undefined. A pragmatic - and much used - solution is to\nassign diseased patient with the lowest possible QoL score. This makes medical\nsense, but creates a statistical problem since traditional tests such as\nt-tests or Wilcox tests potentially looses large amounts of statistical power.\nIn this paper we propose a novel test that can keep the medical relevant\ncomposite outcome, but preserve full statistical power. The test is also\napplicable in other situations where a specific value (say 0 days alive outside\nhospitals) encodes a special meaning. The test is implemented in an R package\nwhich is available for download.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 14:23:16 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Jensen", "Andreas Kryger", ""], ["Lange", "Theis", ""]]}, {"id": "1910.12327", "submitter": "Sourav Chatterjee", "authors": "Mona Azadkia, Sourav Chatterjee", "title": "A simple measure of conditional dependence", "comments": "41 pages, 2 tables. Final version. To appear in Ann. Statist. An R\n  package is available at https://CRAN.R-project.org/package=FOCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coefficient of conditional dependence between two random\nvariables $Y$ and $Z$ given a set of other variables $X_1,\\ldots,X_p$, based on\nan i.i.d. sample. The coefficient has a long list of desirable properties, the\nmost important of which is that under absolutely no distributional assumptions,\nit converges to a limit in $[0,1]$, where the limit is $0$ if and only if $Y$\nand $Z$ are conditionally independent given $X_1,\\ldots,X_p$, and is $1$ if and\nonly if $Y$ is equal to a measurable function of $Z$ given $X_1,\\ldots,X_p$.\nMoreover, it has a natural interpretation as a nonlinear generalization of the\nfamiliar partial $R^2$ statistic for measuring conditional dependence by\nregression. Using this statistic, we devise a new variable selection algorithm,\ncalled Feature Ordering by Conditional Independence (FOCI), which is\nmodel-free, has no tuning parameters, and is provably consistent under sparsity\nassumptions. A number of applications to synthetic and real datasets are worked\nout.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 19:14:39 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 22:14:11 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 07:14:17 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 20:44:42 GMT"}, {"version": "v5", "created": "Sun, 10 Jan 2021 08:57:37 GMT"}, {"version": "v6", "created": "Sun, 28 Mar 2021 05:52:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Azadkia", "Mona", ""], ["Chatterjee", "Sourav", ""]]}, {"id": "1910.12389", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Variable Selection with Copula Entropy", "comments": "To appear at Chinese Journal of Applied Probability and Statistics.\n  Code is available on GitHub at https://github.com/majianthu/aps2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is of significant importance for classification and\nregression tasks in machine learning and statistical applications where both\npredictability and explainability are needed. In this paper, a Copula Entropy\n(CE) based method for variable selection which use CE based ranks to select\nvariables is proposed. The method is both model-free and tuning-free.\nComparison experiments between the proposed method and traditional variable\nselection methods, such as Distance Correlation, Hilbert-Schmidt Independence\nCriterion, Stepwise Selection, regularized generalized linear models and\nAdaptive LASSO, were conducted on the UCI heart disease data. Experimental\nresults show that CE based method can select the `right' variables out more\neffectively and derive better interpretable results than traditional methods do\nwithout sacrificing accuracy performance. It is believed that CE based variable\nselection can help to build more explainable models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 00:47:00 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 11:22:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "1910.12394", "submitter": "Albert Vexler", "authors": "Albert Vexler", "title": "Univariate Likelihood Projections and Characterizations of the\n  Multivariate Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of characterizing a multivariate distribution of a random vector\nusing examination of univariate combinations of vector components is an\nessential issue of multivariate analysis. The likelihood principle plays a\nprominent role in developing powerful statistical inference tools. In this\ncontext, we raise the question: can the univariate likelihood function based on\na random vector be used to provide the uniqueness in reconstructing the vector\ndistribution? In multivariate normal (MN) frameworks, this question links to a\nreverse of Cochran's theorem that concerns the distribution of quadratic forms\nin normal variables. We characterize the MN distribution through the univariate\nlikelihood type projections. The proposed principle is employed to illustrate\nsimple techniques for assessing multivariate normality via well-known tests\nthat use univariate observations. The presented testing strategy can exhibit\nhigh and stable power characteristics in comparison to the well-known\nprocedures in various scenarios when observed vectors are non-MN distributed,\nwhereas their components are normally distributed random variables. In such\ncases, the classical multivariate normality tests may break down completely.\n  KEY WORDS: Characterization, Goodness of fit, Infinity divisible, Likelihood,\nMultivariate normal distribution, Projection, Quadratic form, Test for\nmultivariate normality.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 01:11:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Vexler", "Albert", ""]]}, {"id": "1910.12431", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Gianluca Detommaso, Robert Scheichl", "title": "Multilevel Dimension-Independent Likelihood-Informed MCMC for\n  Large-Scale Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-trivial integration of dimension-independent\nlikelihood-informed (DILI) MCMC (Cui, Law, Marzouk, 2016) and the multilevel\nMCMC (Dodwell et al., 2015) to explore the hierarchy of posterior\ndistributions. This integration offers several advantages: First, DILI-MCMC\nemploys an intrinsic likelihood-informed subspace (LIS) (Cui et al., 2014) --\nwhich involves a number of forward and adjoint model simulations -- to design\naccelerated operator-weighted proposals. By exploiting the multilevel structure\nof the discretised parameters and discretised forward models, we design a\nRayleigh-Ritz procedure to significantly reduce the computational effort in\nbuilding the LIS and operating with DILI proposals. Second, the resulting\nDILI-MCMC can drastically improve the sampling efficiency of MCMC at each\nlevel, and hence reduce the integration error of the multilevel algorithm for\nfixed CPU time. To be able to fully exploit the power of multilevel MCMC and to\nreduce the dependencies of samples on different levels for a parallel\nimplementation, we also suggest a new pooling strategy for allocating\ncomputational resources across different levels and constructing Markov chains\nat higher levels conditioned on those simulated on lower levels. Numerical\nresults confirm the improved computational efficiency of the multilevel DILI\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 04:10:30 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cui", "Tiangang", ""], ["Detommaso", "Gianluca", ""], ["Scheichl", "Robert", ""]]}, {"id": "1910.12457", "submitter": "Ruixuan Rachel Zhou", "authors": "Ruixuan Rachel Zhou, Liewei Wang, Sihai Dave Zhao", "title": "Estimation and inference for the indirect effect in high-dimensional\n  linear mediation models", "comments": "To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis is difficult when the number of potential mediators is\nlarger than the sample size. In this paper we propose new inference procedures\nfor the indirect effect in the presence of high-dimensional mediators for\nlinear mediation models. We develop methods for both incomplete mediation,\nwhere a direct effect may exist, as well as complete mediation, where the\ndirect effect is known to be absent. We prove consistency and asymptotic\nnormality of our indirect effect estimators. Under complete mediation, where\nthe indirect effect is equivalent to the total effect, we further prove that\nour approach gives a more powerful test compared to directly testing for the\ntotal effect. We confirm our theoretical results in simulations, as well as in\nan integrative analysis of gene expression and genotype data from a\npharmacogenomic study of drug response. We present a novel analysis of gene\nsets to understand the molecular mechanisms of drug response, and also identify\na genome-wide significant noncoding genetic variant that cannot be detected\nusing standard analysis methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:02:57 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhou", "Ruixuan Rachel", ""], ["Wang", "Liewei", ""], ["Zhao", "Sihai Dave", ""]]}, {"id": "1910.12471", "submitter": "Shuchi Goyal", "authors": "Shuchi Goyal, Gauri Sankar Datta, Abhyuday Mandal", "title": "A Hierarchical Bayes Unit-Level Small Area Estimation Model for Normal\n  Mixture Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National statistical agencies are regularly required to produce estimates\nabout various subpopulations, formed by demographic and/or geographic\nclassifications, based on a limited number of samples. Traditional direct\nestimates computed using only sampled data from individual subpopulations are\nusually unreliable due to small sample sizes. Subpopulations with small samples\nare termed small areas or small domains. To improve on the less reliable direct\nestimates, model-based estimates, which borrow information from suitable\nauxiliary variables, have been extensively proposed in the literature. However,\nstandard model-based estimates rely on the normality assumptions of the error\nterms. In this research we propose a hierarchical Bayesian (HB) method for the\nunit-level nested error regression model based on a normal mixture for the\nunit-level error distribution. To implement our proposal we use a uniform prior\nfor the regression parameters, random effects variance parameter, and the\nmixing proportion, and we use a partially proper non-informative prior\ndistribution for the two unit-level error variance components in the mixture.\nWe apply our method to two examples to predict summary characteristics of farm\nproducts at the small area level. One of the examples is prediction of twelve\ncounty-level crop areas cultivated for corn in some Iowa counties. The other\nexample involves total cash associated in farm operations in twenty-seven\nfarming regions in Australia. We compare predictions of small area\ncharacteristics based on the proposed method with those obtained by applying\nthe Datta and Ghosh (1991) and the Chakraborty et al. (2018) HB methods. Our\nsimulation study comparing these three Bayesian methods showed the superiority\nof our proposed method, measured by prediction mean squared error, coverage\nprobabilities and lengths of credible intervals for the small area means.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 07:18:31 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Goyal", "Shuchi", ""], ["Datta", "Gauri Sankar", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "1910.12486", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Claudia Kirch", "title": "Two-stage data segmentation permitting multiscale change points, heavy\n  tails and dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of a time series into piecewise stationary segments, a.k.a.\nmultiple change point analysis, is an important problem both in time series\nanalysis and signal processing. In the presence of multiscale change points\nwith both large jumps over short intervals and small changes over long\nstationary intervals, multiscale methods achieve good adaptivity in their\nlocalisation but at the same time, require the removal of false positives and\nduplicate estimators via a model selection step. In this paper, we propose a\nlocalised application of Schwarz information criterion which, as a generic\nmethodology, is applicable with any multiscale candidate generating procedure\nfulfilling mild assumptions. We establish the theoretical consistency of the\nproposed localised pruning method in estimating the number and locations of\nmultiple change points under general assumptions permitting heavy tails and\ndependence. Further, we show that combined with a MOSUM-based candidate\ngenerating procedure, it attains minimax optimality in terms of detection lower\nbound and localisation for i.i.d. sub-Gaussian errors. A careful comparison\nwith the existing methods by means of (a) theoretical properties such as\ngenerality, optimality and algorithmic complexity, (b) performance on simulated\ndatasets and run time, as well as (c) performance on real data applications,\nconfirm the overall competitiveness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 08:02:16 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 13:22:21 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 06:36:24 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Cho", "Haeran", ""], ["Kirch", "Claudia", ""]]}, {"id": "1910.12521", "submitter": "Belhal Karimi", "authors": "Belhal Karimi, Hoi-To Wai, Eric Moulines, Marc Lavielle", "title": "On the Global Convergence of (Fast) Incremental Expectation Maximization\n  Methods", "comments": "25 pages, Accepted at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is one of the most popular algorithm for inference in latent\ndata models. The original formulation of the EM algorithm does not scale to\nlarge data set, because the whole data set is required at each iteration of the\nalgorithm. To alleviate this problem, Neal and Hinton have proposed an\nincremental version of the EM (iEM) in which at each iteration the conditional\nexpectation of the latent data (E-step) is updated only for a mini-batch of\nobservations. Another approach has been proposed by Capp\\'e and Moulines in\nwhich the E-step is replaced by a stochastic approximation step, closely\nrelated to stochastic gradient. In this paper, we analyze incremental and\nstochastic version of the EM algorithm as well as the variance reduced-version\nof Chen et. al. in a common unifying framework. We also introduce a new version\nincremental version, inspired by the SAGA algorithm by Defazio et. al. We\nestablish non-asymptotic convergence bounds for global convergence. Numerical\napplications are presented in this article to illustrate our findings.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 09:51:14 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Belhal", ""], ["Wai", "Hoi-To", ""], ["Moulines", "Eric", ""], ["Lavielle", "Marc", ""]]}, {"id": "1910.12775", "submitter": "Veronica Vinciotti Dr", "authors": "Luigi Augugliaro, Gianluca Sottile, Veronica Vinciotti", "title": "The conditional censored graphical lasso estimator", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-020-09945-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applied fields, such as genomics, different types of data are\ncollected on the same system, and it is not uncommon that some of these\ndatasets are subject to censoring as a result of the measurement technologies\nused, such as data generated by polymerase chain reactions and flow cytometer.\nWhen the overall objective is that of network inference, at possibly different\nlevels of a system, information coming from different sources and/or different\nsteps of the analysis can be integrated into one model with the use of\nconditional graphical models. In this paper, we develop a doubly penalized\ninferential procedure for a conditional Gaussian graphical model when data can\nbe subject to censoring. The computational challenges of handling censored data\nin high dimensionality are met with the development of an efficient\nExpectation-Maximization algorithm, based on approximate calculations of the\nmoments of truncated Gaussian distributions and on a suitably derived two-step\nprocedure alternating graphical lasso with a novel block-coordinate\nmultivariate lasso approach. We evaluate the performance of this approach on an\nextensive simulation study and on gene expression data generated by RT-qPCR\ntechnologies, where we are able to integrate network inference, differential\nexpression detection and data normalization into one model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:01:58 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Augugliaro", "Luigi", ""], ["Sottile", "Gianluca", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1910.12797", "submitter": "Chao Gao", "authors": "Chao Gao and Zongming Ma", "title": "Testing Equivalence of Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we test whether two datasets share a common clustering\nstructure. As a leading example, we focus on comparing clustering structures in\ntwo independent random samples from two mixtures of multivariate normal\ndistributions. Mean parameters of these normal distributions are treated as\npotentially unknown nuisance parameters and are allowed to differ. Assuming\nknowledge of mean parameters, we first determine the phase diagram of the\ntesting problem over the entire range of signal-to-noise ratios by providing\nboth lower bounds and tests that achieve them. When nuisance parameters are\nunknown, we propose tests that achieve the detection boundary adaptively as\nlong as ambient dimensions of the datasets grow at a sub-linear rate with the\nsample size.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:48:49 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 18:52:53 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""]]}, {"id": "1910.12815", "submitter": "Kimia Nadjahi", "authors": "Kimia Nadjahi, Valentin De Bortoli, Alain Durmus, Roland Badeau, Umut\n  \\c{S}im\\c{s}ekli", "title": "Approximate Bayesian Computation with the Sliced-Wasserstein Distance", "comments": "Accepted at ICASSP 2020 (publication and oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a popular method for approximate\ninference in generative models with intractable but easy-to-sample likelihood.\nIt constructs an approximate posterior distribution by finding parameters for\nwhich the simulated data are close to the observations in terms of summary\nstatistics. These statistics are defined beforehand and might induce a loss of\ninformation, which has been shown to deteriorate the quality of the\napproximation. To overcome this problem, Wasserstein-ABC has been recently\nproposed, and compares the datasets via the Wasserstein distance between their\nempirical distributions, but does not scale well to the dimension or the number\nof samples. We propose a new ABC technique, called Sliced-Wasserstein ABC and\nbased on the Sliced-Wasserstein distance, which has better computational and\nstatistical properties. We derive two theoretical results showing the\nasymptotical consistency of our approach, and we illustrate its advantages on\nsynthetic data and an image denoising task.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:18:25 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:09:02 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Nadjahi", "Kimia", ""], ["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Badeau", "Roland", ""], ["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1910.12845", "submitter": "Yuxuan Zhao", "authors": "Yuxuan Zhao and Madeleine Udell", "title": "Missing Value Imputation for Mixed Data via Gaussian Copula", "comments": "Accepted by KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data imputation forms the first critical step of many data analysis\npipelines. The challenge is greatest for mixed data sets, including real,\nBoolean, and ordinal data, where standard techniques for imputation fail basic\nsanity checks: for example, the imputed values may not follow the same\ndistributions as the data. This paper proposes a new semiparametric algorithm\nto impute missing values, with no tuning parameters. The algorithm models mixed\ndata as a Gaussian copula. This model can fit arbitrary marginals for\ncontinuous variables and can handle ordinal variables with many levels,\nincluding Boolean variables as a special case. We develop an efficient\napproximate EM algorithm to estimate copula parameters from incomplete mixed\ndata. The resulting model reveals the statistical associations among variables.\nExperimental results on several synthetic and real datasets show superiority of\nour proposed algorithm to state-of-the-art imputation algorithms for mixed\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 17:53:30 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 20:43:29 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 03:25:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhao", "Yuxuan", ""], ["Udell", "Madeleine", ""]]}, {"id": "1910.12925", "submitter": "Metin Bulus", "authors": "Metin Bulus", "title": "Minimum Detectable Effect Size Computations for Cluster-Level Regression\n  Discontinuity: Quadratic Functional Form and Beyond", "comments": "Please do not cite this draft without author's permission. It\n  includes many typos and errors (final derivations, conclusions and\n  implications do not change). It also criticizes rdpower R and Stata commands\n  developed by Cattaneo, Titiunik, and Vazquez-Bare (2019) in the wrong context\n  (rdpower allows ex-ante power computations). Issues are fixed in a\n  peer-reviewed draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study extends power formulas proposed by Schochet (2008) assuming that\nthe cluster-level score variable follows quadratic functional form. Results\nreveal that we need not be concerned with treatment by linear term interaction,\nand polynomial degree up to second order for symmetric truncation intervals. In\ncomparison, every slight change in the functional form alters sample size\nrequirements for asymmetric truncation intervals. Finally, an empirical\nframework beyond quadratic functional form is provided when the asymptotic\nvariance of the treatment effect is untraceable. In this case, the CRD design\neffect is either computed from moments of the sample or approximate population\nmoments via simulation. Formulas for quadratic functional form and the extended\nempirical framework are implemented in the cosa R package and companion Shiny\nweb application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:32:00 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 07:59:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bulus", "Metin", ""]]}, {"id": "1910.12937", "submitter": "Fan Chen", "authors": "Fan Chen, Yini Zhang, and Karl Rohe", "title": "Targeted sampling from massive block model graphs with personalized\n  PageRank", "comments": "61 pages, 5 figures", "journal-ref": "J. R. Stat. Soc. B (2020), 82: 99-126", "doi": "10.1111/rssb.12349", "report-no": null, "categories": "cs.SI cs.CC cs.DL stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper provides statistical theory and intuition for personalized PageRank\n(called \"PPR\"): a popular technique that samples a small community from a\nmassive network. We study a setting where the entire network is expensive to\nobtain thoroughly or to maintain, but we can start from a seed node of interest\nand \"crawl\" the network to find other nodes through their connections. By\ncrawling the graph in a designed way, the PPR vector can be approximated\nwithout querying the entire massive graph, making it an alternative to snowball\nsampling. Using the degree-corrected stochastic block model, we study whether\nthe PPR vector can select nodes that belong to the same block as the seed node.\nWe provide a simple and interpretable form for the PPR vector, highlighting its\nbiases towards high degree nodes outside the target block. We examine a simple\nadjustment based on node degrees and establish consistency results for PPR\nclustering that allows for directed graphs. These results are enabled by recent\ntechnical advances showing the elementwise convergence of eigenvectors. We\nillustrate the method with the massive Twitter friendship graph, which we crawl\nby using the Twitter application programming interface. We find that the\nadjusted and unadjusted PPR techniques are complementary approaches, where the\nadjustment makes the results particularly localized around the seed node, and\nthat the bias adjustment greatly benefits from degree regularization.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 01:48:57 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 17:07:19 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Zhang", "Yini", ""], ["Rohe", "Karl", ""]]}, {"id": "1910.12970", "submitter": "Lan Gao", "authors": "Lan Gao, Yingying Fan, Jinchi Lv and Qi-Man Shao", "title": "Asymptotic Distributions of High-Dimensional Distance Correlation\n  Inference", "comments": "67 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance correlation has become an increasingly popular tool for detecting\nthe nonlinear dependence between a pair of potentially high-dimensional random\nvectors. Most existing works have explored its asymptotic distributions under\nthe null hypothesis of independence between the two random vectors when only\nthe sample size or the dimensionality diverges. Yet its asymptotic null\ndistribution for the more realistic setting when both sample size and\ndimensionality diverge in the full range remains largely underdeveloped. In\nthis paper, we fill such a gap and develop central limit theorems and\nassociated rates of convergence for a rescaled test statistic based on the\nbias-corrected distance correlation in high dimensions under some mild\nregularity conditions and the null hypothesis. Our new theoretical results\nreveal an interesting phenomenon of blessing of dimensionality for\nhigh-dimensional distance correlation inference in the sense that the accuracy\nof normal approximation can increase with dimensionality. Moreover, we provide\na general theory on the power analysis under the alternative hypothesis of\ndependence, and further justify the capability of the rescaled distance\ncorrelation in capturing the pure nonlinear dependency under moderately high\ndimensionality for a certain type of alternative hypothesis. The theoretical\nresults and finite-sample performance of the rescaled statistic are illustrated\nwith several simulation examples and a blockchain application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 20:58:44 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 01:21:55 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 01:40:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gao", "Lan", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""], ["Shao", "Qi-Man", ""]]}, {"id": "1910.13074", "submitter": "Yumou Qiu", "authors": "Song Xi Chen, Bin Guo, Yumou Qiu", "title": "Multi-level Thresholding Test for High Dimensional Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing the equality of two high-dimensional covariance matrices\nby carrying out a multi-level thresholding procedure, which is designed to\ndetect sparse and faint differences between the covariances. A novel\nU-statistic composition is developed to establish the asymptotic distribution\nof the thresholding statistics in conjunction with the matrix blocking and the\ncoupling techniques. We propose a multi-thresholding test that is shown to be\npowerful in detecting sparse and weak differences between two covariance\nmatrices. The test is shown to have attractive detection boundary and to attain\nthe optimal minimax rate in the signal strength under different regimes of high\ndimensionality and the sparsity of the signal. Simulation studies are conducted\nto demonstrate the utility of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 03:58:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chen", "Song Xi", ""], ["Guo", "Bin", ""], ["Qiu", "Yumou", ""]]}, {"id": "1910.13080", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Masahiko Gosho, Ryota Ishii, Koji Oba and Toshi A.\n  Furukawa", "title": "Outlier detection and influence diagnostics in network meta-analysis", "comments": null, "journal-ref": "Res Synth Methods 2020;11(6):891-902", "doi": "10.1002/jrsm.1455", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network meta-analysis has been gaining prominence as an evidence synthesis\nmethod that enables the comprehensive synthesis and simultaneous comparison of\nmultiple treatments. In many network meta-analyses, some of the constituent\nstudies may have markedly different characteristics from the others, and may be\ninfluential enough to change the overall results. The inclusion of these\n\"outlying\" studies might lead to biases, yielding misleading results. In this\narticle, we propose effective methods for detecting outlying and influential\nstudies in a frequentist framework. In particular, we propose suitable\ninfluence measures for network meta-analysis models that involve missing\noutcomes and adjust the degree of freedoms appropriately. We propose three\ninfluential measures by a leave-one-trial-out cross-validation scheme: (1)\ncomparison-specific studentized residual, (2) relative change measure for\ncovariance matrix of the comparative effectiveness parameters, (3) relative\nchange measure for heterogeneity covariance matrix. We also propose (4) a\nmodel-based approach using a likelihood ratio statistic by a mean-shifted\noutlier detection model. We illustrate the effectiveness of the proposed\nmethods via applications to a network meta-analysis of antihypertensive drugs.\nUsing the four proposed methods, we could detect three potential influential\ntrials involving an obvious outlier that was retracted because of data\nfalsifications. We also demonstrate that the overall results of comparative\nefficacy estimates and the ranking of drugs were altered by omitting these\nthree influential studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 04:29:05 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Noma", "Hisashi", ""], ["Gosho", "Masahiko", ""], ["Ishii", "Ryota", ""], ["Oba", "Koji", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1910.13119", "submitter": "Xu Chen", "authors": "Xu Chen and Surya T. Tokdar", "title": "Joint Quantile Regression for Spatial Data", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear quantile regression is a powerful tool to investigate how predictors\nmay affect a response heterogeneously across different quantile levels.\nUnfortunately, existing approaches find it extremely difficult to adjust for\nany dependency between observation units, largely because such methods are not\nbased upon a fully generative model of the data. For analyzing spatially\nindexed data, we address this difficulty by generalizing the joint quantile\nregression model of Yang and Tokdar (2017) and characterizing spatial\ndependence via a Gaussian or $t$ copula process on the underlying quantile\nlevels of the observation units. A Bayesian semiparametric approach is\nintroduced to perform inference of model parameters and carry out spatial\nquantile smoothing. An effective model comparison criteria is provided,\nparticularly for selecting between different model specifications of tail\nheaviness and tail dependence. Extensive simulation studies and an application\nto particulate matter concentration in northeast US are presented to illustrate\nsubstantial gains in inference quality, accuracy and uncertainty quantification\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 07:30:42 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chen", "Xu", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1910.13152", "submitter": "Rapha\\\"el Jauslin", "authors": "Rapha\\\"el Jauslin and Yves Till\\'e", "title": "Spatial Spread Sampling Using Weakly Associated Vectors", "comments": "To appear in JABES", "journal-ref": "Journal of Agricultural, Biological and Environmental Statistics\n  25 (2020) 431-451", "doi": "10.1007/s13253-020-00407-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographical data are generally autocorrelated. In this case, it is\npreferable to select spread units. In this paper, we propose a new method for\nselecting well-spread samples from a finite spatial population with equal or\nunequal inclusion probabilities. The proposed method is based on the definition\nof a spatial structure by using a stratification matrix. Our method exactly\nsatisfies given inclusion probabilities and provides samples that are very\nwell-spread. A set of simulations shows that our method outperforms other\nexisting methods such as the Generalized Random Tessellation Stratified (GRTS)\nor the Local Pivotal Method (LPM). Analysis of the variance on a real dataset\nshows that our method is more accurate than these two. Furthermore, a variance\nestimator is proposed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 09:44:35 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 12:51:18 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jauslin", "Rapha\u00ebl", ""], ["Till\u00e9", "Yves", ""]]}, {"id": "1910.13289", "submitter": "Yi Yu", "authors": "Oscar Hernan Madrid Padilla and Yi Yu and Daren Wang and Alessandro\n  Rinaldo", "title": "Optimal nonparametric multivariate change point detection and\n  localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multivariate nonparametric change point detection problem, where\nthe data are a sequence of independent $p$-dimensional random vectors whose\ndistributions are piecewise-constant with Lipschitz densities changing at\nunknown times, called change points. We quantify the size of the distributional\nchange at any change point with the supremum norm of the difference between the\ncorresponding densities. We are concerned with the localization task of\nestimating the positions of the change points. In our analysis, we allow for\nthe model parameters to vary with the total number of time points, including\nthe minimal spacing between consecutive change points and the magnitude of the\nsmallest distributional change. We provide information-theoretic lower bounds\non both the localization rate and the minimal signal-to-noise ratio required to\nguarantee consistent localization. We formulate a novel algorithm based on\nkernel density estimation that nearly achieves the minimax lower bound, save\npossibly for logarithm factors. We have provided extensive numerical evidence\nto support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:27:40 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 09:29:55 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:37:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1910.13293", "submitter": "Jose Ameijeiras-Alonso", "authors": "Jose Ameijeiras-Alonso and Christophe Ley", "title": "Sine-skewed toroidal distributions and their application in protein\n  bioinformatics", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxaa039", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the bioinformatics field, there has been a growing interest in modelling\ndihedral angles of amino acids by viewing them as data on the torus. This has\nmotivated, over the past years, new proposals of distributions on the bivariate\ntorus. The main drawback of most of these models is that the related densities\nare (pointwise) symmetric, despite the fact that the data usually present\nasymmetric patterns. This motivates the need to find a new way of constructing\nasymmetric toroidal distributions starting from a symmetric distribution. We\ntackle this problem in this paper by introducing the sine-skewed toroidal\ndistributions. The general properties of the new models are derived. Based on\nthe initial symmetric model, explicit expressions for the shape parameters are\nobtained, a simple algorithm for generating random numbers is provided, and\nasymptotic results for the maximum likelihood estimators are established. An\nimportant feature of our construction is that no normalizing constant needs to\nbe calculated, leading to more flexible distributions without increasing the\ncomplexity of the models. The benefit of employing these new sine-skewed\ndistributions is shown on the basis of protein data, where, in general, the new\nmodels outperform their symmetric antecedents.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 14:31:43 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ameijeiras-Alonso", "Jose", ""], ["Ley", "Christophe", ""]]}, {"id": "1910.13418", "submitter": "Alexander Petersen", "authors": "Alexander Petersen, Xi Liu, and Afshin A. Divani", "title": "Wasserstein $F$-tests and Confidence Bands for the Fr\\`echet Regression\n  of Density Response Curves", "comments": "58 pages (with Appendix), 5 figures, accepted at Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data consisting of samples of probability density functions are increasingly\nprevalent, necessitating the development of methodologies for their analysis\nthat respect the inherent nonlinearities associated with densities. In many\napplications, density curves appear as functional response objects in a\nregression model with vector predictors. For such models, inference is key to\nunderstand the importance of density-predictor relationships, and the\nuncertainty associated with the estimated conditional mean densities, defined\nas conditional Fr\\'echet means under a suitable metric. Using the Wasserstein\ngeometry of optimal transport, we consider the Fr\\'echet regression of density\ncurve responses and develop tests for global and partial effects, as well as\nsimultaneous confidence bands for estimated conditional mean densities. The\nasymptotic behavior of these objects is based on underlying functional central\nlimit theorems within Wasserstein space, and we demonstrate that they are\nasymptotically of the correct size and coverage, with uniformly strong\nconsistency of the proposed tests under sequences of contiguous alternatives.\nThe accuracy of these methods, including nominal size, power, and coverage, is\nassessed through simulations, and their utility is illustrated through a\nregression analysis of post-intracerebral hemorrhage hematoma densities and\ntheir associations with a set of clinical and radiological covariates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:30:57 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 16:37:19 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Petersen", "Alexander", ""], ["Liu", "Xi", ""], ["Divani", "Afshin A.", ""]]}, {"id": "1910.13493", "submitter": "Daniel Kumor", "authors": "Daniel Kumor, Bryant Chen, Elias Bareinboim", "title": "Efficient Identification in Linear Structural Causal Models with\n  Instrumental Cutsets", "comments": "To appear at 33rd Conference on Neural Information Processing Systems\n  (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": "R-49", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common mistakes made when performing data analysis is\nattributing causal meaning to regression coefficients. Formally, a causal\neffect can only be computed if it is identifiable from a combination of\nobservational data and structural knowledge about the domain under\ninvestigation (Pearl, 2000, Ch. 5). Building on the literature of instrumental\nvariables (IVs), a plethora of methods has been developed to identify causal\neffects in linear systems. Almost invariably, however, the most powerful such\nmethods rely on exponential-time procedures. In this paper, we investigate\ngraphical conditions to allow efficient identification in arbitrary linear\nstructural causal models (SCMs). In particular, we develop a method to\nefficiently find unconditioned instrumental subsets, which are generalizations\nof IVs that can be used to tame the complexity of many canonical algorithms\nfound in the literature. Further, we prove that determining whether an effect\ncan be identified with TSID (Weihs et al., 2017), a method more powerful than\nunconditioned instrumental sets and other efficient identification algorithms,\nis NP-Complete. Finally, building on the idea of flow constraints, we introduce\na new and efficient criterion called Instrumental Cutsets (IC), which is able\nto solve for parameters missed by all other existing polynomial-time\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 19:36:32 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Kumor", "Daniel", ""], ["Chen", "Bryant", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1910.13524", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Christopher K. Wikle", "title": "Deep Integro-Difference Equation Models for Spatio-Temporal Forecasting", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100408", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integro-difference equation (IDE) models describe the conditional dependence\nbetween the spatial process at a future time point and the process at the\npresent time point through an integral operator. Nonlinearity or temporal\ndependence in the dynamics is often captured by allowing the operator\nparameters to vary temporally, or by re-fitting a model with a\ntemporally-invariant linear operator in a sliding window. Both procedures tend\nto be excellent for prediction purposes over small time horizons, but are\ngenerally time-consuming and, crucially, do not provide a global prior model\nfor the temporally-varying dynamics that is realistic. Here, we tackle these\ntwo issues by using a deep convolution neural network (CNN) in a hierarchical\nstatistical IDE framework, where the CNN is designed to extract process\ndynamics from the process' most recent behaviour. Once the CNN is fitted,\nprobabilistic forecasting can be done extremely quickly online using an\nensemble Kalman filter with no requirement for repeated parameter estimation.\nWe conduct an experiment where we train the model using 13 years of daily\nsea-surface temperature data in the North Atlantic Ocean. Forecasts are seen to\nbe accurate and calibrated. A key advantage of our approach is that the CNN\nprovides a global prior model for the dynamics that is realistic,\ninterpretable, and computationally efficient. We show the versatility of the\napproach by successfully producing 10-minute nowcasts of weather radar\nreflectivities in Sydney using the same model that was trained on daily\nsea-surface temperature data in the North Atlantic Ocean.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:49:25 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 21:21:49 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 23:24:41 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1910.13570", "submitter": "Bradley Rava", "authors": "Gareth M. James, Peter Radchenko and Bradley Rava", "title": "Irrational Exuberance: Correcting Bias in Probability Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the common setting where one observes probability estimates for a\nlarge number of events, such as default risks for numerous bonds.\nUnfortunately, even with unbiased estimates, selecting events corresponding to\nthe most extreme probabilities can result in systematically underestimating the\ntrue level of uncertainty. We develop an empirical Bayes approach \"Excess\nCertainty Adjusted Probabilities\" (ECAP), using a variant of Tweedie's formula,\nwhich updates probability estimates to correct for selection bias. ECAP is a\nflexible non-parametric method, which directly estimates the score function\nassociated with the probability estimates, so it does not need to make any\nrestrictive assumptions about the prior on the true probabilities. ECAP also\nworks well in settings where the probability estimates are biased. We\ndemonstrate through theoretical results, simulations, and an analysis of two\nreal world data sets, that ECAP can provide significant improvements over the\noriginal probability estimates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 22:59:31 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 19:13:25 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 00:40:58 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["James", "Gareth M.", ""], ["Radchenko", "Peter", ""], ["Rava", "Bradley", ""]]}, {"id": "1910.13627", "submitter": "Matias Quiroz", "authors": "Robert Salomone, Matias Quiroz, Robert Kohn, Mattias Villani,\n  Minh-Ngoc Tran", "title": "Spectral Subsampling MCMC for Stationary Time Series", "comments": "Empirical section significantly revised and extended", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets\nhas developed rapidly in recent years. However, the underlying methods are\ngenerally limited to relatively simple settings where the data have specific\nforms of independence. We propose a novel technique for speeding up MCMC for\ntime series data by efficient data subsampling in the frequency domain. For\nseveral challenging time series models, we demonstrate a speedup of up to two\norders of magnitude while incurring negligible bias compared to MCMC on the\nfull dataset. We also propose alternative control variates for variance\nreduction based on data grouping and coreset constructions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 02:31:56 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2020 00:27:50 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Salomone", "Robert", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Villani", "Mattias", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1910.13632", "submitter": "Gaoxiang Jia", "authors": "Gaoxiang Jia, Xinlei Wang, Qiwei Li, Wei Lu, Ximing Tang, Ignacio\n  Wistuba, and Yang Xie", "title": "RCRnorm: An integrated system of random-coefficient hierarchical\n  regression models for normalizing NanoString nCounter data", "comments": null, "journal-ref": "Ann. Appl. Stat. 13 (2019), no. 3, 1617--1647.\n  https://projecteuclid.org/euclid.aoas/1571277766", "doi": "10.1214/19-AOAS1249", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Formalin-fixed paraffin-embedded (FFPE) samples have great potential for\nbiomarker discovery, retrospective studies and diagnosis or prognosis of\ndiseases. Their application, however, is hindered by the unsatisfactory\nperformance of traditional gene expression profiling techniques on damaged\nRNAs. NanoString nCounter platform is well suited for profiling of FFPE samples\nand measures gene expression with high sensitivity which may greatly facilitate\nrealization of scientific and clinical values of FFPE samples. However,\nmethodological development for normalization, a critical step when analyzing\nthis type of data, is far behind. Existing methods designed for the platform\nuse information from different types of internal controls separately and rely\non an overly-simplified assumption that expression of housekeeping genes is\nconstant across samples for global scaling. Thus, these methods are not\noptimized for the nCounter system, not mentioning that they were not developed\nfor FFPE samples. We construct an integrated system of random-coefficient\nhierarchical regression models to capture main patterns and characteristics\nobserved from NanoString data of FFPE samples and develop a Bayesian approach\nto estimate parameters and normalize gene expression across samples. Our\nmethod, labeled RCRnorm, incorporates information from all aspects of the\nexperimental design and simultaneously removes biases from various sources. It\neliminates the unrealistic assumption on housekeeping genes and offers great\ninterpretability. Furthermore, it is applicable to freshly frozen or like\nsamples that can be generally viewed as a reduced case of FFPE samples.\nSimulation and applications showed the superior performance of RCRnorm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:38:40 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Jia", "Gaoxiang", ""], ["Wang", "Xinlei", ""], ["Li", "Qiwei", ""], ["Lu", "Wei", ""], ["Tang", "Ximing", ""], ["Wistuba", "Ignacio", ""], ["Xie", "Yang", ""]]}, {"id": "1910.13673", "submitter": "Mingyuan Zhou", "authors": "Zhendong Wang and Mingyuan Zhou", "title": "Thompson Sampling via Local Uncertainty", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is an efficient algorithm for sequential decision making,\nwhich exploits the posterior uncertainty to address the\nexploration-exploitation dilemma. There has been significant recent interest in\nintegrating Bayesian neural networks into Thompson sampling. Most of these\nmethods rely on global variable uncertainty for exploration. In this paper, we\npropose a new probabilistic modeling framework for Thompson sampling, where\nlocal latent variable uncertainty is used to sample the mean reward.\nVariational inference is used to approximate the posterior of the local\nvariable, and semi-implicit structure is further introduced to enhance its\nexpressiveness. Our experimental results on eight contextual bandit benchmark\ndatasets show that Thompson sampling guided by local uncertainty achieves\nstate-of-the-art performance while having low computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:05:30 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:38:02 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:18:02 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Wang", "Zhendong", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1910.13719", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz and Moritz Berger", "title": "Tree-Structured Scale Effects in Binary and Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary and ordinal regression one can distinguish between a location\ncomponent and a scaling component. While the former determines the location\nwithin the range of the response categories, the scaling indicates variance\nheterogeneity. In particular since it has been demonstrated that misleading\neffects can occur if one ignores the presence of a scaling component it is\nimportant to account for potential scaling effects in the regression model,\nwhich is not possible in available recursive partitioning methods. The proposed\nrecursive partitioning method yields two trees, one for the location and one\nfor the scaling. They show in a simple interpretable way how variables interact\nto determine the binary or ordinal response. The developed algorithm controls\nfor the global significance level and automatically selects the variables that\nhave an impact on the response. The modelling approach is illustrated by\nseveral real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:45:39 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Tutz", "Gerhard", ""], ["Berger", "Moritz", ""]]}, {"id": "1910.13848", "submitter": "Antonio Forcina", "authors": "Antonio Forcina, Maria Kateri", "title": "An extended class of RC association models: estimation and main\n  properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extended class of multiplicative row-column (RC) association models,\nintroduced in this paper for two-way contingency tables, allows users to select\nboth the type of logit (local, global, continuation, reverse continuation)\nsuitable for the row and column classification variables and the scale on which\ninteractions are measured. As in \\cite{Kateri95} for the case of local logits,\nour extended class of bivariate interactions is linked to divergence measures\nand, by means of a representation theorem, we provide reconstruction formulas\nfor the joint probabilities depending on pairs of logit types. These results\nare the key to show that, given marginal logits, our extended interactions\ndetermine uniquely the bivariate distribution. We also determine the kind of\npositive association which is implied by our extended interactions being non\nnegative. Quick model selection within this wide class can be performed by an\nefficient algorithm for computing maximum likelihood estimates which exploits\nthe properties of a reduced rank constraint imposed on the matrix of extended\ninteractions and allows for additional linear constraint on marginal logits. An\napplication to social mobility data is presented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:44:44 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 10:35:57 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 20:26:47 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 06:29:28 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Forcina", "Antonio", ""], ["Kateri", "Maria", ""]]}, {"id": "1910.13954", "submitter": "Bingkai Wang", "authors": "Bingkai Wang, Ryoko Susukida, Ramin Mojtabai, Masoumeh Amin-Esmaeili,\n  Michael Rosenblum", "title": "Model-Robust Inference for Clinical Trials that Improve Precision by\n  Stratified Randomization and Covariate Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two commonly used methods for improving precision and power in clinical\ntrials are stratified randomization and covariate adjustment. However, many\ntrials do not fully capitalize on the combined precision gains from these two\nmethods, which can lead to wasted resources in terms of sample size and trial\nduration. We derive consistency and asymptotic normality of model-robust\nestimators that combine these two methods, and show that these estimators can\nlead to substantial gains in precision and power. Our theorems cover a class of\nestimators that handle continuous, binary, and time-to-event outcomes; missing\noutcomes under the missing at random assumption are handled as well. For each\nestimator, we give a formula for a consistent variance estimator that is\nmodel-robust and that fully captures variance reductions from stratified\nrandomization and covariate adjustment. Also, we give the first proof (to the\nbest of our knowledge) of consistency and asymptotic normality of the\nKaplan-Meier estimator under stratified randomization, and we derive its\nasymptotic variance. The above results also hold for the biased-coin\ncovariate-adaptive design. We demonstrate our results using three completed,\nphase 3, randomized trial data sets of treatments for substance use disorder,\nwhere the variance reduction due to stratified randomization and covariate\nadjustment ranges from 1% to 36%.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:11:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 03:39:42 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 15:09:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Wang", "Bingkai", ""], ["Susukida", "Ryoko", ""], ["Mojtabai", "Ramin", ""], ["Amin-Esmaeili", "Masoumeh", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1910.13960", "submitter": "Antoniya Shivarova", "authors": "Sven Husmann, Antoniya Shivarova, Rick Steinert", "title": "Cross-validated covariance estimators for high-dimensional\n  minimum-variance portfolios", "comments": "Covariance Estimation; Portfolio Optimization; High-dimensionality;\n  Cross-validation", "journal-ref": null, "doi": "10.1007/s11408-020-00376-y", "report-no": null, "categories": "q-fin.PM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global minimum-variance portfolio is a typical choice for investors\nbecause of its simplicity and broad applicability. Although it requires only\none input, namely the covariance matrix of asset returns, estimating the\noptimal solution remains a challenge. In the presence of high-dimensionality in\nthe data, the sample covariance estimator becomes ill-conditioned and leads to\nsuboptimal portfolios out-of-sample. To address this issue, we review recently\nproposed efficient estimation methods for the covariance matrix and extend the\nliterature by suggesting a multi-fold cross-validation technique for selecting\nthe necessary tuning parameters within each method. Conducting an extensive\nempirical analysis with four datasets based on the S&P 500, we show that the\ndata-driven choice of specific tuning parameters with the proposed\ncross-validation improves the out-of-sample performance of the global\nminimum-variance portfolio. In addition, we identify estimators that are\nstrongly influenced by the choice of the tuning parameter and detect a clear\nrelationship between the selection criterion within the cross-validation and\nthe evaluated performance measure.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 16:18:13 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 08:49:06 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 17:12:15 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2020 13:48:47 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 07:55:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Husmann", "Sven", ""], ["Shivarova", "Antoniya", ""], ["Steinert", "Rick", ""]]}, {"id": "1910.14078", "submitter": "Subharup Guha", "authors": "Subharup Guha and Sujit K. Ghosh", "title": "Probabilistic Detection and Estimation of Conic Sections from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring unknown conic sections on the basis of noisy data is a challenging\nproblem with applications in computer vision. A major limitation of the\ncurrently available methods for conic sections is that estimation methods rely\non the underlying shape of the conics (being known to be ellipse, parabola or\nhyperbola). A general purpose Bayesian hierarchical model is proposed for conic\nsections and corresponding estimation method based on noisy data is shown to\nwork even when the specific nature of the conic section is unknown. The model,\nthus, provides probabilistic detection of the underlying conic section and\ninference about the associated parameters of the conic section. Through\nextensive simulation studies where the true conics may not be known, the\nmethodology is demonstrated to have practical and methodological advantages\nrelative to many existing techniques. In addition, the proposed method provides\nprobabilistic measures of uncertainty of the estimated parameters. Furthermore,\nwe observe high fidelity to the true conics even in challenging situations,\nsuch as data arising from partial conics in arbitrarily rotated and\nnon-standard form, and where a visual inspection is unable to correctly\nidentify the type of conic section underlying the data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:42:54 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 19:16:19 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Guha", "Subharup", ""], ["Ghosh", "Sujit K.", ""]]}, {"id": "1910.14101", "submitter": "Mark Risser", "authors": "Mark D. Risser and Daniel Turek", "title": "Bayesian inference for high-dimensional nonstationary Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the diverse literature on nonstationary spatial modeling and\napproximate Gaussian process (GP) methods, there are no general approaches for\nconducting fully Bayesian inference for moderately sized nonstationary spatial\ndata sets on a personal laptop. For statisticians and data scientists who wish\nto learn about spatially-referenced data and conduct posterior inference and\nprediction with appropriate uncertainty quantification, the lack of such\napproaches and corresponding software is a significant limitation. In this\npaper, we develop methodology for implementing formal Bayesian inference for a\ngeneral class of nonstationary GPs. Our novel approach uses pre-existing\nframeworks for characterizing nonstationarity in a new way that is applicable\nfor small to moderately sized data sets via modern GP likelihood\napproximations. Posterior sampling is implemented using flexible MCMC methods,\nwith nonstationary posterior prediction conducted as a post-processing step. We\ndemonstrate our novel methods on two data sets, ranging from several hundred to\nseveral thousand locations, and compare our methodology with related\nstatistical methods that provide off-the-shelf software. All of our methods are\nimplemented in the freely available BayesNSGP software package for R.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 19:41:57 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 16:42:04 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Risser", "Mark D.", ""], ["Turek", "Daniel", ""]]}, {"id": "1910.14119", "submitter": "Jaco Visagie", "authors": "J. S. Allison, S. Betsch, B. Ebner and I. J. H. Visagie", "title": "New weighted $L^2$-type tests for the inverse Gaussian distribution", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of goodness-of-fit tests for the inverse Gaussian\ndistribution. The proposed tests are weighted $L^2$-type tests depending on a\ntuning parameter. We develop the asymptotic theory under the null hypothesis\nand under a broad class of alternative distributions. These results are used to\nshow that the parametric bootstrap procedure, which we employ to implement the\ntest, is asymptotically valid and that the whole test procedure is consistent.\nA comparative simulation study for finite sample sizes shows that the new\nprocedure is competitive to classical and recent tests, outperforming these\nother methods almost uniformly over a large set of alternative distributions.\nThe use of the newly proposed test is illustrated with two observed data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 20:28:44 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Allison", "J. S.", ""], ["Betsch", "S.", ""], ["Ebner", "B.", ""], ["Visagie", "I. J. H.", ""]]}, {"id": "1910.14130", "submitter": "Bo Zhang", "authors": "Bo Zhang and Eric J. Tchetgen Tchetgen", "title": "A Semiparametric Approach to Model-based Sensitivity Analysis in\n  Observational Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When drawing causal inference from observational data, there is always\nconcern about unmeasured confounding. One way to tackle this is to conduct a\nsensitivity analysis. One widely-used sensitivity analysis framework\nhypothesizes the existence of a scalar unmeasured confounder U and asks how the\ncausal conclusion would change were U measured and included in the primary\nanalysis. Works along this line often make various parametric assumptions on U,\nfor the sake of mathematical and computational simplicity. In this article, we\nsubstantively further this line of research by developing a valid sensitivity\nanalysis that leaves the distribution of U unrestricted. Our semiparametric\nestimator has three desirable features compared to many existing methods in the\nliterature. First, our method allows for a larger and more flexible family of\nmodels, and mitigates observable implications (Franks et al., 2019). Second,\nour methods work seamlessly with any primary analysis that models the outcome\nregression parametrically. Third, our method is easy to use and interpret. We\nconstruct both pointwise confidence intervals and confidence bands that are\nuniformly valid over a given sensitivity parameter space, thus formally\naccounting for unknown sensitivity parameters. We apply our proposed method on\nan influential yet controversial study of the causal relationship between war\nexperiences and political activeness using observational data from Uganda.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 20:52:57 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zhang", "Bo", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1910.14195", "submitter": "Matthew Miller", "authors": "Matthew J. Miller, Matthew J. Cabral, Elizabeth C. Dickey, James M.\n  LeBeau and Brian J. Reich", "title": "Accounting for Location Measurement Error in Imaging Data with\n  Application to Atomic Resolution Images of Crystalline Materials", "comments": "29 pages including appendices, 4 figures, 13 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists use imaging to identify objects of interest and infer properties\nof these objects. The locations of these objects are often measured with error,\nwhich when ignored leads to biased parameter estimates and inflated variance.\nCurrent measurement error methods require an estimate or knowledge of the\nmeasurement error variance to correct these estimates, which may not be\navailable. Instead, we create a spatial Bayesian hierarchical model that treats\nthe locations as parameters, it using the image itself to incorporate\npositional uncertainty. We lower the computational burden by approximating the\nlikelihood using a non-contiguous block design around the object locations. We\napply this model in a materials science setting to study the relationship\nbetween the chemistry and displacement of hundreds of atom columns in crystal\nstructures directly imaged via scanning transmission electron microscopy.\nGreater knowledge of this relationship can lead to engineering materials with\nimproved properties of interest. We find strong evidence of a negative\nrelationship between atom column displacement and the intensity of neighboring\natom columns, which is related to the local chemistry. A simulation study shows\nour method corrects the bias in the parameter of interest and drastically\nimproves coverage in high noise scenarios compared to non-measurement error\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 00:41:19 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 13:02:50 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Miller", "Matthew J.", ""], ["Cabral", "Matthew J.", ""], ["Dickey", "Elizabeth C.", ""], ["LeBeau", "James M.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1910.14227", "submitter": "Anthony Ebert", "authors": "Anthony Ebert, Pierre Pudlo, Kerrie Mengersen, Paul Wu, Christopher\n  Drovandi", "title": "Combined parameter and state inference with automatically calibrated ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models contain time-indexed parameters, termed states, as well as\nstatic parameters, simply termed parameters. The problem of inferring both\nstatic parameters as well as states simultaneously, based on time-indexed\nobservations, is the subject of much recent literature. This problem is\ncompounded once we consider models with intractable likelihoods. In these\nsituations, some emerging approaches have incorporated existing likelihood-free\ntechniques for static parameters, such as approximate Bayesian computation\n(ABC) into likelihood-based algorithms for combined inference of parameters and\nstates. These emerging approaches currently require extensive manual\ncalibration of a time-indexed tuning parameter: the acceptance threshold.\n  We design an SMC$^2$ algorithm (Chopin et al., 2013, JRSS B) for\nlikelihood-free approximation with automatically tuned thresholds. We prove\nconsistency of the algorithm and discuss the proposed calibration. We\ndemonstrate this algorithm's performance with three examples. We begin with two\nexamples of state space models. The first example is a toy example, with an\nemission distribution that is a skew normal distribution. The second example is\na stochastic volatility model involving an intractable stable distribution. The\nlast example is the most challenging; it deals with an inhomogeneous Hawkes\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:03:44 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 22:58:27 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Ebert", "Anthony", ""], ["Pudlo", "Pierre", ""], ["Mengersen", "Kerrie", ""], ["Wu", "Paul", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1910.14233", "submitter": "Debangan Dey", "authors": "Debangan Dey, Vadim Zipunnikov", "title": "Connecting population-level AUC and latent scale-invariant $R^2$ via\n  Semiparametric Gaussian Copula and rank correlations", "comments": "4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Area Under the Curve (AUC) is arguably the most popular measure of\nclassification accuracy. We use a semiparametric framework to introduce a\nlatent scale-invariant $R^2$, a novel measure of variation explained for an\nobserved binary outcome and an observed continuous predictor, and then directly\nlink the latent $R^2$ to AUC. This enables a mutually consistent simultaneous\nuse of AUC as a measure of classification accuracy and the latent $R^2$ as a\nscale-invariant measure of explained variation. Specifically, we employ\nSemiparametric Gaussian Copula (SGC) to model a joint dependence between\nobserved binary outcome and observed continuous predictor via the correlation\nof latent standard normal random variables. Under SGC, we show how, both\npopulation-level AUC and latent scale-invariant $R^2$, defined as a squared\nlatent correlation, can be estimated using any of the four rank statistics\ncalculated on binary-continuous pairs: Wilcoxon rank-sum, Kendall's Tau,\nSpearman's Rho, and Quadrant rank correlations. We then focus on three\nimplications and applications: i) we explicitly show that under SGC, the\npopulation-level AUC and the population-level latent $R^2$ are related via a\nmonotone function that depends on the population-level prevalence rate, ii) we\npropose Quadrant rank correlation as a robust semiparametric version of AUC;\niii) we demonstrate how, under complex-survey designs, Wilcoxon rank sum\nstatistics and Spearman and Quadrant rank correlations provide asymptotically\nconsistent estimators of the population-level AUC using only single-participant\nsurvey weights. We illustrate these applications using binary outcome of\nfive-year mortality and continuous predictors including Albumin, Systolic Blood\nPressure, and accelerometry-derived measures of total volume of physical\nactivity collected in 2003-2006 National Health and Nutrition Examination\nSurvey (NHANES) cohorts.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:25:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Dey", "Debangan", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1910.14498", "submitter": "Yicheng Zeng", "authors": "Yicheng Zeng and Lixing Zhu", "title": "Order Determination for Spiked Models", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by dimension reduction in regression analysis and signal detection,\nwe investigate the order determination for large dimension matrices including\nspiked models of which the numbers of covariates are proportional to the sample\nsizes for different models. Because the asymptotic behaviour of the estimated\neigenvalues of the corresponding matrices differ completely from those in fixed\ndimension scenarios, we then discuss the largest possible number we can\nidentify and introduce a \"valley-cliff\" criterion. We propose two versions of\nthe criterion: one based on the original differences of eigenvalues and the\nother based on the transformed differences, which reduces the effects of ridge\nselection in the former one. This generic method is very easy to implement and\ncomputationally inexpensive, and it can be applied to various matrices. As\nexamples, we focus on spiked population models, spiked Fisher matrices and\nfactor models with auto-covariance matrices. Numerical studies are conducted to\nexamine the method's finite sample performances and to compare it with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 14:35:03 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Zeng", "Yicheng", ""], ["Zhu", "Lixing", ""]]}, {"id": "1910.14590", "submitter": "Rom\\'an Salmer\\'on", "authors": "Rom\\'an Salmer\\'on and Catalina Garc\\'ia and Jos\\'e Garc\\'ia", "title": "\"multiColl\": An R package to detect multicollinearity", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a guide for the use of some of the functions of the R\npackage \"multiColl\" for the detection of near multicollinearity. The main\ncontribution, in comparison to other existing packages in R or other\neconometric software, is the treatment of qualitative independent variables and\nthe intercept in the simple/multiple linear regression model.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:43:02 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Salmer\u00f3n", "Rom\u00e1n", ""], ["Garc\u00eda", "Catalina", ""], ["Garc\u00eda", "Jos\u00e9", ""]]}]