[{"id": "1803.00001", "submitter": "Ngom Papa", "authors": "Mactar Ndaw, Macoumba Ndour, Papa Ngom", "title": "The Alpha-Beta-Symetric Divergence and their Positive Definite Kernel", "comments": "1o pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the field of Hilbertian metrics and positive definit\n(pd) kernels on probability measures, they have a real interest in kernel\nmethods. Firstly we will make a study based on the Alpha-Beta-divergence to\nhave a Hilbercan metric by proposing an improvement of this divergence by\nconstructing it so that its is symmetrical the Alpha-Beta-Symmetric-divergence\n(ABS-divergence) and also do some studies on these properties but also propose\nthe kernels associated with this divergence. Secondly we will do mumerical\nstudies incorporating all proposed metrics/kernels into support vector machine\n(SVM). Finally we presented a algorithm for image classification by using our\ndivergence.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 18:10:26 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 22:19:15 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Ndaw", "Mactar", ""], ["Ndour", "Macoumba", ""], ["Ngom", "Papa", ""]]}, {"id": "1803.00138", "submitter": "Mostafa Reisi Gahrooei", "authors": "Mostafa Reisi Gahrooei, Hao Yan, Kamran Paynabar, Jianjun Shi", "title": "A novel approach for fusion of heterogeneous sources of data", "comments": null, "journal-ref": "Technometrics, 2020", "doi": "10.1080/00401706.2019.1708463", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advancements in sensor technology, a heterogeneous set of data,\ncontaining samples of scalar, waveform signal, image, or even structured point\ncloud are becoming increasingly popular. Developing a statistical model,\nrepresenting the behavior of the underlying system based upon such a\nheterogeneous set of data can be used in monitoring, control, and optimization\nof the system. Unfortunately, available methods only focus on the scalar and\ncurve data and do not provide a general framework that can integrate different\nsources of data to construct a model. This paper poses the problem of\nestimating a process output, measured by a scalar, curve, an image, or a point\ncloud by a set of heterogeneous process variables such as scalar process\nsetting, sensor readings, and images. We introduce a general approach in which\neach set of input data (predictor) as well as the output measurements are\nrepresented by tensors. We formulate a linear regression model between the\ninput and output tensors and estimate the parameters by minimizing a least\nsquare loss function. In order to avoid overfitting and to reduce the number of\nparameters to be estimated, we decompose the model parameters using several\nbases, spanning the input and output spaces. Next, we learn both the bases and\ntheir spanning coefficients when minimizing the loss function using an\nalternating least square (ALS) algorithm. We show that such a minimization has\na closed-form solution in each iteration and can be computed very efficiently.\nThrough several simulation and case studies, we evaluate the performance of the\nproposed method. The results reveal the advantage of the proposed method over\nsome benchmarks in the literature in terms of the mean square prediction error.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 23:55:12 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 16:09:20 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 01:00:47 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gahrooei", "Mostafa Reisi", ""], ["Yan", "Hao", ""], ["Paynabar", "Kamran", ""], ["Shi", "Jianjun", ""]]}, {"id": "1803.00257", "submitter": "Leon Abdillah", "authors": "Ansari Saleh Ahmar, Suryo Guritno, Abdurakhman, Abdul Rahman, Awi,\n  Alimuddin, Ilham Minggi, M. Arif Tiro, M. Kasim Aidid, Suwardi Annas, Dian\n  Utami Sutiksno, S. Ahmar Dewi, H. Ahmar Kurniawan, A. Abqary Ahmar, Ahmad\n  Zaki, Dahlan Abdullah, Robbi Rahim, Heri Nurdiyanto, Rahmat Hidayat, Darmawan\n  Napitupulu, Janner Simarmata, Nuning Kurniasih, Leon Andretti Abdillah, Andri\n  Pranolo, Haviluddin, Wahyudin Albra, A. Nurani M Arifin", "title": "Modeling Data Containing Outliers using ARIMA Additive Outlier\n  (ARIMA-AO)", "comments": "13 pages", "journal-ref": "A. S. Ahmar, et al., \"Modeling Data Containing Outliers using\n  ARIMA Additive Outlier (ARIMA-AO),\" Journal of Physics: Conference Series,\n  vol. 954, p. 012010, 2018", "doi": "10.1088/1742-6596/954/1/012010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim this study is discussed on the detection and correction of data\ncontaining the additive outlier (AO) on the model ARIMA (p, d, q). The process\nof detection and correction of data using an iterative procedure popularized by\nBox, Jenkins, and Reinsel (1994). By using this method we obtained an ARIMA\nmodels were fit to the data containing AO, this model is added to the original\nmodel of ARIMA coefficients obtained from the iteration process using\nregression methods. This shows that there is an improvement of forecasting\nerror rate data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 09:02:58 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Ahmar", "Ansari Saleh", ""], ["Guritno", "Suryo", ""], ["Abdurakhman", "", ""], ["Rahman", "Abdul", ""], ["Awi", "", ""], ["Alimuddin", "", ""], ["Minggi", "Ilham", ""], ["Tiro", "M. Arif", ""], ["Aidid", "M. Kasim", ""], ["Annas", "Suwardi", ""], ["Sutiksno", "Dian Utami", ""], ["Dewi", "S. Ahmar", ""], ["Kurniawan", "H. Ahmar", ""], ["Ahmar", "A. Abqary", ""], ["Zaki", "Ahmad", ""], ["Abdullah", "Dahlan", ""], ["Rahim", "Robbi", ""], ["Nurdiyanto", "Heri", ""], ["Hidayat", "Rahmat", ""], ["Napitupulu", "Darmawan", ""], ["Simarmata", "Janner", ""], ["Kurniasih", "Nuning", ""], ["Abdillah", "Leon Andretti", ""], ["Pranolo", "Andri", ""], ["Haviluddin", "", ""], ["Albra", "Wahyudin", ""], ["Arifin", "A. Nurani M", ""]]}, {"id": "1803.00276", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Hien D. Nguyen", "title": "Model-Based Clustering and Classification of Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of complex data analysis is a central topic of modern statistical\nscience and learning systems and is becoming of broader interest with the\nincreasing prevalence of high-dimensional data. The challenge is to develop\nstatistical models and autonomous algorithms that are able to acquire knowledge\nfrom raw data for exploratory analysis, which can be achieved through\nclustering techniques or to make predictions of future data via classification\n(i.e., discriminant analysis) techniques. Latent data models, including mixture\nmodel-based approaches are one of the most popular and successful approaches in\nboth the unsupervised context (i.e., clustering) and the supervised one (i.e,\nclassification or discrimination). Although traditionally tools of multivariate\nanalysis, they are growing in popularity when considered in the framework of\nfunctional data analysis (FDA). FDA is the data analysis paradigm in which the\nindividual data units are functions (e.g., curves, surfaces), rather than\nsimple vectors. In many areas of application, the analyzed data are indeed\noften available in the form of discretized values of functions or curves (e.g.,\ntime series, waveforms) and surfaces (e.g., 2d-images, spatio-temporal data).\nThis functional aspect of the data adds additional difficulties compared to the\ncase of a classical multivariate (non-functional) data analysis. We review and\npresent approaches for model-based clustering and classification of functional\ndata. We derive well-established statistical models along with efficient\nalgorithmic tools to address problems regarding the clustering and the\nclassification of these high-dimensional data, including their heterogeneity,\nmissing information, and dynamical hidden structure. The presented models and\nalgorithms are illustrated on real-world functional data analysis problems from\nseveral application area.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 10:02:13 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 04:03:28 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien D.", ""]]}, {"id": "1803.00360", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "Computing Bayes factors to measure evidence from experiments: An\n  extension of the BIC approximation", "comments": "to appear in Biometrical Letters", "journal-ref": "Biometrical Letters 55 (2018) 31-43", "doi": "10.2478/bile-2018-0003", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference affords scientists with powerful tools for testing\nhypotheses. One of these tools is the Bayes factor, which indexes the extent to\nwhich support for one hypothesis over another is updated after seeing the data.\nPart of the hesitance to adopt this approach may stem from an unfamiliarity\nwith the computational tools necessary for computing Bayes factors. Previous\nwork has shown that closed form approximations of Bayes factors are relatively\neasy to obtain for between- groups methods, such as an analysis of variance or\nt-test. In this paper, I extend this approximation to develop a formula for the\nBayes factor that directly uses information that is typically reported for\nANOVAs (e.g., the F ratio and degrees of freedom). After giving two examples of\nits use, I report the results of simulations which show that even with minimal\ninput, this approximate Bayes factor produces similar results to existing\nsoftware solutions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 13:44:59 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 22:43:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1803.00609", "submitter": "Alberto Abadie", "authors": "Alberto Abadie", "title": "On Statistical Non-Significance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significance tests are probably the most extended form of inference in\nempirical research, and significance is often interpreted as providing greater\ninformational content than non-significance. In this article we show, however,\nthat rejection of a point null often carries very little information, while\nfailure to reject may be highly informative. This is particularly true in\nempirical contexts where data sets are large and where there are rarely reasons\nto put substantial prior probability on a point null. Our results challenge the\nusual practice of conferring point null rejections a higher level of scientific\nsignificance than non-rejections. In consequence, we advocate a visible\nreporting and discussion of non-significant results in empirical practice.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 20:15:28 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Abadie", "Alberto", ""]]}, {"id": "1803.00698", "submitter": "Kellie Ottoboni", "authors": "Mark Lindeman, Neal McBurnett, Kellie Ottoboni, Philip B. Stark", "title": "Next Steps for the Colorado Risk-Limiting Audit (CORLA) Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorado conducted risk-limiting tabulation audits (RLAs) across the state in\n2017, including both ballot-level comparison audits and ballot-polling audits.\nThose audits only covered contests restricted to a single county; methods to\nefficiently audit contests that cross county boundaries and combine ballot\npolling and ballot-level comparisons have not been available.\n  Colorado's current audit software (RLATool) needs to be improved to audit\nthese contests that cross county lines and to audit small contests efficiently.\n  This paper addresses these needs. It presents extremely simple but\ninefficient methods, more efficient methods that combine ballot polling and\nballot-level comparisons using stratified samples, and methods that combine\nballot-level comparison and variable-size batch comparison audits in a way that\ndoes not require stratified sampling.\n  We conclude with some recommendations, and illustrate our recommended method\nusing examples that compare them to existing approaches. Exemplar open-source\ncode and interactive Jupyter notebooks are provided that implement the methods\nand allow further exploration.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 03:46:37 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Lindeman", "Mark", ""], ["McBurnett", "Neal", ""], ["Ottoboni", "Kellie", ""], ["Stark", "Philip B.", ""]]}, {"id": "1803.00715", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Sivaraman Balakrishnan, Larry Wasserman", "title": "Robust Multivariate Nonparametric Tests via Projection-Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we generalize the Cram\\'er-von Mises statistic via\nprojection-averaging to obtain a robust test for the multivariate two-sample\nproblem. The proposed test is consistent against all fixed alternatives, robust\nto heavy-tailed data and minimax rate optimal against a certain class of\nalternatives. Our test statistic is completely free of tuning parameters and is\ncomputationally efficient even in high dimensions. When the dimension tends to\ninfinity, the proposed test is shown to have comparable power to the existing\nhigh-dimensional mean tests under certain location models. As a by-product of\nour approach, we introduce a new metric called the angular distance which can\nbe thought of as a robust alternative to the Euclidean distance. Using the\nangular distance, we connect the proposed method to the reproducing kernel\nHilbert space approach. In addition to the Cram\\'er-von Mises statistic, we\ndemonstrate that the projection-averaging technique can be used to define\nrobust, multivariate tests in many other problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 04:19:33 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 15:58:58 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 16:18:29 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kim", "Ilmun", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1803.00798", "submitter": "Federico Bugni", "authors": "Federico A. Bugni, Joel L. Horowitz", "title": "Permutation Tests for Equality of Distributions of Functional Data", "comments": "49 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic data are often generated by stochastic processes that take place in\ncontinuous time, though observations may occur only at discrete times. For\nexample, electricity and gas consumption take place in continuous time. Data\ngenerated by a continuous time stochastic process are called functional data.\nThis paper is concerned with comparing two or more stochastic processes that\ngenerate functional data. The data may be produced by a randomized experiment\nin which there are multiple treatments. The paper presents a method for testing\nthe hypothesis that the same stochastic process generates all the functional\ndata. The test described here applies to both functional data and multiple\ntreatments. It is implemented as a combination of two permutation tests. This\nensures that in finite samples, the true and nominal probabilities that each\ntest rejects a correct null hypothesis are equal. The paper presents upper and\nlower bounds on the asymptotic power of the test under alternative hypotheses.\nThe results of Monte Carlo experiments and an application to an experiment on\nbilling and pricing of natural gas illustrate the usefulness of the test.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 10:39:58 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 19:16:12 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 11:07:10 GMT"}, {"version": "v4", "created": "Mon, 14 Jun 2021 11:26:30 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bugni", "Federico A.", ""], ["Horowitz", "Joel L.", ""]]}, {"id": "1803.00889", "submitter": "Audrey Repetti", "authors": "Audrey Repetti, Marcelo Pereyra and Yves Wiaux", "title": "Scalable Bayesian uncertainty quantification in imaging inverse problems\n  via convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian uncertainty quantification method for large-scale\nimaging inverse problems. Our method applies to all Bayesian models that are\nlog-concave, where maximum-a-posteriori (MAP) estimation is a convex\noptimization problem. The method is a framework to analyse the confidence in\nspecific structures observed in MAP estimates (e.g., lesions in medical\nimaging, celestial sources in astronomical imaging), to enable using them as\nevidence to inform decisions and conclusions. Precisely, following Bayesian\ndecision theory, we seek to assert the structures under scrutiny by performing\na Bayesian hypothesis test that proceeds as follows: firstly, it postulates\nthat the structures are not present in the true image, and then seeks to use\nthe data and prior knowledge to reject this null hypothesis with high\nprobability. Computing such tests for imaging problems is generally very\ndifficult because of the high dimensionality involved. A main feature of this\nwork is to leverage probability concentration phenomena and the underlying\nconvex geometry to formulate the Bayesian hypothesis test as a convex problem,\nthat we then efficiently solve by using scalable optimization algorithms. This\nallows scaling to high-resolution and high-sensitivity imaging problems that\nare computationally unaffordable for other Bayesian computation approaches. We\nillustrate our methodology, dubbed BUQO (Bayesian Uncertainty Quantification by\nOptimization), on a range of challenging Fourier imaging problems arising in\nastronomy and medicine.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 15:24:55 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 12:26:04 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Repetti", "Audrey", ""], ["Pereyra", "Marcelo", ""], ["Wiaux", "Yves", ""]]}, {"id": "1803.01150", "submitter": "Richard Samworth", "authors": "Yi Yu, Jelena Bradic and Richard J. Samworth", "title": "Confidence intervals for high-dimensional Cox models", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in high-dimensional Cox proportional hazards regression\nmodels where the number of covariates may be larger than the sample size. Our\ndebiased estimator construction is similar to those in Zhang and Zhang (2014)\nand van de Geer et al. (2014), but the time-dependent covariates and censored\nrisk sets introduce considerable additional challenges. Our theoretical\nresults, which provide conditions under which our confidence intervals are\nasymptotically valid, are supported by extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 12:30:01 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Yu", "Yi", ""], ["Bradic", "Jelena", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1803.01231", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Bayesian Projected Calibration of Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian approach called Bayesian projected calibration to\naddress the problem of calibrating an imperfect computer model using\nobservational data from a complex physical system. The calibration parameter\nand the physical system are parametrized in an identifiable fashion via\n$L_2$-projection. The physical process is assigned a Gaussian process prior,\nwhich naturally induces a prior distribution on the calibration parameter\nthrough the $L_2$-projection constraint. The calibration parameter is estimated\nthrough its posterior distribution, which provides a natural and non-asymptotic\nway for the uncertainty quantification. We provide a rigorous large sample\njustification for the proposed approach by establishing the asymptotic\nnormality of the posterior of the calibration parameter with the efficient\ncovariance matrix. In addition, two efficient computational algorithms based on\nstochastic approximation are designed with theoretical guarantees. Through\nextensive simulation studies and two real-world datasets analyses, we show that\nthe Bayesian projected calibration can accurately estimate the calibration\nparameters, appropriately calibrate the computer models, and compare favorably\nto alternative approaches.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:32:29 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 17:31:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1803.01381", "submitter": "Zhongzhi Lawrence He", "authors": "Zhongzhi Lawrence He", "title": "Generalized Information Ratio", "comments": "47 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alpha-based performance evaluation may fail to capture correlated residuals\ndue to model errors. This paper proposes using the Generalized Information\nRatio (GIR) to measure performance under misspecified benchmarks. Motivated by\nthe theoretical link between abnormal returns and residual covariance matrix,\nGIR is derived as alphas scaled by the inverse square root of residual\ncovariance matrix. GIR nests alphas and Information Ratio as special cases,\ndepending on the amount of information used in the residual covariance matrix.\nWe show that GIR is robust to various degrees of model misspecification and\nproduces stable out-of-sample returns. Incorporating residual correlations\nleads to substantial gains that alleviate model error concerns of active\nmanagement.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 17:04:49 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 02:42:17 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["He", "Zhongzhi Lawrence", ""]]}, {"id": "1803.01422", "submitter": "Bryon Aragam", "authors": "Xun Zheng, Bryon Aragam, Pradeep Ravikumar, Eric P. Xing", "title": "DAGs with NO TEARS: Continuous Optimization for Structure Learning", "comments": "22 pages, 8 figures, accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the structure of directed acyclic graphs (DAGs, also known as\nBayesian networks) is a challenging problem since the search space of DAGs is\ncombinatorial and scales superexponentially with the number of nodes. Existing\napproaches rely on various local heuristics for enforcing the acyclicity\nconstraint. In this paper, we introduce a fundamentally different strategy: We\nformulate the structure learning problem as a purely \\emph{continuous}\noptimization problem over real matrices that avoids this combinatorial\nconstraint entirely. This is achieved by a novel characterization of acyclicity\nthat is not only smooth but also exact. The resulting problem can be\nefficiently solved by standard numerical algorithms, which also makes\nimplementation effortless. The proposed method outperforms existing ones,\nwithout imposing any structural assumptions on the graph such as bounded\ntreewidth or in-degree. Code implementing the proposed algorithm is open-source\nand publicly available at https://github.com/xunzheng/notears.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 21:09:13 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 01:29:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zheng", "Xun", ""], ["Aragam", "Bryon", ""], ["Ravikumar", "Pradeep", ""], ["Xing", "Eric P.", ""]]}, {"id": "1803.01639", "submitter": "Tuomas Rajala", "authors": "T. Rajala and S. Olhede and D.J. Murrell", "title": "When do we have the power to detect biological interactions in spatial\n  point patterns?", "comments": "Main text 18 pages on 12pt font, 4 figures. Appendix 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the relative importance of environmental factors, biotic\ninteractions and stochasticity in assembling and maintaining species-rich\ncommunities remains a major challenge in ecology. In plant communities,\ninteractions between individuals of different species are expected to leave a\nspatial signature in the form of positive or negative spatial correlations over\ndistances relating to the spatial scale of interaction. Most studies using\nspatial point process tools have found relatively little evidence for\ninteractions between pairs of species. More interactions tend to be detected in\ncommunities with fewer species. However, there is currently no understanding of\nhow the power to detect spatial interactions may change with sample size, or\nthe scale and intensity of interactions.\n  We use a simple 2-species model where the scale and intensity of interactions\nare controlled to simulate point pattern data. In combination with an\napproximation to the variance of the spatial summary statistics that we sample,\nwe investigate the power of current spatial point pattern methods to correctly\nreject the null model of bivariate species independence.\n  We show that the power to detect interactions is positively related to the\nabundances of the species tested, and the intensity and scale of interactions.\nIncreasing imbalance in abundances has a negative effect on the power to detect\ninteractions. At population sizes typically found in currently available\ndatasets for species-rich plant communities we find only a very low power to\ndetect interactions. Differences in power may explain the increased frequency\nof interactions in communities with fewer species. Furthermore, the\ncommunity-wide frequency of detected interactions is very sensitive to a\nminimum abundance criterion for including species in the analyses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 12:50:19 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 10:35:54 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Rajala", "T.", ""], ["Olhede", "S.", ""], ["Murrell", "D. J.", ""]]}, {"id": "1803.01699", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao, Yingying Ma, Hansheng Wang, Qiwei Yao", "title": "Banded Spatio-Temporal Autoregressions", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of spatio-temporal models with unknown and banded\nautoregressive coefficient matrices. The setting represents a sparse structure\nfor high-dimensional spatial panel dynamic models when panel members represent\neconomic (or other type) individuals at many different locations. The structure\nis practically meaningful when the order of panel members is arranged\nappropriately. Note that the implied autocovariance matrices are unlikely to be\nbanded, and therefore, the proposal is radically different from the existing\nliterature on the inference for high-dimensional banded covariance matrices.\nDue to the innate endogeneity, we apply the least squares method based on a\nYule-Walker equation to estimate autoregressive coefficient matrices. The\nestimators based on multiple Yule-Walker equations are also studied. A\nratio-based method for determining the bandwidth of autoregressive matrices is\nalso proposed. Some asymptotic properties of the inference methods are\nestablished. The proposed methodology is further illustrated using both\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 14:30:41 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Ma", "Yingying", ""], ["Wang", "Hansheng", ""], ["Yao", "Qiwei", ""]]}, {"id": "1803.01984", "submitter": "Matt Johnson", "authors": "Matthew C. Johnson and Mike West", "title": "Bayesian Predictive Synthesis: Forecast Calibration and Combination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of forecast densities, whether they result from a set of\nmodels, a group of consulted experts, or other sources, is becoming\nincreasingly important in the fields of economics, policy and finance, among\nothers. Requiring methodology that goes beyond standard Bayesian model\nuncertainty and model mixing - with its well-known limitations based on a\nclearly proscribed theoretical basis - multiple 'density combination' methods\nhave been proposed. While some proposals have demonstrated empirical success,\nmost apparently lack a core philosophical and theoretical foundation.\nInteresting recent examples generalize the common 'linear opinion pool' with\nflexible mixing weights that depend on the forecast variable itself - i.e.,\noutcome-dependent mixing. Taking a foundational subjective Bayesian\nperspective, we show that such a density combination scheme is in fact\njustified as one example of Bayesian agent opinion analysis, or 'predictive\nsynthesis'. This logically coherent framework clearly delineates the underlying\nassumptions as well as the theoretical constraints and limitations of many\ncombination 'rules', defining a broad class of Bayesian models for the general\nproblem. A number of examples, including an application to a set of predictive\ndensities in foreign exchange, provide illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 02:10:36 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 01:46:30 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Johnson", "Matthew C.", ""], ["West", "Mike", ""]]}, {"id": "1803.01999", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi", "title": "ABC and Indirect Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  Indirect inference (II) is a classical likelihood-free approach that\npre-dates the main developments of ABC and relies on simulation from a\nparametric model of interest to determine point estimates of the parameters. It\nis not surprising then that some likelihood-free Bayesian approaches have\nharnessed the II literature. This chapter provides an introduction to II and\ndetails the connections between ABC and II. A particular focus is placed on the\nuse of an auxiliary model with a tractable likelihood function, an approach\ncommonly adopted in the II literature, to facilitate likelihood-free Bayesian\ninferences.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 03:15:50 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Drovandi", "Christopher C", ""]]}, {"id": "1803.02024", "submitter": "Peng Ding", "authors": "Fan Yang, Peng Ding", "title": "Using Survival Information in Truncation by Death Problems Without the\n  Monotonicity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some randomized clinical trials, patients may die before the measurements\nof their outcomes. Even though randomization generates comparable treatment and\ncontrol groups, the remaining survivors often differ significantly in\nbackground variables that are prognostic to the outcomes. This is called the\ntruncation by death problem. Under the potential outcomes framework, the only\nwell-defined causal effect on the outcome is within the subgroup of patients\nwho would always survive under both treatment and control. Because the\ndefinition of the subgroup depends on the potential values of the survival\nstatus that could not be observed jointly, without making strong parametric\nassumptions, we cannot identify the causal effect of interest and consequently\ncan only obtain bounds of it. Unfortunately, however, many bounds are too wide\nto be useful. We propose to use detailed survival information before and after\nthe measurements of the outcomes to sharpen the bounds of the subgroup causal\neffect. Because survival times contain useful information about the final\noutcome, carefully utilizing them could improve statistical inference without\nimposing strong parametric assumptions. Moreover, we propose to use a copula\nmodel to relax the commonly-invoked but often doubtful monotonicity assumption\nthat the treatment extends the survival time for all patients.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 05:55:34 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Yang", "Fan", ""], ["Ding", "Peng", ""]]}, {"id": "1803.02302", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Michael G. Hudgens, John D. Clemens, Mohammad Ali,\n  Michael E. Emch", "title": "Randomization inference with general interference and censoring", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13125", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference occurs between individuals when the treatment (or exposure) of\none individual affects the outcome of another individual. Previous work on\ncausal inference methods in the presence of interference has focused on the\nsetting where a priori it is assumed there is 'partial interference,' in the\nsense that individuals can be partitioned into groups wherein there is no\ninterference between individuals in different groups. Bowers, Fredrickson, and\nPanagopoulos (2012) and Bowers, Fredrickson, and Aronow (2016) consider\nrandomization-based inferential methods that allow for more general\ninterference structures in the context of randomized experiments. In this\npaper, extensions of Bowers et al. which allow for failure time outcomes\nsubject to right censoring are proposed. Permitting right censored outcomes is\nchallenging because standard randomization-based tests of the null hypothesis\nof no treatment effect assume that whether an individual is censored does not\ndepend on treatment. The proposed extension of Bowers et al. to allow for\ncensoring entails adapting the method of Wang, Lagakos, and Gray (2010) for two\nsample survival comparisons in the presence of unequal censoring. The methods\nare examined via simulation studies and utilized to assess the effects of\ncholera vaccination in an individually-randomized trial of 73,000 children and\nwomen in Matlab, Bangladesh.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 17:08:22 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 18:05:53 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 08:37:30 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Hudgens", "Michael G.", ""], ["Clemens", "John D.", ""], ["Ali", "Mohammad", ""], ["Emch", "Michael E.", ""]]}, {"id": "1803.02488", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Eric D. Kolaczyk and Qiwei Yao", "title": "Estimation of subgraph density in noisy networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is common practice in applied network analysis to report various\nstandard network summary statistics, these numbers are rarely accompanied by\nuncertainty quantification. Yet any error inherent in the measurements\nunderlying the construction of the network, or in the network construction\nprocedure itself, necessarily must propagate to any summary statistics\nreported. Here we study the problem of estimating the density of an arbitrary\nsubgraph, given a noisy version of some underlying network as data. Under a\nsimple model of network error, we show that consistent estimation of such\ndensities is impossible when the rates of error are unknown and only a single\nnetwork is observed. Accordingly, we develop method-of-moment estimators of\nnetwork subgraph densities and error rates for the case where a minimal number\nof network replicates are available. These estimators are shown to be\nasymptotically normal as the number of vertices increases to infinity. We also\nprovide confidence intervals for quantifying the uncertainty in these estimates\nbased on the asymptotic normality. To construct the confidence intervals, a new\nand non-standard bootstrap method is proposed to compute asymptotic variances,\nwhich is infeasible otherwise. We illustrate the proposed methods in the\ncontext of gene coexpression networks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 00:33:41 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 12:43:01 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 15:38:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chang", "Jinyuan", ""], ["Kolaczyk", "Eric D.", ""], ["Yao", "Qiwei", ""]]}, {"id": "1803.02532", "submitter": "Norbert Remenyi", "authors": "Norbert Rem\\'enyi and Brani Vidakovic", "title": "Bayesian nonparametric regression using complex wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new adaptive wavelet denoising methodology using\ncomplex wavelets. The method is based on a fully Bayesian hierarchical model in\nthe complex wavelet domain that uses a bivariate mixture prior on the wavelet\ncoefficients. The heart of the procedure is computational, where the posterior\nmean is computed through Markov chain Monte Carlo (MCMC) simulations. We show\nthat the method has good performance, as demonstrated by simulations on the\nwell-known test functions and by comparison to a well-established complex\nwavelet-based denoising procedure. An application to real-life data set is also\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 05:56:14 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Rem\u00e9nyi", "Norbert", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1803.02575", "submitter": "Xiaowei Zhang", "authors": "Liang Ding and Xiaowei Zhang", "title": "Scalable Stochastic Kriging with Markovian Covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic kriging is a popular technique for simulation metamodeling due to\nits exibility and analytical tractability. Its computational bottleneck is the\ninversion of a covariance matrix, which takes $O(n^3)$ time in general and\nbecomes prohibitive for large n, where n is the number of design points.\nMoreover, the covariance matrix is often ill-conditioned for large n, and thus\nthe inversion is prone to numerical instability, resulting in erroneous\nparameter estimation and prediction. These two numerical issues preclude the\nuse of stochastic kriging at a large scale. This paper presents a novel\napproach to address them. We construct a class of covariance functions, called\nMarkovian covariance functions (MCFs), which have two properties: (i) the\nassociated covariance matrices can be inverted analytically, and (ii) the\ninverse matrices are sparse. With the use of MCFs, the inversion-related\ncomputational time is reduced to $O(n^2)$ in general, and can be further\nreduced by orders of magnitude with additional assumptions on the simulation\nerrors and design points. The analytical invertibility also enhance the\nnumerical stability dramatically. The key in our approach is that we identify a\ngeneral functional form of covariance functions that can induce sparsity in the\ncorresponding inverse matrices. We also establish a connection between MCFs and\nlinear ordinary differential equations. Such a connection provides a flexible,\nprincipled approach to constructing a wide class of MCFs. Extensive numerical\nexperiments demonstrate that stochastic kriging with MCFs can handle\nlarge-scale problems in an both computationally efficient and numerically\nstable manner.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 09:53:13 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Ding", "Liang", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1803.02596", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang", "title": "Revisiting differentially private linear regression: optimal and\n  adaptive prediction & estimation in unbounded domain", "comments": "Uncertainty in Artificial Intelligence (UAI-2018), Monterey, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of linear regression under a differential privacy\nconstraint. By consolidating existing pieces in the literature, we clarify the\ncorrect dependence of the feature, label and coefficient domains in the\noptimization error and estimation error, hence revealing the delicate price of\ndifferential privacy in statistical estimation and statistical learning.\nMoreover, we propose simple modifications of two existing DP algorithms: (a)\nposterior sampling, (b) sufficient statistics perturbation, and show that they\ncan be upgraded into **adaptive** algorithms that are able to exploit\ndata-dependent quantities and behave nearly optimally **for every instance**.\nExtensive experiments are conducted on both simulated data and real data, which\nconclude that both AdaOPS and AdaSSP outperform the existing techniques on\nnearly all 36 data sets that we test on.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 11:03:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 10:56:30 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Wang", "Yu-Xiang", ""]]}, {"id": "1803.02704", "submitter": "Felix Bestehorn", "authors": "Felix Bestehorn and Maike Bestehorn and Markus Bestehorn and Christian\n  Kirches", "title": "A deterministic balancing score algorithm to avoid common pitfalls of\n  propensity score matching", "comments": "25 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching (PSM) is the de-facto standard for estimating\ncausal effects in observational studies. We show that PSM and its\nimplementations are susceptible to several major drawbacks and illustrate these\nfindings using a case study with $17,427$ patients. We derive four formal\nproperties an optimal statistical matching algorithm should meet, and propose\nDeterministic Balancing Score exact Matching (DBSeM) which meets the\naforementioned properties for an exact matching. Furthermore, we investigate\none of the main problems of PSM, that is that common PSM results in one valid\nset of matched pairs or a bootstrapped PSM in a selection of possible valid\nsets of matched pairs. For exact matchings we provide the mathematical proof,\nthat DBSeM, as a result, delivers the expected value of all valid sets of\nmatched pairs for the investigated dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:08:08 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 06:52:13 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 15:43:47 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 15:06:06 GMT"}, {"version": "v5", "created": "Fri, 17 May 2019 12:53:28 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Bestehorn", "Felix", ""], ["Bestehorn", "Maike", ""], ["Bestehorn", "Markus", ""], ["Kirches", "Christian", ""]]}, {"id": "1803.02714", "submitter": "Gaorong Li", "authors": "Sanying Feng, Gaorong Li, Heng Peng, Tiejun Tong", "title": "Varying Coefficient Panel Data Model with Interactive Fixed Effects", "comments": "58 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a varying coefficient panel data model with\nunobservable multiple interactive fixed effects that are correlated with the\nregressors. We approximate each coefficient function by B-spline, and propose a\nrobust nonlinear iteration scheme based on the least squares method to estimate\nthe coefficient functions of interest. We also establish the asymptotic theory\nof the resulting estimators under certain regularity assumptions, including the\nconsistency, the convergence rate and the asymptotic distribution. Furthermore,\nwe develop a least squares dummy variable method to study an important special\ncase of the proposed model: the varying coefficient panel data model with\nadditive fixed effects. To construct the pointwise confidence intervals for the\ncoefficient functions, a residual-based block bootstrap method is proposed to\nreduce the computational burden as well as to avoid the accumulative errors.\nSimulation studies and a real data analysis are also carried out to assess the\nperformance of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:27:39 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Feng", "Sanying", ""], ["Li", "Gaorong", ""], ["Peng", "Heng", ""], ["Tong", "Tiejun", ""]]}, {"id": "1803.02734", "submitter": "John Hughes", "authors": "John Hughes", "title": "Sklar's Omega: A Gaussian Copula-Based Framework for Assessing Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical measurement of agreement is important in a number of fields,\ne.g., content analysis, education, computational linguistics, biomedical\nimaging. We propose Sklar's Omega, a Gaussian copula-based framework for\nmeasuring intra-coder, inter-coder, and inter-method agreement as well as\nagreement relative to a gold standard. We demonstrate the efficacy and\nadvantages of our approach by applying it to both simulated and experimentally\nobserved datasets, including data from two medical imaging studies. Application\nof our proposed methodology is supported by our open-source R package,\nsklarsomega, which is available for download from the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:03:07 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Hughes", "John", ""]]}, {"id": "1803.02764", "submitter": "Andreas Hagemann", "authors": "Andreas Hagemann", "title": "Placebo inference on treatment effects when the number of clusters is\n  small", "comments": "37 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1016/j.jeconom.2019.04.011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a general, Fisher-style randomization testing framework to\nconduct nearly exact inference about the lack of effect of a binary treatment\nin the presence of very few, large clusters when the treatment effect is\nidentified across clusters. The proposed randomization test formalizes and\nextends the intuitive notion of generating null distributions by assigning\nplacebo treatments to untreated clusters. I show that under simple and easily\nverifiable conditions, the placebo test leads to asymptotically valid inference\nin a very large class of empirically relevant models. Examples discussed\nexplicitly are (i) least squares regression with cluster-level treatment, (ii)\ndifference-in-differences estimation, and (iii) binary choice models with\ncluster-level treatment. A simulation study and an empirical example are\nprovided. The proposed inference procedure is easy to implement and performs\nwell with as few as three treated and three untreated clusters.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:55:47 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hagemann", "Andreas", ""]]}, {"id": "1803.02876", "submitter": "Jean Pouget-Abadie", "authors": "Jean Pouget-Abadie and David C. Parkes and Vahab Mirrokni and Edoardo\n  M. Airoldi", "title": "Optimizing cluster-based randomized experiments under a monotonicity\n  assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-based randomized experiments are popular designs for mitigating the\nbias of standard estimators when interference is present and classical causal\ninference and experimental design assumptions (such as SUTVA or ITR) do not\nhold. Without an exact knowledge of the interference structure, it can be\nchallenging to understand which partitioning of the experimental units is\noptimal to minimize the estimation bias. In the paper, we introduce a\nmonotonicity condition under which a novel two-stage experimental design allows\nus to determine which of two cluster-based designs yields the least biased\nestimator. We then consider the setting of online advertising auctions and show\nthat reserve price experiments verify the monotonicity condition and the\nproposed framework and methodology applies. We validate our findings on an\nadvertising auction dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 21:07:51 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Parkes", "David C.", ""], ["Mirrokni", "Vahab", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1803.02881", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Luca Tardella", "title": "Algorithms and diagnostics for the analysis of preference rankings with\n  the Extended Plackett-Luce model", "comments": "14 pages, 2 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice behavior and preferences typically involve numerous and subjective\naspects that are difficult to be identified and quantified. For this reason,\ntheir exploration is frequently conducted through the collection of ordinal\nevidence in the form of ranking data. A ranking is an ordered sequence\nresulting from the comparative evaluation of a given set of items according to\na specific criterion. Multistage ranking models, including the popular\nPlackett-Luce distribution (PL), rely on the assumption that the ranking\nprocess is performed sequentially, by assigning the positions from the top to\nthe bottom one (forward order). A recent contribution to the ranking literature\nrelaxed this assumption with the addition of the discrete reference order\nparameter, yielding the novel Extended Plackett-Luce model (EPL). Inference on\nthe EPL and its generalization into a finite mixture framework was originally\naddressed from the frequentist perspective. In this work, we propose the\nBayesian estimation of the EPL with order constraints on the reference order\nparameter. The restrictions for the discrete parameter reflect a meaningful\nrank assignment process and, in combination with the data augmentation strategy\nand the conjugacy of the Gamma prior distribution with the EPL, facilitate the\nconstruction of a tuned joint Metropolis-Hastings algorithm within Gibbs\nsampling to simulate from the posterior distribution. We additionally propose a\nnovel model diagnostic to assess the adequacy of the EPL parametric\nspecification. The usefulness of the proposal is illustrated with applications\nto simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 21:24:15 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 17:20:13 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "1803.03085", "submitter": "Amelia Sim\\'o", "authors": "Amelia Sim\\'o, M. Victoria Ib\\'a\\~nez, Irene Epifanio and Vicent\n  Gimeno", "title": "Generalized partially linear models on Riemannian manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized partially linear models on Riemannian manifolds are\nintroduced. These models, like ordinary generalized linear models, are a\ngeneralization of partially linear models on Riemannian manifolds that allow\nfor response variables with error distribution models other than a normal\ndistribution. Partially linear models are particularly useful when some of the\ncovariates of the model are elements of a Riemannian manifold, because the\ncurvature of these spaces makes it difficult to define parametric models. The\nmodel was developed to address an interesting application, the prediction of\nchildren's garment fit based on 3D scanning of their body. For this reason, we\nfocus on logistic and ordinal models and on the important and difficult case\nwhere the Riemannian manifold is the three-dimensional case of Kendall's shape\nspace. An experimental study with a well-known 3D database is carried out to\ncheck the goodness of the procedure. Finally it is applied to a 3D database\nobtained from an anthropometric survey of the Spanish child population. A\ncomparative study with related techniques is carried out.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 13:42:23 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Sim\u00f3", "Amelia", ""], ["Ib\u00e1\u00f1ez", "M. Victoria", ""], ["Epifanio", "Irene", ""], ["Gimeno", "Vicent", ""]]}, {"id": "1803.03333", "submitter": "Maikol Sol\\'is", "authors": "Maikol Sol\\'is", "title": "Nonparametric estimation of the first order Sobol indices with bootstrap\n  bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that $Y = \\psi(X_1, \\ldots, X_p)$, where $(X_1,\\ldots, X_p)^\\top$ are\nrandom inputs, $Y$ is the output, and $\\psi(\\cdot)$ is an unknown link\nfunction. The Sobol indices gauge the sensitivity of each $X$ against $Y$ by\nestimating the regression curve's variability between them. In this paper, we\nestimate these curves with a kernel-based method. The method allows to estimate\nthe first order indices when the link between the independent and dependent\nvariables is unknown. The kernel-based methods need a bandwidth to average the\nobservations. For finite samples, the cross-validation method is famous to\ndecide this bandwidth. However, it produces a structural bias. To remedy this,\nwe propose a bootstrap procedure which reconstruct the model residuals and\nre-estimate the non-parametric regression curve. With the new set of curves,\nthe procedure corrects the bias in the Sobol index. To test the developed\nmethod, we implemented simulated numerical examples with complex functions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 23:22:38 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 23:29:51 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 21:56:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Sol\u00eds", "Maikol", ""]]}, {"id": "1803.03344", "submitter": "Matthew Dunlop", "authors": "Victor Chen, Matthew M. Dunlop, Omiros Papaspiliopoulos, Andrew M.\n  Stuart", "title": "Dimension-Robust MCMC in Bayesian Inverse Problems", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methodology developed in this article is motivated by a wide range of\nprediction and uncertainty quantification problems that arise in Statistics,\nMachine Learning and Applied Mathematics, such as non-parametric regression,\nmulti-class classification and inversion of partial differential equations. One\npopular formulation of such problems is as Bayesian inverse problems, where a\nprior distribution is used to regularize inference on a high-dimensional latent\nstate, typically a function or a field. It is common that such priors are\nnon-Gaussian, for example piecewise-constant or heavy-tailed, and/or\nhierarchical, in the sense of involving a further set of low-dimensional\nparameters, which, for example, control the scale or smoothness of the latent\nstate. In this formulation prediction and uncertainty quantification relies on\nefficient exploration of the posterior distribution of latent states and\nparameters. This article introduces a framework for efficient MCMC sampling in\nBayesian inverse problems that capitalizes upon two fundamental ideas in MCMC,\nnon-centred parameterisations of hierarchical models and dimension-robust\nsamplers for latent Gaussian processes. Using a range of diverse applications\nwe showcase that the proposed framework is dimension-robust, that is, the\nefficiency of the MCMC sampling does not deteriorate as the dimension of the\nlatent state gets higher. We showcase the full potential of the machinery we\ndevelop in the article in semi-supervised multi-class classification, where our\nsampling algorithm is used within an active learning framework to guide the\nselection of input data to manually label in order to achieve high predictive\naccuracy with a minimal number of labelled data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 01:02:47 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 17:46:37 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chen", "Victor", ""], ["Dunlop", "Matthew M.", ""], ["Papaspiliopoulos", "Omiros", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1803.03348", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, George Michailidis", "title": "Joint Estimation and Inference for Data Integration Problems based on\n  Multiple Multi-layered Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of high-throughput technologies has enabled the\ngeneration of data from biological or disease processes that span multiple\nlayers, like genomic, proteomic or metabolomic data, and further pertain to\nmultiple sources, like disease subtypes or experimental conditions. In this\nwork, we propose a general statistical framework based on Gaussian graphical\nmodels for horizontal (i.e. across conditions or subtypes) and vertical (i.e.\nacross different layers containing data on molecular compartments) integration\nof information in such datasets. We start with decomposing the multi-layer\nproblem into a series of two-layer problems. For each two-layer problem, we\nmodel the outcomes at a node in the lower layer as dependent on those of other\nnodes in that layer, as well as all nodes in the upper layer. We use a\ncombination of neighborhood selection and group-penalized regression to obtain\nsparse estimates of all model parameters. Following this, we develop a\ndebiasing technique and asymptotic distributions of inter-layer directed edge\nweights that utilize already computed neighborhood selection coefficients for\nnodes in the upper layer. Subsequently, we establish global and simultaneous\ntesting procedures for these edge weights. Performance of the proposed\nmethodology is evaluated on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 01:30:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 22:51:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Michailidis", "George", ""]]}, {"id": "1803.03356", "submitter": "Brian Segal", "authors": "Brian D. Segal", "title": "Towards replicability with confidence intervals for the exceedance\n  probability", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": "10.1080/00031305.2019.1678521", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several scientific fields including psychology are undergoing a replication\ncrisis. There are many reasons for this problem, one of which is a misuse of\np-values. There are several alternatives to p-values, and in this paper we\ndescribe a complement that is geared towards replication. In particular, we\nfocus on confidence intervals for the probability that a parameter estimate\nwill exceed a specified value in an exact replication study. These intervals\nconvey uncertainty in a way that p-values and standard confidence intervals do\nnot, and can help researchers to draw sounder scientific conclusions. After\nbriefly reviewing background on p-values and a few alternatives, we describe\nour approach and provide examples with simulated and real data. For linear\nmodels, we also describe how confidence intervals for the exceedance\nprobability are related to p-values and confidence intervals for parameters.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:26:49 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 22:51:47 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 00:06:51 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2019 13:32:43 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Segal", "Brian D.", ""]]}, {"id": "1803.03512", "submitter": "Justin Chown", "authors": "Justin Chown, Cedric Heuchenne and Ingrid Van Keilegom", "title": "The nonparametric location-scale mixture cure model", "comments": "1 figure. Preprint submitted for consideration of publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose completely nonparametric methodology to investigate location-scale\nmodelling of two-component mixture cure models, where the responses of interest\nare only indirectly observable due to the presence of censoring and the\npresence of so-called long-term survivors that are always censored. We use\ncovariate-localized nonparametric estimators, which depend on a bandwidth\nsequence, to propose an estimator of the error distribution function that has\nnot been considered before in the literature. When this bandwidth belongs to a\ncertain range of undersmoothing bandwidths, the asymptotic distribution of the\nproposed estimator of the error distribution function does not depend on this\nbandwidth, and this estimator is shown to be root-n consistent. This suggests\nthat a computationally costly bandwidth selection procedure is unnecessary to\nobtain an effective estimator of the error distribution, and that a simpler\nrule-of-thumb approach can be used instead. A simulation study investigates the\nfinite sample properties of our approach, and the methodology is illustrated\nusing data obtained to study the behavior of distant metastasis in\nlymph-node-negative breast cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 13:57:58 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Chown", "Justin", ""], ["Heuchenne", "Cedric", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1803.03627", "submitter": "Marcela Svarc", "authors": "Lucas Fernandez-Piana and Marcela Svarc", "title": "A local depth measure for general data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Integrated Dual Local Depth which is a local depth measure\nfor data in a Banach space based on the use of one-dimensional projections. The\nproperties of a depth measure are analyzed under this setting and a proper\ndefinition of local symmetry is given. Moreover, strong consistency results for\nthe local depth and also for the local depth regions are attained. Finally,\napplications to descriptive data analysis and classification are analyzed,\nmaking the special focus on multivariate functional data, where we obtain very\npromising results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 18:27:22 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 18:21:27 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 22:00:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fernandez-Piana", "Lucas", ""], ["Svarc", "Marcela", ""]]}, {"id": "1803.03858", "submitter": "Sara Algeri", "authors": "Sara Algeri and David A. van Dyk", "title": "Testing One Hypothesis Multiple Times: The Multidimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of new rare signals in data, the detection of a sudden\nchange in a trend, and the selection of competing models, are among the most\nchallenging problems in statistical practice. These challenges can be tackled\nusing a test of hypothesis where a nuisance parameter is present only under the\nalternative, and a computationally efficient solution can be obtained by the\n\"Testing One Hypothesis Multiple times\" (TOHM) method. In the one-dimensional\nsetting, a fine discretization of the space of the non-identifiable parameter\nis specified, and a global p-value is obtained by approximating the\ndistribution of the supremum of the resulting stochastic process. In this\npaper, we propose a computationally efficient inferential tool to perform TOHM\nin the multidimensional setting. Here, the approximations of interest typically\ninvolve the expected Euler Characteristics (EC) of the excursion set of the\nunderlying random field. We introduce a simple algorithm to compute the EC in\nmultiple dimensions and for arbitrary large significance levels. This leads to\nan highly generalizable computational tool to perform inference under\nnon-standard regularity conditions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 19:49:26 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 17:39:19 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Algeri", "Sara", ""], ["van Dyk", "David A.", ""]]}, {"id": "1803.03875", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Chun-Shu Chen, Yu-Fen Li, Jin-Hua Chen", "title": "Empirical Likelihood Based Summary ROC Curve for Meta-Analysis of\n  Diagnostic Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: This study provides an effective model selection method based on\nthe empirical likelihood approach for constructing summary receiver operating\ncharacteristic (sROC) curves from meta-analyses of diagnostic studies.\n  Methods: We considered models from combinations of family indices and\nspecific pairs of transformations, which cover several widely used methods for\nbivariate summary of sensitivity and specificity. Then a final model was\nselected using the proposed empirical likelihood method. Simulation scenarios\nwere conducted based on different number of studies and different population\ndistributions for the disease and non-disease cases. The performance of our\nproposal and other model selection criteria was also compared.\n  Results: Although parametric likelihood-based methods are often applied in\npractice due to its asymptotic property, they fail to consistently choose\nappropriate models for summary under the limited number of studies. For these\nsituations, our proposed method almost always performs better.\n  Conclusion: When the number of studies is as small as 10 or 5, we recommend\nchoosing a summary model via the proposed empirical likelihood method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 01:22:13 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Chen", "Chun-Shu", ""], ["Li", "Yu-Fen", ""], ["Chen", "Jin-Hua", ""]]}, {"id": "1803.03895", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Reduction of Restricted Maximum Likelihood for Random Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted maximum likelihood (REML) estimator of the dispersion matrix\nfor random coefficient models is rewritten in terms of the sufficient\nstatistics of the individual regressions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:01:17 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03896", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Improved Asymptotics for Zeros of Kernel Estimates via a Reformulation\n  of the Leadbetter-Cryer Integral", "comments": null, "journal-ref": "Statistics & Probability Letters Volume 32, Issue 4, 1 April 1997,\n  Pages 351-356", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.PR math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected number of false inflection points of kernel smoothers is\nevaluated. To obtain the small noise limit, we use a reformulation of the\nLeadbetter-Cryer integral for the expected number of zero crossings of a\ndifferentiable Gaussian process.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:07:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03897", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Optimal Data-based Kernel Estimation of Evolutionary Spectra", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 41, Issue: 7, Jul\n  1993 ) Page(s): 2439 - 2447", "doi": "10.1109/78.224252", "report-no": null, "categories": "stat.ME eess.AS eess.IV eess.SP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex demodulation of evolutionary spectra is formulated as a\ntwo-dimensional kernel smoother in the time-frequency domain. In the first\nstage, a tapered Fourier transform, $y_{nu}(f,t)$, is calculated. Second, the\nlog-spectral estimate, $\\hat{\\theta}_{\\nu}(f,t) \\equiv \\ln(|y_{nu}(f,t)|^2$, is\nsmoothed. As the characteristic widths of the kernel smoother increase, the\nbias from temporal and frequency averaging increases while the variance\ndecreases. The demodulation parameters, such as the order, length, and\nbandwidth of spectral taper and the kernel smoother, are determined by\nminimizing the expected error. For well-resolved evolutionary spectra, the\noptimal taper length is a small fraction of the optimal kernel half-width. The\noptimal frequency bandwidth, $w$, for the spectral window scales as $w^2\n\\approx \\lambda_F/ \\tau $, where $\\tau$ is the characteristic time, and\n$\\lambda_F$ is the characteristic frequency scale-length. In contrast, the\noptimal half-widths for the second stage kernel smoother scales as $h \\approx\n1/(\\tau \\lambda_F)^{1 \\over ( p+2) }$, where $p$ is the order of the kernel\nsmoother. The ratio of the optimal frequency half-width to the optimal time\nhalf-width satisfies $h_F / h_T ~ (|\\partial_t ^p \\theta | / |\\partial_f^p\n\\theta|)$. Since the expected loss depends on the unknown evolutionary spectra,\nwe initially estimate $|\\partial_t^p \\theta|^2$ and $|\\partial_f^p \\theta|^2$\nusing a higher order kernel smoothers, and then substitute the estimated\nderivatives into the expected loss criteria.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:21:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03899", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation: Pilot Estimators", "comments": "PDF on https://projecteuclid.org/download/pdf_1/euclid.aos/1030741086", "journal-ref": "Annals of Statistics Volume 25, Number 6 (1997), 2592-2606", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given noisy data, function estimation is considered when the unknown function\nis known a priori to consist of a small number of regions where the function is\neither convex or concave. When the number of regions is unknown, the model\nselection problem is to determine the number of convexity change points. For\nkernel estimates in Gaussian noise, the number of false change points is\nevaluated as a function of the smoothing parameter. To ensure that the number\nof false convexity change points tends to zero, the smoothing level must be\nlarger than is generically optimal for minimizing the mean integrated square\nerror (MISE). A two-stage estimator is proposed and shown to achieve the\noptimal rate of convergence of the MISE. In the first-stage, the number and\nlocation of the change points is estimated using strong smoothing. In the\nsecond-stage, a constrained smoothing spline fit is performed with the\nsmoothing level chosen to minimize the MISE. The imposed constraint is that a\nsingle change point occur in a region about each empirical change point from\nthe first-stage estimate. This constraint is equivalent to the requirement that\nthe third derivative of the second-stage estimate have a single sign in a small\nneighborhood about each first-stage change point. The change points from the\nsecond-stage are in a neighborhood of the first-stage change points, but need\nnot be at the identical locations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:28:48 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03901", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation: Representations, Duality and Model\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SP eess.SY math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spline estimates which preserve prescribed piecewise convex\nproperties of the unknown function. A robust version of the penalized\nlikelihood is given and shown to correspond to a variable halfwidth kernel\nsmoother where the halfwidth adaptively decreases in regions of rapid change of\nthe unknown function. When the convexity change points are prescribed, we\nderive representation results and smoothness properties of the estimates. A\ndual formulation is given which reduces the estimate is reduced to a finite\ndimensional convex optimization in the dual space.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:38:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03903", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation and Model Selection", "comments": "arXiv admin note: text overlap with arXiv:1803.03901", "journal-ref": "Approximation Theory Viii - Volume 1: Approximation And\n  Interpolation edited by Chui Charles K, Schumaker Larry L 1995 by World\n  Scientific Publishing", "doi": null, "report-no": null, "categories": "stat.ME cs.LG eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given noisy data, function estimation is considered when the unknown function\nis known apriori to consist of a small number of regions where the function is\neither convex or concave. When the regions are known apriori, the estimate is\nreduced to a finite dimensional convex optimization in the dual space. When the\nnumber of regions is unknown, the model selection problem is to determine the\nnumber of convexity change points. We use a pilot estimator based on the\nexpected number of false inflection points.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:45:57 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03904", "submitter": "Kurt Riedel", "authors": "Andrew P. Mullhaupt, Kurt S. Riedel", "title": "Banded Matrix Fraction Representation of Triangular Input Normal Pairs", "comments": null, "journal-ref": "IEEE Transactions on Automatic Control ( Volume: 46, Issue: 12,\n  Dec 2001 ) Page(s): 2018 - 2022", "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.OC math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An input pair $(A,B)$ is triangular input normal if and only if $A$ is\ntriangular and $AA^* + BB^* = I_n$, where $I_n$ is theidentity matrix. Input\nnormal pairs generate an orthonormal basis for the impulse response. Every\ninput pair may be transformed to a triangular input normal pair. A new system\nrepresentation is given: $(A,B)$ is triangular normal and $A$ is a matrix\nfraction, $A=M^{-1}N$, where $M$ and $N$ are triangular matrices of low\nbandwidth. For single input pairs, $M$ and $N$ are bidiagonal and an explicit\nparameterization is given in terms of the eigenvalues of $A$. This band\nfraction structure allows for fast updates of state space systems and fast\nsystem identification. When A has only real eigenvalues, one state advance\nrequires $3n$ multiplications for the single input case.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:51:58 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Mullhaupt", "Andrew P.", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03906", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel\n  Analysis", "comments": null, "journal-ref": "Approximation Theory VIII: Approximation And Interpolation, pg\n  519-528, edited by Chui, Schumaker, 1995 World Scientific", "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.AS eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid estimator of the log-spectral density of a stationary time series is\nproposed. First, a multiple taper estimate is performed, followed by kernel\nsmoothing the log-multitaper estimate. This procedure reduces the expected mean\nsquare error by $({\\pi^2 \\over 4})^{.8}$ over simply smoothing the log tapered\nperiodogram. The optimal number of tapers is $O(N^{8/15})$. A data adaptive\nimplementation of a variable bandwidth kernel smoother is given. When the\nspectral density is discontinuous, one sided smoothing estimates are used.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 05:14:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03908", "submitter": "Kurt Riedel", "authors": "Andrew P. Mullhaupt, Kurt S. Riedel", "title": "Fast Adaptive Identification of Stable Innovation Filters", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Volume: 45, Issue: 10, Oct\n  1997, pgs. 2616 - 2619", "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SP eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive identification of the impulse response of an innovation filter\nis considered. The impulse response is a finite sum of known basis functions\nwith unknown coefficients. These unknown coefficients are estimated using a\npseudolinear regression. This estimate is implemented using a square root\nalgorithm based on a displacement rank structure. When the initial conditions\nhave low displacement rank, the filter update is $O(n)$. If the filter\narchitecture is chosen to be triangular input balanced, the estimation problem\nis well-conditioned and a simple, low rank initialization is available.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 05:21:20 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Mullhaupt", "Andrew P.", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03911", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Optimal Estimation of Dynamically Evolving Diffusivities", "comments": null, "journal-ref": "J. Computational Physics Vol 115, pg 1-11, 1995", "doi": "10.1006/jcph.1994.1173", "report-no": null, "categories": "stat.ME eess.SP math.OC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The augmented, iterated Kalman smoother is applied to system identification\nfor inverse problems in evolutionary differential equations. In the augmented\nsmoother, the unknown, time-dependent coefficients are included in the state\nvector, and have a stochastic component. At each step in the iteration, the\nestimate of the time evolution of the coefficients is linear. We update the\nslowly varying mean temperature and conductivity by averaging the estimates of\nthe Kalman smoother. Applications include the estimation of anomalous diffusion\ncoefficients in turbulent fluids and the plasma rotation velocity in plasma\ntomography.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 06:02:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03919", "submitter": "Yingxiang Yang", "authors": "Yingxiang Yang, Adams Wei Yu, Zhaoran Wang and Tuo Zhao", "title": "Detecting Nonlinear Causality in Multivariate Time Series with Sparse\n  Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric method for detecting nonlinear causal relationship\nwithin a set of multidimensional discrete time series, by using sparse additive\nmodels (SpAMs). We show that, when the input to the SpAM is a $\\beta$-mixing\ntime series, the model can be fitted by first approximating each unknown\nfunction with a linear combination of a set of B-spline bases, and then solving\na group-lasso-type optimization problem with nonconvex regularization.\nTheoretically, we characterize the oracle statistical properties of the\nproposed sparse estimator in function estimation and model selection.\nNumerically, we propose an efficient pathwise iterative shrinkage thresholding\nalgorithm (PISTA), which tames the nonconvexity and guarantees linear\nconvergence towards the desired sparse estimator with high probability.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 07:46:24 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 04:24:14 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Yang", "Yingxiang", ""], ["Yu", "Adams Wei", ""], ["Wang", "Zhaoran", ""], ["Zhao", "Tuo", ""]]}, {"id": "1803.03995", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, A. Sidorenko", "title": "Adaptive Smoothing of the Log-Spectrum with Multiple Tapering", "comments": null, "journal-ref": "IEEE Trans. Signal Process., vol. 44, no. 7, pp. 1794-1800, Jul.\n  1996", "doi": "10.1109/78.510625", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid estimator of the log-spectral density of a stationary time series is\nproposed. First, a multiple taper estimate is performed, followed by kernel\nsmoothing the log-multiple taper estimate. This procedure reduces the expected\nmean square error by $(\\pi^2/ 4)^{4/5} $ over simply smoothing the log tapered\nperiodogram. A data adaptive implementation of a variable bandwidth kernel\nsmoother is given.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 17:45:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "A.", ""]]}, {"id": "1803.03999", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, A. Sidorenko", "title": "Function Estimation Using Data Adaptive Kernel Estimation - How Much\n  Smoothing?", "comments": "Available at https://aip.scitation.org/doi/pdf/10.1063/1.4823316", "journal-ref": "Computers in Physics Volume 8 Issue 4, July/Aug. 1994 Pages\n  402-409", "doi": "10.1063/1.4823316", "report-no": null, "categories": "stat.ME cs.AI eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the expected error by smoothing the data locally. Then we\noptimize the shape of the kernel smoother to minimize the error. Because the\noptimal estimator depends on the unknown function, our scheme automatically\nadjusts to the unknown function. By self-consistently adjusting the kernel\nsmoother, the total estimator adapts to the data.\n  Goodness of fit estimators select a kernel halfwidth by minimizing a function\nof the halfwidth which is based on the average square residual fit error:\n$ASR(h)$. A penalty term is included to adjust for using the same data to\nestimate the function and to evaluate the mean square error. Goodness of fit\nestimators are relatively simple to implement, but the minimum (of the goodness\nof fit functional) tends to be sensitive to small perturbations. To remedy this\nsensitivity problem, we fit the mean square error %goodness of fit functional\nto a two parameter model prior to determining the optimal halfwidth.\n  Plug-in derivative estimators estimate the second derivative of the unknown\nfunction in an initial step, and then substitute this estimate into the\nasymptotic formula.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 18:03:07 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "A.", ""]]}, {"id": "1803.04046", "submitter": "Kurt Riedel", "authors": "Andrew Mullhaupt, Kurt Riedel", "title": "Exponential Condition Number of Solutions of the Discrete Lyapunov\n  Equation", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Volume: 52, Issue: 5, May\n  2004, pgs. 1257 - 1265", "doi": "10.1109/TSP.2004.826177", "report-no": null, "categories": "stat.ME cs.NA cs.SY eess.SY math.NA math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition number of the $n\\ x\\ n$ matrix $P$ is examined, where $P$\nsolves %the discete Lyapunov equation, $P - A P A^* = BB^*$, and $B$ is a $n\\\nx\\ d$ matrix. Lower bounds on the condition number, $\\kappa$, of $P$ are given\nwhen $A$ is normal, a single Jordan block or in Frobenius form. The bounds show\nthat the ill-conditioning of $P$ grows as $\\exp(n/d) >> 1$. These bounds are\nrelated to the condition number of the transformation that takes $A$ to input\nnormal form. A simulation shows that $P$ is typically ill-conditioned in the\ncase of $n>>1$ and $d=1$. When $A_{ij}$ has an independent Gaussian\ndistribution (subject to restrictions), we observe that $\\kappa(P)^{1/n} ~=\n3.3$. The effect of auto-correlated forcing on the conditioning on state space\nsystems is examined\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 21:29:54 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mullhaupt", "Andrew", ""], ["Riedel", "Kurt", ""]]}, {"id": "1803.04075", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Kernel estimation of the instantaneous frequency", "comments": null, "journal-ref": "I.E.E.E. Trans. Signal Processing 42, pp. 2644-2649 (1994)", "doi": "10.1109/78.324730", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider kernel estimators of the instantaneous frequency of a slowly\nevolving sinusoid in white noise. The expected estimation error consists of two\nterms. The systematic bias error grows as the kernel halfwidth increases while\nthe random error decreases. For a non-modulated signal, $g(t)$, the kernel\nhalfwidth which minimizes the expected error scales as$h \\sim \\left[{ \\sigma^2\n\\over\n  N| \\partial_t^2 g^{}|^2 } \\right]^{1/ 5}$, where %$A^{(\\ell)}$ is the\ncoherent signal at frequency, $f_{\\ell}$, $\\sigma^2$ is the noise variance and\n$N$ is the number of measurements per unit time. We show that estimating the\ninstantaneous frequency corresponds to estimating the first derivative of a\nmodulated signal, $A(t)\\exp(i\\phi(t))$. For instantaneous frequency estimation,\nthe halfwidth which minimizes the expected error is larger: $h_{1,3} \\sim\n\\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 }\n\\right]^{1/ 7}$. Since the optimal halfwidths depend on derivatives of the\nunknown function, we initially estimate these derivatives prior to estimating\nthe actual signal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:43:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.04077", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Statistical tests for evaluating an earthquake prediction method", "comments": null, "journal-ref": "Geophysical Research Letters 23, pp. 1407-1409 (1996)", "doi": "10.1029/96GL00476", "report-no": null, "categories": "stat.ME physics.data-an physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of including postcursors in the null hypothesis test is discussed.\nUnequal prediction probabilities can be included in the null hypothesis test\nusing a generalization of the central limit theorem. A test for determining the\nenhancement factor over random chance is given. The seismic earthquake signal\nmay preferentially precede earthquakes even if the VAN methodology fails to\nforecast the earthquakes. We formulate a statistical test for this possibility.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:49:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.04078", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Alexander Sidorenko", "title": "Minimum bias multiple taper spectral estimation", "comments": null, "journal-ref": "I.E.E.E. Trans. Signal Processing 43, pp. 188-195 (1995)", "doi": "10.1109/78.365298", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two families of orthonormal tapers are proposed for multi-taper spectral\nanalysis: minimum bias tapers, and sinusoidal tapers $\\{ \\bf{v}^{(k)}\\}$, where\n$v_n^{(k)}=\\sqrt{\\frac{2}{N+1}}\\sin\\frac{\\pi kn}{N+1}$, and $N$ is the number\nof points. The resulting sinusoidal multitaper spectral estimate is\n$\\hat{S}(f)=\\frac{1}{2K(N+1)} \\sum_{j=1}^K |y(f+\\frac{j}{2N+2})\n-y(f-\\frac{j}{2N+2})|^2$, where $y(f)$ is the Fourier transform of the\nstationary time series, $S(f)$ is the spectral density, and $K$ is the number\nof tapers. For fixed $j$, the sinusoidal tapers converge to the minimum bias\ntapers like $1/N$. Since the sinusoidal tapers have analytic expressions, no\nnumerical eigenvalue decomposition is necessary. Both the minimum bias and\nsinusoidal tapers have no additional parameter for the spectral bandwidth. The\nbandwidth of the $j$th taper is simply $\\frac{1}{N}$ centered about the\nfrequencies $\\frac{\\pm j}{2N+2}$. Thus the bandwidth of the multitaper spectral\nestimate can be adjusted locally by simply adding or deleting tapers. The band\nlimited spectral concentration, $\\int_{-w}^w |V(f)|^2 df$, of both the minimum\nbias and sinusoidal tapers is very close to the optimal concentration achieved\nby the Slepian tapers. In contrast, the Slepian tapers can have the local bias,\n$\\int_{-1/2}^{1/2} f^2 |V(f)|^2 df$, much larger than of the minimum bias\ntapers and the sinusoidal tapers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 01:01:27 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 03:13:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "Alexander", ""]]}, {"id": "1803.04142", "submitter": "Mohamed-Salem Ahmed", "authors": "Ahmed and Dabo", "title": "Partially Linear Spatial Probit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A partially linear probit model for spatially dependent data is considered. A\ntriangular array setting is used to cover various patterns of spatial data.\nConditional spatial heteroscedasticity and non-identically distributed\nobservations and a linear process for disturbances are assumed, allowing\nvarious spatial dependencies. The estimation procedure is a combination of a\nweighted likelihood and a generalized method of moments. The procedure first\nfixes the parametric components of the model and then estimates the\nnon-parametric part using weighted likelihood; the obtained estimate is then\nused to construct a GMM parametric component estimate. The consistency and\nasymptotic distribution of the estimators are established under sufficient\nconditions. Some simulation experiments are provided to investigate the finite\nsample performance of the estimators.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 07:43:02 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Ahmed", "", ""], ["Dabo", "", ""]]}, {"id": "1803.04235", "submitter": "In\u00e9s del Puerto", "authors": "M. Gonz\\'alez, R. Mart\\'inez, C. Minuesa, I. del Puerto", "title": "Approximate Bayesian Computation in controlled branching processes: the\n  role of summary statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled branching processes are stochastic growth population models in\nwhich the number of individuals with reproductive capacity in each generation\nis controlled by a random control function. The purpose of this work is to\nexamine the Approximate Bayesian Computation (ABC) methods and to propose\nappropriate summary statistics for them in the context of these processes. This\nmethodology enables to approximate the posterior distribution of the parameters\nof interest satisfactorily without explicit likelihood calculations and under a\nminimal set of assumptions. In particular, the tolerance rejection algorithm,\nthe sequential Monte Carlo ABC algorithm, and a post-sampling correction method\nbased on local-linear regression are provided. The accuracy of the proposed\nmethods are illustrated and compared with a \"likelihood free\" Markov chain\nMonte Carlo technique by the way of a simulated example developed with the\nstatistical software R.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:15:33 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 19:01:30 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Gonz\u00e1lez", "M.", ""], ["Mart\u00ednez", "R.", ""], ["Minuesa", "C.", ""], ["del Puerto", "I.", ""]]}, {"id": "1803.04262", "submitter": "Marco Valtorta", "authors": "Mohammad Ali Javidian and Marco Valtorta", "title": "On the Properties of MVR Chain Graphs", "comments": "Extended version of a paper submitted to a conference. arXiv admin\n  note: text overlap with arXiv:0906.2098 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depending on the interpretation of the type of edges, a chain graph can\nrepresent different relations between variables and thereby independence\nmodels. Three interpretations, known by the acronyms LWF, MVR, and AMP, are\nprevalent. Multivariate regression chain graphs (MVR CGs) were introduced by\nCox and Wermuth in 1993. We review Markov properties for MVR chain graphs and\npropose an alternative global and local Markov property for them. Except for\npairwise Markov properties, we show that for MVR chain graphs all Markov\nproperties in the literature are equivalent for semi-graphoids. We derive a new\nfactorization formula for MVR chain graphs which is more explicit than and\ndifferent from the proposed factorizations for MVR chain graphs in the\nliterature. Finally, we provide a summary table comparing different features of\nLWF, AMP, and MVR chain graphs.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 00:39:19 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 00:11:45 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 19:13:29 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 22:26:02 GMT"}, {"version": "v5", "created": "Wed, 18 Jul 2018 19:50:09 GMT"}, {"version": "v6", "created": "Wed, 1 Aug 2018 19:05:16 GMT"}, {"version": "v7", "created": "Mon, 11 Feb 2019 22:01:36 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""]]}, {"id": "1803.04397", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov and Thomas Jaki", "title": "An information-theoretic Phase I/II design for molecularly targeted\n  agents that does not require an assumption of monotonicity", "comments": "7 figures", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 68 (2), pp 1-21, 2018", "doi": "10.1111/rssc.12293", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many years Phase I and Phase II clinical trials were conducted\nseparately, but there was a recent shift to combine these Phases. While a\nvariety of Phase~I/II model-based designs for cytotoxic agents were proposed in\nthe literature, methods for molecularly targeted agents (TA) are just starting\nto develop. The main challenge of the TA setting is the unknown dose-efficacy\nrelation that can have either an increasing, plateau or umbrella shape. To\ncapture these, approaches with more parameters are needed to model the\ndose-efficacy relationship or, alternatively, more orderings of the\ndose-efficacy relationship are required to account for the uncertainty in the\ncurve shape. As a result, designs for more complex clinical trials, for\nexample, trials looking at schedules of a combination treatment involving TA,\nhave not been extensively studied yet. We propose a novel regimen-finding\ndesign which is based on a derived efficacy-toxicity trade-off function. Due to\nits special properties, an accurate regimen selection can be achieved without\nany parametric or monotonicity assumptions. We illustrate how this design can\nbe applied in the context of a complex combination-schedule clinical trial. We\ndiscuss practical and ethical issues such as coherence, delayed and missing\nefficacy responses, safety and futility constraints.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 17:47:00 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:56:16 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""]]}, {"id": "1803.04464", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Hamid Javadi", "title": "False Discovery Rate Control via Debiased Lasso", "comments": "accepted for publication in the Electronic Journal of statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in high-dimensional statistical\nmodels where the goal is to report a set of variables, out of many predictors\n$X_1, \\dotsc, X_p$, that are relevant to a response of interest. For linear\nhigh-dimensional model, where the number of parameters exceeds the number of\nsamples $(p>n)$, we propose a procedure for variables selection and prove that\nit controls the \"directional\" false discovery rate (FDR) below a pre-assigned\nsignificance level $q\\in [0,1]$. We further analyze the statistical power of\nour framework and show that for designs with subgaussian rows and a common\nprecision matrix $\\Omega\\in\\mathbb{R}^{p\\times p}$, if the minimum nonzero\nparameter $\\theta_{\\min}$ satisfies $$\\sqrt{n} \\theta_{\\min} - \\sigma\n\\sqrt{2(\\max_{i\\in [p]}\\Omega_{ii})\\log\\left(\\frac{2p}{qs_0}\\right)} \\to\n\\infty\\,,$$ then this procedure achieves asymptotic power one. Our framework is\nbuilt upon the debiasing approach and assumes the standard condition $s_0 =\no(\\sqrt{n}/(\\log p)^2)$, where $s_0$ indicates the number of true positives\namong the $p$ features. Notably, this framework achieves exact directional FDR\ncontrol without any assumption on the amplitude of unknown regression\nparameters, and does not require any knowledge of the distribution of\ncovariates or the noise level. We test our method in synthetic and real data\nexperiments to assess its performance and to corroborate our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:03:33 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 06:14:48 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Javanmard", "Adel", ""], ["Javadi", "Hamid", ""]]}, {"id": "1803.04485", "submitter": "Mojgan Golzy", "authors": "Mojgan Golzy and Marianthi Markatou", "title": "Poisson Kernel-Based Clustering on the Sphere: Convergence Properties,\n  Identifiability, and a Method of Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many applications of interest involve data that can be analyzed as unit\nvectors on a d-dimensional sphere. Specific examples include text mining, in\nparticular clustering of documents, biology, astronomy and medicine among\nothers. Previous work has proposed a clustering method using mixtures of\nPoisson kernel-based distributions (PKBD) on the sphere. We prove\nidentifiability of mixtures of the aforementioned model, convergence of the\nassociated EM-type algorithm and study its operational characteristics.\nFurthermore, we propose an empirical densities distance plot for estimating the\nnumber of clusters in a PKBD model. Finally, we propose a method to simulate\ndata from Poisson kernel-based densities and exemplify our methods via\napplication on real data sets and simulation experiments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:39:37 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Golzy", "Mojgan", ""], ["Markatou", "Marianthi", ""]]}, {"id": "1803.04499", "submitter": "Jiannan Lu", "authors": "Jiannan Lu", "title": "On finite-population Bayesian inferences for $2^K$ factorial designs\n  with binary outcomes", "comments": "To appear in Journal of Statistical Computation and Simulation", "journal-ref": null, "doi": "10.1080/00949655.2019.1574793", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the pioneering work of Rubin (1978), we employ the potential\noutcomes framework to develop a finite-population Bayesian causal inference\nframework for randomized controlled $2^K$ factorial designs with binary\noutcomes, which are common in medical research. As demonstrated by simulated\nand empirical examples, the proposed framework corrects the well-known variance\nover-estimation issue of the classic \"Neymanian\" inference framework, under\nvarious settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 20:00:46 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 05:51:44 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lu", "Jiannan", ""]]}, {"id": "1803.04503", "submitter": "Jiannan Lu", "authors": "Jiannan Lu", "title": "Improved Neymanian analysis for $2^K$ factorial designs with binary\n  outcomes", "comments": "Accepted by Statistica Neerlandica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $2^K$ factorial designs are widely adopted by statisticians and the broader\nscientific community. In this short note, under the potential outcomes\nframework (Neyman, 1923; Rubin, 1974), we adopt the partial identification\napproach and derive the sharp lower bound of the sampling variance of the\nestimated factorial effects, which leads to an \"improved\" Neymanian variance\nestimator that mitigates the over-estimation issue suffered by the classic\nNeymanian variance estimator by Dasgupta et al. (2015).\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 20:05:08 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 18:52:26 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 07:53:37 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 02:49:16 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Lu", "Jiannan", ""]]}, {"id": "1803.04559", "submitter": "Jianeng Xu", "authors": "Michael Newton, Nicholas G. Polson, Jianeng Xu", "title": "Weighted Bayesian Bootstrap for Scalable Bayes", "comments": null, "journal-ref": "Canadian Journal of Statistics 2020", "doi": "10.1002/cjs.11570", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a weighted Bayesian Bootstrap (WBB) for machine learning and\nstatistics. WBB provides uncertainty quantification by sampling from a high\ndimensional posterior distribution. WBB is computationally fast and scalable\nusing only off-theshelf optimization software such as TensorFlow. We provide\nregularity conditions which apply to a wide range of machine learning and\nstatistical models. We illustrate our methodology in regularized regression,\ntrend filtering and deep learning. Finally, we conclude with directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:34:08 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Newton", "Michael", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1803.04582", "submitter": "Dalia Chakrabarty Dr.", "authors": "Kangrui Wang and Dalia Chakrabarty", "title": "Deep Bayesian Supervised Learning given Hypercuboidally-shaped,\n  Discontinuous Data, using Compound Tensor-Variate & Scalar-Variate Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We undertake Bayesian learning of the high-dimensional functional\nrelationship between a system parameter vector and an observable, that is in\ngeneral tensor-valued. The ultimate aim is Bayesian inverse prediction of the\nsystem parameters, at which test data is recorded. We attempt such learning\ngiven hypercuboidally-shaped data that displays strong discontinuities,\nrendering learning challenging. We model the sought high-dimensional function,\nwith a tensor-variate Gaussian Process (GP), and use three independent ways for\nlearning covariance matrices of the resulting likelihood, which is\nTensor-Normal. We demonstrate that the discontinuous data demands that\nimplemented covariance kernels be non-stationary--achieved by modelling each\nkernel hyperparameter, as a function of the sample function of the invoked\ntensor-variate GP. Each such function can be shown to be temporally-evolving,\nand treated as a realisation from a distinct scalar-variate GP, with covariance\ndescribed adaptively by collating information from a historical set of samples\nof chosen sample-size. We prove that deep-learning using 2-\"layers\", suffice,\nwhere the outer-layer comprises the tensor-variate GP, compounded with multiple\nscalar-variate GPs in the \"inner-layer\", and undertake inference with\nMetropolis-within-Gibbs. We apply our method to a cuboidally-shaped,\ndiscontinuous, real dataset, and subsequently perform forward prediction to\ngenerate data from our model, given our results--to perform model-checking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 00:16:18 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 07:34:16 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1803.04820", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers, Peter J. Rousseeuw, Iwein Vranckx", "title": "Discussion of \"The power of monitoring\"", "comments": null, "journal-ref": "Statistical Methods and Applications, 2018, Vol. 27, 589-594", "doi": "10.1007/s10260-018-0425-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an invited comment on the discussion paper \"The power of monitoring:\nhow to make the most of a contaminated multivariate sample\" by A. Cerioli, M.\nRiani, A. Atkinson and A. Corbellini that will appear in the journal\nStatistical Methods & Applications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:08:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Vranckx", "Iwein", ""]]}, {"id": "1803.04839", "submitter": "Kayanan Manickavasagar", "authors": "Manickavasagar Kayanan and Pushpakanthie Wijekoon", "title": "Optimal estimators in misspecified linear regression model with an\n  application to real-world data", "comments": "18 pages, 8 figures", "journal-ref": "Kayanan M. & Wijekoon P. (2018). Optimal estimators in\n  misspecified linear regression model with an application to real-world data,\n  Communications in Statistics: Case Studies, Data Analysis and Applications,\n  4:3-4, 151-163", "doi": "10.1080/23737484.2018.1536863", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose the Sample Information Optimal Estimator (SIOE)\nand the Stochastic Restricted Optimal Estimator (SROE) for misspecified linear\nregression model when multicollinearity exists among explanatory variables.\nFurther, we obtain the superiority conditions of proposed estimators over some\nother existing estimators in the Mean Square Error Matrix (MSEM) criterion in a\nstandard form which can apply to all estimators considered in this study.\nFinally, a real world example and a Monte Carlo simulation study are presented\nfor the proposed estimators to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:12:04 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 10:43:18 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 11:10:43 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Kayanan", "Manickavasagar", ""], ["Wijekoon", "Pushpakanthie", ""]]}, {"id": "1803.04853", "submitter": "Vivien Goepp", "authors": "Vivien Goepp (MAP5 - UMR 8145, UPD5, UPD5 Math\\'ematiques\n  Informatique), Jean-Christophe Thalabard (MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\\'edecine), Gr\\'egory Nuel (LPMA), Olivier Bouaziz (MAP5 - UMR 8145, UPD5,\n  UPD5 Math\\'ematiques Informatique, IUT - Paris Descartes)", "title": "Regularized Bidimensional Estimation of the Hazard Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological or demographic studies, with variable age at onset, a\ntypical quantity of interest is the incidence of a disease (for example the\ncancer incidence). In these studies, the individuals are usually highly\nheterogeneous in terms of dates of birth (the cohort) and with respect to the\ncalendar time (the period) and appropriate estimation methods are needed. In\nthis article a new estimation method is presented which extends classical\nage-period-cohort analysis by allowing interactions between age, period and\ncohort effects. This paper introduces a bidimensional regularized estimate of\nthe hazard rate where a penalty is introduced on the likelihood of the model.\nThis penalty can be designed either to smooth the hazard rate or to enforce\nconsecutive values of the hazard to be equal, leading to a parsimonious\nrepresentation of the hazard rate. In the latter case, we make use of an\niterative penalized likelihood scheme to approximate the L0 norm, which makes\nthe computation tractable. The method is evaluated on simulated data and\napplied on breast cancer survival data from the SEER program.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:49:54 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 16:53:17 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 11:58:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Goepp", "Vivien", "", "MAP5 - UMR 8145, UPD5, UPD5 Math\u00e9matiques\n  Informatique"], ["Thalabard", "Jean-Christophe", "", "MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\u00e9decine"], ["Nuel", "Gr\u00e9gory", "", "LPMA"], ["Bouaziz", "Olivier", "", "MAP5 - UMR 8145, UPD5,\n  UPD5 Math\u00e9matiques Informatique, IUT - Paris Descartes"]]}, {"id": "1803.04874", "submitter": "Giuseppe Buccheri", "authors": "Giuseppe Buccheri, Giacomo Bormetti, Fulvio Corsi and Fabrizio Lillo", "title": "Filtering and Smoothing with Score-Driven Models", "comments": "39 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for filtering, smoothing and assessing parameter and\nfiltering uncertainty in score-driven models. Our technique is based on a\ngeneral representation of the Kalman filter and smoother recursions for linear\nGaussian models in terms of the score of the conditional log-likelihood. We\nprove that, when data is generated by a nonlinear non-Gaussian state-space\nmodel, the proposed methodology results from a local expansion of the true\nfiltering density. A formal characterization of the approximation error is\nprovided. As shown in extensive Monte Carlo analyses, our methodology performs\nvery similarly to exact simulation-based methods, while remaining\ncomputationally extremely simple. We illustrate empirically the advantages in\nemploying score-driven models as approximate filters rather than purely\npredictive processes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:19:10 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 16:45:30 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 09:25:43 GMT"}, {"version": "v4", "created": "Sat, 20 Feb 2021 19:42:56 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Buccheri", "Giuseppe", ""], ["Bormetti", "Giacomo", ""], ["Corsi", "Fulvio", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1803.04877", "submitter": "David Benkeser", "authors": "David Benkeser, Andrew Mertens, Benjamin F. Arnold, John M. Colford\n  Jr., Alan Hubbard, Aryeh Stein, N. Lntshotshole Jumbe, Mark van der Laan", "title": "A machine learning-based approach for estimating and testing\n  associations with multivariate outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for summarizing the strength of association between a set\nof variables and a multivariate outcome. Classical summary measures are\nappropriate when linear relationships exist between covariates and outcomes,\nwhile our approach provides an alternative that is useful in situations where\ncomplex relationships may be present. We utilize ensemble machine learning to\ndetect nonlinear relationships and covariate interactions and propose a measure\nof association that captures these relationships. A hypothesis test about the\nproposed associative measure can be used to test the strong null hypothesis of\nno association between a set of variables and a multivariate outcome.\nSimulations demonstrate that this hypothesis test has greater power than\nexisting methods against alternatives where covariates have nonlinear\nrelationships with outcomes. We additionally propose measures of variable\nimportance for groups of variables, which summarize each groups' association\nwith the outcome. We demonstrate our methodology using data from a birth cohort\nstudy on childhood health and nutrition in the Philippines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:21:06 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 15:48:21 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Benkeser", "David", ""], ["Mertens", "Andrew", ""], ["Arnold", "Benjamin F.", ""], ["Colford", "John M.", "Jr."], ["Hubbard", "Alan", ""], ["Stein", "Aryeh", ""], ["Jumbe", "N. Lntshotshole", ""], ["van der Laan", "Mark", ""]]}, {"id": "1803.04906", "submitter": "Victoria Volodina", "authors": "Victoria Volodina and Daniel B. Williamson", "title": "Diagnostic-Driven Nonstationary Emulators Using Kernel Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly stationary Gaussian processes (GPs) are the principal tool in the\nstatistical approaches to the design and analysis of computer experiments (or\nUncertainty Quantification). Such processes are fitted to computer model output\nusing a set of training runs to learn the parameters of the process covariance\nkernel. The stationarity assumption is often adequate, yet can lead to poor\npredictive performance when the model response exhibits nonstationarity, for\nexample, if its smoothness varies across the input space. In this paper, we\nintroduce a diagnostic-led approach to fitting nonstationary GP emulators by\nspecifying finite mixtures of region-specific covariance kernels. Our method\nfirst fits a stationary GP and, if traditional diagnostics exhibit\nnonstationarity, those diagnostics are used to fit appropriate mixing functions\nfor a covariance kernel mixture designed to capture the nonstationarity,\nensuring an emulator that is continuous in parameter space and readily\ninterpretable. We compare our approach to the principal nonstationary GP models\nin the literature and illustrate its performance on a number of idealised test\ncases and in an application to modelling the cloud parameterization of the\nFrench climate model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 15:59:41 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 16:34:09 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Volodina", "Victoria", ""], ["Williamson", "Daniel B.", ""]]}, {"id": "1803.04947", "submitter": "Matthew Dixon", "authors": "Matthew Dixon and Tyler Ward", "title": "Takeuchi's Information Criteria as a form of Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Takeuchi's Information Criteria (TIC) is a linearization of maximum\nlikelihood estimator bias which shrinks the model parameters towards the\nmaximum entropy distribution, even when the model is mis-specified. In\nstatistical machine learning, $L_2$ regularization (a.k.a. ridge regression)\nalso introduces a parameterized bias term with the goal of minimizing\nout-of-sample entropy, but generally requires a numerical solver to find the\nregularization parameter. This paper presents a novel regularization approach\nbased on TIC; the approach does not assume a data generation process and\nresults in a higher entropy distribution through more efficient sample noise\nsuppression. The resulting objective function can be directly minimized to\nestimate and select the best model, without the need to select a regularization\nparameter, as in ridge regression. Numerical results applied to a synthetic\nhigh dimensional dataset generated from a logistic regression model demonstrate\nsuperior model performance when using the TIC based regularization over a $L_1$\nand a $L_2$ penalty term.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:31:17 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 22:40:48 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Dixon", "Matthew", ""], ["Ward", "Tyler", ""]]}, {"id": "1803.04991", "submitter": "Martin Weidner", "authors": "Koen Jochmans, Martin Weidner", "title": "Inference on a Distribution from Noisy Draws", "comments": "24 pages main text, 22 pages appendix (including references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a situation where the distribution of a random variable is being\nestimated by the empirical distribution of noisy measurements of that variable.\nThis is common practice in, for example, teacher value-added models and other\nfixed-effect models for panel data. We use an asymptotic embedding where the\nnoise shrinks with the sample size to calculate the leading bias in the\nempirical distribution arising from the presence of noise. The leading bias in\nthe empirical quantile function is equally obtained. These calculations are new\nin the literature, where only results on smooth functionals such as the mean\nand variance have been derived. Given a closed-form expression for the bias,\nbias-corrected estimator of the distribution function and quantile function can\nbe constructed. We provide both analytical and jackknife corrections that\nrecenter the limit distribution and yield confidence intervals with correct\ncoverage in large samples. These corrections are non-parametric and easy to\nimplement. Our approach can be connected to corrections for selection bias and\nshrinkage estimation and is to be contrasted with deconvolution. Simulation\nresults confirm the much-improved sampling behavior of the corrected\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 18:09:12 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 22:54:54 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 13:28:54 GMT"}, {"version": "v4", "created": "Tue, 10 Sep 2019 15:45:10 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Jochmans", "Koen", ""], ["Weidner", "Martin", ""]]}, {"id": "1803.05066", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo", "title": "Discussion on Bayesian Cluster Analysis: Point Estimation and Credible\n  Balls by Sara Wade and Zoubin Ghahramani", "comments": "2 pages", "journal-ref": "Bayesian Anal., Volume 13, Number 2 (2018), 599-600", "doi": "10.1214/17-BA1073", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I begin my discussion by giving an overview of the main results. Then I\nproceed to touch upon issues about whether the credible ball constructed can be\ninterpreted as a confidence ball, suggestions on reducing computational costs,\nand posterior consistency or contraction rates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 22:28:17 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 19:17:29 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Yoo", "William Weimin", ""]]}, {"id": "1803.05083", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Block Diagonally Dominant Positive Definite Sub-optimal Filters and\n  Smoothers", "comments": null, "journal-ref": "Automatica 29, pp. 779-783 (1993", "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine stochastic dynamical systems where the transition matrix, $\\Phi$,\nand the system noise, $\\bf{\\Gamma}\\bf{Q}\\bf{\\Gamma}^T$, covariance are nearly\nblock diagonal. When $\\bf{H}^T \\bf{R}^{-1} \\bf{H}$ is also nearly block\ndiagonal, where $\\bf{R}$ is the observation noise covariance and $\\bf{H}$ is\nthe observation matrix, our suboptimal filter/smoothers are always positive\nsemi-definite, and have improved numerical properties. Applications for\ndistributed dynamical systems with time dependent pixel imaging are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 00:24:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.05087", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Kaya Imre", "title": "Smoothing Spline Growth Curves With Covariates", "comments": null, "journal-ref": "Comm. in Statistics 22, pp. 1795-1818 (1993)", "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an physics.plasm-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the interactive spline model of Wahba to growth curves with\ncovariates. The smoothing spline formulation permits a non-parametric\nrepresentation of the growth curves. In the limit when the discretization error\nis small relative to the estimation error, the resulting growth curve estimates\noften depend only weakly on the number and locations of the knots. The\nsmoothness parameter is determined from the data by minimizing an empirical\nestimate of the expected error. We show that the risk estimate of Craven and\nWahba is a weighted goodness of fit estimate. A modified loss estimate is\ngiven, where $\\sigma^2$ is replaced by its unbiased estimate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 00:32:19 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Imre", "Kaya", ""]]}, {"id": "1803.05130", "submitter": "Kurt Riedel", "authors": "Kurt Riedel", "title": "Signal Processing and Piecewise Convex Estimation", "comments": null, "journal-ref": "ICIAM Proceedings 1993", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems on signal processing reduce to nonparametric function\nestimation. We propose a new methodology, piecewise convex fitting (PCF), and\ngive a two-stage adaptive estimate. In the first stage, the number and location\nof the change points is estimated using strong smoothing. In the second stage,\na constrained smoothing spline fit is performed with the smoothing level chosen\nto minimize the MSE. The imposed constraint is that a single change point\noccurs in a region about each empirical change point of the first-stage\nestimate. This constraint is equivalent to requiring that the third derivative\nof the second-stage estimate has a single sign in a small neighborhood about\neach first-stage change point. We sketch how PCF may be applied to signal\nrecovery, instantaneous frequency estimation, surface reconstruction, image\nsegmentation, spectral estimation and multivariate adaptive regression.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 04:17:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt", ""]]}, {"id": "1803.05284", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "A Unified View of False Discovery Rate Control: Reconciliation of\n  Bayesian and Frequentist Approaches", "comments": "6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the intrinsic connections between the Bayesian false\ndiscovery rate (FDR) control procedures and their counterpart of frequentist\nprocedures. We attempt to offer a unified view of FDR control within and beyond\nthe setting of testing exchangeable hypotheses. Under the standard two-groups\nmodel and the Oracle condition, we show that the Bayesian and the frequentist\nmethods can achieve asymptotically equivalent FDR control at arbitrary levels.\nBuilt on this result, we further illustrate that rigorous post-fitting model\ndiagnosis is necessary and effective to ensure robust FDR controls for\nparametric Bayesian approaches. Additionally, we show that the Bayesian FDR\ncontrol approaches are coherent and naturally extended to the setting beyond\ntesting exchangeable hypotheses. Particularly, we illustrate that $p$-values\nare no longer the natural statistical instruments for optimal frequentist FDR\ncontrol in testing non-exchangeable hypotheses. Finally, we illustrate that\nsimple numerical recipes motivated by our theoretical results can be effective\nin examining some key model assumptions commonly assumed in both Bayesian and\nfrequentist procedures (e.g., zero assumption).\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 14:01:07 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1803.05384", "submitter": "David S. Robertson", "authors": "David S. Robertson and James M. S. Wason", "title": "Familywise error control in multi-armed response-adaptive trials", "comments": "55 pages, 3 figures", "journal-ref": "Biometrics 2019, 75(3):885-894", "doi": "10.1111/biom.13042", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-adaptive designs allow the randomization probabilities to change\nduring the course of a trial based on cumulated response data, so that a\ngreater proportion of patients can be allocated to the better performing\ntreatments. A major concern over the use of response-adaptive designs in\npractice, particularly from a regulatory viewpoint, is controlling the type I\nerror rate. In particular, we show that the naive z-test can have an inflated\ntype I error rate even after applying a Bonferroni correction. Simulation\nstudies have often been used to demonstrate error control, but do not provide a\nguarantee. In this paper, we present adaptive testing procedures for normally\ndistributed outcomes that ensure strong familywise error control, by\niteratively applying the conditional invariance principle. Our approach can be\nused for fully sequential and block randomized trials, and for a large class of\nadaptive randomization rules found in the literature. We show there is a high\nprice to pay in terms of power to guarantee familywise error control for\nrandomization schemes with extreme allocation probabilities. However, for\nproposed Bayesian adaptive randomization schemes in the literature, our\nadaptive tests maintain or increase the power of the trial compared to the\nz-test. We illustrate our method using a three-armed trial in primary\nhypercholesterolemia.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 16:26:08 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Robertson", "David S.", ""], ["Wason", "James M. S.", ""]]}, {"id": "1803.05403", "submitter": "Marco Geraci", "authors": "Marco Geraci", "title": "Additive quantile regression for clustered data with an application to\n  children's physical activity", "comments": "50 pages, 4 figures, 2 tables (18 supplementary tables)", "journal-ref": "Journal of the Royal Statistical Society - Series C, 2018", "doi": "10.1111/rssc.12333", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models are flexible regression tools that handle linear as well as\nnonlinear terms. The latter are typically modelled via smoothing splines.\nAdditive mixed models extend additive models to include random terms when the\ndata are sampled according to cluster designs (e.g., longitudinal). These\nmodels find applications in the study of phenomena like growth, certain disease\nmechanisms and energy consumption in humans, when repeated measurements are\navailable. In this paper, we propose a novel additive mixed model for quantile\nregression. Our methods are motivated by an application to physical activity\nbased on a dataset with more than half million accelerometer measurements in\nchildren of the UK Millennium Cohort Study. In a simulation study, we assess\nthe proposed methods against existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 17:05:20 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 12:14:12 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Geraci", "Marco", ""]]}, {"id": "1803.05411", "submitter": "Haiyan Liu", "authors": "Haiyan Liu and Jeanine Houwing-Duistermaat", "title": "On trend and its derivatives estimation in repeated time series with\n  subordinated long-range dependent errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For temporal regularly spaced datasets, a lot of methods are available and\nthe properties of these methods are extensively investigated. Less research has\nbeen performed on irregular temporal datasets subject to measurement error with\ncomplex dependence structures, while this type of datasets is widely available.\nIn this paper, the performance of kernel smoother for trend and its derivatives\nis considered under dependent measurement errors and irregularly spaced\nsampling scheme. The error processes are assumed to be subordinated Gaussian\nlong memory processes and have varying marginal distributions. The functional\ncentral limit theorem for the estimators of trend and its derivatives are\nderived and bandwidth selection problem is addressed.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 17:17:07 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Liu", "Haiyan", ""], ["Houwing-Duistermaat", "Jeanine", ""]]}, {"id": "1803.05484", "submitter": "I-Sheng Yang", "authors": "I-Sheng Yang", "title": "Removing Skill Bias from Gaming Statistics", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"The chance to win given a certain move\" is an easily obtainable quantity\nfrom data and often quoted in gaming statistics. It is also the fundamental\nquantity that reinforcement learning AI bases on. Unfortunately, this\nconditional probability can be misleading. Unless all players are equally\nskilled, this number does not tell us the intrinsic value of such move. That is\nbecause conditioning on one good move also inevitably selects a subset of\nbetter players. They tend to make other good moves, which also contribute to\nthe extra winning chance. We present a simple toy model to quantify this \"skill\nbias\" effect, and then propose a general method to remove it. Our method is\nmodular, generalizable, and also only requires easily obtainable quantities\nfrom data. In particular, it gets the same answer independent of whether the\ndata comes from a group of good or bad players. This may help us to eventually\nbreak free from the conventional wisdom of \"learning from the experts\" and\navoid the Group Thinking pitfall.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 19:21:41 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Yang", "I-Sheng", ""]]}, {"id": "1803.05544", "submitter": "Junyong Park", "authors": "Junyong Park, Iris Ivy Gauran", "title": "Testing the homogeneity of risk differences with sparse count data", "comments": "There are some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider testing the homogeneity of risk differences in\nindependent binomial distributions especially when data are sparse. We point\nout some drawback of existing tests in either controlling a nominal size or\nobtaining powers through theoretical and numerical studies. The proposed test\nis designed to avoid such drawback of existing tests. We present the asymptotic\nnull distributions and asymptotic powers for our proposed test. We also provide\nnumerical studies including simulations and real data examples showing the\nproposed test has reliable results compared to existing testing procedures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 00:03:46 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 19:36:32 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 02:05:50 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Park", "Junyong", ""], ["Gauran", "Iris Ivy", ""]]}, {"id": "1803.05554", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Tamara Broderick and Caroline Uhler", "title": "Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models", "comments": "Proceedings of the 30th International Conference on Machine Learning.\n  2018, to appear. 16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 00:53:25 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 23:44:43 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 15:52:36 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""], ["Uhler", "Caroline", ""]]}, {"id": "1803.05582", "submitter": "Kurt Riedel", "authors": "Werner Kozek, Kurt Riedel", "title": "On the Underspread/Overspread Classification of Random Processes", "comments": null, "journal-ref": "Conference: Time-Frequency and Time-Scale Analysis, Oct. 1994.,\n  Proceedings of the IEEE-SP International Symposium on", "doi": null, "report-no": null, "categories": "stat.ME eess.AS eess.SP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the impact of the recently introduced underspread/overspread\nclassificationon the spectra of processes with square-integrable covariance\nfunctions. We briefly review the most prominent definitions of a time-varying\npower spectrum and point out their limited applicability for {\\em general}\nnonstationary processes. The time-frequency-parametrized approximation of the\nnonstationary Wiener filter provides an excellent example for the main\nconclusion: It is the class of underspread processeswhere a time--varying power\nspectrum can be used in the same manner as the time--invariant power spectrum\nof stationary processes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 03:40:13 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kozek", "Werner", ""], ["Riedel", "Kurt", ""]]}, {"id": "1803.05649", "submitter": "Jakub Tomczak Ph.D.", "authors": "Rianne van den Berg and Leonard Hasenclever and Jakub M. Tomczak and\n  Max Welling", "title": "Sylvester Normalizing Flows for Variational Inference", "comments": "Published at UAI 2018, 12 pages, 3 figures, code at:\n  https://github.com/riannevdberg/sylvester-flows", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference relies on flexible approximate posterior distributions.\nNormalizing flows provide a general recipe to construct flexible variational\nposteriors. We introduce Sylvester normalizing flows, which can be seen as a\ngeneralization of planar flows. Sylvester normalizing flows remove the\nwell-known single-unit bottleneck from planar flows, making a single\ntransformation much more flexible. We compare the performance of Sylvester\nnormalizing flows against planar flows and inverse autoregressive flows and\ndemonstrate that they compare favorably on several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:15:14 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 18:36:23 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Berg", "Rianne van den", ""], ["Hasenclever", "Leonard", ""], ["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1803.05793", "submitter": "Luca Rossini", "authors": "Federico Bassetti, Roberto Casarin, Luca Rossini", "title": "Hierarchical Species Sampling Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general class of hierarchical nonparametric prior\ndistributions. The random probability measures are constructed by a hierarchy\nof generalized species sampling processes with possibly non-diffuse base\nmeasures. The proposed framework provides a general probabilistic foundation\nfor hierarchical random measures with either atomic or mixed base measures and\nallows for studying their properties, such as the distribution of the marginal\nand total number of clusters. We show that hierarchical species sampling models\nhave a Chinese Restaurants Franchise representation and can be used as prior\ndistributions to undertake Bayesian nonparametric inference. We provide a\nmethod to sample from the posterior distribution together with some numerical\nillustrations. Our class of priors includes some new hierarchical mixture\npriors such as the hierarchical Gnedin measures, and other well-known prior\ndistributions such as the hierarchical Pitman-Yor and the hierarchical\nnormalized random measures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 15:04:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Bassetti", "Federico", ""], ["Casarin", "Roberto", ""], ["Rossini", "Luca", ""]]}, {"id": "1803.05874", "submitter": "Jingchen Hu", "authors": "Joerg Drechsler, Jingchen Hu", "title": "Synthesizing geocodes to facilitate access to detailed geographical\n  information in large scale administrative data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether generating synthetic data can be a viable strategy for\nproviding access to detailed geocoding information for external researchers,\nwithout compromising the confidentiality of the units included in the database.\nOur work was motivated by a recent project at the Institute for Employment\nResearch (IAB) in Germany that linked exact geocodes to the Integrated\nEmployment Biographies (IEB), a large administrative database containing\nseveral million records. We evaluate the performance of three synthesizers\nregarding the trade-off between preserving analytical validity and limiting\ndisclosure risks: One synthesizer employs Dirichlet Process mixtures of\nproducts of multinomials (DPMPM), while the other two use different versions of\nClassification and Regression Trees (CART). In terms of preserving analytical\nvalidity, our proposed synthesis strategy for geocodes based on categorical\nCART models outperforms the other two. If the risks of the synthetic data\ngenerated by the categorical CART synthesizer are deemed too high, we\ndemonstrate that synthesizing additional variables is the preferred strategy to\naddress the risk-utility trade-off in practice, compared to limiting the size\nof the regression trees or relying on the strategy of providing geographical\ninformation only on an aggregated level. We also propose strategies for making\nthe synthesizers scalable for large files, present analytical validity measures\nand disclosure risk measures for the generated data, and provide general\nrecommendations for statistical agencies considering the synthetic data\napproach for disseminating detailed geographical information.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:21:25 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:30:52 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 20:51:04 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Drechsler", "Joerg", ""], ["Hu", "Jingchen", ""]]}, {"id": "1803.05926", "submitter": "Benjamin Deonovic", "authors": "Benjamin Deonovic, Michael Yudelson, Maria Bolsinova, Meirav Attali,\n  and Gunter Maris", "title": "Learning meets Assessment: On the relation between Item Response Theory\n  and Bayesian Knowledge Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few models have been more ubiquitous in their respective fields than Bayesian\nknowledge tracing and item response theory. Both of these models were developed\nto analyze data on learners. However, the study designs that these models are\ndesigned for differ; Bayesian knowledge tracing is designed to analyze\nlongitudinal data while item response theory is built for cross-sectional data.\nThis paper illustrates a fundamental connection between these two models.\nSpecifically, the stationary distribution of the latent variable and the\nobserved response variable in Bayesian knowledge Tracing are related to an item\nresponse theory model. This connection between these two models highlights a\nkey missing component: the role of education in these models. A research agenda\nis outlined which answers how to move forward with modeling learner data.\n%Furthermore, recent advances in network psychometrics demonstrate how this\nrelationship can be exploited and generalized to a network model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 18:03:35 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 15:11:14 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Deonovic", "Benjamin", ""], ["Yudelson", "Michael", ""], ["Bolsinova", "Maria", ""], ["Attali", "Meirav", ""], ["Maris", "Gunter", ""]]}, {"id": "1803.06040", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "False discovery rate control for multiple testing based on p-values with\n  c\\`adl\\`ag distribution functions", "comments": "18 pages (including 6 figures)", "journal-ref": "Biometrical Journal, 2020", "doi": "10.1002/bimj.201900163", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For multiple testing based on p-values with c\\`{a}dl\\`{a}g distribution\nfunctions, we propose an FDR procedure \"BH+\" with proven conservativeness. BH+\nis at least as powerful as the BH procedure when they are applied to\nsuper-uniform p-values. Further, when applied to mid p-values, BH+ is more\npowerful than it is applied to conventional p-values. An easily verifiable\nnecessary and sufficient condition for this is provided. BH+ is perhaps the\nfirst conservative FDR procedure applicable to mid p-values. BH+ is applied to\nmultiple testing based on discrete p-values in a methylation study, an HIV\nstudy and a clinical safety study, where it makes considerably more discoveries\nthan the BH procedure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 00:10:22 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "1803.06044", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Optimal Boundary Kernels and Weightings for Local Polynomial Regression", "comments": "Manuscript date: 1993", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel smoothers are considered near the boundary of the interval. Kernels\nwhich minimize the expected mean square error are derived. These kernels are\nequivalent to using a linear weighting function in the local polynomial\nregression. It is shown that any kernel estimator that satisfies the moment\nconditions up to order $m$ is equivalent to a local polynomial regression of\norder $m$ with some non-negative weight function if and only if the kernel has\nat most $m$ sign changes. A fast algorithm is proposed for computing the kernel\nestimate in the boundary region for an arbitrary placement of data points.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 00:52:06 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 17:42:59 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.06048", "submitter": "Lo-Hua Yuan", "authors": "Lo-Hua Yuan, Avi Feller, Luke W. Miratrix", "title": "Identifying and Estimating Principal Causal Effects in Multi-site Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials are often conducted with separate randomizations across\nmultiple sites such as schools, voting districts, or hospitals. These sites can\ndiffer in important ways, including the site's implementation, local\nconditions, and the composition of individuals. An important question in\npractice is whether---and under what assumptions---researchers can leverage\nthis cross-site variation to learn more about the intervention. We address\nthese questions in the principal stratification framework, which describes\ncausal effects for subgroups defined by post-treatment quantities. We show that\nresearchers can estimate certain principal causal effects via the multi-site\ndesign if they are willing to impose the strong assumption that the\nsite-specific effects are uncorrelated with the site-specific distribution of\nstratum membership. We motivate this approach with a multi-site trial of the\nEarly College High School Initiative, a unique secondary education program with\nthe goal of increasing high school graduation rates and college enrollment. Our\nanalyses corroborate previous studies suggesting that the initiative had\npositive effects for students who would have otherwise attended a low-quality\nhigh school, although power is limited.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:12:26 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Yuan", "Lo-Hua", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke W.", ""]]}, {"id": "1803.06050", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Sufficient Conditions for a Linear Estimator to be a Local Polynomial\n  Regression", "comments": "Manuscript date 1993", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that any linear estimator that satisfies the moment conditions up\nto order $p$ is equivalent to a local polynomial regression of order $p$ with\nsome non-negative weight function if and only if the kernel has at most $p$\nsign changes. If the data points are placed symmetrically about the estimation\npoint, a linear weighting function is equivalent to the standard quadratic\nweighting function.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:23:47 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.06053", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Paul Gustafson", "title": "The world of research has gone berserk: modeling the consequences of\n  requiring \"greater statistical stringency\" for scientific publication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to growing concern about the reliability and reproducibility of\npublished science, researchers have proposed adopting measures of greater\nstatistical stringency, including suggestions to require larger sample sizes\nand to lower the highly criticized p<0.05 significance threshold. While pros\nand cons are vigorously debated, there has been little to no modeling of how\nadopting these measures might affect what type of science is published. In this\npaper, we develop a novel optimality model that, given current incentives to\npublish, predicts a researcher's most rational use of resources in terms of the\nnumber of studies to undertake, the statistical power to devote to each study,\nand the desirable pre-study odds to pursue. We then develop a methodology that\nallows one to estimate the reliability of published research by considering a\ndistribution of preferred research strategies. Using this approach, we\ninvestigate the merits of adopting measures of `greater statistical stringency'\nwith the goal of informing the ongoing debate.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:43:58 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 00:07:12 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Campbell", "Harlan", ""], ["Gustafson", "Paul", ""]]}, {"id": "1803.06200", "submitter": "Dong Wan Shin Dr", "authors": "Ji-Eun Choi, Dong Wan Shin", "title": "Quantile correlation coefficient: a new tail dependence measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new measure related with tail dependence in terms of\ncorrelation: quantile correlation coefficient of random variables X, Y. The\nquantile correlation is defined by the geometric mean of two quantile\nregression slopes of X on Y and Y on X in the same way that the Pearson\ncorrelation is related with the regression coefficients of Y on X and X on Y.\nThe degree of tail dependent association in X, Y, if any, is well reflected in\nthe quantile correlation. The quantile correlation makes it possible to measure\nsensitivity of a conditional quantile of a random variable with respect to\nchange of the other variable. The properties of the quantile correlation are\nsimilar to those of the correlation. This enables us to interpret it from the\nperspective of correlation, on which tail dependence is reflected. We construct\nmeasures for tail dependent correlation and tail asymmetry and develop\nstatistical tests for them. We prove asymptotic normality of the estimated\nquantile correlation and limiting null distributions of the proposed tests,\nwhich is well supported in finite samples by a Monte-Carlo study. The proposed\nquantile correlation methods are well illustrated by analyzing birth weight\ndata set and stock return data set.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:54:52 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Choi", "Ji-Eun", ""], ["Shin", "Dong Wan", ""]]}, {"id": "1803.06258", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu, Benjamin Paul Chamberlain", "title": "Online Controlled Experiments for Personalised e-Commerce Strategies:\n  Design, Challenges, and Pitfalls", "comments": "Not peer-reviewed but retained for historic interest. Removed an\n  erroneous statement on Welch's t-test assumptions in Section 3.2. 9 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online controlled experiments are the primary tool for measuring the causal\nimpact of product changes in digital businesses. It is increasingly common for\ndigital products and services to interact with customers in a personalised way.\nUsing online controlled experiments to optimise personalised interaction\nstrategies is challenging because the usual assumption of statistically\nequivalent user groups is violated. Additionally, challenges are introduced by\nusers qualifying for strategies based on dynamic, stochastic attributes.\nTraditional A/B tests can salvage statistical equivalence by pre-allocating\nusers to control and exposed groups, but this dilutes the experimental metrics\nand reduces the test power. We present a stacked incrementality test framework\nthat addresses problems with running online experiments for personalised user\nstrategies. We derive bounds that show that our framework is superior to the\nbest simple A/B test given enough users and that this condition is easily met\nfor large scale online experiments. In addition, we provide a test power\ncalculator and describe a selection of pitfalls and lessons learnt from our\nexperience using it.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 14:59:06 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 14:09:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1803.06287", "submitter": "Ranjan Maitra", "authors": "Karl T. Pazdernik and Ranjan Maitra and Douglas Nychka and Stephen\n  Sain", "title": "Reduced Basis Kriging for Big Spatial Fields", "comments": "Sankhya, Series A, accepted for publication", "journal-ref": "Sankhya, Series A, 80:2:280--300, 2018", "doi": "10.1007/s13171-018-0129-7", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, a common method for prediction over a Gaussian random\nfield (GRF) is maximum likelihood estimation combined with kriging. For massive\ndata sets, kriging is computationally intensive, both in terms of CPU time and\nmemory, and so fixed rank kriging has been proposed as a solution. The method\nhowever still involves operations on large matrices, so we develop an\nalteration to this method by utilizing the approximations made in fixed rank\nkriging combined with restricted maximum likelihood estimation and sparse\nmatrix methodology. Experiments show that our methodology can provide\nadditional gains in computational efficiency over fixed-rank kriging without\nloss of accuracy in prediction. The methodology is applied to climate data\narchived by the United States National Climate Data Center, with very good\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 03:37:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 12:11:14 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pazdernik", "Karl T.", ""], ["Maitra", "Ranjan", ""], ["Nychka", "Douglas", ""], ["Sain", "Stephen", ""]]}, {"id": "1803.06365", "submitter": "Marlena Maziarz", "authors": "Marlena Maziarz, Yukun Liu, Jing Qin and Ruth Pfeiffer", "title": "Inference for case-control studies with incident and prevalent cases", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a fully efficient method to estimate associations of an\nexposure with disease incidence when both, incident cases and prevalent cases,\ni.e. individuals who were diagnosed with the disease at some prior time point\nand are alive at the time of sampling, are included in a case-control study. We\nextend the exponential tilting model for the relationship between exposure and\ncase status to accommodate two case groups, and correct for the survival bias\nin the prevalent cases through a tilting term that depends on the parametric\ndistribution of the backward time, i.e. the time from disease diagnosis to\nstudy enrollment. We construct an empirical likelihood that also incorporates\nthe observed backward times for prevalent cases, obtain efficient estimates of\nodds ratio parameters that relate exposure to disease incidence and propose a\nlikelihood ratio test for model parameters that has a standard chi-squared\ndistribution. We quantify the changes in efficiency of association parameters\nwhen incident cases are supplemented with, or replaced by, prevalent cases in\nsimulations. We illustrate our methods by estimating associations of single\nnucleotide polymorphisms (SNPs) with breast cancer incidence in a sample of\ncontrols and incident and prevalent cases from the U.S. Radiologic\nTechnologists Health Study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 18:29:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Maziarz", "Marlena", ""], ["Liu", "Yukun", ""], ["Qin", "Jing", ""], ["Pfeiffer", "Ruth", ""]]}, {"id": "1803.06383", "submitter": "Gul Inan", "authors": "Gul Inan, Mahbub A.H.M. Latif, and John Preisser", "title": "A prediction criterion for working correlation structure selection in\n  GEE", "comments": "28 pages, 1 figure, 20 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized estimating equations (GEE) is one of the most commonly used\nmethods for marginal regression analysis of longitudinal data, especially with\ndiscrete outcomes. The GEE method models the association among the responses of\na subject through a working correlation matrix and correct specification of the\nworking correlation structure ensures efficient estimation of the regression\nparameters. This study proposes a predicted residual sum of squares (PRESS)\nstatistic as a working correlation selection criterion in GEE. An extensive\nsimulation study is designed to assess the performance of the proposed GEE\nPRESS criterion and to compare its performance with well-known existing\ncriteria in the literature. The results show that the GEE PRESS criterion has\nbetter performance than the weighted error sum of squares SC criterion in all\ncases and is comparable with that of other existing criteria when the true\nworking correlation structure is AR(1) or exchangeable. Lastly, the working\ncorrelation selection criteria are illustrated with the Coronary Artery Risk\nDevelopment in Young Adults study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 19:45:47 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Inan", "Gul", ""], ["Latif", "Mahbub A. H. M.", ""], ["Preisser", "John", ""]]}, {"id": "1803.06517", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner, Rainer Schwabe, Heinz Holling", "title": "Optimal Designs for the Generalized Partial Credit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing ordinal data becomes increasingly important in psychology,\nespecially in the context of item response theory. The generalized partial\ncredit model (GPCM) is probably the most widely used ordinal model and finds\napplication in many large scale educational assessment studies such as PISA. In\nthe present paper, optimal test designs are investigated for estimating\npersons' abilities with the GPCM for calibrated tests when item parameters are\nknown from previous studies. We will derive that local optimality may be\nachieved by assigning non-zero probability only to the first and last category\nindependently of a person's ability. That is, when using such a design, the\nGPCM reduces to the dichotomous 2PL model. Since locally optimal designs\nrequire the true ability to be known, we consider alternative Bayesian design\ncriteria using weight distributions over the ability parameter space. For\nsymmetric weight distributions, we derive necessary conditions for the optimal\none-point design of two response categories to be Bayes optimal. Furthermore,\nwe discuss examples of common symmetric weight distributions and investigate,\nin which cases the necessary conditions are also sufficient. Since the 2PL\nmodel is a special case of the GPCM, all of these results hold for the 2PL\nmodel as well.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:07:52 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:49:51 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""], ["Schwabe", "Rainer", ""], ["Holling", "Heinz", ""]]}, {"id": "1803.06518", "submitter": "Eric Chi", "authors": "Eric C. Chi and Brian R. Gaines and Will Wei Sun and Hua Zhou and Jian\n  Yang", "title": "Provable Convex Co-clustering of Tensors", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research, 21(214):1-58, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:15:28 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:53:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chi", "Eric C.", ""], ["Gaines", "Brian R.", ""], ["Sun", "Will Wei", ""], ["Zhou", "Hua", ""], ["Yang", "Jian", ""]]}, {"id": "1803.06550", "submitter": "Beatriz Bueno-Larraz", "authors": "Jos\\'e R. Berrendero and Beatriz Bueno-Larraz and Antonio Cuevas", "title": "On Mahalanobis distance in functional settings", "comments": "27 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mahalanobis distance is a classical tool in multivariate analysis. We suggest\nhere an extension of this concept to the case of functional data. More\nprecisely, the proposed definition concerns those statistical problems where\nthe sample data are real functions defined on a compact interval of the real\nline. The obvious difficulty for such a functional extension is the\nnon-invertibility of the covariance operator in infinite-dimensional cases.\nUnlike other recent proposals, our definition is suggested and motivated in\nterms of the Reproducing Kernel Hilbert Space (RKHS) associated with the\nstochastic process that generates the data. The proposed distance is a true\nmetric; it depends on a unique real smoothing parameter which is fully\nmotivated in RKHS terms. Moreover, it shares some properties of its finite\ndimensional counterpart: it is invariant under isometries, it can be\nconsistently estimated from the data and its sampling distribution is known\nunder Gaussian models. An empirical study for two statistical applications,\noutliers detection and binary classification, is included. The obtained results\nare quite competitive when compared to other recent proposals of the\nliterature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 18:31:49 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Bueno-Larraz", "Beatriz", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1803.06557", "submitter": "Haiqing Xu", "authors": "Jason Abrevaya, Haiqing Xu", "title": "Estimation of treatment effects under endogeneous heteroskedasticity", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical literature on program evaluation limits its scope almost\nexclusively to models where treatment effects are homogenous for\nobservationally identical individuals. This paper considers a treatment effect\nmodel in which treatment effects may be heterogeneous, even among\nobservationally identical individuals. Specifically, extending the classical\ninstrumental variables (IV) model with an endogenous binary treatment and a\nbinary instrument, we allow the heteroskedasticity of the error disturbance to\nalso depend upon the treatment variable so that treatment has both mean and\nvariance effects on the outcome. In this endogenous heteroskedasticity IV\n(EHIV) model with heterogeneous individual treatment effects, the standard IV\nestimator can be inconsistent and lead to incorrect inference. After showing\nidentification of the mean and variance treatment effects in a nonparametric\nversion of the EHIV model, we provide closed-form estimators for the linear\nEHIV for the mean and variance treatment effects and the individual treatment\neffects (ITE). Asymptotic properties of the estimators are provided. A Monte\nCarlo simulation investigates the performance of the proposed approach, and an\nempirical application regarding the effects of fertility on female labor supply\nis considered.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 19:39:45 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 00:53:12 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 19:46:42 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Abrevaya", "Jason", ""], ["Xu", "Haiqing", ""]]}, {"id": "1803.06571", "submitter": "Kurt Riedel", "authors": "Andrew Mullhaupt, Kurt Riedel", "title": "Orthogonal Representations for Output System Pairs", "comments": "Work done in 200. Minor Revision 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of canonical forms is given proposed in which $(A, C)$ is in\nHessenberg observer or Schur form and output normal: $\\bf{I} - A^*A =C^*C$.\nHere, $C$ is the $d \\times n$ measurement matrix and $A$ is the advance matrix.\nThe $(C, A)$ stack is expressed as the product of $n$ orthogonal matrices, each\nof which depends on $d$ parameters. State updates require only ${\\cal O}(nd)$\noperations and derivatives of the system with respect to the parameters are\nfast and convenient to compute. Restrictions are given such that these models\nare generically identifiable. Since the observability Grammian is the identity\nmatrix, system identification is better conditioned than other classes of\nmodels with fast updates.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 21:19:27 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Mullhaupt", "Andrew", ""], ["Riedel", "Kurt", ""]]}, {"id": "1803.06578", "submitter": "Klaus Holst K", "authors": "Klaus K\\\"ahler Holst and Esben Budtz-J{\\o}rgensen", "title": "A two-stage estimation procedure for non-linear structural equation\n  models", "comments": null, "journal-ref": null, "doi": "10.1093/biostatistics/kxy082", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of structural equation models (SEMs) are often restricted to\nlinear associations between variables. Maximum likelihood (ML) estimation in\nnon-linear models may be complex and require numerical integration.\nFurthermore, ML inference is sensitive to distributional assumptions. In this\npaper, we introduce a simple two-stage estimation technique for estimation of\nnon-linear associations between latent variables. Here both steps are based on\nfitting linear SEMs: first a linear model is fitted to data on the latent\npredictor and terms describing the non-linear effect are predicted by their\nconditional means. In the second step, the predictions are included in a linear\nmodel for the latent outcome variable. We show that this procedure is\nconsistent and identifies its asymptotic distribution. We also illustrate how\nthis framework easily allows the association between latent variables to be\nmodelled using restricted cubic splines and we develop a modified estimator\nwhich is robust to non-normality of the latent predictor. In a simulation\nstudy, we compare the proposed method to MLE and alternative two-stage\nestimation techniques.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 21:56:28 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 20:13:17 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Holst", "Klaus K\u00e4hler", ""], ["Budtz-J\u00f8rgensen", "Esben", ""]]}, {"id": "1803.06645", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi, Clara Grazian, Kerrie Mengersen, Christian\n  Robert", "title": "Approximating the Likelihood in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  The conceptual and methodological framework that underpins approximate\nBayesian computation (ABC) is targetted primarily towards problems in which the\nlikelihood is either challenging or missing. ABC uses a simulation-based\nnon-parametric estimate of the likelihood of a summary statistic and assumes\nthat the generation of data from the model is computationally cheap. This\nchapter reviews two alternative approaches for estimating the intractable\nlikelihood, with the goal of reducing the necessary model simulations to\nproduce an approximate posterior. The first of these is a Bayesian version of\nthe synthetic likelihood (SL), initially developed by Wood (2010), which uses a\nmultivariate normal approximation to the summary statistic likelihood. Using\nthe parametric approximation as opposed to the non-parametric approximation of\nABC, it is possible to reduce the number of model simulations required. The\nsecond likelihood approximation method we consider in this chapter is based on\nthe empirical likelihood (EL), which is a non-parametric technique and involves\nmaximising a likelihood constructed empirically under a set of moment\nconstraints. Mengersen et al (2013) adapt the EL framework so that it can be\nused to form an approximate posterior for problems where ABC can be applied,\nthat is, for models with intractable likelihoods. However, unlike ABC and the\nBayesian SL (BSL), the Bayesian EL (BCel) approach can be used to completely\navoid model simulations in some cases. The BSL and BCel methods are illustrated\non models of varying complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 11:43:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Drovandi", "Christopher C", ""], ["Grazian", "Clara", ""], ["Mengersen", "Kerrie", ""], ["Robert", "Christian", ""]]}, {"id": "1803.06669", "submitter": "Adria Caballe", "authors": "Adria Caballe, Natalia Bochkina, Claus Mayer, Ioannis Papastathopoulos", "title": "Testing for equal correlation matrices with application to paired gene\n  expression data", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for testing the hypothesis of equality of two\ncorrelation matrices using paired high-dimensional datasets. We consider test\nstatistics based on the average of squares, maximum and sum of exceedances of\nFisher transform sample correlations and we derive approximate null\ndistributions using asymptotic and non-parametric distributions. Theoretical\nresults on the power of the tests are presented and backed up by a range of\nsimulation experiments. We apply the methodology to a case study of colorectal\ntumour gene expression data with the aim of discovering biological pathway\nlists of genes that present significantly different correlation matrices on\nhealthy and tumour samples. We find strong evidence for a large part of the\npathway lists correlation matrices to change among the two medical conditions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 14:32:41 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 07:13:45 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Caballe", "Adria", ""], ["Bochkina", "Natalia", ""], ["Mayer", "Claus", ""], ["Papastathopoulos", "Ioannis", ""]]}, {"id": "1803.06673", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson and Ravi Varadhan", "title": "Damped Anderson acceleration with restarts and monotonicity control for\n  accelerating EM and EM-like algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is a well-known iterative method\nfor computing maximum likelihood estimates from incomplete data. Despite its\nnumerous advantages, a main drawback of the EM algorithm is its frequently\nobserved slow convergence which often hinders the application of EM algorithms\nin high-dimensional problems or in other complex settings.To address the need\nfor more rapidly convergent EM algorithms, we describe a new class of\nacceleration schemes that build on the Anderson acceleration technique for\nspeeding fixed-point iterations. Our approach is effective at greatly\naccelerating the convergence of EM algorithms and is automatically scalable to\nhigh dimensional settings. Through the introduction of periodic algorithm\nrestarts and a damping factor, our acceleration scheme provides faster and more\nrobust convergence when compared to un-modified Anderson acceleration while\nalso improving global convergence. Crucially, our method works as an\n\"off-the-shelf\" method in that it may be directly used to accelerate any EM\nalgorithm without relying on the use of any model-specific features or\ninsights. Through a series of simulation studies involving five representative\nproblems, we show that our algorithm is substantially faster than the existing\nstate-of-art acceleration schemes.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:01:22 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 13:59:42 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1803.06675", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Jacob Bien", "title": "Rare Feature Selection in High Dimensions", "comments": "42 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in modern prediction problems for many predictor variables to be\ncounts of rarely occurring events. This leads to design matrices in which many\ncolumns are highly sparse. The challenge posed by such \"rare features\" has\nreceived little attention despite its prevalence in diverse areas, ranging from\nnatural language processing (e.g., rare words) to biology (e.g., rare species).\nWe show, both theoretically and empirically, that not explicitly accounting for\nthe rareness of features can greatly reduce the effectiveness of an analysis.\nWe next propose a framework for aggregating rare features into denser features\nin a flexible manner that creates better predictors of the response. Our\nstrategy leverages side information in the form of a tree that encodes feature\nsimilarity.\n  We apply our method to data from TripAdvisor, in which we predict the\nnumerical rating of a hotel based on the text of the associated review. Our\nmethod achieves high accuracy by making effective use of rare words; by\ncontrast, the lasso is unable to identify highly predictive words if they are\ntoo rare. A companion R package, called rare, implements our new estimator,\nusing the alternating direction method of multipliers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:15:49 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:30:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1803.06732", "submitter": "Helton Saulo", "authors": "Helton Saulo, Jeremias Leao, Juvencio Nobre, N. Balakrishnan", "title": "A class of asymmetric regression models for left-censored data", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common assumption regarding the standard tobit model is the normality of\nthe error distribution. However, asymmetry and bimodality may be present and\nalternative tobit models must be used. In this paper, we propose a tobit model\nbased on the class of log-symmetric distributions, which includes as special\ncases heavy and light tailed distributions and bimodal distributions. We\nimplement a likelihood-based approach for parameter estimation and derive a\ntype of residual. We then discuss the problem of performing testing inference\nin the proposed class by using the likelihood ratio and gradient statistics,\nwhich are particularly convenient for tobit models, as they do not require the\ninformation matrix. A thorough Monte Carlo study is presented to evaluate the\nperformance of the maximum likelihood estimators and the likelihood ratio and\ngradient tests. Finally, we illustrate the proposed methodology by using a\nreal-world data set.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:29:02 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Saulo", "Helton", ""], ["Leao", "Jeremias", ""], ["Nobre", "Juvencio", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1803.06735", "submitter": "Rui Zhu", "authors": "Rui Zhu and Subhashis Ghosal", "title": "Bayesian ROC surface estimation under verification bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Receiver Operating Characteristic (ROC) surface is a generalization of\nROC curve and is widely used for assessment of the accuracy of diagnostic tests\non three categories. A complication called the verification bias, meaning that\nnot all subjects have their true disease status verified often occur in real\napplication of ROC analysis. This is a common problem since the gold standard\ntest, which is used to generate true disease status, can be invasive and\nexpensive. In this paper, we will propose a Bayesian approach for estimating\nthe ROC surface based on continuous data under a semi-parametric trinormality\nassumption. Our proposed method often adopted in ROC analysis can also be\nextended to situation in the presence of verification bias. We compute the\nposterior distribution of the parameters under trinormality assumption by using\na rank-based likelihood. Consistency of the posterior under mild conditions is\nalso established. We compare our method with the existing methods for\nestimating ROC surface and conclude that our method performs well in terms of\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:35:05 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zhu", "Rui", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1803.06738", "submitter": "Kenichiro McAlinn", "authors": "Daniele Bianchi and Kenichiro McAlinn", "title": "Large-Scale Dynamic Predictive Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel \"decouple-recouple\" dynamic predictive strategy and\ncontribute to the literature on forecasting and economic decision making in a\ndata-rich environment. Under this framework, clusters of predictors generate\ndifferent latent states in the form of predictive densities that are later\nsynthesized within an implied time-varying latent factor model. As a result,\nthe latent inter-dependencies across predictive densities and biases are\nsequentially learned and corrected. Unlike sparse modeling and variable\nselection procedures, we do not assume a priori that there is a given subset of\nactive predictors, which characterize the predictive density of a quantity of\ninterest. We test our procedure by investigating the predictive content of a\nlarge set of financial ratios and macroeconomic variables on both the equity\npremium across different industries and the inflation rate in the U.S., two\ncontexts of topical interest in finance and macroeconomics. We find that our\npredictive synthesis framework generates both statistically and economically\nsignificant out-of-sample benefits while maintaining interpretability of the\nforecasting variables. In addition, the main empirical results highlight that\nour proposed framework outperforms both LASSO-type shrinkage regressions,\nfactor based dimension reduction, sequential variable selection, and\nequal-weighted linear pooling methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 21:01:01 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Bianchi", "Daniele", ""], ["McAlinn", "Kenichiro", ""]]}, {"id": "1803.06823", "submitter": "Matteo Iacopini", "authors": "Dominque Gu\\'egan and Matteo Iacopini", "title": "Nonparametric forecasting of multivariate probability density functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of dependence between random variables is the core of theoretical\nand applied statistics. Static and dynamic copula models are useful for\ndescribing the dependence structure, which is fully encrypted in the copula\nprobability density function. However, these models are not always able to\ndescribe the temporal change of the dependence patterns, which is a key\ncharacteristic of financial data. We propose a novel nonparametric framework\nfor modelling a time series of copula probability density functions, which\nallows to forecast the entire function without the need of post-processing\nprocedures to grant positiveness and unit integral. We exploit a suitable\nisometry that allows to transfer the analysis in a subset of the space of\nsquare integrable functions, where we build on nonparametric functional data\nanalysis techniques to perform the analysis. The framework does not assume the\ndensities to belong to any parametric family and it can be successfully applied\nalso to general multivariate probability density functions with bounded or\nunbounded support. Finally, a noteworthy field of application pertains the\nstudy of time varying networks represented through vine copula models. We apply\nthe proposed methodology for estimating and forecasting the time varying\ndependence structure between the S\\&P500 and NASDAQ indices.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 07:44:17 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Gu\u00e9gan", "Dominque", ""], ["Iacopini", "Matteo", ""]]}, {"id": "1803.06964", "submitter": "Emmanuel Candes", "authors": "Pragya Sur and Emmanuel J. Candes", "title": "A modern maximum-likelihood theory for high-dimensional logistic\n  regression", "comments": "29 pages, 14 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every student in statistics or data science learns early on that when the\nsample size largely exceeds the number of variables, fitting a logistic model\nproduces estimates that are approximately unbiased. Every student also learns\nthat there are formulas to predict the variability of these estimates which are\nused for the purpose of statistical inference; for instance, to produce\np-values for testing the significance of regression coefficients. Although\nthese formulas come from large sample asymptotics, we are often told that we\nare on reasonably safe grounds when $n$ is large in such a way that $n \\ge 5p$\nor $n \\ge 10p$. This paper shows that this is far from the case, and\nconsequently, inferences routinely produced by common software packages are\noften unreliable.\n  Consider a logistic model with independent features in which $n$ and $p$\nbecome increasingly large in a fixed ratio. Then we show that (1) the MLE is\nbiased, (2) the variability of the MLE is far greater than classically\npredicted, and (3) the commonly used likelihood-ratio test (LRT) is not\ndistributed as a chi-square. The bias of the MLE is extremely problematic as it\nyields completely wrong predictions for the probability of a case based on\nobserved values of the covariates. We develop a new theory, which\nasymptotically predicts (1) the bias of the MLE, (2) the variability of the\nMLE, and (3) the distribution of the LRT. We empirically also demonstrate that\nthese predictions are extremely accurate in finite samples. Further, an\nappealing feature is that these novel predictions depend on the unknown\nsequence of regression coefficients only through a single scalar, the overall\nstrength of the signal. This suggests very concrete procedures to adjust\ninference; we describe one such procedure learning a single parameter from data\nand producing accurate inference\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 14:49:27 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 22:18:02 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 20:13:08 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 04:06:47 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sur", "Pragya", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1803.07018", "submitter": "Antony Overstall", "authors": "Antony M. Overstall and James M. McGree", "title": "Bayesian design of experiments for intractable likelihood models using\n  coupled auxiliary models and multivariate emulation", "comments": "Minor & final update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian design is given by maximising an expected utility over a design\nspace. The utility is chosen to represent the aim of the experiment and its\nexpectation is taken with respect to all unknowns: responses, parameters and/or\nmodels. Although straightforward in principle, there are several challenges to\nfinding Bayesian designs in practice. Firstly, the utility and expected utility\nare rarely available in closed form and require approximation. Secondly, the\ndesign space can be of high-dimensionality. In the case of intractable\nlikelihood models, these problems are compounded by the fact that the\nlikelihood function, whose evaluation is required to approximate the expected\nutility, is not available in closed form. A strategy is proposed to find\nBayesian designs for intractable likelihood models. It relies on the\ndevelopment of an automatic, auxiliary modelling approach, using multivariate\nGaussian process emulators, to approximate the likelihood function. This is\nthen combined with a copula-based approach to approximate the marginal\nlikelihood (a quantity commonly required to evaluate many utility functions).\nThese approximations are demonstrated on examples of stochastic process models\ninvolving experimental aims of both parameter estimation and model comparison.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 16:22:22 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 11:38:33 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 09:35:32 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Overstall", "Antony M.", ""], ["McGree", "James M.", ""]]}, {"id": "1803.07069", "submitter": "Bruno Ebner", "authors": "Steffen Betsch and Bruno Ebner", "title": "Testing normality via a distributional fixed point property in the Stein\n  characterization", "comments": null, "journal-ref": "TEST, Volume 29, Issue 1, pages 105-138, (2020)", "doi": "10.1007/s11749-019-00630-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two families of tests for the classical goodness-of-fit problem to\nunivariate normality. The new procedures are based on $L^2$-distances of the\nempirical zero-bias transformation to the normal distribution or the empirical\ndistribution of the data, respectively. Weak convergence results are derived\nunder the null hypothesis, under fixed alternatives as well as under contiguous\nalternatives. Empirical critical values are provided and a comparative\nfinite-sample power study shows the competitiveness to classical procedures.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 17:59:12 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""]]}, {"id": "1803.07172", "submitter": "Alessandro Lomi", "authors": "Tom A.B.Snijders and Alessandro Lomi", "title": "Beyond Homophily: Incorporating Actor Variables in Actor-oriented\n  Network Models", "comments": "33 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the specification of effects of numerical actor attributes in\nstatistical models for directed social networks. A fundamental mechanism is\nhomophily or assortativity, where actors have a higher likelihood to be tied\nwith others having similar values of the variable under study. But there are\nother mechanisms that may also play a role in how the attribute values of two\nactors influence the likelihood of a tie. We discuss three additional\nmechanisms: aspiration to send ties to others having high values; conformity in\nthe sense of sending more ties to others whose values are close to what may be\nconsidered the `social norm'; and sociability, where those having higher values\nwill tend to send more ties generally. These mechanisms may operate jointly,\nand then their effects will be confounded. We present a specification\nrepresenting these effects simultaneously by a four-parameter quadratic\nfunction of the values of sender and receiver. Greater flexibility can be\nobtained by a five-parameter extension. We argue that empirical researchers\noften overlook the possibility that homophily may be confounded with these\nother mechanisms, and that for actor attributes that have important effects on\ndirected networks, these specifications may provide an improvement. An\nillustration is given of the dependence of advice ties on academic grades in a\nnetwork of MBA students, analyzed by the Stochastic Actor-oriented Model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 21:35:21 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 06:39:13 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Snijders", "Tom A. B.", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1803.07184", "submitter": "Zhanglong Cao", "authors": "Zhanglong Cao, David Bryant, Tim Molteno, Colin Fox, Matthew Parry", "title": "Adaptive Smoothing for Trajectory Reconstruction", "comments": "25 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory reconstruction is the process of inferring the path of a moving\nobject between successive observations. In this paper, we propose a smoothing\nspline -- which we name the V-spline -- that incorporates position and velocity\ninformation and a penalty term that controls acceleration. We introduce a\nparticular adaptive V-spline designed to control the impact of irregularly\nsampled observations and noisy velocity measurements. A cross-validation scheme\nfor estimating the V-spline parameters is given and we detail the performance\nof the V-spline on four particularly challenging test datasets. Finally, an\napplication of the V-spline to vehicle trajectory reconstruction in two\ndimensions is given, in which the penalty term is allowed to further depend on\nknown operational characteristics of the vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 22:32:05 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 04:21:29 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Cao", "Zhanglong", ""], ["Bryant", "David", ""], ["Molteno", "Tim", ""], ["Fox", "Colin", ""], ["Parry", "Matthew", ""]]}, {"id": "1803.07247", "submitter": "Ziping Zhao", "authors": "Ziping Zhao, Daniel P. Palomar", "title": "Sparse Reduced Rank Regression With Nonconvex Regularization", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the estimation problem for sparse reduced rank regression\n(SRRR) model is considered. The SRRR model is widely used for dimension\nreduction and variable selection with applications in signal processing,\neconometrics, etc. The problem is formulated to minimize the least squares loss\nwith a sparsity-inducing penalty considering an orthogonality constraint.\nConvex sparsity-inducing functions have been used for SRRR in literature. In\nthis work, a nonconvex function is proposed for better sparsity inducing. An\nefficient algorithm is developed based on the alternating minimization (or\nprojection) method to solve the nonconvex optimization problem. Numerical\nsimulations show that the proposed algorithm is much more efficient compared to\nthe benchmark methods and the nonconvex function can result in a better\nestimation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 04:07:02 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Zhao", "Ziping", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1803.07418", "submitter": "Yang Feng", "authors": "Emre Demirkaya, Yang Feng, Pallavi Basu, Jinchi Lv", "title": "Large-Scale Model Selection with Misspecification", "comments": "38 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1412.7468", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:10:12 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Demirkaya", "Emre", ""], ["Feng", "Yang", ""], ["Basu", "Pallavi", ""], ["Lv", "Jinchi", ""]]}, {"id": "1803.07548", "submitter": "Wei Deng", "authors": "Wei Q. Deng and Radu V. Craiu", "title": "Data Distillery: Effective Dimension Estimation via Penalized\n  Probabilistic PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper tackles the unsupervised estimation of the effective dimension of a\nsample of dependent random vectors. The proposed method uses the principal\ncomponents (PC) decomposition of sample covariance to establish a low-rank\napproximation that helps uncover the hidden structure. The number of PCs to be\nincluded in the decomposition is determined via a Probabilistic Principal\nComponents Analysis (PPCA) embedded in a penalized profile likelihood\ncriterion. The choice of penalty parameter is guided by a data-driven procedure\nthat is justified via analytical derivations and extensive finite sample\nsimulations. Application of the proposed penalized PPCA is illustrated with\nthree gene expression datasets in which the number of cancer subtypes is\nestimated from all expression measurements. The analyses point towards hidden\nstructures in the data, e.g. additional subgroups, that could be of scientific\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:42:53 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 15:32:13 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Deng", "Wei Q.", ""], ["Craiu", "Radu V.", ""]]}, {"id": "1803.07859", "submitter": "Jack Kuipers", "authors": "Jack Kuipers, Polina Suter and Giusi Moffa", "title": "Efficient Sampling and Structure Learning of Bayesian Networks", "comments": "Revised version. 40 pages including 16 pages of supplement, 5 figures\n  and 15 supplemental figures; R package BiDAG is available at\n  https://CRAN.R-project.org/package=BiDAG", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are probabilistic graphical models widely employed to\nunderstand dependencies in high dimensional data, and even to facilitate causal\ndiscovery. Learning the underlying network structure, which is encoded as a\ndirected acyclic graph (DAG) is highly challenging mainly due to the vast\nnumber of possible networks. Efforts have focussed on two fronts:\nconstraint-based methods that perform conditional independence tests to exclude\nedges and score and search approaches which explore the DAG space with greedy\nor MCMC schemes. Here we synthesise these two fields in a novel hybrid method\nwhich reduces the complexity of MCMC approaches to that of a constraint-based\nmethod. Individual steps in the MCMC scheme only require simple table lookups\nso that very long chains can be efficiently obtained. Furthermore, the scheme\nincludes an iterative procedure to correct for errors from the conditional\nindependence tests. The algorithm offers markedly superior performance to\nalternatives, particularly because DAGs can also be sampled from the posterior\ndistribution, enabling full Bayesian model averaging for much larger Bayesian\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 11:12:42 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 15:25:13 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 18:33:10 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Kuipers", "Jack", ""], ["Suter", "Polina", ""], ["Moffa", "Giusi", ""]]}, {"id": "1803.07961", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang and Yuguo Chen", "title": "Modularity based community detection in heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous networks are networks consisting of different types of nodes\nand multiple types of edges linking such nodes. While community detection has\nbeen extensively developed as a useful technique for analyzing networks that\ncontain only one type of nodes, very few community detection techniques have\nbeen developed for heterogeneous networks. In this paper, we propose a\nmodularity based community detection framework for heterogeneous networks.\nUnlike existing methods, the proposed approach has the flexibility to treat the\nnumber of communities as an unknown quantity. We describe a Louvain type\nmaximization method for finding the community structure that maximizes the\nmodularity function. Our simulation results show the advantages of the proposed\nmethod over existing methods. Moreover, the proposed modularity function is\nshown to be consistent under a heterogeneous stochastic blockmodel framework.\nAnalyses of the DBLP four-area dataset and a MovieLens dataset demonstrate the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 15:23:07 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Zhang", "Jingfei", ""], ["Chen", "Yuguo", ""]]}, {"id": "1803.07984", "submitter": "Dan Li", "authors": "Dan Li and Sonia Martinez", "title": "Online data assimilation in distributionally robust optimization", "comments": "Appeared in CDC 2018", "journal-ref": null, "doi": "10.1109/CDC.2018.8619159", "report-no": null, "categories": "math.OC cs.SY eess.SP eess.SY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a class of real-time decision making problems to\nminimize the expected value of a function that depends on a random variable\n$\\xi$ under an unknown distribution $\\mathbb{P}$. In this process, samples of\n$\\xi$ are collected sequentially in real time, and the decisions are made,\nusing the real-time data, to guarantee out-of-sample performance. We approach\nthis problem in a distributionally robust optimization framework and propose a\nnovel Online Data Assimilation Algorithm for this purpose. This algorithm\nguarantees the out-of-sample performance in high probability, and gradually\nimproves the quality of the data-driven decisions by incorporating the\nstreaming data. We show that the Online Data Assimilation Algorithm guarantees\nconvergence under the streaming data, and a criteria for termination of the\nalgorithm after certain number of data has been collected.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:09:44 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 20:48:19 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 22:35:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Dan", ""], ["Martinez", "Sonia", ""]]}, {"id": "1803.08000", "submitter": "Indrayudh Ghosal", "authors": "Indrayudh Ghosal, Giles Hooker", "title": "Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its\n  Variance Estimate", "comments": "39 pages, 7 tables, 3 figures", "journal-ref": null, "doi": "10.1080/10618600.2020.1820345", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose using the principle of boosting to reduce the bias\nof a random forest prediction in the regression setting. From the original\nrandom forest fit we extract the residuals and then fit another random forest\nto these residuals. We call the sum of these two random forests a\n\\textit{one-step boosted forest}. We show with simulated and real data that the\none-step boosted forest has a reduced bias compared to the original random\nforest. The paper also provides a variance estimate of the one-step boosted\nforest by an extension of the infinitesimal Jackknife estimator. Using this\nvariance estimate we can construct prediction intervals for the boosted forest\nand we show that they have good coverage probabilities. Combining the bias\nreduction and the variance estimate we show that the one-step boosted forest\nhas a significant reduction in predictive mean squared error and thus an\nimprovement in predictive performance. When applied on datasets from the UCI\ndatabase, one-step boosted forest performs better than random forest and\ngradient boosting machine algorithms. Theoretically we can also extend such a\nboosting process to more than one step and the same principles outlined in this\npaper can be used to find variance estimates for such predictors. Such boosting\nwill reduce bias even further but it risks over-fitting and also increases the\ncomputational burden.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:41:30 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 16:34:47 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 20:00:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Ghosal", "Indrayudh", ""], ["Hooker", "Giles", ""]]}, {"id": "1803.08027", "submitter": "Ranjan Maitra", "authors": "Ranjan Maitra", "title": "Efficient Bandwidth Estimation in Two-dimensional Filtered\n  Backprojection Reconstruction", "comments": "12 pages, 7 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2919428", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized cross-validation approach to estimate the reconstruction filter\nbandwidth in two-dimensional Filtered Backprojection is presented. The method\nwrites the reconstruction equation in equivalent backprojected filtering form,\nderives results on eigendecomposition of symmetric two-dimensional circulant\nmatrices and applies them to make bandwidth estimation a computationally\nefficient operation within the context of standard backprojected filtering\nreconstruction. Performance evaluations on a wide range of simulated emission\ntomography experiments give promising results. The superior performance holds\nat both low and high total expected counts, pointing to the method's\napplicability even in weaker signal-noise situations. The approach also applies\nto the more general class of elliptically symmetric filters, with\nreconstruction performance often better than even that obtained with the true\noptimal radially symmetric filter.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:26:41 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 11:13:50 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 16:20:11 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 18:32:06 GMT"}, {"version": "v5", "created": "Sun, 21 Apr 2019 15:35:53 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Maitra", "Ranjan", ""]]}, {"id": "1803.08154", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val and Martin Weidner", "title": "Network and Panel Quantile Effects Via Distribution Regression", "comments": "71 pages, 8 figures, 3 tables, includes supplementary appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a method to construct simultaneous confidence bands for\nquantile functions and quantile effects in nonlinear network and panel models\nwith unobserved two-way effects, strictly exogenous covariates, and possibly\ndiscrete outcome variables. The method is based upon projection of simultaneous\nconfidence bands for distribution functions constructed from fixed effects\ndistribution regression estimators. These fixed effects estimators are debiased\nto deal with the incidental parameter problem. Under asymptotic sequences where\nboth dimensions of the data set grow at the same rate, the confidence bands for\nthe quantile functions and effects have correct joint coverage in large\nsamples. An empirical application to gravity models of trade illustrates the\napplicability of the methods to network data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 22:26:15 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 18:14:29 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 14:18:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Weidner", "Martin", ""]]}, {"id": "1803.08155", "submitter": "Gwena\\\"el GR Leday", "authors": "Gwena\\\"el G.R. Leday and Sylvia Richardson", "title": "Fast Bayesian inference in large Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite major methodological developments, Bayesian inference for Gaussian\ngraphical models remains challenging in high dimension due to the tremendous\nsize of the model space. This article proposes a method to infer the marginal\nand conditional independence structures between variables by multiple testing\nof hypotheses. Specifically, we introduce closed-form Bayes factors under the\nGaussian conjugate model to evaluate the null hypotheses of marginal and\nconditional independence between variables. Their computation for all pairs of\nvariables is shown to be extremely efficient, thereby allowing us to address\nlarge problems with thousands of nodes. Moreover, we derive exact tail\nprobabilities from the null distributions of the Bayes factors. These allow the\nuse of any multiplicity correction procedure to control error rates for\nincorrect edge inclusion. We demonstrate the proposed approach to graphical\nmodel selection on various simulated examples as well as on a large gene\nexpression data set from The Cancer Genome Atlas.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 22:26:26 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 19:15:27 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Leday", "Gwena\u00ebl G. R.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1803.08255", "submitter": "Maria Francesca  Marino", "authors": "Maria Francesca Marino, Marco Alfo'", "title": "A non-homogeneous hidden Markov model for partially observed\n  longitudinal responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout represents a typical issue to be addressed when dealing with\nlongitudinal studies. If the mechanism leading to missing information is\nnon-ignorable, inference based on the observed data only may be severely\nbiased. A frequent strategy to obtain reliable parameter estimates is based on\nthe use of individual-specific random coefficients that help capture sources of\nunobserved heterogeneity and, at the same time, define a reasonable structure\nof dependence between the longitudinal and the missing data process. We refer\nto elements in this class as random coefficient based dropout models (RCBDMs).\nWe propose a dynamic, semi-parametric, version of the standard RCBDM to deal\nwith discrete time to event. Time-varying random coefficients that evolve over\ntime according to a non-homogeneous hidden Markov chain are considered to model\ndependence between longitudinal responses recorded from the same subject. A\nseparate set of random coefficients is considered to model dependence between\nmissing data indicators. Last, the joint distribution of the random\ncoefficients in the two equations helps describe the dependence between the two\nprocesses. To ensure model flexibility and avoid unverifiable assumptions, we\nleave the joint distribution of the random coefficients unspecified and\nestimate it via nonparametric maximum likelihood. The proposal is applied to\ndata from the Leiden 85+ study on the evolution of cognitive functioning in the\nelderly.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 08:04:10 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Marino", "Maria Francesca", ""], ["Alfo'", "Marco", ""]]}, {"id": "1803.08393", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "Calibrating Model-Based Inferences and Decisions", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the frontiers of applied statistics progress through increasingly complex\nexperiments we must exploit increasingly sophisticated inferential models to\nanalyze the observations we make. In order to avoid misleading or outright\nerroneous inferences we then have to be increasingly diligent in scrutinizing\nthe consequences of those modeling assumptions. Fortunately model-based methods\nof statistical inference naturally define procedures for quantifying the scope\nof inferential outcomes and calibrating corresponding decision making\nprocesses. In this paper I review the construction and implementation of the\nparticular procedures that arise within frequentist and Bayesian methodologies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:06:40 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1803.08553", "submitter": "Grzegorz Sikora", "authors": "Grzegorz Sikora", "title": "Statistical test for fractional Brownian motion based on detrending\n  moving average algorithm", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.08.031", "report-no": null, "categories": "physics.data-an math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by contemporary and rich applications of anomalous diffusion\nprocesses we propose a new statistical test for fractional Brownian motion,\nwhich is one of the most popular models for anomalous diffusion systems. The\ntest is based on detrending moving average statistic and its probability\ndistribution. Using the theory of Gaussian quadratic forms we determined it as\na generalized chi-squared distribution. The proposed test could be generalized\nfor statistical testing of any centered non-degenerate Gaussian process.\nFinally, we examine the test via Monte Carlo simulations for two exemplary\nscenarios of subdiffusive and superdiffusive dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 19:20:06 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sikora", "Grzegorz", ""]]}, {"id": "1803.08671", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "Nonparametric inference on L\\'evy measures of compound Poisson-driven\n  Ornstein-Uhlenbeck processes under macroscopic discrete observations", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines a nonparametric inference on a stationary L\\'evy-driven\nOrnstein-Uhlenbeck (OU) process $X = (X_{t})_{t \\geq 0}$ with a compound\nPoisson subordinator. We propose a new spectral estimator for the L\\'evy\nmeasure of the L\\'evy-driven OU process $X$ under macroscopic observations. We\nalso derive, for the estimator, multivariate central limit theorems over a\nfinite number of design points, and high-dimensional central limit theorems in\nthe case wherein the number of design points increases with an increase in the\nsample size. Built on these asymptotic results, we develop methods to construct\nconfidence bands for the L\\'evy measure and propose a practical method for\nbandwidth selection.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 07:08:43 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:55:42 GMT"}, {"version": "v3", "created": "Sat, 21 Apr 2018 09:54:18 GMT"}, {"version": "v4", "created": "Tue, 5 Mar 2019 08:46:57 GMT"}, {"version": "v5", "created": "Thu, 11 Jul 2019 11:50:59 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "1803.08764", "submitter": "Xavier de Luna", "authors": "Eva Cantoni and Xavier de Luna", "title": "Robust semiparametric inference with missing data", "comments": "51 pages with appendices", "journal-ref": "Econometrics and Statistics, Volume 16, October 2020, Pages\n  108-120", "doi": "10.1016/j.ecosta.2020.01.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical semiparametric inference with missing outcome data is not robust to\ncontamination of the observed data and a single observation can have\narbitrarily large influence on estimation of a parameter of interest. This\nsensitivity is exacerbated when inverse probability weighting methods are used,\nwhich may overweight contaminated observations. We introduce inverse\nprobability weighted, double robust and outcome regression estimators of\nlocation and scale parameters, which are robust to contamination in the sense\nthat their influence function is bounded. We give asymptotic properties and\nstudy finite sample behaviour. Our simulated experiments show that\ncontamination can be more serious a threat to the quality of inference than\nmodel misspecification. An interesting aspect of our results is that the\nauxiliary outcome model used to adjust for ignorable missingness by some of the\nestimators, is also useful to protect against contamination. We also illustrate\nthrough a case study how both adjustment to ignorable missingness and\nprotection against contamination are achieved through weighting schemes, which\ncan be contrasted to gain further insights.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 12:24:05 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 10:27:33 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Cantoni", "Eva", ""], ["de Luna", "Xavier", ""]]}, {"id": "1803.08784", "submitter": "Stephan Bongers", "authors": "Stephan Bongers, Joris M. Mooij", "title": "From Random Differential Equations to Structural Causal Models: the\n  stochastic case", "comments": "Submitted to UAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Differential Equations provide a natural extension of Ordinary\nDifferential Equations to the stochastic setting. We show how, and under which\nconditions, every equilibrium state of a Random Differential Equation (RDE) can\nbe described by a Structural Causal Model (SCM), while pertaining the causal\nsemantics. This provides an SCM that captures the stochastic and causal\nbehavior of the RDE, which can model both cycles and confounders. This enables\nthe study of the equilibrium states of the RDE by applying the theory and\nstatistical tools available for SCMs, for example, marginalizations and Markov\nproperties, as we illustrate by means of an example. Our work thus provides a\ndirect connection between two fields that so far have been developing in\nisolation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 13:20:56 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 09:09:52 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Bongers", "Stephan", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1803.08882", "submitter": "Alexander B\\\"ottcher", "authors": "Alexander B\\\"ottcher, Wieland Brendel, Bernhard Englitz, Matthias\n  Bethge", "title": "Trace your sources in large-scale data: one ring to find them all", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important preprocessing step in most data analysis pipelines aims to\nextract a small set of sources that explain most of the data. Currently used\nalgorithms for blind source separation (BSS), however, often fail to extract\nthe desired sources and need extensive cross-validation. In contrast, their\nrarely used probabilistic counterparts can get away with little\ncross-validation and are more accurate and reliable but no simple and scalable\nimplementations are available. Here we present a novel probabilistic BSS\nframework (DECOMPOSE) that can be flexibly adjusted to the data, is extensible\nand easy to use, adapts to individual sources and handles large-scale data\nthrough algorithmic efficiency. DECOMPOSE encompasses and generalises many\ntraditional BSS algorithms such as PCA, ICA and NMF and we demonstrate\nsubstantial improvements in accuracy and robustness on artificial and real\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 16:56:13 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["B\u00f6ttcher", "Alexander", ""], ["Brendel", "Wieland", ""], ["Englitz", "Bernhard", ""], ["Bethge", "Matthias", ""]]}, {"id": "1803.09055", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Pape Djiby Mergane, Thilabola Atozou Kpanzou, Mohamed\n  Cheikh Haidara", "title": "Asymptotic Representations of Statistics in the Functional Empirical\n  process : A portal and some applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research monograph, we deal with a very general asymptotic\nrepresentation for statistics named GRI expressed in the functional empirical\nprocess, both one-dimensional and multidimensional, and another call residual\nempirical process. Most of statistics in form of combination of L-statistics\nare covered by the asymptotic theory dealt here. This treatise is conceived to\nbe a kind of \\textbf{spaceship} on which modules are hanged. The spaceship is a\nfunctional Gaussian process and each module is the asymptotic representation of\none statistic in terms of that Gaussian process. In that way, it is possible to\nnavigate from one module to another, that is, to find the joint distribution of\nany pair of statistics, to compare them with respect to the areas and the\ntimes. In order to be able to do so, we should have a broad conception at the\nbeginning. Within the constructed frame, the asymptotic joint law of any finite\nnumber of other statistics is automatically given as well as the joint\ndistribution of its spatial variation or temporal variation, in absolute or\nrelative values. We also deal with the general problem of decomposability of\nstatistics by comparing statistical decomposability, a new view we introduce,\nversus functional decomposability. A general result only based on the GRI is\nprovided. \\noindent This monograph is also the portal of a handbook of GRI that\nwill cover the largest number possible of statistics. In prevision of that, we\ntreat three important examples as show cases. It is expected that this portal\nand the handbook will attract the attention of researchers working in the\nasymptotic area and will furnish useful tools to scientists who are interested\nin application of asymptotic tests, completed by computer packages.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 05:06:36 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lo", "Gane Samb", ""], ["Mergane", "Pape Djiby", ""], ["Kpanzou", "Thilabola Atozou", ""], ["Haidara", "Mohamed Cheikh", ""]]}, {"id": "1803.09159", "submitter": "Edward McFowland Iii", "authors": "Edward McFowland III, Sriram Somanchi, Daniel B. Neill", "title": "Efficient Discovery of Heterogeneous Treatment Effects in Randomized\n  Experiments via Anomalous Pattern Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent literature on estimating heterogeneous treatment effects, each\nproposed method makes its own set of restrictive assumptions about the\nintervention's effects and which subpopulations to explicitly estimate.\nMoreover, the majority of the literature provides no mechanism to identify\nwhich subpopulations are the most affected--beyond manual inspection--and\nprovides little guarantee on the correctness of the identified subpopulations.\nTherefore, we propose Treatment Effect Subset Scan (TESS), a new method for\ndiscovering which subpopulation in a randomized experiment is most\nsignificantly affected by a treatment. We frame this challenge as a pattern\ndetection problem where we efficiently maximize a nonparametric scan statistic\nover subpopulations. Furthermore, we identify the subpopulation which\nexperiences the largest distributional change as a result of the intervention,\nwhile making minimal assumptions about the intervention's effects or the\nunderlying data generating process. In addition to the algorithm, we\ndemonstrate that the asymptotic Type I and II error can be controlled, and\nprovide sufficient conditions for detection consistency--i.e., exact\nidentification of the affected subpopulation. Finally, we validate the efficacy\nof the method by discovering heterogeneous treatment effects in simulations and\nin real-world data from a well-known program evaluation study.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 20:21:06 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 22:05:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["McFowland", "Edward", "III"], ["Somanchi", "Sriram", ""], ["Neill", "Daniel B.", ""]]}, {"id": "1803.09321", "submitter": "Zi Ye", "authors": "Zi Ye, Giles Hooker", "title": "Local Quadratic Estimation of the Curvature in a Functional Single Index\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonlinear effects of environmental variability on species abundance plays\nan important role in the maintenance of ecological diversity. Nonetheless, many\ncommon models use parametric nonlinear terms pre-determining ecological\nconclusions. Motivated by this concern, we study the estimate of the second\nderivative (curvature) of the link function g in a functional single index\nmodel. Since the coefficient function and the link function are both unknown,\nthe estimate is expressed as a nested optimization. For a fixed and unknown\ncoefficient function, the link function and its second derivative are estimated\nby local quadratic approximation, then the coefficient function is estimated by\nminimizing the MSE of the model. In this paper, we derive the rate of\nconvergence of the estimation. In addition, we prove that the argument of g,\ncan be estimated root-n consistently. However, practical implementation of the\nmethod requires solving a nonlinear optimization problem, and our results show\nthat the estimates of the link function and the coefficient function are quite\nsensitive to the choices of starting values.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 19:48:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ye", "Zi", ""], ["Hooker", "Giles", ""]]}, {"id": "1803.09365", "submitter": "Joseph Marion", "authors": "Joseph Marion and Scott C. Schmidler", "title": "Finite Sample Complexity of Sequential Monte Carlo Estimators", "comments": "Correcting an error in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present bounds for the finite sample error of sequential Monte Carlo\nsamplers on static spaces. Our approach explicitly relates the performance of\nthe algorithm to properties of the chosen sequence of distributions and mixing\nproperties of the associated Markov kernels. This allows us to give the first\nfinite sample comparison to other Monte Carlo schemes. We obtain bounds for the\ncomplexity of sequential Monte Carlo approximations for a variety of target\ndistributions including finite spaces, product measures, and log-concave\ndistributions including Bayesian logistic regression. The bounds obtained are\nwithin a logarithmic factor of similar bounds obtainable for Markov chain Monte\nCarlo.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 23:15:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 12:35:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Marion", "Joseph", ""], ["Schmidler", "Scott C.", ""]]}, {"id": "1803.09404", "submitter": "Kurt Riedel", "authors": "Kaya Imre, Kurt S. Riedel, Beatrix Schunke", "title": "A Hierarchy of Empirical Models of Plasma Profiles and Transport", "comments": null, "journal-ref": "Physics of Plasmas 2, 1614 (1995)", "doi": "10.1063/1.871311", "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two families of statistical models are presented which generalize global\nconfinement expressions to plasma profiles and local transport coefficients.\nThe temperature or diffusivity is parameterized as a function of the normalized\nflux radius, $\\bar{\\psi}$, and the engineering variables, ${\\bf u} =\n(I_p,B_t,\\bar{n},q_{95})^\\dagger$. The log-additive temperature model assumes\nthat $\\ln [T(\\bar{\\psi}, {\\bf u})] =$ $f_0 (\\bar{\\psi}) + f_I\n(\\bar{\\psi})\\ln[I_p]$ $+ f_B (\\bar{\\psi}) \\ln [B_t]$ $+ f_n (\\bar{\\psi}) \\ln [\n\\bar{n}] + f_{q}\\ln[q_{95}]$. The unknown $f_i (\\bar{\\psi})$ are estimated\nusing smoothing splines. A 43 profile Ohmic data set from the Joint European\nTorus is analyzed and its shape dependencies are described. The best fit has an\naverage error of 152 eV which is 10.5 \\% percent of the typical line average\ntemperature. The average error is less than the estimated measurement error\nbars. The second class of models is log-additive diffusivity models where $\\ln\n[ \\chi (\\bar{\\psi}, {\\bf u})] $ $=\\ g_0 (\\bar{\\psi}) + g_I (\\bar{\\psi})\n\\ln[I_p]$ $+ g_B (\\bar{\\psi}) \\ln [B_t ]$ $+ g_n (\\bar{\\psi}) \\ln [ \\bar{n} ]$.\nThese log-additive diffusivity models are useful when the diffusivity is varied\nsmoothly with the plasma parameters. A penalized nonlinear regression technique\nis recommended to estimate the $g_i (\\bar{\\psi})$. The physics implications of\nthe two classes of models, additive log-temperature models and additive\nlog-diffusivity models, are different. The additive log-diffusivity models\nadjust the temperature profile shape as the radial distribution of sinks and\nsources. In contrast, the additive log-temperature model predicts that the\ntemperature profile depends only on the global parameters and not on the radial\nheat deposition.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 04:00:16 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Imre", "Kaya", ""], ["Riedel", "Kurt S.", ""], ["Schunke", "Beatrix", ""]]}, {"id": "1803.09460", "submitter": "Giacomo Zanella", "authors": "Omiros Papaspiliopoulos, Gareth O. Roberts and Giacomo Zanella", "title": "Scalable inference for crossed random effects models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the complexity of Gibbs samplers for inference in crossed random\neffect models used in modern analysis of variance. We demonstrate that for\ncertain designs the plain vanilla Gibbs sampler is not scalable, in the sense\nthat its complexity is worse than proportional to the number of parameters and\ndata. We thus propose a simple modification leading to a collapsed Gibbs\nsampler that is provably scalable. Although our theory requires some\nbalancedness assumptions on the data designs, we demonstrate in simulated and\nreal datasets that the rates it predicts match remarkably the correct rates in\ncases where the assumptions are violated. We also show that the collapsed Gibbs\nsampler, extended to sample further unknown hyperparameters, outperforms\nsignificantly alternative state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:15:54 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth O.", ""], ["Zanella", "Giacomo", ""]]}, {"id": "1803.09691", "submitter": "Michael Grayling", "authors": "Michael Grayling, David Robertson, James Wason, Adrian Mander", "title": "Design optimisation and post-trial analysis in group sequential\n  stepped-wedge cluster randomised trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, methodology was presented to facilitate the incorporation of\ninterim analyses in stepped-wedge (SW) cluster randomised trials (CRTs). Here,\nwe extend this previous discussion. We detail how the stopping boundaries,\nallocation sequences, and per-cluster per-period sample size of a group\nsequential SW-CRT can be optimised. We then describe methods by which point\nestimates, p-values, and confidence intervals, which account for the sequential\nnature of the design, can be calculated. We demonstrate that optimal sequential\ndesigns can reduce the expected required number of measurements under the null\nhypothesis, compared to the classical design, by up to 30%, with no cost to the\nmaximal possible required number of measurements. Furthermore, the adjusted\nanalysis procedure almost universally reduces the average bias in the point\nestimate, and consistently provides a confidence interval with coverage close\nto the nominal level. In contrast, the coverage of a naive 95% confidence\ninterval is observed to range between 92 and 98%. Methodology is now readily\navailable for the efficient design and analysis of group sequential SW-CRTs. In\nscenarios in which there are substantial ethical or financial reasons to\nterminate a SW-CRT as soon as possible, trialists should strongly consider a\ngroup sequential approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 16:13:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Grayling", "Michael", ""], ["Robertson", "David", ""], ["Wason", "James", ""], ["Mander", "Adrian", ""]]}, {"id": "1803.09713", "submitter": "Ricardo Maronna", "authors": "Ricardo A. Maronna (University of La Plata and University of Buenos\n  Aires)", "title": "Robust principal components for irregularly spaced longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider longitudinal data $x_{ij},$ with $i=1,...,n$ and $j=1,...,p_{i},$\nwhere $x_{ij}$ is the $j-$th observation of the random function $X_{i}\\left(\n.\\right) $ observed at time $t_{j}.$ The goal of this paper is to develop a\nparsimonious representation of the data by a linear combination of a set of $q$\nsmooth functions $H_{k}\\left( .\\right) $ ($k=1,..,q)$ in the sense that\n$x_{ij}\\approx\\mu_{j}+\\sum_{k=1}^{q}\\beta_{ki}H_{k}\\left( t_{j}\\right) ,$ such\nthat it fulfills three goals: it is resistant to atypical $X_{i}$'s ('case\ncontamination'), it is resistant to isolated gross errors at some $t_{ij}$\n('cell contamination'), and it can be applied when some of the $x_{ij}$ are\nmissing ('irregularly spaced' ---or 'incomplete'-- data).\n  Two approaches will be proposed for this problem. One deals with the three\ngoals stated above, and is based on ideas similar to MM-estimation (Yohai\n1987). The other is a simple and fast estimator which can be applied to\ncomplete data with case- and cellwise contamination, and is based on applying a\nstandard robust principal components estimate and smoothing the principal\ndirections. Experiments with real and simulated data suggest that with complete\ndata the simple estimator outperforms its competitors, while the MM estimator\nis competitive for incomplete data.\n  Keywords: Principal components, MM-estimator, longitudinal .data, B-splines,\nincomplete data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:01:52 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Maronna", "Ricardo A.", "", "University of La Plata and University of Buenos\n  Aires"]]}, {"id": "1803.09735", "submitter": "Haim Bar", "authors": "Haim Bar, James Booth, Martin T. Wells", "title": "A Scalable Empirical Bayes Approach to Variable Selection in Generalized\n  Linear Models", "comments": "arXiv admin note: text overlap with arXiv:1510.03781", "journal-ref": "Journal of Computational and Graphical Statistics, 29:3, (2020)\n  535-546", "doi": "10.1080/10618600.2019.1706542", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new empirical Bayes approach to variable selection in the context of\ngeneralized linear models is developed. The proposed algorithm scales to\nsituations in which the number of putative explanatory variables is very large,\npossibly much larger than the number of responses. The coefficients in the\nlinear predictor are modeled as a three-component mixture allowing the\nexplanatory variables to have a random positive effect on the response, a\nrandom negative effect, or no effect. A key assumption is that only a small\n(but unknown) fraction of the candidate variables have a non-zero effect. This\nassumption, in addition to treating the coefficients as random effects\nfacilitates an approach that is computationally efficient. In particular, the\nnumber of parameters that have to be estimated is small, and remains constant\nregardless of the number of explanatory variables. The model parameters are\nestimated using a Generalized Alternating Maximization algorithm which is\nscalable, and leads to significantly faster convergence compared with\nsimulation-based fully Bayesian methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 17:46:39 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 22:06:45 GMT"}, {"version": "v3", "created": "Mon, 21 Oct 2019 19:05:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim", ""], ["Booth", "James", ""], ["Wells", "Martin T.", ""]]}, {"id": "1803.09830", "submitter": "Lior Rennert", "authors": "Lior Rennert and Sharon X. Xie", "title": "Cox Regression Model Under Dependent Truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncation is a statistical phenomenon that occurs in many time to event\nstudies. For example, autopsy-confirmed studies of neurodegenerative diseases\nare subject to an inherent left and right truncation, also known as double\ntruncation. When the goal is to study the effect of risk factors on survival,\nthe standard Cox regression model cannot be used when the data is subject to\ntruncation. Existing methods which adjust for both left and right truncation in\nthe Cox regression model require independence between the survival times and\ntruncation times, which may not be a reasonable assumption in practice. We\npropose an expectation-maximization algorithm to relax the independence\nassumption in the Cox regression model under left, right, or double truncation,\nto an assumption of conditional independence. The resulting regression\ncoefficient estimators are consistent and asymptotically normal. We demonstrate\nthrough extensive simulations that the proposed estimators have little bias\nand, in most practical situations, have a lower mean-squared error compared to\nexisting estimators. We implement our approach to assess the effect of\noccupation on survival in subjects with autopsy-confirmed Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 20:32:22 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Rennert", "Lior", ""], ["Xie", "Sharon X.", ""]]}, {"id": "1803.09927", "submitter": "Takashi Takahashi", "authors": "Takashi Takahashi, Yoshiyuki Kabashima", "title": "A statistical mechanics approach to de-biasing and uncertainty\n  estimation in LASSO for random measurements", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": "10.1088/1742-5468/aace2e", "report-no": null, "categories": "stat.ME cond-mat.stat-mech physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional statistical inference in which the number of parameters\nto be estimated is larger than that of the holding data, regularized linear\nestimation techniques are widely used. These techniques have, however, some\ndrawbacks. First, estimators are biased in the sense that their absolute values\nare shrunk toward zero because of the regularization effect. Second, their\nstatistical properties are difficult to characterize as they are given as\nnumerical solutions to certain optimization problems. In this manuscript, we\ntackle such problems concerning LASSO, which is a widely used method for sparse\nlinear estimation, when the measurement matrix is regarded as a sample from a\nrotationally invariant ensemble. We develop a new computationally feasible\nscheme to construct a de-biased estimator with a confidence interval and\nconduct hypothesis testing for the null hypothesis that a certain parameter\nvanishes. It is numerically confirmed that the proposed method successfully\nde-biases the LASSO estimator and constructs confidence intervals and p-values\nby experiments for noisy linear measurements.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 07:07:31 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Takahashi", "Takashi", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1803.09971", "submitter": "Ting Yan", "authors": "Ting Yan", "title": "A Probit Network Model with Arbitrary Dependence", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adopt a latent variable method to formulate a network model\nwith arbitrarily dependent structure. We assume that the latent variables\nfollow a multivariate normal distribution and a link between two nodes forms if\nthe sum of the corresponding node parameters exceeds the latent variable. The\ndependent structure among edges is induced by the covariance matrix of the\nlatent variables. The marginal distribution of an edge is a probit function. We\nrefer this model to as the \\emph{Probit Network Model}. We show that the moment\nestimator of the node parameter is consistent. To the best of our knowledge,\nthis is the first time to derive consistency result in a single observed\nnetwork with globally dependent structures. We extend the model to allow node\ncovariate information.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 09:09:45 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yan", "Ting", ""]]}, {"id": "1803.10031", "submitter": "Umberto Simola Mr.", "authors": "Umberto Simola and Jessi Cisewski-Kehe and Robert L. Wolpert", "title": "Approximate Bayesian Computation for Finite Mixture Models", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models are used in statistics and other disciplines, but\ninference for mixture models is challenging due, in part, to the multimodality\nof the likelihood function and the so-called label switching problem. We\npropose extensions of the Approximate Bayesian Computation-Population Monte\nCarlo (ABC-PMC) algorithm as an alternative framework for inference on finite\nmixture models. There are several decisions to make when implementing an\nABC-PMC algorithm for finite mixture models, including the selection of the\nkernels used for moving the particles through the iterations, how to address\nthe label switching problem, and the choice of informative summary statistics.\nExamples are presented to demonstrate the performance of the proposed ABC-PMC\nalgorithm for mixture modeling. The performance of the proposed method is\nevaluated in a simulation study and for the popular recessional velocity galaxy\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:06:51 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 11:12:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Simola", "Umberto", ""], ["Cisewski-Kehe", "Jessi", ""], ["Wolpert", "Robert L.", ""]]}, {"id": "1803.10043", "submitter": "C\\'ecile Proust-Lima", "authors": "C\\'ecile Proust-Lima, Viviane Philipps, Jean-Fran\\c{c}ois Dartigues", "title": "A joint model for multiple dynamic processes and clinical endpoints:\n  application to Alzheimer's disease", "comments": null, "journal-ref": "Stat Med. 2019 Oct 15;38(23):4702-4717", "doi": "10.1002/sim.8328", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As other neurodegenerative diseases, Alzheimer's disease, the most frequent\ndementia in the elderly, is characterized by multiple progressive impairments\nin the brain structure and in clinical functions such as cognitive functioning\nand functional disability. Until recently, these components were mostly studied\nindependently since no joint model for multivariate longitudinal data and time\nto event was available in the statistical community. Yet, these components are\nfundamentally inter-related in the degradation process towards dementia and\nshould be analyzed together. We thus propose a joint model to simultaneously\ndescribe the dynamics of multiple correlated components. Each component,\ndefined as a latent process, is measured by one or several continuous markers\n(not necessarily Gaussian). Rather than considering the associated time to\ndiagnosis as in standard joint models, we assume diagnosis corresponds to the\npassing above a covariate-specific threshold (to be estimated) of a\npathological process which is modelled as a combination of the\ncomponent-specific latent processes. This definition captures the clinical\ncomplexity of diagnoses such as dementia diagnosis but also benefits from\nsimplifications for the computation of Maximum Likelihood Estimates. We show\nthat the model and estimation procedure can also handle competing clinical\nendpoints. The estimation procedure, implemented in a R package, is validated\nby simulations and the method is illustrated on a large French population-based\ncohort of cerebral aging in which we focused on the dynamics of three clinical\nmanifestations and the associated risk of dementia and death before dementia.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:37:28 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 11:04:54 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Proust-Lima", "C\u00e9cile", ""], ["Philipps", "Viviane", ""], ["Dartigues", "Jean-Fran\u00e7ois", ""]]}, {"id": "1803.10052", "submitter": "Leonhard Held", "authors": "Leonhard Held", "title": "The Assessment of Intrinsic Credibility and a New Argument for p<0.005", "comments": "arXiv admin note: text overlap with arXiv:1712.03032", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of intrinsic credibility has been recently introduced to check\nthe credibility of \"out of the blue\" findings without any prior support. A\nsignificant result is deemed intrinsically credible if it is in conflict with a\nsceptical prior derived from the very same data that would make the effect\nnon-significant. In this paper I propose to use Bayesian prior-predictive tail\nprobabilities to assess intrinsic credibility. For the standard 5% significance\nlevel, this leads to a new p-value threshold that is remarkably close to the\nrecently proposed p<0.005 standard. I also introduce the credibility ratio, the\nratio of the upper to the lower limit of a standard confidence interval for the\ncorresponding effect size. I show that the credibility ratio has to be smaller\nthan 5.8 such that a significant finding is also intrinsically credible.\nFinally, a p-value for intrinsic credibility is proposed that is a simple\nfunction of the ordinary p-value and has a direct frequentist interpretation in\nterms of the probability of replicating an effect.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:56:39 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 13:53:56 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Held", "Leonhard", ""]]}, {"id": "1803.10119", "submitter": "Alexandre B\\^one", "authors": "Alexandre B\\^one, Olivier Colliot, Stanley Durrleman", "title": "Learning distributions of shape trajectories from longitudinal datasets:\n  a hierarchical model on a manifold of diffeomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn a distribution of shape trajectories from\nlongitudinal data, i.e. the collection of individual objects repeatedly\nobserved at multiple time-points. The method allows to compute an average\nspatiotemporal trajectory of shape changes at the group level, and the\nindividual variations of this trajectory both in terms of geometry and time\ndynamics. First, we formulate a non-linear mixed-effects statistical model as\nthe combination of a generic statistical model for manifold-valued longitudinal\ndata, a deformation model defining shape trajectories via the action of a\nfinite-dimensional set of diffeomorphisms with a manifold structure, and an\nefficient numerical scheme to compute parallel transport on this manifold.\nSecond, we introduce a MCMC-SAEM algorithm with a specific approach to shape\nsampling, an adaptive scheme for proposal variances, and a log-likelihood\ntempering strategy to estimate our model. Third, we validate our algorithm on\n2D simulated data, and then estimate a scenario of alteration of the shape of\nthe hippocampus 3D brain structure during the course of Alzheimer's disease.\nThe method shows for instance that hippocampal atrophy progresses more quickly\nin female subjects, and occurs earlier in APOE4 mutation carriers. We finally\nillustrate the potential of our method for classifying pathological\ntrajectories versus normal ageing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:03:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 14:35:22 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["B\u00f4ne", "Alexandre", ""], ["Colliot", "Olivier", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1803.10121", "submitter": "Jessie Hendricks", "authors": "J. H. Hendricks, C. Neumann, C. P. Saunders", "title": "Quantification of the weight of fingerprint evidence using a ROC-based\n  Approximate Bayesian Computation algorithm for model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than a century, fingerprints have been used with considerable\nsuccess to identify criminals or verify the identity of individuals. The\ncategorical conclusion scheme used by fingerprint examiners, and more generally\nthe inference process followed by forensic scientists, have been heavily\ncriticised in the scientific and legal literature. Instead, scholars have\nproposed to characterise the weight of forensic evidence using the Bayes factor\nas the key element of the inference process. In forensic science, quantifying\nthe magnitude of support is equally as important as determining which model is\nsupported. Unfortunately, the complexity of fingerprint patterns render\nlikelihood-based inference impossible. In this paper, we use an Approximate\nBayesian Computation model selection algorithm to quantify the weight of\nfingerprint evidence. We supplement the ABC algorithm using a Receiver\nOperating Characteristic curve to mitigate the effect of the curse of\ndimensionality. Our modified algorithm is computationally efficient and makes\nit easier to monitor convergence as the number of simulations increase. We use\nour method to quantify the weight of fingerprint evidence in forensic science,\nbut we note that it can be applied to any other forensic pattern evidence.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:04:08 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 20:38:05 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 18:11:27 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hendricks", "J. H.", ""], ["Neumann", "C.", ""], ["Saunders", "C. P.", ""]]}, {"id": "1803.10130", "submitter": "Michael Grayling", "authors": "Michael Grayling, Adrian Mander, James Wason", "title": "Blinded and unblinded sample size re-estimation in crossover trials\n  balanced for period", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of the sample size required by a crossover trial typically\ndepends on the specification of one or more variance components. Uncertainty\nabout the value of these parameters at the design stage means that there is\noften a risk a trial may be under- or over-powered. For many study designs,\nthis problem has been addressed by considering adaptive design methodology that\nallows for the re-estimation of the required sample size during a trial. Here,\nwe propose and compare several approaches for this in multi-treatment crossover\ntrials. Specifically, regulators favour re-estimation procedures to maintain\nthe blinding of the treatment allocations. We therefore develop blinded\nestimators for the within and between person variances, following simple or\nblock randomisation. We demonstrate that, provided an equal number of patients\nare allocated to sequences that are balanced for period, the proposed\nestimators following block randomisation are unbiased. We further provide a\nformula for the bias of the estimators following simple randomisation. The\nperformance of these procedures, along with that of an unblinded approach, is\nthen examined utilising three motivating examples, including one based on a\nrecently completed four-treatment four-period crossover trial. Simulation\nresults show that the performance of the proposed blinded procedures is in many\ncases similar to that of the unblinded approach, and thus they are an\nattractive alternative.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 15:22:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Grayling", "Michael", ""], ["Mander", "Adrian", ""], ["Wason", "James", ""]]}, {"id": "1803.10405", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "A Sherman-Morrison-Woodbury Identity for Rank Augmenting Matrices with\n  Application to Centering", "comments": "Better in Mathematics, Spectral Theory, General, or Numerical\n  Analysis", "journal-ref": "SIAM Journal on Numerical Analysis Vol. 31, No. 4 (Aug., 1994),\n  pp. 1219-1225", "doi": null, "report-no": null, "categories": "stat.ME cs.NA cs.SY eess.SY math.FA math.NA math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices of the form $\\bf{A} + (\\bf{V}_1 + \\bf{W}_1)\\bf{G}(\\bf{V}_2 +\n\\bf{W}_2)^*$ are considered where $\\bf{A}$ is a $singular$ $\\ell \\times \\ell$\nmatrix and $\\bf{G}$ is a nonsingular $k \\times k$ matrix, $k \\le \\ell$. Let the\ncolumns of $\\bf{V}_1$ be in the column space of $\\bf{A}$ and the columns of\n$\\bf{W}_1$ be orthogonal to $\\bf{A}$. Similarly, let the columns of $\\bf{V}_2$\nbe in the column space of $\\bf{A}^*$ and the columns of $\\bf{W}_2$ be\northogonal to $\\bf{A}^*$. An explicit expression for the inverse is given,\nprovided that $\\bf{W}_i^* \\bf{W}_i$ has rank $k$. %and $\\bf{W}_1$ and\n$\\bf{W}_2$ have the same column space. An application to centering covariance\nmatrices about the mean is given.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 04:05:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.10429", "submitter": "Annamaria Guolo Dr.", "authors": "Annamaria Guolo", "title": "Improving likelihood-based inference in control rate regression", "comments": null, "journal-ref": "Statistics in Medicine 37 (2018) 157-166", "doi": "10.1002/sim.7511", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control rate regression is a diffuse approach to account for heterogeneity\namong studies in meta-analysis by including information about the outcome risk\nof patients in the control condition. Correcting for the presence of\nmeasurement error affecting risk information in the treated and in the control\ngroup has been recognized as a necessary step to derive reliable inferential\nconclusions. Within this framework, the paper considers the problem of small\nsample size as an additional source of misleading inference about the slope of\nthe control rate regression. Likelihood procedures relying on first-order\napproximations are shown to be substantially inaccurate, especially when\ndealing with increasing heterogeneity and correlated measurement errors. We\nsuggest to address the problem by relying on higher-order asymptotics. In\nparticular, we derive Skovgaard's statistic as an instrument to improve the\naccuracy of the approximation of the signed profile log-likelihood ratio\nstatistic to the standard normal distribution. The proposal is shown to provide\nmuch more accurate results than standard likelihood solutions, with no\nappreciable computational effort. The advantages of Skovgaard's statistic in\ncontrol rate regression are shown in a series of simulation experiments and\nillustrated in a real data example. R code for applying first- and second-order\nstatistic for inference on the slope on the control rate regression is\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 06:57:35 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Guolo", "Annamaria", ""]]}, {"id": "1803.10535", "submitter": "Vahe Asvatourian Msc", "authors": "Vah\\'e Asvatourian, Cl\\'elia Coutzac, Nathalie Chaput, Caroline\n  Robert, Stefan Michiels, Emilie Lanoy", "title": "Estimating causal effects of time-dependent exposures on a binary\n  endpoint in a high-dimensional setting", "comments": "16 pages + 20 pages of appendices, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the intervention calculus when the DAG is absent (IDA) method was\ndeveloped to estimate lower bounds of causal effects from observational\nhigh-dimensional data. Originally it was introduced to assess the effect of\nbaseline biomarkers which do not vary over time. However, in many clinical\nsettings, measurements of biomarkers are repeated at fixed time points during\ntreatment exposure and, therefore, this method need to be extended. The purpose\nof this paper is then to extend the first step of the IDA, the Peter Clarks\n(PC)-algorithm, to a time-dependent exposure in the context of a binary\noutcome. We generalised the PC-algorithm for taking into account the\nchronological order of repeated measurements of the exposure and propose to\napply the IDA with our new version, the chronologically ordered PC-algorithm\n(COPC-algorithm). A simulation study has been performed before applying the\nmethod for estimating causal effects of time-dependent immunological biomarkers\non toxicity, death and progression in patients with metastatic melanoma. The\nsimulation study showed that the completed partially directed acyclic graphs\n(CPDAGs) obtained using COPC-algorithm were structurally closer to the true\nCPDAG than CPDAGs obtained using PC-algorithm. Also, causal effects were more\naccurate when they were estimated based on CPDAGs obtained using\nCOPC-algorithm. Moreover, CPDAGs obtained by COPC-algorithm allowed removing\nnon-chronologic arrows with a variable measured at a time t pointing to a\nvariable measured at a time t' where t'< t. Bidirected edges were less present\nin CPDAGs obtained with the COPC-algorithm, supporting the fact that there was\nless variability in causal effects estimated from these CPDAGs. The\nCOPC-algorithm provided CPDAGs that keep the chronological structure present in\nthe data, thus allowed to estimate lower bounds of the causal effect of\ntime-dependent biomarkers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 11:34:41 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 09:52:04 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Asvatourian", "Vah\u00e9", ""], ["Coutzac", "Cl\u00e9lia", ""], ["Chaput", "Nathalie", ""], ["Robert", "Caroline", ""], ["Michiels", "Stefan", ""], ["Lanoy", "Emilie", ""]]}, {"id": "1803.10568", "submitter": "Mathias Lindholm", "authors": "Andreas Lager{\\aa}s and Mathias Lindholm", "title": "How to ask sensitive multiple choice questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent failures of polling to estimate populist party support,\nwe propose and analyse two methods for asking sensitive multiple choice\nquestions where the respondent retains some privacy and therefore might answer\nmore truthfully. The first method consists of asking for the true choice along\nwith a choice picked at random. The other method presents a list of choices and\nasks whether the preferred one is on the list or not. Different respondents are\nshown different lists. The methods are easy to explain, which makes it likely\nthat the respondent understands how her privacy is protected and may thus\nentice her to participate in the survey and answer truthfully. The methods are\nalso easy to implement and scale up.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:54:22 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Lager\u00e5s", "Andreas", ""], ["Lindholm", "Mathias", ""]]}, {"id": "1803.10655", "submitter": "Sharmistha Guha", "authors": "Sharmistha Guha and Abel Rodriguez", "title": "Bayesian Regression with Undirected Network Predictors with an\n  Application to Brain Connectome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a Bayesian approach to regression with a continuous\nscalar response and an undirected network predictor. Undirected network\npredictors are often expressed in terms of symmetric adjacency matrices, with\nrows and columns of the matrix representing the nodes, and zero entries\nsignifying no association between two corresponding nodes. Network predictor\nmatrices are typically vectorized prior to any analysis, thus failing to\naccount for the important structural information in the network. This results\nin poor inferential and predictive performance in presence of small sample\nsizes. We propose a novel class of network shrinkage priors for the coefficient\ncorresponding to the undirected network predictor. The proposed framework is\ndevised to detect both nodes and edges in the network predictive of the\nresponse. Our framework is implemented using an efficient Markov Chain Monte\nCarlo algorithm. Empirical results in simulation studies illustrate strikingly\nsuperior inferential and predictive gains of the proposed framework in\ncomparison with the ordinary high dimensional Bayesian shrinkage priors and\npenalized optimization schemes. We apply our method to a brain connectome\ndataset that contains information on brain networks along with a measure of\ncreativity for multiple individuals. Here, interest lies in building a\nregression model of the creativity measure on the network predictor to identify\nimportant regions and connections in the brain strongly associated with\ncreativity. To the best of our knowledge, our approach is the first principled\nBayesian method that is able to detect scientifically interpretable regions and\nconnections in the brain actively impacting the continuous response\n(creativity) in the presence of a small sample size.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 14:40:55 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Guha", "Sharmistha", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1803.10766", "submitter": "Chen Wang", "authors": "Benjamin Kedem, Lemeng Pan, Paul Smith, and Chen Wang", "title": "Repeated out of Sample Fusion in the Estimation of Small Tail\n  Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often, it is required to estimate the probability that a quantity such as\ntoxicity level, plutonium, temperature, rainfall, damage, wind speed, wave\nsize, earthquake magnitude, risk, etc., exceeds an unsafe high threshold. The\nprobability in question is then very small. To estimate such a probability,\ninformation is needed about large values of the quantity of interest. However,\nin many cases, the data only contain values below or even far below the\ndesignated threshold, let alone exceedingly large values. It is shown that by\nrepeated fusion of the data with externally generated random data, more\ninformation about small tail probabilities is obtained with the aid of certain\nnew statistical functions. This provides relatively short, yet reliable\ninterval estimates based on moderately large samples. A comparison of the\napproach with a method from extreme values theory (Peaks over Threshold, or\nPOT), using both artificial and real data, points to the merit of repeated out\nof sample fusion.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 06:22:46 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 02:44:59 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 21:47:02 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Kedem", "Benjamin", ""], ["Pan", "Lemeng", ""], ["Smith", "Paul", ""], ["Wang", "Chen", ""]]}, {"id": "1803.10884", "submitter": "Adam Gustafson", "authors": "Adam Gustafson, Matthew Hirn, Kitty Mohammed, Hariharan Narayanan, and\n  Jason Xu", "title": "Structural Risk Minimization for $C^{1,1}(\\mathbb{R}^d)$ Regression", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One means of fitting functions to high-dimensional data is by providing\nsmoothness constraints. Recently, the following smooth function approximation\nproblem was proposed: given a finite set $E \\subset \\mathbb{R}^d$ and a\nfunction $f: E \\rightarrow \\mathbb{R}$, interpolate the given information with\na function $\\widehat{f} \\in \\dot{C}^{1, 1}(\\mathbb{R}^d)$ (the class of\nfirst-order differentiable functions with Lipschitz gradients) such that\n$\\widehat{f}(a) = f(a)$ for all $a \\in E$, and the value of\n$\\mathrm{Lip}(\\nabla \\widehat{f})$ is minimal. An algorithm is provided that\nconstructs such an approximating function $\\widehat{f}$ and estimates the\noptimal Lipschitz constant $\\mathrm{Lip}(\\nabla \\widehat{f})$ in the noiseless\nsetting.\n  We address statistical aspects of reconstructing the approximating function\n$\\widehat{f}$ from a closely-related class $C^{1, 1}(\\mathbb{R}^d)$ given\nsamples from noisy data. We observe independent and identically distributed\nsamples $y(a) = f(a) + \\xi(a)$ for $a \\in E$, where $\\xi(a)$ is a noise term\nand the set $E \\subset \\mathbb{R}^d$ is fixed and known. We obtain uniform\nbounds relating the empirical risk and true risk over the class\n$\\mathcal{F}_{\\widetilde{M}} = \\{f \\in C^{1, 1}(\\mathbb{R}^d) \\mid\n\\mathrm{Lip}(\\nabla f) \\leq \\widetilde{M}\\}$, where the quantity\n$\\widetilde{M}$ grows with the number of samples at a rate governed by the\nmetric entropy of the class $C^{1, 1}(\\mathbb{R}^d)$. Finally, we provide an\nimplementation using Vaidya's algorithm, supporting our results via numerical\nexperiments on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 00:19:45 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:49:35 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Gustafson", "Adam", ""], ["Hirn", "Matthew", ""], ["Mohammed", "Kitty", ""], ["Narayanan", "Hariharan", ""], ["Xu", "Jason", ""]]}, {"id": "1803.10901", "submitter": "Bikram Karmakar", "authors": "Bikram Karmakar and Indranil Mukhopadhyay", "title": "Statistical Validity and Consistency of Big Data Analytics: A General\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Informatics and technological advancements have triggered generation of huge\nvolume of data with varied complexity in its management and analysis. Big Data\nanalytics is the practice of revealing hidden aspects of such data and making\ninferences from it. Although storage, retrieval and management of Big Data seem\npossible through efficient algorithm and system development, concern about\nstatistical consistency remains to be addressed in view of its specific\ncharacteristics. Since Big Data does not conform to standard analytics, we need\nproper modification of the existing statistical theory and tools. Here we\npropose, with illustrations, a general statistical framework and an algorithmic\nprinciple for Big Data analytics that ensure statistical accuracy of the\nconclusions. The proposed framework has the potential to push forward\nadvancement of Big Data analytics in the right direction. The\npartition-repetition approach proposed here is broad enough to encompass all\npractical data analytic problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 02:15:03 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Karmakar", "Bikram", ""], ["Mukhopadhyay", "Indranil", ""]]}, {"id": "1803.10978", "submitter": "Zhanlin Liu", "authors": "Zhanlin Liu and Youngjun Choe", "title": "Data-Driven Sensitivity Indices for Models With Dependent Inputs Using\n  the Polynomial Chaos Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainties exist in both physics-based and data-driven models.\nVariance-based sensitivity analysis characterizes how the variance of a model\noutput is propagated from the model inputs. The Sobol index is one of the most\nwidely used sensitivity indices for models with independent inputs. For models\nwith dependent inputs, different approaches have been explored to obtain\nsensitivity indices in the literature. Typical approaches are based on\nprocedures of transforming the dependent inputs into independent inputs.\nHowever, such transformation requires additional information about the inputs,\nsuch as the dependency structure or the conditional probability density\nfunctions. In this paper, data-driven sensitivity indices are proposed for\nmodels with dependent inputs. We first construct ordered partitions of linearly\nindependent polynomials of the inputs. The modified Gram-Schmidt algorithm is\nthen applied to the ordered partitions to generate orthogonal polynomials with\nrespect to the empirical measure based on observed data of model inputs and\noutputs. Using the polynomial chaos expansion with the orthogonal polynomials,\nwe obtain the proposed data-driven sensitivity indices. The sensitivity indices\nprovide intuitive interpretations of how the dependent inputs affect the\nvariance of the output without a priori knowledge on the dependence structure\nof the inputs. Three numerical examples are used to validate the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 09:26:02 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 19:00:36 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 08:17:23 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Zhanlin", ""], ["Choe", "Youngjun", ""]]}, {"id": "1803.11033", "submitter": "Chang-Yun Lin Dr.", "authors": "Chang-Yun Lin", "title": "Generalized Bayesian D criterion for single-stratum and multistratum\n  designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DuMouchel and Jones (1994) proposed the Bayesian D criterion by modifying the\nD-optimality approach to reduce dependence of the selected design on an assumed\nmodel. This criterion has been applied to select various single-stratum designs\nfor completely randomized experiments when the number of effects is greater\nthan the sample size. In many industrial experiments, complete randomization is\nsometimes expensive or infeasible and, hence, designs used for the experiments\noften have multistratum structures. However, the original Bayesian D criterion\nwas developed under the framework of single-stratum structures and cannot be\napplied to select multistratum designs. In this paper, we study how to extend\nthe Bayesian approach for more complicated experiments and develop the\ngeneralized Bayesian D criterion, which generalizes the original Bayesian D\ncriterion and can be applied to select single-stratum and multistratum designs\nfor various experiments when the number of effects is greater than the rank of\nthe model matrix.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 12:44:41 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Lin", "Chang-Yun", ""]]}, {"id": "1803.11202", "submitter": "Youssef Taleb", "authors": "Youssef Taleb, Edward A. K. Cohen", "title": "Multiresolution analysis of point processes and statistical thresholding\n  for wavelet-based intensity estimation", "comments": "48 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a wavelet based approach to the analysis of point processes and the\nestimation of the first order intensity under a continuous time setting. A\nmultiresolution analysis of a point process is formulated which motivates the\ndefinition of homogeneity at different scales of resolution, termed $J$-th\nlevel homogeneity. Further to this, the activity in a point processes' first\norder behavior at different scales of resolution is also defined and termed\n$L$-th level innovation. Likelihood ratio tests for both these properties are\nproposed with asymptotic distributions provided, even when only a single\nrealization of the point process is observed. The test for $L$-th level\ninnovation forms the basis for a collection of statistical strategies for\nthresholding coefficients in a wavelet based estimator of the intensity\nfunction. These thresholding strategies are shown to outperform the existing\nlocal hard thresholding strategy on a range of simulation scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 18:05:00 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Taleb", "Youssef", ""], ["Cohen", "Edward A. K.", ""]]}, {"id": "1803.11251", "submitter": "Guanyang Wang", "authors": "Persi Diaconis, Guanyang Wang", "title": "Bayesian Goodness of Fit Tests: A Conversation for David Mumford", "comments": "22 pages, 5 figures; added references", "journal-ref": null, "doi": "10.4310/AMSA.2018.v3.n1.a9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of making practical, useful goodness of fit tests in the Bayesian\nparadigm is largely open. We introduce a class of special cases (testing for\nuniformity: have the cards been shuffled enough; does my random generator work)\nand a class of sensible Bayes tests inspired by Mumford, Wu and Zhu.\nCalculating these tests presents the challenge of 'doubly intractable\ndistributions'. In present circumstances, modern MCMC techniques are up to the\nchallenge. But many other problems remain. Our paper is didactic, we hope to\ninduce the reader to help take it further.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 20:56:34 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 21:07:40 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Diaconis", "Persi", ""], ["Wang", "Guanyang", ""]]}, {"id": "1803.11266", "submitter": "Patrick Schratz", "authors": "Patrick Schratz, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter,\n  Alexander Brenning", "title": "Performance evaluation and hyperparameter tuning of statistical and\n  machine-learning models using spatial data", "comments": null, "journal-ref": "Ecological Modelling Volume 406, 24 August 2019, Pages 109-120", "doi": "10.1016/j.ecolmodel.2019.06.002", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine-learning algorithms have gained popularity in recent years in the\nfield of ecological modeling due to their promising results in predictive\nperformance of classification problems. While the application of such\nalgorithms has been highly simplified in the last years due to their\nwell-documented integration in commonly used statistical programming languages\nsuch as R, there are several practical challenges in the field of ecological\nmodeling related to unbiased performance estimation, optimization of algorithms\nusing hyperparameter tuning and spatial autocorrelation. We address these\nissues in the comparison of several widely used machine-learning algorithms\nsuch as Boosted Regression Trees (BRT), k-Nearest Neighbor (WKNN), Random\nForest (RF) and Support Vector Machine (SVM) to traditional parametric\nalgorithms such as logistic regression (GLM) and semi-parametric ones like\ngeneralized additive models (GAM). Different nested cross-validation methods\nincluding hyperparameter tuning methods are used to evaluate model performances\nwith the aim to receive bias-reduced performance estimates. As a case study the\nspatial distribution of forest disease Diplodia sapinea in the Basque Country\nin Spain is investigated using common environmental variables such as\ntemperature, precipitation, soil or lithology as predictors. Results show that\nGAM and RF (mean AUROC estimates 0.708 and 0.699) outperform all other methods\nin predictive accuracy. The effect of hyperparameter tuning saturates at around\n50 iterations for this data set. The AUROC differences between the bias-reduced\n(spatial cross-validation) and overoptimistic (non-spatial cross-validation)\nperformance estimates of the GAM and RF are 0.167 (24%) and 0.213 (30%),\nrespectively. It is recommended to also use spatial partitioning for\ncross-validation hyperparameter tuning of spatial data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 21:48:11 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Schratz", "Patrick", ""], ["Muenchow", "Jannes", ""], ["Iturritxa", "Eugenia", ""], ["Richter", "Jakob", ""], ["Brenning", "Alexander", ""]]}, {"id": "1803.11273", "submitter": "Yu-hsuan Wang", "authors": "Y. Samuel Wang, Mathias Drton", "title": "High-Dimensional Causal Discovery Under non-Gaussianity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider graphical models based on a recursive system of linear structural\nequations. This implies that there is an ordering, $\\sigma$, of the variables\nsuch that each observed variable $Y_v$ is a linear function of a variable\nspecific error term and the other observed variables $Y_u$ with $\\sigma(u) <\n\\sigma (v)$. The causal relationships, i.e., which other variables the linear\nfunctions depend on, can be described using a directed graph. It has been\npreviously shown that when the variable specific error terms are non-Gaussian,\nthe exact causal graph, as opposed to a Markov equivalence class, can be\nconsistently estimated from observational data. We propose an algorithm that\nyields consistent estimates of the graph also in high-dimensional settings in\nwhich the number of variables may grow at a faster rate than the number of\nobservations, but in which the underlying causal structure features suitable\nsparsity; specifically, the maximum in-degree of the graph is controlled. Our\ntheoretical analysis is couched in the setting of log-concave error\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 22:28:10 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 01:38:39 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 19:35:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Drton", "Mathias", ""]]}, {"id": "1803.11331", "submitter": "Rajarshi Guhaniyogi", "authors": "Rajarshi Guhaniyogi and Bruno Sanso", "title": "Large Multi-scale Spatial Kriging Using Tree Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multiscale spatial kernel convolution technique with higher\norder functions to capture fine scale local features and lower order terms to\ncapture large scale features. To achieve parsimony, the coefficients in the\nmultiscale kernel convolution model is assigned a new class of \"Tree shrinkage\nprior\" distributions. Tree shrinkage priors exert increasing shrinkage on the\ncoefficients as resolution grows so as to adapt to the necessary degree of\nresolution at any sub-domain. Our proposed model has a number of significant\nfeatures over the existing multi-scale spatial models for big data. In contrast\nto the existing multiscale approaches, the proposed approach auto-tunes the\ndegree of resolution necessary to model a subregion in the domain, achieves\nscalability by suitable parallelization of local updating of parameters and is\nbuttressed by theoretical support. Excellent empirical performances are\nillustrated using several simulation experiments and a geostatistical analysis\nof the sea surface temperature data from the pacific ocean.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 04:02:43 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Sanso", "Bruno", ""]]}, {"id": "1803.11354", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis and R. M. Huggins", "title": "Two-stage approaches to the analysis of occupancy data II. The\n  heterogeneous model and conditional likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models involve both the probability a site is occupied and the\nprobability occupancy is detected. The homogeneous occupancy model, where the\noccupancy and detection probabilities are the same at each site, admits an\northogonal parameter transformation that yields a two-stage process to\ncalculate the maximum likelihood estimates so that it is not necessary to\nsimultaneously estimate the occupancy and detection probabilities. The\ntwo-stage approach is examined here for the heterogeneous occupancy model where\nthe occupancy and detection probabilities now depend on covariates that may\nvary between sites and over time. There is no longer an orthogonal\ntransformation but this approach effectively reduces the parameter space and\nallows fuller use of the R functionality. This permits use of existing vector\ngeneralised linear models methods to fit models for detection and allows the\ndevelopment of an iterative weighted least squares approach to fit models for\noccupancy. Efficiency is examined in a simulation study and the full maximum\nlikelihood and two-stage approaches are compared on several data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:22:54 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 05:49:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Huggins", "R. M.", ""]]}, {"id": "1803.11459", "submitter": "Dexter Cahoy", "authors": "Dexter O. Cahoy and Wojbor A. Woyczy\\'nski", "title": "Log-moment estimators for the generalized Linnik and Mittag-Leffler\n  distributions with applications to financial modeling", "comments": null, "journal-ref": "Journal of Mathematics and Statistics 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose formal estimation procedures for the parameters of the\ngeneralized, three-parameter Linnik $gL(\\alpha,\\mu, \\delta)$ and Mittag-Leffler\n$gML(\\alpha,\\mu, \\delta)$ distributions. The estimators are derived from the\nmoments of the log-transformed random variables, and are shown to be\nasymptotically unbiased. The estimation algorithms are computationally\nefficient and the proposed procedures are tested using the daily S\\&P 500 and\nDow Jones index data. The results show that the standard two-parameter Linnik\nand Mittag-Leffler models are not flexible enough to accurately model the\ncurrent stock market data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 13:50:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 03:48:18 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 14:55:34 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Cahoy", "Dexter O.", ""], ["Woyczy\u0144ski", "Wojbor A.", ""]]}]