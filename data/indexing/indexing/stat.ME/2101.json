[{"id": "2101.00057", "submitter": "JInglai Li", "authors": "Junda Xiong, Xin Cai, Jinglai Li", "title": "Clustered active-subspace based local Gaussian Process emulator for\n  high-dimensional and complex computer models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying uncertainties in physical or engineering systems often requires a\nlarge number of simulations of the underlying computer models that are\ncomputationally intensive. Emulators or surrogate models are often used to\naccelerate the computation in such problems, and in this regard the Gaussian\nProcess (GP) emulator is a popular choice for its ability to quantify the\napproximation error in the emulator itself. However, a major limitation of the\nGP emulator is that it can not handle problems of very high dimensions, which\nis often addressed with dimension reduction techniques. In this work we hope to\naddress an issue that the models of interest are so complex that they admit\ndifferent low dimensional structures in different parameter regimes. Building\nupon the active subspace method for dimension reduction, we propose a clustered\nactive subspace method which identifies the local low-dimensional structures as\nwell as the parameter regimes they are in (represented as clusters), and then\nconstruct low dimensional and local GP emulators within the clusters.\nSpecifically we design a clustering method based on the gradient information to\nidentify these clusters, and a local GP construction procedure to construct the\nGP emulator within a local cluster. With numerical examples, we demonstrate\nthat the proposed method is effective when the underlying models are of complex\nlow-dimensional structures.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:24:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xiong", "Junda", ""], ["Cai", "Xin", ""], ["Li", "Jinglai", ""]]}, {"id": "2101.00059", "submitter": "Hong Zhang", "authors": "Hong Zhang, Qing Li, Devan V. Mehrotra and Judong Shen", "title": "CauchyCP: a powerful test under non-proportional hazards using Cauchy\n  combination of change-point Cox regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-proportional hazards data are routinely encountered in randomized\nclinical trials. In such cases, classic Cox proportional hazards model can\nsuffer from severe power loss, with difficulty in interpretation of the\nestimated hazard ratio since the treatment effect varies over time. We propose\nCauchyCP, an omnibus test of change-point Cox regression models, to overcome\nboth challenges while detecting signals of non-proportional hazards patterns.\nExtensive simulation studies demonstrate that, compared to existing treatment\ncomparison tests under non-proportional hazards, the proposed CauchyCP test 1)\ncontrols the type I error better at small $\\alpha$ levels ($< 0.01$); 2)\nincreases the power of detecting time-varying effects; and 3) is more\ncomputationally efficient. The superior performance of CauchyCP is further\nillustrated using retrospective analyses of two randomized clinical trial\ndatasets and a pharmacogenetic biomarker study dataset. The R package\n$\\textit{CauchyCP}$ is publicly available on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:26:06 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Hong", ""], ["Li", "Qing", ""], ["Mehrotra", "Devan V.", ""], ["Shen", "Judong", ""]]}, {"id": "2101.00074", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Daniel Sheldon, Tao Sun, John Pickering, and Thomas G.\n  Dietterich", "title": "Three-quarter Sibling Regression for Denoising Observational Data", "comments": null, "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ecological studies and conservation policies are based on field\nobservations of species, which can be affected by systematic variability\nintroduced by the observation process. A recently introduced causal modeling\ntechnique called 'half-sibling regression' can detect and correct for\nsystematic errors in measurements of multiple independent random variables.\nHowever, it will remove intrinsic variability if the variables are dependent,\nand therefore does not apply to many situations, including modeling of species\ncounts that are controlled by common causes. We present a technique called\n'three-quarter sibling regression' to partially overcome this limitation. It\ncan filter the effect of systematic noise when the latent variables have\nobserved common causes. We provide theoretical justification of this approach,\ndemonstrate its effectiveness on synthetic data, and show that it reduces\nsystematic detection variability due to moon brightness in moth surveys.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:18:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shankar", "Shiv", ""], ["Sheldon", "Daniel", ""], ["Sun", "Tao", ""], ["Pickering", "John", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "2101.00105", "submitter": "Pengfei Li", "authors": "Jing Qin, Yukun Liu, and Pengfei Li", "title": "A selective review on calibration information from similar studies based\n  on parametric likelihood or empirical likelihood", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-center clinical trials, due to various reasons, the individual-level\ndata are strictly restricted to be assessed publicly. Instead, the summarized\ninformation is widely available from published results. With the advance of\ncomputational technology, it has become very common in data analyses to run on\nhundreds or thousands of machines simultaneous, with the data distributed\nacross those machines and no longer available in a single central location. How\nto effectively assemble the summarized clinical data information or information\nfrom each machine in parallel computation has become a challenging task for\nstatisticians and computer scientists. In this paper, we selectively review\nsome recently-developed statistical methods, including communication efficient\ndistributed statistical inference, and renewal estimation and incremental\ninference, which can be regarded as the latest development of calibration\ninformation methods in the era of big data. Even though those methods were\ndeveloped in different fields and in different statistical frameworks, in\nprinciple, they are asymptotically equivalent to those well known methods\ndeveloped in meta analysis. Almost no or little information is lost compared\nwith the case when full data are available. As a general tool to integrate\ninformation, we also review the generalized method of moments and estimating\nequations approach by using empirical likelihood method.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 23:05:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Qin", "Jing", ""], ["Liu", "Yukun", ""], ["Li", "Pengfei", ""]]}, {"id": "2101.00362", "submitter": "Xi Yang", "authors": "Xi Yang, Jan Hannig, J.S. Marron", "title": "Visual High Dimensional Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory data analysis of known classes of high dimensional data, a\ncentral question is how distinct are the classes? The Direction Projection\nPermutation (DiProPerm) hypothesis test provides an answer to this that is\ndirectly connected to a visual analysis of the data. In this paper, we propose\nan improved DiProPerm test that solves 3 major challenges of the original\nversion. First, we implement only balanced permutations to increase the test\npower for data with strong signals. Second, our mathematical analysis leads to\nan adjustment to correct the null behavior of both balanced and the\nconventional all permutations. Third, new confidence intervals (reflecting\npermutation variation) for test significance are also proposed for comparison\nof results across different contexts. This improvement of DiProPerm inference\nis illustrated in the context of comparing cancer types in examples from The\nCancer Genome Atlas.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 03:34:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yang", "Xi", ""], ["Hannig", "Jan", ""], ["Marron", "J. S.", ""]]}, {"id": "2101.00455", "submitter": "Nicholas Clark", "authors": "Nicholas Clark, Matthew Dabkowski, Patrick Driscoll, Dereck Kennedy,\n  Ian Kloo and Heidy Shi", "title": "Empirical Decision Rules for Improving the Uncertainty Reporting of\n  Small Sample System Usability Scale Scores", "comments": null, "journal-ref": null, "doi": "10.1080/10447318.2020.1870831", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The System Usability Scale (SUS) is a short, survey-based approach used to\ndetermine the usability of a system from an end user perspective once a\nprototype is available for assessment. Individual scores are gathered using a\n10-question survey with the survey results reported in terms of central\ntendency (sample mean) as an estimate of the system's usability (the SUS study\nscore), and confidence intervals on the sample mean are used to communicate\nuncertainty levels associated with this point estimate. When the number of\nindividuals surveyed is large, the SUS study scores and accompanying confidence\nintervals relying upon the central limit theorem for support are appropriate.\nHowever, when only a small number of users are surveyed, reliance on the\ncentral limit theorem falls short, resulting in confidence intervals that\nsuffer from parameter bound violations and interval widths that confound\nmappings to adjective and other constructed scales. These shortcomings are\nespecially pronounced when the underlying SUS score data is skewed, as it is in\nmany instances. This paper introduces an empirically-based remedy for such\nsmall-sample circumstances, proposing a set of decision rules that leverage\neither an extended bias-corrected accelerated (BCa) bootstrap confidence\ninterval or an empirical Bayesian credibility interval about the sample mean to\nrestore and bolster subsequent confidence interval accuracy. Data from\nhistorical SUS assessments are used to highlight shortfalls in current\npractices and to demonstrate the improvements these alternate approaches offer\nwhile remaining statistically defensible. A freely available, online\napplication is introduced and discussed that automates SUS analysis under these\ndecision rules, thereby assisting usability practitioners in adopting the\nadvocated approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 14:31:18 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 16:15:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Clark", "Nicholas", ""], ["Dabkowski", "Matthew", ""], ["Driscoll", "Patrick", ""], ["Kennedy", "Dereck", ""], ["Kloo", "Ian", ""], ["Shi", "Heidy", ""]]}, {"id": "2101.00484", "submitter": "Fan Li", "authors": "Fan Li, Hengshi Yu, Paul J. Rathouz, Elizabeth L. Turner, John S.\n  Preisser", "title": "Marginal modeling of cluster-period means and intraclass correlations in\n  stepped wedge designs with binary outcomes", "comments": "28 pages, 2 figures, 3 tables", "journal-ref": "Biostatistics (2021)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stepped wedge cluster randomized trials (SW-CRTs) with binary outcomes are\nincreasingly used in prevention and implementation studies. Marginal models\nrepresent a flexible tool for analyzing SW-CRTs with population-averaged\ninterpretations, but the joint estimation of the mean and intraclass\ncorrelation coefficients (ICCs) can be computationally intensive due to large\ncluster-period sizes. Motivated by the need for marginal inference in SW-CRTs,\nwe propose a simple and efficient estimating equations approach to analyze\ncluster-period means. We show that the quasi-score for the marginal mean\ndefined from individual-level observations can be reformulated as the\nquasi-score for the same marginal mean defined from the cluster-period means.\nAn additional mapping of the individual-level ICCs into correlations for the\ncluster-period means further provides a rigorous justification for the\ncluster-period approach. The proposed approach addresses a long-recognized\ncomputational burden associated with estimating equations defined based on\nindividual-level observations, and enables fast point and interval estimation\nof the intervention effect and correlations. We further propose matrix-adjusted\nestimating equations to improve the finite-sample inference for ICCs. By\nproviding a valid approach to estimate ICCs within the class of generalized\nlinear models for correlated binary outcomes, this article operationalizes key\nrecommendations from the CONSORT extension to SW-CRTs, including the reporting\nof ICCs.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:40:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Fan", ""], ["Yu", "Hengshi", ""], ["Rathouz", "Paul J.", ""], ["Turner", "Elizabeth L.", ""], ["Preisser", "John S.", ""]]}, {"id": "2101.00491", "submitter": "Adam Walder", "authors": "Adam Walder and Ephraim M. Hanks", "title": "A New Framework for Inference on Markov Population Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we construct a joint Gaussian likelihood for approximate\ninference on Markov population models. We demonstrate that Markov population\nmodels can be approximated by a system of linear stochastic differential\nequations with time-varying coefficients. We show that the system of stochastic\ndifferential equations converges to a set of ordinary differential equations.\nWe derive our proposed joint Gaussian deterministic limiting approximation\n(JGDLA) model from the limiting system of ordinary differential equations. The\nresults is a method for inference on Markov population models that relies\nsolely on the solution to a system deterministic equations. We show that our\nmethod requires no stochastic infill and exhibits improved predictive power in\ncomparison to the Euler-Maruyama scheme on simulated\nsusceptible-infected-recovered data sets. We use the JGDLA to fit a stochastic\nsusceptible-exposed-infected-recovered system to the Princess Diamond COVID-19\ncruise ship data set.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 18:15:49 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Walder", "Adam", ""], ["Hanks", "Ephraim M.", ""]]}, {"id": "2101.00514", "submitter": "Liliana Forzani", "authors": "Dennis Cook, Liliana Forzani, Lan Liu", "title": "Envelopes for multivariate linear regression with linearly constrained\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A constrained multivariate linear model is a multivariate linear model with\nthe columns of its coefficient matrix constrained to lie in a known subspace.\nThis class of models includes those typically used to study growth curves and\nlongitudinal data. Envelope methods have been proposed to improve estimation\nefficiency in the class of unconstrained multivariate linear models, but have\nnot yet been developed for constrained models that we develop in this article.\nWe first compare the standard envelope estimator based on an unconstrained\nmultivariate model with the standard estimator arising from a constrained\nmultivariate model in terms of bias and efficiency. Then, to further improve\nefficiency, we propose a novel envelope estimator based on a constrained\nmultivariate model. Novel envelope-based testing methods are also proposed. We\nprovide support for our proposals by simulations and by studying the classical\ndental data and data from the China Health and Nutrition Survey and a study of\nprobiotic capacity to reduced Salmonella infection.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 20:44:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cook", "Dennis", ""], ["Forzani", "Liliana", ""], ["Liu", "Lan", ""]]}, {"id": "2101.00527", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai", "title": "Statistical Inference on the Hilbert Sphere with Application to Random\n  Densities", "comments": "N/A", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infinite-dimensional Hilbert sphere $S^\\infty$ has been widely employed\nto model density functions and shapes, extending the finite-dimensional\ncounterpart. We consider the Fr\\'echet mean as an intrinsic summary of the\ncentral tendency of data lying on $S^\\infty$. To break a path for sound\nstatistical inference, we derive properties of the Fr\\'echet mean on $S^\\infty$\nby establishing its existence and uniqueness as well as a root-$n$ central\nlimit theorem (CLT) for the sample version, overcoming obstructions from\ninfinite-dimensionality and lack of compactness on $S^\\infty$. Intrinsic CLTs\nfor the estimated tangent vectors and covariance operator are also obtained.\nAsymptotic and bootstrap hypothesis tests for the Fr\\'echet mean based on\nprojection and norm are then proposed and are shown to be consistent. The\nproposed two-sample tests are applied to make inference for daily taxi demand\npatterns over Manhattan modeled as densities, of which the square roots are\nanalyzed on the Hilbert sphere. Numerical properties of the proposed hypothesis\ntests which utilize the spherical geometry are studied in the real data\napplication and simulations, where we demonstrate that the tests based on the\nintrinsic geometry compare favorably to those based on an extrinsic or flat\ngeometry.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 22:52:14 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dai", "Xiongtao", ""]]}, {"id": "2101.00565", "submitter": "Jose Figueroa-Lopez", "authors": "Jos\\'e E. Figueroa-L\\'opez, Ruoting Gong, and Yuchen Han", "title": "Estimation of Tempered Stable L\\'{e}vy Models of Infinite Variation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new method for the estimation of a semiparametric\ntempered stable L\\'{e}vy model. The estimation procedure combines iteratively\nan approximate semiparametric method of moment estimator, Truncated Realized\nQuadratic Variations (TRQV), and a newly found small-time high-order\napproximation for the optimal threshold of the TRQV of tempered stable\nprocesses. The method is tested via simulations to estimate the volatility and\nthe Blumenthal-Getoor index of the generalized CGMY model as well as the\nintegrated volatility of a Heston type model with CGMY jumps. The method\noutperforms other efficient alternatives proposed in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 06:00:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Figueroa-L\u00f3pez", "Jos\u00e9 E.", ""], ["Gong", "Ruoting", ""], ["Han", "Yuchen", ""]]}, {"id": "2101.00592", "submitter": "Weijian Luo", "authors": "Weijian Luo and Mai Wo", "title": "Binary Outcome Copula Regression Model with Sampling Gradient Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Use copula to model dependency of variable extends multivariate gaussian\nassumption. In this paper we first empirically studied copula regression model\nwith continous response. Both simulation study and real data study are given.\nSecondly we give a novel copula regression model with binary outcome, and we\npropose a score gradient estimation algorithms to fit the model. Both\nsimulation study and real data study are given for our model and fitting\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 09:35:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Luo", "Weijian", ""], ["Wo", "Mai", ""]]}, {"id": "2101.00726", "submitter": "Pavlo Mozharovskyi", "authors": "Jevgenijs Ivanovs and Pavlo Mozharovskyi", "title": "Distributionally robust halfspace depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tukey's halfspace depth can be seen as a stochastic program and as such it is\nnot guarded against optimizer's curse, so that a limited training sample may\neasily result in a poor out-of-sample performance. We propose a generalized\nhalfspace depth concept relying on the recent advances in distributionally\nrobust optimization, where every halfspace is examined using the respective\nworst-case distribution in the Wasserstein ball of radius $\\delta\\geq 0$\ncentered at the empirical law. This new depth can be seen as a smoothed and\nregularized classical halfspace depth which is retrieved as $\\delta\\downarrow\n0$. It inherits most of the main properties of the latter and, additionally,\nenjoys various new attractive features such as continuity and strict positivity\nbeyond the convex hull of the support. We provide numerical illustrations of\nthe new depth and its advantages, and develop some fundamental theory. In\nparticular, we study the upper level sets and the median region including their\nbreakdown properties.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 23:30:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ivanovs", "Jevgenijs", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "2101.00987", "submitter": "Silvia Bacci Prof.", "authors": "Bruno Arpino, Silvia Bacci, Leonardo Grilli, Raffaele Guetto and Carla\n  Rampichini", "title": "Conditioning on the pre-test versus gain score modeling: revisiting the\n  controversy in a multilevel setting", "comments": "34 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the effect of a treatment on the progress of subjects\ntested both before and after treatment assignment. A vast literature compares\nthe competing approaches of modeling the post-test score conditionally on the\npre-test score versus modeling the difference, namely the gain score. Our\ncontribution resides in analyzing the merits and drawbacks of the two\napproaches in a multilevel setting. This is relevant in many fields, for\nexample education with students nested into schools. The multilevel structure\nraises peculiar issues related to the contextual effects and the distinction\nbetween individual-level and cluster-level treatment. We derive approximate\nanalytical results and compare the two approaches by a simulation study. For an\nindividual-level treatment our findings are in line with the literature,\nwhereas for a cluster-level treatment we point out the key role of the cluster\nmean of the pre-test score, which favors the conditioning approach in settings\nwith large clusters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:02:33 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Arpino", "Bruno", ""], ["Bacci", "Silvia", ""], ["Grilli", "Leonardo", ""], ["Guetto", "Raffaele", ""], ["Rampichini", "Carla", ""]]}, {"id": "2101.01085", "submitter": "Michele Cantarella", "authors": "Michele Cantarella, Andrea Neri and Maria Giovanna Ranalli", "title": "Mind the wealth gap: a new allocation method to match micro and macro\n  statistics for household wealth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The financial and economic crisis recently experienced by many European\ncountries has increased demand for timely, coherent and consistent\ndistributional information for the household sector. In the Euro area, most of\nthe NCBs collect such information through income and wealth surveys, which are\noften used to inform their decisions. These surveys, however, can often suffer\nfrom biases, usually caused by non-response and under-reporting behaviours,\nleading to a mismatch with macroeconomic aggregates. In this paper, we develop\na novel allocation method which combines information from a power law (Pareto)\nmodel and imputation procedures so to address these issues simultaneously, when\nonly limited external information is available. We provide two important\ncontributions: first, we adjust the weights of observed survey households for\nnon-response bias, then, we correct for measurement error. Finally, we produce\ndistributional indicators for four Euro-Area countries.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:45:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 12:06:53 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cantarella", "Michele", ""], ["Neri", "Andrea", ""], ["Ranalli", "Maria Giovanna", ""]]}, {"id": "2101.01093", "submitter": "Yusuke Narita", "authors": "Atila Abdulkadiroglu, Joshua D. Angrist, Yusuke Narita, and Parag\n  Pathak", "title": "Breaking Ties: Regression Discontinuity Design Meets Market Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many schools in large urban districts have more applicants than seats.\nCentralized school assignment algorithms ration seats at over-subscribed\nschools using randomly assigned lottery numbers, non-lottery tie-breakers like\ntest scores, or both. The New York City public high school match illustrates\nthe latter, using test scores and other criteria to rank applicants at\n``screened'' schools, combined with lottery tie-breaking at unscreened\n``lottery'' schools. We show how to identify causal effects of school\nattendance in such settings. Our approach generalizes regression discontinuity\nmethods to allow for multiple treatments and multiple running variables, some\nof which are randomly assigned. The key to this generalization is a local\npropensity score that quantifies the school assignment probabilities induced by\nlottery and non-lottery tie-breakers. The local propensity score is applied in\nan empirical assessment of the predictive value of New York City's school\nreport cards. Schools that receive a high grade indeed improve SAT math scores\nand increase graduation rates, though by much less than OLS estimates suggest.\nSelection bias in OLS estimates is egregious for screened schools.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 09:00:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Abdulkadiroglu", "Atila", ""], ["Angrist", "Joshua D.", ""], ["Narita", "Yusuke", ""], ["Pathak", "Parag", ""]]}, {"id": "2101.01157", "submitter": "Kidus Asfaw", "authors": "Kidus Asfaw, Joonha Park, Allister Ho, Aaron A. King, Edward Ionides", "title": "Partially observed Markov processes with spatial structure via the R\n  package spatPomp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address inference for a partially observed nonlinear non-Gaussian latent\nstochastic system comprised of interacting units. Each unit has a state, which\nmay be discrete or continuous, scalar or vector valued. In biological\napplications, the state may represent a structured population or the abundances\nof a collection of species at a single location. Units can have spatial\nlocations, allowing the description of spatially distributed interacting\npopulations arising in ecology, epidemiology and elsewhere. We consider models\nwhere the collection of states is a latent Markov process, and a time series of\nnoisy or incomplete measurements is made on each unit. A model of this form is\ncalled a spatiotemporal partially observed Markov process (SpatPOMP). The R\npackage spatPomp provides an environment for implementing SpatPOMP models,\nanalyzing data, and developing new inference approaches. We describe the\nspatPomp implementations of some methods with scaling properties suited to\nSpatPOMP models. We demonstrate the package on a simple Gaussian system and on\na nontrivial epidemiological model for measles transmission within and between\ncities. We show how to construct user-specified SpatPOMP models within\nspatPomp.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:40:28 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 22:39:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Asfaw", "Kidus", ""], ["Park", "Joonha", ""], ["Ho", "Allister", ""], ["King", "Aaron A.", ""], ["Ionides", "Edward", ""]]}, {"id": "2101.01170", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha and Andrew H. McCallum and Nathan Seegert", "title": "Better Bunching, Nicer Notching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bunching identification strategy for an elasticity parameter\nthat summarizes agents' response to changes in slope (kink) or intercept\n(notch) of a schedule of incentives. A notch identifies the elasticity but a\nkink does not, when the distribution of agents is fully flexible. We propose\nnew non-parametric and semi-parametric identification assumptions on the\ndistribution of agents that are weaker than assumptions currently made in the\nliterature. We revisit the original empirical application of the bunching\nestimator and find that our weaker identification assumptions result in\nmeaningfully different estimates. We provide the Stata package \"bunching\" to\nimplement our procedures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:58:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bertanha", "Marinho", ""], ["McCallum", "Andrew H.", ""], ["Seegert", "Nathan", ""]]}, {"id": "2101.01235", "submitter": "Staci Hepler", "authors": "Staci A. Hepler (Department of Mathematics and Statistics, Wake Forest\n  University), David Kline (Center for Biostatistics, Department of Biomedical\n  Informatics, The Ohio State University), Andrea Bonny (Division of Adolescent\n  Medicine, Nationwide Children's Hospital, Department of Pediatrics, The Ohio\n  State University), Erin McKnight (Division of Adolescent Medicine, Nationwide\n  Children's Hospital, Department of Pediatrics, The Ohio State University),\n  and Lance A. Waller (Department of Biostatistics and Bioinformatics, Emory\n  University)", "title": "A Bayesian spatio-temporal abundance model for surveillance of the\n  opioid epidemic", "comments": "* Authors Hepler and Kline contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Opioid misuse is a national epidemic and a significant drug related threat to\nthe United States. While the scale of the problem is undeniable, estimates of\nthe local prevalence of opioid misuse are lacking, despite their importance to\npolicy-making and resource allocation. This is due, in part, to the challenge\nof directly measuring opioid misuse at a local level. In this paper, we develop\na Bayesian hierarchical spatio-temporal abundance model that integrates\nindirect county-level data on opioid overdose deaths and treatment admissions\nwith state-level survey estimates on prevalence of opioid misuse to estimate\nthe latent county-level prevalence and counts of people who misuse opioids. A\nsimulation study shows that our joint model accurately recovers the latent\ncounts and prevalence and thus overcomes known limitations with identifiability\nin abundance models with non-replicated observations. We apply our model to\ncounty-level surveillance data from the state of Ohio. Our proposed framework\ncan be applied to other applications of small area estimation for hard to reach\npopulations, which is a common occurrence with many health conditions such as\nthose related to illicit behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:55:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 16:27:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hepler", "Staci A.", "", "Department of Mathematics and Statistics, Wake Forest\n  University"], ["Kline", "David", "", "Center for Biostatistics, Department of Biomedical\n  Informatics, The Ohio State University"], ["Bonny", "Andrea", "", "Division of Adolescent\n  Medicine, Nationwide Children's Hospital, Department of Pediatrics, The Ohio\n  State University"], ["McKnight", "Erin", "", "Division of Adolescent Medicine, Nationwide\n  Children's Hospital, Department of Pediatrics, The Ohio State University"], ["Waller", "Lance A.", "", "Department of Biostatistics and Bioinformatics, Emory\n  University"]]}, {"id": "2101.01245", "submitter": "Marinho Bertanha", "authors": "Marinho Bertanha", "title": "Regression Discontinuity Design with Many Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous empirical studies employ regression discontinuity designs with\nmultiple cutoffs and heterogeneous treatments. A common practice is to\nnormalize all the cutoffs to zero and estimate one effect. This procedure\nidentifies the average treatment effect (ATE) on the observed distribution of\nindividuals local to existing cutoffs. However, researchers often want to make\ninferences on more meaningful ATEs, computed over general counterfactual\ndistributions of individuals, rather than simply the observed distribution of\nindividuals local to existing cutoffs. This paper proposes a consistent and\nasymptotically normal estimator for such ATEs when heterogeneity follows a\nnon-parametric function of cutoff characteristics in the sharp case. The\nproposed estimator converges at the minimax optimal rate of root-n for a\nspecific choice of tuning parameters. Identification in the fuzzy case, with\nmultiple cutoffs, is impossible unless heterogeneity follows a\nfinite-dimensional function of cutoff characteristics. Under parametric\nheterogeneity, this paper proposes an ATE estimator for the fuzzy case that\noptimally combines observations to maximize its precision.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 21:46:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bertanha", "Marinho", ""]]}, {"id": "2101.01253", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu and Sinan Y{\\i}ld{\\i}r{\\i}m and Arnaud Doucet and\n  Nicolas Chopin", "title": "Metropolis-Hastings with Averaged Acceptance Ratios", "comments": "arXiv admin note: text overlap with arXiv:1803.09527", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods to sample from a probability\ndistribution $\\pi$ defined on a space $(\\Theta,\\mathcal{T})$ consist of the\nsimulation of realisations of Markov chains $\\{\\theta_{n},n\\geq1\\}$ of\ninvariant distribution $\\pi$ and such that the distribution of $\\theta_{i}$\nconverges to $\\pi$ as $i\\rightarrow\\infty$. In practice one is typically\ninterested in the computation of expectations of functions, say $f$, with\nrespect to $\\pi$ and it is also required that averages\n$M^{-1}\\sum_{n=1}^{M}f(\\theta_{n})$ converge to the expectation of interest.\nThe iterative nature of MCMC makes it difficult to develop generic methods to\ntake advantage of parallel computing environments when interested in reducing\ntime to convergence. While numerous approaches have been proposed to reduce the\nvariance of ergodic averages, including averaging over independent realisations\nof $\\{\\theta_{n},n\\geq1\\}$ simulated on several computers, techniques to reduce\nthe \"burn-in\" of MCMC are scarce. In this paper we explore a simple and generic\napproach to improve convergence to equilibrium of existing algorithms which\nrely on the Metropolis-Hastings (MH) update, the main building block of MCMC.\nThe main idea is to use averages of the acceptance ratio w.r.t. multiple\nrealisations of random variables involved, while preserving $\\pi$ as invariant\ndistribution. The methodology requires limited change to existing code, is\nnaturally suited to parallel computing and is shown on our examples to provide\nsubstantial performance improvements both in terms of convergence to\nequilibrium and variance of ergodic averages. In some scenarios gains are\nobserved even on a serial machine.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:10:46 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Andrieu", "Christophe", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""], ["Doucet", "Arnaud", ""], ["Chopin", "Nicolas", ""]]}, {"id": "2101.01299", "submitter": "Shaowu Yuchi", "authors": "Henry Shaowu Yuchi, Simon Mak, Yao Xie", "title": "Bayesian Uncertainty Quantification for Low-rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of uncertainty quantification for an unknown low-rank\nmatrix $\\mathbf{X}$, given a partial and noisy observation of its entries. This\nquantification of uncertainty is essential for many real-world problems,\nincluding image processing, satellite imaging, and seismology, providing a\nprincipled framework for validating scientific conclusions and guiding\ndecision-making. However, existing literature has largely focused on the\ncompletion (i.e., point estimation) of the matrix $\\mathbf{X}$, with little\nwork on investigating its uncertainty. To this end, we propose in this work a\nnew Bayesian modeling framework, called BayeSMG, which parametrizes the unknown\n$\\mathbf{X}$ via its underlying row and column subspaces. This Bayesian\nsubspace parametrization allows for efficient posterior inference on matrix\nsubspaces, which represents interpretable phenomena in many applications. This\ncan then be leveraged for improved matrix recovery. We demonstrate the\neffectiveness of BayeSMG over existing Bayesian matrix recovery methods in\nnumerical experiments and a seismic sensor network application.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:47:32 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 06:06:59 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Yuchi", "Henry Shaowu", ""], ["Mak", "Simon", ""], ["Xie", "Yao", ""]]}, {"id": "2101.01535", "submitter": "Haoyang Cheng", "authors": "Wenquan Cui and Haoyang Cheng", "title": "An RKHS-Based Semiparametric Approach to Nonlinear Sufficient Dimension\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the theory of reproducing kernel Hilbert space (RKHS) and\nsemiparametric method, we propose a new approach to nonlinear dimension\nreduction. The method extends the semiparametric method into a more generalized\ndomain where both the interested parameters and nuisance parameters to be\ninfinite dimensional. By casting the nonlinear dimensional reduction problem in\na generalized semiparametric framework, we calculate the orthogonal complement\nspace of generalized nuisance tangent space to derive the estimating equation.\nSolving the estimating equation by the theory of RKHS and regularization, we\nobtain the estimation of dimension reduction directions of the sufficient\ndimension reduction (SDR) subspace and also show the asymptotic property of\nestimator. Furthermore, the proposed method does not rely on the linearity\ncondition and constant variance condition. Simulation and real data studies are\nconducted to demonstrate the finite sample performance of our method in\ncomparison with several existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:27:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Cui", "Wenquan", ""], ["Cheng", "Haoyang", ""]]}, {"id": "2101.01551", "submitter": "Martijn Schuemie", "authors": "Martijn J. Schuemie, Yong Chen, David Madigan, Marc A. Suchard", "title": "Combining Cox Regressions Across a Heterogeneous Distributed Research\n  Network Facing Small and Zero Counts", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of the effects of medical interventions increasingly take place in\ndistributed research settings using data from multiple clinical data sources\nincluding electronic health records and administrative claims. In such\nsettings, privacy concerns typically prohibit sharing of individual patient\ndata, and instead, analyses can only utilize summary statistics from the\nindividual databases. In the specific but very common context of the Cox\nproportional hazards model, we show that standard meta analysis methods then\nlead to substantial bias when outcome counts are small. This bias derives\nprimarily from the normal approximations that the methods utilize. Here we\npropose and evaluate methods that eschew normal approximations in favor of\nthree more flexible approximations: a skew-normal, a one-dimensional grid, and\na custom parametric function that mimics the behavior of the Cox likelihood\nfunction. In extensive simulation studies we demonstrate how these\napproximations impact bias in the context of both fixed-effects and (Bayesian)\nrandom-effects models. We then apply these approaches to three real-world\nstudies of the comparative safety of antidepressants, each using data from four\nobservational healthcare databases.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:35:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Schuemie", "Martijn J.", ""], ["Chen", "Yong", ""], ["Madigan", "David", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2101.01599", "submitter": "Linbo Wang", "authors": "Zhenhua Lin, Dehan Kong, Linbo Wang", "title": "Causal Inference on Non-linear Spaces: Distribution Functions and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding causal relationships is one of the most important goals of\nmodern science. So far, the causal inference literature has focused almost\nexclusively on outcomes coming from a linear space, most commonly the Euclidean\nspace. However, it is increasingly common that complex datasets collected\nthrough electronic sources, such as wearable devices and medical imaging,\ncannot be represented as data points from linear spaces. In this paper, we\npresent a formal definition of causal effects for outcomes from non-linear\nspaces, with a focus on the Wasserstein space of cumulative distribution\nfunctions. We develop doubly robust estimators and associated asymptotic theory\nfor these causal effects. Our framework extends to outcomes from certain\nRiemannian manifolds. As an illustration, we use our framework to quantify the\ncausal effect of marriage on physical activity patterns using wearable device\ndata collected through the National Health and Nutrition Examination Survey.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:38:11 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Lin", "Zhenhua", ""], ["Kong", "Dehan", ""], ["Wang", "Linbo", ""]]}, {"id": "2101.01603", "submitter": "Artuur Leeuwenberg", "authors": "Artuur M. Leeuwenberg, Maarten van Smeden, Johannes A. Langendijk,\n  Arjen van der Schaaf, Murielle E. Mauer, Karel G.M. Moons, Johannes B.\n  Reitsma, Ewoud Schuit", "title": "Comparing methods addressing multi-collinearity when developing\n  prediction models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical prediction models are developed widely across medical disciplines.\nWhen predictors in such models are highly collinear, unexpected or spurious\npredictor-outcome associations may occur, thereby potentially reducing\nface-validity and explainability of the prediction model. Collinearity can be\ndealt with by exclusion of collinear predictors, but when there is no a priori\nmotivation (besides collinearity) to include or exclude specific predictors,\nsuch an approach is arbitrary and possibly inappropriate. We compare different\nmethods to address collinearity, including shrinkage, dimensionality reduction,\nand constrained optimization. The effectiveness of these methods is illustrated\nvia simulations. In the conducted simulations, no effect of collinearity was\nobserved on predictive outcomes. However, a negative effect of collinearity on\nthe stability of predictor selection was found, affecting all compared methods,\nbut in particular methods that perform strong predictor selection (e.g.,\nLasso).}\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:42:11 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Leeuwenberg", "Artuur M.", ""], ["van Smeden", "Maarten", ""], ["Langendijk", "Johannes A.", ""], ["van der Schaaf", "Arjen", ""], ["Mauer", "Murielle E.", ""], ["Moons", "Karel G. M.", ""], ["Reitsma", "Johannes B.", ""], ["Schuit", "Ewoud", ""]]}, {"id": "2101.01624", "submitter": "Marco Mingione", "authors": "Pierfrancesco Alaimo Di Loro and Marco Mingione and Jonah Lipsitt and\n  Christina M. Batteate and Michael Jerrett and Sudipto Banerjee", "title": "Bayesian hierarchical modeling and analysis for physical activity\n  trajectories using actigraph data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid developments in streaming data technologies are continuing to generate\nincreased interest in monitoring human activity. Wearable devices, such as\nwrist-worn sensors that monitor gross motor activity (actigraphy), have become\nprevalent. An actigraph unit continually records the activity level of an\nindividual, producing a very large amount of data at a high-resolution that can\nbe immediately downloaded and analyzed. While this kind of \\textit{big data}\nincludes both spatial and temporal information, the variation in such data\nseems to be more appropriately modeled by considering stochastic evolution\nthrough time while accounting for spatial information separately. We propose a\ncomprehensive Bayesian hierarchical modeling and inferential framework for\nactigraphy data reckoning with the massive sizes of such databases while\nattempting to offer full inference. Building upon recent developments in this\nfield, we construct Nearest Neighbour Gaussian Processes (NNGPs) for actigraphy\ndata to compute at large temporal scales. More specifically, we construct a\ntemporal NNGP and we focus on the optimized implementation of the collapsed\nalgorithm in this specific context. This approach permits improved model\nscaling while also offering full inference. We test and validate our methods on\nsimulated data and subsequently apply and verify their predictive ability on an\noriginal dataset concerning a health study conducted by the Fielding School of\nPublic Health of the University of California, Los Angeles.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:15:24 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 11:20:07 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 16:21:12 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Di Loro", "Pierfrancesco Alaimo", ""], ["Mingione", "Marco", ""], ["Lipsitt", "Jonah", ""], ["Batteate", "Christina M.", ""], ["Jerrett", "Michael", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2101.01871", "submitter": "Sanjeena Subedi", "authors": "Wangshu Tu and Sanjeena Subedi", "title": "Logistic Normal Multinomial Factor Analyzers for Clustering Microbiome\n  Data", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human microbiome plays an important role in human health and disease\nstatus. Next generating sequencing technologies allow for quantifying the\ncomposition of the human microbiome. Clustering these microbiome data can\nprovide valuable information by identifying underlying patterns across samples.\nRecently, Fang and Subedi (2020) proposed a logistic normal multinomial mixture\nmodel (LNM-MM) for clustering microbiome data. As microbiome data tends to be\nhigh dimensional, here, we develop a family of logistic normal multinomial\nfactor analyzers (LNM-FA) by incorporating a factor analyzer structure in the\nLNM-MM. This family of models is more suitable for high-dimensional data as the\nnumber of parameters in LNM-FA can be greatly reduced by assuming that the\nnumber of latent factors is small. Parameter estimation is done using a\ncomputationally efficient variant of the alternating expectation conditional\nmaximization algorithm that utilizes variational Gaussian approximations. The\nproposed method is illustrated using simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:01:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tu", "Wangshu", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2101.01908", "submitter": "Bo Zhang", "authors": "Bo Zhang, Guangming Pan, Qiwei Yao and Wang Zhou", "title": "Factor Modelling for Clustering High-dimensional Time Series", "comments": "10 figures, 12 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new unsupervised learning method for clustering a large number\nof time series based on a latent factor structure. Each cluster is\ncharacterized by its own cluster-specific factors in addition to some common\nfactors which impact on all the time series concerned. Our setting also offers\nthe flexibility that some time series may not belong to any clusters. The\nconsistency with explicit convergence rates is established for the estimation\nof the common factors, the cluster-specific factors, the latent clusters.\nNumerical illustration with both simulated data as well as a real data example\nis also reported. As a spin-off, the proposed new approach also advances\nsignificantly the statistical inference for the factor model of Lam and Yao\n(2012).\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:55:15 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Bo", ""], ["Pan", "Guangming", ""], ["Yao", "Qiwei", ""], ["Zhou", "Wang", ""]]}, {"id": "2101.01932", "submitter": "Jakob Dambon", "authors": "Jakob A. Dambon and Fabio Sigrist and Reinhard Furrer", "title": "Joint Variable Selection of both Fixed and Random Effects for Gaussian\n  Process-based Spatially Varying Coefficient Models", "comments": "26 pages including appendix. Containing 6 figures and 6 tables.\n  Updated Declarations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Spatially varying coefficient (SVC) models are a type of regression model for\nspatial data where covariate effects vary over space. If there are several\ncovariates, a natural question is which covariates have a spatially varying\neffect and which not. We present a new variable selection approach for Gaussian\nprocess-based SVC models. It relies on a penalized maximum likelihood\nestimation (PMLE) and allows variable selection both with respect to fixed\neffects and Gaussian process random effects. We validate our approach both in a\nsimulation study as well as a real world data set. Our novel approach shows\ngood selection performance in the simulation study. In the real data\napplication, our proposed PMLE yields sparser SVC models and achieves a smaller\ninformation criterion than classical MLE. In a cross-validation applied on the\nreal data, we show that sparser PML estimated SVC models are on par with ML\nestimated SVC models with respect to predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 09:09:40 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 10:30:04 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Dambon", "Jakob A.", ""], ["Sigrist", "Fabio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2101.01960", "submitter": "Rebecca Killick", "authors": "Xueheng Shi, Colin Gallagher, Robert Lund and Rebecca Killick", "title": "A Comparison of Single and Multiple Changepoint Techniques for Time\n  Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes and compares several prominent single and multiple\nchangepoint techniques for time series data. Due to their importance in\ninferential matters, changepoint research on correlated data has accelerated\nrecently. Unfortunately, small perturbations in model assumptions can\ndrastically alter changepoint conclusions; for example, heavy positive\ncorrelation in a time series can be misattributed to a mean shift should\ncorrelation be ignored. This paper considers both single and multiple\nchangepoint techniques. The paper begins by examining cumulative sum (CUSUM)\nand likelihood ratio tests and their variants for the single changepoint\nproblem; here, various statistics, boundary cropping scenarios, and scaling\nmethods (e.g., scaling to an extreme value or Brownian Bridge limit) are\ncompared. A recently developed test based on summing squared CUSUM statistics\nover all times is shown to have realistic Type I errors and superior detection\npower. The paper then turns to the multiple changepoint setting. Here,\npenalized likelihoods drive the discourse, with AIC, BIC, mBIC, and MDL\npenalties being considered. Binary and wild binary segmentation techniques are\nalso compared. We introduce a new distance metric specifically designed to\ncompare two multiple changepoint segmentations. Algorithmic and computational\nconcerns are discussed and simulations are provided to support all conclusions.\nIn the end, the multiple changepoint setting admits no clear methodological\nwinner, performance depending on the particular scenario. Nonetheless, some\npractical guidance will emerge.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:36:57 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Shi", "Xueheng", ""], ["Gallagher", "Colin", ""], ["Lund", "Robert", ""], ["Killick", "Rebecca", ""]]}, {"id": "2101.02028", "submitter": "Ye Tian", "authors": "Ye Tian", "title": "A Multilayer Correlated Topic Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 21:50:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tian", "Ye", ""]]}, {"id": "2101.02035", "submitter": "David Degras", "authors": "David Degras", "title": "Scalable Feature Matching Across Large Data Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with matching feature vectors in a one-to-one fashion\nacross large collections of datasets. Formulating this task as a\nmultidimensional assignment problem with decomposable costs (MDADC), we develop\nextremely fast algorithms with time complexity linear in the number $n$ of\ndatasets and space complexity a small fraction of the data size. These\nremarkable properties hinge on using the squared Euclidean distance as\ndissimilarity function, which can reduce ${n \\choose 2}$ matching problems\nbetween pairs of datasets to $n$ problems and enable calculating assignment\ncosts on the fly. To our knowledge, no other method applicable to the MDADC\npossesses these linear scaling and low-storage properties necessary to\nlarge-scale applications. In numerical experiments, the novel algorithms\noutperform competing methods and show excellent computational and optimization\nperformances. An application of feature matching to a large neuroimaging\ndatabase is presented. The algorithms of this paper are implemented in the R\npackage matchFeat available at https://github.com/ddegras/matchFeat.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:52:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Degras", "David", ""]]}, {"id": "2101.02148", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Tobias Ruckstuhl, Helena Obrist, Peter B\\\"uhlmann", "title": "Graphical Elastic Net and Target Matrices: Fast Algorithms and Software\n  for Sparse Precision Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of undirected Gaussian graphical models and inverse\ncovariances in high-dimensional scenarios by penalizing the corresponding\nprecision matrix. While single $L_1$ (Graphical Lasso) and $L_2$ (Graphical\nRidge) penalties for the precision matrix have already been studied, we propose\nthe combination of both, yielding an Elastic Net type penalty. We enable\nadditional flexibility by allowing to include diagonal target matrices for the\nprecision matrix. We generalize existing algorithms for the Graphical Lasso and\nprovide corresponding software with an efficient implementation to facilitate\nusage for practitioners. Our software borrows computationally favorable parts\nfrom a number of existing packages for the Graphical Lasso, leading to an\noverall fast(er) implementation and at the same time yielding also much more\nmethodological flexibility.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:28:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Ruckstuhl", "Tobias", ""], ["Obrist", "Helena", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.02206", "submitter": "Xinwei Deng", "authors": "Xia Cai, Li Xu, C. Devon Lin, Yili Hong, Xinwei Deng", "title": "Sequential Design of Computer Experiments with Quantitative and\n  Qualitative Factors in Applications to HPC Performance Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer experiments with both qualitative and quantitative factors are\nwidely used in many applications. Motivated by the emerging need of optimal\nconfiguration in the high-performance computing (HPC) system, this work\nproposes a sequential design, denoted as adaptive composite exploitation and\nexploration (CEE), for optimization of computer experiments with qualitative\nand quantitative factors. The proposed adaptive CEE method combines the\npredictive mean and standard deviation based on the additive Gaussian process\nto achieve a meaningful balance between exploitation and exploration for\noptimization. Moreover, the adaptiveness of the proposed sequential procedure\nallows the selection of next design point from the adaptive design region.\nTheoretical justification of the adaptive design region is provided. The\nperformance of the proposed method is evaluated by several numerical examples\nin simulations. The case study of HPC performance optimization further\nelaborates the merits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 11:46:19 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Cai", "Xia", ""], ["Xu", "Li", ""], ["Lin", "C. Devon", ""], ["Hong", "Yili", ""], ["Deng", "Xinwei", ""]]}, {"id": "2101.02233", "submitter": "Raphael Huser", "authors": "Zhongwei Zhang, Reinaldo B. Arellano-Valle, Marc G. Genton and\n  Rapha\\\"el Huser", "title": "Tractable Bayes of Skew-Elliptical Link Models for Correlated Binary\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlated binary response data with covariates are ubiquitous in\nlongitudinal or spatial studies. Among the existing statistical models the most\nwell-known one for this type of data is the multivariate probit model, which\nuses a Gaussian link to model dependence at the latent level. However, a\nsymmetric link may not be appropriate if the data are highly imbalanced. Here,\nwe propose a multivariate skew-elliptical link model for correlated binary\nresponses, which includes the multivariate probit model as a special case.\nFurthermore, we perform Bayesian inference for this new model and prove that\nthe regression coefficients have a closed-form unified skew-elliptical\nposterior. The new methodology is illustrated by application to COVID-19\npandemic data from three different counties of the state of California, USA. By\njointly modeling extreme spikes in weekly new cases, our results show that the\nspatial dependence cannot be neglected. Furthermore, the results also show that\nthe skewed latent structure of our proposed model improves the flexibility of\nthe multivariate probit model and provides better fit to our highly imbalanced\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:11:58 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhang", "Zhongwei", ""], ["Arellano-Valle", "Reinaldo B.", ""], ["Genton", "Marc G.", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "2101.02273", "submitter": "Kejin Wu", "authors": "Kejin Wu, Sayar Karmakar", "title": "Boosting Model-free predictions for econometric datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose some novel non-parametric prediction methods to\nperform short- and long-term aggregated forecasting for the log-returns of\neconometric datasets. The previous works in the regime of NoVaS model-free\nprediction are restricted to short-term forecasting only. Often practitioners\nand traders want to understand the future trend for a longer time into the\nfuture. This article serves two purposes. First it explores robustness of\nexisting model-free methods for long-term predictions. Then it introduces, with\nsystematic justification, some new methods that improve the existing ones for\nboth short- and long-term predictions. We provide detailed discussions of the\nexisting and new methods and challenge the new ones with extensive simulations\nand real-life data. Interesting features of our methods are that these entail\nsignificant improvements compared to existing methods for a longer horizon,\nstrong volatile movements and shorter sample size.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:33:34 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Wu", "Kejin", ""], ["Karmakar", "Sayar", ""]]}, {"id": "2101.02354", "submitter": "Di Wang", "authors": "Di Wang, Wen Ye, Kevin He", "title": "Kullback-Leibler-Based Discrete Relative Risk Models for Integration of\n  Published Prediction Models with New Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing literature for prediction of time-to-event data has primarily\nfocused on risk factors from an individual dataset. However, these analyses may\nsuffer from small sample sizes, high dimensionality and low signal-to-noise\nratios. To improve prediction stability and better understand risk factors\nassociated with outcomes of interest, we propose a Kullback-Leibler-based\ndiscrete relative risk modeling procedure. Simulations and real data analysis\nare conducted to show the advantage of the proposed methods compared with those\nsolely based on local dataset or prior models.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:46:00 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Wang", "Di", ""], ["Ye", "Wen", ""], ["He", "Kevin", ""]]}, {"id": "2101.02400", "submitter": "Anqi Zhao", "authors": "Anqi Zhao and Peng Ding", "title": "Regression-based causal inference with factorial experiments: estimands,\n  model specifications, and design-based properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Factorial designs are widely used due to their ability to accommodate\nmultiple factors simultaneously. The factor-based regression with main effects\nand some interactions is the dominant strategy for downstream data analysis,\ndelivering point estimators and standard errors via one single regression.\nJustification of these convenient estimators from the design-based perspective\nrequires quantifying their sampling properties under the assignment mechanism\nconditioning on the potential outcomes. To this end, we derive the sampling\nproperties of the factor-based regression estimators from both saturated and\nunsaturated models, and demonstrate the appropriateness of the robust standard\nerrors for the Wald-type inference. We then quantify the bias-variance\ntrade-off between the saturated and unsaturated models from the design-based\nperspective, and establish a novel design-based Gauss--Markov theorem that\nensures the latter's gain in efficiency when the nuisance effects omitted\nindeed do not exist. As a byproduct of the process, we unify the definitions of\nfactorial effects in various literatures and propose a location-shift strategy\nfor their direct estimation from factor-based regressions. Our theory and\nsimulation suggest using factor-based inference for general factorial effects,\npreferably with parsimonious specifications in accordance with the prior\nknowledge of zero nuisance effects.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:47:06 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2101.02404", "submitter": "Mitchell Krock", "authors": "Mitchell Krock, William Kleiber, Dorit Hammerling, and Stephen Becker", "title": "Modeling massive highly-multivariate nonstationary spatial data with the\n  basis graphical lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new modeling framework for highly-multivariate spatial processes\nthat synthesizes ideas from recent multiscale and spectral approaches with\ngraphical models. The basis graphical lasso writes a univariate Gaussian\nprocess as a linear combination of basis functions weighted with entries of a\nGaussian graphical vector whose graph is estimated from optimizing an $\\ell_1$\npenalized likelihood. This paper extends the setting to a multivariate Gaussian\nprocess where the basis functions are weighted with Gaussian graphical vectors.\nWe motivate a model where the basis functions represent different levels of\nresolution and the graphical vectors for each level are assumed to be\nindependent. Using an orthogonal basis grants linear complexity and memory\nusage in the number of spatial locations, the number of basis functions, and\nthe number of realizations. An additional fusion penalty encourages a\nparsimonious conditional independence structure in the multilevel graphical\nmodel. We illustrate our method on a large climate ensemble from the National\nCenter for Atmospheric Research's Community Atmosphere Model that involves 40\nspatial processes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:01:54 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:15:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Krock", "Mitchell", ""], ["Kleiber", "William", ""], ["Hammerling", "Dorit", ""], ["Becker", "Stephen", ""]]}, {"id": "2101.02405", "submitter": "Surin Ahn", "authors": "Surin Ahn, Wei-Ning Chen and Ayfer Ozgur", "title": "Adaptive Group Testing on Networks with Community Structure", "comments": "26 pages, 5 figures, to be presented in part at the 2021 IEEE\n  International Symposium on Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of the group testing problem in World War II, one of the\nprevailing assumptions in the probabilistic variant of the problem has been\nthat individuals in the population are infected by a disease independently.\nHowever, this assumption rarely holds in practice, as diseases typically spread\nthrough interactions between individuals and therefore cause infections to be\ncorrelated. Inspired by characteristics of COVID-19 and similar diseases, we\nconsider an infection model over networks which generalizes the traditional\ni.i.d. model from probabilistic group testing. Under this infection model, we\nask whether knowledge of the network structure can be leveraged to perform\ngroup testing more efficiently, focusing specifically on community-structured\ngraphs drawn from the stochastic block model. We prove that when the network\nand infection parameters are conducive to \"strong community structure,\" our\nproposed adaptive, graph-aware algorithm outperforms the baseline binary\nsplitting algorithm, and is even order-optimal in certain parameter regimes. We\nsupport our results with numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:05:20 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 21:48:59 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 16:10:48 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ahn", "Surin", ""], ["Chen", "Wei-Ning", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2101.02439", "submitter": "Xu Gao", "authors": "Xu Gao, Weining Shen, Jing Ning, Ziding Feng, Jianhua Hu", "title": "Addressing patient heterogeneity in disease predictive model development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses patient heterogeneity associated with prediction\nproblems in biomedical applications. We propose a systematic hypothesis testing\napproach to determine the existence of patient subgroup structure and the\nnumber of subgroups in patient population if subgroups exist. A mixture of\ngeneralized linear models is considered to model the relationship between the\ndisease outcome and patient characteristics and clinical factors, including\ntargeted biomarker profiles. We construct a test statistic based on expectation\nmaximization (EM) algorithm and derive its asymptotic distribution under the\nnull hypothesis. An important computational advantage of the test is that the\ninvolved parameter estimates under the complex alternative hypothesis can be\nobtained through a small number of EM iterations, rather than optimizing the\nobjective function. We demonstrate the finite sample performance of the\nproposed test in terms of type-I error rate and power, using extensive\nsimulation studies. The applicability of the proposed method is illustrated\nthrough an application to a multi-center prostate cancer study.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:15:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gao", "Xu", ""], ["Shen", "Weining", ""], ["Ning", "Jing", ""], ["Feng", "Ziding", ""], ["Hu", "Jianhua", ""]]}, {"id": "2101.02491", "submitter": "Taeho Kim", "authors": "Alexander Goldenshluger and Taeho Kim", "title": "Density Deconvolution with Non-Standard Error Distributions: Rates of\n  Convergence and Adaptive Estimation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a typical standard assumption in the density deconvolution problem that\nthe characteristic function of the measurement error distribution is non-zero\non the real line. While this condition is assumed in the majority of existing\nworks on the topic, there are many problem instances of interest where it is\nviolated. In this paper we focus on non--standard settings where the\ncharacteristic function of the measurement errors has zeros, and study how\nzeros multiplicity affects the estimation accuracy. For a prototypical problem\nof this type we demonstrate that the best achievable estimation accuracy is\ndetermined by the multiplicity of zeros, the rate of decay of the error\ncharacteristic function, as well as by the smoothness and the tail behavior of\nthe estimated density. We derive lower bounds on the minimax risk and develop\noptimal in the minimax sense estimators. In addition, we consider the problem\nof adaptive estimation and propose a data-driven estimator that automatically\nadapts to unknown smoothness and tail behavior of the density to be estimated.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:20:46 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Goldenshluger", "Alexander", ""], ["Kim", "Taeho", ""]]}, {"id": "2101.02506", "submitter": "Gregor Zens", "authors": "Gregor Zens, Sylvia Fr\\\"uhwirth-Schnatter, Helga Wagner", "title": "Efficient Bayesian Modeling of Binary and Categorical Data in R: The UPG\n  Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the UPG package for highly efficient Bayesian inference in\nprobit, logit, multinomial logit and binomial logit models. UPG offers a\nconvenient estimation framework for balanced and imbalanced data settings where\nsampling efficiency is ensured through Markov chain Monte Carlo boosting\nmethods. All sampling algorithms are implemented in C++, allowing for rapid\nparameter estimation. In addition, UPG provides several methods for fast\nproduction of output tables and summary plots that are easily accessible to a\nbroad range of users.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:55:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zens", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Wagner", "Helga", ""]]}, {"id": "2101.02558", "submitter": "Akira Horiguchi", "authors": "Akira Horiguchi and Thomas J. Santner and Ying Sun and Matthew T.\n  Pratola", "title": "Using BART for Multiobjective Optimization of Noisy Multiple Objectives", "comments": "45 pages, 12 figures, submitted to Industry 4.0 special issue of\n  Technometrics journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Techniques to reduce the energy burden of an Industry 4.0 ecosystem often\nrequire solving a multiobjective optimization problem. However, collecting\nexperimental data can often be either expensive or time-consuming. In such\ncases, statistical methods can be helpful. This article proposes Pareto Front\n(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression\nTrees (BART), which is a non-parametric model whose assumptions are typically\nless restrictive than popular alternatives, such as Gaussian Processes. The\nperformance of our BART-based method is compared to a GP-based method using\nanalytic test functions, demonstrating convincing advantages. Finally, our\nBART-based methodology is applied to a motivating Industry 4.0 engineering\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:58:37 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Horiguchi", "Akira", ""], ["Santner", "Thomas J.", ""], ["Sun", "Ying", ""], ["Pratola", "Matthew T.", ""]]}, {"id": "2101.02580", "submitter": "Souvik Banerjee", "authors": "Gajendra K. Vishwakarma, Atanu Bhattacharjee, Souvik Banerjee", "title": "Handling Missingness Value on Jointly Measured Time-Course and\n  Time-to-event Data", "comments": "20 pages, 2 figures, 6 tables. Communications in Statistics -\n  Simulation and Computation (2020)", "journal-ref": null, "doi": "10.1080/03610918.2020.1851711", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Joint modeling technique is a recent advancement in effectively analyzing the\nlongitudinal history of patients with the occurrence of an event of interest\nattached to it. This procedure is successfully implemented in biomarker studies\nto examine parents with the occurrence of tumor. One of the typical problem\nthat influences the necessary inference is the presence of missing values in\nthe longitudinal responses as well as in covariates. The occurrence of\nmissingness is very common due to the dropout of patients from the study. This\narticle presents an effective and detailed way to handle the missing values in\nthe covariates and response variable. This study discusses the effect of\ndifferent multiple imputation techniques on the inferences of joint modeling\nimplemented on imputed datasets. A simulation study is carried out to replicate\nthe complex data structures and conveniently perform our analysis to show its\nefficacy in terms of parameter estimation. This analysis is further illustrated\nwith the longitudinal and survival outcomes of biomarkers' study by assessing\nproper codes in R programming language.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 15:10:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vishwakarma", "Gajendra K.", ""], ["Bhattacharjee", "Atanu", ""], ["Banerjee", "Souvik", ""]]}, {"id": "2101.02703", "submitter": "Anastasios Angelopoulos", "authors": "Stephen Bates and Anastasios Angelopoulos and Lihua Lei and Jitendra\n  Malik and Michael I. Jordan", "title": "Distribution-Free, Risk-Controlling Prediction Sets", "comments": "Project website available at\n  http://www.angelopoulos.ai/blog/posts/rcps/ and codebase available at\n  https://github.com/aangelopoulos/rcps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:59:33 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 03:48:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bates", "Stephen", ""], ["Angelopoulos", "Anastasios", ""], ["Lei", "Lihua", ""], ["Malik", "Jitendra", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2101.02786", "submitter": "Trung Pham", "authors": "Trung Pham, Alex A. Gorodetsky", "title": "Ensemble approximate control variate estimators: Applications to\n  multi-fidelity importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recent growth in multi-fidelity uncertainty quantification has given rise\nto a large set of variance reduction techniques that leverage information from\nmodel ensembles to provide variance reduction for estimates of the statistics\nof a high-fidelity model. In this paper we provide two contributions: (1) we\nutilize an ensemble estimator to account for uncertainties in the optimal\nweights of approximate control variate (ACV) approaches and derive lower bounds\non the number of samples required to guarantee variance reduction; and (2) we\nextend an existing multi-fidelity importance sampling (MFIS) scheme to leverage\ncontrol variates. As such we make significant progress towards both increasing\nthe practicality of approximate control variates$-$for instance, by accounting\nfor the effect of pilot samples$-$and using multi-fidelity approaches more\neffectively for estimating low-probability events. The numerical results\nindicate our hybrid MFIS-ACV estimator achieves up to 50% improvement in\nvariance reduction over the existing state-of-the-art MFIS estimator, which had\nalready shown outstanding convergence rate compared to the Monte Carlo method,\non several problems of computational mechanics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 22:05:23 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Pham", "Trung", ""], ["Gorodetsky", "Alex A.", ""]]}, {"id": "2101.02800", "submitter": "Kelly Ramsay", "authors": "Kelly Ramsay and Shoja'eddin Chenouri", "title": "Differentially private depth functions and their associated medians", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the differentially private estimation of data\ndepth functions and their associated medians. We introduce several methods for\nprivatizing depth values at a fixed point, and show that for some depth\nfunctions, when the depth is computed at an out of sample point, privacy can be\ngained for free when $n\\rightarrow \\infty$. We also present a method for\nprivately estimating the vector of sample point depth values. Additionally, we\nintroduce estimation methods for depth-based medians for both depth functions\nwith low global sensitivity and depth functions with only highly probable, low\nlocal sensitivity. We provide a general result (Lemma 1) which can be used to\nprove consistency of an estimator produced by the exponential mechanism,\nprovided the limiting cost function is sufficiently smooth at a unique\nminimizer. We also introduce a general algorithm to privately estimate a\nminimizer of a cost function which has, with high probability, low local\nsensitivity. This algorithm combines the propose-test-release algorithm with\nthe exponential mechanism. An application of this algorithm to generate\nconsistent estimates of the projection depth-based median is presented. Thus,\nfor these private depth-based medians, we show that it is possible for privacy\nto be obtained for free when $n\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:56:24 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 01:39:45 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 20:18:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ramsay", "Kelly", ""], ["Chenouri", "Shoja'eddin", ""]]}, {"id": "2101.02931", "submitter": "Paris Giampouras", "authors": "Paris V. Giampouras, Athanasios A. Rontogiannis, Eleftherios Kofidis", "title": "Block-Term Tensor Decomposition Model Selection and Computation: The\n  Bayesian Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The so-called block-term decomposition (BTD) tensor model, especially in its\nrank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention\ndue to its enhanced ability of representing systems and signals that are\ncomposed of \\emph{blocks} of rank higher than one, a scenario encountered in\nnumerous and diverse applications. Uniqueness conditions and fitting methods\nhave thus been thoroughly studied. Nevertheless, the challenging problem of\nestimating the BTD model structure, namely the number of block terms, $R$, and\ntheir individual ranks, $L_r$, has only recently started to attract significant\nattention, mainly through regularization-based approaches which entail the need\nto tune the regularization parameter(s). In this work, we build on ideas of\nsparse Bayesian learning (SBL) and put forward a fully automated Bayesian\napproach. Through a suitably crafted multi-level \\emph{hierarchical}\nprobabilistic model, which gives rise to heavy-tailed prior distributions for\nthe BTD factors, structured sparsity is \\emph{jointly} imposed. Ranks are then\nestimated from the numbers of blocks ($R$) and columns ($L_r$) of\nnon-negligible energy. Approximate posterior inference is implemented, within\nthe variational inference framework. The resulting iterative algorithm\ncompletely avoids hyperparameter tuning, which is a significant defect of\nregularization-based methods. Alternative probabilistic models are also\nexplored and the connections with their regularization-based counterparts are\nbrought to light with the aid of the associated maximum a-posteriori (MAP)\nestimators. We report simulation results with both synthetic and real-word\ndata, which demonstrate the merits of the proposed method in terms of both rank\nestimation and model fitting as compared to state-of-the-art relevant methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 09:37:21 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 13:16:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Giampouras", "Paris V.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Kofidis", "Eleftherios", ""]]}, {"id": "2101.03079", "submitter": "Jacob Vorstrup Goldman", "authors": "Jacob Vorstrup Goldman and Sumeetpal Sidhu Singh", "title": "Spatiotemporal blocking of the bouncy particle sampler for efficient\n  inference in state space models", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel blocked version of the continuous-time bouncy particle\nsampler of [Bouchard-C\\^ot\\'e et al., 2018] which is applicable to any\ndifferentiable probability density. This alternative implementation is\nmotivated by blocked Gibbs sampling for state space models [Singh et al., 2017]\nand leads to significant improvement in terms of effective sample size per\nsecond, and furthermore, allows for significant parallelization of the\nresulting algorithm. The new algorithms are particularly efficient for latent\nstate inference in high-dimensional state space models, where blocking in both\nspace and time is necessary to avoid degeneracy of MCMC. The efficiency of our\nblocked bouncy particle sampler, in comparison with both the standard\nimplementation of the bouncy particle sampler and the particle Gibbs algorithm\nof Andrieu et al. [2010], is illustrated numerically for both simulated data\nand a challenging real-world financial dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:14:23 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 07:48:04 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Goldman", "Jacob Vorstrup", ""], ["Singh", "Sumeetpal Sidhu", ""]]}, {"id": "2101.03108", "submitter": "David Ginsbourger", "authors": "David Ginsbourger and Cedric Sch\\\"arer", "title": "Fast calculation of Gaussian Process multiple-fold cross-validation\n  residuals and their covariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize fast Gaussian process leave-one-out formulae to multiple-fold\ncross-validation, highlighting in turn in broad settings the covariance\nstructure of cross-validation residuals. The employed approach, that relies on\nblock matrix inversion via Schur complements, is applied to both Simple and\nUniversal Kriging frameworks. We illustrate how resulting covariances affect\nmodel diagnostics and how to properly transform residuals in the first place.\nBeyond that, we examine how accounting for dependency between such residuals\naffect cross-validation-based estimation of the scale parameter. It is found in\ntwo distinct cases, namely in scale estimation and in broader covariance\nparameter estimation via pseudo-likelihood, that correcting for covariances\nbetween cross-validation residuals leads back to maximum likelihood estimation\nor to an original variation thereof. The proposed fast calculation of Gaussian\nProcess multiple-fold cross-validation residuals is implemented and benchmarked\nagainst a naive implementation, all in R language. Numerical experiments\nhighlight the accuracy of our approach as well as the substantial speed-ups\nthat it enables. It is noticeable however, as supported by a discussion on the\nmain drivers of computational costs and by a dedicated numerical benchmark,\nthat speed-ups steeply decline as the number of folds (say, all sharing the\nsame size) decreases. Overall, our results enable fast multiple-fold\ncross-validation, have direct consequences in GP model diagnostics, and pave\nthe way to future work on hyperparameter fitting as well as on the promising\nfield of goal-oriented fold design.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 17:02:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ginsbourger", "David", ""], ["Sch\u00e4rer", "Cedric", ""]]}, {"id": "2101.03170", "submitter": "Dai Feng", "authors": "Dai Feng and Lili Zhao", "title": "BDNNSurv: Bayesian deep neural networks for survival analysis using\n  pseudo values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There has been increasing interest in modeling survival data using deep\nlearning methods in medical research. In this paper, we proposed a Bayesian\nhierarchical deep neural networks model for modeling and prediction of survival\ndata. Compared with previously studied methods, the new proposal can provide\nnot only point estimate of survival probability but also quantification of the\ncorresponding uncertainty, which can be of crucial importance in predictive\nmodeling and subsequent decision making. The favorable statistical properties\nof point and uncertainty estimates were demonstrated by simulation studies and\nreal data analysis. The Python code implementing the proposed approach was\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:18:43 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Feng", "Dai", ""], ["Zhao", "Lili", ""]]}, {"id": "2101.03268", "submitter": "Evan Sidrow", "authors": "Evan Sidrow, Nancy Heckman, Sarah M.E. Fortune, Andrew W. Trites, Ian\n  Murphy, Marie Auger-M\\'eth\\'e", "title": "Modelling multi-scale state-switching functional data with hidden Markov\n  models", "comments": "23 pages, 8 figures, 2 tables. Supplementary material appended to\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data sets comprised of sequences of curves sampled at high frequencies in\ntime are increasingly common in practice, but they can exhibit complicated\ndependence structures that cannot be modelled using common methods of\nFunctional Data Analysis (FDA). We detail a hierarchical approach which treats\nthe curves as observations from a hidden Markov model (HMM). The distribution\nof each curve is then defined by another fine-scale model which may involve\nauto-regression and require data transformations using moving-window summary\nstatistics or Fourier analysis. This approach is broadly applicable to\nsequences of curves exhibiting intricate dependence structures. As a case\nstudy, we use this framework to model the fine-scale kinematic movement of a\nnorthern resident killer whale (Orcinus orca) off the coast of British\nColumbia, Canada. Through simulations, we show that our model produces more\ninterpretable state estimation and more accurate parameter estimates compared\nto existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 01:23:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sidrow", "Evan", ""], ["Heckman", "Nancy", ""], ["Fortune", "Sarah M. E.", ""], ["Trites", "Andrew W.", ""], ["Murphy", "Ian", ""], ["Auger-M\u00e9th\u00e9", "Marie", ""]]}, {"id": "2101.03408", "submitter": "Anna Yanchenko", "authors": "Anna K. Yanchenko, Di Daniel Deng, Jinglan Li, Andrew J. Cron and Mike\n  West", "title": "Hierarchical Dynamic Modeling for Individualized Bayesian Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study and methodological developments in large-scale\nhierarchical dynamic modeling for personalized prediction in commerce. The\ncontext is supermarket sales, where improved forecasting of\ncustomer/household-specific purchasing behavior informs decisions about\npersonalized pricing and promotions on a continuing basis. This is a big data,\nbig modeling and forecasting setting involving many thousands of customers and\nitems on sale, requiring sequential analysis, addressing information flows at\nmultiple levels over time, and with heterogeneity of customer profiles and item\ncategories. Models developed are fully Bayesian, interpretable and multi-scale,\nwith hierarchical forms overlaid on the inherent structure of the retail\nsetting. Customer behavior is modeled at several levels of aggregation, and\ninformation flows from aggregate to individual levels. Forecasting at an\nindividual household level infers price sensitivity to inform personalized\npricing and promotion decisions. Methodological innovations include extensions\nof Bayesian dynamic mixture models, their integration into multi-scale systems,\nand forecast evaluation with context-specific metrics. The use of simultaneous\npredictors from multiple hierarchical levels improves forecasts at the\ncustomer-item level of main interest. This is evidenced across many different\nhouseholds and items, indicating the utility of the modeling framework for this\nand other individualized forecasting applications.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 18:58:20 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yanchenko", "Anna K.", ""], ["Deng", "Di Daniel", ""], ["Li", "Jinglan", ""], ["Cron", "Andrew J.", ""], ["West", "Mike", ""]]}, {"id": "2101.03463", "submitter": "Xialing Wen", "authors": "Xialing Wen, Ying Yan, Wenliang Pan, Xianyang Zhang", "title": "Kernel-Distance-Based Covariate Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common concern in observational studies focuses on properly evaluating the\ncausal effect, which usually refers to the average treatment effect or the\naverage treatment effect on the treated. In this paper, we propose a data\npreprocessing method, the Kernel-distance-based covariate balancing, for\nobservational studies with binary treatments. This proposed method yields a set\nof unit weights for the treatment and control groups, respectively, such that\nthe reweighted covariate distributions can satisfy a set of pre-specified\nbalance conditions. This preprocessing methodology can effectively reduce\nconfounding bias of subsequent estimation of causal effects. We demonstrate the\nimplementation and performance of Kernel-distance-based covariate balancing\nwith Monte Carlo simulation experiments and a real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 03:15:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wen", "Xialing", ""], ["Yan", "Ying", ""], ["Pan", "Wenliang", ""], ["Zhang", "Xianyang", ""]]}, {"id": "2101.03579", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley", "title": "Grid-Parametrize-Split (GriPS) for Improved Scalable Inference in\n  Spatial Big Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid advancements in spatial technologies including Geographic Information\nSystems (GIS) and remote sensing have generated massive amounts of spatially\nreferenced data in a variety of scientific and data-driven industrial\napplications. These advancements have led to a substantial, and still\nexpanding, literature on the modeling and analysis of spatially oriented big\ndata. In particular, Bayesian inferences for high-dimensional spatial processes\nare being sought in a variety of remote-sensing applications including, but not\nlimited to, modeling next generation Light Detection and Ranging (LiDAR)\nsystems and other remotely sensed data. Massively scalable spatial processes,\nin particular Gaussian processes (GPs), are being explored extensively for the\nincreasingly encountered big data settings. Recent developments include GPs\nconstructed from sparse Directed Acyclic Graphs (DAGs) with a limited number of\nneighbors (parents) to characterize dependence across the spatial domain. The\nDAG can be used to devise fast algorithms for posterior sampling of the latent\nprocess, but these may exhibit pathological behavior in estimating covariance\nparameters. While these issues are mitigated by considering marginalized\nsamplers that exploit the underlying sparse precision matrix, these algorithms\nare slower, less flexible, and oblivious of structure in the data. The current\narticle introduces the Grid-Parametrize-Split (GriPS) approach for conducting\nBayesian inference in spatially oriented big data settings by a combination of\ncareful model construction and algorithm design to effectuate substantial\nimprovements in MCMC convergence. We demonstrate the effectiveness of our\nproposed methods through simulation experiments and subsequently undertake the\nmodeling of LiDAR outcomes and production of their predictive maps using G-LiHT\nand other remotely sensed variables.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 16:51:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Peruzzi", "Michele", ""], ["Banerjee", "Sudipto", ""], ["Dunson", "David B.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2101.03622", "submitter": "F\\'abio Silveira", "authors": "F\\'abio V. J. Silveira, Frank Gomes-Silva, C\\'icero C. R. Brito, Jader\n  S. Jale, Felipe R. S. Gusm\\~ao, S\\'ilvio F. A. Xavier-J\\'unior, Jo\\~ao S.\n  Rocha", "title": "Modelling wind speed with a univariate probability distribution\n  depending on two baseline functions", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Characterizing the wind speed distribution properly is essential for the\nsatisfactory production of potential energy in wind farms, being the mixture\nmodels usually employed in the description of such data. However, some mixture\nmodels commonly have the undesirable property of non-identifiability. In this\nwork, we present an alternative distribution which is able to fit the wind\nspeed data adequately. The new model, called Normal-Weibull-Weibull, is\nidentifiable and its cumulative distribution function is written as a\ncomposition of two baseline functions. We discuss structural properties of the\nclass that generates the proposed model, such as the linear representation of\nthe probability density function, moments and moment generating function. We\nperform a Monte Carlo simulation study to investigate the behavior of the\nmaximum likelihood estimates of the parameters. Finally, we present\napplications of the new distribution for modelling wind speed data measured in\nfive different cities of the Northeastern Region of Brazil.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 20:19:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Silveira", "F\u00e1bio V. J.", ""], ["Gomes-Silva", "Frank", ""], ["Brito", "C\u00edcero C. R.", ""], ["Jale", "Jader S.", ""], ["Gusm\u00e3o", "Felipe R. S.", ""], ["Xavier-J\u00fanior", "S\u00edlvio F. A.", ""], ["Rocha", "Jo\u00e3o S.", ""]]}, {"id": "2101.03684", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Mami Kajita, Seiji Kajita, Tomoko Matsui", "title": "Compositionally-warped additive mixed modeling for a wide variety of\n  non-Gaussian spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As with the advancement of geographical information systems, non-Gaussian\nspatial data sets are getting larger and more diverse. This study develops a\ngeneral framework for fast and flexible non-Gaussian regression, especially for\nspatial/spatiotemporal modeling. The developed model, termed the\ncompositionally-warped additive mixed model (CAMM), combines an additive mixed\nmodel (AMM) and the compositionally-warped Gaussian process to model a wide\nvariety of non-Gaussian continuous data including spatial and other effects. A\nspecific advantage of the proposed CAMM is that it requires no explicit\nassumption of data distribution unlike existing AMMs. Monte Carlo experiments\nshow the estimation accuracy and computational efficiency of CAMM for modeling\nnon-Gaussian data including fat-tailed and/or skewed distributions. Finally,\nthe model is applied to crime data to examine the empirical performance of the\nregression analysis and prediction. The result shows that CAMM provides\nintuitively reasonable coefficient estimates and outperforms AMM in terms of\nprediction accuracy. CAMM is verified to be a fast and flexible model that\npotentially covers a wide variety of non-Gaussian data modeling. The proposed\napproach is implemented in an R package spmoran.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 03:19:50 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:41:40 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Murakami", "Daisuke", ""], ["Kajita", "Mami", ""], ["Kajita", "Seiji", ""], ["Matsui", "Tomoko", ""]]}, {"id": "2101.03698", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin and Jean-Fran\\c{c}ois Coeurjolly and Fr\\'ed\\'erique\n  Letu\\'e", "title": "Adaptive lasso and Dantzig selector for spatial point processes\n  intensity estimation", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lasso and Dantzig selector are standard procedures able to perform variable\nselection and estimation simultaneously. This paper is concerned with extending\nthese procedures to spatial point process intensity estimation. We propose\nadaptive versions of these procedures, develop efficient computational\nmethodologies and derive asymptotic results for a large class of spatial point\nprocesses under the setting where the number of parameters, i.e. the number of\nspatial covariates considered, increases with the volume of the observation\ndomain. Both procedures are compared theoretically and in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 04:55:55 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 04:07:40 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Letu\u00e9", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "2101.03849", "submitter": "Vivekananda Roy", "authors": "Yalin Rao and Vivekananda Roy", "title": "Block Gibbs samplers for logistic mixed models: convergence properties\n  and a comparison with full Gibbs samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logistic linear mixed model (LLMM) is one of the most widely used statistical\nmodels. Generally, Markov chain Monte Carlo algorithms are used to explore the\nposterior densities associated with Bayesian LLMMs. Polson, Scott and Windle's\n(2013) Polya-Gamma data augmentation (DA) technique can be used to construct\nfull Gibbs (FG) samplers for LLMMs. Here, we develop efficient block Gibbs (BG)\nsamplers for Bayesian LLMMs using the Polya-Gamma DA method. We compare the FG\nand BG samplers in the context of simulated and real data examples as the\ncorrelation between the fixed and random effects changes as well as when the\ndimensions of the design matrices vary. These numerical examples demonstrate\nsuperior performance of the BG samplers over the FG samplers. We also derive\nconditions guaranteeing geometric ergodicity of the BG Markov chain when the\npopular improper uniform prior is assigned on the regression coefficients and\nproper or improper priors are placed on the variance parameters of the random\neffects. This theoretical result has important practical implications as it\njustifies the use of asymptotically valid Monte Carlo standard errors for\nMarkov chain based estimates of posterior quantities.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:38:35 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 15:14:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rao", "Yalin", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2101.03875", "submitter": "Mirrelijn van Nee", "authors": "Mirrelijn M. van Nee, Tim van de Brug, Mark A. van de Wiel", "title": "Fast marginal likelihood estimation of penalties for group-adaptive\n  elastic net", "comments": "16 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, clinical research routinely uses omics data, such as gene\nexpression, for predicting clinical outcomes or selecting markers.\nAdditionally, so-called co-data are often available, providing complementary\ninformation on the covariates, like p-values from previously published studies\nor groups of genes corresponding to pathways. Elastic net penalisation is\nwidely used for prediction and covariate selection. Group-adaptive elastic net\npenalisation learns from co-data to improve the prediction and covariate\nselection, by penalising important groups of covariates less than other groups.\nExisting methods are, however, computationally expensive. Here we present a\nfast method for marginal likelihood estimation of group-adaptive elastic net\npenalties for generalised linear models. We first derive a low-dimensional\nrepresentation of the Taylor approximation of the marginal likelihood and its\nfirst derivative for group-adaptive ridge penalties, to efficiently estimate\nthese penalties. Then we show by using asymptotic normality of the linear\npredictors that the marginal likelihood for elastic net models may be\napproximated well by the marginal likelihood for ridge models. The ridge group\npenalties are then transformed to elastic net group penalties by using the\nvariance function. The method allows for overlapping groups and unpenalised\nvariables. We demonstrate the method in a model-based simulation study and an\napplication to cancer genomics. The method substantially decreases computation\ntime and outperforms or matches other methods by learning from co-data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 13:30:24 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["van Nee", "Mirrelijn M.", ""], ["van de Brug", "Tim", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "2101.03997", "submitter": "Yan Liu", "authors": "Yan Liu, Mireille Schnitzer, Guanbo Wang, Edward Kennedy, Piret\n  Viiklepp, Mario H. Vargas, Giovanni Sotgiu, Dick Menzies and Andrea Benedetti", "title": "Modeling Treatment Effect Modification in Multidrug-Resistant\n  Tuberculosis in an Individual Patient Data Meta-Analysis", "comments": "16 pages, 1 table, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effect modification occurs while the effect of the treatment is not\nhomogeneous across the different strata of patient characteristics. When the\neffect of treatment may vary from individual to individual, precision medicine\ncan be improved by identifying patient covariates to estimate the size and\ndirection of the effect at the individual level. However, this task is\nstatistically challenging and typically requires large amounts of data.\nInvestigators may be interested in using the individual patient data (IPD) from\nmultiple studies to estimate these treatment effect models. Our data arise from\na systematic review of observational studies contrasting different treatments\nfor multidrug-resistant tuberculosis (MDR-TB), where multiple antimicrobial\nagents are taken concurrently to cure the infection. We propose a marginal\nstructural model (MSM) for effect modification by different patient\ncharacteristics and co-medications in a meta-analysis of observational IPD. We\ndevelop, evaluate, and apply a targeted maximum likelihood estimator (TMLE) for\nthe doubly robust estimation of the parameters of the proposed MSM in this\ncontext. In particular, we allow for differential availability of treatments\nacross studies, measured confounding within and across studies, and random\neffects by study.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:12:07 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 00:52:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Yan", ""], ["Schnitzer", "Mireille", ""], ["Wang", "Guanbo", ""], ["Kennedy", "Edward", ""], ["Viiklepp", "Piret", ""], ["Vargas", "Mario H.", ""], ["Sotgiu", "Giovanni", ""], ["Menzies", "Dick", ""], ["Benedetti", "Andrea", ""]]}, {"id": "2101.04011", "submitter": "Sven Knoth", "authors": "Sven Knoth", "title": "Controlling the EWMA S^2 control chart false alarm behavior when the\n  in-control variance level must be estimated", "comments": null, "journal-ref": null, "doi": "10.1002/asmb.2613", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the problem of setting control limits in the case of parameter\nuncertainty is more accessible when monitoring the variance because only one\nparameter has to be estimated. Simply ignoring the induced uncertainty\nfrequently leads to control charts with poor false alarm performances.\nAdjusting the unconditional in-control (IC) average run length (ARL) makes the\nsituation even worse. Guaranteeing a minimum conditional IC ARL with some given\nprobability is another very popular approach to solving these difficulties.\nHowever, it is very conservative as well as more complex and more difficult to\ncommunicate. We utilize the probability of a false alarm within the planned\nnumber of points to be plotted on the control chart. It turns out that\nadjusting this probability produces notably different limit adjustments\ncompared to controlling the unconditional IC ARL. We then develop numerical\nalgorithms to determine the respective modifications of the upper and two-sided\nexponentially weighted moving average (EWMA) charts based on the sample\nvariance for normally distributed data. These algorithms are made available\nwithin an R package. Finally, the impacts of the EWMA smoothing constant and\nthe size of the preliminary sample on the control chart design and its\nperformance are studied.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:36:37 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Knoth", "Sven", ""]]}, {"id": "2101.04038", "submitter": "Sascha Ranftl", "authors": "Sascha Ranftl and Wolfgang von der Linden", "title": "Bayesian Surrogate Analysis and Uncertainty Propagation with Explicit\n  Surrogate Uncertainties and Implicit Spatio-temporal Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Bayesian Probability Theory to investigate uncertainty\npropagation based on meta-models. We approach the problem from the perspective\nof data analysis, with a given (however almost-arbitrary) input probability\ndistribution and a given \"training\" set of computer simulations. While proven\nmathematically to be the unique consistent probability calculus, the subject of\nthis paper is not to demonstrate beauty but usefulness. We explicitly list all\npropositions and lay open the general structure of any uncertainty propagation\nbased on meta-models. The former allows rigorous treatment at any stage, while\nthe latter allows us to quantify the interaction of the surrogate uncertainties\nwith the usual parameter uncertainties. Additionally, we show a simple way to\nimplicitly include spatio-temporal correlations. We then apply the framework\njointly to a family of generalized linear meta-model that implicitly includes\nPolynomial Chaos Expansions as a special case. While we assume a Gaussian\nsurrogate-uncertainty, we do not assume a scale for the surrogate uncertainty\nto be known, i.e. a Student-t. We end up with semi-analytic formulas for\nsurrogate uncertainties and uncertainty propagation\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:15:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ranftl", "Sascha", ""], ["von der Linden", "Wolfgang", ""]]}, {"id": "2101.04040", "submitter": "Vladim\\'ir Hol\\'y", "authors": "Vladim\\'ir Hol\\'y and Jan Zouhar", "title": "Modelling Time-Varying Rankings with Autoregressive and Score-Driven\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new statistical model to analyse time-varying ranking data. The\nmodel can be used with a large number of ranked items, accommodates exogenous\ntime-varying covariates and partial rankings, and is estimated via maximum\nlikelihood in a straightforward manner. Rankings are modelled using the\nPlackett-Luce distribution with time-varying worth parameters that follow a\nmean-reverting time series process. To capture the dependence of the worth\nparameters on past rankings, we utilize the conditional score in the fashion of\nthe generalized autoregressive score (GAS) models. Simulation experiments show\nthat small-sample properties of the maximum-likelihood estimator improve\nrapidly with the length of the time series and suggest that statistical\ninference relying on conventional Hessian-based standard errors is usable even\nfor medium-sized samples. As an illustration, we apply the model to the results\nof the Ice Hockey World Championships. We also discuss applications to rankings\nbased on underlying indices, repeated surveys, and non-parametric efficiency\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:23:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hol\u00fd", "Vladim\u00edr", ""], ["Zouhar", "Jan", ""]]}, {"id": "2101.04235", "submitter": "Philip White", "authors": "Xavier Emery, Emilio Porcu, Philip White", "title": "Flexible Validity Conditions for the Multivariate Mat\\'ern Covariance in\n  any Spatial Dimension and for any Number of Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible multivariate covariance models for spatial data are on demand. This\npaper addresses the problem of parametric constraints for positive\nsemidefiniteness of the multivariate Mat{\\'e}rn model. Much attention has been\ngiven to the bivariate case, while highly multivariate cases have been explored\nto a limited extent only. The existing conditions often imply severe\nrestrictions on the upper bounds for the collocated correlation coefficients,\nwhich makes the multivariate Mat{\\'e}rn model appealing for the case of weak\nspatial cross-dependence only. We provide a collection of validity conditions\nfor the multivariate Mat{\\'e}rn covariance model that allows for more flexible\nparameterizations than those currently available. We also prove that, in\nseveral cases, we can attain much higher upper bounds for the collocated\ncorrelation coefficients in comparison with our competitors. We conclude with a\nsimple illustration on a trivariate geochemical dataset and show that our\nenlarged parametric space allows for better fitting performance with respect to\nour competitors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:24:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Emery", "Xavier", ""], ["Porcu", "Emilio", ""], ["White", "Philip", ""]]}, {"id": "2101.04253", "submitter": "Sandro Sperandei", "authors": "Sandro Sperandei, Leonardo S. Bastos, Marcelo Ribeiro-Alves, Arianne\n  Reis, Francisco I. Bastos", "title": "Evaluation of Logistic Regression Applied to Respondent-Driven Samples:\n  Simulated and Real Data", "comments": "24 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To investigate the impact of different logistic regression\nestimators applied to RDS samples obtained by simulation and real data.\nMethods: Four simulated populations were created combining different\nconnectivity models, levels of clusterization and infection processes. Each\nsubject in the population received two attributes, only one of them related to\nthe infection process. From each population, RDS samples with different sizes\nwere obtained. Similarly, RDS samples were obtained from a real-world dataset.\nThree logistic regression estimators were applied to assess the association\nbetween the attributes and the infection status, and subsequently the observed\ncoverage of each was measured. Results: The type of connectivity had more\nimpact on estimators performance than the clusterization level. In simulated\ndatasets, unweighted logistic regression estimators emerged as the best option,\nalthough all estimators showed a fairly good performance. In the real dataset,\nthe performance of weighted estimators presented some instabilities, making\nthem a risky option. Conclusion: An unweighted logistic regression estimator is\na reliable option to be applied to RDS samples, with similar performance to\nrandom samples and, therefore, should be the preferred option.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:27:33 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Sperandei", "Sandro", ""], ["Bastos", "Leonardo S.", ""], ["Ribeiro-Alves", "Marcelo", ""], ["Reis", "Arianne", ""], ["Bastos", "Francisco I.", ""]]}, {"id": "2101.04263", "submitter": "Shengchun Kong", "authors": "Shengchun Kong, Dominik Heinzmann, Sabine Lauer, Tian Lu", "title": "Weighted Approach for Estimating Effects in Principal Strata with\n  Missing Data for a Categorical Post-Baseline Variable in Randomized\n  Controlled Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research was motivated by studying anti-drug antibody (ADA) formation\nand its potential impact on long-term benefit of a biologic treatment in a\nrandomized controlled trial, in which ADA status was not only unobserved in the\ncontrol arm but also in a subset of patients from the experimental treatment\narm. Recent literature considers the principal stratum estimand strategy to\nestimate treatment effect in groups of patients defined by an intercurrent\nstatus, i.e. in groups defined by a post-randomization variable only observed\nin one arm and potentially associated with the outcome. However, status\ninformation might be missing even for a non-negligible number of patients in\nthe experimental arm. For this setting, a novel weighted principal stratum\napproach is presented: Data from patients with missing intercurrent event\nstatus were re-weighted based on baseline covariates and additional\nlongitudinal information. A theoretical justification of the proposed approach\nis provided for different types of outcomes, and assumptions allowing for\ncausal conclusions on treatment effect are specified and investigated.\nSimulations demonstrated that the proposed method yielded valid inference and\nwas robust against certain violations of assumptions. The method was shown to\nperform well in a clinical study with ADA status as an intercurrent event.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:24:37 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kong", "Shengchun", ""], ["Heinzmann", "Dominik", ""], ["Lauer", "Sabine", ""], ["Lu", "Tian", ""]]}, {"id": "2101.04276", "submitter": "Yao Zheng", "authors": "Di Wang, Yao Zheng and Guodong Li", "title": "High-Dimensional Low-Rank Tensor Autoregressive Time Series Modeling", "comments": "61 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern technological advances have enabled an unprecedented amount of\nstructured data with complex temporal dependence, urging the need for new\nmethods to efficiently model and forecast high-dimensional tensor-valued time\nseries. This paper provides the first practical tool to accomplish this task\nvia autoregression (AR). By considering a low-rank Tucker decomposition for the\ntransition tensor, the proposed tensor autoregression can flexibly capture the\nunderlying low-dimensional tensor dynamics, providing both substantial\ndimension reduction and meaningful dynamic factor interpretation. For this\nmodel, we introduce both low-dimensional rank-constrained estimator and\nhigh-dimensional regularized estimators, and derive their asymptotic and\nnon-asymptotic properties. In particular, by leveraging the special balanced\nstructure of the AR transition tensor, a novel convex regularization approach,\nbased on the sum of nuclear norms of square matricizations, is proposed to\nefficiently encourage low-rankness of the coefficient tensor. A truncation\nmethod is further introduced to consistently select the Tucker ranks.\nSimulation experiments and real data analysis demonstrate the advantages of the\nproposed approach over various competing ones.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:15:51 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Di", ""], ["Zheng", "Yao", ""], ["Li", "Guodong", ""]]}, {"id": "2101.04288", "submitter": "Daniel Andr\\'es D\\'iaz-Pach\\'on", "authors": "Tianhao Liu, Daniel Andr\\'es D\\'iaz-Pach\\'on, J. Sunil Rao, Jean-Eudes\n  Dazard", "title": "Mode Hunting Using Pettiest Components Analysis", "comments": "10 pages, 2 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Principal component analysis has been used to reduce dimensionality of\ndatasets for a long time. In this paper, we will demonstrate that in mode\ndetection the components of smallest variance, the pettiest components, are\nmore important. We prove that when the data follows a multivariate normal\ndistribution, by implementing \"pettiest component analysis\" when the data is\nnormally distributed, we obtain boxes of optimal size in the sense that their\nsize is minimal over all possible boxes with the same number of dimensions and\ngiven probability. We illustrate our result with a simulation revealing that\npettiest component analysis works better than its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:17:26 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Tianhao", ""], ["D\u00edaz-Pach\u00f3n", "Daniel Andr\u00e9s", ""], ["Rao", "J. Sunil", ""], ["Dazard", "Jean-Eudes", ""]]}, {"id": "2101.04351", "submitter": "Jaeyong Lee", "authors": "Kyoungjae Lee, Seongil Jo, Jaeyong Lee", "title": "The Beta-Mixture Shrinkage Prior for Sparse Covariances with Posterior\n  Minimax Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical inference for sparse covariance matrices is crucial to reveal\ndependence structure of large multivariate data sets, but lacks scalable and\ntheoretically supported Bayesian methods. In this paper, we propose\nbeta-mixture shrinkage prior, computationally more efficient than the spike and\nslab prior, for sparse covariance matrices and establish its minimax optimality\nin high-dimensional settings. The proposed prior consists of beta-mixture\nshrinkage and gamma priors for off-diagonal and diagonal entries, respectively.\nTo ensure positive definiteness of the resulting covariance matrix, we further\nrestrict the support of the prior to a subspace of positive definite matrices.\nWe obtain the posterior convergence rate of the induced posterior under the\nFrobenius norm and establish a minimax lower bound for sparse covariance\nmatrices. The class of sparse covariance matrices for the minimax lower bound\nconsidered in this paper is controlled by the number of nonzero off-diagonal\nelements and has more intuitive appeal than those appeared in the literature.\nThe obtained posterior convergence rate coincides with the minimax lower bound\nunless the true covariance matrix is extremely sparse. In the simulation study,\nwe show that the proposed method is computationally more efficient than\ncompetitors, while achieving comparable performance. Advantages of the\nshrinkage prior are demonstrated based on two real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:40:59 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Jo", "Seongil", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2101.04390", "submitter": "Setareh Ranjbar", "authors": "Setareh Ranjbar and Elvezio Ronchetti and Stefan Sperlich", "title": "New Bias Calibration for Robust Estimation in Small Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using sample surveys as a cost effective tool to provide estimates for\ncharacteristics of interest at population and sub-populations (area/domain)\nlevel has a long tradition in \"small area estimation\". However, the existence\nof outliers in the sample data can significantly affect the estimation for\nareas in which they occur, especially where the domain-sample size is small.\nBased on existing robust estimators for small area estimation we propose two\nnovel approaches for bias calibration. A series of simulations shows that our\nmethods lead to more efficient estimators in comparison with other existing\nbias-calibration methods. As a real data example we apply our estimators to\nobtain \\textit{Gini} coefficients in labour market areas of the Tuscany region\nof Italy, where our sources of information are the EU-SILC survey and the\nItalian census. This analysis shows that the new methods reveal a different\npicture than existing methods. We extend our ideas to predictions for\nnon-sampled areas.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:31:33 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ranjbar", "Setareh", ""], ["Ronchetti", "Elvezio", ""], ["Sperlich", "Stefan", ""]]}, {"id": "2101.04408", "submitter": "Daniel Baker", "authors": "Daniel H. Baker", "title": "Statistical analysis of periodic data in neuroscience", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many experimental paradigms in neuroscience involve driving the nervous\nsystem with periodic sensory stimuli. Neural signals recorded using a variety\nof techniques will then include phase-locked oscillations at the stimulation\nfrequency. The analysis of such data often involves standard univariate\nstatistics such as T-tests, conducted on the Fourier amplitude components\n(ignoring phase), either to test for the presence of a signal, or to compare\nsignals across different conditions. However, the assumptions of these tests\nwill sometimes be violated because amplitudes are not normally distributed, and\nfurthermore weak signals might be missed if the phase information is discarded.\nAn alternative approach is to conduct multivariate statistical tests using the\nreal and imaginary Fourier components. Here the performance of two multivariate\nextensions of the T-test are compared: Hotelling's $T^2$ and a variant called\n$T^2_{circ}$. A novel test of the assumptions of $T^2_{circ}$ is developed,\nbased on the condition index of the data (the square root of the ratio of\neigenvalues of a bounding ellipse), and a heuristic for excluding outliers\nusing the Mahalanobis distance is proposed. The $T^2_{circ}$ statistic is then\nextended to multi-level designs, resulting in a new statistical test termed\n$ANOVA^2_{circ}$. This has identical assumptions to $T^2_{circ}$, and is shown\nto be more sensitive than MANOVA when these assumptions are met. The use of\nthese tests is demonstrated for two publicly available empirical data sets, and\npractical guidance is suggested for choosing which test to run. Implementations\nof these novel tools are provided as an R package and a Matlab toolbox, in the\nhope that their wider adoption will improve the sensitivity of statistical\ninferences involving periodic data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:10:17 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 10:22:09 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 06:00:54 GMT"}, {"version": "v4", "created": "Wed, 7 Jul 2021 13:27:32 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Baker", "Daniel H.", ""]]}, {"id": "2101.04426", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Pietro Spitali, Cristina Al-Khalili Szigyarto, The\n  MARK-MD Consortium, Roula Tsonaka", "title": "Penalized regression calibration: a method for the prediction of\n  survival outcomes using complex longitudinal and high-dimensional data", "comments": "Minor changes from version 1 (typos in legends of figures,\n  acknowledgements, etc.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Longitudinal and high-dimensional measurements have become increasingly\ncommon in biomedical research. However, methods to predict survival outcomes\nusing covariates that are both longitudinal and high-dimensional are currently\nmissing. In this article we propose penalized regression calibration (PRC), a\nmethod that can be employed to predict survival in such situations.\n  PRC comprises three modelling steps: first, the trajectories described by the\nlongitudinal predictors are flexibly modelled through the specification of\nmultivariate latent process mixed models. Second, subject-specific summaries of\nthe longitudinal trajectories are derived from the fitted mixed effects models.\nThird, the time to event outcome is predicted using the subject-specific\nsummaries as covariates in a penalized Cox model.\n  To ensure a proper internal validation of the fitted PRC models, we\nfurthermore develop a cluster bootstrap optimism correction procedure (CBOCP)\nthat allows to correct for the optimistic bias of apparent measures of\npredictiveness.\n  After studying the behaviour of PRC via simulations, we conclude by\nillustrating an application of PRC to data from an observational study that\ninvolved patients affected by Duchenne muscular dystrophy (DMD), where the goal\nis predict time to loss of ambulation using longitudinal blood biomarkers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:56:18 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:49:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Signorelli", "Mirko", ""], ["Spitali", "Pietro", ""], ["Szigyarto", "Cristina Al-Khalili", ""], ["Consortium", "The MARK-MD", ""], ["Tsonaka", "Roula", ""]]}, {"id": "2101.04437", "submitter": "Dootika Vats", "authors": "Kushagra Gupta, Dootika Vats, Snigdhansu Chatterjee", "title": "Bayesian equation selection on sparse data for discovery of stochastic\n  dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the underlying system of differential equations driving a stochastic\ndynamical system is assumed to be known, with inference conditioned on this\nassumption. We present a Bayesian framework for discovering this system of\ndifferential equations under assumptions that align with real-life scenarios,\nincluding the availability of relatively sparse data. Further, we discuss\ncomputational strategies that are critical in teasing out the important details\nabout the dynamical system and algorithmic innovations to solve for acute\nparameter interdependence in the absence of rich data. This gives a complete\nBayesian pathway for model identification via a variable selection paradigm and\nparameter estimation of the corresponding model using only the observed data.\nWe present detailed computations and analysis of the Lorenz-96, Lorenz-63, and\nthe Orstein-Uhlenbeck system using the Bayesian framework we propose.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:21:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gupta", "Kushagra", ""], ["Vats", "Dootika", ""], ["Chatterjee", "Snigdhansu", ""]]}, {"id": "2101.04651", "submitter": "Philipp Klein", "authors": "Claudia Kirch, Philipp Klein", "title": "Moving sum data segmentation for stochastics processes based on\n  invariance", "comments": "34 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of data into stationary stretches also known as multiple\nchange point problem is important for many applications in time series analysis\nas well as signal processing. Based on strong invariance principles, we analyse\ndata segmentation methodology using moving sum (MOSUM) statistics for a class\nof regime-switching multivariate processes where each switch results in a\nchange in the drift. In particular, this framework includes the data\nsegmentation of multivariate partial sum, integrated diffusion and renewal\nprocesses even if the distance between change points is sublinear. We study the\nasymptotic behaviour of the corresponding change point estimators, show\nconsistency and derive the corresponding localisation rates which are minimax\noptimal in a variety of situations including an unbounded number of changes in\nWiener processes with drift. Furthermore, we derive the limit distribution of\nthe change point estimators for local changes - a result that can in principle\nbe used to derive confidence intervals for the change points.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:20:42 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kirch", "Claudia", ""], ["Klein", "Philipp", ""]]}, {"id": "2101.04748", "submitter": "Fabian K\\\"achele", "authors": "Oliver Grothe, Fabian K\\\"achele, Friedrich Schmid", "title": "A multivariate extension of the Lorenz curve based on copulas and a\n  related multivariate Gini coefficient", "comments": "17 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the univariate Lorenz curve and of the Gini\ncoefficient to the multivariate case, i.e., to simultaneously measure\ninequality in more than one variable. Our extensions are based on copulas and\nmeasure inequality stemming from inequality in every single variable as well as\ninequality stemming from the dependence structure of the variables. We derive\nsimple nonparametric estimators for both instruments and apply them exemplary\nto data of individual income and wealth for various countries.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:50:50 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Grothe", "Oliver", ""], ["K\u00e4chele", "Fabian", ""], ["Schmid", "Friedrich", ""]]}, {"id": "2101.04783", "submitter": "Hailin Sang", "authors": "Janet Nakarmi, Hailin Sang and Lin Ge", "title": "Variable bandwidth kernel regression estimation", "comments": "accepted by ESAIM: PS. 36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose a variable bandwidth kernel regression estimator for\n$i.i.d.$ observations in $\\mathbb{R}^2$ to improve the classical\nNadaraya-Watson estimator. The bias is improved to the order of $O(h_n^4)$\nunder the condition that the fifth order derivative of the density function and\nthe sixth order derivative of the regression function are bounded and\ncontinuous. We also establish the central limit theorems for the proposed ideal\nand true variable kernel regression estimators. The simulation study confirms\nour results and demonstrates the advantage of the variable bandwidth kernel\nmethod over the classical kernel method.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:34:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Nakarmi", "Janet", ""], ["Sang", "Hailin", ""], ["Ge", "Lin", ""]]}, {"id": "2101.04805", "submitter": "Albert Vexler", "authors": "Ablert Vexler, Gregory Gurevich and Li Zou", "title": "Exact Multivariate Two-Sample Density-Based Empirical Likelihood Ratio\n  Tests Applicable to Retrospective and Group Sequential Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests for equality of multivariate distributions are frequently\ndesired in research. It is commonly required that test-procedures based on\nrelatively small samples of vectors accurately control the corresponding Type I\nError (TIE) rates. Often, in the multivariate testing, extensions of\nnull-distribution-free univariate methods, e.g., Kolmogorov-Smirnov and\nCramer-von Mises type schemes, are not exact, since their null distributions\ndepend on underlying data distributions. The present paper extends the\ndensity-based empirical likelihood technique in order to nonparametrically\napproximate the most powerful test for the multivariate two-sample (MTS)\nproblem, yielding an exact finite-sample test statistic. We rigorously\nestablish and apply one-to-one-mapping between the equality of vectors\ndistributions and the equality of distributions of relevant univariate linear\nprojections. In this framework, we prove an algorithm that simplifies the use\nof projection pursuit, employing only a few of the infinitely many linear\ncombinations of observed vectors components. The displayed distribution-free\nstrategy is employed in retrospective and group sequential manners. The\nasymptotic consistency of the proposed technique is shown. Monte Carlo studies\ndemonstrate that the proposed procedures exhibit extremely high and stable\npower characteristics across a variety of settings. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:54:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Vexler", "Ablert", ""], ["Gurevich", "Gregory", ""], ["Zou", "Li", ""]]}, {"id": "2101.04809", "submitter": "Yuxuan Zhao", "authors": "Yuxuan Zhao, David S. Matteson, Mary Beth Nebel, Stewart H. Mostofsky\n  and Benjamin Risk", "title": "Group Linear non-Gaussian Component Analysis with Applications to\n  Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is an unsupervised learning method\npopular in functional magnetic resonance imaging (fMRI). Group ICA has been\nused to search for biomarkers in neurological disorders including autism\nspectrum disorder and dementia. However, current methods use a principal\ncomponent analysis (PCA) step that may remove low-variance features. Linear\nnon-Gaussian component analysis (LNGCA) enables simultaneous dimension\nreduction and feature estimation including low-variance features in\nsingle-subject fMRI. We present a group LNGCA model to extract group components\nshared by more than one subject and subject-specific components. To determine\nthe total number of components in each subject, we propose a parametric\nresampling test that samples spatially correlated Gaussian noise to match the\nspatial dependence observed in data. In simulations, our estimated group\ncomponents achieve higher accuracy compared to group ICA. We apply our method\nto a resting-state fMRI study on autism spectrum disorder in 342 children (252\ntypically developing, 90 with autism), where the group signals include\nresting-state networks. We find examples of group components that appear to\nexhibit different levels of temporal engagement in autism versus typically\ndeveloping children, as revealed using group LNGCA. This novel approach to\nmatrix decomposition is a promising direction for feature detection in\nneuroimaging.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:05:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhao", "Yuxuan", ""], ["Matteson", "David S.", ""], ["Nebel", "Mary Beth", ""], ["Mostofsky", "Stewart H.", ""], ["Risk", "Benjamin", ""]]}, {"id": "2101.04941", "submitter": "Lars Andersen N.", "authors": "Asger Hobolth (1), Mogens Bladt (2), Lars N{\\o}rvang Andersen (1) ((1)\n  Aarhus University, (2) University of Copenhagen)", "title": "Multivariate phase-type theory for the site frequency spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Linear functions of the site frequency spectrum (SFS) play a major role for\nunderstanding and investigating genetic diversity. Estimators of the mutation\nrate (e.g. based on the total number of segregating sites or average of the\npairwise differences) and tests for neutrality (e.g. Tajima's D) are perhaps\nthe most well-known examples. The distribution of linear functions of the SFS\nis important for constructing confidence intervals for the estimators, and to\ndetermine significance thresholds for neutrality tests. These distributions are\noften approximated using simulation procedures. In this paper we use\nmultivariate phase-type theory to specify, characterize and calculate the\ndistribution of linear functions of the site frequency spectrum. In particular,\nwe show that many of the classical estimators of the mutation rate are\ndistributed according to a discrete phase-type distribution. Neutrality tests,\nhowever, are generally not discrete phase-type distributed. For neutrality\ntests we derive the probability generating function using continuous\nmultivariate phase-type theory, and numerically invert the function to obtain\nthe distribution. A main result is an analytically tractable formula for the\nprobability generating function of the SFS. Software implementation of the\nphase-type methodology is available in the R package phasty, and R code for the\nreproduction of our results is available as an accompanying vignette.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:06:42 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hobolth", "Asger", ""], ["Bladt", "Mogens", ""], ["Andersen", "Lars N\u00f8rvang", ""]]}, {"id": "2101.05026", "submitter": "Satarupa Bhattacharjee", "authors": "Satarupa Bhattacharjee and Hans-Georg Mueller", "title": "Concurrent Object Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern-day problems in statistics often face the challenge of exploring and\nanalyzing complex non-Euclidean object data that do not conform to vector space\nstructures or operations. Examples of such data objects include covariance\nmatrices, graph Laplacians of networks and univariate probability distribution\nfunctions. In the current contribution a new concurrent regression model is\nproposed to characterize the time-varying relation between an object in a\ngeneral metric space (as response) and a vector in $\\reals^p$ (as predictor),\nwhere concepts from Fr\\'echet regression is employed. Concurrent regression has\nbeen a well-developed area of research for Euclidean predictors and responses,\nwith many important applications for longitudinal studies and functional data.\nWe develop generalized versions of both global least squares regression and\nlocally weighted least squares smoothing in the context of concurrent\nregression for responses which are situated in general metric spaces and\npropose estimators that can accommodate sparse and/or irregular designs.\nConsistency results are demonstrated for sample estimates of appropriate\npopulation targets along with the corresponding rates of convergence. The\nproposed models are illustrated with mortality data and resting state\nfunctional Magnetic Resonance Imaging data (fMRI) as responses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:03:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bhattacharjee", "Satarupa", ""], ["Mueller", "Hans-Georg", ""]]}, {"id": "2101.05135", "submitter": "Joris Mulder", "authors": "Joris Mulder and Peter D. Hoff", "title": "A Latent Variable Model for Relational Events with Multiple Receivers", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Directional relational event data, such as email data, often include multiple\nreceivers for each event. Statistical methods for adequately modeling such data\nare limited however. In this article, a multiplicative latent factor model is\nproposed for relational event data with multiple receivers. For a given event\n(or message) all potential receiver actors are given a suitability score. When\nthis score exceeds a sender-specific threshold value, the actor is added to the\nreceiver set. The suitability score of a receiver actor for a given message can\ndepend on observed sender and receiver specific characteristics, and on the\nlatent variables of the sender, of the receiver, and of the message. One way to\nview these latent variables as the degree of specific unobserved topics on\nwhich an actor can be active as sender, as receiver, or that are relevant for a\ngiven message. Bayesian estimation of the model is relatively straightforward\ndue to the Gaussian distribution of the latent suitability scale. The\napplicability of the model is illustrated on simulated data and on Enron email\ndata for which about a third of the messages have at least two receivers.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:31:08 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mulder", "Joris", ""], ["Hoff", "Peter D.", ""]]}, {"id": "2101.05350", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung", "title": "Estimating functional parameters for understanding the impact of weather\n  and government interventions on COVID-19 outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the coronavirus disease 2019 (COVID-19) has shown profound effects on\npublic health and the economy worldwide, it becomes crucial to assess the\nimpact on the virus transmission and develop effective strategies to address\nthe challenge. A new statistical model derived from the SIR epidemic model with\nfunctional parameters is proposed to understand the impact of weather and\ngovernment interventions on the virus spread and also provide the forecasts of\nCOVID-19 infections among eight metropolitan areas in the United States. The\nmodel uses Bayesian inference with Gaussian process priors to study the\nfunctional parameters nonparametrically, and sensitivity analysis is adopted to\ninvestigate the main and interaction effects of these factors. This analysis\nreveals several important results including the potential interaction effects\nbetween weather and government interventions, which shed new light on the\neffective strategies for policymakers to mitigate the COVID-19 outbreak.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:22:03 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sung", "Chih-Li", ""]]}, {"id": "2101.05352", "submitter": "Glen McGee", "authors": "Glen McGee, Ander Wilson, Thomas F. Webster, Brent A. Coull", "title": "Bayesian Multiple Index Models for Environmental Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of environmental health research is to assess the risk\nposed by mixtures of environmental exposures. Two popular classes of models for\nmixtures analyses are response-surface methods and exposure-index methods.\nResponse-surface methods estimate high-dimensional surfaces and are thus highly\nflexible but difficult to interpret. In contrast, exposure-index methods\ndecompose coefficients from a linear model into an overall mixture effect and\nindividual index weights; these models yield easily interpretable effect\nestimates and efficient inferences when model assumptions hold, but, like most\nparsimonious models, incur bias when these assumptions do not hold. In this\npaper we propose a Bayesian multiple index model framework that combines the\nstrengths of each, allowing for non-linear and non-additive relationships\nbetween exposure indices and a health outcome, while reducing the\ndimensionality of the exposure vector and estimating index weights with\nvariable selection. This framework contains response-surface and exposure-index\nmodels as special cases, thereby unifying the two analysis strategies. This\nunification increases the range of models possible for analyzing environmental\nmixtures and health, allowing one to select an appropriate analysis from a\nspectrum of models varying in flexibility and interpretability. In an analysis\nof the association between telomere length and 18 organic pollutants in the\nNational Health and Nutrition Examination Survey (NHANES), the proposed\napproach fits the data as well as more complex response-surface methods and\nyields more interpretable results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:26:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["McGee", "Glen", ""], ["Wilson", "Ander", ""], ["Webster", "Thomas F.", ""], ["Coull", "Brent A.", ""]]}, {"id": "2101.05394", "submitter": "Francisco Cuevas-Pacheco Mr.", "authors": "Alfredo Alegr\\'ia, Francisco Cuevas-Pacheco, Peter Diggle, Emilio\n  Porcu", "title": "The $\\mathcal{F}$-family of covariance functions: A Mat\\'ern analogue\n  for modeling random fields on spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The Mat{\\'e}rn family of isotropic covariance functions has been central to\nthe theoretical development and application of statistical models for\ngeospatial data. For global data defined over the whole sphere representing\nplanet Earth, the natural distance between any two locations is the great\ncircle distance. In this setting, the Mat{\\'e}rn family of covariance functions\nhas a restriction on the smoothness parameter, making it an unappealing choice\nto model smooth data. Finding a suitable analogue for modelling data on the\nsphere is still an open problem. This paper proposes a new family of isotropic\ncovariance functions for random fields defined over the sphere. The proposed\nfamily has a parameter that indexes the mean square differentiability of the\ncorresponding Gaussian field, and allows for any admissible range of fractal\ndimension. Our simulation study mimics the fixed domain asymptotic setting,\nwhich is the most natural regime for sampling on a closed and bounded set. As\nexpected, our results support the analogous results (under the same asymptotic\nscheme) for planar processes that not all parameters can be estimated\nconsistently. We apply the proposed model to a dataset of precipitable water\ncontent over a large portion of the Earth, and show that the model gives more\nprecise predictions of the underlying process at unsampled locations than does\nthe Mat{\\'e}rn model using chordal distances.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:52:20 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alegr\u00eda", "Alfredo", ""], ["Cuevas-Pacheco", "Francisco", ""], ["Diggle", "Peter", ""], ["Porcu", "Emilio", ""]]}, {"id": "2101.05539", "submitter": "Jin Ming", "authors": "Suprateek Kundu, Jin Ming, Joe Nocera, and Keith M. McGregor", "title": "Integrative Learning for Population of Dynamic Networks with Covariates", "comments": "52 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although there is a rapidly growing literature on dynamic connectivity\nmethods, the primary focus has been on separate network estimation for each\nindividual, which fails to leverage common patterns of information. We propose\nnovel graph-theoretic approaches for estimating a population of dynamic\nnetworks that are able to borrow information across multiple heterogeneous\nsamples in an unsupervised manner and guided by covariate information.\nSpecifically, we develop a Bayesian product mixture model that imposes\nindependent mixture priors at each time scan and uses covariates to model the\nmixture weights, which results in time-varying clusters of samples designed to\npool information. The computation is carried out using an efficient\nExpectation-Maximization algorithm. Extensive simulation studies illustrate\nsharp gains in recovering the true dynamic network over existing dynamic\nconnectivity methods. An analysis of fMRI block task data with behavioral\ninterventions reveal sub-groups of individuals having similar dynamic\nconnectivity, and identifies intervention-related dynamic network changes that\nare concentrated in biologically interpretable brain regions. In contrast,\nexisting dynamic connectivity approaches are able to detect minimal or no\nchanges in connectivity over time, which seems biologically unrealistic and\nhighlights the challenges resulting from the inability to systematically borrow\ninformation across samples.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:29:08 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kundu", "Suprateek", ""], ["Ming", "Jin", ""], ["Nocera", "Joe", ""], ["McGregor", "Keith M.", ""]]}, {"id": "2101.05568", "submitter": "Rapha\\\"el Jauslin", "authors": "Rapha\\\"el Jauslin, Esther Eustache and Yves Till\\'e", "title": "Enhanced Cube Implementation For Highly Stratified Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A balanced sampling design should always be the adopted strategies if\nauxiliary information is available. Besides, integrating a stratified structure\nof the population in the sampling process can considerably reduce the variance\nof the estimators. We propose here a new method to handle the selection of a\nbalanced sample in a highly stratified population. The method improves\nsubstantially the commonly used sampling design and reduces the time-consuming\nproblem that could arise if inclusion probabilities within strata do not sum to\nan integer.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 12:36:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jauslin", "Rapha\u00ebl", ""], ["Eustache", "Esther", ""], ["Till\u00e9", "Yves", ""]]}, {"id": "2101.05630", "submitter": "Paul Wiemann", "authors": "Paul Wiemann and Thomas Kneib", "title": "Adaptive shrinkage of smooth functional effects towards a predefined\n  functional subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new horseshoe-type prior hierarchy for adaptively\nshrinking spline-based functional effects towards a predefined vector space of\nparametric functions. Instead of shrinking each spline coefficient towards\nzero, we use an adapted horseshoe prior to control the deviation from the\npredefined vector space. For this purpose, the modified horseshoe prior is set\nup with one scale parameter per spline and not one per coefficient. The\npresented prior allows for a large number of basis functions to capture all\nkinds of functional effects while the estimated functional effect is prevented\nfrom a highly oscillating overfit. We achieve this by integrating a smoothing\npenalty similar to the random walk prior commonly applied in Bayesian P-spline\npriors. In a simulation study, we demonstrate the properties of the new prior\nspecification and compare it to other approaches from the literature.\nFurthermore, we showcase the applicability of the proposed method by estimating\nthe energy consumption in Germany over the course of a day. For inference, we\nrely on Markov chain Monte Carlo simulations combining Gibbs sampling for the\nspline coefficients with slice sampling for all scale parameters in the model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:38:07 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wiemann", "Paul", ""], ["Kneib", "Thomas", ""]]}, {"id": "2101.05635", "submitter": "Yihan Cao Ms.", "authors": "Yihan Cao and Jarle Tufto", "title": "Bayesian inference with tmbstan for a state-space model with VAR(1)\n  state equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When using R package tmbstan for Bayesian inference, the built-in feature\nLaplace approximation to the marginal likelihood with random effects integrated\nout can be switched on and off. There exists no guideline on whether Laplace\napproximation should be used to achieve better efficiency especially when the\nstatistical model for estimating selection is complicated. To answer this\nquestion, we conducted simulation studies under different scenarios with a\nstate-space model employing a VAR(1) state equation. We found that turning on\nLaplace approximation in tmbstan would probably lower the computational\nefficiency, and only when there is a good amount of data, both tmbstan with and\nwithout Laplace approximation are worth trying since in this case, Laplace\napproximation is more likely to be accurate and may also lead to slightly\nhigher computational efficiency. The transition parameters and scale parameters\nin a VAR(1) process are hard to be estimated accurately and increasing the\nsample size at each time point do not help in the estimation, only more time\npoints in the data contain more information on these parameters and make the\nlikelihood dominate the posterior likelihood, thus lead to accurate estimates\nfor them.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:44:41 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Cao", "Yihan", ""], ["Tufto", "Jarle", ""]]}, {"id": "2101.05644", "submitter": "Huiling Yuan", "authors": "Huiling Yuan, Yong Zhou, Lu Xu, Yun Lei Sun, and Xiang Yu Cui", "title": "A new volatility model: GQARCH-It\\^{o} model", "comments": "25 pages, 1 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Volatility asymmetry is a hot topic in high-frequency financial market. In\nthis paper, we propose a new econometric model, which could describe volatility\nasymmetry based on high-frequency historical data and low-frequency historical\ndata. After providing the quasi-maximum likelihood estimators for the\nparameters, we establish their asymptotic properties. We also conduct a series\nof simulation studies to check the finite sample performance and volatility\nforecasting performance of the proposed methodologies. And an empirical\napplication is demonstrated that the new model has stronger volatility\nprediction power than GARCH-It\\^{o} model in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:58:03 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yuan", "Huiling", ""], ["Zhou", "Yong", ""], ["Xu", "Lu", ""], ["Sun", "Yun Lei", ""], ["Cui", "Xiang Yu", ""]]}, {"id": "2101.05654", "submitter": "Kirsten Schorning", "authors": "Kirsten Schorning and Holger Dette", "title": "Optimal designs for comparing regression curves -- dependence within and\n  between groups", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of designing experiments for the comparison of two\nregression curves describing the relation between a predictor and a response in\ntwo groups, where the data between and within the group may be dependent. In\norder to derive efficient designs we use results from stochastic analysis to\nidentify the best linear unbiased estimator (BLUE) in a corresponding\ncontinuous time model. It is demonstrated that in general simultaneous\nestimation using the data from both groups yields more precise results than\nestimation of the parameters separately in the two groups. Using the BLUE from\nsimultaneous estimation, we then construct an efficient linear estimator for\nfinite sample size by minimizing the mean squared error between the optimal\nsolution in the continuous time model and its discrete approximation with\nrespect to the weights (of the linear estimator). Finally, the optimal design\npoints are determined by minimizing the maximal width of a simultaneous\nconfidence band for the difference of the two regression functions. The\nadvantages of the new approach are illustrated by means of a simulation study,\nwhere it is shown that the use of the optimal designs yields substantially\nnarrower confidence bands than the application of uniform designs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:08:29 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Schorning", "Kirsten", ""], ["Dette", "Holger", ""]]}, {"id": "2101.05769", "submitter": "Marc Vidal", "authors": "Marc Vidal, Mattia Rosso, Ana M. Aguilera", "title": "Bi-Smoothed Functional Independent Component Analysis for EEG Artifact\n  Removal", "comments": "9 pages, 3 figures, 1 table", "journal-ref": "Mathematics 9, no. 11: 1243 (2021)", "doi": "10.3390/math9111243", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by mapping adverse artifactual events caused by body movements in\nelectroencephalographic (EEG) signals, we present a functional independent\ncomponent analysis based on the spectral decomposition of the kurtosis operator\nof a smoothed principal component expansion. A discrete roughness penalty is\nintroduced in the orthonormality constraint of the covariance eigenfunctions in\norder to obtain the smoothed basis for the proposed independent component\nmodel. To select the tuning parameters, a cross-validation method that\nincorporates shrinkage is used to enhance the performance on functional\nrepresentations with large basis dimension. This method provides an estimation\nstrategy to determine the penalty parameter and the optimal number of\ncomponents. Our independent component approach is applied to real EEG data to\nestimate genuine brain potentials from a contaminated signal. As a result, it\nis possible to control high-frequency remnants of neural origin overlapping\nartifactual sources to optimize their removal from the signal. An R package\nimplementing our methods is available at CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:20:20 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 08:53:45 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 17:58:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vidal", "Marc", ""], ["Rosso", "Mattia", ""], ["Aguilera", "Ana M.", ""]]}, {"id": "2101.05774", "submitter": "Nicolas Apfel", "authors": "Nicolas Apfel and Xiaoran Liang", "title": "Agglomerative Hierarchical Clustering for Selecting Valid Instrumental\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an instrumental variable (IV) selection procedure which combines\nthe agglomerative hierarchical clustering method and the Hansen-Sargan\noveridentification test for selecting valid instruments for IV estimation from\na large set of candidate instruments. Some of the instruments may be invalid in\nthe sense that they may fail the exclusion restriction. We show that under the\nplurality rule, our method can achieve oracle selection and estimation results.\nCompared to the previous IV selection methods, our method has the advantages\nthat it can deal with the weak instruments problem effectively, and can be\neasily extended to settings where there are multiple endogenous regressors and\nheterogenous treatment effects. We conduct Monte Carlo simulations to examine\nthe performance of our method, and compare it with two existing methods, the\nHard Thresholding method (HT) and the Confidence Interval method (CIM). The\nsimulation results show that our method achieves oracle selection and\nestimation results in both single and multiple endogenous regressors settings\nin large samples when all the instruments are strong. Also, our method works\nwell when some of the candidate instruments are weak, outperforming HT and CIM.\nWe apply our method to the estimation of the effect of immigration on wages in\nthe US.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:25:26 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Apfel", "Nicolas", ""], ["Liang", "Xiaoran", ""]]}, {"id": "2101.05899", "submitter": "Zhi Zhao", "authors": "Zhi Zhao, Marco Banterle, Alex Lewin, Manuela Zucknick", "title": "Structured Bayesian variable selection for multiple correlated response\n  variables and high-dimensional predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is becoming increasingly common to study complex associations between\nmultiple phenotypes and high-dimensional genomic features in biomedicine.\nHowever, it requires flexible and efficient joint statistical models if there\nare correlations between multiple response variables and between\nhigh-dimensional predictors. We propose a structured multivariate Bayesian\nvariable selection model to identify sparse predictors associated with multiple\ncorrelated response variables. The approach makes use of known structure\ninformation between the multiple response variables and high-dimensional\npredictors via a Markov random field (MRF) prior for the latent indicator\nvariables of the coefficient matrix of a sparse seemingly unrelated regressions\n(SSUR). The structure information included in the MRF prior can improve the\nmodel performance (i.e., variable selection and response prediction) compared\nto other common priors. In addition, we employ random effects to capture\nheterogeneity of grouped samples. The proposed approach is validated by\nsimulation studies and applied to a pharmacogenomic study which includes\npharmacological profiling and multi-omics data (i.e., gene expression, copy\nnumber variation and mutation) from in vitro anti-cancer drug sensitivity\nscreening.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 22:51:25 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:14:38 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhao", "Zhi", ""], ["Banterle", "Marco", ""], ["Lewin", "Alex", ""], ["Zucknick", "Manuela", ""]]}, {"id": "2101.05928", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A practical test for a planted community in heterogeneous networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental task in graph data mining is to find a planted\ncommunity(dense subgraph), which has wide application in biology, finance, spam\ndetection and so on. For a real network data, the existence of a dense subgraph\nis generally unknown. Statistical tests have been devised to testing the\nexistence of dense subgraph in a homogeneous random graph. However, many\nnetworks present extreme heterogeneity, that is, the degrees of nodes or\nvertexes don't concentrate on a typical value. The existing tests designed for\nhomogeneous random graph are not straightforwardly applicable to the\nheterogeneous case. Recently, scan test was proposed for detecting a dense\nsubgraph in heterogeneous(inhomogeneous) graph(\\cite{BCHV19}). However, the\ncomputational complexity of the scan test is generally not polynomial in the\ngraph size, which makes the test impractical for large or moderate networks. In\nthis paper, we propose a polynomial-time test that has the standard normal\ndistribution as the null limiting distribution. The power of the test is\ntheoretically investigated and we evaluate the performance of the test by\nsimulation and real data example.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:34:14 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.06034", "submitter": "Julian Wagner", "authors": "Julian Wagner and G\\\"oran Kauermann and Ralf M\\\"unnich", "title": "Matrix-free Penalized Spline Smoothing with Multiple Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper motivates high dimensional smoothing with penalized splines and its\nnumerical calculation in an efficient way. If smoothing is carried out over\nthree or more covariates the classical tensor product spline bases explode in\ntheir dimension bringing the estimation to its numerical limits. A recent\napproach by Siebenborn and Wagner(2019) circumvents storage expensive\nimplementations by proposing matrix-free calculations which allows to smooth\nover several covariates. We extend their approach here by linking penalized\nsmoothing and its Bayesian formulation as mixed model which provides a\nmatrix-free calculation of the smoothing parameter to avoid the use of\nhigh-computational cross validation. Further, we show how to extend the ideas\ntowards generalized regression models. The extended approach is applied to\nremote sensing satellite data in combination with spatial smoothing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:52:30 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wagner", "Julian", ""], ["Kauermann", "G\u00f6ran", ""], ["M\u00fcnnich", "Ralf", ""]]}, {"id": "2101.06077", "submitter": "Simon Hochgerner", "authors": "Florian Gach, Simon Hochgerner", "title": "Estimation of future discretionary benefits in traditional life\n  insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of traditional life insurance, the future discretionary\nbenefits ($FDB$), which are a central item for Solvency~II reporting, are\ngenerally calculated by computationally expensive Monte Carlo algorithms. We\nderive analytic formulas for lower and upper bounds for the $FDB$. This yields\nan estimation interval for the $FDB$, and the average of lower and upper bound\nis a simple estimator. These formulae are designed for real world applications,\nand we compare the results to publicly available reporting data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:52:57 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:48:56 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gach", "Florian", ""], ["Hochgerner", "Simon", ""]]}, {"id": "2101.06188", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Terrance D. Savitsky, Matthew R. Williams", "title": "Private Tabular Survey Data Products through Synthetic Microdata\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose three synthetic microdata approaches to generate private tabular\nsurvey data products for public release. We adapt a disclosure risk\nbased-weighted pseudo posterior mechanism to survey data with a focus on\nproducing tabular products under a formal privacy guarantee. Two of our\napproaches synthesize the observed sample distribution of the outcome and\nsurvey weights, jointly, such that both quantities together possess a\nprobabilistic differential privacy guarantee. The privacy-protected outcome and\nsampling weights are used to construct tabular cell estimates and associated\nstandard errors to correct for survey sampling bias. The third approach\nsynthesizes the population distribution from the observed sample under a pseudo\nposterior construction that treats survey sampling weights as fixed to correct\nthe sample likelihood to approximate that for the population. Each by-record\nsampling weight in the pseudo posterior is, in turn, multiplied by the\nassociated privacy, risk-based weight for that record to create a composite\npseudo posterior mechanism that both corrects for survey bias and provides a\nprivacy guarantee for the observed sample. Through a simulation study and a\nreal data application to the Survey of Doctorate Recipients public use file, we\ndemonstrate that our three microdata synthesis approaches to construct tabular\nproducts provide superior utility preservation as compared to the\nadditive-noise approach of the Laplace Mechanism. Moreover, all our approaches\nallow the release of microdata to the public, enabling additional analyses at\nno extra privacy cost.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:56:49 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Hu", "Jingchen", ""], ["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""]]}, {"id": "2101.06211", "submitter": "Prateek Bansal", "authors": "Thijs Dekker and Prateek Bansal", "title": "A Bayesian perspective on sampling of alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply a Bayesian perspective to sampling of alternatives\nfor multinomial logit (MNL) and mixed multinomial logit (MMNL) models. We find\nthree theoretical results -- i) McFadden's correction factor under the uniform\nsampling protocol can be transferred to the Bayesian context in MNL; ii) the\nuniform sampling protocol minimises the loss in information on the parameters\nof interest (i.e. the kernel of the posterior density) and thereby has\ndesirable small sample properties in MNL; and iii) our theoretical results\nextend to Bayesian MMNL models using data augmentation. Notably, sampling of\nalternatives in Bayesian MMNL models does not require the inclusion of the\nadditional correction factor, as identified by Guevara and Ben-Akiva (2013a) in\nclassical settings. Accordingly, due to desirable small and large sample\nproperties, uniform sampling is the recommended sampling protocol in MNL and\nMMNL, irrespective of the estimation framework selected.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:59:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dekker", "Thijs", ""], ["Bansal", "Prateek", ""]]}, {"id": "2101.06214", "submitter": "Philipp Seufert", "authors": "Philipp Seufert and Jan Schwientek and Michael Bortz", "title": "An Adaptive Algorithm based on High-Dimensional Function Approximation\n  to obtain Optimal Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms which compute locally optimal continuous designs often rely on a\nfinite design space or on repeatedly solving a complex non-linear program. Both\nmethods require extensive evaluations of the Jacobian Df of the underlying\nmodel. These evaluations present a heavy computational burden. Based on the\nKiefer-Wolfowitz Equivalence Theorem we present a novel design of experiments\nalgorithm which computes optimal designs in a continuous design space. For this\niterative algorithm we combine an adaptive Bayes-like sampling scheme with\nGaussian process regression to approximate the directional derivative of the\ndesign criterion. The approximation allows us to adaptively select new design\npoints on which to evaluate the model. The adaptive selection of the algorithm\nrequires significantly less evaluations of Df and reduces the runtime of the\ncomputations. We show the viability of the new algorithm on two examples from\nchemical engineering.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:00:13 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Seufert", "Philipp", ""], ["Schwientek", "Jan", ""], ["Bortz", "Michael", ""]]}, {"id": "2101.06237", "submitter": "Luis Leon-Novelo", "authors": "Luis G. Leon-Novelo and Terrance D. Savitsky", "title": "Fully Bayesian Estimation under Dependent and Informative Cluster\n  Sampling", "comments": "Total of 28 pages including 5 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Survey data are often collected under multistage sampling designs where units\nare binned to clusters that are sampled in a first stage. The unit-indexed\npopulation variables of interest are typically dependent within cluster. We\npropose a Fully Bayesian method that constructs an exact likelihood for the\nobserved sample to incorporate unit-level marginal sampling weights for\nperforming unbiased inference for population parameters while simultaneously\naccounting for the dependence induced by sampling clusters of units to produce\ncorrect uncertainty quantification. Our approach parameterizes cluster-indexed\nrandom effects in both a marginal model for the response and a conditional\nmodel for published, unit-level sampling weights. We compare our method to\nplug-in Bayesian and frequentist alternatives in a simulation study and\ndemonstrate that our method most closely achieves correct uncertainty\nquantification for model parameters, including the generating variances for\ncluster-indexed random effects. We demonstrate our method in two applications\nwith NHANES data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:51:23 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Leon-Novelo", "Luis G.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "2101.06289", "submitter": "Manuel Eichenlaub", "authors": "Manuel M. Eichenlaub", "title": "On the relationship between a Gamma distributed precision parameter and\n  the associated standard deviation in the context of Bayesian parameter\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In Bayesian inference, an unknown measurement uncertainty is often quantified\nin terms of a Gamma distributed precision parameter, which is impractical when\nprior information on the standard deviation of the measurement uncertainty\nshall be utilised during inference. This paper thus introduces a method for\ntransforming between a gamma distributed precision parameter and the\ndistribution of the associated standard deviation. The proposed method is based\non numerical optimisation and shows adequate results for a wide range of\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 20:07:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Eichenlaub", "Manuel M.", ""]]}, {"id": "2101.06382", "submitter": "Jason Whyte", "authors": "Jason M. Whyte", "title": "Model structures and structural identifiability: What? Why? How?", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": "10.1007/978-3-030-62497-2_10", "report-no": null, "categories": "stat.ME q-bio.BM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We may attempt to encapsulate what we know about a physical system by a model\nstructure, $S$. This collection of related models is defined by parametric\nrelationships between system features; say observables (outputs), unobservable\nvariables (states), and applied inputs. Each parameter vector in some parameter\nspace is associated with a completely specified model in $S$. Before choosing a\nmodel in $S$ to predict system behaviour, we must estimate its parameters from\nsystem observations. Inconveniently, multiple models (associated with distinct\nparameter estimates) may approximate data equally well. Yet, if these equally\nvalid alternatives produce dissimilar predictions of unobserved quantities,\nthen we cannot confidently make predictions. Thus, our study may not yield any\nuseful result. We may anticipate the non-uniqueness of parameter estimates\nahead of data collection by testing $S$ for structural global identifiability\n(SGI). Here we will provide an overview of the importance of SGI, some\nessential theory and distinctions, and demonstrate these in testing some\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 06:35:56 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Whyte", "Jason M.", ""]]}, {"id": "2101.06388", "submitter": "Ruizhong Miao", "authors": "Ruizhong Miao and Tianxi Li", "title": "Informative core identification in complex networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network analysis, the core structure of modeling interest is usually\nhidden in a larger network in which most structures are not informative. The\nnoise and bias introduced by the non-informative component in networks can\nobscure the salient structure and limit many network modeling procedures'\neffectiveness. This paper introduces a novel core-periphery model for the\nnon-informative periphery structure of networks without imposing a specific\nform for the informative core structure. We propose spectral algorithms for\ncore identification as a data preprocessing step for general downstream network\nanalysis tasks based on the model. The algorithm enjoys a strong theoretical\nguarantee of accuracy and is scalable for large networks. We evaluate the\nproposed method by extensive simulation studies demonstrating various\nadvantages over many traditional core-periphery methods. The method is applied\nto extract the informative core structure from a citation network and give more\ninformative results in the downstream hierarchical community detection.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:19:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Miao", "Ruizhong", ""], ["Li", "Tianxi", ""]]}, {"id": "2101.06415", "submitter": "Guangxing Wang", "authors": "Guangxing Wang, Sisheng Liu, Fang Han and Chongzhi Di", "title": "Robust Functional Principal Component Analysis via Functional Pairwise\n  Spatial Signs", "comments": "23 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Functional principal component analysis (FPCA) has been widely used to\ncapture major modes of variation and reduce dimensions in functional data\nanalysis. However, standard FPCA based on the sample covariance estimator does\nnot work well in the presence of outliers. To address this challenge, a new\nrobust functional principal component analysis approach based on the functional\npairwise spatial sign (PASS) operator, termed PASS FPCA, is introduced where we\npropose estimation procedures for both eigenfunctions and eigenvalues with and\nwithout measurement error. Compared to existing robust FPCA methods, the\nproposed one requires weaker distributional assumptions to conserve the\neigenspace of the covariance function. In particular, a class of distributions\ncalled the weakly functional coordinate symmetric (weakly FCS) is introduced\nthat allows for severe asymmetry and is strictly larger than the functional\nelliptical distribution class, the latter of which has been well used in the\nrobust statistics literature. The robustness of the PASS FPCA is demonstrated\nvia simulation studies and analyses of accelerometry data from a large-scale\nepidemiological study of physical activity on older women that partly motivates\nthis work.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:24:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Guangxing", ""], ["Liu", "Sisheng", ""], ["Han", "Fang", ""], ["Di", "Chongzhi", ""]]}, {"id": "2101.06494", "submitter": "Debashis Chatterjee", "authors": "Debashis Chatterjee", "title": "Novel Bayesian Procrustes Variance-based Inferences in Geometric\n  Morphometrics & Novel R package: BPviGM1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compared to abundant classical statistics-based literature, to date, very\nlittle Bayesian literature exists on Procrustes shape analysis in Geometric\nMorphometrics, probably because of being a relatively new branch of statistical\nresearch and because of inherent computational difficulty associated with\nBayesian analysis. Moreover, we may obtain a plethora of novel inferences from\nBayesian Procrustes analysis of shape parameter distributions. In this paper,\nwe propose to regard the posterior of Procrustes shape variance as\nmorphological variability indicators. Here we propose novel Bayesian\nmethodologies for Procrustes shape analysis based on landmark data's isotropic\nvariance assumption and propose a Bayesian statistical test for model\nvalidation of new species discovery using morphological variation reflected in\nthe posterior distribution of landmark-variance of objects studied under\nGeometric Morphometrics. We will consider Gaussian distribution-based and\nheavy-tailed t distribution-based models for Procrustes analysis.\n  To date, we are not aware of any direct R package for Bayesian Procrustes\nanalysis for landmark-based Geometric Morphometrics. Hence, we introduce a\nnovel, simple R package \\textbf{BPviGM1} (\"Bayesian Procrustes Variance-based\ninferences in Geometric Morphometrics 1\"), which essentially contains the R\ncode implementations of the computations for proposed models and methodologies,\nsuch as R function for Markov Chain Monte Carlo (MCMC) run for drawing samples\nfrom posterior of parameters of concern and R function for the proposed\nBayesian test of model validation based on significance morphological\nvariation. As an application, we can quantitatively show that primate male-face\nmay be genetically viable to more shape-variation than the same for females.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 18:12:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 13:25:14 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chatterjee", "Debashis", ""]]}, {"id": "2101.06531", "submitter": "Sheng Jiang", "authors": "Sheng Jiang, Surya Tokdar", "title": "Consistent Bayesian Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic Block Models (SBMs) are a fundamental tool for community detection\nin network analysis. But little theoretical work exists on the statistical\nperformance of Bayesian SBMs, especially when the community count is unknown.\nThis paper studies a special class of SBMs whose community-wise connectivity\nprobability matrix is diagonally dominant, i.e., members of the same community\nare more likely to connect with one another than with members from other\ncommunities. The diagonal dominance constraint is embedded within an otherwise\nweak prior, and, under mild regularity conditions, the resulting posterior\ndistribution is shown to concentrate on the true community count and membership\nallocation as the network size grows to infinity. A reversible-jump Markov\nChain Monte Carlo posterior computation strategy is developed by adapting the\nallocation sampler of Mcdaid et al (2013). Finite sample properties are\nexamined via simulation studies in which the proposed method offers competitive\nestimation accuracy relative to existing methods under a variety of challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:08:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jiang", "Sheng", ""], ["Tokdar", "Surya", ""]]}, {"id": "2101.06536", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal, Steve Yadlowsky, Negar Rostamzadeh and Katherine Heller", "title": "Deep Cox Mixtures for Survival Regression", "comments": "Machine Learning for Healthcare Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n  We perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:41:22 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:28:29 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 11:11:32 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Nagpal", "Chirag", ""], ["Yadlowsky", "Steve", ""], ["Rostamzadeh", "Negar", ""], ["Heller", "Katherine", ""]]}, {"id": "2101.06592", "submitter": "Simon Mak", "authors": "Simon Mak, Yuanshuo Zhou, Lavonne Hoang, C. F. Jeff Wu", "title": "TSEC: a framework for online experimentation under experimental\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is a popular algorithm for solving multi-armed bandit\nproblems, and has been applied in a wide range of applications, from website\ndesign to portfolio optimization. In such applications, however, the number of\nchoices (or arms) $N$ can be large, and the data needed to make adaptive\ndecisions require expensive experimentation. One is then faced with the\nconstraint of experimenting on only a small subset of $K \\ll N$ arms within\neach time period, which poses a problem for traditional Thompson sampling. We\npropose a new Thompson Sampling under Experimental Constraints (TSEC) method,\nwhich addresses this so-called \"arm budget constraint\". TSEC makes use of a\nBayesian interaction model with effect hierarchy priors, to model correlations\nbetween rewards on different arms. This fitted model is then integrated within\nThompson sampling, to jointly identify a good subset of arms for\nexperimentation and to allocate resources over these arms. We demonstrate the\neffectiveness of TSEC in two problems with arm budget constraints. The first is\na simulated website optimization study, where TSEC shows noticeable\nimprovements over industry benchmarks. The second is a portfolio optimization\napplication on industry-based exchange-traded funds, where TSEC provides more\nconsistent and greater wealth accumulation over standard investment strategies.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 05:04:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mak", "Simon", ""], ["Zhou", "Yuanshuo", ""], ["Hoang", "Lavonne", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2101.06638", "submitter": "Cecilia Dao", "authors": "Cecilia Dao, Jiming Jiang, Debashis Paul, Hongyu Zhao", "title": "Variance Estimation and Confidence Intervals from High-dimensional\n  Genome-wide Association Studies Through Misspecified Mixed Model Analysis", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variance estimation and associated confidence intervals for\nparameters characterizing genetic effects from genome-wide association studies\n(GWAS) misspecified mixed model analysis. Previous studies have shown that, in\nspite of the model misspecification, certain quantities of genetic interests\nare estimable, and consistent estimators of these quantities can be obtained\nusing the restricted maximum likelihood (REML) method under a misspecified\nlinear mixed model. However, the asymptotic variance of such a REML estimator\nis complicated and not ready to be implemented for practical use. In this\npaper, we develop practical and computationally convenient methods for\nestimating such asymptotic variances and constructing the associated confidence\nintervals. Performance of the proposed methods is evaluated empirically based\non Monte-Carlo simulations and real-data application.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:19:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dao", "Cecilia", ""], ["Jiang", "Jiming", ""], ["Paul", "Debashis", ""], ["Zhao", "Hongyu", ""]]}, {"id": "2101.06662", "submitter": "Pengzhou (Abel) Wu", "authors": "Pengzhou Wu and Kenji Fukumizu", "title": "Intact-VAE: Estimating Treatment Effects under Unobserved Confounding", "comments": "A major update of the ICLR submission. About 80% of the paper is\n  rewritten, and the theoretical part is totally new. For detailed notes on the\n  update, see https://openreview.net/forum?id=D3TNqCspFpM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important problem of causal inference, we discuss the identification\nand estimation of treatment effects under unobserved confounding. Representing\nthe confounder as a latent variable, we propose Intact-VAE, a new variant of\nvariational autoencoder (VAE), motivated by the prognostic score that is\nsufficient for identifying treatment effects. We theoretically show that, under\ncertain settings, treatment effects are identified by our model, and further,\nbased on the identifiability of our model (i.e., determinacy of\nrepresentation), our VAE is a consistent estimator with representation balanced\nfor treatment groups. Experiments on (semi-)synthetic datasets show\nstate-of-the-art performance under diverse settings.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:03:44 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 01:12:31 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wu", "Pengzhou", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "2101.06823", "submitter": "Yizhen Xu", "authors": "Yizhen Xu, Joseph W. Hogan, Michael J. Daniels, Rami Kantor, Ann\n  Mwangi", "title": "Inference for BART with Multinomial Outcomes", "comments": "23 pages, 12 tables, 6 figures, with appendix, 49 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The multinomial probit Bayesian additive regression trees (MPBART) framework\nwas proposed by Kindo et al. (KD), approximating the latent utilities in the\nmultinomial probit (MNP) model with BART (Chipman et al. 2010). Compared to\nmultinomial logistic models, MNP does not assume independent alternatives and\nthe correlation structure among alternatives can be specified through\nmultivariate Gaussian distributed latent utilities. We introduce two new\nalgorithms for fitting the MPBART and show that the theoretical mixing rates of\nour proposals are equal or superior to the existing algorithm in KD. Through\nsimulations, we explore the robustness of the methods to the choice of\nreference level, imbalance in outcome frequencies, and the specifications of\nprior hyperparameters for the utility error term. The work is motivated by the\napplication of generating posterior predictive distributions for mortality and\nengagement in care among HIV-positive patients based on electronic health\nrecords (EHRs) from the Academic Model Providing Access to Healthcare (AMPATH)\nin Kenya. In both the application and simulations, we observe better\nperformance using our proposals as compared to KD in terms of MCMC convergence\nrate and posterior predictive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:59:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Yizhen", ""], ["Hogan", "Joseph W.", ""], ["Daniels", "Michael J.", ""], ["Kantor", "Rami", ""], ["Mwangi", "Ann", ""]]}, {"id": "2101.06839", "submitter": "Teng Wu", "authors": "Teng Wu, Runmin Wang, Hao Yan, Xiaofeng Shao", "title": "Adaptive Change Point Monitoring for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a class of monitoring statistics for a mean shift\nin a sequence of high-dimensional observations. Inspired by the recent\nU-statistic based retrospective tests developed by Wang et al.(2019) and Zhang\net al.(2020), we advance the U-statistic based approach to the sequential\nmonitoring problem by developing a new adaptive monitoring procedure that can\ndetect both dense and sparse changes in real-time. Unlike Wang et al.(2019) and\nZhang et al.(2020), where self-normalization was used in their tests, we\ninstead introduce a class of estimators for $q$-norm of the covariance matrix\nand prove their ratio consistency. To facilitate fast computation, we further\ndevelop recursive algorithms to improve the computational efficiency of the\nmonitoring procedure. The advantage of the proposed methodology is demonstrated\nvia simulation studies and real data illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:10:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wu", "Teng", ""], ["Wang", "Runmin", ""], ["Yan", "Hao", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2101.06950", "submitter": "Armeen Taeb", "authors": "Armeen Taeb, Peter B\\\"uhlmann", "title": "Perturbations and Causality in Gaussian Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference is a challenging problem with observational data alone. The\ntask becomes easier when having access to data from perturbing the underlying\nsystem, even when happening in a non-randomized way: this is the setting we\nconsider, encompassing also latent confounding variables. To identify causal\nrelations among a collections of covariates and a response variable, existing\nprocedures rely on at least one of the following assumptions: i) the response\nvariable remains unperturbed, ii) the latent variables remain unperturbed, and\niii) the latent effects are dense. In this paper, we examine a perturbation\nmodel for interventional data, which can be viewed as a mixed-effects linear\nstructural causal model, over a collection of Gaussian variables that does not\nsatisfy any of these conditions. We propose a maximum-likelihood estimator --\ndubbed DirectLikelihood -- that exploits system-wide invariances to uniquely\nidentify the population causal structure from unspecific perturbation data, and\nour results carry over to linear structural causal models without requiring\nGaussianity. We illustrate the utility of our framework on synthetic data as\nwell as real data involving California reservoirs and protein expressions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:24:08 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 15:25:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Taeb", "Armeen", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.07097", "submitter": "Ian Silver", "authors": "Ian A Silver", "title": "The Violating Assumptions Series: Simulated demonstrations to illustrate\n  how assumptions can affect statistical estimates", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.13339.69921", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When teaching and discussing statistical assumptions, our focus is oftentimes\nplaced on how to test and address potential violations rather than the effects\nof violating assumptions on the estimates produced by our statistical models.\nThe latter represents a potential avenue to help us better understand the\nimpact of researcher degrees of freedom on the statistical estimates we\nproduce. The Violating Assumptions Series is an endeavor I have undertaken to\ndemonstrate the effects of violating assumptions on the estimates produced\nacross various statistical models. The series will review assumptions\nassociated with estimating causal associations, as well as more complicated\nstatistical models including, but not limited to, multilevel models, path\nmodels, structural equation models, and Bayesian models. In addition to the\nprimary goal, the series of posts is designed to illustrate how simulations can\nbe used to develop a comprehensive understanding of applied statistics.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 14:42:35 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 16:10:06 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 18:27:11 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 01:11:55 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 15:25:23 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Silver", "Ian A", ""]]}, {"id": "2101.07141", "submitter": "Achim Zeileis", "authors": "Susanne K\\\"oll, Ioannis Kosmidis, Christian Kleiber, Achim Zeileis", "title": "Bias Reduction as a Remedy to the Consequences of Infinite Estimates in\n  Poisson and Tobit Regression", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data separation is a well-studied phenomenon that can cause problems in the\nestimation and inference from binary response models. Complete or\nquasi-complete separation occurs when there is a combination of regressors in\nthe model whose value can perfectly predict one or both outcomes. In such\ncases, and such cases only, the maximum likelihood estimates and the\ncorresponding standard errors are infinite. It is less widely known that the\nsame can happen in further microeconometric models. One of the few works in the\narea is Santos Silva and Tenreyro (2010) who note that the finiteness of the\nmaximum likelihood estimates in Poisson regression depends on the data\nconfiguration and propose a strategy to detect and overcome the consequences of\ndata separation. However, their approach can lead to notable bias on the\nparameter estimates when the regressors are correlated. We illustrate how\nbias-reducing adjustments to the maximum likelihood score equations can\novercome the consequences of separation in Poisson and Tobit regression models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:07:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["K\u00f6ll", "Susanne", ""], ["Kosmidis", "Ioannis", ""], ["Kleiber", "Christian", ""], ["Zeileis", "Achim", ""]]}, {"id": "2101.07167", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jennifer L. Wadsworth", "title": "Spatial deformation for non-stationary extremal dependence", "comments": "41 pages, 10 figures", "journal-ref": "Environmetrics, e2671 (2021)", "doi": "10.1002/env.2671", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the extremal dependence structure of spatial data is considerably\neasier if that structure is stationary. However, for data observed over large\nor complicated domains, non-stationarity will often prevail. Current methods\nfor modelling non-stationarity in extremal dependence rely on models that are\neither computationally difficult to fit or require prior knowledge of\ncovariates. Sampson and Guttorp (1992) proposed a simple technique for handling\nnon-stationarity in spatial dependence by smoothly mapping the sampling\nlocations of the process from the original geographical space to a latent space\nwhere stationarity can be reasonably assumed. We present an extension of this\nmethod to a spatial extremes framework by considering least squares\nminimisation of pairwise theoretical and empirical extremal dependence\nmeasures. Along with some practical advice on applying these deformations, we\nprovide a detailed simulation study in which we propose three spatial processes\nwith varying degrees of non-stationarity in their extremal and central\ndependence structures. The methodology is applied to Australian summer\ntemperature extremes and UK precipitation to illustrate its efficacy compared\nto a naive modelling approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:16:47 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Richards", "Jordan", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2101.07250", "submitter": "Xujun Liu", "authors": "Xujun Liu, Olgica Milenkovic, George V. Moustakides", "title": "Query-Based Selection of Optimal Candidates under the Mallows Model", "comments": "32 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of query-based selection of the optimal candidate in\nrank-ordered lists generated by the Mallows model. In this setting, one is\npresented with a list of candidates drawn according to a Gaussian-like\ndistribution for permutations and the goal is to identify the highest ranked\ncandidate through a sequential interview process that does not allow rejected\ncandidates to be revisited. The new modeling assumption consists in being able\nto query a Genie at the time a selection is to be made. The query provides an\nanswer indicating if the candidate in question is the highest-ranked or not. If\nthe Genie confirms the query, the selection process terminates. Otherwise, the\nsequential examinations continue until a new potentially optimal candidate is\nidentified. Our results include optimal interview strategies for a bounded\nnumber of queries that can be exactly determined through numerical evaluations\nas well as the expected number of interviews until the optimal candidate is\nidentified or the interview process completed. Although the problem exhibits\nsimilarities with the Dowry problem with multiple choices of Gilbert and\nMosteller, the proposed Genie-based model substantially differs from it as it\nallows for early stopping and addresses nonuniform candidate interview\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:58:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Xujun", ""], ["Milenkovic", "Olgica", ""], ["Moustakides", "George V.", ""]]}, {"id": "2101.07329", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan, Kyra H. Grantz, and Eleanor Murray", "title": "Quantifying Uncertainty in Infectious Disease Mechanistic Models", "comments": "American Journal of Epidemiology, 2021", "journal-ref": null, "doi": "10.1093/aje/kwab013", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This primer describes the statistical uncertainty in mechanistic models and\nprovides R code to quantify it. We begin with an overview of mechanistic models\nfor infectious disease, and then describe the sources of statistical\nuncertainty in the context of a case study on SARS-CoV-2. We describe the\nstatistical uncertainty as belonging to three categories: data uncertainty,\nstochastic uncertainty, and structural uncertainty. We demonstrate how to\naccount for each of these via statistical uncertainty measures and sensitivity\nanalyses broadly, as well as in a specific case study on estimating the basic\nreproductive number, $R_0$, for SARS-CoV-2.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:12:33 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 20:02:59 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Grantz", "Kyra H.", ""], ["Murray", "Eleanor", ""]]}, {"id": "2101.07359", "submitter": "Zeyu Bian", "authors": "Zeyu Bian, Erica EM Moodie, Susan M Shortreed and Sahir Bhatnagar", "title": "Variable Selection in Regression-based Estimation of Dynamic Treatment\n  Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic treatment regimes (DTRs) consist of a sequence of decision rules, one\nper stage of intervention, that finds effective treatments for individual\npatients according to patient information history. DTRs can be estimated from\nmodels which include the interaction between treatment and a small number of\ncovariates which are often chosen a priori. However, with increasingly large\nand complex data being collected, it is difficult to know which prognostic\nfactors might be relevant in the treatment rule. Therefore, a more data-driven\napproach of selecting these covariates might improve the estimated decision\nrules and simplify models to make them easier to interpret. We propose a\nvariable selection method for DTR estimation using penalized dynamic weighted\nleast squares. Our method has the strong heredity property, that is, an\ninteraction term can be included in the model only if the corresponding main\nterms have also been selected. Through simulations, we show our method has both\nthe double robustness property and the oracle property, and the newly proposed\nmethods compare favorably with other variable selection approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:53:55 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bian", "Zeyu", ""], ["Moodie", "Erica EM", ""], ["Shortreed", "Susan M", ""], ["Bhatnagar", "Sahir", ""]]}, {"id": "2101.07374", "submitter": "Kaiqiong Zhao", "authors": "Kaiqiong Zhao, Karim Oualkacha, Lajmi Lakhal-Chaieb, Aur\\'elie Labbe,\n  Kathleen Klein, Sasha Bernatsky, Marie Hudson, In\\'es Colmegna, Celia M.T.\n  Greenwood", "title": "Detecting differentially methylated regions in bisulfite sequencing data\n  using quasi-binomial mixed models with smooth covariate effect estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying disease-associated changes in DNA methylation can help to gain a\nbetter understanding of disease etiology. Bisulfite sequencing technology\nallows the generation of methylation profiles at single base of DNA. We\npreviously developed a method for estimating smooth covariate effects and\nidentifying differentially methylated regions (DMRs) from bisulfite sequencing\ndata, which copes with experimental errors and variable read depths; this\nmethod utilizes the binomial distribution to characterize the variability in\nthe methylated counts. However, bisulfite sequencing data frequently include\nlow-count integers and can exhibit over or under dispersion relative to the\nbinomial distribution. We present a substantial improvement to our previous\nwork by proposing a quasi-likelihood-based regional testing approach which\naccounts for multiplicative and additive sources of dispersion. We demonstrate\nthe theoretical properties of the resulting tests, as well as their marginal\nand conditional interpretations. Simulations show that the proposed method\nprovides correct inference for smooth covariate effects and captures the major\nmethylation patterns with excellent power.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:29:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhao", "Kaiqiong", ""], ["Oualkacha", "Karim", ""], ["Lakhal-Chaieb", "Lajmi", ""], ["Labbe", "Aur\u00e9lie", ""], ["Klein", "Kathleen", ""], ["Bernatsky", "Sasha", ""], ["Hudson", "Marie", ""], ["Colmegna", "In\u00e9s", ""], ["Greenwood", "Celia M. T.", ""]]}, {"id": "2101.07456", "submitter": "Ali Rafei", "authors": "Ali Rafei, Carol A. C. Flannagan, Brady T. West and Michael R. Elliott", "title": "Robust Bayesian Inference for Big Data: Combining Sensor-based Records\n  with Traditional Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data often presents as massive non-probability samples. Not only is the\nselection mechanism often unknown, but larger data volume amplifies the\nrelative contribution of selection bias to total error. Existing bias\nadjustment approaches assume that the conditional mean structures have been\ncorrectly specified for the selection indicator or key substantive measures. In\nthe presence of a reference probability sample, these methods rely on a\npseudo-likelihood method to account for the sampling weights of the reference\nsample, which is parametric in nature. Under a Bayesian framework, handling the\nsampling weights is an even bigger hurdle. To further protect against model\nmisspecification, we expand the idea of double robustness such that more\nflexible non-parametric methods as well as Bayesian models can be used for\nprediction. In particular, we employ Bayesian additive regression trees, which\nnot only capture non-linear associations automatically but permit direct\nquantification of the uncertainty of point estimates through its posterior\npredictive draws. We apply our method to sensor-based naturalistic driving data\nfrom the second Strategic Highway Research Program using the 2017 National\nHousehold Travel Survey as a benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 04:20:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rafei", "Ali", ""], ["Flannagan", "Carol A. C.", ""], ["West", "Brady T.", ""], ["Elliott", "Michael R.", ""]]}, {"id": "2101.07466", "submitter": "Eunhye Song", "authors": "Eunhye Song", "title": "Sequential Bayesian Risk Set Inference for Robust Discrete Optimization\n  via Simulation", "comments": "Under review since September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization via simulation (OvS) procedures that assume the simulation\ninputs are generated from the real-world distributions are subject to the risk\nof selecting a suboptimal solution when the distributions are substituted with\ninput models estimated from finite real-world data -- known as input model\nrisk. Focusing on discrete OvS, this paper proposes a new Bayesian framework\nfor analyzing input model risk of implementing an arbitrary solution, $x$,\nwhere uncertainty about the input models is captured by a posterior\ndistribution. We define the $\\alpha$-level risk set of solution $x$ as the set\nof solutions whose expected performance is better than $x$ by a practically\nmeaningful margin $(>\\delta)$ given common input models with significant\nprobability ($>\\alpha$) under the posterior distribution. The user-specified\nparameters, $\\delta$ and $\\alpha$, control robustness of the procedure to the\ndesired level as well as guards against unnecessary conservatism. An empty risk\nset implies that there is no practically better solution than $x$ with\nsignificant probability even though the real-world input distributions are\nunknown. For efficient estimation of the risk set, the conditional mean\nperformance of a solution given a set of input distributions is modeled as a\nGaussian process (GP) that takes the solution-distributions pair as an input.\nIn particular, our GP model allows both parametric and nonparametric input\nmodels. We propose the sequential risk set inference procedure that estimates\nthe risk set and selects the next solution-distributions pair to simulate using\nthe posterior GP at each iteration. We show that simulating the pair expected\nto change the risk set estimate the most in the next iteration is the\nasymptotic one-step optimal sampling rule that minimizes the number of\nincorrectly classified solutions, if the procedure runs without stopping.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 05:11:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Song", "Eunhye", ""]]}, {"id": "2101.07620", "submitter": "Angelika Geroldinger", "authors": "Rainer Puhr, Georg Heinze, Mariana Nold, Lara Lusa and Angelika\n  Geroldinger", "title": "Firth's logistic regression with rare events: accurate effect estimates\n  AND predictions?", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7273", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firth-type logistic regression has become a standard approach for the\nanalysis of binary outcomes with small samples. Whereas it reduces the bias in\nmaximum likelihood estimates of coefficients, bias towards 1/2 is introduced in\nthe predicted probabilities. The stronger the imbalance of the outcome, the\nmore severe is the bias in the predicted probabilities. We propose two simple\nmodifications of Firth-type logistic regression resulting in unbiased predicted\nprobabilities. The first corrects the predicted probabilities by a post-hoc\nadjustment of the intercept. The other is based on an alternative formulation\nof Firth-types estimation as an iterative data augmentation procedure. Our\nsuggested modification consists in introducing an indicator variable which\ndistinguishes between original and pseudo observations in the augmented data.\nIn a comprehensive simulation study these approaches are compared to other\nattempts to improve predictions based on Firth-type penalization and to other\npublished penalization strategies intended for routine use. For instance, we\nconsider a recently suggested compromise between maximum likelihood and\nFirth-type logistic regression. Simulation results are scrutinized both with\nregard to prediction and regression coefficients. Finally, the methods\nconsidered are illustrated and compared for a study on arterial closure devices\nin minimally invasive cardiac surgery.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:54:28 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Puhr", "Rainer", ""], ["Heinze", "Georg", ""], ["Nold", "Mariana", ""], ["Lusa", "Lara", ""], ["Geroldinger", "Angelika", ""]]}, {"id": "2101.07640", "submitter": "Angelika Geroldinger", "authors": "Angelika Geroldinger, Lara Lusa, Mariana Nold, and Georg Heinze", "title": "On resampling methods for model assessment in penalized and unpenalized\n  logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized logistic regression methods are frequently used to investigate the\nrelationship between a binary outcome and a set of explanatory variables. The\nmodel performance can be assessed by measures such as the concordance statistic\n(c-statistic), the discrimination slope and the Brier score. Often, data\nresampling techniques, e.g. crossvalidation, are employed to correct for\noptimism in these model performance criteria. Especially with small samples or\na rare binary outcome variable, leave-one-out crossvalidation is a popular\nchoice. Using simulations and a real data example, we compared the effect of\ndifferent resampling techniques on the estimation of c-statistics,\ndiscrimination slopes and Brier scores for three estimators of logistic\nregression models, including the maximum likelihood and two maximum\npenalized-likelihood estimators. Our simulation study confirms earlier studies\nreporting that leave-one-out crossvalidated c-statistics can be strongly biased\ntowards zero. In addition, our study reveals that this bias is more pronounced\nfor estimators shrinking predicted probabilities towards the observed event\nrate, such as ridge regression. Leave-one-out crossvalidation also provided\npessimistic estimates of the discrimination slope but nearly unbiased estimates\nof the Brier score. We recommend to use leave-pair-out crossvalidation,\nfive-fold crossvalidation with repetition, the enhanced or the .632+ bootstrap\nto estimate c-statistics and leave-pair-out or five-fold crossvalidation to\nestimate discrimination slopes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:30:24 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Geroldinger", "Angelika", ""], ["Lusa", "Lara", ""], ["Nold", "Mariana", ""], ["Heinze", "Georg", ""]]}, {"id": "2101.07710", "submitter": "Mohammad Fayaz", "authors": "Mohammad Fayaz, Alireza Abadi, Soheila Khodakarim", "title": "The effect of Hybrid Principal Components Analysis on the Signal\n  Compression Functional Regression: With EEG-fMRI Application", "comments": "It has 11 pages with 3 tables and 3 figures. It presented at \"The\n  13th International Conference of the ERCIM WG on Computational and\n  Methodological Statistics (CMStatistics 2020) (Virtual), 19-21 December 2020\"\n  (http://www.cmstatistics.org/CMStatistics2020/index.php). We plan to publish\n  it in statistical journals, especially in the conference's recommended\n  journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: In some situations that exist both scalar and functional data,\ncalled mixed and hybrid data, the hybrid PCA (HPCA) was introduced. Among the\nregression models for the hybrid data, we can count covariate-adjusted HPCA,\nthe Semi-functional partial linear regression, function-on-function (FOF)\nregression with signal compression, and functional additive regression, models.\nIn this article, we study the effects of HPCA decomposition of hybrid data on\nthe prediction accuracy of the FOF regression with signal compressions. Method:\nWe stated a two-step procedure for incorporating the HPCA in the functional\nregressions. The first step is reconstructing the data based on the HPCAs and\nthe second step is merging data on the other dimensions and calculate the\npoint-wise average of the desired functional dimension. We also choose the\nnumber of HPCA based on Mean Squared Perdition Error (MSPE). Result: In the two\nsimulations, we show that the regression models with the first HPCA have the\nbest accuracy prediction and model fit summaries among no HPCA and all HPCAs\nwith a training/testing approach. Finally, we applied our methodology to the\nEEG-fMRI dataset. Conclusions: We conclude that our methodology improves the\nprediction of the experiments with the EEG datasets. And we recommend that\ninstead of using the functional PCA on the desired dimension, reconstruct the\ndata with HPCA and average it on the other two dimensions for functional\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:26:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Fayaz", "Mohammad", ""], ["Abadi", "Alireza", ""], ["Khodakarim", "Soheila", ""]]}, {"id": "2101.07766", "submitter": "Louis Raynal", "authors": "Louis Raynal and Jukka-Pekka Onnela", "title": "Selection of Summary Statistics for Network Model Choice with\n  Approximate Bayesian Computation", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) now serves as one of the major\nstrategies to perform model choice and parameter inference on models with\nintractable likelihoods. An essential component of ABC involves comparing a\nlarge amount of simulated data with the observed data through summary\nstatistics. To avoid the curse of dimensionality, summary statistic selection\nis of prime importance, and becomes even more critical when applying ABC to\nmechanistic network models. Indeed, while many summary statistics can be used\nto encode network structures, their computational complexity can be highly\nvariable. For large networks, computation of summary statistics can quickly\ncreate a bottleneck, making the use of ABC difficult. To reduce this\ncomputational burden and make the analysis of mechanistic network models more\npractical, we investigated two questions in a model choice framework. First, we\nstudied the utility of cost-based filter selection methods to account for\ndifferent summary costs during the selection process. Second, we performed\nselection using networks generated with a smaller number of nodes to reduce the\ntime required for the selection step. Our findings show that computationally\ninexpensive summary statistics can be efficiently selected with minimal impact\non classification accuracy. Furthermore, we found that networks with a smaller\nnumber of nodes can only be employed to eliminate a moderate number of\nsummaries. While this latter finding is network specific, the former is general\nand can be adapted to any ABC application.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:21:06 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Raynal", "Louis", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2101.07776", "submitter": "Yuchen Xu", "authors": "Yuchen Xu, Marie-Christine D\\\"uker, David S. Matteson", "title": "Testing Simultaneous Diagonalizability", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes novel methods to test for simultaneous diagonalization of\npossibly asymmetric matrices. Motivated by various applications, a two-sample\ntest as well as a generalization for multiple matrices are proposed. A partial\nversion of the test is also studied to check whether a partial set of\neigenvectors is shared across samples. Additionally, a novel algorithm for the\nconsidered testing methods is introduced. Simulation studies demonstrate\nfavorable performance for all designs. Finally, the theoretical results are\nutilized to decouple vector autoregression models into multiple univariate time\nseries, and to test for the same stationary distribution in recurrent Markov\nchains. These applications are demonstrated using macroeconomic indices of 8\ncountries and streamflow data, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:43:25 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Xu", "Yuchen", ""], ["D\u00fcker", "Marie-Christine", ""], ["Matteson", "David S.", ""]]}, {"id": "2101.07919", "submitter": "Jonas Krampe", "authors": "Alexander Braumann, Jonas Krampe, Jens-Peter Kreiss and Efstathios\n  Paparoditis", "title": "Estimation of the Distribution of the Individual Reproduction Number:\n  The Case of the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the distribution of the individual\nreproduction number governing the COVID-19 pandemic. Under the assumption that\nthis random variable follows a Negative Binomial distribution, we focus on\nconstructing estimators of the parameters of this distribution using reported\ninfection data and taking into account issues like under-reporting or the time\nbehavior of the infection and of the reporting processes. To this end, we\nextract information from regionally dissaggregated data reported by German\nhealth authorities, in order to estimate not only the mean but also the\nvariance of the distribution of the individual reproduction number. In contrast\nto the mean, the latter parameter also depends on the unknown under-reporting\nrate of the pandemic. The estimates obtained allow not only for a better\nunderstanding of the time-varying behavior of the expected value of the\nindividual reproduction number but also of its dispersion, for the construction\nof bootstrap confidence intervals and for a discussion of the implications of\ndifferent policy interventions. Our methodological investigations are\naccompanied by an empirical study of the development of the COVID-19 pandemic\nin Germany, which shows a strong overdispersion of the individual reproduction\nnumber.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:17:56 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Braumann", "Alexander", ""], ["Krampe", "Jonas", ""], ["Kreiss", "Jens-Peter", ""], ["Paparoditis", "Efstathios", ""]]}, {"id": "2101.07934", "submitter": "Xinyue Qi", "authors": "Xinyue Qi, Shouhao Zhou, Yucai Wang, Michael L. Wang, Chan Shen", "title": "Bayesian Meta-analysis of Rare Events with Non-ignorable Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis is a powerful tool for drug safety assessment by synthesizing\ntreatment-related toxicological findings from independent clinical trials.\nHowever, published clinical studies may or may not report all adverse events\n(AEs) if the observed number of AEs were fewer than a pre-specified\nstudy-dependent cutoff. Subsequently, with censored information ignored, the\nestimated incidence rate of AEs could be significantly biased. To address this\nnon-ignorable missing data problem in meta-analysis, we propose a Bayesian\nmultilevel regression model to accommodate the censored rare event data. The\nperformance of the proposed Bayesian model of censored data compared to other\nexisting methods is demonstrated through simulation studies under various\ncensoring scenarios. Finally, the proposed approach is illustrated using data\nfrom a recent meta-analysis of 125 clinical trials involving PD-1/PD-L1\ninhibitors with respect to their toxicity profiles.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:30:56 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Qi", "Xinyue", ""], ["Zhou", "Shouhao", ""], ["Wang", "Yucai", ""], ["Wang", "Michael L.", ""], ["Shen", "Chan", ""]]}, {"id": "2101.07954", "submitter": "Lauren J Beesley", "authors": "Lauren J Beesley and Jeremy M G Taylor", "title": "Accounting for not-at-random missingness through imputation stacking", "comments": "See also: Supplementary Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Not-at-random missingness presents a challenge in addressing missing data in\nmany health research applications. In this paper, we propose a new approach to\naccount for not-at-random missingness after multiple imputation through\nweighted analysis of stacked multiple imputations. The weights are easily\ncalculated as a function of the imputed data and assumptions about the\nnot-at-random missingness. We demonstrate through simulation that the proposed\nmethod has excellent performance when the missingness model is correctly\nspecified. In practice, the missingness mechanism will not be known. We show\nhow we can use our approach in a sensitivity analysis framework to evaluate the\nrobustness of model inference to different assumptions about the missingness\nmechanism, and we provide R package StackImpute to facilitate implementation as\npart of routine sensitivity analyses. We apply the proposed method to account\nfor not-at-random missingness in human papillomavirus test results in a study\nof survival for patients diagnosed with oropharyngeal cancer.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:14:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Beesley", "Lauren J", ""], ["Taylor", "Jeremy M G", ""]]}, {"id": "2101.07997", "submitter": "Zhanlin Liu", "authors": "Zhanlin Liu and Youngjun Choe", "title": "Data-driven sparse polynomial chaos expansion for models with dependent\n  inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCEs) have been used in many real-world\nengineering applications to quantify how the uncertainty of an output is\npropagated from inputs. PCEs for models with independent inputs have been\nextensively explored in the literature. Recently, different approaches have\nbeen proposed for models with dependent inputs to expand the use of PCEs to\nmore real-world applications. Typical approaches include building PCEs based on\nthe Gram-Schmidt algorithm or transforming the dependent inputs into\nindependent inputs. However, the two approaches have their limitations\nregarding computational efficiency and additional assumptions about the input\ndistributions, respectively. In this paper, we propose a data-driven approach\nto build sparse PCEs for models with dependent inputs. The proposed algorithm\nrecursively constructs orthonormal polynomials using a set of monomials based\non their correlations with the output. The proposed algorithm on building\nsparse PCEs not only reduces the number of minimally required observations but\nalso improves the numerical stability and computational efficiency. Four\nnumerical examples are implemented to validate the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:06:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 01:44:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Liu", "Zhanlin", ""], ["Choe", "Youngjun", ""]]}, {"id": "2101.08007", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "On the Non-Monotonicity of a Non-Differentially Mismeasured Binary\n  Confounder", "comments": "arXiv admin note: text overlap with arXiv:2005.13245", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are interested in the average causal effect of a binary\ntreatment on an outcome when this relationship is confounded by a binary\nconfounder. Suppose that the confounder is unobserved but a non-differential\nbinary proxy of it is observed. We identify conditions under which adjusting\nfor the proxy comes closer to the incomputable true average causal effect than\nnot adjusting at all. Unlike other works, we do not assume that the average\ncausal effect of the confounder on the outcome is in the same direction among\ntreated and untreated.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:42:54 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 16:04:50 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 09:02:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2101.08083", "submitter": "Marouane Il Idrissi", "authors": "Marouane Il Idrissi, Vincent Chabridon, Bertrand Iooss", "title": "Developments and applications of Shapley effects to reliability-oriented\n  sensitivity analysis with correlated inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-oriented sensitivity analysis methods have been developed for\nunderstanding the influence of model inputs relative to events which\ncharacterize the failure of a system (e.g., a threshold exceedance of the model\noutput). In this field, the target sensitivity analysis focuses primarily on\ncapturing the influence of the inputs on the occurrence of such a critical\nevent. This paper proposes new target sensitivity indices, based on the Shapley\nvalues and called \"target Shapley effects\", allowing for interpretable\nsensitivity measures under dependent inputs. Two algorithms (one based on Monte\nCarlo sampling, and a given-data algorithm based on a nearest-neighbors\nprocedure) are proposed for the estimation of these target Shapley effects\nbased on the $\\ell^2$ norm. Additionally, the behavior of these target Shapley\neffects are theoretically and empirically studied through various toy-cases.\nFinally, the application of these new indices in two real-world use-cases (a\nriver flood model and a COVID-19 epidemiological model) is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:39:24 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:01:07 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:43:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Idrissi", "Marouane Il", ""], ["Chabridon", "Vincent", ""], ["Iooss", "Bertrand", ""]]}, {"id": "2101.08175", "submitter": "Silvia Montagna", "authors": "Patric Dolmeta, Raffaele Argiento, Silvia Montagna", "title": "Bayesian GARCH Modeling of Functional Sports Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of statistical methods in sport analytics has gained a rapidly\ngrowing interest over the last decade, and nowadays is common practice. In\nparticular, the interest in understanding and predicting an athlete's\nperformance throughout his/her career is motivated by the need to evaluate the\nefficacy of training programs, anticipate fatigue to prevent injuries and\ndetect unexpected of disproportionate increases in performance that might be\nindicative of doping. Moreover, fast evolving data gathering technologies\nrequire up to date modelling techniques that adapt to the distinctive features\nof sports data. In this work, we propose a hierarchical Bayesian model for\ndescribing and predicting the evolution of performance over time for shot put\nathletes. To account for seasonality and heterogeneity in recorded results, we\nrely both on a smooth functional contribution and on a linear mixed effect\nmodel with heteroskedastic errors to represent the athlete-specific\ntrajectories. The resulting model provides an accurate description of the\nperformance trajectories and helps specifying both the intra- and\ninter-seasonal variability of measurements. Further, the model allows for the\nprediction of athletes' performance in future seasons. We apply our model to an\nextensive real world data set on performance data of professional shot put\nathletes recorded at elite competitions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:16:10 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Dolmeta", "Patric", ""], ["Argiento", "Raffaele", ""], ["Montagna", "Silvia", ""]]}, {"id": "2101.08188", "submitter": "Vartan Choulakian", "authors": "Vartan Choulakian and Jacques Allard (Universit\\'e de Moncton, Canada)", "title": "Uncovering and Displaying the Coherent Groups of Rank Data by\n  Exploratory Riffle Shuffling", "comments": "34 pages, 12 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let n respondents rank order d items, and suppose that d << n. Our main task\nis to uncover and display the structure of the observed rank data by an\nexploratory riffle shuffling procedure which sequentially decomposes the n\nvoters into a finite number of coherent groups plus a noisy group : where the\nnoisy group represents the outlier voters and each coherent group is composed\nof a finite number of coherent clusters. We consider exploratory riffle\nshuffling of a set of items to be equivalent to optimal two blocks seriation of\nthe items with crossing of some scores between the two blocks. A riffle\nshuffled coherent cluster of voters within its coherent group is essentially\ncharacterized by the following facts : a) Voters have identical first TCA\nfactor score, where TCA designates taxicab correspondence analysis, an L1\nvariant of correspondence analysis ; b) Any preference is easily interpreted as\nriffle shuffling of its items ; c) The nature of different riffle shuffling of\nitems can be seen in the structure of the contingency table of the first-order\nmarginals constructed from the Borda scorings of the voters ; d) The first TCA\nfactor scores of the items of a coherent cluster are interpreted as Borda scale\nof the items. We also introduce a crossing index, which measures the extent of\ncrossing of scores of voters between the two blocks seriation of the items. The\nnovel approach is explained on the benchmarking SUSHI data set, where we show\nthat this data set has a very simple structure, which can also be communicated\nin a tabular form.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:34:33 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Choulakian", "Vartan", "", "Universit\u00e9 de Moncton, Canada"], ["Allard", "Jacques", "", "Universit\u00e9 de Moncton, Canada"]]}, {"id": "2101.08224", "submitter": "Lucas Kook", "authors": "Lucas Kook, Beate Sick, Peter B\\\"uhlmann", "title": "Distributional Anchor Regression", "comments": "22 pages, 9 figures, 2 tables in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prediction models often fail if train and test data do not stem from the same\ndistribution. Out-of-distribution (OOD) generalization to unseen, perturbed\ntest data is a desirable but difficult-to-achieve property for prediction\nmodels and in general requires strong assumptions on the data generating\nprocess (DGP). In a causally inspired perspective on OOD generalization, the\ntest data arise from a specific class of interventions on exogenous random\nvariables of the DGP, called anchors. Anchor regression models, introduced by\nRothenh\\\"ausler et al. (2018), protect against distributional shifts in the\ntest data by employing causal regularization. However, so far anchor regression\nhas only been used with a squared-error loss which is inapplicable to common\nresponses such as censored continuous or ordinal data. Here, we propose a\ndistributional version of anchor regression which generalizes the method to\npotentially censored responses with at least an ordered sample space. To this\nend, we combine a flexible class of parametric transformation models for\ndistributional regression with an appropriate causal regularizer under a more\ngeneral notion of residuals. In an exemplary application and several simulation\nscenarios we demonstrate the extent to which OOD generalization is possible.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 17:32:37 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 09:46:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kook", "Lucas", ""], ["Sick", "Beate", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.08365", "submitter": "Sobom Matthieu Som\\'e", "authors": "C\\'elestin C. Kokonendji and Sobom M. Som\\'e", "title": "Bayesian Bandwidths in Semiparametric Modelling for Nonnegative Orthant\n  Data with Diagnostics", "comments": "33 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multivariate nonnegative orthant data are real vectors bounded to the left by\nthe null vector, and they can be continuous, discrete or mixed. We first review\nthe recent relative variability indexes for multivariate nonnegative continuous\nand count distributions. As a prelude, the classification of two comparable\ndistributions having the same mean vector is done through under-, equi- and\nover-variability with respect to the reference distribution. Multivariate\nassociated kernel estimators are then reviewed with new proposals that can\naccommodate any nonnegative orthant dataset. We focus on bandwidth matrix\nselections by adaptive and local Bayesian methods for semicontinuous and\ncounting supports, respectively. We finally introduce a flexible semiparametric\napproach for estimating all these distributions on nonnegative supports. The\ncorresponding estimator is directed by a given parametric part, and a\nnonparametric part which is a weight function to be estimated through\nmultivariate associated kernels. A diagnostic model is also discussed to make\nan appropriate choice between the parametric, semiparametric and nonparametric\napproaches. The retention of pure nonparametric means the inconvenience of\nparametric part used in the modelization. Multivariate real data examples in\nsemicontinuous setup as reliability are gradually considered to illustrate the\nproposed approach. Concluding remarks are made for extension to other multiple\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:54:29 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Kokonendji", "C\u00e9lestin C.", ""], ["Som\u00e9", "Sobom M.", ""]]}, {"id": "2101.08436", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Aaron J. Molstad", "title": "Mixed-type multivariate response regression with covariance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for multivariate response regressions where the\nelements of the response vector can be of mixed types, for example some\ncontinuous and some discrete. Our method is based on a model which assumes the\nobservable mixed-type response vector is connected to a latent multivariate\nnormal response linear regression through a link function. We explore the\nproperties of this model and show its parameters are identifiable under\nreasonable conditions. We impose no parametric restrictions on the covariance\nof the latent normal other than positive definiteness, thereby avoiding\nassumptions about unobservable variables which can be difficult to verify. To\naccommodate this generality, we propose a novel algorithm for approximate\nmaximum likelihood estimation that works \"off-the-shelf\" with many different\ncombinations of response types, and which scales well in the dimension of the\nresponse vector. Our method typically gives better predictions and parameter\nestimates than fitting separate models for the different response types and\nallows for approximate likelihood ratio testing of relevant hypotheses such as\nindependence of responses. The usefulness of the proposed method is illustrated\nin simulations; and one biomedical and one genomic data example.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 04:48:44 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:42:45 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Molstad", "Aaron J.", ""]]}, {"id": "2101.08505", "submitter": "YunPeng Li", "authors": "YunPeng Li, ZhaoHui Ye", "title": "Boosting in Univariate Nonparametric Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3065881", "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric maximum likelihood estimation is intended to infer the unknown\ndensity distribution while making as few assumptions as possible. To alleviate\nthe over parameterization in nonparametric data fitting, smoothing assumptions\nare usually merged into the estimation. In this paper a novel boosting-based\nmethod is introduced to the nonparametric estimation in univariate cases. We\ndeduce the boosting algorithm by the second-order approximation of\nnonparametric log-likelihood. Gaussian kernel and smooth spline are chosen as\nweak learners in boosting to satisfy the smoothing assumptions. Simulations and\nreal data experiments demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:46:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "YunPeng", ""], ["Ye", "ZhaoHui", ""]]}, {"id": "2101.08590", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph and Ivan Diaz", "title": "When the ends don't justify the means: Learning a treatment strategy to\n  prevent harmful indirect effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing literature on finding so-called optimal treatment rules,\nwhich are rules by which to assign treatment to individuals based on an\nindividual's characteristics, such that a desired outcome is maximized. A\nrelated goal entails identifying individuals who are predicted to have a\nharmful indirect effect (the effect of treatment on an outcome through\nmediators) even in the presence of an overall beneficial effect of the\ntreatment on the outcome. In some cases, the likelihood of a harmful indirect\neffect may outweigh a likely beneficial overall effect, and would be reason to\ncaution against treatment for indicated individuals. We build on both the\ncurrent mediation and optimal treatment rule literature to propose a method of\nidentifying a subgroup for which the treatment effect through the mediator is\nharmful. Our approach is nonparametric, incorporates post-treatment variables\nthat may confound the mediator-outcome relationship, and does not make\nrestrictions on the distribution of baseline covariates, mediating variables\n(considered jointly), or outcomes. We apply the proposed approach to identify a\nsubgroup of boys in the Moving to Opportunity housing voucher experiment who\nare predicted to have harmful indirect effects, though the average total effect\nis beneficial.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:05:16 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Diaz", "Ivan", ""]]}, {"id": "2101.08608", "submitter": "Hana Sulieman", "authors": "Hana Sulieman", "title": "Improving D-Optimality in Nonlinear Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Experimental designs based on the classical D-optimal criterion minimize the\nvolume of the linear-approximation inference regions for the parameters using\nlocal sensitivity coefficients. For nonlinear models, these designs can be\nunreliable because the linearized inference regions do not always provide a\ntrue indication of the exact parameter inference regions. In this article, we\napply the profile-based sensitivity coefficients developed by Sulieman et.al.\n[12] in designing D-optimal experiments for parameter estimation in some\nselected nonlinear models. Profile-based sensitivity coefficients are defined\nby the total derivative of the model function with respect to the parameters.\nThey have been shown to account for both parameter co-dependencies and model\nnonlinearity up to second order-derivative. This work represents a first\nattempt to construct experiments using profile-based sensitivity coefficients.\nTwo common nonlinear models are used to illustrate the computational aspects of\nthe profile-based designs and simulation studies are conducted to demonstrate\nthe efficiency of the constructed experiments.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:52:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Sulieman", "Hana", ""]]}, {"id": "2101.08639", "submitter": "Xiaoyu Ma", "authors": "Xiaoyu Ma, Lu Lin, Yujie Gai", "title": "A General Framework of Online Updating Variable Selection for\n  Generalized Linear Models with Streaming Datasets", "comments": "35 pages, 2 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the research field of big data, one of important issues is how to recover\nthe sequentially changing sets of true features when the data sets arrive\nsequentially. The paper presents a general framework for online updating\nvariable selection and parameter estimation in generalized linear models with\nstreaming datasets. This is a type of online updating penalty likelihoods with\ndifferentiable or non-differentiable penalty function. The online updating\ncoordinate descent algorithm is proposed to solve the online updating\noptimization problem. Moreover, a tuning parameter selection is suggested in an\nonline updating way. The selection and estimation consistencies, and the oracle\nproperty are established, theoretically. Our methods are further examined and\nillustrated by various numerical examples from both simulation experiments and\na real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:35:22 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ma", "Xiaoyu", ""], ["Lin", "Lu", ""], ["Gai", "Yujie", ""]]}, {"id": "2101.08765", "submitter": "Shulei Wang", "authors": "Shulei Wang", "title": "Robust Differential Abundance Test in Compositional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential abundance tests in compositional data are essential and\nfundamental tasks in various biomedical applications, such as single-cell, bulk\nRNA-seq, and microbiome data analysis. However, despite the recent developments\nin these fields, differential abundance analysis in compositional data remains\na complicated and unsolved statistical problem, because of the compositional\nconstraint and prevalent zero counts in the dataset. This study introduces a\nnew differential abundance test, the robust differential abundance (RDB) test,\nto address these challenges. Compared with existing methods, the RDB test 1) is\nsimple and computationally efficient, 2) is robust to prevalent zero counts in\ncompositional datasets, 3) can take the data's compositional nature into\naccount, and 4) has a theoretical guarantee of controlling false discoveries in\na general setting. Furthermore, in the presence of observed covariates, the RDB\ntest can work with the covariate balancing techniques to remove the potential\nconfounding effects and draw reliable conclusions. Finally, we apply the new\ntest to several numerical examples using simulated and real datasets to\ndemonstrate its practical merits.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:37:24 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:27:04 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Shulei", ""]]}, {"id": "2101.08954", "submitter": "Yuling Yao", "authors": "Yuling Yao, Gregor Pir\\v{s}, Aki Vehtari, Andrew Gelman", "title": "Bayesian hierarchical stacking: Some models are (somewhere) useful", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking is a widely used model averaging technique that asymptotically\nyields optimal predictions among linear averages. We show that stacking is most\neffective when model predictive performance is heterogeneous in inputs, and we\ncan further improve the stacked mixture with a hierarchical model. We\ngeneralize stacking to Bayesian hierarchical stacking. The model weights are\nvarying as a function of data, partially-pooled, and inferred using Bayesian\ninference. We further incorporate discrete and continuous inputs, other\nstructured priors, and time series and longitudinal data. To verify the\nperformance gain of the proposed method, we derive theory bounds, and\ndemonstrate on several applied problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 05:19:49 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:14:30 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yao", "Yuling", ""], ["Pir\u0161", "Gregor", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2101.09039", "submitter": "Matteo Pegoraro", "authors": "Matteo Pegoraro and Mario Beraha", "title": "Projected Statistical Methods for Distributional Data on the Real Line\n  with the Wasserstein Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel class of projected methods, to perform statistical\nanalysis on a data set of probability distributions on the real line, with the\n2-Wasserstein metric. We focus in particular on Principal Component Analysis\n(PCA) and regression. To define these models, we exploit a representation of\nthe Wasserstein space closely related to its weak Riemannian structure, by\nmapping the data to a suitable linear space and using a metric projection\noperator to constrain the results in the Wasserstein space. By carefully\nchoosing the tangent point, we are able to derive fast empirical methods,\nexploiting a constrained B-spline approximation. As a byproduct of our\napproach, we are also able to derive faster routines for previous work on PCA\nfor distributions. By means of simulation studies, we compare our approaches to\npreviously proposed methods, showing that our projected PCA has similar\nperformance for a fraction of the computational cost and that the projected\nregression is extremely flexible even under misspecification. Several\ntheoretical properties of the models are investigated and asymptotic\nconsistency is proven. Two real world applications to Covid-19 mortality in the\nUS and wind speed forecasting are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:24:49 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Pegoraro", "Matteo", ""], ["Beraha", "Mario", ""]]}, {"id": "2101.09053", "submitter": "Jaromir Antoch", "authors": "Jaromir Antoch, Francesco Mola, Ondrej Vozar", "title": "New randomized response technique for estimating the population total of\n  a quantitative variable", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new randomized response technique aimed at protecting\nrespondents' privacy is proposed. It is designed for estimating the population\ntotal, or the population mean, of a quantitative characteristic. It provides\na~high degree of protection to the interviewed individuals, hence it may be\nfavorably perceived by them and increase their willingness to cooperate.\nInstead of revealing the true value of the characteristic under investigation,\na respondent only states whether the value is greater (or smaller) than\na~number which is selected by him/her at random, and is unknown to the\ninterviewer. For each respondent this number, a sort of individual threshold,\nis generated as a pseudorandom number from the uniform distribution. Further,\ntwo modifications of the proposed technique are presented. The first\nmodification assumes that the interviewer also knows the generated random\nnumber. The second modification deals with the issue that, for certain\nvariables, such as income, it may be embarrassing for the respondents to report\neither high or low values. Thus, depending on the value of the pseudorandom\nlower bound, the respondent is asked different questions to avoid being\nembarrassed. The suggested approach is applied in detail to the simple random\nsampling without replacement, but it can also be applied to many currently used\nsampling schemes, including cluster sampling, two-stage sampling, etc. Results\nof simulations illustrate the behavior of the proposed procedure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:11:23 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Antoch", "Jaromir", ""], ["Mola", "Francesco", ""], ["Vozar", "Ondrej", ""]]}, {"id": "2101.09109", "submitter": "Hisashi Kobayashi", "authors": "Hisashi Kobayashi", "title": "Stochastic Modeling of an Infectious Disease Part III-A: Analysis of\n  Time-Nonhomogeneous Models", "comments": "35 figures. An earlier version was presented at ITC-32 as a keynote\n  on September 23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our BDI (birth-death-immigration) process based stochastic model of\nan infectious disease to time-nonhomogeneous cases. First, we discuss the\ndeterministic model, and derive the expected value of the infection process.\nThen as an application we consider that a government issues a decree to its\ncitizens to curtail their activities that may incur further infections and show\nhow the public's tardy response may further increase infections and prolong the\nepidemic much longer than one might think. We seek to solve a partial\ndifferential equation for the probability generating function. We find,\nhowever, that an exact solution is obtainable only for the BD process, i.e., no\narrivals of the infected from outside. The coefficient of variation for the\nnonhomogeneous BD process is found to be well over unity. This result implies\nthat the variations among different sample paths will be as large as in the\nnegative binomial distribution with r<1, which was found in Part I for the\nhomogeneous BDI model. In the final section, we illustrate, using our running\nexample, how much information we can derive from the time dependent PMF\n(probability mass function) P_k(t)=Pr[I(t)=k]. We present graphical plots of\nthe PMF at various t's, and cross-sections of this function at various k's. A\nmesh plot of the function over the (k, t) plane summarizes the above numerous\nplots. The results of this paper reinforce our earlier claim (see Abstract of\nPart II) that it would be a futile effort to attempt to identify all possible\nreasons why environments of similar situations differ so much in their epidemic\npatterns. Mere \"luck\" plays a more significant role than most of us may\nbelieve. We should be prepared for a worse possible scenario, which only a\nstochastic model can provide with probabilistic qualification. An empirical\nvalidation of the above results will be given in Part III-B.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:57:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Kobayashi", "Hisashi", ""]]}, {"id": "2101.09110", "submitter": "Erica Ponzi", "authors": "Erica Ponzi, Magne Thoresen, Abhik Ghosh", "title": "RaJIVE: Robust Angle Based JIVE for Integrating Noisy Multi-Source Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing availability of high dimensional, multi-source data, the\nidentification of joint and data specific patterns of variability has become a\nsubject of interest in many research areas. Several matrix decomposition\nmethods have been formulated for this purpose, for example JIVE (Joint and\nIndividual Variation Explained), and its angle based variation, aJIVE. Although\nthe effect of data contamination on the estimated joint and individual\ncomponents has not been considered in the literature, gross errors and outliers\nin the data can cause instability in such methods, and lead to incorrect\nestimation of joint and individual variance components. We focus on the aJIVE\nfactorization method and provide a thorough analysis of the effect outliers on\nthe resulting variation decomposition. After showing that such effect is not\nnegligible when all data-sources are contaminated, we propose a robust\nextension of aJIVE (RaJIVE) that integrates a robust formulation of the\nsingular value decomposition into the aJIVE approach. The proposed RaJIVE is\nshown to provide correct decompositions even in the presence of outliers and\nimproves the performance of aJIVE. We use extensive simulation studies with\ndifferent levels of data contamination to compare the two methods. Finally, we\ndescribe an application of RaJIVE to a multi-omics breast cancer dataset from\nThe Cancer Genome Atlas. We provide the R package RaJIVE with a ready-to-use\nimplementation of the methods and documentation of code and examples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:02:15 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ponzi", "Erica", ""], ["Thoresen", "Magne", ""], ["Ghosh", "Abhik", ""]]}, {"id": "2101.09195", "submitter": "Xinran Li", "authors": "Devin Caughey, Allan Dafoe, Xinran Li, Luke Miratrix", "title": "Randomization Inference beyond the Sharp Null: Bounded Null Hypotheses\n  and Quantiles of Individual Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomization (a.k.a. permutation) inference is typically interpreted as\ntesting Fisher's \"sharp\" null hypothesis that all effects are exactly zero.\nThis hypothesis is often criticized as uninteresting and implausible. We show,\nhowever, that many randomization tests are also valid for a \"bounded\" null\nhypothesis under which effects are all negative (or positive) for all units but\notherwise heterogeneous. The bounded null is closely related to important\nconcepts such as monotonicity and Pareto efficiency. Inverting tests of this\nhypothesis yields confidence intervals for the maximum (or minimum) individual\ntreatment effect. We then extend randomization tests to infer other quantiles\nof individual effects, which can be used to infer the proportion of units with\neffects larger (or smaller) than any threshold. The proposed confidence\nintervals for all quantiles of individual effects are simultaneously valid, in\nthe sense that no correction due to multiple analyses is needed. In sum, we\nprovide a broader justification for Fisher randomization tests, and develop\nexact nonparametric inference for quantiles of heterogeneous individual\neffects. We illustrate our methods with simulations and applications, where we\nfind that Stephenson rank statistics often provide the most informative\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:39:06 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Caughey", "Devin", ""], ["Dafoe", "Allan", ""], ["Li", "Xinran", ""], ["Miratrix", "Luke", ""]]}, {"id": "2101.09197", "submitter": "Jennifer Pohle", "authors": "Jennifer Pohle, Timo Adam and Larissa T. Beumer", "title": "Flexible estimation of the state dwell-time distribution in hidden\n  semi-Markov models", "comments": "Main manuscript with 16 pages and supplementary material with 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden semi-Markov models generalise hidden Markov models by explicitly\nmodelling the time spent in a given state, the so-called dwell time, using some\ndistribution defined on the natural numbers. While the (shifted) Poisson and\nnegative binomial distribution provide natural choices for such distributions,\nin practice, parametric distributions can lack the flexibility to adequately\nmodel the dwell times. To overcome this problem, a penalised maximum likelihood\napproach is proposed that allows for a flexible and data-driven estimation of\nthe dwell-time distributions without the need to make any distributional\nassumption. This approach is suitable for direct modelling purposes or as an\nexploratory tool to investigate the latent state dynamics. The feasibility and\npotential of the suggested approach is illustrated by modelling muskox\nmovements in northeast Greenland using GPS tracking data. The proposed method\nis implemented in the R-package PHSMM which is available on CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:41:02 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 08:19:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pohle", "Jennifer", ""], ["Adam", "Timo", ""], ["Beumer", "Larissa T.", ""]]}, {"id": "2101.09233", "submitter": "Andrew Spieker", "authors": "Andrew J. Spieker, Joseph A.C. Delaney, Robyn L. McClelland", "title": "Semi-parametric estimation of biomarker age trends with endogenous\n  medication use in longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cohort studies, non-random medication use can pose barriers to estimation\nof the natural history trend in a mean biomarker value (namely, the association\nbetween a predictor of interest and a biomarker outcome that would be observed\nin the absence of biomarker-specific treatment). Common causes of treatment and\noutcomes are often unmeasured, obscuring our ability to easily account for\nmedication use with commonly invoked assumptions such as ignorability. Further,\nabsent some variable satisfying the exclusion restriction, use of instrumental\nvariable approaches may be difficult to justify. Heckman's hybrid model with\nstructural shift (sometimes referred to less specifically as the treatment\neffects model) can be used to correct endogeneity bias via a homogeneity\nassumption (i.e., that average treatment effects do not vary across covariates)\nand parametric specification of a joint model for the outcome and treatment. In\nrecent work, we relaxed the homogeneity assumption by allowing observed\ncovariates to serve as treatment effect modifiers. While this method has been\nshown to be reasonably robust in settings of cross-sectional data, application\nof this methodology to settings of longitudinal data remains unexplored. We\ndemonstrate how the assumptions of the treatment effects model can be extended\nto accommodate clustered data arising from longitudinal studies. Our proposed\napproach is semi-parametric in nature in that valid inference can be obtained\nwithout the need to specify the longitudinal correlation structure. As an\nillustrative example, we use data from the Multi-Ethnic Study of\nAtherosclerosis to evaluate trends in low-density lipoprotein by age and\ngender. We confirm that our generalization of the treatment effects model can\nserve as a useful tool to uncover natural history trends in longitudinal data\nthat are obscured by endogenous treatment.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:36:49 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Spieker", "Andrew J.", ""], ["Delaney", "Joseph A. C.", ""], ["McClelland", "Robyn L.", ""]]}, {"id": "2101.09271", "submitter": "Eliana Duarte", "authors": "Eliana Duarte, Liam Solus", "title": "Representation of Context-Specific Causal Models with Observational and\n  Interventional Data", "comments": "26 pages, supplementary material 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of representing causal models that encode\ncontext-specific information for discrete data. To represent such models we use\na proper subclass of staged tree models which we call CStrees. We show that the\ncontext-specific information encoded by a CStree can be equivalently expressed\nvia a collection of DAGs. As not all staged tree models admit this property,\nCStrees are a subclass that provides a transparent, intuitive and compact\nrepresentation of context-specific causal information. Model equivalence for\nCStrees also takes a simpler form than for general staged trees: We provide a\ncharacterization of the complete set of asymmetric conditional independence\nrelations encoded by a CStree. As a consequence, we obtain a global Markov\nproperty for CStrees which leads to a graphical criterion of model equivalence\nfor CStrees generalizing that of Verma and Pearl for DAG models. In addition,\nwe provide a closed-form formula for the maximum likelihood estimator of a\nCStree and use it to show that the Bayesian information criterion is a locally\nconsistent score function for this model class. We also give an analogous\nglobal Markov property and characterization of model equivalence for general\ninterventions in CStrees. As examples, we apply these results to two real data\nsets, and examine how BIC-optimal CStrees for each provide a clear and concise\nrepresentation of the learned context-specific causal structure.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:48:29 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 17:10:20 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Duarte", "Eliana", ""], ["Solus", "Liam", ""]]}, {"id": "2101.09304", "submitter": "Serge Aleshin-Guendel", "authors": "Serge Aleshin-Guendel, Mauricio Sadinle, Jon Wakefield", "title": "Revisiting Identifying Assumptions for Population Size Estimation", "comments": "48 pages. The material presented in Appendix A previously appeared in\n  an unpublished preprint written by the first author: arXiv:2008.09865", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the size of a population based on a subset of\nindividuals observed across multiple data sources is often referred to as\ncapture-recapture or multiple-systems estimation. This is fundamentally a\nmissing data problem, where the number of unobserved individuals represents the\nmissing data. As with any missing data problem, multiple-systems estimation\nrequires users to make an untestable identifying assumption in order to\nestimate the population size from the observed data. Approaches to\nmultiple-systems estimation often do not emphasize the role of the identifying\nassumption during model specification, which makes it difficult to decouple the\nspecification of the model for the observed data from the identifying\nassumption. We present a re-framing of the multiple-systems estimation problem\nthat decouples the specification of the observed-data model from the\nidentifying assumptions, and discuss how log-linear models and the associated\nno-highest-order interaction assumption fit into this framing. We present an\napproach to computation in the Bayesian setting which takes advantage of\nexisting software and facilitates various sensitivity analyses. We demonstrate\nour approach in a case study of estimating the number of civilian casualties in\nthe Kosovo war. Code used to produce this manuscript is available at\nhttps://github.com/aleshing/revisiting-identifying-assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:30:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Aleshin-Guendel", "Serge", ""], ["Sadinle", "Mauricio", ""], ["Wakefield", "Jon", ""]]}, {"id": "2101.09352", "submitter": "Raphael Huser", "authors": "Matheus B. Guerrero, Rapha\\\"el Huser and Hernando Ombao", "title": "Conex-Connect: Learning Patterns in Extremal Brain Connectivity From\n  Multi-Channel EEG Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is a chronic neurological disorder affecting more than 50 million\npeople globally. An epileptic seizure acts like a temporary shock to the\nneuronal system, disrupting normal electrical activity in the brain. Epilepsy\nis frequently diagnosed with electroencephalograms (EEGs). Current methods\nstudy the time-varying spectra and coherence but do not directly model changes\nin extreme behavior. Thus, we propose a new approach to characterize brain\nconnectivity based on the joint tail behavior of the EEGs. Our proposed method,\nthe conditional extremal dependence for brain connectivity (Conex-Connect), is\na pioneering approach that links the association between extreme values of\nhigher oscillations at a reference channel with the other brain network\nchannels. Using the Conex-Connect method, we discover changes in the extremal\ndependence driven by the activity at the foci of the epileptic seizure. Our\nmodel-based approach reveals that, pre-seizure, the dependence is notably\nstable for all channels when conditioning on extreme values of the focal\nseizure area. Post-seizure, by contrast, the dependence between channels is\nweaker, and dependence patterns are more \"chaotic\". Moreover, in terms of\nspectral decomposition, we find that high values of the high-frequency\nGamma-band are the most relevant features to explain the conditional extremal\ndependence of brain connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:53:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Guerrero", "Matheus B.", ""], ["Huser", "Rapha\u00ebl", ""], ["Ombao", "Hernando", ""]]}, {"id": "2101.09398", "submitter": "Jann Spiess", "authors": "Lea Bottmer, Guido Imbens, Jann Spiess, Merrill Warnick", "title": "A Design-Based Perspective on Synthetic Control Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since their introduction in Abadie and Gardeazabal (2003), Synthetic Control\n(SC) methods have quickly become one of the leading methods for estimating\ncausal effects in observational studies with panel data. Formal discussions\noften motivate SC methods by the assumption that the potential outcomes were\ngenerated by a factor model. Here we study SC methods from a design-based\nperspective, assuming a model for the selection of the treated unit(s), e.g.,\nrandom selection as guaranteed in a randomized experiment. We show that SC\nmethods offer benefits even in settings with randomized assignment, and that\nthe design perspective offers new insights into SC methods for observational\ndata. A first insight is that the standard SC estimator is not unbiased under\nrandom assignment. We propose a simple modification of the SC estimator that\nguarantees unbiasedness in this setting and derive its exact,\nrandomization-based, finite sample variance. We also propose an unbiased\nestimator for this variance. We show in settings with real data that under\nrandom assignment this Modified Unbiased Synthetic Control (MUSC) estimator can\nhave a root mean-squared error (RMSE) that is substantially lower than that of\nthe difference-in-means estimator. We show that such an improvement is weakly\nguaranteed if the treated period is similar to the other periods, for example,\nif the treated period was randomly selected. The improvement is most likely to\nbe substantial if the number of pre-treatment periods is large relative to the\nnumber of control units.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:57:45 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bottmer", "Lea", ""], ["Imbens", "Guido", ""], ["Spiess", "Jann", ""], ["Warnick", "Merrill", ""]]}, {"id": "2101.09424", "submitter": "Zezhong Wang", "authors": "Zezhong Wang and Inez Maria Zwetsloot", "title": "A Change-Point Based Control Chart for Detecting Sparse Changes in\n  High-Dimensional Heteroscedastic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the curse-of-dimensionality, high-dimensional processes present\nchallenges to traditional multivariate statistical process monitoring (SPM)\ntechniques. In addition, the unknown underlying distribution and complicated\ndependency among variables such as heteroscedasticity increase uncertainty of\nestimated parameters, and decrease the effectiveness of control charts. In\naddition, the requirement of sufficient reference samples limits the\napplication of traditional charts in high dimension low sample size scenarios\n(small n, large p). More difficulties appear in detecting and diagnosing\nabnormal behaviors that are caused by a small set of variables, i.e., sparse\nchanges. In this article, we propose a changepoint based control chart to\ndetect sparse shifts in the mean vector of high-dimensional heteroscedastic\nprocesses. Our proposed method can start monitoring when the number of\nobservations is a lot smaller than the dimensionality. The simulation results\nshow its robustness to nonnormality and heteroscedasticity. A real data example\nis used to illustrate the effectiveness of the proposed control chart in\nhigh-dimensional applications. Supplementary material and code are provided\nonline.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:38:17 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Zezhong", ""], ["Zwetsloot", "Inez Maria", ""]]}, {"id": "2101.09587", "submitter": "Wenyi Wang", "authors": "Zeya Wang, Veera Baladandayuthapan, Ahmed O. Kaseb, Hesham M. Amin,\n  Manal M. Hassan, Wenyi Wang, Jeffrey S. Morris", "title": "Bayesian Edge Regression in Undirected Graphical Models to Characterize\n  Interpatient Heterogeneity in Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are commonly used to discover associations within gene or\nprotein networks for complex diseases such as cancer. Most existing methods\nestimate a single graph for a population, while in many cases, researchers are\ninterested in characterizing the heterogeneity of individual networks across\nsubjects with respect to subject-level covariates. Examples include assessments\nof how the network varies with patient-specific prognostic scores or\ncomparisons of tumor and normal graphs while accounting for tumor purity as a\ncontinuous predictor. In this paper, we propose a novel edge regression model\nfor undirected graphs, which estimates conditional dependencies as a function\nof subject-level covariates. Bayesian shrinkage algorithms are used to induce\nsparsity in the underlying graphical models. We assess our model performance\nthrough simulation studies focused on comparing tumor and normal graphs while\nadjusting for tumor purity and a case study assessing how blood protein\nnetworks in hepatocellular carcinoma patients vary with severity of disease,\nmeasured by HepatoScore, a novel biomarker signature measuring disease\nseverity.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 21:24:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Zeya", ""], ["Baladandayuthapan", "Veera", ""], ["Kaseb", "Ahmed O.", ""], ["Amin", "Hesham M.", ""], ["Hassan", "Manal M.", ""], ["Wang", "Wenyi", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "2101.09596", "submitter": "Wendy Chan", "authors": "Wendy Chan", "title": "The Role of Distributional Overlap on the Precision Gain of Bounds for\n  Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past ten years, propensity score methods have made an important\ncontribution to improving generalizations from studies that do not select\nsamples randomly from a population of inference. However, these methods require\nassumptions and recent work has considered the role of bounding approaches that\nprovide a range of treatment impact estimates that are consistent with the\nobservable data. An important limitation to bound estimates is that they can be\nuninformatively wide. This has motivated research on the use of propensity\nscore stratification to narrow bounds. This article assesses the role of\ndistributional overlap in propensity scores on the effectiveness of\nstratification to tighten bounds. Using the results of two simulation studies\nand two case studies, I evaluate the relationship between distributional\noverlap and precision gain and discuss the implications when propensity score\nstratification is used as a method to improve precision in the bounding\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 22:25:03 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chan", "Wendy", ""]]}, {"id": "2101.09605", "submitter": "Art Owen", "authors": "Dan M. Kluger and Art B. Owen", "title": "Local linear tie-breaker designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tie-breaker experimental designs are hybrids of Randomized Control Trials\n(RCTs) and Regression Discontinuity Designs (RDDs) in which subjects with\nmoderate scores are placed in an RCT while subjects with extreme scores are\ndeterministically assigned to the treatment or control group. The design\nmaintains the benefits of randomization for causal estimation while avoiding\nthe possibility of excluding the most deserving recipients from the treatment\ngroup. The causal effect estimator for a tie-breaker design can be estimated by\nfitting local linear regressions for both the treatment and control group, as\nis typically done for RDDs. We study the statistical efficiency of such local\nlinear regression-based causal estimators as a function of $\\Delta$, the radius\nof the interval in which treatment randomization occurs. In particular, we\ndetermine the efficiency of the estimator as a function of $\\Delta$ for a\nfixed, arbitrary bandwidth under the assumption of a uniform assignment\nvariable. To generalize beyond uniform assignment variables and asymptotic\nregimes, we also demonstrate on the Angrist and Lavy (1999) classroom size\ndataset that prior to conducting an experiment, an experimental designer can\nestimate the efficiency for various experimental radii choices by using Monte\nCarlo as long as they have access to the distribution of the assignment\nvariable. For both uniform and triangular kernels, we show that increasing the\nradius of randomized experiment interval will increase the efficiency until the\nradius is the size of the local-linear regression bandwidth, after which no\nadditional efficiency benefits are conferred.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:41:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kluger", "Dan M.", ""], ["Owen", "Art B.", ""]]}, {"id": "2101.10005", "submitter": "Yasin Memari", "authors": "Yasin Memari", "title": "Low incidence rate of COVID-19 undermines confidence in estimation of\n  the vaccine efficacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowing the true effect size of clinical interventions in randomised clinical\ntrials is key to informing the public health policies. Vaccine efficacy is\ndefined in terms of the relative risk or the ratio of two disease risks.\nHowever, only approximate methods are available for estimating the variance of\nthe relative risk. In this article, we show using a probabilistic model that\nuncertainty in the efficacy rate could be underestimated when the disease risk\nis low. Factoring in the baseline rate of the disease, we estimate broader\nconfidence intervals for the efficacy rates of the vaccines recently developed\nfor COVID-19. We propose new confidence intervals for the relative risk. We\nfurther show that sample sizes required for phase 3 efficacy trials are\nroutinely underestimated and propose a new method for sample size calculation\nwhere the efficacy is of interest. We also discuss the deleterious effects of\nclassification bias which is particularly relevant at low disease prevalence.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:45:42 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 14:51:26 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Memari", "Yasin", ""]]}, {"id": "2101.10058", "submitter": "Yikun Zhang", "authors": "Yikun Zhang, Yen-Chi Chen", "title": "The EM Perspective of Directional Mean Shift Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The directional mean shift (DMS) algorithm is a nonparametric method for\npursuing local modes of densities defined by kernel density estimators on the\nunit hypersphere. In this paper, we show that any DMS iteration can be viewed\nas a generalized Expectation-Maximization (EM) algorithm; in particular, when\nthe von Mises kernel is applied, it becomes an exact EM algorithm. Under the\n(generalized) EM framework, we provide a new proof for the ascending property\nof density estimates and demonstrate the global convergence of directional mean\nshift sequences. Finally, we give a new insight into the linear convergence of\nthe DMS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:17:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2101.10261", "submitter": "Youssef Aboutaleb", "authors": "Youssef M. Aboutaleb, Mazen Danaf, Yifei Xie, Moshe Ben-Akiva", "title": "Discrete Choice Analysis with Machine Learning Capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses capabilities that are essential to models applied in\npolicy analysis settings and the limitations of direct applications of\noff-the-shelf machine learning methodologies to such settings. Traditional\neconometric methodologies for building discrete choice models for policy\nanalysis involve combining data with modeling assumptions guided by\nsubject-matter considerations. Such considerations are typically most useful in\nspecifying the systematic component of random utility discrete choice models\nbut are typically of limited aid in determining the form of the random\ncomponent. We identify an area where machine learning paradigms can be\nleveraged, namely in specifying and systematically selecting the best\nspecification of the random component of the utility equations. We review two\nrecent novel applications where mixed-integer optimization and cross-validation\nare used to algorithmically select optimal specifications for the random\nutility components of nested logit and logit mixture models subject to\ninterpretability constraints.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:34:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Aboutaleb", "Youssef M.", ""], ["Danaf", "Mazen", ""], ["Xie", "Yifei", ""], ["Ben-Akiva", "Moshe", ""]]}, {"id": "2101.10373", "submitter": "Yuqi Gu", "authors": "Yuqi Gu, David B. Dunson", "title": "Identifying Interpretable Discrete Latent Structures from Discrete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional categorical data are routinely collected in biomedical and\nsocial sciences. It is of great importance to build interpretable models that\nperform dimension reduction and uncover meaningful latent structures from such\ndiscrete data. Identifiability is a fundamental requirement for valid modeling\nand inference in such scenarios, yet is challenging to address when there are\ncomplex latent structures. In this article, we propose a class of interpretable\ndiscrete latent structure models for discrete data and develop a general\nidentifiability theory. Our theory is applicable to various types of latent\nstructures, ranging from a single latent variable to deep layers of latent\nvariables organized in a sparse graph (termed a Bayesian pyramid). The proposed\nidentifiability conditions can ensure Bayesian posterior consistency under\nsuitable priors. As an illustration, we consider the two-latent-layer model and\npropose a Bayesian shrinkage estimation approach. Simulation results for this\nmodel corroborate identifiability and estimability of the model parameters.\nApplications of the methodology to DNA nucleotide sequence data uncover\ndiscrete latent features that are both interpretable and highly predictive of\nsequence types. The proposed framework provides a recipe for interpretable\nunsupervised learning of discrete data, and can be a useful alternative to\npopular machine learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:43:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gu", "Yuqi", ""], ["Dunson", "David B.", ""]]}, {"id": "2101.10466", "submitter": "Andrew Spieker", "authors": "Nicholas Illenberger, Nandita Mitra, Andrew J. Spieker", "title": "A regression framework for a probabilistic measure of cost-effectiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make informed health policy decisions regarding a treatment, we must\nconsider both its cost and its clinical effectiveness. In past work, we\nintroduced the net benefit separation (NBS) as a novel measure of\ncost-effectiveness. The NBS is a probabilistic measure that characterizes the\nextent to which a treated patient will be more likely to experience benefit as\ncompared to an untreated patient. Due to variation in treatment response across\npatients, uncovering factors that influence cost-effectiveness can assist\npolicy makers in population-level decisions regarding resource allocation. In\nthis paper, we introduce a regression framework for NBS in order to estimate\ncovariate-specific NBS and find determinants of variation in NBS. Our approach\nis able to accommodate informative cost censoring through inverse probability\nweighting techniques, and addresses confounding through a semiparametric\nstandardization procedure. Through simulations, we show that NBS regression\nperforms well in a variety of common scenarios. We apply our proposed\nregression procedure to a realistic simulated data set as an illustration of\nhow our approach could be used to investigate the association between cancer\nstage, comorbidities and cost-effectiveness when comparing adjuvant radiation\ntherapy and chemotherapy in post-hysterectomy endometrial cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 23:01:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Illenberger", "Nicholas", ""], ["Mitra", "Nandita", ""], ["Spieker", "Andrew J.", ""]]}, {"id": "2101.10515", "submitter": "Rui Zhang", "authors": "Rui Zhang, Junting Chen, Yao Xie, Alexander Shapiro, Urbashi Mitra", "title": "Testing Rank of Incomplete Unimodal Matrices", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3070524", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistics-based detectors, based on unimodal matrix models, for\ndetermining the number of sources in a field are designed. A new variance ratio\nstatistic is proposed, and its asymptotic distribution is analyzed. The\nvariance ratio detector is shown to outperform the alternatives. It is shown\nthat further improvements are achievable via optimally selected rotations.\nNumerical experiments demonstrate the performance gains of our detection\nmethods over the baseline approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:52:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhang", "Rui", ""], ["Chen", "Junting", ""], ["Xie", "Yao", ""], ["Shapiro", "Alexander", ""], ["Mitra", "Urbashi", ""]]}, {"id": "2101.10671", "submitter": "Stefano Fortunati", "authors": "Stefano Fortunati, Alexandre Renaux, Fr\\'ed\\'eric Pascal", "title": "Joint Estimation of Location and Scatter in Complex Elliptical\n  Distributions: A robust semiparametric and computationally efficient\n  $R$-estimator of the shape matrix", "comments": "This paper has been submitted to the Special Issue (related to MLSP)\n  of the Journal of Signal Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint estimation of the location vector and the shape matrix of a set of\nindependent and identically Complex Elliptically Symmetric (CES) distributed\nobservations is investigated from both the theoretical and computational\nviewpoints. This joint estimation problem is framed in the original context of\nsemiparametric models allowing us to handle the (generally unknown) density\ngenerator as an \\textit{infinite-dimensional} nuisance parameter. In the first\npart of the paper, a computationally efficient and memory saving implementation\nof the robust and semiparmaetric efficient $R$-estimator for shape matrices is\nderived. Building upon this result, in the second part, a joint estimator,\nrelying on the Tyler's $M$-estimator of location and on the $R$-estimator of\nshape matrix, is proposed and its Mean Squared Error (MSE) performance compared\nwith the Semiparametric Cram\\'{e}r-Rao Bound (CSCRB).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:02:23 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Fortunati", "Stefano", ""], ["Renaux", "Alexandre", ""], ["Pascal", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2101.10769", "submitter": "Hans-Peter Piepho", "authors": "Hans-Peter Piepho, Emlyn R. Williams", "title": "Regression Models for Order-of-Addition Experiments", "comments": "25 pages, 7 Tables, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of order-of-addition (OofA) experiments is to identify the best\norder in a sequence of m components in a system or treatment. Such experiments\nmay be analysed by various regression models, the most popular ones being based\non pairwise ordering (PWO) factors or on component-position (CP) factors. This\npaper reviews these models and extensions and proposes a new class of models\nbased on response surface (RS) regression using component position numbers as\npredictor variables. Using two published examples, it is shown that RS models\ncan be quite competitive. In case of model uncertainty, we advocate the use of\nmodel averaging for analysis. The averaging idea leads naturally to a design\napproach based on a compound optimality criterion assigning weights to each\ncandidate model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:25:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Piepho", "Hans-Peter", ""], ["Williams", "Emlyn R.", ""]]}, {"id": "2101.10880", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "USP: an independence test that improves on Pearson's chi-squared and the\n  $G$-test", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the $U$-Statistic Permutation (USP) test of independence in the\ncontext of discrete data displayed in a contingency table. Either Pearson's\nchi-squared test of independence, or the $G$-test, are typically used for this\ntask, but we argue that these tests have serious deficiencies, both in terms of\ntheir inability to control the size of the test, and their power properties. By\ncontrast, the USP test is guaranteed to control the size of the test at the\nnominal level for all sample sizes, has no issues with small (or zero) cell\ncounts, and is able to detect distributions that violate independence in only a\nminimal way. The test statistic is derived from a $U$-statistic estimator of a\nnatural population measure of dependence, and we prove that this is the unique\nminimum variance unbiased estimator of this population quantity. The practical\nutility of the USP test is demonstrated on both simulated data, where its power\ncan be dramatically greater than those of Pearson's test and the $G$-test, and\non real data. The USP test is implemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:42:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2101.10962", "submitter": "Song Wei", "authors": "Song Wei, Yao Xie, Dobromir Rahnev", "title": "Inferring serial correlation with dynamic backgrounds", "comments": "39 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential data with serial correlation and an unknown, unstructured, and\ndynamic background is ubiquitous in neuroscience, psychology, and econometrics.\nInferring serial correlation for such data is a fundamental challenge in\nstatistics. We propose a total variation constrained least square estimator\ncoupled with hypothesis tests to infer the serial correlation in the presence\nof unknown and unstructured dynamic background. The total variation constraint\non the dynamic background encourages a piece-wise constant structure, which can\napproximate a wide range of dynamic backgrounds. The tuning parameter is\nselected via the Ljung-Box test to control the bias-variance trade-off. We\nestablish a non-asymptotic upper bound for the estimation error through\nvariational inequalities. We also derive a lower error bound via Fano's method\nand show the proposed method is near-optimal. Numerical simulation and a real\nstudy in psychology demonstrate the excellent performance of our proposed\nmethod compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:40:51 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 22:22:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wei", "Song", ""], ["Xie", "Yao", ""], ["Rahnev", "Dobromir", ""]]}, {"id": "2101.11083", "submitter": "Naoki Awaya", "authors": "Naoki Awaya and Li Ma", "title": "Tree boosting for learning probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probability measures based on an i.i.d. sample is a fundamental\ninference task, but is challenging when the sample space is high-dimensional.\nInspired by the success of tree boosting in high-dimensional classification and\nregression, we propose a tree boosting method for learning high-dimensional\nprobability distributions. We formulate concepts of \"addition\" and \"residuals\"\non probability distributions in terms of compositions of a new, more general\nnotion of multivariate cumulative distribution functions (CDFs) than classical\nCDFs. This then gives rise to a simple boosting algorithm based on\nforward-stagewise (FS) fitting of an additive ensemble of measures, which\nsequentially minimizes the entropy loss. The output of the FS algorithm allows\nanalytic computation of the probability density function for the fitted\ndistribution. It also provides an exact simulator for drawing independent Monte\nCarlo samples from the fitted measure. Typical considerations in applying\nboosting--namely choosing the number of trees, setting the appropriate level of\nshrinkage/regularization in the weak learner, and the evaluation of variable\nimportance--can all be accomplished in an analogous fashion to traditional\nboosting in supervised learning. Numerical experiments confirm that boosting\ncan substantially improve the fit to multivariate distributions compared to the\nstate-of-the-art single-tree learner and is computationally efficient. We\nillustrate through an application to a data set from mass cytometry how the\nsimulator can be used to investigate various aspects of the underlying\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:03:27 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 19:41:48 GMT"}, {"version": "v3", "created": "Thu, 29 Apr 2021 23:28:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2101.11159", "submitter": "Shanshan Xie", "authors": "Shanshan Xie (1), Tim Hillel (2), Ying Jin (1) ((1) Department of\n  Architecture, University of Cambridge, (2) Transportation and Mobility\n  Laboratory, EPFL)", "title": "An Early Stopping Bayesian Data Assimilation Approach for Mixed-Logit\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixed-logit model is a flexible tool in transportation choice analysis,\nwhich provides valuable insights into inter and intra-individual behavioural\nheterogeneity. However, applications of mixed-logit models are limited by the\nhigh computational and data requirements for model estimation. When estimating\non small samples, the Bayesian estimation approach becomes vulnerable to over\nand under-fitting. This is problematic for investigating the behaviour of\nspecific population sub-groups or market segments with low data availability.\nSimilar challenges arise when transferring an existing model to a new location\nor time period, e.g., when estimating post-pandemic travel behaviour. We\npropose an Early Stopping Bayesian Data Assimilation (ESBDA) simulator for\nestimation of mixed-logit which combines a Bayesian statistical approach with\nMachine Learning methodologies. The aim is to improve the transferability of\nmixed-logit models and to enable the estimation of robust choice models with\nlow data availability. This approach can provide new insights into choice\nbehaviour where the traditional estimation of mixed-logit models was not\npossible due to low data availability, and open up new opportunities for\ninvestment and planning decisions support. The ESBDA estimator is benchmarked\nagainst the Direct Application approach, a basic Bayesian simulator with random\nstarting parameter values and a Bayesian Data Assimilation (BDA) simulator\nwithout early stopping. The ESBDA approach is found to effectively overcome\nunder and over-fitting and non-convergence issues in simulation. Its resulting\nmodels clearly outperform those of the reference simulators in predictive\naccuracy. Furthermore, models estimated with ESBDA tend to be more robust, with\nsignificant parameters with signs and values consistent with behavioural\ntheory, even when estimated on small samples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:35:25 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Xie", "Shanshan", ""], ["Hillel", "Tim", ""], ["Jin", "Ying", ""]]}, {"id": "2101.11202", "submitter": "Cong Xu", "authors": "Cong Xu, Hans Moritz G\\\"unther, Vinay L. Kashyap, Thomas C. M. Lee,\n  Andreas Zezas", "title": "Change point detection and image segmentation for time series of\n  astrophysical images", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": "10.3847/1538-3881/abe0b6", "report-no": null, "categories": "astro-ph.IM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many astrophysical phenomena are time-varying, in the sense that their\nintensity, energy spectrum, and/or the spatial distribution of the emission\nsuddenly change. This paper develops a method for modeling a time series of\nimages. Under the assumption that the arrival times of the photons follow a\nPoisson process, the data are binned into 4D grids of voxels (time, energy\nband, and x-y coordinates), and viewed as a time series of non-homogeneous\nPoisson images. The method assumes that at each time point, the corresponding\nmulti-band image stack is an unknown 3D piecewise constant function including\nPoisson noise. It also assumes that all image stacks between any two adjacent\nchange points (in time domain) share the same unknown piecewise constant\nfunction. The proposed method is designed to estimate the number and the\nlocations of all the change points (in time domain), as well as all the unknown\npiecewise constant functions between any pairs of the change points. The method\napplies the minimum description length (MDL) principle to perform this task. A\npractical algorithm is also developed to solve the corresponding complicated\noptimization problem. Simulation experiments and applications to real datasets\nshow that the proposed method enjoys very promising empirical properties.\nApplications to two real datasets, the XMM observation of a flaring star and an\nemerging solar coronal loop, illustrate the usage of the proposed method and\nthe scientific insight gained from it.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:37:53 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xu", "Cong", ""], ["G\u00fcnther", "Hans Moritz", ""], ["Kashyap", "Vinay L.", ""], ["Lee", "Thomas C. M.", ""], ["Zezas", "Andreas", ""]]}, {"id": "2101.11227", "submitter": "David Issa Mattos", "authors": "David Issa Mattos, \\'Erika Martins Silva Ramos", "title": "Bayesian Paired-Comparison with the bpcs Package", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the bpcs R package (Bayesian Paired Comparison in\nStan) and the statistical models implemented in the package. This package aims\nto facilitate the use of Bayesian models for paired comparison data in\nbehavioral research. Bayesian analysis of paired comparison data allows\nparameter estimation even in conditions where the maximum likelihood does not\nexist, allows easy extension of paired comparison models, provide\nstraightforward interpretation of the results with credible intervals, have\nbetter control of type I error, have more robust evidence towards the null\nhypothesis, allows propagation of uncertainties, includes prior information,\nand perform well when handling models with many parameters and latent\nvariables. The bpcs package provides a consistent interface for R users and\nseveral functions to evaluate the posterior distribution of all parameters, to\nestimate the posterior distribution of any contest between items, and to obtain\nthe posterior distribution of the ranks. Three reanalyses of recent studies\nthat used the frequentist Bradley-Terry model are presented. These reanalyses\nare conducted with the Bayesian models of the bpcs package, and all the code\nused to fit the models, generate the figures, and the tables are available in\nthe online appendix.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:13:46 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 13:10:11 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 10:42:15 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mattos", "David Issa", ""], ["Ramos", "\u00c9rika Martins Silva", ""]]}, {"id": "2101.11230", "submitter": "Hana \\v{S}inkovec", "authors": "Hana \\v{S}inkovec, Georg Heinze, Rok Blagus, Angelika Geroldinger", "title": "To tune or not to tune, a case study of ridge logistic regression in\n  small or sparse datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For finite samples with binary outcomes penalized logistic regression such as\nridge logistic regression (RR) has the potential of achieving smaller mean\nsquared errors (MSE) of coefficients and predictions than maximum likelihood\nestimation. There is evidence, however, that RR is sensitive to small or sparse\ndata situations, yielding poor performance in individual datasets. In this\npaper, we elaborate this issue further by performing a comprehensive simulation\nstudy, investigating the performance of RR in comparison to Firth's correction\nthat has been shown to perform well in low-dimensional settings. Performance of\nRR strongly depends on the choice of complexity parameter that is usually tuned\nby minimizing some measure of the out-of-sample prediction error or information\ncriterion. Alternatively, it may be determined according to prior assumptions\nabout true effects. As shown in our simulation and illustrated by a data\nexample, values optimized in small or sparse datasets are negatively correlated\nwith optimal values and suffer from substantial variability which translates\ninto large MSE of coefficients and large variability of calibration slopes. In\ncontrast, if the degree of shrinkage is pre-specified, accurate coefficients\nand predictions can be obtained even in non-ideal settings such as encountered\nin the context of rare outcomes or sparse predictors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:19:48 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["\u0160inkovec", "Hana", ""], ["Heinze", "Georg", ""], ["Blagus", "Rok", ""], ["Geroldinger", "Angelika", ""]]}, {"id": "2101.11370", "submitter": "Francesco Finazzi", "authors": "Yaqiong Wang, Francesco Finazzi, Alessandro Fass\\`o", "title": "D-STEM v2: A Software for Modelling Functional Spatio-Temporal Data", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional spatio-temporal data naturally arise in many environmental and\nclimate applications where data are collected in a three-dimensional space over\ntime. The MATLAB D-STEM v1 software package was first introduced for modelling\nmultivariate space-time data and has been recently extended to D-STEM v2 to\nhandle functional data indexed across space and over time. This paper\nintroduces the new modelling capabilities of D-STEM v2 as well as the\ncomplexity reduction techniques required when dealing with large data sets.\nModel estimation, validation and dynamic kriging are demonstrated in two case\nstudies, one related to ground-level air quality data in Beijing, China, and\nthe other one related to atmospheric profile data collected globally through\nradio sounding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:06:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wang", "Yaqiong", ""], ["Finazzi", "Francesco", ""], ["Fass\u00f2", "Alessandro", ""]]}, {"id": "2101.11470", "submitter": "Peter Aronow", "authors": "J. Sophia Wang and Peter M. Aronow", "title": "Listwise Deletion in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the properties of listwise deletion when both $n$ and the number\nof variables grow large. We show that when (i) all data has some idiosyncratic\nmissingness and (ii) the number of variables grows superlogarithmically in $n$,\nthen, for large $n$, listwise deletion will drop all rows with probability 1.\nUsing two canonical datasets from the study of comparative politics and\ninternational relations, we provide numerical illustration that these problems\nmay emerge in real world settings. These results suggest, in practice, using\nlistwise deletion may mean using few of the variables available to the\nresearcher.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:10:39 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 19:49:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "J. Sophia", ""], ["Aronow", "Peter M.", ""]]}, {"id": "2101.11568", "submitter": "Youngki Shin", "authors": "Rui Fan, Ji Hyung Lee, Youngki Shin", "title": "Predictive Quantile Regression with Mixed Roots and Increasing\n  Dimensions", "comments": "47 pages, 3 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study the benefit of using the adaptive LASSO for predictive\nquantile regression. It is common that predictors in predictive quantile\nregression have various degrees of persistence and exhibit different signal\nstrengths in explaining the dependent variable. We show that the adaptive LASSO\nhas the consistent variable selection and the oracle properties under the\nsimultaneous presence of stationary, unit root and cointegrated predictors.\nSome encouraging simulation and out-of-sample prediction results are reported.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:50:42 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Fan", "Rui", ""], ["Lee", "Ji Hyung", ""], ["Shin", "Youngki", ""]]}, {"id": "2101.11583", "submitter": "Sally Paganin", "authors": "Sally Paganin, Christopher J. Paciorek, Claudia Wehrhahn, Abel\n  Rodriguez, Sophia Rabe-Hesketh, Perry de Valpine", "title": "Computational methods for Bayesian semiparametric Item Response Theory\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item response theory (IRT) models are widely used to obtain interpretable\ninference when analyzing data from questionnaires, scaling binary responses\ninto continuous constructs. Typically, these models rely on a normality\nassumption for the latent trait characterizing individuals in the population\nunder study. However, this assumption can be unrealistic and lead to biased\nresults. We relax the normality assumption by considering a flexible Dirichlet\nProcess mixture model as a nonparametric prior on the distribution of the\nindividual latent traits. Although this approach has been considered in the\nliterature before, there is a lack of comprehensive studies of such models or\ngeneral software tools. To fill this gap, we show how the NIMBLE framework for\nhierarchical statistical modeling enables the use of flexible priors on the\nlatent trait distribution, specifically illustrating the use of Dirichlet\nProcess mixtures in two-parameter logistic (2PL) IRT models. We study how\ndifferent sets of constraints can lead to model identifiability and give\nguidance on eliciting prior distributions. Using both simulated and real-world\ndata, we conduct an in-depth study of Markov chain Monte Carlo posterior\nsampling efficiency for several sampling strategies. We conclude that having\naccess to semiparametric models can be broadly useful, as it allows inference\non the entire underlying ability distribution and its functionals, with NIMBLE\nbeing a flexible framework for estimation of such models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:17:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Paganin", "Sally", ""], ["Paciorek", "Christopher J.", ""], ["Wehrhahn", "Claudia", ""], ["Rodriguez", "Abel", ""], ["Rabe-Hesketh", "Sophia", ""], ["de Valpine", "Perry", ""]]}, {"id": "2101.11595", "submitter": "Sergey Tarima", "authors": "Sergey Tarima and Nancy Flournoy", "title": "Most Powerful Test Sequences with Early Stopping Options", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential likelihood ratio testing is found to be most powerful in\nsequential studies with early stopping rules when grouped data come from the\none-parameter exponential family.\n  First, to obtain this elusive result, the probability measure of a group\nsequential design is constructed with support for all possible outcome events,\nas is useful for designing an experiment prior to having data. This\nconstruction identifies impossible events that are not part of the support. The\noverall probability distribution is dissected into stage specific components.\nThese components are sub-densities of interim test statistics first described\nby Armitage, McPherson and Rowe (1969) that are commonly used to create\nstopping boundaries given an $\\alpha$-spending function and a set of interim\nanalysis times. Likelihood expressions conditional on reaching a stage are\ngiven to connect pieces of the probability anatomy together.\n  The reduction of the support caused by the adoption of an early stopping rule\ninduces sequential truncation (not nesting) in the probability distributions of\npossible events. Multiple testing induces mixtures on the adapted support. Even\nasymptotic distributions of inferential statistics are mixtures of truncated\ndistributions. In contrast to the classical result on local asymptotic\nnormality (Le Cam 1960), statistics that are asymptotically normal without\nstopping options have asymptotic distributions that are mixtures of truncated\nnormal distributions under local alternatives with stopping options; under\nfixed alternatives, asymptotic distributions of test statistics are degenerate.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:41:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tarima", "Sergey", ""], ["Flournoy", "Nancy", ""]]}, {"id": "2101.11682", "submitter": "Mohsen Soltanifar", "authors": "Mohsen Soltanifar, Michael Escobar, Annie Dupuis, Andre Chevrier and\n  Russell Schachar", "title": "The AL-Gaussian Distribution as the Descriptive Model for the Internal\n  Proactive Inhibition in the Standard Stop Signal Task", "comments": "KEYWORDS Proactive Inhibition, Reaction Times, Ex-Gaussian,\n  Asymmetric Laplace Gaussian, Bayesian Parametric Approach, Hazard function", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measurements of response inhibition components of reactive inhibition and\nproactive inhibition within the stop signal paradigm have been of special\ninterest for researchers since the 1980s. While frequentist nonparametric and\nBayesian parametric methods have been proposed to precisely estimate the entire\ndistribution of reactive inhibition, quantified by stop signal reaction\ntimes(SSRT), there is no method yet in the stop-signal task literature to\nprecisely estimate the entire distribution of proactive inhibition. We\nintroduce an Asymmetric Laplace Gaussian (ALG) model to describe the\ndistribution of proactive inhibition. The proposed method is based on two\nassumptions of independent trial type(go/stop) reaction times, and Ex-Gaussian\n(ExG) models for them. Results indicated that the four parametric, ALG model\nuniquely describes the proactive inhibition distribution and its key shape\nfeatures; and, its hazard function is monotonically increasing as are its three\nparametric ExG components. In conclusion, both response inhibition components\ncan be uniquely modeled via variations of the four parametric ALG model\ndescribed with their associated similar distributional features.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:41:04 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 01:13:00 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Soltanifar", "Mohsen", ""], ["Escobar", "Michael", ""], ["Dupuis", "Annie", ""], ["Chevrier", "Andre", ""], ["Schachar", "Russell", ""]]}, {"id": "2101.11807", "submitter": "Xiaoxi Shen", "authors": "Xiaoxi Shen, Xiaoran Tong, and Qing Lu", "title": "A Kernel-Based Neural Network for High-dimensional Genetic Risk\n  Prediction Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk prediction capitalizing on emerging human genome findings holds great\npromise for new prediction and prevention strategies. While the large amounts\nof genetic data generated from high-throughput technologies offer us a unique\nopportunity to study a deep catalog of genetic variants for risk prediction,\nthe high-dimensionality of genetic data and complex relationships between\ngenetic variants and disease outcomes bring tremendous challenges to risk\nprediction analysis. To address these rising challenges, we propose a\nkernel-based neural network (KNN) method. KNN inherits features from both\nlinear mixed models (LMM) and classical neural networks and is designed for\nhigh-dimensional risk prediction analysis. To deal with datasets with millions\nof variants, KNN summarizes genetic data into kernel matrices and use the\nkernel matrices as inputs. Based on the kernel matrices, KNN builds a\nsingle-layer feedforward neural network, which makes it feasible to consider\ncomplex relationships between genetic variants and disease outcomes. The\nparameter estimation in KNN is based on MINQUE and we show, that under certain\nconditions, the average prediction error of KNN can be smaller than that of\nLMM. Simulation studies also confirm the results.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:58:38 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shen", "Xiaoxi", ""], ["Tong", "Xiaoran", ""], ["Lu", "Qing", ""]]}, {"id": "2101.11872", "submitter": "Hwiyoung Lee", "authors": "Hwiyoung Lee", "title": "Robust Extrinsic Regression Analysis for Manifold Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been a growing need in analyzing data on manifolds owing\nto their important role in diverse fields of science and engineering. In the\nliterature of manifold-valued data analysis up till now, however, only a few\nworks have been carried out concerning the robustness of estimation against\nnoises, outliers, and other sources of perturbations. In this regard, we\nintroduce a novel extrinsic framework for analyzing manifold valued data in a\nrobust manner. First, by extending the notion of the geometric median, we\npropose a new robust location parameter on manifolds, so-called the extrinsic\nmedian. A robust extrinsic regression method is also developed by incorporating\nthe conditional extrinsic median into the classical local polynomial regression\nmethod. We present the Weiszfeld's algorithm for implementing the proposed\nmethods. The promising performance of our approach against existing methods is\nillustrated through simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:59:48 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lee", "Hwiyoung", ""]]}, {"id": "2101.12156", "submitter": "Nianqiao Ju", "authors": "Nianqiao Ju, Jeremy Heng, and Pierre E. Jacob", "title": "Sequential Monte Carlo algorithms for agent-based models of disease\n  transmission", "comments": "39 pages, 7 figures, 2. tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO nlin.CG q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based models of disease transmission involve stochastic rules that\nspecify how a number of individuals would infect one another, recover or be\nremoved from the population. Common yet stringent assumptions stipulate\ninterchangeability of agents and that all pairwise contact are equally likely.\nUnder these assumptions, the population can be summarized by counting the\nnumber of susceptible and infected individuals, which greatly facilitates\nstatistical inference. We consider the task of inference without such\nsimplifying assumptions, in which case, the population cannot be summarized by\nlow-dimensional counts. We design improved particle filters, where each\nparticle corresponds to a specific configuration of the population of agents,\nthat take either the next or all future observations into account when\nproposing population configurations. Using simulated data sets, we illustrate\nthat orders of magnitude improvements are possible over bootstrap particle\nfilters. We also provide theoretical support for the approximations employed to\nmake the algorithms practical.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:08:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ju", "Nianqiao", ""], ["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "2101.12179", "submitter": "Sewon Park", "authors": "Sewon Park, Hee-Seok Oh, Jaeyong Lee", "title": "L\\'{e}vy Adaptive B-spline Regression via Overcomplete Systems", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of functions with varying degrees of smoothness is a\nchallenging problem in the nonparametric function estimation. In this paper, we\npropose the LABS (L\\'{e}vy Adaptive B-Spline regression) model, an extension of\nthe LARK models, for the estimation of functions with varying degrees of\nsmoothness. LABS model is a LARK with B-spline bases as generating kernels. The\nB-spline basis consists of piecewise k degree polynomials with k-1 continuous\nderivatives and can express systematically functions with varying degrees of\nsmoothness. By changing the orders of the B-spline basis, LABS can\nsystematically adapt the smoothness of functions, i.e., jump discontinuities,\nsharp peaks, etc. Results of simulation studies and real data examples support\nthat this model catches not only smooth areas but also jumps and sharp peaks of\nfunctions. The proposed model also has the best performance in almost all\nexamples. Finally, we provide theoretical results that the mean function for\nthe LABS model belongs to the certain Besov spaces based on the orders of the\nB-spline basis and that the prior of the model has the full support on the\nBesov spaces.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:40:07 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 08:27:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Park", "Sewon", ""], ["Oh", "Hee-Seok", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2101.12282", "submitter": "Christoph Breunig", "authors": "Christoph Breunig, Xiaohong Chen", "title": "Adaptive Estimation of Quadratic Functionals in Nonparametric\n  Instrumental Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers adaptive estimation of quadratic functionals in the\nnonparametric instrumental variables (NPIV) models. Minimax estimation of a\nquadratic functional of a NPIV is an important problem in optimal estimation of\na nonlinear functional of an ill-posed inverse regression with an unknown\noperator using one random sample. We first show that a leave-one-out, sieve\nNPIV estimator of the quadratic functional proposed by \\cite{BC2020} attains a\nconvergence rate that coincides with the lower bound previously derived by\n\\cite{ChenChristensen2017}. The minimax rate is achieved by the optimal choice\nof a key tuning parameter (sieve dimension) that depends on unknown NPIV model\nfeatures. We next propose a data driven choice of the tuning parameter based on\nLepski's method. The adaptive estimator attains the minimax optimal rate in the\nseverely ill-posed case and in the regular, mildly ill-posed case, but up to a\nmultiplicative $\\sqrt{\\log n}$ in the irregular, mildly ill-posed case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:14:02 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Breunig", "Christoph", ""], ["Chen", "Xiaohong", ""]]}, {"id": "2101.12285", "submitter": "Louis Jensen", "authors": "Louis G. Jensen, David J. Williamson, Ute Hahn", "title": "Semiparametric point process modelling of blinking artifacts in PALM", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photoactivated localization microscopy (PALM) is a powerful imaging technique\nfor characterization of protein organization in biological cells. Due to the\nstochastic blinking of fluorescent probes, and camera discretization effects,\neach protein gives rise to a cluster of artificial observations. These blinking\nartifacts are an obstacle for qualitative analysis of PALM data, and tools for\ntheir correction are in high demand. We develop the Independent Blinking\nCluster point process (IBCpp) family of models, and present results on the mark\ncorrelation function. We then construct the PALM-IBCpp - a semiparametric IBCpp\ntailored for PALM data. We describe a procedure for estimation of parameters,\nwhich can be used without parametric assumptions on the spatial organization of\nproteins. The parameters include the kinetic rates that control blinking, and\nas such can be used to correct subsequent data analysis. The method is\ndemonstrated on real data, and in a simulation study, where blinking artifacts\nwere precisely quantified in a range of realistic settings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:31:56 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Jensen", "Louis G.", ""], ["Williamson", "David J.", ""], ["Hahn", "Ute", ""]]}, {"id": "2101.12318", "submitter": "Molly Offer-Westort", "authors": "Molly Offer-Westort, Drew Dimmery", "title": "Experimentation for Homogenous Policy Change", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the Stable Unit Treatment Value Assumption (SUTVA) is violated and there\nis interference among units, there is not a uniquely defined Average Treatment\nEffect (ATE), and alternative estimands may be of interest, among them average\nunit-level differences in outcomes under different homogeneous treatment\npolicies. We term this target the Homogeneous Assignment Average Treatment\nEffect (HAATE). We consider approaches to experimental design with multiple\ntreatment conditions under partial interference and, given the estimand of\ninterest, we show that difference-in-means estimators may perform better than\ncorrectly specified regression models in finite samples on root mean squared\nerror (RMSE). With errors correlated at the cluster level, we demonstrate that\ntwo-stage randomization procedures with intra-cluster correlation of treatment\nstrictly between zero and one may dominate one-stage randomization designs on\nthe same metric. Simulations demonstrate performance of this approach; an\napplication to online experiments at Facebook is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 23:19:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Offer-Westort", "Molly", ""], ["Dimmery", "Drew", ""]]}, {"id": "2101.12333", "submitter": "Lina Montoya", "authors": "Lina Montoya, Jennifer Skeem, Mark van der Laan, Maya Petersen", "title": "Performance and Application of Estimators for the Value of an Optimal\n  Dynamic Treatment Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an (optimal) dynamic treatment rule, it may be of interest to evaluate\nthat rule -- that is, to ask the causal question: what is the expected outcome\nhad every subject received treatment according to that rule? In this paper, we\nstudy the performance of estimators that approximate the true value of: 1) an\n$a$ $priori$ known dynamic treatment rule 2) the true, unknown optimal dynamic\ntreatment rule (ODTR); 3) an estimated ODTR, a so-called \"data-adaptive\nparameter,\" whose true value depends on the sample. Using simulations of\npoint-treatment data, we specifically investigate: 1) the impact of\nincreasingly data-adaptive estimation of nuisance parameters and/or of the ODTR\non performance; 2) the potential for improved efficiency and bias reduction\nthrough the use of semiparametric efficient estimators; and, 3) the importance\nof sample splitting based on CV-TMLE for accurate inference. In the simulations\nconsidered, there was very little cost and many benefits to using the\ncross-validated targeted maximum likelihood estimator (CV-TMLE) to estimate the\nvalue of the true and estimated ODTR; importantly, and in contrast to non\ncross-validated estimators, the performance of CV-TMLE was maintained even when\nhighly data-adaptive algorithms were used to estimate both nuisance parameters\nand the ODTR. In addition, we apply these estimators for the value of the rule\nto the \"Interventions\" Study, an ongoing randomized controlled trial, to\nidentify whether assigning cognitive behavioral therapy (CBT) to criminal\njustice-involved adults with mental illness using an ODTR significantly reduces\nthe probability of recidivism, compared to assigning CBT in a\nnon-individualized way.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:57:04 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 20:38:28 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 01:31:08 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Montoya", "Lina", ""], ["Skeem", "Jennifer", ""], ["van der Laan", "Mark", ""], ["Petersen", "Maya", ""]]}, {"id": "2101.12357", "submitter": "Yangfan Zhang", "authors": "Yangfan Zhang, Runmin Wang and Xiaofeng Shao", "title": "Adaptive Inference for Change Points in High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we propose a class of test statistics for a change point in\nthe mean of high-dimensional independent data. Our test integrates the\nU-statistic based approach in a recent work by \\cite{hdcp} and the $L_q$-norm\nbased high-dimensional test in \\cite{he2018}, and inherits several appealing\nfeatures such as being tuning parameter free and asymptotic independence for\ntest statistics corresponding to even $q$s. A simple combination of test\nstatistics corresponding to several different $q$s leads to a test with\nadaptive power property, that is, it can be powerful against both sparse and\ndense alternatives. On the estimation front, we obtain the convergence rate of\nthe maximizer of our test statistic standardized by sample size when there is\none change-point in mean and $q=2$, and propose to combine our tests with a\nwild binary segmentation (WBS) algorithm to estimate the change-point number\nand locations when there are multiple change-points. Numerical comparisons\nusing both simulated and real data demonstrate the advantage of our adaptive\ntest and its corresponding estimation method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:05:21 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhang", "Yangfan", ""], ["Wang", "Runmin", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2101.12492", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Qian Wen", "title": "A Practical Two-Sample Test for Weighted Random Graphs", "comments": "to appear in Journal of Applied Statistics", "journal-ref": "Journal of Applied Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network (graph) data analysis is a popular research topic in statistics and\nmachine learning. In application, one is frequently confronted with graph\ntwo-sample hypothesis testing where the goal is to test the difference between\ntwo graph populations. Several statistical tests have been devised for this\npurpose in the context of binary graphs. However, many of the practical\nnetworks are weighted and existing procedures can't be directly applied to\nweighted graphs. In this paper, we study the weighted graph two-sample\nhypothesis testing problem and propose a practical test statistic. We prove\nthat the proposed test statistic converges in distribution to the standard\nnormal distribution under the null hypothesis and analyze its power\ntheoretically. The simulation study shows that the proposed test has\nsatisfactory performance and it substantially outperforms the existing\ncounterpart in the binary graph case. A real data application is provided to\nillustrate the method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:59:45 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Yuan", "Mingao", ""], ["Wen", "Qian", ""]]}, {"id": "2101.12499", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Tom F. O'Callaghan and Thomas Brendan Murphy", "title": "Parsimonious Bayesian Factor Analysis for modelling latent structures in\n  spectroscopy data", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years animal diet has been receiving increased attention, in\nparticular examining the impact of pasture-based feeding strategies on the\nquality of milk and dairy products, in line with the increased prevalence of\ngrass-fed dairy products appearing on market shelves. To date, there are\nlimited testing methods available for the verification of grass-fed dairy\ntherefore these products are susceptible to food fraud and adulteration. Hence\nstatistical tools studying potential differences among milk samples coming from\nanimals on different feeding systems are required, thus providing increased\nsecurity around the authenticity of the products. Infrared spectroscopy\ntechniques are widely used to collect data on milk samples and to predict milk\nrelated traits. While these data are routinely used to predict the composition\nof the macro components of milk, each spectrum provides a reservoir of\nunharnessed information about the sample. The interpretation of these data\npresents a number of challenges due to their high-dimensionality and the\nrelationships amongst the spectral variables. In this work we propose a\nmodification of the standard factor analysis to induce a parsimonious summary\nof spectroscopic data. The procedure maps the observations into a\nlow-dimensional latent space while simultaneously clustering observed\nvariables. The method indicates possible redundancies in the data and it helps\ndisentangle the complex relationships among the wavelengths. A flexible\nBayesian estimation procedure is proposed for model fitting, providing\nreasonable values for the number of latent factors and clusters. The method is\napplied on milk mid-infrared spectroscopy data from dairy cows on different\npasture and non-pasture based diets, providing accurate modelling of the data\ncorrelation, the clustering of variables and information on differences between\nmilk samples from cows on different diets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:06:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Casa", "Alessandro", ""], ["O'Callaghan", "Tom F.", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2101.12503", "submitter": "Ines Wilms", "authors": "Ines Wilms and Jacob Bien", "title": "Tree-based Node Aggregation in Sparse Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional graphical models are often estimated using regularization\nthat is aimed at reducing the number of edges in a network. In this work, we\nshow how even simpler networks can be produced by aggregating the nodes of the\ngraphical model. We develop a new convex regularized method, called the\ntree-aggregated graphical lasso or tag-lasso, that estimates graphical models\nthat are both edge-sparse and node-aggregated. The aggregation is performed in\na data-driven fashion by leveraging side information in the form of a tree that\nencodes node similarity and facilitates the interpretation of the resulting\naggregated nodes. We provide an efficient implementation of the tag-lasso by\nusing the locally adaptive alternating direction method of multipliers and\nillustrate our proposal's practical advantages in simulation and in\napplications in finance and biology.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:17:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wilms", "Ines", ""], ["Bien", "Jacob", ""]]}, {"id": "2101.12525", "submitter": "Corinne Emmenegger", "authors": "Corinne Emmenegger and Peter B\\\"uhlmann", "title": "Regularizing Double Machine Learning in Partially Linear Endogenous\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the linear coefficient in a partially linear model with\nconfounding variables. We rely on double machine learning (DML) and extend it\nwith an additional regularization and selection scheme. We allow for more\ngeneral dependence structures among the model variables than what has been\ninvestigated previously, and we prove that this DML estimator remains\nasymptotically Gaussian and converges at the parametric rate. The DML estimator\nhas a two-stage least squares interpretation and may produce overly wide\nconfidence intervals. To address this issue, we propose the\nregularization-selection regsDML method that leads to narrower confidence\nintervals. It is fully data driven and optimizes an estimated asymptotic mean\nsquared error of the coefficient estimate. Empirical examples demonstrate our\nmethodological and theoretical developments. Software code for our regsDML\nmethod will be made available in the R-package dmlalg.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 11:24:18 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Emmenegger", "Corinne", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2101.12642", "submitter": "Indranil Sahoo", "authors": "Edward L. Boone, Abdel-Salam G. Abdel-Salam, Indranil Sahoo, Ryad\n  Ghanam, Xi Chen and Aiman Hanif", "title": "Monitoring SEIRD model parameters using MEWMA for the COVID-19 pandemic\n  with application to the State of Qatar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the current COVID-19 pandemic, decision makers are tasked with\nimplementing and evaluating strategies for both treatment and disease\nprevention. In order to make effective decisions, they need to simultaneously\nmonitor various attributes of the pandemic such as transmission rate and\ninfection rate for disease prevention, recovery rate which indicates treatment\neffectiveness as well as the mortality rate and others. This work presents a\ntechnique for monitoring the pandemic by employing an Susceptible, Exposed,\nInfected, Recovered Death model regularly estimated by an augmented particle\nMarkov chain Monte Carlo scheme in which the posterior distribution samples are\nmonitored via Multivariate Exponentially Weighted Average process monitoring.\nThis is illustrated on the COVID-19 data for the State of Qatar.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:34:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Boone", "Edward L.", ""], ["Abdel-Salam", "Abdel-Salam G.", ""], ["Sahoo", "Indranil", ""], ["Ghanam", "Ryad", ""], ["Chen", "Xi", ""], ["Hanif", "Aiman", ""]]}, {"id": "2101.12693", "submitter": "Xiaochun Meng", "authors": "Carol Alexander, Michael Coulon, Yang Han, Xiaochun Meng", "title": "Evaluating the Discrimination Ability of Proper Multivariate Scoring\n  Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper scoring rules are commonly applied to quantify the accuracy of\ndistribution forecasts. Given an observation they assign a scalar score to each\ndistribution forecast, with the the lowest expected score attributed to the\ntrue distribution. The energy and variogram scores are two rules that have\nrecently gained some popularity in multivariate settings because their\ncomputation does not require a forecast to have parametric density function and\nso they are broadly applicable. Here we conduct a simulation study to compare\nthe discrimination ability between the energy score and three variogram scores.\nCompared with other studies, our simulation design is more realistic because it\nis supported by a historical data set containing commodity prices, currencies\nand interest rates, and our data generating processes include a diverse\nselection of models with different marginal distributions, dependence\nstructure, and calibration windows. This facilitates a comprehensive comparison\nof the performance of proper scoring rules in different settings. To compare\nthe scores we use three metrics: the mean relative score, error rate and a\ngeneralised discrimination heuristic. Overall, we find that the variogram score\nwith parameter p=0.5 outperforms the energy score and the other two variogram\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:23:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Alexander", "Carol", ""], ["Coulon", "Michael", ""], ["Han", "Yang", ""], ["Meng", "Xiaochun", ""]]}, {"id": "2101.12720", "submitter": "Tim Breitenbach", "authors": "Tim Breitenbach, Lauritz Rasbach, Chunguang Liang, Patrick Jahnke", "title": "A principle feature analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A key task of data science is to identify relevant features linked to certain\noutput variables that are supposed to be modeled or predicted. To obtain a\nsmall but meaningful model, it is important to find stochastically independent\nvariables capturing all the information necessary to model or predict the\noutput variables sufficiently. Therefore, we introduce in this work a framework\nto detect linear and non-linear dependencies between different features. As we\nwill show, features that are actually functions of other features do not\nrepresent further information. Consequently, a model reduction neglecting such\nfeatures conserves the relevant information, reduces noise and thus improves\nthe quality of the model. Furthermore, a smaller model makes it easier to adopt\na model of a given system. In addition, the approach structures dependencies\nwithin all the considered features. This provides advantages for classical\nmodeling starting from regression ranging to differential equations and for\nmachine learning.\n  To show the generality and applicability of the presented framework 2154\nfeatures of a data center are measured and a model for classification for\nfaulty and non-faulty states of the data center is set up. This number of\nfeatures is automatically reduced by the framework to 161 features. The\nprediction accuracy for the reduced model even improves compared to the model\ntrained on the total number of features. A second example is the analysis of a\ngene expression data set where from 9513 genes 9 genes are extracted from whose\nexpression levels two cell clusters of macrophages can be distinguished.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:30:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Breitenbach", "Tim", ""], ["Rasbach", "Lauritz", ""], ["Liang", "Chunguang", ""], ["Jahnke", "Patrick", ""]]}]