[{"id": "0811.0121", "submitter": "Ann Lee", "authors": "Ann B. Lee and Larry Wasserman", "title": "Spectral Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral kernel methods are techniques for transforming data into a\ncoordinate system that efficiently reveals the geometric structure - in\nparticular, the \"connectivity\" - of the data. These methods depend on certain\ntuning parameters. We analyze the dependence of the method on these tuning\nparameters. We focus on one particular technique - diffusion maps - but our\nanalysis can be used for other methods as well. We identify the population\nquantities implicitly being estimated, we explain how these methods relate to\nclassical kernel smoothing and we define an appropriate risk function for\nanalyzing the estimators. We also show that, in some cases, fast rates of\nconvergence are possible even in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 02:10:18 GMT"}], "update_date": "2008-11-04", "authors_parsed": [["Lee", "Ann B.", ""], ["Wasserman", "Larry", ""]]}, {"id": "0811.0148", "submitter": "Astrid Jourdan", "authors": "Astrid Jourdan (LMA-Pau)", "title": "Plans D'Experiences D'Information De Kullback-Leibler Minimale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental designs are tools which can dramatically reduce the number of\nsimulations required by time-consuming computer codes. Because we don't know\nthe true relation between the response and inputs, designs should allow one to\nfit a variety of models and should provide information about all portions of\nthe experimental region. One strategy for selecting the values of the inputs at\nwhich to observe the response is to choose these values so they are spread\nevenly throughout the experimental region, according to \"space-filling\ndesigns\". In this article, we suggest a new method based on comparing the\nempirical distribution of the points in a design to the uniform distribution\nwith the Kullback-Leibler information. The considered approach consists in\nestimating this difference or, reciprocally, the Shannon entropy. The entropy\nis estimated by a Monte Carlo method where the density function is replaced by\nits kernel density estimator or by using the nearest neighbor distances\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2008 09:23:05 GMT"}], "update_date": "2008-11-04", "authors_parsed": [["Jourdan", "Astrid", "", "LMA-Pau"]]}, {"id": "0811.0659", "submitter": "A. M. H. Al-Khazaleh", "authors": "R. Ahmad Mahir, A. M. H. Al-Khazaleh", "title": "Estimation of missing data by using the filtering process in a time\n  series modeling", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2008_324", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new method to estimate the missing data by using the\nfiltering process. We used datasets without missing data and randomly missing\ndata to evaluate the new method of estimation by using the Box - Jenkins\nmodeling technique to predict monthly average rainfall for site 5504035 Lahar\nIkan Mati at Kepala Batas, P. Pinang station in Malaysia. The rainfall data was\ncollected from the $1^{st}$ January 1969 to $31^{st}$ December 1997 in the\nstation. The data used in the development of the model to predict rainfall were\nrepresented by an autoregressive integrated moving - average (ARIMA) model. The\nmodel for both datasets was ARIMA$(1,0,0)(0,1,1)_s$. The result checked with\nthe Naive test, which is the Thiel's statistic and was found to be equal to\n$U=0.72086$ for the complete data and $U=0.726352$ for the missing data, which\nmean they were good models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 07:37:46 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Mahir", "R. Ahmad", ""], ["Al-Khazaleh", "A. M. H.", ""]]}, {"id": "0811.0753", "submitter": "Philippe Barbe", "authors": "Philippe Barbe (CNRS)", "title": "An elementary approach to extreme values theory", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents a rather intuitive approach to extreme value theory. This\napproach was devised mostly for pedagogical reason.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 15:44:57 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Barbe", "Philippe", "", "CNRS"]]}, {"id": "0811.1297", "submitter": "Andrey Novikov", "authors": "Andrey Novikov", "title": "Optimal sequential multiple hypothesis tests", "comments": "To appear in Kybernetika (Prague)", "journal-ref": "Kybernetika 45 (2009), no. 2, 309-330.", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work deals with a general problem of testing multiple hypotheses about\nthe distribution of a discrete-time stochastic process. Both the Bayesian and\nthe conditional settings are considered. The structure of optimal sequential\ntests is characterized.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2008 22:19:44 GMT"}], "update_date": "2009-12-23", "authors_parsed": [["Novikov", "Andrey", ""]]}, {"id": "0811.1606", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina, Fred L. Mannering, Andrew P. Tarko", "title": "Markov switching negative binomial models: an application to vehicle\n  accident frequencies", "comments": "This is a preprint, the final paper is available on Accident Analysis\n  and Prevention website (the preprint has 21 pages, 3 tables, 2 figures,\n  accepted for publication in Accident Analysis and Prevention)", "journal-ref": "Accident Analysis and Prevention, 2009, Volume 41, Issue 2, pages\n  217-226", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, two-state Markov switching models are proposed to study\naccident frequencies. These models assume that there are two unobserved states\nof roadway safety, and that roadway entities (roadway segments) can switch\nbetween these states over time. The states are distinct, in the sense that in\nthe different states accident frequencies are generated by separate counting\nprocesses (by separate Poisson or negative binomial processes). To demonstrate\nthe applicability of the approach presented herein, two-state Markov switching\nnegative binomial models are estimated using five-year accident frequencies on\nIndiana interstate highway segments. Bayesian inference methods and Markov\nChain Monte Carlo (MCMC) simulations are used for model estimation. The\nestimated Markov switching models result in a superior statistical fit relative\nto the standard (single-state) negative binomial model. It is found that the\nmore frequent state is safer and it is correlated with better weather\nconditions. The less frequent state is found to be less safe and to be\ncorrelated with adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2008 00:28:55 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2008 20:09:36 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""], ["Tarko", "Andrew P.", ""]]}, {"id": "0811.2026", "submitter": "Seyoung Kim", "authors": "Seyoung Kim, Kyung-Ah Sohn and Eric P. Xing", "title": "A Multivariate Regression Approach to Association Analysis of\n  Quantitative Trait Network", "comments": "Submitted to The American Journal of Human Genetics", "journal-ref": null, "doi": null, "report-no": "CMU-ML-08-113", "categories": "stat.ML q-bio.GN q-bio.MN q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex disease syndromes such as asthma consist of a large number of\nhighly related, rather than independent, clinical phenotypes, raising a new\ntechnical challenge in identifying genetic variations associated simultaneously\nwith correlated traits. In this study, we propose a new statistical framework\ncalled graph-guided fused lasso (GFlasso) to address this issue in a principled\nway. Our approach explicitly represents the dependency structure among the\nquantitative traits as a network, and leverages this trait network to encode\nstructured regularizations in a multivariate regression model over the\ngenotypes and traits, so that the genetic markers that jointly influence\nsubgroups of highly correlated traits can be detected with high sensitivity and\nspecificity. While most of the traditional methods examined each phenotype\nindependently and combined the results afterwards, our approach analyzes all of\nthe traits jointly in a single statistical method, and borrow information\nacross correlated phenotypes to discover the genetic markers that perturbe a\nsubset of correlated triats jointly rather than a single trait. Using simulated\ndatasets based on the HapMap consortium data and an asthma dataset, we compare\nthe performance of our method with the single-marker analysis, and other sparse\nregression methods such as the ridge regression and the lasso that do not use\nany structural information in the traits. Our results show that there is a\nsignificant advantage in detecting the true causal SNPs when we incorporate the\ncorrelation pattern in traits using our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 03:44:48 GMT"}], "update_date": "2008-11-16", "authors_parsed": [["Kim", "Seyoung", ""], ["Sohn", "Kyung-Ah", ""], ["Xing", "Eric P.", ""]]}, {"id": "0811.2177", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen, Lukas Meier and Peter B\\\"uhlmann", "title": "P-values for high-dimensional regression", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning significance in high-dimensional regression is challenging. Most\ncomputationally efficient selection algorithms cannot guard against inclusion\nof noise variables. Asymptotically valid p-values are not available. An\nexception is a recent proposal by Wasserman and Roeder (2008) which splits the\ndata into two parts. The number of variables is then reduced to a manageable\nsize using the first split, while classical variable selection techniques can\nbe applied to the remaining variables, using the data from the second split.\nThis yields asymptotic error control under minimal conditions. It involves,\nhowever, a one-time random split of the data. Results are sensitive to this\narbitrary choice: it amounts to a `p-value lottery' and makes it difficult to\nreproduce results. Here, we show that inference across multiple random splits\ncan be aggregated, while keeping asymptotic control over the inclusion of noise\nvariables. We show that the resulting p-values can be used for control of both\nfamily-wise error (FWER) and false discovery rate (FDR). In addition, the\nproposed aggregation is shown to improve power while reducing the number of\nfalsely selected variables substantially.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2008 20:15:30 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2009 14:07:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2009 12:50:33 GMT"}], "update_date": "2009-06-12", "authors_parsed": [["Meinshausen", "Nicolai", ""], ["Meier", "Lukas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "0811.3355", "submitter": "Richard Wilkinson", "authors": "Richard D. Wilkinson", "title": "Approximate Bayesian computation (ABC) gives exact results under the\n  assumption of model error", "comments": "33 pages, 1 figure, to appear in Statistical Applications in Genetics\n  and Molecular Biology 2013", "journal-ref": "Stat.App.Gen.Mol.Bio. 12 (2013) 129-141", "doi": "10.1515/sagmb-2013-0010", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) or likelihood-free inference\nalgorithms are used to find approximations to posterior distributions without\nmaking explicit use of the likelihood function, depending instead on simulation\nof sample data sets from the model. In this paper we show that under the\nassumption of the existence of a uniform additive model error term, ABC\nalgorithms give exact results when sufficient summaries are used. This\ninterpretation allows the approximation made in many previous application\npapers to be understood, and should guide the choice of metric and tolerance in\nfuture work. ABC algorithms can be generalized by replacing the 0-1 cut-off\nwith an acceptance probability that varies with the distance of the simulated\ndata from the observed data. The acceptance density gives the distribution of\nthe error term, enabling the uniform error usually used to be replaced by a\ngeneral distribution. This generalization can also be applied to approximate\nMarkov chain Monte Carlo algorithms. In light of this work, ABC algorithms can\nbe seen as calibration techniques for implicit stochastic models, inferring\nparameter values in light of the computer model, data, prior beliefs about the\nparameter values, and any measurement or model errors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2008 15:36:00 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 11:38:03 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Wilkinson", "Richard D.", ""]]}, {"id": "0811.3639", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina and Fred L. Mannering", "title": "Zero-state Markov switching count-data models: an empirical assessment", "comments": "19 pages, 2 figures, 2 tables, the final version can be found in\n  Accident Analysis and Prevention", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a two-state Markov switching count-data model is proposed as\nan alternative to zero-inflated models to account for the preponderance of\nzeros sometimes observed in transportation count data, such as the number of\naccidents occurring on a roadway segment over some period of time. For this\naccident-frequency case, zero-inflated models assume the existence of two\nstates: one of the states is a zero-accident count state, in which accident\nprobabilities are so low that they cannot be statistically distinguished from\nzero, and the other state is a normal count state, in which counts can be\nnon-negative integers that are generated by some counting process, for example,\na Poisson or negative binomial. In contrast to zero-inflated models, Markov\nswitching models allow specific roadway segments to switch between the two\nstates over time. An important advantage of this Markov switching approach is\nthat it allows for the direct statistical estimation of the specific\nroadway-segment state (i.e., zero or count state) whereas traditional\nzero-inflated models do not. To demonstrate the applicability of this approach,\na two-state Markov switching negative binomial model (estimated with Bayesian\ninference) and standard zero-inflated negative binomial models are estimated\nusing five-year accident frequencies on Indiana interstate highway segments. It\nis shown that the Markov switching model is a viable alternative and results in\na superior statistical fit relative to the zero-inflated models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 21:42:23 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2009 17:28:59 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0811.3644", "submitter": "Nataliya Malyshkina", "authors": "Nataliya V. Malyshkina, Fred L. Mannering", "title": "Markov switching multinomial logit model: an application to accident\n  injury severities", "comments": "24 pages, 1 figure, 3 tables", "journal-ref": "Accident Analysis and Prevention, 2009, volume 41, issue 4, pages\n  829-838", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, two-state Markov switching multinomial logit models are\nproposed for statistical modeling of accident injury severities. These models\nassume Markov switching in time between two unobserved states of roadway\nsafety. The states are distinct, in the sense that in different states accident\nseverity outcomes are generated by separate multinomial logit processes. To\ndemonstrate the applicability of the approach presented herein, two-state\nMarkov switching multinomial logit models are estimated for severity outcomes\nof accidents occurring on Indiana roads over a four-year time interval.\nBayesian inference methods and Markov Chain Monte Carlo (MCMC) simulations are\nused for model estimation. The estimated Markov switching models result in a\nsuperior statistical fit relative to the standard (single-state) multinomial\nlogit models. It is found that the more frequent state of roadway safety is\ncorrelated with better weather conditions. The less frequent state is found to\nbe correlated with adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2008 22:05:53 GMT"}], "update_date": "2009-08-02", "authors_parsed": [["Malyshkina", "Nataliya V.", ""], ["Mannering", "Fred L.", ""]]}, {"id": "0811.4167", "submitter": "Dabao Zhang", "authors": "Dabao Zhang, Yanzhu Lin and Min Zhang", "title": "Penalized Orthogonal-Components Regression for Large p Small n Data", "comments": "12 pages", "journal-ref": null, "doi": "10.1214/09-EJS354", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized orthogonal-components regression (POCRE) for large p\nsmall n data. Orthogonal components are sequentially constructed to maximize,\nupon standardization, their correlation to the response residuals. A new\npenalization framework, implemented via empirical Bayes thresholding, is\npresented to effectively identify sparse predictors of each component. POCRE is\ncomputationally efficient owing to its sequential construction of leading\nsparse principal components. In addition, such construction offers other\nproperties such as grouping highly correlated predictors and allowing for\ncollinear or nearly collinear predictors. With multivariate responses, POCRE\ncan construct common components and thus build up latent-variable models for\nlarge p small n data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 19:53:33 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2008 15:10:07 GMT"}, {"version": "v3", "created": "Thu, 18 Dec 2008 01:45:00 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Zhang", "Dabao", ""], ["Lin", "Yanzhu", ""], ["Zhang", "Min", ""]]}, {"id": "0811.4463", "submitter": "Jie Peng", "authors": "Jie Peng, Pei Wang, Nengfeng Zhou, Ji Zhu", "title": "Partial Correlation Estimation by Joint Sparse Regression Models", "comments": "A paper based on this report has been accepted for publication on\n  Journal of the American Statistical\n  Association(http://www.amstat.org/publications/JASA/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a computationally efficient approach --\nspace(Sparse PArtial Correlation Estimation)-- for selecting non-zero partial\ncorrelations under the high-dimension-low-sample-size setting. This method\nassumes the overall sparsity of the partial correlation matrix and employs\nsparse regression techniques for model fitting. We illustrate the performance\nof space by extensive simulation studies. It is shown that space performs well\nin both non-zero partial correlation selection and the identification of hub\nvariables, and also outperforms two existing methods. We then apply space to a\nmicroarray breast cancer data set and identify a set of hub genes which may\nprovide important insights on genetic regulatory networks. Finally, we prove\nthat, under a set of suitable assumptions, the proposed procedure is\nasymptotically consistent in terms of model selection and parameter estimation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2008 02:37:46 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Peng", "Jie", ""], ["Wang", "Pei", ""], ["Zhou", "Nengfeng", ""], ["Zhu", "Ji", ""]]}]