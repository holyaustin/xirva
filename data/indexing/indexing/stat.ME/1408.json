[{"id": "1408.0047", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Cumulative Restricted Boltzmann Machines for Ordinal Matrix Data\n  Analysis", "comments": "JMLR: Workshop and Conference Proceedings 25:1-16, 2012; Asian\n  Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal data is omnipresent in almost all multiuser-generated feedback -\nquestionnaires, preferences etc. This paper investigates modelling of ordinal\ndata with Gaussian restricted Boltzmann machines (RBMs). In particular, we\npresent the model architecture, learning and inference procedures for both\nvector-variate and matrix-variate ordinal data. We show that our model is able\nto capture latent opinion profile of citizens around the world, and is\ncompetitive against state-of-art collaborative filtering techniques on\nlarge-scale public datasets. The model thus has the potential to extend\napplication of RBMs to diverse domains such as recommendation systems, product\nreviews and expert assessments.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 23:54:16 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0055", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Thurstonian Boltzmann Machines: Learning from Multiple Inequalities", "comments": "Proceedings of the 30 th International Conference on Machine\n  Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture\nthat can naturally incorporate a wide range of data inputs at the same time.\nOur motivation rests in the Thurstonian view that many discrete data types can\nbe considered as being generated from a subset of underlying latent continuous\nvariables, and in the observation that each realisation of a discrete type\nimposes certain inequalities on those variables. Thus learning and inference in\nTBM reduce to making sense of a set of inequalities. Our proposed TBM naturally\nsupports the following types: Gaussian, intervals, censored, binary,\ncategorical, muticategorical, ordinal, (in)-complete rank with and without\nties. We demonstrate the versatility and capacity of the proposed model on\nthree applications of very different natures; namely handwritten digit\nrecognition, collaborative filtering and complex social survey analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 00:32:32 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.0318", "submitter": "Tzu-Yu Liu", "authors": "Tzu-Yu Liu, Laura Trinchera, Arthur Tenenhaus, Dennis Wei, Alfred O.\n  Hero", "title": "Jointly Sparse Global SIMPLS Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial least squares (PLS) regression combines dimensionality reduction and\nprediction using a latent variable model. Since partial least squares\nregression (PLS-R) does not require matrix inversion or diagonalization, it can\nbe applied to problems with large numbers of variables. As predictor dimension\nincreases, variable selection becomes essential to avoid over-fitting, to\nprovide more accurate predictors and to yield more interpretable parameters. We\npropose a global variable selection approach that penalizes the total number of\nvariables across all PLS components. Put another way, the proposed global\npenalty encourages the selected variables to be shared among the PLS\ncomponents. We formulate PLS-R with joint sparsity as a variational\noptimization problem with objective function equal to a novel global SIMPLS\ncriterion plus a mixed norm sparsity penalty on the weight matrix. The mixed\nnorm sparsity penalty is the $\\ell_1$ norm of the $\\ell_2$ norm on the weights\ncorresponding to the same variable used over all the PLS components. A novel\naugmented Lagrangian method is proposed to solve the optimization problem and\nsoft thresholding for sparsity occurs naturally as part of the iterative\nsolution. Experiments show that the modified PLS-R attains better or as good\nperformance with many fewer selected predictor variables.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 23:19:49 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Liu", "Tzu-Yu", ""], ["Trinchera", "Laura", ""], ["Tenenhaus", "Arthur", ""], ["Wei", "Dennis", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1408.0462", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn and Hedibert Lopes", "title": "Shrinkage priors for linear instrumental variable models with many\n  instruments", "comments": "27 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper addresses the weak instruments problem in linear instrumental\nvariable models from a Bayesian perspective. The new approach has two\ncomponents. First, a novel predictor-dependent shrinkage prior is developed for\nthe many instruments setting. The prior is constructed based on a factor model\ndecomposition of the matrix of observed instruments, allowing many instruments\nto be incorporated into the analysis in a robust way.\n  Second, the new prior is implemented via an importance sampling scheme, which\nutilizes posterior Monte Carlo samples from a first-stage Bayesian regression\nanalysis. This modular computation makes sensitivity analyses straightforward.\n  Two simulation studies are provided to demonstrate the advantages of the new\nmethod. As an empirical illustration, the new method is used to estimate a key\nparameter in macro-economic models: the elasticity of inter-temporal\nsubstitution. The empirical analysis produces substantive conclusions in line\nwith previous studies, but certain inconsistencies of earlier analyses are\nresolved.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 06:50:05 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Hahn", "P. Richard", ""], ["Lopes", "Hedibert", ""]]}, {"id": "1408.0464", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn and Carlos M. Carvalho", "title": "Decoupling shrinkage and selection in Bayesian linear models: a\n  posterior summary perspective", "comments": "30 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Selecting a subset of variables for linear models remains an active area of\nresearch. This paper reviews many of the recent contributions to the Bayesian\nmodel selection and shrinkage prior literature. A posterior variable selection\nsummary is proposed, which distills a full posterior distribution over\nregression coefficients into a sequence of sparse linear predictors.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 07:17:31 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Hahn", "P. Richard", ""], ["Carvalho", "Carlos M.", ""]]}, {"id": "1408.0534", "submitter": "Bjoern Bornkamp", "authors": "Emma McCallum and Bj\\\"orn Bornkamp", "title": "Accounting for parameter uncertainty in two-stage designs for Phase II\n  dose-response studies", "comments": null, "journal-ref": "Updated version has been published in the book \"Modern Adaptive\n  Randomized Clinical Trials: Statistical and Practical Aspects\", 2015, Chapman\n  and Hall/CRC, p. 427-450", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider two-stage adaptive dose-response study designs,\nwhere the study design is changed at an interim analysis based on the\ninformation collected so far. In a simulation study, two approaches will be\ncompared for these type of designs; (i) updating the study design by\ncalculating the maximum likelihood estimate for the dose-response model\nparameters and then calculating the design for the second stage that is locally\noptimal for this estimate, and (ii) using the complete posterior distribution\nof the model parameter at interim to calculate a Bayesian optimal design (i.e.\ntaking into account parameter uncertainty). In particular, for an early interim\nanalysis respecting parameter uncertainty seems more adequate, on the other\nhand for a Bayesian approach dependency on the prior is expected and an\nadequately thought-through prior is required. A computationally efficient\nmethod is proposed for calculating the Bayesian design at interim based on\napproximating the full posterior sample using k-means clustering. The sigmoid\nEmax dose-response model and the D-optimality criterion will be used in this\npaper.\n", "versions": [{"version": "v1", "created": "Sun, 3 Aug 2014 19:39:56 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 07:19:16 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["McCallum", "Emma", ""], ["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1408.0705", "submitter": "Francis DiTraglia", "authors": "Francis J. DiTraglia", "title": "Using Invalid Instruments on Purpose: Focused Moment Selection and\n  Averaging for GMM", "comments": null, "journal-ref": "Journal of Econometrics, Volume 195, Issue 2, December 2016, Pages\n  187-208", "doi": "10.1016/j.jeconom.2016.07.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finite samples, the use of a slightly endogenous but highly relevant\ninstrument can reduce mean-squared error (MSE). Building on this observation, I\npropose a novel moment selection procedure for GMM -- the Focused Moment\nSelection Criterion (FMSC) -- in which moment conditions are chosen not based\non their validity but on the MSE of their associated estimator of a\nuser-specified target parameter. The FMSC mimics the situation faced by an\napplied researcher who begins with a set of relatively mild \"baseline\"\nassumptions and must decide whether to impose any of a collection of stronger\nbut more controversial \"suspect\" assumptions. When the (correctly specified)\nbaseline moment conditions identify the model, the FMSC provides an\nasymptotically unbiased estimator of asymptotic MSE, allowing us to select over\nthe suspect moment conditions. I go on to show how the framework used to derive\nthe FMSC can address the problem of inference post-moment selection. Treating\npost-selection estimators as a special case of moment-averaging, in which\nestimators based on different moment sets are given data-dependent weights, I\npropose simulation-based procedures for inference that can be applied to a\nvariety of formal and informal moment-selection and averaging procedures. Both\nthe FMSC and confidence interval procedures perform well in simulations. I\nconclude with an empirical example examining the effect of instrument selection\non the estimated relationship between malaria and income per capita.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 15:15:42 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 00:22:48 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 11:21:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["DiTraglia", "Francis J.", ""]]}, {"id": "1408.0711", "submitter": "Florence Forbes", "authors": "Darren Wraith and Florence Forbes", "title": "Clustering using skewed multivariate heavy tailed distributions with\n  flexible tail behaviour", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of location and scale mixtures of Gaussians has the ability to\ngenerate a number of flexible distributional forms. It nests as particular\ncases several important asymmetric distributions like the Generalised\nHyperbolic distribution. The Generalised Hyperbolic distribution in turn nests\nmany other well known distributions such as the Normal Inverse Gaussian (NIG)\nwhose practical relevance has been widely documented in the literature. In a\nmultivariate setting, we propose to extend the standard location and scale\nmixture concept into a so called multiple scaled framework which has the\nadvantage of allowing different tail and skewness behaviours in each dimension\nof the variable space with arbitrary correlation between dimensions. Estimation\nof the parameters is provided via an EM algorithm with a particular focus on\nNIG distributions. Inference is then extended to cover the case of mixtures of\nsuch multiple scaled distributions for application to clustering. Assessments\non simulated and real data confirm the gain in degrees of freedom and\nflexibility in modelling data of varying tail behaviour and directional shape.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 16:01:37 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Wraith", "Darren", ""], ["Forbes", "Florence", ""]]}, {"id": "1408.0856", "submitter": "Eric Chi", "authors": "Eric C. Chi, Genevera I. Allen, and Richard G. Baraniuk", "title": "Convex Biclustering", "comments": "29 pages, 3 figures", "journal-ref": "Biometrics 73 (1):10-19, 2017", "doi": "10.1111/biom.12540", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the biclustering problem, we seek to simultaneously group observations and\nfeatures. While biclustering has applications in a wide array of domains,\nranging from text mining to collaborative filtering, the problem of identifying\nstructure in high dimensional genomic data motivates this work. In this\ncontext, biclustering enables us to identify subsets of genes that are\nco-expressed only within a subset of experimental conditions. We present a\nconvex formulation of the biclustering problem that possesses a unique global\nminimizer and an iterative algorithm, COBRA, that is guaranteed to identify it.\nOur approach generates an entire solution path of possible biclusters as a\nsingle tuning parameter is varied. We also show how to reduce the problem of\nselecting this tuning parameter to solving a trivial modification of the convex\nbiclustering problem. The key contributions of our work are its simplicity,\ninterpretability, and algorithmic guarantees - features that arguably are\nlacking in the current alternative algorithms. We demonstrate the advantages of\nour approach, which includes stably and reproducibly identifying biclusterings,\non simulated and real microarray data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 03:27:17 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 21:39:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 00:09:49 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 19:57:55 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chi", "Eric C.", ""], ["Allen", "Genevera I.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1408.0881", "submitter": "James Dowty", "authors": "James G. Dowty", "title": "Volumes of logistic regression models with applications to model\n  selection", "comments": "Improved the section on volume jumps and added a new volume bound\n  (Theorem 13) for models with generic design matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models with $n$ observations and $q$ linearly-independent\ncovariates are shown to have Fisher information volumes which are bounded below\nby $\\pi^q$ and above by ${n \\choose q} \\pi^q$. This is proved with a novel\ngeneralization of the classical theorems of Pythagoras and de Gua, which is of\nindependent interest. The finding that the volume is always finite is new, and\nit implies that the volume can be directly interpreted as a measure of model\ncomplexity. The volume is shown to be a continuous function of the design\nmatrix $X$ at generic $X$, but to be discontinuous in general. This means that\nmodels with sparse design matrices can be significantly less complex than\nnearby models, so the resulting model-selection criterion prefers sparse\nmodels. This is analogous to the way that $\\ell^1$-regularisation tends to\nprefer sparse model fits, though in our case this behaviour arises\nspontaneously from general principles. Lastly, an unusual topological duality\nis shown to exist between the ideal boundaries of the natural and expectation\nparameter spaces of logistic regression models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 07:29:02 GMT"}, {"version": "v2", "created": "Mon, 1 Sep 2014 06:37:46 GMT"}, {"version": "v3", "created": "Fri, 17 Oct 2014 01:56:04 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Dowty", "James G.", ""]]}, {"id": "1408.1027", "submitter": "Maria DeYoreo", "authors": "Maria DeYoreo and Athanasios Kottas", "title": "Bayesian Nonparametric Modeling for Multivariate Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Univariate or multivariate ordinal responses are often assumed to arise from\na latent continuous parametric distribution, with covariate effects which enter\nlinearly. We introduce a Bayesian nonparametric modeling approach for\nunivariate and multivariate ordinal regression, which is based on mixture\nmodeling for the joint distribution of latent responses and covariates. The\nmodeling framework enables highly flexible inference for ordinal regression\nrelationships, avoiding assumptions of linearity or additivity in the covariate\neffects. In standard parametric ordinal regression models, computational\nchallenges arise from identifiability constraints and estimation of parameters\nrequiring nonstandard inferential techniques. A key feature of the\nnonparametric model is that it achieves inferential flexibility, while avoiding\nthese difficulties. In particular, we establish full support of the\nnonparametric mixture model under fixed cut-off points that relate through\ndiscretization the latent continuous responses with the ordinal responses. The\npractical utility of the modeling approach is illustrated through application\nto two data sets from econometrics, an example involving regression\nrelationships for ozone concentration, and a multirater agreement problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Aug 2014 16:40:06 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 14:59:17 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 15:02:17 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["DeYoreo", "Maria", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1408.1149", "submitter": "Richard Olshen", "authors": "Yi Liu, Andrew Z. Fire, Scott Boyd and Richard A. Olshen", "title": "Estimating Clonality", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenges of assessing complexity and clonality in populations of mixed\nspecies arise in diverse areas of modern biology, including estimating\ndiversity and clonality in microbiome populations, measuring patterns of T and\nB cell clonality, and determining the underlying tumor cell population\nstructure in cancer. Here we address the problem of quantifying populations,\nwith our analysis directed toward systems for which previously defined\nalgorithms allow the sequence-based identification of clonal subpopulations.\nData come from replicate sequencing libraries generated from a sample,\npotentially with very different depths. While certain properties of the\nunderlying clonal distribution (most notably the total number of clones) are\ndifficult to estimate accurately from data representing a small fraction of the\ntotal population, the population-level \"clonality\" metric that is the sum of\nsquared probabilities of the respective species can be calculated. (This is the\nsum of squared entries of a high-dimensional vector $p$ of relative\nfrequencies.) The clonality score is the probability of a clonal relationship\nbetween two randomly chosen members of the population of interest. A principal\ntakeaway message is that knowing a functional of $p$ well may not depend on\nknowing $p$ itself very well.\n  Our work has led to software, which we call {\\it lymphclon}; it has been\ndeposited in the CRAN library.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 00:14:20 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Liu", "Yi", ""], ["Fire", "Andrew Z.", ""], ["Boyd", "Scott", ""], ["Olshen", "Richard A.", ""]]}, {"id": "1408.1160", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Mixed-Variate Restricted Boltzmann Machines", "comments": "Originally published in Proceedings of ACML'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern datasets are becoming heterogeneous. To this end, we present in this\npaper Mixed-Variate Restricted Boltzmann Machines for simultaneously modelling\nvariables of multiple types and modalities, including binary and continuous\nresponses, categorical options, multicategorical choices, ordinal assessment\nand category-ranked preferences. Dependency among variables is modeled using\nlatent binary variables, each of which can be interpreted as a particular\nhidden aspect of the data. The proposed model, similar to the standard RBMs,\nallows fast evaluation of the posterior for the latent variables. Hence, it is\nnaturally suitable for many common tasks including, but not limited to, (a) as\na pre-processing step to convert complex input data into a more convenient\nvectorial representation through the latent posteriors, thereby offering a\ndimensionality reduction capacity, (b) as a classifier supporting binary,\nmulticlass, multilabel, and label-ranking outputs, or a regression tool for\ncontinuous outputs and (c) as a data completion tool for multimodal and\nheterogeneous data. We evaluate the proposed model on a large-scale dataset\nusing the world opinion survey results on three tasks: feature extraction and\nvisualization, data completion and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 01:43:05 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1408.1162", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh, Hung H. Bui", "title": "MCMC for Hierarchical Semi-Markov Conditional Random Fields", "comments": "NIPS'09 Workshop on Deep Learning for Speech Recognition and Related\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep architecture such as hierarchical semi-Markov models is an important\nclass of models for nested sequential data. Current exact inference schemes\neither cost cubic time in sequence length, or exponential time in model depth.\nThese costs are prohibitive for large-scale problems with arbitrary length and\ndepth. In this contribution, we propose a new approximation technique that may\nhave the potential to achieve sub-cubic time complexity in length and linear\ntime depth, at the cost of some loss of quality. The idea is based on two\nwell-known methods: Gibbs sampling and Rao-Blackwellisation. We provide some\nsimulation-based evaluation of the quality of the RGBS with respect to run time\nand sequence length.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 02:04:43 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""], ["Bui", "Hung H.", ""]]}, {"id": "1408.1187", "submitter": "Mattia Ciollaro", "authors": "Mattia Ciollaro, Christopher Genovese, Jing Lei and Larry Wasserman", "title": "The functional mean-shift algorithm for mode hunting and clustering in\n  infinite dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the functional mean-shift algorithm, an iterative algorithm for\nestimating the local modes of a surrogate density from functional data. We show\nthat the algorithm can be used for cluster analysis of functional data. We\npropose a test based on the bootstrap for the significance of the estimated\nlocal modes of the surrogate density. We present two applications of our\nmethodology. In the first application, we demonstrate how the functional\nmean-shift algorithm can be used to perform spike sorting, i.e. cluster neural\nactivity curves. In the second application, we use the functional mean-shift\nalgorithm to distinguish between original and fake signatures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 05:12:04 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Ciollaro", "Mattia", ""], ["Genovese", "Christopher", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1408.1191", "submitter": "Duncan Lee", "authors": "Duncan Lee and Andrew Lawson", "title": "Cluster detection and risk estimation for spatio-temporal health data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological disease mapping one aims to estimate the spatio-temporal\npattern in disease risk and identify high-risk clusters, allowing health\ninterventions to be appropriately targeted. Bayesian spatio-temporal models are\nused to estimate smoothed risk surfaces, but this is contrary to the aim of\nidentifying groups of areal units that exhibit elevated risks compared with\ntheir neighbours. Therefore, in this paper we propose a new Bayesian\nhierarchical modelling approach for simultaneously estimating disease risk and\nidentifying high-risk clusters in space and time. Inference for this model is\nbased on Markov chain Monte Carlo simulation, using the freely available R\npackage CARBayesST that has been developed in conjunction with this paper. Our\nmethodology is motivated by two case studies, the first of which assesses if\nthere is a relationship between Public health Districts and colon cancer\nclusters in Georgia, while the second looks at the impact of the smoking ban in\npublic places in England on cardiovascular disease clusters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 06:18:48 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 06:42:03 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Lee", "Duncan", ""], ["Lawson", "Andrew", ""]]}, {"id": "1408.1239", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "The Minimum S-Divergence Estimator under Continuous Models: The\n  Basu-Lindsay Approach", "comments": "Pre-Print, 34 pages", "journal-ref": null, "doi": "10.1007/s00362-015-0701-3", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical maximum likelihood based\ntechniques. Recently Ghosh et al. (2013) proposed a general class of divergence\nmeasures for robust statistical inference, named the S-Divergence Family. Ghosh\n(2014) discussed its asymptotic properties for the discrete model of densities.\nIn the present paper, we develop the asymptotic properties of the proposed\nminimum S-Divergence estimators under continuous models. Here we use the\nBasu-Lindsay approach (1994) of smoothing the model densities that, unlike\nprevious approaches, avoids much of the complications of the kernel bandwidth\nselection. Illustrations are presented to support the performance of the\nresulting estimators both in terms of efficiency and robustness through\nextensive simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 10:50:45 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:33:29 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1408.1368", "submitter": "Georgios Papageorgiou", "authors": "Georgios Papageorgiou and Sylvia Richardson and Nicky Best", "title": "Bayesian nonparametric models for spatially indexed data of mixed type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Bayesian nonparametric models for spatially indexed data of mixed\ntype. Our work is motivated by challenges that occur in environmental\nepidemiology, where the usual presence of several confounding variables that\nexhibit complex interactions and high correlations makes it difficult to\nestimate and understand the effects of risk factors on health outcomes of\ninterest. The modeling approach we adopt assumes that responses and confounding\nvariables are manifestations of continuous latent variables, and uses\nmultivariate Gaussians to jointly model these. Responses and confounding\nvariables are not treated equally as relevant parameters of the distributions\nof the responses only are modeled in terms of explanatory variables or risk\nfactors. Spatial dependence is introduced by allowing the weights of the\nnonparametric process priors to be location specific, obtained as probit\ntransformations of Gaussian Markov random fields. Confounding variables and\nspatial configuration have a similar role in the model, in that they only\ninfluence, along with the responses, the allocation probabilities of the areas\ninto the mixture components, thereby allowing for flexible adjustment of the\neffects of observed confounders, while allowing for the possibility of residual\nspatial structure, possibly occurring due to unmeasured or undiscovered\nspatially varying factors. Aspects of the model are illustrated in simulation\nstudies and an application to a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 17:54:46 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 13:26:18 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 21:19:19 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Papageorgiou", "Georgios", ""], ["Richardson", "Sylvia", ""], ["Best", "Nicky", ""]]}, {"id": "1408.2128", "submitter": "Paul McNicholas", "authors": "Antonio Punzo, Martin Blostein and Paul D. McNicholas", "title": "High-dimensional unsupervised classification via parsimonious\n  contaminated mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contaminated Gaussian distribution represents a simple heavy-tailed\nelliptical generalization of the Gaussian distribution; unlike the\noften-considered t-distribution, it also allows for automatic detection of mild\noutlying or \"bad\" points in the same way that observations are typically\nassigned to the groups in the finite mixture model context. Starting from this\ndistribution, we propose the contaminated factor analysis model as a method for\ndimensionality reduction and detection of bad points in higher dimensions. A\nmixture of contaminated Gaussian factor analyzers (MCGFA) model follows\ntherefrom, and extends the recently proposed mixture of contaminated Gaussian\ndistributions to high-dimensional data. We introduce a family of 32\nparsimonious models formed by introducing constraints on the covariance and\ncontamination structures of the general MCGFA model. We outline a variant of\nthe expectation-maximization algorithm for parameter estimation. Various\nimplementation issues are discussed, and the novel family of models is compared\nto well-established approaches on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 15:39:13 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 22:13:32 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 21:23:37 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 18:33:38 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Punzo", "Antonio", ""], ["Blostein", "Martin", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1408.2153", "submitter": "Kiranmoy Chatterjee Mr.", "authors": "Kiranmoy Chatterjee and Diganta Mukherjee", "title": "On the Estimation of Homogeneous Population Size in a Complex\n  Dual-record System", "comments": "25 pages, 3 figures", "journal-ref": "Kiranmoy Chatterjee & Diganta Mukherjee (2016) On the estimation\n  of homogeneous population size from a complex dual-record system, Journal of\n  Statistical Computation and Simulation, 86:17, 3562-3581", "doi": "10.1080/00949655.2016.1173695", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-record system (DRS) (equivalently two sample Capture-recapture\nexperiment) model with time and behavioral response variation, has attracted\nmuch attention specifically in the domain of Official Statistics and\nEpidemiology. The relevant model suffers from parameter identifiability problem\nand proper Bayesian methodologies could be helpful to overcome the situation.\nIn this article, we have formulated the population size estimation problem in\nDRS as a missing data analysis under both the known and unknown directional\nnature of underlying behavioral response effect. Two simple empirical Bayes\napproaches are proposed and investigated their performances for this complex\nmodel along with a fully Bayes treatment. Extensive simulation studies are\ncarried out to compare the performances of these competitive approaches and a\nreal data example is also illustrated. Finally, some features of these methods\nand recommendations to implement them in practice are explored depending upon\nthe availability of knowledge on the nature of behavioral response effect.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 21:01:18 GMT"}, {"version": "v2", "created": "Tue, 12 Aug 2014 06:13:40 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Mukherjee", "Diganta", ""]]}, {"id": "1408.2255", "submitter": "Ali Akbar Jafari", "authors": "Hojatollah Zakerzadeh and Ali Akbar Jafari", "title": "Comparing the Shape Parameters of Two Weibull Distributions Using\n  Records: A Generalized Inference", "comments": "Accepted for publication in Journal of Statistical Research of Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weibull distribution is a very applicable model for the lifetime data.\nFor inference about two Weibull distributions using records, the shape\nparameters of the distributions are usually considered equal. However, there is\nnot an appropriate method for comparing the shape parameters in the literature.\nTherefore, comparing the shape parameters of two Weibull distributions is very\nimportant. In this paper, we propose a method for constructing confidence\ninterval and testing hypotheses about the ratio and difference of shape\nparameters using the concept of the generalized {\\it p}-value and the\ngeneralized confidence interval. Simulation studies showed that our method is\nsatisfactory. In the end, a real example is proposed to illustrate this method.\n", "versions": [{"version": "v1", "created": "Sun, 10 Aug 2014 17:38:52 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Zakerzadeh", "Hojatollah", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1408.2441", "submitter": "Grant Schneider", "authors": "Grant Schneider, Peter F. Craigmile, Radu Herbei", "title": "Maximum Likelihood Estimation for Stochastic Differential Equations\n  Using Sequential Kriging-Based Optimization", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Differential Equations (SDEs) are used as statistical models in\nmany disciplines. However, intractable likelihood functions for SDEs make\ninference challenging, and we need to resort to simulation-based techniques to\nestimate and maximize the likelihood function. While sequential Monte Carlo\nmethods have allowed for the accurate evaluation of likelihoods at fixed\nparameter values, there is still a question of how to find the maximum\nlikelihood estimate. In this article we propose an efficient\nGaussian-process-based method for exploring the parameter space using estimates\nof the likelihood from a sequential Monte Carlo sampler. Our method accounts\nfor the inherent Monte Carlo variability of the estimated likelihood, and does\nnot require knowledge of gradients. The procedure adds potential parameter\nvalues by maximizing the so-called expected improvement, leveraging the fact\nthat the likelihood function is assumed to be smooth. Our simulations\ndemonstrate that our method has significant computational and efficiency gains\nover existing grid- and gradient-based techniques. Our method is applied to\nmodeling the closing stock price of three technology firms.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 15:29:51 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Schneider", "Grant", ""], ["Craigmile", "Peter F.", ""], ["Herbei", "Radu", ""]]}, {"id": "1408.2504", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Compressed Sensing with Very Sparse Gaussian Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of very sparse random projections for compressed sensing\n(sparse signal recovery) when the signal entries can be either positive or\nnegative. In our setting, the entries of a Gaussian design matrix are randomly\nsparsified so that only a very small fraction of the entries are nonzero. Our\nproposed decoding algorithm is simple and efficient in that the major cost is\none linear scan of the coordinates. We have developed two estimators: (i) the\n{\\em tie estimator}, and (ii) the {\\em absolute minimum estimator}. Using only\nthe tie estimator, we are able to recover a $K$-sparse signal of length $N$\nusing $1.551 eK \\log K/\\delta$ measurements (where $\\delta\\leq 0.05$ is the\nconfidence). Using only the absolute minimum estimator, we can detect the\nsupport of the signal using $eK\\log N/\\delta$ measurements. For a particular\ncoordinate, the absolute minimum estimator requires fewer measurements (i.e.,\nwith a constant $e$ instead of $1.551e$). Thus, the two estimators can be\ncombined to form an even more practical decoding framework.\n  Prior studies have shown that existing one-scan (or roughly one-scan)\nrecovery algorithms using sparse matrices would require substantially more\n(e.g., one order of magnitude) measurements than L1 decoding by linear\nprogramming, when the nonzero entries of signals can be either negative or\npositive. In this paper, following a known experimental setup, we show that, at\nthe same number of measurements, the recovery accuracies of our proposed method\nare (at least) similar to the standard L1 decoding.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2014 19:55:11 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1408.2757", "submitter": "Scott Holan", "authors": "Wen-Hsi Yang and Scott H. Holan and Christopher K. Wikle", "title": "Bayesian Lattice Filters for Time-Varying Autoregression and\n  Time-Frequency Analysis", "comments": "49 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling nonstationary processes is of paramount importance to many\nscientific disciplines including environmental science, ecology, and finance,\namong others. Consequently, flexible methodology that provides accurate\nestimation across a wide range of processes is a subject of ongoing interest.\nWe propose a novel approach to model-based time-frequency estimation using\ntime-varying autoregressive models. In this context, we take a fully Bayesian\napproach and allow both the autoregressive coefficients and innovation variance\nto vary over time. Importantly, our estimation method uses the lattice filter\nand is cast within the partial autocorrelation domain. The marginal posterior\ndistributions are of standard form and, as a convenient by-product of our\nestimation method, our approach avoids undesirable matrix inversions. As such,\nestimation is extremely computationally efficient and stable. To illustrate the\neffectiveness of our approach, we conduct a comprehensive simulation study that\ncompares our method with other competing methods and find that, in most cases,\nour approach performs superior in terms of average squared error between the\nestimated and true time-varying spectral density. Lastly, we demonstrate our\nmethodology through three modeling applications; namely, insect communication\nsignals, environmental data (wind components), and macroeconomic data (US gross\ndomestic product (GDP) and consumption).\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 15:58:36 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Yang", "Wen-Hsi", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1408.2863", "submitter": "Karim Abou-Moustafa", "authors": "Karim T. Abou-Moustafa", "title": "Divergence Measures as Diversity Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Entropy measures of probability distributions are widely used measures in\necology, biology, genetics, and in other fields, to quantify species diversity\nof a community. Unfortunately, entropy-based diversity indices, or diversity\nindices for short, suffer from three problems. First, when computing the\ndiversity for samples withdrawn from communities with different structures,\ndiversity indices can easily yield non-comparable and hard to interpret\nresults. Second, diversity indices impose weighting schemes on the species\ndistributions that unnecessarily emphasize low abundant rare species, or\nerroneously identified ones. Third, diversity indices do not allow for\ncomparing distributions against each other, which is necessary when a community\nhas a well-known species' distribution.\n  In this paper we propose a new general methodology based on information\ntheoretic principles to quantify the species diversity of a community. Our\nmethodology, comprised of two steps, naturally overcomes the previous mentioned\nproblems, and yields comparable and easy to interpret diversity values. We show\nthat our methodology retains all the functional properties of any diversity\nindex, and yet is far more flexible than entropy--based diversity indices. Our\nmethodology is easy to implement and is applicable to any community of\ninterest.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 21:47:03 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Abou-Moustafa", "Karim T.", ""]]}, {"id": "1408.2923", "submitter": "Panos Toulis", "authors": "Panos Toulis and Edoardo M. Airoldi", "title": "Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients", "comments": "Annals of Statistics, 2016, forthcoming; 71 pages, 37-page main body;\n  9 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent procedures have gained popularity for parameter\nestimation from large data sets. However, their statistical properties are not\nwell understood, in theory. And in practice, avoiding numerical instability\nrequires careful tuning of key parameters. Here, we introduce implicit\nstochastic gradient descent procedures, which involve parameter updates that\nare implicitly defined. Intuitively, implicit updates shrink standard\nstochastic gradient descent updates. The amount of shrinkage depends on the\nobserved Fisher information matrix, which does not need to be explicitly\ncomputed; thus, implicit procedures increase stability without increasing the\ncomputational burden. Our theoretical analysis provides the first full\ncharacterization of the asymptotic behavior of both standard and implicit\nstochastic gradient descent-based estimators, including finite-sample error\nbounds. Importantly, analytical expressions for the variances of these\nstochastic gradient-based estimators reveal their exact loss of efficiency. We\nalso develop new algorithms to compute implicit stochastic gradient\ndescent-based estimators for generalized linear models, Cox proportional\nhazards, M-estimators, in practice, and perform extensive experiments. Our\nresults suggest that implicit stochastic gradient descent procedures are poised\nto become a workhorse for approximate inference from large data sets\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 06:47:25 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 16:43:26 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 00:27:00 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 17:17:20 GMT"}, {"version": "v5", "created": "Sun, 4 Oct 2015 21:11:17 GMT"}, {"version": "v6", "created": "Wed, 28 Sep 2016 15:29:27 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1408.3027", "submitter": "Emanuela Dreassi prof", "authors": "Emanuela Dreassi and Emilia Rocco", "title": "A Bayesian semiparametric model for semicontinuous data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the target variable exhibits a semicontinuous behaviour (i.e. a point\nmass in a single value and a continuous distribution elsewhere) parametric\n`two-part regression models' have been extensively used and investigated. In\nthis paper, a semiparametric Bayesian two-part regression model for dealing\nwith such variables is proposed. The model allows a semiparametric expression\nfor the two part of the model by using Dirichlet processes. A motivating\nexample (in the `small area estimation' framework) based on pseudo-real data on\ngrapewine production in Tuscany, is used to evaluate the capabilities of the\nmodel. Results show a satisfactory performance of the suggested approach to\nmodel and predict semicontinuous data when parametric assumptions\n(distributional and/or relationship) are not reasonable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 15:20:55 GMT"}], "update_date": "2014-08-14", "authors_parsed": [["Dreassi", "Emanuela", ""], ["Rocco", "Emilia", ""]]}, {"id": "1408.3041", "submitter": "Sourabh Bhattacharya", "authors": "Satyaki Mazumder and Sourabh Bhattacharya", "title": "Bayesian Nonparametric Dynamic State Space Modeling with Circular Latent\n  States", "comments": "A revised version, particularly demonstrating quite encouraging\n  performances of our model and methods on a simulated, and two real data sets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models are well-known for their versatility in modeling dynamic\nsystems that arise in various scientific disciplines. Although parametric state\nspace models are well studied, nonparametric approaches are much less explored\nin comparison. In this article we propose a novel Bayesian nonparametric\napproach to state space modeling assuming that both the observational and\nevolutionary functions are unknown and are varying with time; crucially, we\nassume that the unknown evolutionary equation describes dynamic evolution of\nsome latent circular random variable.\n  Based on appropriate kernel convolution of the standard Wiener process we\nmodel the time-varying observational and evolutionary functions as suitable\nGaussian processes that take both linear and circular variables as arguments.\nAdditionally, for the time-varying evolutionary function, we wrap the Gaussian\nprocess thus constructed around the unit circle to form an appropriate circular\nGaussian process. We show that our process thus created satisfies desirable\nproperties.\n  For the purpose of inference we develop an MCMC based methodology combining\nGibbs sampling and Metropolis-Hastings algorithms. Applications to a simulated\ndata set, a real wind speed data set and a real ozone data set demonstrated\nquite encouraging performances of our model and methodologies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 16:09:33 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 15:29:56 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Mazumder", "Satyaki", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1408.3174", "submitter": "Reza Hosseini", "authors": "Reza Hosseini", "title": "Utilizing wind in spatial covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a covariance function which allows for a stronger spatial\ncorrelation for pairs of points in the direction of a vector such as wind and\nweaker for pairs which are perpendicular to it. It derives a simple covariance\nfunction by stretching the space along the wind axes (upwind and across wind\naxes). It is shown that this covariance function is anisotropy in the original\nspace and the functions is explicitly calculated.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 00:43:32 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Hosseini", "Reza", ""]]}, {"id": "1408.3237", "submitter": "Dan Jackson", "authors": "Rose Baker and Dan Jackson", "title": "A new distribution for robust least squares", "comments": "29 pages. 5 figures provided at the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new distribution is introduced, which we call the twin-t distribution. This\ndistribution is heavy-tailed like the t distribution, but closer to normality\nin the central part of the curve. Its properties are described, e.g. the pdf,\nthe distribution function, moments, and random number generation. This\ndistribution could have many applications, but here we focus on its use as an\naid to robustness. We give examples of its application in robust regression and\nin curve fitting. Extensions such as skew and multivariate twin-t\ndistributions, and a twin of\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 09:59:05 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Baker", "Rose", ""], ["Jackson", "Dan", ""]]}, {"id": "1408.3333", "submitter": "Amy Willis", "authors": "A. Willis and J. Bunge", "title": "Estimating Diversity via Frequency Ratios", "comments": "17 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to estimate the total number of classes in a population based on\nsample counts, especially in the presence of high latent diversity. Drawing on\nprobability theory that characterizes distributions on the integers by ratios\nof consecutive probabilities, we construct a nonlinear regression model for the\nratios of consecutive frequency counts. This allows us to predict the\nunobserved count and hence estimate the total diversity. We believe that this\nis the first approach to depart from the classical mixed Poisson model in this\nproblem. Our method is geometrically intuitive and yields good fits to data\nwith reasonable standard errors. It is especially well-suited to analyzing high\ndiversity datasets derived from next-generation sequencing in microbial\necology. We demonstrate the method's performance in this context and via\nsimulation, and we present a dataset for which our method outperforms all\ncompetitors.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 16:31:08 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 17:16:29 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Willis", "A.", ""], ["Bunge", "J.", ""]]}, {"id": "1408.3386", "submitter": "Marianna Pensky", "authors": "Marianna Pensky", "title": "Solution of linear ill-posed problems using overcomplete dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider application of overcomplete dictionaries to\nsolution of general ill-posed linear inverse problems. Construction of an\nadaptive optimal solution for such problems usually relies either on a singular\nvalue decomposition or representation of the solution via an orthonormal basis.\nThe shortcoming of both approaches lies in the fact that, in many situations,\nneither the eigenbasis of the linear operator nor a standard orthonormal basis\nconstitutes an appropriate collection of functions for sparse representation of\nthe unknown function. In the context of regression problems, there have been an\nenormous amount of effort to recover an unknown function using an overcomplete\ndictionary. One of the most popular methods, Lasso, is based on minimizing the\nempirical likelihood and requires stringent assumptions on the dictionary, the,\nso called, compatibility conditions. While these conditions may be satisfied\nfor the original dictionary functions, they usually do not hold for their\nimages due to contraction imposed by the linear operator. In what follows, we\nbypass this difficulty by a novel approach which is based on inverting each of\nthe dictionary functions and matching the resulting expansion to the true\nfunction, thus, avoiding unrealistic assumptions on the dictionary and using\nLasso in a predictive setting. We examine both the white noise and the\nobservational model formulations and also discuss how exact inverse images of\nthe dictionary functions can be replaced by their approximate counterparts.\nFurthermore, we show how the suggested methodology can be extended to the\nproblem of estimation of a mixing density in a continuous mixture. For all the\nsituations listed above, we provide the oracle inequalities for the risk in a\nfinite sample setting. Simulation studies confirm good computational properties\nof the Lasso-based technique.\n", "versions": [{"version": "v1", "created": "Thu, 14 Aug 2014 19:00:11 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 17:53:01 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Pensky", "Marianna", ""]]}, {"id": "1408.3467", "submitter": "Qianqian Xu", "authors": "Qianqian Xu and Jiechao Xiong and Xiaochun Cao and Qingming Huang and\n  Yuan Yao", "title": "Evaluating Visual Properties via Robust HodgeRank", "comments": "25 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, how to effectively evaluate visual properties has become a popular\ntopic for fine-grained visual comprehension. In this paper we study the problem\nof how to estimate such visual properties from a ranking perspective with the\nhelp of the annotators from online crowdsourcing platforms. The main challenges\nof our task are two-fold. On one hand, the annotations often contain\ncontaminated information, where a small fraction of label flips might ruin the\nglobal ranking of the whole dataset. On the other hand, considering the large\ndata capacity, the annotations are often far from being complete. What is\nworse, there might even exist imbalanced annotations where a small subset of\nsamples are frequently annotated. Facing such challenges, we propose a robust\nranking framework based on the principle of Hodge decomposition of imbalanced\nand incomplete ranking data. According to the HodgeRank theory, we find that\nthe major source of the contamination comes from the cyclic ranking component\nof the Hodge decomposition. This leads us to an outlier detection formulation\nas sparse approximations of the cyclic ranking projection. Taking a step\nfurther, it facilitates a novel outlier detection model as Huber's LASSO in\nrobust statistics. Moreover, simple yet scalable algorithms are developed based\non Linearized Bregman Iteration to achieve an even less biased estimator.\nStatistical consistency of outlier detection is established in both cases under\nnearly the same conditions. Our studies are supported by experiments with both\nsimulated examples and real-world data. The proposed framework provides us a\npromising tool for robust ranking with large scale crowdsourcing data arising\nfrom computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 05:18:19 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:06:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1408.3490", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "Properties and Applications of some Distributions derived from\n  Frullani's integral", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frullani's integral dates from 1821, but a probabilistic interpretation of it\nhas never been made. In this paper, Frullani's integral formula is shown to\nresult from mixing a lifetime distribution by allowing the logarithm of the\nscale factor to be uniformly distributed over a finite range. This gives a\nclass of long-tailed distributions related to slash distributions, where the\npdf is simply expressed in terms of the survival function of the `parent'\ndistribution. The resulting survival distributions have all moments finite, and\ncan exhibit the bimodal hazard functions sometimes seen in practice. A\ndistribution of this type analogous to the t-distribution is derived, the\ncorresponding multivariate distributions are given, and two skewed versions of\nthis distribution are derived. The use of the mixed distributions for inference\nis exemplified by fitting them to several datasets. It is expected that there\nwill be many applications, in health, reliability, telecommunications, finance,\netc.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 08:03:36 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1408.3584", "submitter": "Daniel Fraiman", "authors": "Daniel Fraiman, Nicolas Fraiman, Ricardo Fraiman", "title": "Non Parametric Statistics of Dynamic Networks with distinguishable nodes", "comments": "24 pages, 6 figures. Title changed, Test (2017)", "journal-ref": null, "doi": "10.1007/s11749-017-0524-8", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech physics.data-an q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of random graphs and networks had an explosive development in the\nlast couple of decades. Meanwhile, techniques for the statistical analysis of\nsequences of networks were less developed. In this paper we focus on networks\nsequences with a fixed number of labeled nodes and study some statistical\nproblems in a nonparametric framework. We introduce natural notions of center\nand a depth function for networks that evolve in time. We develop several\nstatistical techniques including testing, supervised and unsupervised\nclassification, and some notions of principal component sets in the space of\nnetworks. Some examples and asymptotic results are given, as well as two real\ndata examples.\n", "versions": [{"version": "v1", "created": "Fri, 15 Aug 2014 16:38:19 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 01:47:34 GMT"}, {"version": "v3", "created": "Tue, 13 Jan 2015 18:50:14 GMT"}, {"version": "v4", "created": "Wed, 15 Jun 2016 05:32:44 GMT"}, {"version": "v5", "created": "Sat, 15 Apr 2017 14:21:53 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Fraiman", "Daniel", ""], ["Fraiman", "Nicolas", ""], ["Fraiman", "Ricardo", ""]]}, {"id": "1408.3783", "submitter": "Panos Toulis", "authors": "Panos Toulis and David C. Parkes", "title": "Long-term causal effects of economic mechanisms on agent incentives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic mechanisms administer the allocation of resources to interested\nagents based on their self-reported types. One objective in mechanism design is\nto design a strategyproof process so that no agent will have an incentive to\nmisreport its type. However, typical analyses of the incentives properties of\nmechanisms operate under strong, usually untestable assumptions. Empirical,\ndata-oriented approaches are, at best, under-developed. Furthermore,\nmechanism/policy evaluation methods usually ignore the dynamic nature of a\nmulti-agent system and are thus inappropriate for estimating long-term effects.\nWe introduce the problem of estimating the causal effects of mechanisms on\nincentives and frame it under the Rubin causal framework \\citep{rubin74,\nrubin78}. This raises unique technical challenges since the outcome of interest\n(agent truthfulness) is confounded with strategic interactions and,\ninterestingly, is typically never observed under any mechanism. We develop a\nmethodology to estimate such causal effects that using a prior that is based on\na strategic equilibrium model. Working on the domain of kidney exchanges, we\nshow how to apply our methodology to estimate causal effects of kidney\nallocation mechanisms on hospitals' incentives. Our results demonstrate that\nthe use of game-theoretic prior captures the dynamic nature of the kidney\nexchange multiagent system and shrinks the estimates towards long-term effects,\nthus improving upon typical methods that completely ignore agents' strategic\nbehavior.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 01:49:36 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 15:09:23 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Toulis", "Panos", ""], ["Parkes", "David C.", ""]]}, {"id": "1408.3807", "submitter": "David Barber", "authors": "David Barber", "title": "On solving Ordinary Differential Equations using Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of Gaussian Process based approaches that can be used to\nsolve non-linear Ordinary Differential Equations. We suggest an explicit\nprobabilistic solver and two implicit methods, one analogous to Picard\niteration and the other to gradient matching. All methods have greater accuracy\nthan previously suggested Gaussian Process approaches. We also suggest a\ngeneral approach that can yield error estimates from any standard ODE solver.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 09:52:06 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Barber", "David", ""]]}, {"id": "1408.3845", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Patrick Rubin-Delanchy and Nicholas A. Heard", "title": "A test for dependence between two point processes on the real line", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific questions rely on determining whether two sequences of event\ntimes are associated. This article introduces a likelihood ratio test which can\nbe parameterised in several ways to detect different forms of dependence. A\ncommon finite-sample distribution is derived, and shown to be asymptotically\nrelated to a weighted Kolmogorov-Smirnov test. Analysis leading to these\nresults also motivates a more general tool for diagnosing dependence. The\nmethodology is demonstrated on data generated on an email network, showing\nevidence of information flow using only timing information. Implementation code\nis available in the R package `mppa'.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 17:44:16 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 22:51:04 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 14:43:37 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Rubin-Delanchy", "Patrick", ""], ["Heard", "Nicholas A.", ""]]}, {"id": "1408.3979", "submitter": "Holger Drees", "authors": "Holger Drees, Natalie Neumeyer, Leonie Selk", "title": "Hypotheses tests in boundary regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a nonparametric regression model with one-sided errors and\nregression function in a general H\\\"older class. We estimate the regression\nfunction via minimization of the local integral of a polynomial approximation.\nWe show uniform rates of convergence for the simple regression estimator as\nwell as for a smooth version. These rates carry over to mean regression models\nwith a symmetric and bounded error distribution. In such a setting, one obtains\nfaster rates for irregular error distributions concentrating sufficient mass\nnear the endpoints than for the usual regular distributions. The results are\napplied to prove asymptotic $\\sqrt{n}$-equivalence of a residual-based\n(sequential) empirical distribution function to the (sequential) empirical\ndistribution function of unobserved errors in the case of irregular error\ndistributions. This result is remarkably different from corresponding results\nin mean regression with regular errors. It can readily be applied to develop\ngoodness-of-fit tests for the error distribution. We present some examples and\ninvestigate the small sample performance in a simulation study. We further\ndiscuss asymptotically distribution-free hypotheses tests for independence of\nthe error distribution from the points of measurement and for monotonicity of\nthe boundary function as well.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 11:26:18 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 15:27:04 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Drees", "Holger", ""], ["Neumeyer", "Natalie", ""], ["Selk", "Leonie", ""]]}, {"id": "1408.4012", "submitter": "Gilles Blanchard", "authors": "Franziska G\\\"obel, Gilles Blanchard, Ulrike von Luxburg", "title": "Construction of Tight Frames on Graphs and Application to Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a neighborhood graph representation of a finite set of points\n$x_i\\in\\mathbb{R}^d,i=1,\\ldots,n,$ we construct a frame (redundant dictionary)\nfor the space of real-valued functions defined on the graph. This frame is\nadapted to the underlying geometrical structure of the $x_i$, has finitely many\nelements, and these elements are localized in frequency as well as in space.\nThis construction follows the ideas of Hammond et al. (2011), with the key\npoint that we construct a tight (or Parseval) frame. This means we have a very\nsimple, explicit reconstruction formula for every function $f$ defined on the\ngraph from the coefficients given by its scalar product with the frame\nelements. We use this representation in the setting of denoising where we are\ngiven noisy observations of a function $f$ defined on the graph. By applying a\nthresholding method to the coefficients in the reconstruction formula, we\ndefine an estimate of $f$ whose risk satisfies a tight oracle inequality.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 13:55:39 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 00:03:16 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["G\u00f6bel", "Franziska", ""], ["Blanchard", "Gilles", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1408.4026", "submitter": "Ruben Dezeure", "authors": "Ruben Dezeure, Peter B\\\"uhlmann, Lukas Meier, Nicolai Meinshausen", "title": "High-Dimensional Inference: Confidence Intervals, $p$-Values and\n  R-Software hdi", "comments": "Published at http://dx.doi.org/10.1214/15-STS527 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 533-558", "doi": "10.1214/15-STS527", "report-no": "IMS-STS-STS527", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a (selective) review of recent frequentist high-dimensional\ninference methods for constructing $p$-values and confidence intervals in\nlinear and generalized linear models. We include a broad, comparative empirical\nstudy which complements the viewpoint from statistical methodology and theory.\nFurthermore, we introduce and illustrate the R-package hdi which easily allows\nthe use of different methods and supports reproducibility.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 14:51:48 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 13:17:20 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Dezeure", "Ruben", ""], ["B\u00fchlmann", "Peter", ""], ["Meier", "Lukas", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1408.4050", "submitter": "Ignacio Alvarez", "authors": "Ignacio Alvarez, Jarad Niemi, and Matt Simpson", "title": "Bayesian inference for a covariance matrix", "comments": "Final version, already published in proceedings, Proceedings of 26th\n  Annual Conference on Applied Statistics in Agriculture. April 27-29, 2014", "journal-ref": "Anual Conference on Applied Statistics in Agriculture. 26 (2014)\n  71 - 82", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance matrix estimation arises in multivariate problems including\nmultivariate normal sampling models and regression models where random effects\nare jointly modeled, e.g. random-intercept, random-slope models. A Bayesian\nanalysis of these problems requires a prior on the covariance matrix. Here we\nassess, through a simulation study and a real data set, the impact this prior\nchoice has on posterior inference of the covariance matrix.\n  Inverse Wishart distribution is the natural choice for a covariance matrix\nprior because its conjugacy on normal model and simplicity, is usually\navailable in Bayesian statistical software. However inverse Wishart\ndistribution presents some undesirable properties from a modeling point of\nview. It can be too restrictive because assume the same amount of prior\ninformation about every variance parameters and, more important, it shows a\nprior relationship between the variances and correlations.\n  Some alternatives distributions has been proposed. The scaled inverse Wishart\ndistribution, which give more flexibility on the variance priors conserving the\nconjugacy property but does not eliminate the prior relationship between\nvariances and correlations. Secondly, it is possible to fit separate priors for\nindividual correlations and standard deviations. This strategy eliminates any\nprior relationship within the covariance matrix parameters, but it is not\nconjugate and therefore computationally slow.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 15:51:25 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 23:06:41 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Alvarez", "Ignacio", ""], ["Niemi", "Jarad", ""], ["Simpson", "Matt", ""]]}, {"id": "1408.4102", "submitter": "David Choi", "authors": "David S. Choi", "title": "Estimation of Monotone Treatment Effects in Network Experiments", "comments": "new methods and data examples added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments on social networks pose statistical challenges, due to\nthe possibility of interference between units. We propose new methods for\nestimating attributable treatment effects in such settings. The methods do not\nrequire partial interference, but instead require an identifying assumption\nthat is similar to requiring nonnegative treatment effects. Network or spatial\ninformation can be used to customize the test statistic; in principle, this can\nincrease power without making assumptions on the data generating process.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 19:17:26 GMT"}, {"version": "v2", "created": "Tue, 19 Aug 2014 17:35:08 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2015 04:37:35 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Choi", "David S.", ""]]}, {"id": "1408.4334", "submitter": "Erwan Koch", "authors": "Erwan Koch and Philippe Naveau", "title": "A frailty-contagion model for multi-site hourly precipitation driven by\n  atmospheric covariates", "comments": "Presented by Erwan Koch at the conferences: - 12th IMSC, Jeju\n  (Korea), June 2013 - ISI WSC 2013, Hong Kong, Aug.2013. Invited speaker in\n  the session \"Probabilistic and statistical contributions in climate research\"", "journal-ref": null, "doi": "10.1016/j.advwatres.2015.01.001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate stochastic simulations of hourly precipitation are needed for impact\nstudies at local spatial scales. Statistically, hourly precipitation data\nrepresent a difficult challenge. They are non-negative, skewed, heavy tailed,\ncontain a lot of zeros (dry hours) and they have complex temporal structures\n(e.g., long persistence of dry episodes). Inspired by frailty-contagion\napproaches used in finance and insurance, we propose a multi-site precipitation\nsimulator that, given appropriate regional atmospheric variables, can\nsimultaneously handle dry events and heavy rainfall periods. One advantage of\nour model is its conceptual simplicity in its dynamical structure. In\nparticular, the temporal variability is represented by a common factor based on\na few classical atmospheric covariates like temperatures, pressures and others.\nOur inference approach is tested on simulated data and applied on measurements\nmade in the northern part of French Brittany.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 13:37:55 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 12:03:18 GMT"}, {"version": "v3", "created": "Thu, 27 Nov 2014 15:04:00 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Koch", "Erwan", ""], ["Naveau", "Philippe", ""]]}, {"id": "1408.4475", "submitter": "Ning Hao", "authors": "Ning Hao, Bin Dong, Jianqing Fan", "title": "Sparsifying the Fisher Linear Discriminant by Rotation", "comments": "30 pages and 9 figures. This paper has been accepted by Journal of\n  the Royal Statistical Society: Series B (Statistical Methodology). The first\n  two versions of this paper were uploaded to Bin Dong's web site under the\n  title \"A Rotate-and-Solve Procedure for Classification\" in 2013 May and 2014\n  January. This version may be slightly different from the published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional classification techniques have been proposed in the\nliterature based on sparse linear discriminant analysis (LDA). To efficiently\nuse them, sparsity of linear classifiers is a prerequisite. However, this might\nnot be readily available in many applications, and rotations of data are\nrequired to create the needed sparsity. In this paper, we propose a family of\nrotations to create the required sparsity. The basic idea is to use the\nprincipal components of the sample covariance matrix of the pooled samples and\nits variants to rotate the data first and to then apply an existing high\ndimensional classifier. This rotate-and-solve procedure can be combined with\nany existing classifiers, and is robust against the sparsity level of the true\nmodel. We show that these rotations do create the sparsity needed for high\ndimensional classifications and provide theoretical understanding why such a\nrotation works empirically. The effectiveness of the proposed method is\ndemonstrated by a number of simulated and real data examples, and the\nimprovements of our method over some popular high dimensional classification\nrules are clearly shown.\n", "versions": [{"version": "v1", "created": "Sat, 16 Aug 2014 23:53:01 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Hao", "Ning", ""], ["Dong", "Bin", ""], ["Fan", "Jianqing", ""]]}, {"id": "1408.4660", "submitter": "Leo Duan", "authors": "Leo L. Duan, John P. Clancy, Rhonda D. Szczesniak", "title": "Joint Hierarchical Gaussian Process Model with Application to Forecast\n  in Medical Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel extrapolation method is proposed for longitudinal forecasting. A\nhierarchical Gaussian process model is used to combine nonlinear population\nchange and individual memory of the past to make prediction. The prediction\nerror is minimized through the hierarchical design. The method is further\nextended to joint modeling of continuous measurements and survival events. The\nbaseline hazard, covariate and joint effects are conveniently modeled in this\nhierarchical structure. The estimation and inference are implemented in fully\nBayesian framework using the objective and shrinkage priors. In simulation\nstudies, this model shows robustness in latent estimation, correlation\ndetection and high accuracy in forecasting. The model is illustrated with\nmedical monitoring data from cystic fibrosis (CF) patients. Estimation and\nforecasts are obtained in the measurement of lung function and records of acute\nrespiratory events.\n  Keyword: Extrapolation, Joint Model, Longitudinal Model, Hierarchical\nGaussian Process, Cystic Fibrosis, Medical Monitoring\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 14:03:40 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 14:36:56 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Duan", "Leo L.", ""], ["Clancy", "John P.", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1408.4908", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael\n  Mitzenmacher", "title": "Theoretical Foundations of Equitability and the Maximal Information\n  Coefficient", "comments": "46 pages, 3 figures, 2 tables. This paper has been subsumed by\n  arXiv:1505.02213 and arXiv:1505.02212. Please cite those papers instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximal information coefficient (MIC) is a tool for finding the strongest\npairwise relationships in a data set with many variables (Reshef et al., 2011).\nMIC is useful because it gives similar scores to equally noisy relationships of\ndifferent types. This property, called {\\em equitability}, is important for\nanalyzing high-dimensional data sets.\n  Here we formalize the theory behind both equitability and MIC in the language\nof estimation theory. This formalization has a number of advantages. First, it\nallows us to show that equitability is a generalization of power against\nstatistical independence. Second, it allows us to compute and discuss the\npopulation value of MIC, which we call MIC_*. In doing so we generalize and\nstrengthen the mathematical results proven in Reshef et al. (2011) and clarify\nthe relationship between MIC and mutual information. Introducing MIC_* also\nenables us to reason about the properties of MIC more abstractly: for instance,\nwe show that MIC_* is continuous and that there is a sense in which it is a\ncanonical \"smoothing\" of mutual information. We also prove an alternate,\nequivalent characterization of MIC_* that we use to state new estimators of it\nas well as an algorithm for explicitly computing it when the joint probability\ndensity function of a pair of random variables is known. Our hope is that this\npaper provides a richer theoretical foundation for MIC and equitability going\nforward.\n  This paper will be accompanied by a forthcoming companion paper that performs\nextensive empirical analysis and comparison to other methods and discusses the\npractical aspects of both equitability and the use of MIC and its related\nstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 08:17:13 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 19:08:44 GMT"}, {"version": "v3", "created": "Tue, 12 May 2015 19:58:17 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "1408.5060", "submitter": "Jennifer Wadsworth", "authors": "Jennifer Wadsworth, Jonathan Tawn, Anthony Davison and Daniel Elton", "title": "Modelling across extremal dependence classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different dependence scenarios can arise in multivariate extremes, entailing\ncareful selection of an appropriate class of models. In bivariate extremes, the\nvariables are either asymptotically dependent or are asymptotically\nindependent. Most available statistical models suit one or other of these\ncases, but not both, resulting in a stage in the inference that is unaccounted\nfor, but can substantially impact subsequent extrapolation. Existing modelling\nsolutions to this problem are either applicable only on sub-domains, or appeal\nto multiple limit theories. We introduce a unified representation for bivariate\nextremes that encompasses a wide variety of dependence scenarios, and applies\nwhen at least one variable is large. Our representation motivates a parametric\nmodel that encompasses both dependence classes. We implement a simple version\nof this model, and show that it performs well in a range of settings.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 16:45:40 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 14:45:28 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2015 16:10:30 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2015 17:28:38 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Wadsworth", "Jennifer", ""], ["Tawn", "Jonathan", ""], ["Davison", "Anthony", ""], ["Elton", "Daniel", ""]]}, {"id": "1408.5087", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Philippe Rigollet, Weichen Wang", "title": "Estimation of functionals of sparse covariance matrices", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1357 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2706-2737", "doi": "10.1214/15-AOS1357", "report-no": "IMS-AOS-AOS1357", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional statistical tests often ignore correlations to gain\nsimplicity and stability leading to null distributions that depend on\nfunctionals of correlation matrices such as their Frobenius norm and other\n$\\ell_r$ norms. Motivated by the computation of critical values of such tests,\nwe investigate the difficulty of estimation the functionals of sparse\ncorrelation matrices. Specifically, we show that simple plug-in procedures\nbased on thresholded estimators of correlation matrices are sparsity-adaptive\nand minimax optimal over a large class of correlation matrices. Akin to\nprevious results on functional estimation, the minimax rates exhibit an elbow\nphenomenon. Our results are further illustrated in simulated data as well as an\nempirical study of data arising in financial econometrics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Aug 2014 17:59:32 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 11:02:09 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Fan", "Jianqing", ""], ["Rigollet", "Philippe", ""], ["Wang", "Weichen", ""]]}, {"id": "1408.5907", "submitter": "Anru Zhang", "authors": "T. Tony Cai and Anru Zhang", "title": "Inference for High-dimensional Differential Correlation Matrices", "comments": "Accepted for publication in Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2015.08.019", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by differential co-expression analysis in genomics, we consider in\nthis paper estimation and testing of high-dimensional differential correlation\nmatrices. An adaptive thresholding procedure is introduced and theoretical\nguarantees are given. Minimax rate of convergence is established and the\nproposed estimator is shown to be adaptively rate-optimal over collections of\npaired correlation matrices with approximately sparse differences. Simulation\nresults show that the procedure significantly outperforms two other natural\nmethods that are based on separate estimation of the individual correlation\nmatrices. The procedure is also illustrated through an analysis of a breast\ncancer dataset, which provides evidence at the gene co-expression level that\nseveral genes, of which a subset has been previously verified, are associated\nwith the breast cancer. Hypothesis testing on the differential correlation\nmatrices is also considered. A test, which is particularly well suited for\ntesting against sparse alternatives, is introduced. In addition, other related\nproblems, including estimation of a single sparse correlation matrix,\nestimation of the differential covariance matrices, and estimation of the\ndifferential cross-correlation matrices, are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 20:02:16 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 16:55:05 GMT"}], "update_date": "2015-10-22", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhang", "Anru", ""]]}, {"id": "1408.6123", "submitter": "Luc Demortier", "authors": "Luc Demortier and Louis Lyons", "title": "Testing Hypotheses in Particle Physics: Plots of $p_{0}$ Versus $p_{1}$", "comments": "46 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For situations where we are trying to decide which of two hypotheses $H_{0}$\nand $H_{1}$ provides a better description of some data, we discuss the\nusefulness of plots of $p_{0}$ versus $p_{1}$, where $p_{i}$ is the $p$-value\nfor testing $H_{i}$. They provide an interesting way of understanding the\ndifference between the standard way of excluding $H_{1}$ and the $CL_{s}$\napproach; the Punzi definition of sensitivity; the relationship between\n$p$-values and likelihood ratios; and the probability of observing misleading\nevidence. They also help illustrate the Law of the Iterated Logarithm and the\nJeffreys-Lindley paradox.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 14:19:37 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Demortier", "Luc", ""], ["Lyons", "Louis", ""]]}, {"id": "1408.6211", "submitter": "Amanda Turner", "authors": "John Whitehead, Faye Cleary and Amanda Turner", "title": "Bayesian sample sizes for exploratory clinical trials comparing multiple\n  experimental treatments with a control", "comments": "28 pages, 3 tables, 2 figures", "journal-ref": "Statist. Med., 34, 2048-2061, 2015", "doi": "10.1002/sim.6469", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Bayesian approach is developed for simultaneously comparing\nmultiple experimental treatments with a common control treatment in an\nexploratory clinical trial. The sample size is set to ensure that, at the end\nof the study, there will be at least one treatment for which the investigators\nhave a strong belief that it is better than control, or else they have a strong\nbelief that none of the experimental treatments are substantially better than\ncontrol. This criterion bears a direct relationship with conventional\nfrequentist power requirements, while allowing prior opinion to feature in the\nanalysis with a consequent reduction in sample size. If it is concluded that at\nleast one of the experimental treatments shows promise, then it is envisaged\nthat one or more of these promising treatments will be developed further in a\ndefinitive phase III trial. The approach is developed in the context of\nnormally distributed responses sharing a common standard deviation regardless\nof treatment. To begin with, the standard deviation will be assumed known when\nthe sample size is calculated. The final analysis will not rely upon this\nassumption, although the intended properties of the design may not be achieved\nif the anticipated standard deviation turns out to be inappropriate. Methods\nthat formally allow for uncertainty about the standard deviation, expressed in\nthe form of a Bayesian prior, are then explored. Illustrations of the sample\nsizes computed from the new method are presented, and comparisons are made with\nfrequentist methods devised for the same situation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 19:07:20 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Whitehead", "John", ""], ["Cleary", "Faye", ""], ["Turner", "Amanda", ""]]}, {"id": "1408.6440", "submitter": "Didier Ch\\'etelat", "authors": "Didier Ch\\'etelat and Martin T. Wells", "title": "Noise Estimation in the Spiked Covariance Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a spiked covariance matrix in high dimensions under\nFrobenius loss, and the parallel problem of estimating the noise in spiked PCA\nis investigated. We propose an estimator of the noise parameter by minimizing\nan unbiased estimator of the invariant Frobenius risk using calculus of\nvariations. The resulting estimator is shown, using random matrix theory, to be\nstrongly consistent and essentially asymptotically normal and minimax for the\nnoise estimation problem. We apply the construction to construct a robust\nspiked covariance matrix estimator with consistent eigenvalues.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 15:41:48 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Ch\u00e9telat", "Didier", ""], ["Wells", "Martin T.", ""]]}, {"id": "1408.6566", "submitter": "Sijia Liu", "authors": "Sijia Liu and Swarnendu Kar and Makan Fardad and Pramod K. Varshney", "title": "Sparsity-Aware Sensor Collaboration for Linear Coherent Estimation", "comments": "IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2413381", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of distributed estimation, we consider the problem of sensor\ncollaboration, which refers to the act of sharing measurements with neighboring\nsensors prior to transmission to a fusion center. While incorporating the cost\nof sensor collaboration, we aim to find optimal sparse collaboration schemes\nsubject to a certain information or energy constraint. Two types of sensor\ncollaboration problems are studied: minimum energy with an information\nconstraint; and maximum information with an energy constraint. To solve the\nresulting sensor collaboration problems, we present tractable optimization\nformulations and propose efficient methods which render near-optimal solutions\nin numerical experiments. We also explore the situation in which there is a\ncost associated with the involvement of each sensor in the estimation scheme.\nIn such situations, the participating sensors must be chosen judiciously. We\nintroduce a unified framework to jointly design the optimal sensor selection\nand collaboration schemes. For a given estimation performance, we show\nempirically that there exists a trade-off between sensor selection and sensor\ncollaboration.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 21:07:15 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 19:21:48 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Liu", "Sijia", ""], ["Kar", "Swarnendu", ""], ["Fardad", "Makan", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1408.6667", "submitter": "Kushal  Dey", "authors": "Kushal K. Dey and Sourabh Bhattacharya", "title": "On Single Variable Transformation Approach to Markov Chain Monte Carlo", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Walk Metropolis Hastings (RWMH) algorithm, is quite inefficient in\nhigh dimensions because of its abysmally slow acceptance rate. The slow\nacceptance rate results from the fact that RWMH separately updates each\ncoordinate of the chain at every step. Dutta and Bhattacharya (2013) proposed a\nnew technique called Transformation based Markov Chain Monte Carlo (TMCMC)\naimed at overcoming these problems. This method updates all co-ordinates at a\ntime- ensuring stable acceptance in all dimensions.\n  We have shown here that geometric ergodicity is achieved for sub-exponential\ntargets for two versions of TMCMC- the additive and the additive-multiplicative\nhybrid TMCMC schemes. Also, we obtain the optimal scaling by maximizing the\ndiffusion speed of the limiting time-scaled diffusion process for TMCMC. We\nshow that the optimal acceptance rate is 0.439 for TMCMC which is almost twice\nas large as RWMH (0.234). We observe that convergence to stationarity for TMCMC\nis faster than RWMH but the mixing property in RWMH is relatively better.\nHowever TMCMC is more robust with respect to scaling and dimensionality. This\nis attested by simulation runs on Gaussian and nearest neighbor models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 10:06:03 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Dey", "Kushal K.", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1408.6681", "submitter": "Jhan Rodr\\'iguez", "authors": "Jhan Rodr\\'iguez, Andr\\'as B\\'ardossy", "title": "Entropy measure for the quantification of upper quantile interdependence\n  in multivariate distributions", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new measure of interdependence among the components of a\nrandom vector along the main diagonal of the vector copula, i.e. along the line\n$u_{1}=\\ldots=u_{J}$, for\n$\\left(u_{1},\\ldots,u_{J}\\right)\\in\\left[0,1\\right]^{J}$. Our measure is\nrelated to the Shannon entropy of a discrete random variable, hence we call it\nan \"entropy index\". This entropy index is invariant with respect to marginal\nnon-decreasing transformations and can be used to quantify the intensity of the\nvector components association in arbitrary dimensions. We show the\napplicability of our entropy index by an example with real data of 4 stock\nprices of the DAX index. In case the random vector is in the domain of\nattraction of an extreme value distribution, our index is shown to have as\nlimit the distribution's extremal coefficient, which can be interpreted as the\neffective number of asymptotically independent components in the vector.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 11:08:56 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Rodr\u00edguez", "Jhan", ""], ["B\u00e1rdossy", "Andr\u00e1s", ""]]}, {"id": "1408.6937", "submitter": "Aleksey Polunchenko", "authors": "Wenyu Du, Grigory Sokolov, Aleksey S. Polunchenko", "title": "An Exact Formula for the Average Run Length to False Alarm of the\n  Generalized Shiryaev-Roberts Procedure for Change-Point Detection under\n  Exponential Observations", "comments": "9 pages; Accepted for publication in Proceedings of the 12-th\n  German-Polish Workshop on Stochastic Models, Statistics and Their\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive analytically an exact closed-form formula for the standard minimax\nAverage Run Length (ARL) to false alarm delivered by the Generalized\nShiryaev-Roberts (GSR) change-point detection procedure devised to detect a\nshift in the baseline mean of a sequence of independent exponentially\ndistributed observations. Specifically, the formula is found through direct\nsolution of the respective integral (renewal) equation, and is a general result\nin that the GSR procedure's headstart is not restricted to a bounded range, nor\nis there a \"ceiling\" value for the detection threshold. Apart from the\ntheoretical significance (in change-point detection, exact closed-form\nperformance formulae are typically either difficult or impossible to get,\nespecially for the GSR procedure), the obtained formula is also useful to a\npractitioner: in cases of practical interest, the formula is a function linear\nin both the detection threshold and the headstart, and, therefore, the ARL to\nfalse alarm of the GSR procedure can be easily computed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 07:36:22 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 14:48:01 GMT"}, {"version": "v3", "created": "Tue, 21 Oct 2014 20:35:02 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Du", "Wenyu", ""], ["Sokolov", "Grigory", ""], ["Polunchenko", "Aleksey S.", ""]]}, {"id": "1408.7000", "submitter": "Eric Chicken", "authors": "Vladimir J. Geneus, Eric Chicken, Jordan Cuevas, Joseph J. Pignatiello\n  Jr", "title": "A Changepoint Detection Method for Profile Variance", "comments": null, "journal-ref": "Proceedings of the 2015 Industrial and Systems Engineering\n  Research Conference (2015) 1-7", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wavelet-based changepoint method is proposed that determines when the\nvariability of the noise in a sequence of functional profiles goes\nout-of-control from a known, fixed value. The functional portion of the\nprofiles are allowed to come from a large class of functions and may vary from\nprofile to profile. The proposed method makes use of the orthogonal properties\nof wavelet projections to accurately and efficiently monitor the level of noise\nfrom one profile to the next. Several alternative implementations of the\nestimator are compared on a variety of conditions, including allowing the\nwavelet noise subspace to be substantially contaminated by the profile's\nfunctional structure. The proposed method is shown to be very efficient at\ndetecting when the variability has changed through an extensive simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 12:45:51 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Geneus", "Vladimir J.", ""], ["Chicken", "Eric", ""], ["Cuevas", "Jordan", ""], ["Pignatiello", "Joseph J.", "Jr"]]}]