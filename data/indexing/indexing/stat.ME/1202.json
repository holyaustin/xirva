[{"id": "1202.0048", "submitter": "Jonathan Tuke", "authors": "J. Tuke, G. F. V. Glonek, and P. J. Solomon", "title": "P-values, q-values and posterior probabilities for equivalence in\n  genomics studies", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equivalence testing is of emerging importance in genomics studies but has\nhitherto been little studied in this content. In this paper, we define the\nnotion of equivalence of gene expression and determine a `strength of evidence'\nmeasure for gene equivalence. It is common practice in genome-wide studies to\nrank genes according to observed gene-specific P-values or adjusted P-values,\nwhich are assumed to measure the strength of evidence against the null\nhypothesis of no differential gene expression. We show here, both empirically\nand formally, that the equivalence P-value does not satisfy the basic\nconsistency requirements for a valid strength of evidence measure for\nequivalence. This means that the widely-used q-value (Storey, 2002) defined for\neach gene to be the minimum positive false discovery rate that would result in\nthe inclusion of the corresponding P-value in the discovery set, cannot be\ntranslated to the equivalence testing framework. However, when represented as a\nposterior probability, we find that the q-value does satisfy some basic\nconsistency requirements needed to be a credible measure of evidence for\nequivalence. We propose a simple estimate for the q-value from posterior\nprobabilities of equivalence, and analyse data from a mouse stem cell\nmicroarray experiment which demonstrate the theory and methods presented here.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 23:10:04 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Tuke", "J.", ""], ["Glonek", "G. F. V.", ""], ["Solomon", "P. J.", ""]]}, {"id": "1202.0078", "submitter": "Wenjin Mao", "authors": "Wenjin Mao and Jem Corcoran", "title": "A Class Coupler for Perfect Sampling from Continuous Distributions With\n  and Without Atoms", "comments": "21 pages, 9 figures; Journal of Statistical Theory and Applications\n  Volume 10, Number 3, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the simulation of distributions that are a mixture of discrete\nand continuous components. We extend a Metropolis-Hastings-based perfect\nsampling algorithm of Corcoran and Tweedie to allow for a broader class of\ntransition candidate densities. The resulting algorithm, know as a \"class\ncoupler\", is fast to implement and is applicable to purely discrete or purely\ncontinuous densities as well. Our work is motivated by the study of a composite\nhypothesis test in a Bayesian setting via posterior simulation and we give\nsimulation results for some problems in this area.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 01:55:55 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Mao", "Wenjin", ""], ["Corcoran", "Jem", ""]]}, {"id": "1202.0101", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong", "title": "On the Asymptotic Distribution of Variance Weighted KS Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives the asymptotic distribution of variance weighted\nKolmogorov-Smirnov statistics for conditional moment inequality models for the\ncase of a one dimensional covariate. The asymptotic distribution depends on the\ndata generating process only through the variance of a single random variable,\nleading to critical values that can be calculated analytically. By arguments in\nArmstrong (2011b), the resulting tests achieve the best minimax rate for local\nalternatives out of available approaches in a broad class of settings.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 05:53:39 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Armstrong", "Timothy B.", ""]]}, {"id": "1202.0193", "submitter": "Mihail-Ioan Pop", "authors": "Mihail-Ioan Pop", "title": "Maximum entropy estimation of probability distributions with Gaussian\n  conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to computationally estimate the probability density\nfunction of a univariate random variable by applying the maximum entropy\nprinciple with some local conditions given by Gaussian functions. The\nestimation errors and optimal values of parameters are determined. Experimental\nresults are presented. The method estimates the distribution well if a large\nenough selection is used, typically at least 1 000 values. Compared to the\nclassical approach of entropy maximisation, local conditions allow improving\nestimation locally. The method is well suited for a heuristic optimisation\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 15:38:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 21:59:45 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Pop", "Mihail-Ioan", ""]]}, {"id": "1202.0502", "submitter": "Ana Arribas-Gil", "authors": "Ana Arribas-Gil, Karine Bertin, Cristian Meza and Vincent Rivoirard", "title": "Lasso-type estimators for Semiparametric Nonlinear Mixed-Effects Models\n  Estimation", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric nonlinear mixed effects models (NLMEs) are now widely used in\nbiometrical studies, especially in pharmacokinetics research and HIV dynamics\nmodels, due to, among other aspects, the computational advances achieved during\nthe last years. However, this kind of models may not be flexible enough for\ncomplex longitudinal data analysis. Semiparametric NLMEs (SNMMs) have been\nproposed by Ke and Wang (2001). These models are a good compromise and retain\nnice features of both parametric and nonparametric models resulting in more\nflexible models than standard parametric NLMEs. However, SNMMs are complex\nmodels for which estimation still remains a challenge. The estimation procedure\nproposed by Ke and Wang (2001) is based on a combination of log-likelihood\napproximation methods for parametric estimation and smoothing splines\ntechniques for nonparametric estimation. In this work, we propose new\nestimation strategies in SNMMs. On the one hand, we use the Stochastic\nApproximation version of EM algorithm (Delyon et al., 1999) to obtain exact ML\nand REML estimates of the fixed effects and variance components. On the other\nhand, we propose a LASSO-type method to estimate the unknown nonlinear\nfunction. We derive oracle inequalities for this nonparametric estimator. We\ncombine the two approaches in a general estimation procedure that we illustrate\nwith simulated and real data.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 17:50:23 GMT"}], "update_date": "2012-02-03", "authors_parsed": [["Arribas-Gil", "Ana", ""], ["Bertin", "Karine", ""], ["Meza", "Cristian", ""], ["Rivoirard", "Vincent", ""]]}, {"id": "1202.0514", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic, Hongwei Shang and Jun Yan", "title": "A class of goodness-of-fit tests for spatial extremes models based on\n  max-stable processes", "comments": "28 pages, 3 figures, 5 tables", "journal-ref": "Statistics and Its Interface 8:1, pages 45-62, 2015", "doi": "10.4310/SII.2015.v8.n1.a5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric max-stable processes are increasingly used to model spatial\nextremes. Starting from the fact that the dependence structure of a max-stable\nprocess is completely characterized by an extreme-value copula, a class of\ngoodness-of-fit tests is proposed based on the comparison between a\nnonparametric and a parametric estimator of the corresponding unknown\nmultivariate Pickands dependence function. Because of the high-dimensional\nsetting under consideration, these functional estimators are only compared at a\nspecific set of points at which they coincide, up to a multiplicative constant,\nwith estimators of the extremal coefficients. The nonparametric estimators of\nthe Pickands dependence function used in this work are those recently studied\nby Gudendorf and Segers. The parametric estimators rely on the use of the {\\em\npairwise pseudo-likelihood} which extends the concept of pairwise (composite)\nlikelihood to a rank-based context. Approximate $p$-values for the resulting\nmargin-free tests are obtained by means of a {\\em one- or two-level parametric\nbootstrap}. Conditions for the asymptotic validity of these resampling\nprocedures are given based on the work of Genest and R\\'emillard. The\nfinite-sample performance of the tests is investigated in dimension 10 under\nthe Smith, Schlather and geometric Gaussian models. An application of the tests\nto rainfall data is finally presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 19:01:11 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2013 07:23:42 GMT"}, {"version": "v3", "created": "Sun, 4 May 2014 09:51:14 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Shang", "Hongwei", ""], ["Yan", "Jun", ""]]}, {"id": "1202.0515", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P. Xing,\n  Masashi Sugiyama", "title": "High-Dimensional Feature Selection by Feature-Wise Kernelized Lasso", "comments": "18 pages", "journal-ref": "Neural Computation 2014", "doi": "10.1162/NECO_a_00537", "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of supervised feature selection is to find a subset of input\nfeatures that are responsible for predicting output values. The least absolute\nshrinkage and selection operator (Lasso) allows computationally efficient\nfeature selection based on linear dependency between input features and output\nvalues. In this paper, we consider a feature-wise kernelized Lasso for\ncapturing non-linear input-output dependency. We first show that, with\nparticular choices of kernel functions, non-redundant features with strong\nstatistical dependence on output values can be found in terms of kernel-based\nindependence measures. We then show that the globally optimal solution can be\nefficiently computed; this makes the approach scalable to high-dimensional\nproblems. The effectiveness of the proposed method is demonstrated through\nfeature selection experiments with thousands of features.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2012 19:06:02 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2012 02:26:05 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2013 05:25:29 GMT"}, {"version": "v4", "created": "Fri, 4 Jan 2019 00:04:52 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Yamada", "Makoto", ""], ["Jitkrittum", "Wittawat", ""], ["Sigal", "Leonid", ""], ["Xing", "Eric P.", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1202.0625", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by\n  D. A. S. Fraser", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 317-318", "doi": "10.1214/11-STS352A", "report-no": "IMS-STS-STS352A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by D. A.\nS. Fraser [arXiv:1112.5582].\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 08:30:53 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "1202.0627", "submitter": "Kesar Singh", "authors": "Kesar Singh, Minge Xie", "title": "Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by\n  D. A. S. Fraser", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 319-321", "doi": "10.1214/11-STS352B", "report-no": "IMS-STS-STS352B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by D. A.\nS. Fraser [arXiv:1112.5582].\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 08:39:52 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Singh", "Kesar", ""], ["Xie", "Minge", ""]]}, {"id": "1202.0633", "submitter": "Larry Wasserman", "authors": "Larry Wasserman", "title": "Frasian Inference", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 322-325", "doi": "10.1214/11-STS352C", "report-no": "IMS-STS-STS352C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Don Fraser has given an interesting account of the agreements and\ndisagreements between Bayesian posterior probabilities and confidence levels.\nIn this comment I discuss some cases where the lack of such agreement is\nextreme. I then discuss a few cases where it is possible to have Bayes\nprocedures with frequentist validity. Such frequentist-Bayesian---or\nFrasian---methods deserve more attention [arXiv:1112.5582].\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 09:16:16 GMT"}], "update_date": "2012-02-06", "authors_parsed": [["Wasserman", "Larry", ""]]}, {"id": "1202.0709", "submitter": "S. L. Cotter", "authors": "S. L. Cotter, G. O. Roberts, A. M. Stuart, D. White", "title": "MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster", "comments": "Published in at http://dx.doi.org/10.1214/13-STS421 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 424-446", "doi": "10.1214/13-STS421", "report-no": "IMS-STS-STS421", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems arising in applications result in the need to probe a\nprobability distribution for functions. Examples include Bayesian nonparametric\nstatistics and conditioned diffusion processes. Standard MCMC algorithms\ntypically become arbitrarily slow under the mesh refinement dictated by\nnonparametric description of the unknown function. We describe an approach to\nmodifying a whole range of MCMC methods, applicable whenever the target measure\nhas density with respect to a Gaussian process or Gaussian random field\nreference measure, which ensures that their speed of convergence is robust\nunder mesh refinement. Gaussian processes or random fields are fields whose\nmarginal distributions, when evaluated at any finite set of $N$ points, are\n$\\mathbb{R}^N$-valued Gaussians. The algorithmic approach that we describe is\napplicable not only when the desired probability measure has density with\nrespect to a Gaussian process or Gaussian random field reference measure, but\nalso to some useful non-Gaussian reference measures constructed through random\ntruncation. In the applications of interest the data is often sparse and the\nprior specification is an essential part of the overall modelling strategy.\nThese Gaussian-based reference measures are a very flexible modelling tool,\nfinding wide-ranging application. Examples are shown in density estimation,\ndata assimilation in fluid mechanics, subsurface geophysics and image\nregistration. The key design principle is to formulate the MCMC method so that\nit is, in principle, applicable for functions; this may be achieved by use of\nproposals based on carefully chosen time-discretizations of stochastic\ndynamical systems which exactly preserve the Gaussian reference measure. Taking\nthis approach leads to many new algorithms which can be implemented via minor\nmodification of existing algorithms, yet which show enormous speed-up on a wide\nrange of applied problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 14:10:03 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 15:38:50 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2013 07:44:15 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cotter", "S. L.", ""], ["Roberts", "G. O.", ""], ["Stuart", "A. M.", ""], ["White", "D.", ""]]}, {"id": "1202.0850", "submitter": "Jose Fausto de Morais PhD", "authors": "Jose Fausto de Morais", "title": "The Relationship Among the Standard Deviation of a Dataset and the\n  Standard Deviation and Average Two Part of This Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis involves combining summary information for related but\nindependent studies. It uses different relationship to combine position measure\nas well as dispersion measures. The objective of this study is to discuss a\nrelationship among the standard deviation of a data set and the standard\ndeviation and mean of two part of this set. The problem was proposed in a\nsystematic review with meta-analysis that combined two studies with missing\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2012 00:26:19 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["de Morais", "Jose Fausto", ""]]}, {"id": "1202.0889", "submitter": "Nicolai Meinshausen", "authors": "Nicolai Meinshausen", "title": "Sign-constrained least squares estimation for high-dimensional\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many regularization schemes for high-dimensional regression have been put\nforward. Most require the choice of a tuning parameter, using model selection\ncriteria or cross-validation schemes. We show that a simple non-negative or\nsign-constrained least squares is a very simple and effective regularization\ntechnique for a certain class of high-dimensional regression problems. The sign\nconstraint has to be derived via prior knowledge or an initial estimator but no\nfurther tuning or cross-validation is necessary. The success depends on\nconditions that are easy to check in practice. A sufficient condition for our\nresults is that most variables with the same sign constraint are positively\ncorrelated. For a sparse optimal predictor, a non-asymptotic bound on the\nL1-error of the regression coefficients is then proven. Without using any\nfurther regularization, the regression vector can be estimated consistently as\nlong as \\log(p) s/n -> 0 for n -> \\infty, where s is the sparsity of the\noptimal regression vector, p the number of variables and n sample size. Network\ntomography is shown to be an application where the necessary conditions for\nsuccess of non-negative least squares are naturally fulfilled and empirical\nresults confirm the effectiveness of the sign constraint for sparse recovery.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2012 10:41:42 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Meinshausen", "Nicolai", ""]]}, {"id": "1202.0949", "submitter": "Daniel Clark", "authors": "Daniel Edward Clark", "title": "Bayesian filtering for multi-object systems with independently generated\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach for Bayesian filtering of multi-object systems is studied,\nwith particular emphasis on the model where each object generates observations\nindependently of other objects. The approach is based on variational calculus\napplied to generating functionals, using the general version of Faa di Bruno's\nformula for Gateaux differentials. This result enables us to determine some\ngeneral formulae for the updated generating functional after the application of\na multi-object analogue of Bayes' rule.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2012 10:49:02 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Clark", "Daniel Edward", ""]]}, {"id": "1202.0976", "submitter": "Yvo Pokern", "authors": "Y. Pokern, A. M. Stuart, J. H. van Zanten", "title": "Posterior Consistency via Precision Operators for Bayesian Nonparametric\n  Drift Estimation in SDEs", "comments": null, "journal-ref": "Stochastic Process. Appl. 123, no. 2, 603-628, 2013", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a Bayesian approach to nonparametric estimation of the periodic\ndrift function of a one-dimensional diffusion from continuous-time data.\nRewriting the likelihood in terms of local time of the process, and specifying\na Gaussian prior with precision operator of differential form, we show that the\nposterior is also Gaussian with precision operator also of differential form.\nThe resulting expressions are explicit and lead to algorithms which are readily\nimplementable. Using new functional limit theorems for the local time of\ndiffusions on the circle, we bound the rate at which the posterior contracts\naround the true drift function.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2012 16:10:30 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2012 12:33:15 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2012 12:15:18 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Pokern", "Y.", ""], ["Stuart", "A. M.", ""], ["van Zanten", "J. H.", ""]]}, {"id": "1202.1242", "submitter": "Debashis Paul", "authors": "Debashis Paul and Iain M. Johnstone", "title": "Augmented sparse principal component analysis for high dimensional data", "comments": "This manuscript was written in 2007, and a version has been available\n  on the first author's website, but it is posted to arXiv now in its 2007\n  form. Revisions incorporating later work will be posted separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the leading eigenvectors of a\nhigh-dimensional population covariance matrix based on independent Gaussian\nobservations. We establish lower bounds on the rates of convergence of the\nestimators of the leading eigenvectors under $l^q$-sparsity constraints when an\n$l^2$ loss function is used. We also propose an estimator of the leading\neigenvectors based on a coordinate selection scheme combined with PCA and show\nthat the proposed estimator achieves the optimal rate of convergence under a\nsparsity regime. Moreover, we establish that under certain scenarios, the usual\nPCA achieves the minimax convergence rate.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 19:18:19 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Paul", "Debashis", ""], ["Johnstone", "Iain M.", ""]]}, {"id": "1202.1377", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann", "title": "Statistical significance in high-dimensional linear models", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP11 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1212-1242", "doi": "10.3150/12-BEJSP11", "report-no": "IMS-BEJ-BEJSP11", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for constructing p-values for general hypotheses in a\nhigh-dimensional linear model. The hypotheses can be local for testing a single\nregression parameter or they may be more global involving several up to all\nparameters. Furthermore, when considering many hypotheses, we show how to\nadjust for multiple testing taking dependence among the p-values into account.\nOur technique is based on Ridge estimation with an additional correction term\ndue to a substantial projection bias in high dimensions. We prove strong error\ncontrol for our p-values and provide sufficient conditions for detection: for\nthe former, we do not make any assumption on the size of the true underlying\nregression coefficients while regarding the latter, our procedure might not be\noptimal in terms of power. We demonstrate the method in simulated examples and\na real data application.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 09:16:33 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2012 07:47:10 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 07:08:22 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["B\u00fchlmann", "Peter", ""]]}, {"id": "1202.1436", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino, Rosanna Verde", "title": "Linear regression for numeric symbolic variables: an ordinary least\n  squares approach based on Wasserstein Distance", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-015-0197-7", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a linear regression model for modal symbolic data.\nThe observed variables are histogram variables according to the definition\ngiven in the framework of Symbolic Data Analysis and the parameters of the\nmodel are estimated using the classic Least Squares method. An appropriate\nmetric is introduced in order to measure the error between the observed and the\npredicted distributions. In particular, the Wasserstein distance is proposed.\nSome properties of such metric are exploited to predict the response variable\nas direct linear combination of other independent histogram variables. Measures\nof goodness of fit are discussed. An application on real data corroborates the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 14:38:36 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 20:04:16 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Irpino", "Antonio", ""], ["Verde", "Rosanna", ""]]}, {"id": "1202.1561", "submitter": "Yong Wang", "authors": "Yong Wang, Ilze Ziedins, Mark Holmes, Neil Challands", "title": "Tree models for difference and change detection in a complex environment", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS548 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1162-1184", "doi": "10.1214/12-AOAS548", "report-no": "IMS-AOAS-AOAS548", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new family of tree models is proposed, which we call \"differential trees.\"\nA differential tree model is constructed from multiple data sets and aims to\ndetect distributional differences between them. The new methodology differs\nfrom the existing difference and change detection techniques in its\nnonparametric nature, model construction from multiple data sets, and\napplicability to high-dimensional data. Through a detailed study of an arson\ncase in New Zealand, where an individual is known to have been laying\nvegetation fires within a certain time period, we illustrate how these models\ncan help detect changes in the frequencies of event occurrences and uncover\nunusual clusters of events in a complex environment.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 23:28:39 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 13:19:50 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Wang", "Yong", ""], ["Ziedins", "Ilze", ""], ["Holmes", "Mark", ""], ["Challands", "Neil", ""]]}, {"id": "1202.1661", "submitter": "Mohsen Pourahmadi", "authors": "Mohsen Pourahmadi", "title": "Covariance Estimation: The GLM and Regularization Perspectives", "comments": "Published in at http://dx.doi.org/10.1214/11-STS358 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 369-387", "doi": "10.1214/11-STS358", "report-no": "IMS-STS-STS358", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding an unconstrained and statistically interpretable reparameterization\nof a covariance matrix is still an open problem in statistics. Its solution is\nof central importance in covariance estimation, particularly in the recent\nhigh-dimensional data environment where enforcing the positive-definiteness\nconstraint could be computationally expensive. We provide a survey of the\nprogress made in modeling covariance matrices from two relatively complementary\nperspectives: (1) generalized linear models (GLM) or parsimony and use of\ncovariates in low dimensions, and (2) regularization or sparsity for\nhigh-dimensional data. An emerging, unifying and powerful trend in both\nperspectives is that of reducing a covariance estimation problem to that of\nestimating a sequence of regression problems. We point out several instances of\nthe regression-based formulation. A notable case is in sparse estimation of a\nprecision matrix or a Gaussian graphical model leading to the fast graphical\nLASSO algorithm. Some advantages and limitations of the regression-based\nCholesky decomposition relative to the classical spectral (eigenvalue) and\nvariance-correlation decompositions are highlighted. The former provides an\nunconstrained and statistically interpretable reparameterization, and\nguarantees the positive-definiteness of the estimated covariance matrix. It\nreduces the unintuitive task of covariance estimation to that of modeling a\nsequence of regressions at the cost of imposing an a priori order among the\nvariables. Elementwise regularization of the sample covariance matrix such as\nbanding, tapering and thresholding has desirable asymptotic properties and the\nsparse estimated covariance matrix is positive definite with probability\ntending to one for large samples and dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 11:25:29 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Pourahmadi", "Mohsen", ""]]}, {"id": "1202.1998", "submitter": "Eike Christian Brechmann", "authors": "Eike Christian Brechmann", "title": "Hierarchical Kendall copulas: Properties and inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is substantial need for dependence models in higher dimensions,\nmost existing models quickly become rather restrictive and barely balance\nparsimony and flexibility. Hierarchical constructions may improve on that by\ngrouping variables in different levels. In this paper, the new class of\nhierarchical Kendall copulas is proposed and discussed. Hierarchical Kendall\ncopulas are built up by flexible copulas specified for groups of variables,\nwhere aggregation is facilitated by the Kendall distribution function, the\nmultivariate analog to the probability integral transform for univariate random\nvariables. After deriving properties of the general model formulation,\nparticular focus is given to inference techniques of hierarchical Kendall\ncopulas with Archimedean components, for which closed-form analytical\nexpressions can be derived. A substantive application to German stock returns\nfinally shows that hierarchical Kendall copulas perform very well for real\ndata, out-of- as well as in-sample.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:32:14 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2012 09:50:31 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2012 16:48:00 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2013 10:40:18 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Brechmann", "Eike Christian", ""]]}, {"id": "1202.2002", "submitter": "Eike Christian Brechmann", "authors": "Jeffrey Dissmann, Eike Christian Brechmann, Claudia Czado, and Dorota\n  Kurowicka", "title": "Selecting and estimating regular vine copulae and application to\n  financial returns", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 59, 52-69, 2013", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular vine distributions which constitute a flexible class of multivariate\ndependence models are discussed. Since multivariate copulae constructed through\npair-copula decompositions were introduced to the statistical community,\ninterest in these models has been growing steadily and they are finding\nsuccessful applications in various fields. Research so far has however been\nconcentrating on so-called canonical and D-vine copulae, which are more\nrestrictive cases of regular vine copulae. It is shown how to evaluate the\ndensity of arbitrary regular vine specifications. This opens the vine copula\nmethodology to the flexible modeling of complex dependencies even in larger\ndimensions. In this regard, a new automated model selection and estimation\ntechnique based on graph theoretical considerations is presented. This\ncomprehensive search strategy is evaluated in a large simulation study and\napplied to a 16-dimensional financial data set of international equity, fixed\nincome and commodity indices which were observed over the last decade, in\nparticular during the recent financial crisis. The analysis provides\neconomically well interpretable results and interesting insights into the\ndependence structure among these indices.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:42:07 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2012 06:53:30 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Dissmann", "Jeffrey", ""], ["Brechmann", "Eike Christian", ""], ["Czado", "Claudia", ""], ["Kurowicka", "Dorota", ""]]}, {"id": "1202.2008", "submitter": "Jakob Stoeber", "authors": "Carlos Almeida, Claudia Czado, Hans Manner", "title": "Modeling high dimensional time-varying dependence using D-vine SCAR\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling the dependence among many time series. We\nbuild high dimensional time-varying copula models by combining pair-copula\nconstructions (PCC) with stochastic autoregressive copula (SCAR) models to\ncapture dependence that changes over time. We show how the estimation of this\nhighly complex model can be broken down into the estimation of a sequence of\nbivariate SCAR models, which can be achieved by using the method of simulated\nmaximum likelihood. Further, by restricting the conditional dependence\nparameter on higher cascades of the PCC to be constant, we can greatly reduce\nthe number of parameters to be estimated without losing much flexibility. We\nstudy the performance of our estimation method by a large scale Monte Carlo\nsimulation. An application to a large dataset of stock returns of all\nconstituents of the Dax 30 illustrates the usefulness of the proposed model\nclass.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:52:56 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Almeida", "Carlos", ""], ["Czado", "Claudia", ""], ["Manner", "Hans", ""]]}, {"id": "1202.2009", "submitter": "Jakob Stoeber", "authors": "Jakob Stoeber and Claudia Czado", "title": "Detecting regime switches in the dependence structure of high\n  dimensional financial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misperceptions about extreme dependencies between different financial assets\nhave been an im- portant element of the recent financial crisis. This paper\nstudies inhomogeneity in dependence structures using Markov switching regular\nvine copulas. These account for asymmetric depen- dencies and tail dependencies\nin high dimensional data. We develop methods for fast maximum likelihood as\nwell as Bayesian inference. Our algorithms are validated in simulations and\napplied to financial data. We find that regime switches are present in the\ndependence structure of various data sets and show that regime switching models\ncould provide tools for the accurate description of inhomogeneity during times\nof crisis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:58:47 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Stoeber", "Jakob", ""], ["Czado", "Claudia", ""]]}, {"id": "1202.2045", "submitter": "Juergen Laeuter", "authors": "Juergen Laeuter, Maciej Rosolowski and Ekkehard Glimm", "title": "Exact Multivariate Tests - A New Effective Principle of Controlled Model\n  Choice", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional tests are applied to find relevant sets of variables and\nrelevant models. If variables are selected by analyzing the sums of products\nmatrices and a corresponding mean-value test is performed, there is the danger\nthat the nominal error of first kind is exceeded. In the paper, well-known\nmultivariate tests receive a new mathematical interpretation such that the\nerror of first kind of the combined testing and selecting procedure can more\neasily be kept. The null hypotheses on mean values are replaced by hypotheses\non distributional sphericity of the individual score responses. Thus, model\nchoice is possible without too strong restrictions. The method is presented for\nall linear multivariate designs. It is illustrated by an example from\nbioinformatics: The selection of gene sets for the comparison of groups of\npatients suffering from B-cell lymphomas.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 16:54:17 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Laeuter", "Juergen", ""], ["Rosolowski", "Maciej", ""], ["Glimm", "Ekkehard", ""]]}, {"id": "1202.2143", "submitter": "Il Memming Park", "authors": "Il Memming Park, Marcel Nassar, Mijung Park", "title": "Active Bayesian Optimization: Minimizing Minimizer Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of optimization is to find the minimizer of a target\nfunction.However, typical criteria for active optimization often ignore the\nuncertainty about the minimizer. We propose a novel criterion for global\noptimization and an associated sequential active learning strategy using\nGaussian processes.Our criterion is the reduction of uncertainty in the\nposterior distribution of the function minimizer. It can also flexibly\nincorporate multiple global minimizers. We implement a tractable approximation\nof the criterion and demonstrate that it obtains the global minimizer\naccurately compared to conventional Bayesian optimization criteria.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 22:31:01 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Park", "Il Memming", ""], ["Nassar", "Marcel", ""], ["Park", "Mijung", ""]]}, {"id": "1202.2370", "submitter": "Burcu Aydin", "authors": "Burcu Ayd{\\i}n, G\\'abor Pataki, Haonan Wang, Alim Ladha, Elizabeth\n  Bullitt, J.S. Marron", "title": "New Approaches to Principal Component Analysis for Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Oriented Data Analysis is a new area in statistics that studies\npopulations of general data objects. In this article we consider populations of\ntree-structured objects as our focus of interest. We develop improved analysis\ntools for data lying in a binary tree space analogous to classical Principal\nComponent Analysis methods in Euclidean space. Our extensions of PCA are\nanalogs of one dimensional subspaces that best fit the data. Previous work was\nbased on the notion of tree-lines.\n  In this paper, a generalization of the previous tree-line notion is proposed:\nk-tree-lines. Previously proposed tree-lines are k-tree-lines where k=1. New\nsub-cases of k-tree-lines studied in this work are the 2-tree-lines and\ntree-curves, which explain much more variation per principal component than\ntree-lines. The optimal principal component tree-lines were computable in\nlinear time. Because 2-tree-lines and tree-curves are more complex, they are\ncomputationally more expensive, but yield improved data analysis results.\n  We provide a comparative study of all these methods on a motivating data set\nconsisting of brain vessel structures of 98 subjects.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 21:04:33 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Ayd\u0131n", "Burcu", ""], ["Pataki", "G\u00e1bor", ""], ["Wang", "Haonan", ""], ["Ladha", "Alim", ""], ["Bullitt", "Elizabeth", ""], ["Marron", "J. S.", ""]]}, {"id": "1202.2371", "submitter": "Burcu Aydin", "authors": "Carlos A. Alfaro, Burcu Ayd{\\i}n, Elizabeth Bullitt, Alim Ladha,\n  Carlos E. Valencia", "title": "Dimension Reduction in Principal Component Analysis for Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of tree structured data is a new topic in statistics\nwith wide application areas. Some Principal Component Analysis (PCA) ideas were\npreviously developed for binary tree spaces. In this study, we extend these\nideas to the more general space of rooted and labeled trees. We re-define\nconcepts such as tree-line and forward principal component tree-line for this\nmore general space, and generalize the optimal algorithm that finds them.\n  We then develop an analog of classical dimension reduction technique in PCA\nfor the tree space. To do this, we define the components that carry the least\namount of variation of a tree data set, called backward principal components.\nWe present an optimal algorithm to find them. Furthermore, we investigate the\nrelationship of these the forward principal components, and prove a\npath-independency property between the forward and backward techniques.\n  We apply our methods to a data set of brain artery data set of 98 subjects.\nUsing our techniques, we investigate how aging affects the brain artery\nstructure of males and females. We also analyze a data set of organization\nstructure of a large US company and explore the structural differences across\ndifferent types of departments within the company.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 21:21:01 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Alfaro", "Carlos A.", ""], ["Ayd\u0131n", "Burcu", ""], ["Bullitt", "Elizabeth", ""], ["Ladha", "Alim", ""], ["Valencia", "Carlos E.", ""]]}, {"id": "1202.2564", "submitter": "Christoforos Anagnostopoulos Dr", "authors": "David J. Hand, Christoforos Anagnostopoulos", "title": "A better Beta for the H measure of classification performance", "comments": "Preprint. Keywords: supervised classification, classifier\n  performance, AUC, ROC curve, H measure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area under the ROC curve is widely used as a measure of performance of\nclassification rules. However, it has recently been shown that the measure is\nfundamentally incoherent, in the sense that it treats the relative severities\nof misclassifications differently when different classifiers are used. To\novercome this, Hand (2009) proposed the $H$ measure, which allows a given\nresearcher to fix the distribution of relative severities to a\nclassifier-independent setting on a given problem. This note extends the\ndiscussion, and proposes a modified standard distribution for the $H$ measure,\nwhich better matches the requirements of researchers, in particular those faced\nwith heavily unbalanced datasets, the $Beta(\\pi_1+1,\\pi_0+1)$ distribution.\n[Preprint submitted at Pattern Recognition Letters]\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2012 20:32:15 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2013 11:44:54 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Hand", "David J.", ""], ["Anagnostopoulos", "Christoforos", ""]]}, {"id": "1202.2683", "submitter": "Simon Byrne", "authors": "Simon P. J. Byrne and A. Philip Dawid", "title": "Retrospective-prospective symmetry in the likelihood and Bayesian\n  analysis of case-control studies", "comments": "16 pages, to appear in Biometrika", "journal-ref": "Biometrika (2014) 101 (1), pages 189-204", "doi": "10.1093/biomet/ast050", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prentice & Pyke (1979) established that the maximum likelihood estimate of an\nodds-ratio in a case-control study is the same as would be found by running a\nlogistic regression: in other words, for this specific target the incorrect\nprospective model is inferentially equivalent to the correct retrospective\nmodel. Similar results have been obtained for other models, and conditions have\nalso been identified under which the corresponding Bayesian property holds,\nnamely that the posterior distribution of the odds-ratio be the same, whether\ncomputed using the prospective or the retrospective likelihood. Here we\ndemonstrate how these results follow directly from certain parameter\nindependence properties of the models and priors, and identify prior laws that\nsupport such reverse analysis, for both standard and stratified designs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 10:30:53 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 16:07:20 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2013 14:51:45 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2013 15:52:04 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Byrne", "Simon P. J.", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1202.3760", "submitter": "Vinayak Rao", "authors": "Vinayak Rao, Yee Whye Teh", "title": "Fast MCMC sampling for Markov jump processes and continuous time\n  Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-619-626", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov jump processes and continuous time Bayesian networks are important\nclasses of continuous time dynamical systems. In this paper, we tackle the\nproblem of inferring unobserved paths in these models by introducing a fast\nauxiliary variable Gibbs sampler. Our approach is based on the idea of\nuniformization, and sets up a Markov chain over paths by sampling a finite set\nof virtual jump times and then running a standard hidden Markov model forward\nfiltering-backward sampling algorithm over states at the set of extant and\nvirtual jump times. We demonstrate significant computational benefits over a\nstate-of-the-art Gibbs sampler on a number of continuous time Bayesian\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Rao", "Vinayak", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1202.3765", "submitter": "Inma Tur", "authors": "Inma Tur, Robert Castelo", "title": "Learning mixed graphical models from data with p larger than n", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-689-697", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Gaussian graphical models is an extensively studied\nproblem in the classical multivariate setting where the sample size n is larger\nthan the number of random variables p, as well as in the more challenging\nsetting when p>>n. However, analogous approaches for learning the structure of\ngraphical models with mixed discrete and continuous variables when p>>n remain\nlargely unexplored. Here we describe a statistical learning procedure for this\nproblem based on limited-order correlations and assess its performance with\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Tur", "Inma", ""], ["Castelo", "Robert", ""]]}, {"id": "1202.3819", "submitter": "M. G. B. Blum", "authors": "M. G. B. Blum, M. A. Nunes, D. Prangle, S. A. Sisson", "title": "A Comparative Review of Dimension Reduction Methods in Approximate\n  Bayesian Computation", "comments": "Published in at http://dx.doi.org/10.1214/12-STS406 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 189-208", "doi": "10.1214/12-STS406", "report-no": "IMS-STS-STS406", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods make use of comparisons\nbetween simulated and observed summary statistics to overcome the problem of\ncomputationally intractable likelihood functions. As the practical\nimplementation of ABC requires computations based on vectors of summary\nstatistics, rather than full data sets, a central question is how to derive\nlow-dimensional summary statistics from the observed data with minimal loss of\ninformation. In this article we provide a comprehensive review and comparison\nof the performance of the principal methods of dimension reduction proposed in\nthe ABC literature. The methods are split into three nonmutually exclusive\nclasses consisting of best subset selection methods, projection techniques and\nregularization. In addition, we introduce two new methods of dimension\nreduction. The first is a best subset selection method based on Akaike and\nBayesian information criteria, and the second uses ridge regression as a\nregularization procedure. We illustrate the performance of these dimension\nreduction techniques through the analysis of three challenging models and data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 23:47:00 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2012 02:05:09 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2013 11:18:18 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Blum", "M. G. B.", ""], ["Nunes", "M. A.", ""], ["Prangle", "D.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1202.4177", "submitter": "Phillip J. Schulte", "authors": "Phillip J. Schulte, Anastasios A. Tsiatis, Eric B. Laber, Marie\n  Davidian", "title": "$Q$- and $A$-Learning Methods for Estimating Optimal Dynamic Treatment\n  Regimes", "comments": "Published in at http://dx.doi.org/10.1214/13-STS450 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 640-661", "doi": "10.1214/13-STS450", "report-no": "IMS-STS-STS450", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice, physicians make a series of treatment decisions over\nthe course of a patient's disease based on his/her baseline and evolving\ncharacteristics. A dynamic treatment regime is a set of sequential decision\nrules that operationalizes this process. Each rule corresponds to a decision\npoint and dictates the next treatment action based on the accrued information.\nUsing existing data, a key goal is estimating the optimal regime, that, if\nfollowed by the patient population, would yield the most favorable outcome on\naverage. Q- and A-learning are two main approaches for this purpose. We provide\na detailed account of these methods, study their performance, and illustrate\nthem using data from a depression study.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2012 19:17:01 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 16:23:17 GMT"}, {"version": "v3", "created": "Tue, 3 Feb 2015 10:52:21 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Schulte", "Phillip J.", ""], ["Tsiatis", "Anastasios A.", ""], ["Laber", "Eric B.", ""], ["Davidian", "Marie", ""]]}, {"id": "1202.4339", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama and William E. Strawderman", "title": "A new Monte Carlo sampling in Bayesian probit regression", "comments": "The title was changed. William Strawderman joined as coauthor. A new\n  MC sampling is proposed based on the theory in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probit regression from a Bayesian perspective and give an\nalternative form for the posterior distribution when the prior distribution for\nthe regression parameters is the uniform distribution. This new form allows\nsimple Monte Carlo simulation of the posterior as opposed to MCMC simulation\nstudied in much of the literature and may therefore be more efficient\ncomputationally. We also provide alternative explicit expression for the first\nand second moments. Additionally we provide analogous results for Gaussian\npriors.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 14:53:24 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2012 01:49:54 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2012 10:16:14 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Strawderman", "William E.", ""]]}, {"id": "1202.4437", "submitter": "Gustavo Didier", "authors": "Gustavo Didier and John Fricks", "title": "On the wavelet-based simulation of anomalous diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterization of particle diffusion is a classical problem in physics\nand probability theory. The field of microrheology is based on experiments in\nwhich microscopic tracer beads are placed into a non-Newtonian fluid and\ntracked using high speed video capture. The modeling of the behavior of these\nbeads is now an active scientific area which demands multiple stochastic and\nstatistical methods.\n  We propose an approximate wavelet-based simulation technique for two classes\nof continuous time anomalous diffusion models, the fractional\nOrnstein-Uhlenbeck process and the fractional generalized Langevin equation.\nThe proposed algorithm is an iterative method that provides approximate\ndiscretizations that converge quickly and in an appropriate sense to the\ncontinuous time target process. As compared to previous works, it covers cases\nwhere the natural discretization of the target process does not have closed\nform in the time domain. Moreover, we propose smoothing procedures as to speed\nthe time domain decay of the filters.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2012 20:09:04 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 19:02:28 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Didier", "Gustavo", ""], ["Fricks", "John", ""]]}, {"id": "1202.4850", "submitter": "Kengo Kato", "authors": "Kengo Kato", "title": "Estimation in functional linear quantile regression", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1066 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 6, 3108-3136", "doi": "10.1214/12-AOS1066", "report-no": "IMS-AOS-AOS1066", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies estimation in functional linear quantile regression in\nwhich the dependent variable is scalar while the covariate is a function, and\nthe conditional quantile for each fixed quantile index is modeled as a linear\nfunctional of the covariate. Here we suppose that covariates are discretely\nobserved and sampling points may differ across subjects, where the number of\nmeasurements per subject increases as the sample size. Also, we allow the\nquantile index to vary over a given subset of the open unit interval, so the\nslope function is a function of two variables: (typically) time and quantile\nindex. Likewise, the conditional quantile function is a function of the\nquantile index and the covariate. We consider an estimator for the slope\nfunction based on the principal component basis. An estimator for the\nconditional quantile function is obtained by a plug-in method. Since the\nso-constructed plug-in estimator not necessarily satisfies the monotonicity\nconstraint with respect to the quantile index, we also consider a class of\nmonotonized estimators for the conditional quantile function. We establish\nrates of convergence for these estimators under suitable norms, showing that\nthese rates are optimal in a minimax sense under some smoothness assumptions on\nthe covariance kernel of the covariate and the slope function. Empirical choice\nof the cutoff level is studied by using simulations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 08:08:53 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 12:39:03 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Kato", "Kengo", ""]]}, {"id": "1202.4962", "submitter": "Assaf Oron", "authors": "Assaf P. Oron and Peter D. Hoff", "title": "Small-Sample Behavior of Novel Phase I Cancer Trial Designs", "comments": "Somewhat modified version of the version accepted pending final\n  modifications at Clinical Trials. The supplement is in the back", "journal-ref": "Clinical Trials 2013, 10(1):63-92 (including comments and\n  rejoinder)", "doi": "10.1177/1740774512469311", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel dose-finding designs, using estimation to assign the best estimated\nmaximum- tolerated-dose (MTD) at each point in the experiment, most commonly\nvia Bayesian techniques, have recently entered large-scale implementation in\nPhase I cancer clinical trials. We examine the small-sample behavior of these\n\"Bayesian Phase I\" (BP1) designs, and also of non-Bayesian designs sharing the\nsame main \"long-memory\" traits (hereafter: LMP1s).\n  For all LMP1s examined, the number of cohorts treated at the true MTD\n(denoted here as n*) was highly variable between numerical runs drawn from the\nsame toxicity-threshold distribution, especially when compared with\n\"up-and-down\" (U&D) short-memory designs. Further investigation using the same\nset of thresholds in permuted order, produced a nearly-identical magnitude of\nvariability in n*. Therefore, this LMP1 behavior is driven by a strong\nsensitivity to the order in which toxicity thresholds appear in the experiment.\nWe suggest that the sensitivity is related to LMP1's tendency to \"settle\" early\non a specific dose level - a tendency caused by the repeated likelihood-based\n\"winner-takes-all\" dose assignment rule, which grants the early cohorts a\ndisproportionately large influence upon experimental trajectories.\n  Presently, U&D designs offer a simpler and more stable alternative, with\nroughly equivalent MTD estimation performance. A promising direction for\ncombining the two approaches is briefly discussed (note: the '3+3' protocol is\nnot a U&D design).\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 16:35:24 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2012 21:31:20 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Oron", "Assaf P.", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1202.5064", "submitter": "Zhongyang Zhang", "authors": "Zhongyang Zhang, Kenneth Lange, Chiara Sabatti", "title": "Reconstructing DNA copy number by joint segmentation of multiple\n  sequences", "comments": "54 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variation in DNA copy number carries information on the modalities of\ngenome evolution and misregulation of DNA replication in cancer cells; its\nstudy can be helpful to localize tumor suppressor genes, distinguish different\npopulations of cancerous cell, as well identify genomic variations responsible\nfor disease phenotypes. A number of different high throughput technologies can\nbe used to identify copy number variable sites, and the literature documents\nmultiple effective algorithms. We focus here on the specific problem of\ndetecting regions where variation in copy number is relatively common in the\nsample at hand: this encompasses the cases of copy number polymorphisms,\nrelated samples, technical replicates, and cancerous sub-populations from the\nsame individual. We present an algorithm based on regularization approaches\nwith significant computational advantages and competitive accuracy. We\nillustrate its applicability with simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 23:42:08 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2012 08:21:02 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Zhang", "Zhongyang", ""], ["Lange", "Kenneth", ""], ["Sabatti", "Chiara", ""]]}, {"id": "1202.5325", "submitter": "Tlemcani Mounir", "authors": "Zohra Benkamra, Mekki Terbeche, Mounir Tlemcani", "title": "Two stage design for estimating the product of means with cost in the\n  case of the exponential family", "comments": "08 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the product of means of independent\npopulations from the one parameter exponential family in a Bayesian framework.\nWe give a random design which allocates mi the number of observations from\npopulation Pi such that the Bayes risk associated with squared error loss and\ncost per unit observation is as small as possible. The design is shown to be\nasymptotically optimal.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 21:18:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2012 23:13:35 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Benkamra", "Zohra", ""], ["Terbeche", "Mekki", ""], ["Tlemcani", "Mounir", ""]]}, {"id": "1202.5682", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Jun Yan", "title": "Goodness-of-fit testing based on a weighted bootstrap: A fast\n  large-sample alternative to the parametric bootstrap", "comments": "26 pages, 5 tables, 1 figure", "journal-ref": "The Canadian Journal of Statistics 40:3, pages 480-501, 2012", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process comparing the empirical cumulative distribution function of the\nsample with a parametric estimate of the cumulative distribution function is\nknown as the empirical process with estimated parameters and has been\nextensively employed in the literature for goodness-of-fit testing. The\nsimplest way to carry out such goodness-of-fit tests, especially in a\nmultivariate setting, is to use a parametric bootstrap. Although very easy to\nimplement, the parametric bootstrap can become very computationally expensive\nas the sample size, the number of parameters, or the dimension of the data\nincrease. An alternative resampling technique based on a fast weighted\nbootstrap is proposed in this paper, and is studied both theoretically and\nempirically. The outcome of this work is a generic and computationally\nefficient multiplier goodness-of-fit procedure that can be used as a\nlarge-sample alternative to the parametric bootstrap. In order to approximately\ndetermine how large the sample size needs to be for the parametric and weighted\nbootstraps to have roughly equivalent powers, extensive Monte Carlo experiments\nare carried out in dimension one, two and three, and for models containing up\nto nine parameters. The computational gains resulting from the use of the\nproposed multiplier goodness-of-fit procedure are illustrated on trivariate\nfinancial data. A by-product of this work is a fast large-sample\ngoodness-of-fit procedure for the bivariate and trivariate t distribution whose\ndegrees of freedom are fixed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 18:32:01 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Yan", "Jun", ""]]}, {"id": "1202.5846", "submitter": "Alex Lenkoski", "authors": "Anna Karl and Alex Lenkoski", "title": "Instrumental Variable Bayesian Model Averaging via Conditional Bayes\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method to perform model averaging in two-stage linear regression\nsystems subject to endogeneity. Our method extends an existing Gibbs sampler\nfor instrumental variables to incorporate a component of model uncertainty.\nDirect evaluation of model probabilities is intractable in this setting. We\nshow that by nesting model moves inside the Gibbs sampler, model comparison can\nbe performed via conditional Bayes factors, leading to straightforward\ncalculations. This new Gibbs sampler is only slightly more involved than the\noriginal algorithm and exhibits no evidence of mixing difficulties. We conclude\nwith a study of two different modeling challenges: incorporating uncertainty\ninto the determinants of macroeconomic growth, and estimating a demand function\nby instrumenting wholesale on retail prices.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 08:00:30 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 10:43:52 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2012 09:26:34 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Karl", "Anna", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1202.5858", "submitter": "Alex Lenkoski", "authors": "A. Lenkoski, T. S. Eicher and A. E. Raftery", "title": "Two-Stage Bayesian Model Averaging in Endogenous Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Economic modeling in the presence of endogeneity is subject to model\nuncertainty at both the instrument and covariate level. We propose a Two-Stage\nBayesian Model Averaging (2SBMA) methodology that extends the Two-Stage Least\nSquares (2SLS) estimator. By constructing a Two-Stage Unit Information Prior in\nthe endogenous variable model, we are able to efficiently combine established\nmethods for addressing model uncertainty in regression models with the classic\ntechnique of 2SLS. To assess the validity of instruments in the 2SBMA context,\nwe develop Bayesian tests of the identification restriction that are based on\nmodel averaged posterior predictive p-values. A simulation study showed that\n2SBMA has the ability to recover structure in both the instrument and covariate\nset, and substantially improves the sharpness of resulting coefficient\nestimates in comparison to 2SLS using the full specification in an automatic\nfashion. Due to the increased parsimony of the 2SBMA estimate, the Bayesian\nSargan test had a power of 50 percent in detecting a violation of the\nexogeneity assumption, while the method based on 2SLS using the full\nspecification had negligible power. We apply our approach to the problem of\ndevelopment accounting, and find support not only for institutions, but also\nfor geography and integration as development determinants, once both model\nuncertainty and endogeneity have been jointly addressed.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 08:47:32 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Lenkoski", "A.", ""], ["Eicher", "T. S.", ""], ["Raftery", "A. E.", ""]]}, {"id": "1202.5883", "submitter": "Yanan Fan Dr", "authors": "J.-L. Dortet-Bernadet and Y. Fan", "title": "On Bayesian quantile regression curve fitting via auxiliary variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression has received increased attention in the statistics\ncommunity in recent years. This article adapts an auxiliary variable method,\ncommonly used in Bayesian variable selection for mean regression models, to the\nfitting of quantile regression curves. We focus on the fitting of regression\nsplines, with unknown number and location of knots. We provide an efficient\nalgorithm with Metropolis-Hastings updates whose tuning is fully automated. The\nmethod is tested on simulated and real examples and its extension to additive\nmodels is described. Finally we propose a simple postprocessing procedure to\ndeal with the problem of the crossing of multiple separately estimated quantile\ncurves.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 10:42:54 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Dortet-Bernadet", "J. -L.", ""], ["Fan", "Y.", ""]]}, {"id": "1202.5999", "submitter": "John Dawson", "authors": "John A. Dawson and Christina Kendziorski", "title": "Survival-supervised latent Dirichlet allocation models for genomic\n  analysis of time-to-event outcomes", "comments": "21 pages, including 6 figures and 5 pages of appendices", "journal-ref": null, "doi": null, "report-no": "TR 225", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two challenging problems in the clinical study of cancer are the\ncharacterization of cancer subtypes and the classification of individual\npatients according to those subtypes. Statistical approaches addressing these\nproblems are hampered by population heterogeneity and challenges inherent in\ndata integration across high-dimensional, diverse covariates. We have developed\na survival-supervised latent Dirichlet allocation (survLDA) modeling framework\nto address these concerns. LDA models have proven extremely effective at\nidentifying themes common across large collections of text, but applications to\ngenomics have been limited. Our framework extends LDA to the genome by\nconsidering each patient as a `document' with `text' constructed from clinical\nand high-dimensional genomic measurements. We then further extend the framework\nto allow for supervision by a time-to-event response. The model enables the\nefficient identification of collections of clinical and genomic features that\nco-occur within patient subgroups, and then characterizes each patient by those\nfeatures. An application of survLDA to The Cancer Genome Atlas (TCGA) ovarian\nproject identifies informative patient subgroups that are characterized by\ndifferent propensities for exhibiting abnormal mRNA expression and\nmethylations, corresponding to differential rates of survival from primary\ntherapy.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 17:10:38 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Dawson", "John A.", ""], ["Kendziorski", "Christina", ""]]}, {"id": "1202.6011", "submitter": "Tom Trigano", "authors": "Y. Sepulcre, T. Trigano and Y. Ritov", "title": "Sparse regression algorithm for activity estimation in $\\gamma $\n  spectrometry", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2264811", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the counting rate estimation of an unknown radioactive source,\nwhich emits photons at times modeled by an homogeneous Poisson process. A\nspectrometer converts the energy of incoming photons into electrical pulses,\nwhose number provides a rough estimate of the intensity of the Poisson process.\nWhen the activity of the source is high, a physical phenomenon known as pileup\neffect distorts direct measurements, resulting in a significant bias to the\nstandard estimators of the source activities used so far in the field. We show\nin this paper that the problem of counting rate estimation can be interpreted\nas a sparse regression problem. We suggest a post-processed, non-negative,\nversion of the Least Absolute Shrinkage and Selection Operator (LASSO) to\nestimate the photon arrival times. The main difficulty in this problem is that\nno theoretical conditions can guarantee consistency in sparsity of LASSO,\nbecause the dictionary is not ideal and the signal is sampled. We therefore\nderive theoretical conditions and bounds which illustrate that the proposed\nmethod can none the less provide a good, close to the best attainable, estimate\nof the counting rate activity. The good performances of the proposed approach\nare studied on simulations and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 17:59:51 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 16:01:02 GMT"}, {"version": "v3", "created": "Thu, 9 May 2013 13:02:15 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Sepulcre", "Y.", ""], ["Trigano", "T.", ""], ["Ritov", "Y.", ""]]}, {"id": "1202.6046", "submitter": "Nicolas Stadler N.S.", "authors": "Nicolas St\\\"adler, Peter B\\\"uhlmann and Sara van de Geer", "title": "L1-Penalization for Mixture Regression Models", "comments": "This is the author's version of the work (published as a discussion\n  paper in TEST, 2010, Volume 19, 209--285). The final publication is available\n  at http://www.springerlink.com", "journal-ref": "TEST, 2010, Volume 19, 209--285", "doi": "10.1007/s11749-010-0197-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a finite mixture of regressions (FMR) model for high-dimensional\ninhomogeneous data where the number of covariates may be much larger than\nsample size. We propose an l1-penalized maximum likelihood estimator in an\nappropriate parameterization. This kind of estimation belongs to a class of\nproblems where optimization and theory for non-convex functions is needed. This\ndistinguishes itself very clearly from high-dimensional estimation with convex\nloss- or objective functions, as for example with the Lasso in linear or\ngeneralized linear models. Mixture models represent a prime and important\nexample where non-convexity arises.\n  For FMR models, we develop an efficient EM algorithm for numerical\noptimization with provable convergence properties. Our penalized estimator is\nnumerically better posed (e.g., boundedness of the criterion function) than\nunpenalized maximum likelihood estimation, and it allows for effective\nstatistical regularization including variable selection. We also present some\nasymptotic theory and oracle inequalities: due to non-convexity of the negative\nlog-likelihood function, different mathematical arguments are needed than for\nproblems with convex losses. Finally, we apply the new method to both simulated\nand real data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 20:37:59 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["St\u00e4dler", "Nicolas", ""], ["B\u00fchlmann", "Peter", ""], ["van de Geer", "Sara", ""]]}, {"id": "1202.6076", "submitter": "Maria Oliveira", "authors": "M. Oliveira, R. M. Crujeiras and A. Rodr\\'iguez-Casal", "title": "A plug-in rule for bandwidth selection in circular density estimation", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new plug-in rule procedure for bandwidth selection in kernel circular\ndensity estimation is introduced. The performance of this proposal is checked\nthroughout a simulation study considering a variety of circular distributions\nexhibiting multimodality, peakedness and/or skewness. The plug-in rule\nbehaviour is also compared with other existing bandwidth selectors. The method\nis illustrated with two classical datasets of cross-beds layers and animal\norientation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 21:26:09 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Oliveira", "M.", ""], ["Crujeiras", "R. M.", ""], ["Rodr\u00edguez-Casal", "A.", ""]]}, {"id": "1202.6263", "submitter": "Sylvie Huet", "authors": "C\\'ecile Durot, Fran\\c{c}ois Koladjo, Sylvie Huet, St\\'ephane Robin", "title": "Estimation of a convex discrete distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric estimation of a convex discrete distribution may be of\ninterest in several applications, such as the estimation of species abundance\ndistribution in ecology. In this paper we study the least squares estimator of\na discrete distribution under the constraint of convexity. We show that this\nestimator exists and is unique, and that it always outperforms the classical\nempirical estimator in terms of the $\\ell_{2}$-distance. We provide an\nalgorithm for its computation, based on the support reduction algorithm. We\ncompare its performance to those of the empirical estimator, on the basis of a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 16:00:20 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Durot", "C\u00e9cile", ""], ["Koladjo", "Fran\u00e7ois", ""], ["Huet", "Sylvie", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1202.6347", "submitter": "Lie Wang", "authors": "Lie Wang", "title": "L1 penalized LAD estimator for high dimensional linear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the high-dimensional sparse linear regression model is\nconsidered, where the overall number of variables is larger than the number of\nobservations. We investigate the L1 penalized least absolute deviation method.\nDifferent from most of other methods, the L1 penalized LAD method does not need\nany knowledge of standard deviation of the noises or any moment assumptions of\nthe noises. Our analysis shows that the method achieves near oracle\nperformance, i.e. with large probability, the L2 norm of the estimation error\nis of order $O(\\sqrt{k \\log p/n})$. The result is true for a wide range of\nnoise distributions, even for the Cauchy distribution. Numerical results are\nalso presented.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 20:27:55 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Wang", "Lie", ""]]}]