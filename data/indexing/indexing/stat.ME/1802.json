[{"id": "1802.00021", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai and Peter Chien", "title": "Another Look at Statistical Calibration: A Non-Asymptotic Theory and\n  Prediction-Oriented Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide another look at the statistical calibration problem in computer\nmodels. This viewpoint is inspired by two overarching practical considerations\nof computer models: (i) many computer models are inadequate for perfectly\nmodeling physical systems, even with the best-tuned calibration parameters;\n(ii) only a finite number of data points are available from the physical\nexperiment associated with a computer model. Following this new line of\nthinking, we provide a non-asymptotic theory and derive a prediction-oriented\ncalibration method. Our calibration method minimizes the predictive mean\nsquared error for a finite sample size with statistical guarantees. We\nintroduce an algorithm to perform the proposed calibration method and connect\nit to existing Bayesian calibration methods. Synthetic and real examples are\nprovided to corroborate the derived theory and illustrate some advantages of\nthe proposed calibration method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:05:45 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 20:28:59 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Dai", "Xiaowu", ""], ["Chien", "Peter", ""]]}, {"id": "1802.00430", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Mung Chiang, Christoph Studer", "title": "Linearized Binary Regression", "comments": "To be presented at CISS (http://ee-ciss.princeton.edu/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probit regression was first proposed by Bliss in 1934 to study mortality\nrates of insects. Since then, an extensive body of work has analyzed and used\nprobit or related binary regression methods (such as logistic regression) in\nnumerous applications and fields. This paper provides a fresh angle to such\nwell-established binary regression methods. Concretely, we demonstrate that\nlinearizing the probit model in combination with linear estimators performs on\npar with state-of-the-art nonlinear regression methods, such as posterior mean\nor maximum aposteriori estimation, for a broad range of real-world regression\nproblems. We derive exact, closed-form, and nonasymptotic expressions for the\nmean-squared error of our linearized estimators, which clearly separates them\nfrom nonlinear regression methods that are typically difficult to analyze. We\nshowcase the efficacy of our methods and results for a number of synthetic and\nreal-world datasets, which demonstrates that linearized binary regression finds\npotential use in a variety of inference, estimation, signal processing, and\nmachine learning applications that deal with binary-valued observations or\nmeasurements.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 18:49:14 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Lan", "Andrew S.", ""], ["Chiang", "Mung", ""], ["Studer", "Christoph", ""]]}, {"id": "1802.00474", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep (Deep) Mukhopadhyay and Douglas Fletcher", "title": "Bayesian Modeling via Goodness-of-fit", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two key issues of modern Bayesian statistics are: (i) establishing\nprincipled approach for distilling statistical prior that is consistent with\nthe given data from an initial believable scientific prior; and (ii)\ndevelopment of a Bayes-frequentist consolidated data analysis workflow that is\nmore effective than either of the two separately. In this paper, we propose the\nidea of \"Bayes via goodness of fit\" as a framework for exploring these\nfundamental questions, in a way that is general enough to embrace almost all of\nthe familiar probability models. Several illustrative examples show the benefit\nof this new point of view as a practical data analysis tool. Relationship with\nother Bayesian cultures is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 20:15:31 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 03:08:44 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 22:28:39 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Subhadeep", "", "", "Deep"], ["Mukhopadhyay", "", ""], ["Fletcher", "Douglas", ""]]}, {"id": "1802.00495", "submitter": "Lu Zhang", "authors": "Lu Zhang, Abhirup Datta, Sudipto Banerjee", "title": "Practical Bayesian Modeling and Inference for Massive Spatial Datasets\n  On Modest Computing Environments", "comments": "20 pages, 4 figures, 2 tables", "journal-ref": "Statistical Analysis and Data Mining 2019", "doi": "10.1002/sam.11413", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With continued advances in Geographic Information Systems and related\ncomputational technologies, statisticians are often required to analyze very\nlarge spatial datasets. This has generated substantial interest over the last\ndecade, already too vast to be summarized here, in scalable methodologies for\nanalyzing large spatial datasets. Scalable spatial process models have been\nfound especially attractive due to their richness and flexibility and,\nparticularly so in the Bayesian paradigm, due to their presence in hierarchical\nmodel settings. However, the vast majority of research articles present in this\ndomain have been geared toward innovative theory or more complex model\ndevelopment. Very limited attention has been accorded to approaches for easily\nimplementable scalable hierarchical models for the practicing scientist or\nspatial analyst. This article is submitted to the Practice section of the\njournal with the aim of developing massively scalable Bayesian approaches that\ncan rapidly deliver Bayesian inference on spatial process that are practically\nindistinguishable from inference obtained using more expensive alternatives. A\nkey emphasis is on implementation within very standard (modest) computing\nenvironments (e.g., a standard desktop or laptop) using easily available\nstatistical software packages without requiring message-parsing interfaces or\nparallel programming paradigms. Key insights are offered regarding assumptions\nand approximations concerning practical efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 21:33:15 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 18:39:07 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhang", "Lu", ""], ["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1802.00515", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri, Shaowu Yuchi, Geoffrey T. Parks", "title": "Dimension Reduction via Gaussian Ridge Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge functions have recently emerged as a powerful set of ideas for\nsubspace-based dimension reduction. In this paper we begin by drawing parallels\nbetween ridge subspaces, sufficient dimension reduction and active subspaces,\ncontrasting between techniques rooted in statistical regression and those\nrooted in approximation theory. This sets the stage for our new algorithm that\napproximates what we call a Gaussian ridge function---the posterior mean of a\nGaussian process on a dimension-reducing subspace---suitable for both\nregression and approximation problems. To compute this subspace we develop an\niterative algorithm that optimizes over the Stiefel manifold to compute the\nsubspace, followed by an optimization of the hyperparameters of the Gaussian\nprocess. We demonstrate the utility of the algorithm on two analytical\nfunctions, where we obtain near exact ridge recovery, and a turbomachinery case\nstudy, where we compare the efficacy of our approach with three well-known\nsufficient dimension reduction methods: SIR, SAVE, CR. The comparisons motivate\nthe use of the posterior variance as a heuristic for identifying the\nsuitability of a dimension-reducing subspace.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 23:23:50 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 18:54:53 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Seshadri", "Pranay", ""], ["Yuchi", "Shaowu", ""], ["Parks", "Geoffrey T.", ""]]}, {"id": "1802.00517", "submitter": "Manoel Santos Neto", "authors": "Vera Tomazella, Juv\\^encio S. Nobre, Gustavo H.A. Pereira and Manoel\n  Santos-Neto", "title": "Zero-adjusted Birnbaum-Saunders regression model", "comments": "13 pages 9 figures", "journal-ref": null, "doi": "10.1016/j.spl.2019.01.019", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the zero-adjusted Birnbaum-Saunders regression\nmodel. This new model generalizes at least seven Birnbaum-Saunders regression\nmodels. The idea of this modeling is mixing a degenerate distribution at zero\nwith a Birnbaum-Saunders distribution. Besides the capacity to account for\nexcess zeros, the zero-adjusted Birnbaum-Saunders distribution additionally\nproduces an attractive modeling structure to right-skewed data. In this model,\nthe mean and precision parameter of the Birnbaum-Saunders distribution and the\nprobability of zeros can be related to linear and/or non-linear predictors\nthrough link functions. We derive a type of residual to perform diagnostic\nanalysis and a perturbation scheme for identifying those observations that\nexert unusual influence on the estimation process. Finally, two applications to\nreal data show the potential of the model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 23:39:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Tomazella", "Vera", ""], ["Nobre", "Juv\u00eancio S.", ""], ["Pereira", "Gustavo H. A.", ""], ["Santos-Neto", "Manoel", ""]]}, {"id": "1802.00527", "submitter": "John Moreland", "authors": "J. Scott Moreland and Matthew C. Superdock", "title": "Predicting outcomes for games of skill by redefining what it means to\n  win", "comments": "The model is publicly available at https://github.com/morelandjs/melo", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Elo rating system is a highly successful ranking algorithm for games of\nskill where, by construction, one team wins and the other loses. A primary\nlimitation of the original Elo algorithm is its inability to predict\ninformation beyond a match's win-loss probability. Specifically, the victor is\nawarded the same point bounty if he beats a team by 1 point or 10 points; only\nthe rating difference between the team and its opponent affects the match\nbounty. In this work, we explain that Elo ratings and predictions can be\nnaturally extended to include margin-of-victory information by simply\nredefining \"what it means to win.\" We create ratings for each value of the\nmargin-of-victory and use these ratings to predict the full distribution of\npoint spread outcomes for matches which have not yet been played.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 01:04:33 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Moreland", "J. Scott", ""], ["Superdock", "Matthew C.", ""]]}, {"id": "1802.00665", "submitter": "Xiaoqi Zhang", "authors": "Xiaoqi Zhang, Xiaobing Zhao, and Yanqiao Zheng", "title": "A novel approach to estimate the Cox model with temporal covariates and\n  its application to medical cost data", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to estimate the Cox model with temporal\ncovariates. Our new approach treats the temporal covariates as arising from a\nlongitudinal process which is modeled jointly with the event time. Different\nfrom the literature, the longitudinal process in our model is specified as a\nbounded variational process and determined by a family of Initial Value\nProblems associated with an Ordinary Differential Equation. Our specification\nhas the advantage that only the observation of the temporal covariates at the\ntime to event and the time to event itself are required to fit the model, while\nit is fine but not necessary to have more longitudinal observations. This fact\nmakes our approach very useful for many medical outcome datasets, like the New\nYork State Statewide Planning and Research Cooperative System and the National\nInpatient Sample, where it is important to find the hazard rate of being\ndischarged given the accumulative cost but only the total cost at the discharge\ntime is available due to the protection of patient information. Our estimation\nprocedure is based on maximizing the full information likelihood function. The\nresulting estimators are shown to be consistent and asymptotically normally\ndistributed. Variable selection techniques, like Adaptive LASSO, can be easily\nmodified and incorporated into our estimation procedure. The oracle property is\nverified for the resulting estimator of the regression coefficients.\nSimulations and a real example illustrate the practical utility of the proposed\nmodel. Finally, a couple of potential extensions of our approach are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 12:53:31 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Zhang", "Xiaoqi", ""], ["Zhao", "Xiaobing", ""], ["Zheng", "Yanqiao", ""]]}, {"id": "1802.00677", "submitter": "Xiaowei Zhang", "authors": "Lu Zou and Xiaowei Zhang", "title": "Stochastic Kriging for Inadequate Simulation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic kriging is a popular metamodeling technique for representing the\nunknown response surface of a simulation model. However, the simulation model\nmay be inadequate in the sense that there may be a non-negligible discrepancy\nbetween it and the real system of interest. Failing to account for the model\ndiscrepancy may conceivably result in erroneous prediction of the real system's\nperformance and mislead the decision-making process. This paper proposes a\nmetamodel that extends stochastic kriging to incorporate the model discrepancy.\nBoth the simulation outputs and the real data are used to characterize the\nmodel discrepancy. The proposed metamodel can provably enhance the prediction\nof the real system's performance. We derive general results for experiment\ndesign and analysis, and demonstrate the advantage of the proposed metamodel\nrelative to competing methods. Finally, we study the effect of Common Random\nNumbers (CRN). The use of CRN is well known to be detrimental to the prediction\naccuracy of stochastic kriging in general. By contrast, we show that the effect\nof CRN in the new context is substantially more complex. The use of CRN can be\neither detrimental or beneficial depending on the interplay between the\nmagnitude of the observation errors and other parameters involved.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 13:30:40 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 07:14:52 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zou", "Lu", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1802.00796", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Fabrizio Leisen", "title": "Bayes Calculations from Quantile Implied Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical practice, a realistic Bayesian model for a given data set can\nbe defined by a likelihood function that is analytically or computationally\nintractable, due to large data sample size, high parameter dimensionality, or\ncomplex likelihood functional form. This in turn poses challenges to the\ncomputation and inference of the posterior distribution of the model\nparameters. For such a model, a tractable likelihood function is introduced\nwhich approximates the exact likelihood through its quantile function. It is\ndefined by an asymptotic chi-square confidence distribution for a pivotal\nquantity, which is generated by the asymptotic normal distribution of the\nsample quantiles given model parameters. This Quantile Implied Likelihood (QIL)\ngives rise to an approximate posterior distribution which can be estimated by\nusing penalized log-likelihood maximization or any suitable Monte Carlo\nalgorithm. The QIL approach to Bayesian Computation is illustrated through the\nBayesian analysis of simulated and real data sets having sample sizes that\nreach the millions. The analyses involve various models for univariate or\nmultivariate iid or non-iid data, with low or high parameter dimensionality,\nmany of which are defined by intractable likelihoods. The probability models\ninclude the Student's t, g-and-h, and g-and-k distributions; the Bayesian logit\nregression model with many covariates; exponential random graph model, a\ndoubly-intractable model for networks; the multivariate skew normal model, for\nrobust inference of the inverse-covariance matrix when it is large relative to\nthe sample size; and the Wallenius distribution model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 18:47:19 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 19:06:44 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 13:11:26 GMT"}, {"version": "v4", "created": "Sat, 16 Mar 2019 17:31:56 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Karabatsos", "George", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1802.00852", "submitter": "Matthias Chung", "authors": "M. Chung, M. Binois, R.B. Gramacy, D.J. Moquin, A.P. Smith, A.M. Smith", "title": "Parameter and Uncertainty Estimation for Dynamical Systems Using\n  Surrogate Stochastic Processes", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference on unknown quantities in dynamical systems via observational data\nis essential for providing meaningful insight, furnishing accurate predictions,\nenabling robust control, and establishing appropriate designs for future\nexperiments. Merging mathematical theory with empirical measurements in a\nstatistically coherent way is critical and challenges abound, e.g.,:\nill-posedness of the parameter estimation problem, proper regularization and\nincorporation of prior knowledge, and computational limitations on full\nuncertainty qualification. To address these issues, we propose a new method for\nlearning parameterized dynamical systems from data. In many ways, our proposal\nturns the canonical framework on its head. We first fit a surrogate stochastic\nprocess to observational data, enforcing prior knowledge (e.g., smoothness),\nand coping with challenging data features like heteroskedasticity, heavy tails\nand censoring. Then, samples of the stochastic process are used as \"surrogate\ndata\" and point estimates are computed via ordinary point estimation methods in\na modular fashion. An attractive feature of this approach is that it is fully\nBayesian and simultaneously parallelizable. We demonstrate the advantages of\nour new approach on a predator prey simulation study and on a real world\napplication involving within-host influenza virus infection data paired with a\nviral kinetic model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 21:33:20 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chung", "M.", ""], ["Binois", "M.", ""], ["Gramacy", "R. B.", ""], ["Moquin", "D. J.", ""], ["Smith", "A. P.", ""], ["Smith", "A. M.", ""]]}, {"id": "1802.00996", "submitter": "Assaf Rabinowicz", "authors": "Assaf Rabinowicz, Saharon Rosset", "title": "Assessing Prediction Error at Interpolation and Extrapolation Points", "comments": "Corrected typos and editing change. The content remain the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Common model selection criteria, such as $AIC$ and its variants, are based on\nin-sample prediction error estimators. However, in many applications involving\npredicting at interpolation and extrapolation points, in-sample error cannot be\nused for estimating the prediction error. In this paper new prediction error\nestimators, $tAI$ and $Loss(w_{t})$ are introduced. These estimators generalize\nprevious error estimators, however are also applicable for assessing prediction\nerror in cases involving interpolation and extrapolation. Based on the\nprediction error estimators, two model selection criteria with the same spirit\nas $AIC$ are suggested. The advantages of our suggested methods are\ndemonstrated in simulation and real data analysis of studies involving\ninterpolation and extrapolation in a Linear Mixed Model framework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 16:48:41 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 18:30:59 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Rabinowicz", "Assaf", ""], ["Rosset", "Saharon", ""]]}, {"id": "1802.01018", "submitter": "Zach Branson", "authors": "Zach Branson and Luke Miratrix", "title": "Randomization Tests that Condition on Non-Categorical Covariate Balance", "comments": "54 pages, 12 Figures", "journal-ref": null, "doi": "10.1515/jci-2018-0004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A benefit of randomized experiments is that covariate distributions of\ntreatment and control groups are balanced on average, resulting in simple\nunbiased estimators for treatment effects. However, it is possible that a\nparticular randomization yields covariate imbalances that researchers want to\naddress in the analysis stage through adjustment or other methods. Here we\npresent a randomization test that conditions on covariate balance by only\nconsidering treatment assignments that are similar to the observed one in terms\nof covariate balance. Previous conditional randomization tests have only\nallowed for categorical covariates, while our randomization test allows for any\ntype of covariate. Through extensive simulation studies, we find that our\nconditional randomization test is more powerful than unconditional\nrandomization tests and other conditional tests. Furthermore, we find that our\nconditional randomization test is valid (1) unconditionally across levels of\ncovariate balance, and (2) conditional on particular levels of covariate\nbalance. Meanwhile, unconditional randomization tests are valid for (1) but not\n(2). Finally, we find that our conditional randomization test is similar to a\nrandomization test that uses a model-adjusted test statistic.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 19:47:38 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 18:00:38 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 12:30:37 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Branson", "Zach", ""], ["Miratrix", "Luke", ""]]}, {"id": "1802.01085", "submitter": "Rapha\\\"el Huser", "authors": "Thomas Opitz, Rapha\\\"el Huser, Haakon Bakka, H{\\aa}vard Rue", "title": "INLA goes extreme: Bayesian tail regression for the estimation of high\n  spatio-temporal quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work has been motivated by the challenge of the 2017 conference on\nExtreme-Value Analysis (EVA2017), with the goal of predicting daily\nprecipitation quantiles at the $99.8\\%$ level for each month at observed and\nunobserved locations. We here develop a Bayesian generalized additive modeling\nframework tailored to estimate complex trends in marginal extremes observed\nover space and time. Our approach is based on a set of regression equations\nlinked to the exceedance probability above a high threshold and to the size of\nthe excess, the latter being modeled using the generalized Pareto (GP)\ndistribution suggested by Extreme-Value Theory. Latent random effects are\nmodeled additively and semi-parametrically using Gaussian process priors, which\nprovides high flexibility and interpretability. Fast and accurate estimation of\nposterior distributions may be performed thanks to the Integrated Nested\nLaplace approximation (INLA), efficiently implemented in the R-INLA software,\nwhich we also use for determining a nonstationary threshold based on a model\nfor the body of the distribution. We show that the GP distribution meets the\ntheoretical requirements of INLA, and we then develop a penalized complexity\nprior specification for the tail index, which is a crucial parameter for\nextrapolating tail event probabilities. This prior concentrates mass close to a\nlight exponential tail while allowing heavier tails by penalizing the distance\nto the exponential distribution. We illustrate this methodology through the\nmodeling of spatial and seasonal trends in daily precipitation data provided by\nthe EVA2017 challenge. Capitalizing on R-INLA's fast computation capacities and\nlarge distributed computing resources, we conduct an extensive cross-validation\nstudy to select model parameters governing the smoothness of trends. Our\nresults outperform simple benchmarks and are comparable to the best-scoring\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 07:19:00 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Opitz", "Thomas", ""], ["Huser", "Rapha\u00ebl", ""], ["Bakka", "Haakon", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1802.01152", "submitter": "Andrew Blumberg", "authors": "Andrew J. Blumberg, Prithwish Bhaumik, Stephen G. Walker", "title": "Testing to distinguish measures on metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of distinguishing between two distributions on a metric\nspace; i.e., given metric measure spaces $({\\mathbb X}, d, \\mu_1)$ and\n$({\\mathbb X}, d, \\mu_2)$, we are interested in the problem of determining from\nfinite data whether or not $\\mu_1$ is $\\mu_2$. The key is to use pairwise\ndistances between observations and, employing a reconstruction theorem of\nGromov, we can perform such a test using a two sample Kolmogorov--Smirnov test.\nA real analysis using phylogenetic trees and flu data is presented.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 16:25:53 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Blumberg", "Andrew J.", ""], ["Bhaumik", "Prithwish", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1802.01428", "submitter": "Matteo Quartagno", "authors": "Matteo Quartagno, A. Sarah Walker, James R. Carpenter, Patrick P.J.\n  Phillips and Mahesh K.B. Parmar", "title": "Re-thinking non-inferiority: a practical trial design for optimising\n  treatment duration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: trials to identify the minimal effective treatment duration are\nneeded in different therapeutic areas, including bacterial infections, TB and\nHepatitis--C. However, standard non-inferiority designs have several\nlimitations, including arbitrariness of non-inferiority margins, choice of\nresearch arms and very large sample sizes.\n  Methods: we recast the problem of finding an appropriate non-inferior\ntreatment duration in terms of modelling the entire duration-response curve\nwithin a pre-specified range. We propose a multi-arm randomised trial design,\nallocating patients to different treatment durations. We use fractional\npolynomials and spline-based methods to flexibly model the duration-response\ncurve. We compare different methods in terms of a scaled version of the area\nbetween true and estimated prediction curves. We evaluate sensitivity to key\ndesign parameters, including sample size, number and position of arms.\n  Results: a total sample size of $\\sim 500$ patients divided into a moderate\nnumber of equidistant arms (5-7) is sufficient to estimate the\nduration-response curve within a $5\\%$ error margin in $95\\%$ of the\nsimulations. Fractional polynomials provide similar or better results than\nspline-based methods in most scenarios.\n  Conclusions: our proposed practical randomised trial design is an alternative\nto standard non-inferiority designs, avoiding many of their limitations, and\nyet being fairly robust to different possible duration-response curves. The\ntrial outcome is the whole duration-response curve, which could be used by\nclinicians and policy makers to make informed decisions, facilitating a move\naway from a forced binary hypothesis testing paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 14:46:54 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Quartagno", "Matteo", ""], ["Walker", "A. Sarah", ""], ["Carpenter", "James R.", ""], ["Phillips", "Patrick P. J.", ""], ["Parmar", "Mahesh K. B.", ""]]}, {"id": "1802.01535", "submitter": "Linda Mhalla", "authors": "Linda Mhalla and Thomas Opitz and Val\\'erie Chavez-Demoulin", "title": "Exceedance-based nonlinear regression of tail dependence", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability and structure of co-occurrences of extreme values in\nmultivariate data may critically depend on auxiliary information provided by\ncovariates. In this contribution, we develop a flexible generalized additive\nmodeling framework based on high threshold exceedances for estimating\ncovariate-dependent joint tail characteristics for regimes of asymptotic\ndependence and asymptotic independence. The framework is based on suitably\ndefined marginal pretransformations and projections of the random vector along\nthe directions of the unit simplex, which lead to convenient univariate\nrepresentations of multivariate exceedances based on the exponential\ndistribution. Good performance of our estimators of a nonparametrically\ndesigned influence of covariates on extremal coefficients and tail dependence\ncoefficients are shown through a simulation study. We illustrate the usefulness\nof our modeling framework on a large dataset of nitrogen dioxide measurements\nrecorded in France between 1999 and 2012, where we use the generalized additive\nframework for modeling marginal distributions and tail dependence in monthly\nmaxima. Our results imply asymptotic independence of data observed at different\nstations, and we find that the estimated coefficients of tail dependence\ndecrease as a function of spatial distance and show distinct patterns for\ndifferent years and for different types of stations (traffic vs. background).\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 17:58:13 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Mhalla", "Linda", ""], ["Opitz", "Thomas", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""]]}, {"id": "1802.01715", "submitter": "Emanuele Dolera", "authors": "Emanuele Dolera, Stefano Favaro, Andrea Bulgarelli and Alessio Aboudan", "title": "A Wilks' theorem for grouped data", "comments": "arXiv admin note: text overlap with arXiv:1411.0947", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $n$ independent measurements, with the additional information of the\ntimes at which measurements are performed. This paper deals with testing\nstatistical hypotheses when $n$ is large and only a small amount of\nobservations concentrated in short time intervals are relevant to the study. We\ndefine a testing procedure in terms of multiple likelihood ratio (LR)\nstatistics obtained by splitting the observations into groups, and in\naccordance with the following principles: P1) each LR statistic is formed by\ngathering the data included in $G$ consecutive vectors of observations, where\n$G$ is a suitable time window defined a priori with respect to an arbitrary\nchoice of the `origin of time'; P2) the null statistical hypothesis is rejected\nonly if at least $k$ LR statistics are sufficiently small, for a suitable\nchoice of $k$. We show that the application of the classical Wilks' theorem may\nbe affected by the arbitrary choice of the \"origin of time\", in connection with\nP1). We then introduce a Wilks' theorem for grouped data which leads to a\ntesting procedure that overcomes the problem of the arbitrary choice of the\n`origin of time', while fulfilling P1) and P2). Such a procedure is more\npowerful than the corresponding procedure based on Wilks' theorem.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 22:23:19 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 13:43:10 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 20:38:11 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Dolera", "Emanuele", ""], ["Favaro", "Stefano", ""], ["Bulgarelli", "Andrea", ""], ["Aboudan", "Alessio", ""]]}, {"id": "1802.01718", "submitter": "Konstantinos Kaloudis", "authors": "Konstantinos Kaloudis, Spyridon J. Hatjispyros", "title": "A Bayesian Nonparametric Approach to Dynamical Noise Reduction", "comments": null, "journal-ref": null, "doi": "10.1063/1.5025545", "report-no": null, "categories": "stat.ME nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric approach for the noise reduction of a\ngiven chaotic time series contaminated by dynamical noise, based on Markov\nChain Monte Carlo methods (MCMC). The underlying unknown noise process\n(possibly) exhibits heavy tailed behavior. We introduce the Dynamic Noise\nReduction Replicator (DNRR) model with which we reconstruct the unknown dynamic\nequations and in parallel we replicate the dynamics under reduced noise level\ndynamical perturbations. The dynamic noise reduction procedure is demonstrated\nspecifically in the case of polynomial maps. Simulations based on synthetic\ntime series are presented.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 22:32:22 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 19:56:00 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kaloudis", "Konstantinos", ""], ["Hatjispyros", "Spyridon J.", ""]]}, {"id": "1802.01936", "submitter": "Bikramjit Das", "authors": "Bikramjit Das and Vicky Fasen-Hartmann", "title": "Hidden regular variation, copula models, and the limit behavior of\n  conditional excess risk measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk measures like Marginal Expected Shortfall and Marginal Mean Excess\nquantify conditional risk and in particular, aid in the understanding of\nsystemic risk. In many such scenarios, models exhibiting heavy tails in the\nmargins and asymptotic tail independence in the joint behavior play a\nfundamental role. The notion of hidden regular variation has the advantage that\nit models both properties: asymptotic tail independence as well as heavy tails.\nAn alternative approach to addressing these features is via copulas. First, we\nelicit connections between hidden regular variation and the behavior of tail\ncopula parameters extending previous works in this area. Then we study the\nasymptotic behavior of the aforementioned conditional excess risk measures;\nfirst under hidden regular variation and then under restrictions on the tail\ncopula parameters, not necessarily assuming hidden regular variation. We\nprovide a broad variety of examples of models admitting heavy tails and\nasymptotic tail independence along with hidden regular variation and with the\nappropriate limit behavior for the risk measures of interest.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 13:31:27 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Das", "Bikramjit", ""], ["Fasen-Hartmann", "Vicky", ""]]}, {"id": "1802.01946", "submitter": "P{\\aa}l Christie Ryalen", "authors": "P{\\aa}l Christie Ryalen, Mats Julius Stensrud, Kjetil R{\\o}ysland", "title": "The additive hazard estimator is consistent for continuous-time marginal\n  structural models", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) allow for causal analysis of longitudinal\ndata. The MSMs were originally developed as discrete time models. Recently,\ncontinuous-time MSMs were presented as a conceptually appealing alternative for\nsurvival analysis. In applied analyses, it is often assumed that the\ntheoretical treatment weights are known, but these weights are usually unknown\nand must be estimated from the data. Here we provide a sufficient condition for\na class of continuous-time MSMs to be consistent even when the weights are\nestimated, and we show how additive hazard models can be used to estimate such\nweights. Our results suggest that the continuous-time weights perform better\nthan IPTW when the underlying treatment process is continuous. Furthermore, we\nmay wish to transform effect estimates of hazards to other scales that are\neasier to interpret causally. We show that a general transformation strategy\ncan be used on weighted cumulative hazard estimates to obtain a range of other\nparameters in survival analysis, and demonstrate how this strategy can be\napplied on data using our R packages ahw and transform.hazards.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 13:51:09 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 13:24:11 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 14:28:38 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 13:49:18 GMT"}, {"version": "v5", "created": "Tue, 12 Feb 2019 19:47:14 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ryalen", "P\u00e5l Christie", ""], ["Stensrud", "Mats Julius", ""], ["R\u00f8ysland", "Kjetil", ""]]}, {"id": "1802.02163", "submitter": "Brandon Stewart", "authors": "Naoki Egami, Christian J. Fong, Justin Grimmer, Margaret E. Roberts,\n  Brandon M. Stewart", "title": "How to Make Causal Inferences Using Texts", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New text as data techniques offer a great promise: the ability to inductively\ndiscover measures that are useful for testing social science theories of\ninterest from large collections of text. We introduce a conceptual framework\nfor making causal inferences with discovered measures as a treatment or\noutcome. Our framework enables researchers to discover high-dimensional textual\ninterventions and estimate the ways that observed treatments affect text-based\noutcomes. We argue that nearly all text-based causal inferences depend upon a\nlatent representation of the text and we provide a framework to learn the\nlatent representation. But estimating this latent representation, we show,\ncreates new risks: we may introduce an identification problem or overfit. To\naddress these risks we describe a split-sample framework and apply it to\nestimate causal effects from an experiment on immigration attitudes and a study\non bureaucratic response. Our work provides a rigorous foundation for\ntext-based causal inferences.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 19:00:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Egami", "Naoki", ""], ["Fong", "Christian J.", ""], ["Grimmer", "Justin", ""], ["Roberts", "Margaret E.", ""], ["Stewart", "Brandon M.", ""]]}, {"id": "1802.02251", "submitter": "Faming Liang", "authors": "Faming Liang, Bochao Jia, Jingnan Xue, Qizhai Li, Ye Luo", "title": "An Imputation-Consistency Algorithm for High-Dimensional Missing Data\n  Problems and Beyond", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are frequently encountered in high-dimensional problems, but\nthey are usually difficult to deal with using standard algorithms, such as the\nexpectation-maximization (EM) algorithm and its variants. To tackle this\ndifficulty, some problem-specific algorithms have been developed in the\nliterature, but there still lacks a general algorithm. This work is to fill the\ngap: we propose a general algorithm for high-dimensional missing data problems.\nThe proposed algorithm works by iterating between an imputation step and a\nconsistency step. At the imputation step, the missing data are imputed\nconditional on the observed data and the current estimate of parameters; and at\nthe consistency step, a consistent estimate is found for the minimizer of a\nKullback-Leibler divergence defined on the pseudo-complete data. For high\ndimensional problems, the consistent estimate can be found under sparsity\nconstraints. The consistency of the averaged estimate for the true parameter\ncan be established under quite general conditions. The proposed algorithm is\nillustrated using high-dimensional Gaussian graphical models, high-dimensional\nvariable selection, and a random coefficient model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 22:45:38 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Liang", "Faming", ""], ["Jia", "Bochao", ""], ["Xue", "Jingnan", ""], ["Li", "Qizhai", ""], ["Luo", "Ye", ""]]}, {"id": "1802.02368", "submitter": "Olivier Roustant", "authors": "Olivier Roustant (GdR MASCOT-NUM, LIMOS, FAYOL-ENSMSE, UCA), Esperan\n  Padonou, Yves Deville, Alo\\\"is Cl\\'ement (CEA/DAM), Guillaume Perrin\n  (DAM/DIF), Jean Giorla (DAM/DIF), Henry Wynn (LSE)", "title": "Group kernels for Gaussian process metamodels with categorical inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are widely used as a metamodel for emulating\ntime-consuming computer codes. We focus on problems involving categorical\ninputs, with a potentially large number L of levels (typically several tens),\npartitioned in G << L groups of various sizes. Parsimonious covariance\nfunctions, or kernels, can then be defined by block covariance matrices T with\nconstant covariances between pairs of blocks and within blocks. We study the\npositive definiteness of such matrices to encourage their practical use. The\nhierarchical group/level structure, equivalent to a nested Bayesian linear\nmodel, provides a parameterization of valid block matrices T. The same model\ncan then be used when the assumption within blocks is relaxed, giving a\nflexible parametric family of valid covariance matrices with constant\ncovariances between pairs of blocks. The positive definiteness of T is\nequivalent to the positive definiteness of a smaller matrix of size G, obtained\nby averaging each block. The model is applied to a problem in nuclear waste\nanalysis, where one of the categorical inputs is atomic number, which has more\nthan 90 levels.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 09:37:20 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 09:25:54 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Roustant", "Olivier", "", "GdR MASCOT-NUM, LIMOS, FAYOL-ENSMSE, UCA"], ["Padonou", "Esperan", "", "CEA/DAM"], ["Deville", "Yves", "", "CEA/DAM"], ["Cl\u00e9ment", "Alo\u00efs", "", "CEA/DAM"], ["Perrin", "Guillaume", "", "DAM/DIF"], ["Giorla", "Jean", "", "DAM/DIF"], ["Wynn", "Henry", "", "LSE"]]}, {"id": "1802.02467", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Tsung-I Lin, and Geoffrey J. McLachlan", "title": "Mixtures of Factor Analyzers with Fundamental Skew Symmetric\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of factor analyzers (MFA) provide a powerful tool for modelling\nhigh-dimensional datasets. In recent years, several generalizations of MFA have\nbeen developed where the normality assumption of the factors and/or of the\nerrors was relaxed to allow for skewness in the data. However, due to the form\nof the adopted component densities, the distribution of the factors/errors in\nmost of these models is typically limited to modelling skewness oncentrated in\na single direction. Here, we introduce a more flexible finite mixture of factor\nanalyzers based on the class of scale mixtures of canonical fundamental skew\nnormal (SMCFUSN) distributions. This very general class of skew distributions\ncan capture various types of skewness and asymmetry in the data. In particular,\nthe proposed mixture model of SMCFUSN factor analyzers(SMCFUSNFA) can\nsimultaneously accommodate multiple directions of skewness. As such, it\nencapsulates many commonly used models as special and/or limiting cases, such\nas models of some versions of skew normal and skew t-factor analyzers, and skew\nhyperbolic factor analyzers. For illustration, we focus on the t-distribution\nmember of the class of SMCFUSN distributions, leading to mixtures of canonical\nfundamental skew t-factor analyzers (CFUSTFA). Parameter estimation can be\ncarried out by maximum likelihood via an EM-type algorithm. The usefulness and\npotential of the proposed model are demonstrated using two real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 15:09:18 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 08:03:58 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Lee", "Sharon X.", ""], ["Lin", "Tsung-I", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1802.02557", "submitter": "Yang Feng", "authors": "Xin Tong, Lucy Xia, Jiacheng Wang and Yang Feng", "title": "Neyman-Pearson classification: parametrics and sample size requirement", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers\nthat achieve a minimal type II error while enforcing the prioritized type I\nerror controlled under some user-specified level $\\alpha$. This paradigm serves\nnaturally in applications such as severe disease diagnosis and spam detection,\nwhere people have clear priorities among the two error types. Recently, Tong,\nFeng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all\nscoring-type classification methods (e.g., logistic regression, support vector\nmachines, random forest) to respect the given type I error upper bound $\\alpha$\nwith high probability, without specific distributional assumptions on the\nfeatures and the responses. Universal the umbrella algorithm is, it demands an\nexplicit minimum sample size requirement on class $0$, which is often the more\nscarce class, such as in rare disease diagnosis applications. In this work, we\nemploy the parametric linear discriminant analysis (LDA) model and propose a\nnew parametric thresholding algorithm, which does not need the minimum sample\nsize requirements on class $0$ observations and thus is suitable for small\nsample applications such as rare disease diagnosis. Leveraging both the\nexisting nonparametric and the newly proposed parametric thresholding rules, we\npropose four LDA-based NP classifiers, for both low- and high-dimensional\nsettings. On the theoretical front, we prove NP oracle inequalities for one\nproposed classifier, where the rate for excess type II error benefits from the\nexplicit parametric model assumption. Furthermore, as NP classifiers involve a\nsample splitting step of class $0$ observations, we construct a new adaptive\nsample splitting scheme that can be applied universally to NP classifiers, and\nthis adaptive strategy reduces the type II error of these classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:32:47 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 20:23:36 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 14:43:55 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 09:50:12 GMT"}, {"version": "v5", "created": "Tue, 28 Jan 2020 19:37:52 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Tong", "Xin", ""], ["Xia", "Lucy", ""], ["Wang", "Jiacheng", ""], ["Feng", "Yang", ""]]}, {"id": "1802.02558", "submitter": "Richard Zhao", "authors": "Lucy Xia, Richard Zhao, Yanhui Wu and Xin Tong", "title": "Intentional Control of Type I Error over Unconscious Data Distortion: a\n  Neyman-Pearson Approach to Text Classification", "comments": null, "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2020.1740711", "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenges in classifying textual data obtained from\nopen online platforms, which are vulnerable to distortion. Most existing\nclassification methods minimize the overall classification error and may yield\nan undesirably large type I error (relevant textual messages are classified as\nirrelevant), particularly when available data exhibit an asymmetry between\nrelevant and irrelevant information. Data distortion exacerbates this situation\nand often leads to fallacious prediction. To deal with inestimable data\ndistortion, we propose the use of the Neyman-Pearson (NP) classification\nparadigm, which minimizes type II error under a user-specified type I error\nconstraint. Theoretically, we show that the NP oracle is unaffected by data\ndistortion when the class conditional distributions remain the same.\nEmpirically, we study a case of classifying posts about worker strikes obtained\nfrom a leading Chinese microblogging platform, which are frequently prone to\nextensive, unpredictable and inestimable censorship. We demonstrate that, even\nthough the training and test data are susceptible to different distortion and\ntherefore potentially follow different distributions, our proposed NP methods\ncontrol the type I error on test data at the targeted level. The methods and\nimplementation pipeline proposed in our case study are applicable to many other\nproblems involving data distortion.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:33:20 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 19:21:05 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 03:49:46 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Xia", "Lucy", ""], ["Zhao", "Richard", ""], ["Wu", "Yanhui", ""], ["Tong", "Xin", ""]]}, {"id": "1802.02626", "submitter": "Matthew Simpson", "authors": "Matthew Simpson, Scott H. Holan, Christopher K. Wikle, and Jonathan R.\n  Bradley", "title": "Interpolating Population Distributions using Public-use Data with\n  Application to the American Community Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies publish aggregate estimates of various features of the\ndistributions of several socio-demographic quantities of interest based on data\nobtained from a survey. Often these area-level estimates are tabulated at small\ngeographies, but detailed distributional information is not necessarily\navailable at such a fine scale geography due to data quality and/or disclosure\nlimitations. We propose a model-based method to interpolate the disseminated\nestimates for a given variable of interest that improves on previous approaches\nby simultaneously allowing for the use of more types of estimates,\nincorporating the standard error of the estimates into the estimation process,\nand by providing uncertainty quantification so that, for example, interval\nestimates can be obtained for quantities of interest. Our motivating example\nuses the disseminated tabulations and PUMS from the American Community Survey\nto estimate U.S. Census tract-level income distributions and statistics\nassociated with these distributions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:54:17 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 02:54:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Simpson", "Matthew", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""], ["Bradley", "Jonathan R.", ""]]}, {"id": "1802.02649", "submitter": "Rudy Gideon Dr", "authors": "Rudy Gideon", "title": "Correlation Estimation System Minimization Compared to Least Squares\n  Minimization in Simple Linear Regression", "comments": "15 pages including two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general method of minimization using correlation coefficients and order\nstatistics is evaluated relative to least squares procedures in the estimation\nof parameters for normal data in simple linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 21:46:02 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Gideon", "Rudy", ""]]}, {"id": "1802.02691", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams, Curtis B Storlie, Terry M Therneau, Clifford R\n  Jack Jr, Jan Hannig", "title": "A Bayesian Approach to Multi-State Hidden Markov Models: Application to\n  Dementia Progression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are living longer than ever before, and with this arises new\ncomplications and challenges for humanity. Among the most pressing of these\nchallenges is of understanding the role of aging in the development of\ndementia. This paper is motivated by the Mayo Clinic Study of Aging data for\n4742 subjects since 2004, and how it can be used to draw inference on the role\nof aging in the development of dementia. We construct a hidden Markov model\n(HMM) to represent progression of dementia from states associated with the\nbuildup of amyloid plaque in the brain, and the loss of cortical thickness. A\nhierarchical Bayesian approach is taken to estimate the parameters of the HMM\nwith a truly time-inhomogeneous infinitesimal generator matrix, and response\nfunctions of the continuous-valued biomarker measurements are cut-point\nagnostic. A Bayesian approach with these features could be useful in many\ndisease progression models. Additionally, an approach is illustrated for\ncorrecting a common bias in delayed enrollment studies, in which some or all\nsubjects are not observed at baseline. Standard software is incapable of\naccounting for this critical feature, so code to perform the estimation of the\nmodel described below is made available online.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 02:14:50 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 16:06:38 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Williams", "Jonathan P", ""], ["Storlie", "Curtis B", ""], ["Therneau", "Terry M", ""], ["Jack", "Clifford R", "Jr"], ["Hannig", "Jan", ""]]}, {"id": "1802.02698", "submitter": "HaiYing Wang", "authors": "HaiYing Wang", "title": "More Efficient Estimation for Logistic Regression with Optimal Subsample", "comments": null, "journal-ref": "Journal of Machine Learning Research, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose improved estimation method for logistic regression\nbased on subsamples taken according the optimal subsampling probabilities\ndeveloped in Wang et al. 2018 Both asymptotic results and numerical results\nshow that the new estimator has a higher estimation efficiency. We also develop\na new algorithm based on Poisson subsampling, which does not require to\napproximate the optimal subsampling probabilities all at once. This is\ncomputationally advantageous when available random-access memory is not enough\nto hold the full data. Interestingly, asymptotic distributions also show that\nPoisson subsampling produces a more efficient estimator if the sampling rate,\nthe ratio of the subsample size to the full data sample size, does not converge\nto zero. We also obtain the unconditional asymptotic distribution for the\nestimator based on Poisson subsampling. The proposed approach requires to use a\npilot estimator to correct biases of un-weighted estimators. We further show\nthat even if the pilot estimator is inconsistent, the resulting estimators are\nstill consistent and asymptotically normal if the model is correctly specified.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 03:14:31 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 19:49:55 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 18:12:59 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 17:26:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Wang", "HaiYing", ""]]}, {"id": "1802.02821", "submitter": "Karla DiazOrdaz", "authors": "Karla DiazOrdaz, Rhian Daniel, Noemi Kreif", "title": "Data-adaptive doubly robust instrumental variable methods for treatment\n  effect heterogeneity", "comments": "24 pages, 3 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the average treatment effect in the treated as\na function of baseline covariates, where there is a valid (conditional)\ninstrument.\n  We describe two doubly robust (DR) estimators: a locally efficient\ng-estimator, and a targeted minimum loss-based estimator (TMLE). These two DR\nestimators can be viewed as generalisations of the two-stage least squares\n(TSLS) method to semi-parametric models that make weaker assumptions. We\nexploit recent theoretical results that extend to the g-estimator the use of\ndata-adaptive fits for the nuisance parameters.\n  A simulation study is used to compare standard TSLS with the two DR\nestimators' finite-sample performance, (1) when fitted using parametric\nnuisance models, and (2) using data-adaptive nuisance fits, obtained from the\nSuper Learner, an ensemble machine learning method.\n  Data-adaptive DR estimators have lower bias and improved coverage, when\ncompared to incorrectly specified parametric DR estimators and TSLS. When the\nparametric model for the treatment effect curve is correctly specified, the\ng-estimator outperforms all others, but when this model is misspecified, TMLE\nperforms best, while TSLS can result in large biases and zero coverage.\n  Finally, we illustrate the methods by reanalysing the COPERS (COping with\npersistent Pain, Effectiveness Research in Self-management) trial to make\ninference about the causal effect of treatment actually received, and the\nextent to which this is modified by depression at baseline.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 12:32:14 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 12:33:17 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["DiazOrdaz", "Karla", ""], ["Daniel", "Rhian", ""], ["Kreif", "Noemi", ""]]}, {"id": "1802.02825", "submitter": "Constandina Koki", "authors": "Constandina Koki, Loukia Meligkotsidou, Ioannis Vrontos", "title": "Forecasting under model uncertainty:Non-homogeneous hidden Markov models\n  with Polya-Gamma data augmentation", "comments": "36 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two-state Non-Homogeneous Hidden Markov Models (NHHMMs) for\nforecasting univariate time series. Given a set of predictors, the time series\nare modeled via predictive regressions with state dependent coefficients and\ntime-varying transition probabilities that depend on the predictors via a\nlogistic function. In a hidden Markov setting, inference for logistic\nregression coefficients becomes complicated and in some cases impossible due to\nconvergence issues. In this paper, we aim to address this problem using a new\nlatent variable scheme that utilizes the P\\'{o}lya-Gamma class of\ndistributions. We allow for model uncertainty regarding the predictors that\naffect the series both linearly -- in the mean -- and non-linearly -- in the\ntransition matrix. Predictor selection and inference on the model parameters\nare based on a MCMC scheme with reversible jump steps. Single-step and\nmultiple-steps-ahead predictions are obtained by the most probable model,\nmedian probability model or a Bayesian Model Averaging approach. Using\nsimulation experiments, we illustrate the performance of our algorithm in\nvarious setups, in terms of mixing properties, model selection and predictive\nability. An empirical study on realized volatility data shows that our\nmethodology gives improved forecasts compared to benchmark models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 12:35:13 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 08:10:01 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 13:56:47 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Koki", "Constandina", ""], ["Meligkotsidou", "Loukia", ""], ["Vrontos", "Ioannis", ""]]}, {"id": "1802.02920", "submitter": "Anru Zhang", "authors": "Anru Zhang and Mengdi Wang", "title": "Spectral State Compression of Markov Processes", "comments": "to appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model reduction of Markov processes is a basic problem in modeling\nstate-transition systems. Motivated by the state aggregation approach rooted in\ncontrol theory, we study the statistical state compression of a discrete-state\nMarkov chain from empirical trajectories. Through the lens of spectral\ndecomposition, we study the rank and features of Markov processes, as well as\nproperties like representability, aggregability, and lumpability. We develop\nspectral methods for estimating the transition matrix of a low-rank Markov\nmodel, estimating the leading subspace spanned by Markov features, and\nrecovering latent structures like state aggregation and lumpable partition of\nthe state space. We prove statistical upper bounds for the estimation errors\nand nearly matching minimax lower bounds. Numerical studies are performed on\nsynthetic data and a dataset of New York City taxi trips.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:28:46 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 03:34:14 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2019 03:08:25 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhang", "Anru", ""], ["Wang", "Mengdi", ""]]}, {"id": "1802.02928", "submitter": "Andrey Gorshenin", "authors": "V.Yu. Korolev, A.K. Gorshenin, K.P.Belyaev", "title": "Statistical tests for extreme precipitation volumes", "comments": "21 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approaches, based on the negative binomial model for the distribution of\nduration of the wet periods measured in days, are proposed to the definition of\nextreme precipitation. This model demonstrates excellent fit with real data and\nprovides a theoretical base for the determination of asymptotic approximations\nto the distributions of the maximum daily precipitation volume within a wet\nperiod as well as the total precipitation volume over a wet period. The first\napproach to the definition (and determination) of extreme precipitation is\nbased on the tempered Snedecor-Fisher distribution of the maximum daily\nprecipitation. According to this approach, a daily precipitation volume is\nconsidered to be extreme, if it exceeds a certain (pre-defined) quantile of the\ntempered Snedecor--Fisher distribution. The second approach is based on that\nthe total precipitation volume for a wet period has the gamma distribution.\nHence, the hypothesis that the total precipitation volume during a certain wet\nperiod is extremely large can be formulated as the homogeneity hypothesis of a\nsample from the gamma distribution. Two equivalent tests are proposed for\ntesting this hypothesis. Both of these tests deal with the relative\ncontribution of the total precipitation volume for a wet period to the\nconsidered set (sample) of successive wet periods. Within the second approach\nit is possible to introduce the notions of relatively and absolutely extreme\nprecipitation volumes. The results of the application of these tests to real\ndata are presented yielding the conclusion that the intensity of wet periods\nwith extreme large precipitation volume increases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 15:10:50 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 14:00:54 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:20:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Korolev", "V. Yu.", ""], ["Gorshenin", "A. K.", ""], ["Belyaev", "K. P.", ""]]}, {"id": "1802.03127", "submitter": "Takayuki Kawashima", "authors": "Takayuki Kawashima and Hironori Fujisawa", "title": "Robust and Sparse Regression in GLM by Stochastic Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized linear model (GLM) plays a key role in regression analyses.\nIn high-dimensional data, the sparse GLM has been used but it is not robust\nagainst outliers. Recently, the robust methods have been proposed for the\nspecific example of the sparse GLM. Among them, we focus on the robust and\nsparse linear regression based on the $\\gamma$-divergence. The estimator of the\n$\\gamma$-divergence has strong robustness under heavy contamination. In this\npaper, we extend the robust and sparse linear regression based on the\n$\\gamma$-divergence to the robust and sparse GLM based on the\n$\\gamma$-divergence with a stochastic optimization approach in order to obtain\nthe estimate. We adopt the randomized stochastic projected gradient descent as\na stochastic optimization approach and extend the established convergence\nproperty to the classical first-order necessary condition. By virtue of the\nstochastic optimization approach, we can efficiently estimate parameters for\nvery large problems. Particularly, we show the linear regression, logistic\nregression and Poisson regression with $L_1$ regularization in detail as\nspecific examples of robust and sparse GLM. In numerical experiments and real\ndata analysis, the proposed method outperformed comparative methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 04:51:50 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Kawashima", "Takayuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1802.03141", "submitter": "Chi Kuen Wong", "authors": "Chi Kuen Wong, Enes Makalic, Daniel F. Schmidt", "title": "A Minimum Message Length Criterion for Robust Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies the minimum message length principle to inference of\nlinear regression models with Student-t errors. A new criterion for variable\nselection and parameter estimation in Student-t regression is proposed. By\nexploiting properties of the regression model, we derive a suitable\nnon-informative proper uniform prior distribution for the regression\ncoefficients that leads to a simple and easy-to-apply criterion. Our proposed\ncriterion does not require specification of hyperparameters and is invariant\nunder both full rank transformations of the design matrix and linear\ntransformations of the outcomes. We compare the proposed criterion with several\nstandard model selection criteria, such as the Akaike information criterion and\nthe Bayesian information criterion, on simulations and real data with promising\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 06:23:19 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 04:58:47 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Wong", "Chi Kuen", ""], ["Makalic", "Enes", ""], ["Schmidt", "Daniel F.", ""]]}, {"id": "1802.03395", "submitter": "Rosario N. Mantegna", "authors": "Federico Musciotto, Luca Marotta, Salvatore Miccich\\`e and Rosario N.\n  Mantegna", "title": "Bootstrap validation of links of a minimum spanning tree", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.physa.2018.08.020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two different bootstrap methods applied to the detection of a\nminimum spanning tree obtained from a set of multivariate variables. We show\nthat two different bootstrap procedures provide partly distinct information\nthat can be highly informative about the investigated complex system. Our case\nstudy, based on the investigation of daily returns of a portfolio of stocks\ntraded in the US equity markets, shows the degree of robustness and\ncompleteness of the information extracted with popular information filtering\nmethods such as the minimum spanning tree and the planar maximally filtered\ngraph. The first method performs a \"row bootstrap\" whereas the second method\nperforms a \"pair bootstrap\". We show that the parallel use of the two methods\nis suggested especially for complex systems presenting both a nested\nhierarchical organization together with the presence of global feedback\nchannels.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:48:37 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Musciotto", "Federico", ""], ["Marotta", "Luca", ""], ["Miccich\u00e8", "Salvatore", ""], ["Mantegna", "Rosario N.", ""]]}, {"id": "1802.03479", "submitter": "Tingran Gao", "authors": "Tingran Gao, Shahar Z. Kovalsky, Ingrid Daubechies", "title": "Gaussian Process Landmarking on Manifolds", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a means of improving analysis of biological shapes, we propose an\nalgorithm for sampling a Riemannian manifold by sequentially selecting points\nwith maximum uncertainty under a Gaussian process model. This greedy strategy\nis known to be near-optimal in the experimental design literature, and appears\nto outperform the use of user-placed landmarks in representing the geometry of\nbiological objects in our application. In the noiseless regime, we establish an\nupper bound for the mean squared prediction error (MSPE) in terms of the number\nof samples and geometric quantities of the manifold, demonstrating that the\nMSPE for our proposed sequential design decays at a rate comparable to the\noracle rate achievable by any sequential or non-sequential optimal design; to\nour knowledge this is the first result of this type for sequential experimental\ndesign. The key is to link the greedy algorithm to reduced basis methods in the\ncontext of model reduction for partial differential equations. We expect this\napproach will find additional applications in other fields of research.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:50:10 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 16:15:31 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 05:28:25 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 20:07:46 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Gao", "Tingran", ""], ["Kovalsky", "Shahar Z.", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1802.03490", "submitter": "Korbinian Strimmer", "authors": "Takoua Jendoubi and Korbinian Strimmer", "title": "A whitening approach to probabilistic canonical correlation analysis for\n  omics data integration", "comments": "25 pages, 10 figures", "journal-ref": "BMC Bioinformatics 20:15 (2019)", "doi": "10.1186/s12859-018-2572-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Canonical correlation analysis (CCA) is a classic statistical\ntool for investigating complex multivariate data. Correspondingly, it has found\nmany diverse applications, ranging from molecular biology and medicine to\nsocial science and finance. Intriguingly, despite the importance and\npervasiveness of CCA, only recently a probabilistic understanding of CCA is\ndeveloping, moving from an algorithmic to a model-based perspective and\nenabling its application to large-scale settings.\n  Results: Here, we revisit CCA from the perspective of statistical whitening\nof random variables and propose a simple yet flexible probabilistic model for\nCCA in the form of a two-layer latent variable generative model. The advantages\nof this variant of probabilistic CCA include non-ambiguity of the latent\nvariables, provisions for negative canonical correlations, possibility of\nnon-normal generative variables, as well as ease of interpretation on all\nlevels of the model. In addition, we show that it lends itself to\ncomputationally efficient estimation in high-dimensional settings using\nregularized inference. We test our approach to CCA analysis in simulations and\napply it to two omics data sets illustrating the integration of gene expression\ndata, lipid concentrations and methylation levels.\n  Conclusions: Our whitening approach to CCA provides a unifying perspective on\nCCA, linking together sphering procedures, multivariate regression and\ncorresponding probabilistic generative models. Furthermore, we offer an\nefficient computer implementation in the \"whitening\" R package available at\nhttps://CRAN.R-project.org/package=whitening .\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 01:08:07 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 23:09:14 GMT"}, {"version": "v3", "created": "Wed, 21 Nov 2018 22:31:52 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 18:27:38 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Jendoubi", "Takoua", ""], ["Strimmer", "Korbinian", ""]]}, {"id": "1802.03511", "submitter": "Ritwik Mitra", "authors": "Priyam Mitra, Heng Lian, Ritwik Mitra, Hua Liang and Min-ge Xie", "title": "A General Framework For Frequentist Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection strategies have been routinely employed to determine a model\nfor data analysis in statistics, and further study and inference then often\nproceed as though the selected model were the true model that were known a\npriori. This practice does not account for the uncertainty introduced by the\nselection process and the fact that the selected model can possibly be a wrong\none. Model averaging approaches try to remedy this issue by combining\nestimators for a set of candidate models. Specifically, instead of deciding\nwhich model is the 'right' one, a model averaging approach suggests to fit a\nset of candidate models and average over the estimators using certain data\nadaptive weights. In this paper we establish a general frequentist model\naveraging framework that does not set any restrictions on the set of candidate\nmodels. It greatly broadens the scope of the existing methodologies under the\nfrequentist model averaging development. Assuming the data is from an unknown\nmodel, we derive the model averaging estimator and study its limiting\ndistributions and related predictions while taking possible modeling biases\ninto account. We propose a set of optimal weights to combine the individual\nestimators so that the expected mean squared error of the average estimator is\nminimized. Simulation studies are conducted to compare the performance of the\nestimator with that of the existing methods. The results show the benefits of\nthe proposed approach over traditional model selection approaches as well as\nexisting model averaging methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 03:25:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Mitra", "Priyam", ""], ["Lian", "Heng", ""], ["Mitra", "Ritwik", ""], ["Liang", "Hua", ""], ["Xie", "Min-ge", ""]]}, {"id": "1802.03627", "submitter": "Hazem Toutounji", "authors": "Hazem Toutounji (1 and 2) and Daniel Durstewitz (1 and 3) ((1)\n  Department of Theoretical Neuroscience, Bernstein Center for Computational\n  Neuroscience, Central Institute of Mental Health, Medical Faculty Mannheim,\n  Heidelberg University, Mannheim, Germany, (2) Institute of Neuroinformatics,\n  University of Zurich and ETH Zurich, Zurich, Switzerland, (3) Faculty of\n  Physics and Astronomy, Heidelberg University, Heidelberg, Germany)", "title": "Detecting Multiple Change Points Using Adaptive Regression Splines with\n  Application to Neural Recordings", "comments": "35 pages, 9 figures, 2 tables, 3 algorithms", "journal-ref": null, "doi": "10.3389/fninf.2018.00067", "report-no": null, "categories": "stat.ME q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series, as frequently the case in neuroscience, are rarely stationary,\nbut often exhibit abrupt changes due to attractor transitions or bifurcations\nin the dynamical systems producing them. A plethora of methods for detecting\nsuch change points in time series statistics have been developed over the\nyears, in addition to test criteria to evaluate their significance. Issues to\nconsider when developing change point analysis methods include computational\ndemands, difficulties arising from either limited amount of data or a large\nnumber of covariates, and arriving at statistical tests with sufficient power\nto detect as many changes as contained in potentially high-dimensional time\nseries. Here, a general method called Paired Adaptive Regressors for Cumulative\nSum is developed for detecting multiple change points in the mean of\nmultivariate time series. The method's advantages over alternative approaches\nare demonstrated through a series of simulation experiments. This is followed\nby a real data application to neural recordings from rat medial prefrontal\ncortex during learning. Finally, the method's flexibility to incorporate useful\nfeatures from state-of-the-art change point detection techniques is discussed,\nalong with potential drawbacks and suggestions to remedy them.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 17:57:25 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 07:43:56 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 20:00:05 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Toutounji", "Hazem", "", "1 and 2"], ["Durstewitz", "Daniel", "", "1 and 3"]]}, {"id": "1802.03708", "submitter": "Yubo Tao", "authors": "Li Guo, Wolfgang Karl H\\\"ardle, Yubo Tao", "title": "A Time-Varying Network for Cryptocurrencies", "comments": "43 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM q-fin.PM q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies return cross-predictability and technological similarity\nyield information on risk propagation and market segmentation. To investigate\nthese effects, we build a time-varying network for cryptocurrencies, based on\nthe evolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can\nachieve better risk diversification by investing in cryptocurrencies from\ndifferent communities. A cross-sectional portfolio that implements an\ninter-crypto momentum trading strategy earns a 1.08% daily return. By\ndissecting the portfolio returns on behavioral factors, we confirm that our\nresults are not driven by behavioral mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 08:32:31 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 05:42:37 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 15:40:22 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 12:44:59 GMT"}, {"version": "v5", "created": "Fri, 20 Sep 2019 16:04:37 GMT"}, {"version": "v6", "created": "Sat, 13 Jun 2020 05:09:17 GMT"}, {"version": "v7", "created": "Wed, 21 Jul 2021 12:57:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Guo", "Li", ""], ["H\u00e4rdle", "Wolfgang Karl", ""], ["Tao", "Yubo", ""]]}, {"id": "1802.03778", "submitter": "Michelle Norris", "authors": "Michelle Norris", "title": "Sample Design for Audit Populations", "comments": "44 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several tools for the determination of sample size and design for\nMediCal audits. This audit setting involves a population of claims for\nreimbursement by a healthcare provider which need to be reviewed by an auditor\nto determine the correct amount for each claim. The existing literature\nregarding sample planning for audits is incomplete and often includes\nrestrictive assumptions. To fill these gaps, we exploit the special\nrelationship between the known claim amounts and the unknown post-audit\namounts. We propose a hypergeometric generative process for audit populations\nwhich we use to derive estimators of variances needed for sample size\ndetermination. We further develop a criterion for choosing between simple\nexpansion and ratio estimation and an efficient method for determining exact\noptimal strata breakpoints in populations with repeated values. We also derive\na variance estimator under a more general \"partial error\" model than previous\nresearchers have used. These tools apply more generally to audits where an\noverstated book/claim amount is the primary concern and estimation of the total\ndollar value of the claim errors is the goal. The sample design methods we\ndevelop are illustrated on two simulated audit populations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 17:43:04 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Norris", "Michelle", ""]]}, {"id": "1802.03840", "submitter": "Casey Kneale", "authors": "Casey Kneale, Steven D. Brown", "title": "Uncharted Forest a Technique for Exploratory Data Analysis", "comments": null, "journal-ref": "Talanta. 189, (2018), 71-78", "doi": "10.1016/j.talanta.2018.06.061", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory data analysis is crucial for developing and understanding\nclassification models from high-dimensional datasets. We explore the utility of\na new unsupervised tree ensemble called uncharted forest for visualizing class\nassociations, sample-sample associations, class heterogeneity, and\nuninformative classes for provenance studies. The uncharted forest algorithm\ncan be used to partition data using random selections of variables and metrics\nbased on statistical spread. After each tree is grown, a tally of the samples\nthat arrive at every terminal node is maintained. Those tallies are stored in\nsingle sample association matrix and a likelihood measure for each sample being\npartitioned with one another can be made. That matrix may be readily viewed as\na heat map, and the probabilities can be quantified via new metrics that\naccount for class or cluster membership. We display the advantages and\nlimitations of using this technique by applying it to two classification\ndatasets and three provenance study datasets. Two of the metrics presented in\nthis paper are also compared with widely used metrics from two algorithms that\nhave variance-based clustering mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 23:22:36 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 22:21:49 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 12:27:25 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kneale", "Casey", ""], ["Brown", "Steven D.", ""]]}, {"id": "1802.03884", "submitter": "Fangyuan Zhang", "authors": "Hossein Mansouri and Fangyuan Zhang", "title": "Simultaneous Rank Tests in Analysis of Covariance Based on Pairwise\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests provide robust and powerful alternatives to the\ncorresponding least squares methods. There are two approaches to nonparametric\npairwise comparisons of treatment effects, the method based on pairwise\nrankings and the method based on overall ranking. The former is generally\nrecommended in the literature because of its strong control of familywise error\nrate. However, this method is developed only for one-way layouts and randomized\ncomplete blocks. By combining the method of aligned ranks and pairwise ranking,\nwe extend the Steel-Dwass pairwise comparisons to the analysis of covariance\nand factorial models for both one-sided and two-sided comparisons as well as\ntesting for treatment versus control. Unlike the traditional two-sample\nstandardization of test statistics, we propose a weighted estimate of the scale\nparameter for ranks and show through simulation that it has superior small\nsample performance by controlling the familywise error rate at nominal level.\nThis method provides an improvement for large sample approximation of\nSteel-Dwass method for one-way layouts. The marginal and joint asymptotic\ndistributions are derived and power comparisons are made with the method of\naligned rank transformation and the least squares method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:16:01 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Mansouri", "Hossein", ""], ["Zhang", "Fangyuan", ""]]}, {"id": "1802.03945", "submitter": "Yuma Uehara", "authors": "Hiroki Masuda and Yuma Uehara", "title": "Estimating Diffusion With Compound Poisson Jumps Based On\n  Self-normalized Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric estimation of the continuous part of a class of\nergodic diffusions with jumps based on high-frequency samples. Various papers\npreviously proposed threshold based methods, which enable us to distinguish\nwhether observed increments have jumps or not at each small-time interval,\nhence to estimate the unknown parameters separately. However, a data-adapted\nand quantitative choice of the threshold parameter is known to be a subtle and\nsensitive problem. In this paper, we present a simple alternative based on the\nJarque-Bera normality test for the Euler residuals. Different from the\nthreshold based method, the proposed method does not require any sensitive fine\ntuning, hence is of practical value. It is shown that under suitable conditions\nthe proposed estimator is asymptotically equivalent to an estimator constructed\nby the unobserved fluctuation of the continuous part of the solution process,\nhence is asymptotically efficient. Some numerical experiments are conducted to\nobserve finite-sample performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 09:14:33 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 09:12:47 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Masuda", "Hiroki", ""], ["Uehara", "Yuma", ""]]}, {"id": "1802.03967", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer", "title": "Self-exciting Point Processes: Infections and Implementations", "comments": "comment on arXiv:1708.02647v1, submitted to Statistical Science, 4\n  pages", "journal-ref": "Statistical Science (2018); 33(3):327-329", "doi": "10.1214/18-STS653", "report-no": null, "categories": "stat.ME physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on Reinhart's \"Review of Self-Exciting Spatio-Temporal\nPoint Processes and Their Applications\" (arXiv:1708.02647v1). I contribute some\nexperiences from modelling the spread of infectious diseases. Furthermore, I\ntry to complement the review with regard to the availability of software for\nthe described models, which I think is essential in \"paving the way for new\nuses\".\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 10:47:22 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Meyer", "Sebastian", ""]]}, {"id": "1802.04050", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu, Lingsong Zhang, and Chuanhai Liu", "title": "Exact and efficient inference for Partial Bayes problems", "comments": null, "journal-ref": "Electron. J. Statist., Volume 12, Number 2 (2018), 4640-4668", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are useful for statistical inference. However, real-world\nproblems can be challenging using Bayesian methods when the data analyst has\nonly limited prior knowledge. In this paper we consider a class of problems,\ncalled Partial Bayes problems, in which the prior information is only partially\navailable. Taking the recently proposed Inferential Model approach, we develop\na general inference framework for Partial Bayes problems, and derive both exact\nand efficient solutions. In addition to the theoretical investigation,\nnumerical results and real applications are used to demonstrate the superior\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:07:56 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Qiu", "Yixuan", ""], ["Zhang", "Lingsong", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1802.04321", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A. Vsevolozhskaya, Fengjiao Hu, Dmitri V. Zaykin", "title": "Detecting weak signals by combining small P-values in genetic\n  association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach the problem of combining top-ranking association statistics or\nP-value from a new perspective which leads to a remarkably simple and powerful\nmethod. Statistical methods, such as the Rank Truncated Product (RTP), have\nbeen developed for combining top-ranking associations and this general strategy\nproved to be useful in applications for detecting combined effects of multiple\ndisease components. To increase power, these methods aggregate signals across\ntop ranking SNPs, while adjusting for their total number assessed in a study.\nAnalytic expressions for combined top statistics or P-values tend to be\nunwieldy, which complicates interpretation, practical implementation, and\nhinders further developments. Here, we propose the Augmented Rank Truncation\n(ART) method that retains main characteristics of the RTP but is substantially\nsimpler to implement. ART leads to an efficient form of the adaptive algorithm,\nan approach where the number of top ranking SNPs is varied to optimize power.\nWe illustrate our methods by strengthening previously reported associations of\n$\\mu$-opioid receptor variants with sensitivity to pain.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:36:56 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 19:40:23 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 20:24:48 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Vsevolozhskaya", "Olga A.", ""], ["Hu", "Fengjiao", ""], ["Zaykin", "Dmitri V.", ""]]}, {"id": "1802.04380", "submitter": "Miklos Csorgo", "authors": "Mikl\\'os Cs\\\"org\\H{o}", "title": "Randomized Empirical Processes and Confidence Bands via Virtual\n  Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X,X_1,X_2,\\cdots$ be independent real valued random variables with a\ncommon distribution function $F$, and consider $\\{X_1,\\cdots,X_N \\}$, possibly\na big concrete data set, or an imaginary random sample of size $N\\geq 1$ on\n$X$. In the latter case, or when a concrete data set in hand is too big to be\nentirely processed, then the sample distribution function $F_N$ and the the\npopulation distribution function $F$ are both to be estimated. This, in this\npaper, is achieved via viewing $\\{X_1,\\cdots,X_N \\}$ as above, as a finite\npopulation of real valued random variables with $N$ labeled units, and sampling\nits indices $\\{1,\\cdots,N \\}$ with replacement $m_N:= \\sum_{i=1}^N w_{i}^{(N)}$\ntimes so that for each $1\\leq i \\leq N$, $w_{i}^{(N)}$ is the count of number\nof times the index $i$ of $X_i$ is chosen in this virtual resampling process.\nThis exposition extends the Doob-Donsker classical theory of weak convergence\nof empirical processes to that of the thus created randomly weighted empirical\nprocesses when $N, m_N \\rightarrow \\infty$ so that $m_N=o(N^2)$.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 22:27:54 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Cs\u00f6rg\u0151", "Mikl\u00f3s", ""]]}, {"id": "1802.04589", "submitter": "Michael Schomaker", "authors": "Michael Schomaker, Christian Heumann", "title": "When and when not to use optimal model averaging", "comments": null, "journal-ref": null, "doi": "10.1007/s00362-018-1048-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally model averaging has been viewed as an alternative to model\nselection with the ultimate goal to incorporate the uncertainty associated with\nthe model selection process in standard errors and confidence intervals by\nusing a weighted combination of candidate models. In recent years, a new class\nof model averaging estimators has emerged in the literature, suggesting to\ncombine models such that the squared risk, or other risk functions, are\nminimized. We argue that, contrary to popular belief, these estimators do not\nnecessarily address the challenges induced by model selection uncertainty, but\nshould be regarded as attractive complements for the machine learning and\nforecasting literature, as well as tools to identify causal parameters. We\nillustrate our point by means of several targeted simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 12:35:15 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 10:42:42 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 10:56:46 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schomaker", "Michael", ""], ["Heumann", "Christian", ""]]}, {"id": "1802.04826", "submitter": "Pierre-Alexandre Mattei", "authors": "Pierre-Alexandre Mattei and Jes Frellsen", "title": "Leveraging the Exact Likelihood of Deep Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep latent variable models (DLVMs) combine the approximation abilities of\ndeep neural networks and the statistical foundations of generative models.\nVariational methods are commonly used for inference; however, the exact\nlikelihood of these models has been largely overlooked. The purpose of this\nwork is to study the general properties of this quantity and to show how they\ncan be leveraged in practice. We focus on important inferential problems that\nrely on the likelihood: estimation and missing data imputation. First, we\ninvestigate maximum likelihood estimation for DLVMs: in particular, we show\nthat most unconstrained models used for continuous data have an unbounded\nlikelihood function. This problematic behaviour is demonstrated to be a source\nof mode collapse. We also show how to ensure the existence of maximum\nlikelihood estimates, and draw useful connections with nonparametric mixture\nmodels. Finally, we describe an algorithm for missing data imputation using the\nexact conditional likelihood of a deep latent variable model. On several data\nsets, our algorithm consistently and significantly outperforms the usual\nimputation scheme used for DLVMs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 19:27:38 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:44:46 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 14:57:29 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 13:44:24 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Mattei", "Pierre-Alexandre", ""], ["Frellsen", "Jes", ""]]}, {"id": "1802.04849", "submitter": "Michael Gallaugher Ph.D.", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Clustering and Semi-Supervised Classification for Clickstream Data via\n  Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have been used for unsupervised learning for some time,\nand their use within the semi-supervised paradigm is becoming more commonplace.\nClickstream data is one of the various emerging data types that demands\nparticular attention because there is a notable paucity of statistical learning\napproaches currently available. A mixture of first-order continuous time Markov\nmodels is introduced for unsupervised and semi-supervised learning of\nclickstream data. This approach assumes continuous time, which distinguishes it\nfrom existing mixture model-based approaches; practically, this allows account\nto be taken of the amount of time each user spends on each webpage. The\napproach is evaluated, and compared to the discrete time approach, using\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 20:43:04 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 08:28:57 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1802.04876", "submitter": "Weijie J. Su", "authors": "Weijie J. Su and Yuancheng Zhu", "title": "Uncertainty Quantification for Online Learning and Stochastic\n  Approximation via Hierarchical Incremental Gradient Descent", "comments": "Changed the title and polished writing. For more details, please\n  visit the HiGrad webpage http://stat.wharton.upenn.edu/~suw/higrad", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is an immensely popular approach for online\nlearning in settings where data arrives in a stream or data sizes are very\nlarge. However, despite an ever- increasing volume of work on SGD, much less is\nknown about the statistical inferential properties of SGD-based predictions.\nTaking a fully inferential viewpoint, this paper introduces a novel procedure\ntermed HiGrad to conduct statistical inference for online learning, without\nincurring additional computational cost compared with SGD. The HiGrad procedure\nbegins by performing SGD updates for a while and then splits the single thread\ninto several threads, and this procedure hierarchically operates in this\nfashion along each thread. With predictions provided by multiple threads in\nplace, a t-based confidence interval is constructed by decorrelating\npredictions using covariance structures given by a Donsker-style extension of\nthe Ruppert--Polyak averaging scheme, which is a technical contribution of\nindependent interest. Under certain regularity conditions, the HiGrad\nconfidence interval is shown to attain asymptotically exact coverage\nprobability. Finally, the performance of HiGrad is evaluated through extensive\nsimulation studies and a real data example. An R package higrad has been\ndeveloped to implement the method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:15:10 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 15:16:31 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Su", "Weijie J.", ""], ["Zhu", "Yuancheng", ""]]}, {"id": "1802.04885", "submitter": "Jose Blanchet", "authors": "Jose Blanchet, Lin Chen, and Xun Yu Zhou", "title": "Distributionally Robust Mean-Variance Portfolio Selection with\n  Wasserstein Distances", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Markowitz's mean-variance portfolio selection model by considering\na distributionally robust version, where the region of distributional\nuncertainty is around the empirical measure and the discrepancy between\nprobability measures is dictated by the so-called Wasserstein distance. We\nreduce this problem into an empirical variance minimization problem with an\nadditional regularization term. Moreover, we extend recent inference\nmethodology in order to select the size of the distributional uncertainty as\nwell as the associated robust target return rate in a data-driven way.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:49:10 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Blanchet", "Jose", ""], ["Chen", "Lin", ""], ["Zhou", "Xun Yu", ""]]}, {"id": "1802.04906", "submitter": "Subhabrata Majumdar", "authors": "Abhik Ghosh, Subhabrata Majumdar", "title": "Ultrahigh-dimensional Robust and Efficient Sparse Regression using\n  Non-Concave Penalized Density Power Divergence", "comments": "Accepted in IEEE Transactions on Information Theory", "journal-ref": "IEEE Transactions on Information Theory 66 (12), 7812-7827, 2020", "doi": "10.1109/TIT.2020.3013015", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse regression method based on the non-concave penalized\ndensity power divergence loss function which is robust against infinitesimal\ncontamination in very high dimensionality. Present methods of sparse and robust\nregression are based on $\\ell_1$-penalization, and their theoretical properties\nare not well-investigated. In contrast, we use a general class of folded\nconcave penalties that ensure sparse recovery and consistent estimation of\nregression coefficients. We propose an alternating algorithm based on the\nConcave-Convex procedure to obtain our estimate, and demonstrate its robustness\nproperties using influence function analysis. Under some conditions on the\nfixed design matrix and penalty function, we prove that this estimator\npossesses large-sample oracle properties in an ultrahigh-dimensional regime.\nThe performance and effectiveness of our proposed method for parameter\nestimation and prediction compared to state-of-the-art are demonstrated through\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 00:35:38 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 20:41:16 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 22:39:02 GMT"}, {"version": "v4", "created": "Sun, 12 Apr 2020 23:47:00 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 15:58:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ghosh", "Abhik", ""], ["Majumdar", "Subhabrata", ""]]}, {"id": "1802.05005", "submitter": "Michael Schomaker", "authors": "Michael Schomaker, Miguel Angel Luque-Fernandez, Valeriane Leroy,\n  Mary-Ann Davies", "title": "Using Longitudinal Targeted Maximum Likelihood Estimation in Complex\n  Settings with Dynamic Interventions", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8340", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal targeted maximum likelihood estimation (LTMLE) has very rarely\nbeen used to estimate dynamic treatment effects in the context of\ntime-dependent confounding affected by prior treatment when faced with long\nfollow-up times, multiple time-varying confounders, and complex associational\nrelationships simultaneously. Reasons for this include the potential\ncomputational burden, technical challenges, restricted modeling options for\nlong follow-up times, and limited practical guidance in the literature.\nHowever, LTMLE has desirable asymptotic properties, i.e. it is doubly robust,\nand can yield valid inference when used in conjunction with machine learning.\nWe use a topical and sophisticated question from HIV treatment research to show\nthat LTMLE can be used successfully in complex realistic settings and compare\nresults to competing estimators. Our example illustrates the following\npractical challenges common to many epidemiological studies 1) long follow-up\ntime (30 months), 2) gradually declining sample size 3) limited support for\nsome intervention rules of interest 4) a high-dimensional set of potential\nadjustment variables, increasing both the need and the challenge of integrating\nappropriate machine learning methods 5) consideration of collider bias. Our\nanalyses, as well as simulations, shed new light on the application of LTMLE in\ncomplex and realistic settings: we show that (i) LTMLE can yield stable and\ngood estimates, even when confronted with small samples and limited modeling\noptions; (ii) machine learning utilized with a small set of simple learners (if\nmore complex ones can't be fitted) can outperform a single, complex model,\nwhich is tailored to incorporate prior clinical knowledge; (iii) performance\ncan vary considerably depending on interventions and their support in the data,\nand therefore critical quality checks should accompany every LTMLE analysis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 09:41:59 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 15:02:35 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 08:45:06 GMT"}, {"version": "v4", "created": "Sat, 12 Jan 2019 14:05:12 GMT"}, {"version": "v5", "created": "Thu, 21 Mar 2019 13:21:52 GMT"}, {"version": "v6", "created": "Mon, 15 Apr 2019 12:06:18 GMT"}, {"version": "v7", "created": "Fri, 19 Jul 2019 10:14:38 GMT"}, {"version": "v8", "created": "Thu, 17 Oct 2019 12:11:09 GMT"}, {"version": "v9", "created": "Thu, 4 Mar 2021 11:00:49 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schomaker", "Michael", ""], ["Luque-Fernandez", "Miguel Angel", ""], ["Leroy", "Valeriane", ""], ["Davies", "Mary-Ann", ""]]}, {"id": "1802.05046", "submitter": "Yishai Shimoni", "authors": "Yishai Shimoni, Chen Yanover, Ehud Karavani and Yaara Goldschmnidt", "title": "Benchmarking Framework for Performance-Evaluation of Causal Inference\n  Analysis", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Causal inference analysis is the estimation of the effects of actions on\noutcomes. In the context of healthcare data this means estimating the outcome\nof counter-factual treatments (i.e. including treatments that were not\nobserved) on a patient's outcome. Compared to classic machine learning methods,\nevaluation and validation of causal inference analysis is more challenging\nbecause ground truth data of counter-factual outcome can never be obtained in\nany real-world scenario. Here, we present a comprehensive framework for\nbenchmarking algorithms that estimate causal effect. The framework includes\nunlabeled data for prediction, labeled data for validation, and code for\nautomatic evaluation of algorithm predictions using both established and novel\nmetrics. The data is based on real-world covariates, and the treatment\nassignments and outcomes are based on simulations, which provides the basis for\nvalidation. In this framework we address two questions: one of scaling, and the\nother of data-censoring. The framework is available as open source code at\nhttps://github.com/IBM-HRL-MLHLS/IBM-Causal-Inference-Benchmarking-Framework\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 11:41:56 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 16:12:47 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Shimoni", "Yishai", ""], ["Yanover", "Chen", ""], ["Karavani", "Ehud", ""], ["Goldschmnidt", "Yaara", ""]]}, {"id": "1802.05292", "submitter": "Luca Rossini", "authors": "Fabrizio Leisen and Luca Rossini and Cristiano Villa", "title": "Loss-based approach to two-piece location-scale distributions with\n  applications to dependent data", "comments": "26 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-piece location-scale models are used for modeling data presenting\ndepartures from symmetry. In this paper, we propose an objective Bayesian\nmethodology for the tail parameter of two particular distributions of the above\nfamily: the skewed exponential power distribution and the skewed generalised\nlogistic distribution. We apply the proposed objective approach to time series\nmodels and linear regression models where the error terms follow the\ndistributions object of study. The performance of the proposed approach is\nillustrated through simulation experiments and real data analysis. The\nmethodology yields improvements in density forecasts, as shown by the analysis\nwe carry out on the electricity prices in Nordpool markets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 19:22:33 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 16:35:04 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Villa", "Cristiano", ""]]}, {"id": "1802.05444", "submitter": "Claudio Agostinelli", "authors": "Claudio Agostinelli", "title": "A Weighted Likelihood Approach Based on Statistical Data Depths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach to construct weighted likelihood estimating\nequations with the aim of obtain robust estimates. The weight, attached to each\nscore contribution, is evaluated by comparing the statistical data depth at the\nmodel with that of the sample in a given point. Observations are considered\nregular when the ratio of these two depths is close to one, whereas, when the\nratio is large the corresponding score contribution may be downweigthed.\nDetails and examples are provided for the robust estimation of the parameters\nin the multivariate normal model. Because of the form of the weights, we expect\nthat, there will be no downweighting under the true model leading to highly\nefficient estimators. Robustness is illustrated using two real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:28:25 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Agostinelli", "Claudio", ""]]}, {"id": "1802.05475", "submitter": "Shota Katayama", "authors": "Shota Katayama, Hironori Fujisawa and Mathias Drton", "title": "Robust and sparse Gaussian graphical modeling under cell-wise\n  contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical modeling explores dependences among a collection of variables by\ninferring a graph that encodes pairwise conditional independences. For jointly\nGaussian variables, this translates into detecting the support of the precision\nmatrix. Many modern applications feature high-dimensional and contaminated data\nthat complicate this task. In particular, traditional robust methods that\ndown-weight entire observation vectors are often inappropriate as\nhigh-dimensional data may feature partial contamination in many observations.\nWe tackle this problem by giving a robust method for sparse precision matrix\nestimation based on the $\\gamma$-divergence under a cell-wise contamination\nmodel. Simulation studies demonstrate that our procedure outperforms existing\nmethods especially for highly contaminated data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:51:53 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Katayama", "Shota", ""], ["Fujisawa", "Hironori", ""], ["Drton", "Mathias", ""]]}, {"id": "1802.05495", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "How Much Data Do You Need? An Operational, Pre-Asymptotic Metric for\n  Fat-tailedness", "comments": null, "journal-ref": "International Journal of Forecasting, 35-2, 677-686, 2019", "doi": "10.1016/j.ijforecast.2018.10.003", "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note presents an operational measure of fat-tailedness for univariate\nprobability distributions, in $[0,1]$ where 0 is maximally thin-tailed\n(Gaussian) and 1 is maximally fat-tailed. Among others,1) it helps assess the\nsample size needed to establish a comparative $n$ needed for statistical\nsignificance, 2) allows practical comparisons across classes of fat-tailed\ndistributions, 3) helps understand some inconsistent attributes of the\nlognormal, pending on the parametrization of its scale parameter. The\nliterature is rich for what concerns asymptotic behavior, but there is a large\nvoid for finite values of $n$, those needed for operational purposes.\nConventional measures of fat-tailedness, namely 1) the tail index for the power\nlaw class, and 2) Kurtosis for finite moment distributions fail to apply to\nsome distributions, and do not allow comparisons across classes and\nparametrization, that is between power laws outside the Levy-Stable basin, or\npower laws to distributions in other classes, or power laws for different\nnumber of summands. How can one compare a sum of 100 Student T distributed\nrandom variables with 3 degrees of freedom to one in a Levy-Stable or a\nLognormal class? How can one compare a sum of 100 Student T with 3 degrees of\nfreedom to a single Student T with 2 degrees of freedom? We propose an\noperational and heuristic measure that allow us to compare $n$-summed\nindependent variables under all distributions with finite first moment. The\nmethod is based on the rate of convergence of the Law of Large numbers for\nfinite sums, $n$-summands specifically. We get either explicit expressions or\nsimulation results and bounds for the lognormal, exponential, Pareto, and the\nStudent T distributions in their various calibrations --in addition to the\ngeneral Pearson classes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 11:57:08 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 15:24:55 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 15:21:52 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1802.05530", "submitter": "John Paul Gosling", "authors": "Christopher A. Pope, John Paul Gosling, Stuart Barber, Jill Johnson,\n  Takanobu Yamaguchi, Graham Feingold, Paul Blackwell", "title": "Gaussian process modeling of heterogeneity and discontinuities using\n  Voronoi tessellations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods for modelling spatial processes assume global smoothness\nproperties; such assumptions are often violated in practice. We introduce a\nmethod for modelling spatial processes that display heterogeneity or contain\ndiscontinuities. The problem of non-stationarity is dealt with by using a\ncombination of Voronoi tessellation to partition the input space, and a\nseparate Gaussian process to model the data on each region of the partitioned\nspace. Our method is highly flexible because we allow the Voronoi cells to form\nrelationships with each other, which can enable non-convex and disconnected\nregions to be considered. In such problems, identifying the borders between\nregions is often of great importance and we propose an adaptive sampling method\nto gain extra information along such borders. The method is illustrated with\nsimulation studies and application to real data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 13:51:37 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 13:58:25 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 09:19:28 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pope", "Christopher A.", ""], ["Gosling", "John Paul", ""], ["Barber", "Stuart", ""], ["Johnson", "Jill", ""], ["Yamaguchi", "Takanobu", ""], ["Feingold", "Graham", ""], ["Blackwell", "Paul", ""]]}, {"id": "1802.05570", "submitter": "Yoav Zemel", "authors": "Max Sommerfeld, J\\\"orn Schrieber, Yoav Zemel, Axel Munk", "title": "Optimal Transport: Fast Probabilistic Approximation with Exact Solvers", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research 20(105):1-23, 2019", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple subsampling scheme for fast randomized approximate\ncomputation of optimal transport distances. This scheme operates on a random\nsubset of the full data and can use any exact algorithm as a black-box\nback-end, including state-of-the-art solvers and entropically penalized\nversions. It is based on averaging the exact distances between empirical\nmeasures generated from independent samples from the original measures and can\neasily be tuned towards higher accuracy or shorter computation times. To this\nend, we give non-asymptotic deviation bounds for its accuracy in the case of\ndiscrete optimal transport problems. In particular, we show that in many\nimportant instances, including images (2D-histograms), the approximation error\nis independent of the size of the full problem. We present numerical\nexperiments that demonstrate that a very good approximation in typical\napplications can be obtained in a computation time that is several orders of\nmagnitude smaller than what is required for exact computation of the full\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:59:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:11:37 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 23:45:54 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 13:11:07 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Sommerfeld", "Max", ""], ["Schrieber", "J\u00f6rn", ""], ["Zemel", "Yoav", ""], ["Munk", "Axel", ""]]}, {"id": "1802.05631", "submitter": "Yuhao Wang", "authors": "Yuhao Wang, Chandler Squires, Anastasiya Belyaeva and Caroline Uhler", "title": "Direct Estimation of Differences in Causal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the differences between two causal\ndirected acyclic graph (DAG) models with a shared topological order given\ni.i.d. samples from each model. This is of interest for example in genomics,\nwhere changes in the structure or edge weights of the underlying causal graphs\nreflect alterations in the gene regulatory networks. We here provide the first\nprovably consistent method for directly estimating the differences in a pair of\ncausal DAGs without separately learning two possibly large and dense DAG models\nand computing their difference. Our two-step algorithm first uses invariance\ntests between regression coefficients of the two data sets to estimate the\nskeleton of the difference graph and then orients some of the edges using\ninvariance tests between regression residual variances. We demonstrate the\nproperties of our method through a simulation study and apply it to the\nanalysis of gene expression data from ovarian cancer and during T-cell\nactivation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 16:07:38 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 16:20:57 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 15:53:25 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Wang", "Yuhao", ""], ["Squires", "Chandler", ""], ["Belyaeva", "Anastasiya", ""], ["Uhler", "Caroline", ""]]}, {"id": "1802.05753", "submitter": "Atte Aalto", "authors": "Atte Aalto and Jorge Goncalves", "title": "Bayesian variable selection in linear dynamical systems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for reconstructing regulatory interconnection networks\nbetween variables evolving according to a linear dynamical system. The work is\nmotivated by the problem of gene regulatory network inference, that is, finding\ncausal effects between genes from gene expression time series data. In\nbiological applications, the typical problem is that the sampling frequency is\nlow, and consequentially the system identification problem is ill-posed. The\nlow sampling frequency also makes it impossible to estimate derivatives\ndirectly from the data. We take a Bayesian approach to the problem, as it\noffers a natural way to incorporate prior information to deal with the\nill-posedness, through the introduction of sparsity promoting prior for the\nunderlying dynamics matrix. It also provides a framework for modelling both the\nprocess and measurement noises. We develop Markov Chain Monte Carlo samplers\nfor the discrete-valued zero-structure of the dynamics matrix, and for the\ncontinuous-time trajectory of the system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 20:39:23 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Aalto", "Atte", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1802.05761", "submitter": "Johan Strandberg", "authors": "Johan Strandberg, Sara Sj\\\"ostedt de Luna and Jorge Mateu", "title": "Prediction of spatial functional random processes: Comparing functional\n  and spatio-temporal kriging approaches", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present and compare functional and spatio-temporal (Sp.T.)\nkriging approaches to predict spatial functional random processes (which can\nalso be viewed as Sp.T. random processes). Comparisons with respect to\ncomputational time and prediction performance via functional cross-validation\nis evaluated, mainly through a simulation study but also on two real data sets.\nWe restrict comparisons to Sp.T. kriging versus ordinary kriging for functional\ndata (OKFD), since the more flexible functional kriging approaches, pointwise\nfunctional kriging (PWFK) and functional kriging total model, coincide with\nOKFD in several situations. We contribute with new knowledge by proving that\nOKFD and PWFK coincide under certain conditions. From the simulation study, it\nis concluded that the prediction performance for the two kriging approaches in\ngeneral is rather equal for stationary Sp.T. processes, with a tendency for\nfunctional kriging to work better for small sample sizes and Sp.T. kriging to\nwork better for large sample sizes. For non-stationary Sp.T. processes, with a\ncommon deterministic time trend and/or time varying variances and dependence\nstructure, OKFD performs better than Sp.T. kriging irrespective of sample size.\nFor all simulated cases, the computational time for OKFD was considerably lower\ncompared to those for the Sp.T. kriging methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 21:03:25 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Strandberg", "Johan", ""], ["de Luna", "Sara Sj\u00f6stedt", ""], ["Mateu", "Jorge", ""]]}, {"id": "1802.05917", "submitter": "In\\'es del Puerto", "authors": "M. Gonz\\'alez, C. Minuesa, I. del Puerto, A.N. Vidyashankar", "title": "Robust estimation in controlled branching processes: Bayesian estimators\n  via disparities", "comments": "Paper and suplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with Bayesian inferential methods for data from\ncontrolled branching processes that account for model robustness through the\nuse of disparities. Under regularity conditions, we establish that estimators\nbuilt on disparity-based posterior, such as expectation and maximum a\nposteriori estimates, are consistent and efficient under the posited model.\nAdditionally, we show that the estimates are robust to model misspecification\nand presence of aberrant outliers. To this end, we develop several fundamental\nideas relating minimum disparity estimators to Bayesian estimators built on the\ndisparity-based posterior, for dependent tree-structured data. We illustrate\nthe methodology through a simulated example and apply our methods to a real\ndata set from cell kinetics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:57:01 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gonz\u00e1lez", "M.", ""], ["Minuesa", "C.", ""], ["del Puerto", "I.", ""], ["Vidyashankar", "A. N.", ""]]}, {"id": "1802.06009", "submitter": "Joshua Gardner", "authors": "Josh Gardner, Christopher Brooks", "title": "Dropout Model Evaluation in MOOCs", "comments": null, "journal-ref": "Eighth AAAI Symposium on Educational Advances in Artificial\n  Intelligence (EAAI-18), 2018", "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of learning analytics needs to adopt a more rigorous approach for\npredictive model evaluation that matches the complex practice of\nmodel-building. In this work, we present a procedure to statistically test\nhypotheses about model performance which goes beyond the state-of-the-practice\nin the community to analyze both algorithms and feature extraction methods from\nraw data. We apply this method to a series of algorithms and feature sets\nderived from a large sample of Massive Open Online Courses (MOOCs). While a\ncomplete comparison of all potential modeling approaches is beyond the scope of\nthis paper, we show that this approach reveals a large gap in dropout\nprediction performance between forum-, assignment-, and clickstream-based\nfeature extraction methods, where the latter is significantly better than the\nformer two, which are in turn indistinguishable from one another. This work has\nmethodological implications for evaluating predictive or AI-based models of\nstudent success, and practical implications for the design and targeting of\nat-risk student models and interventions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:13:39 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Gardner", "Josh", ""], ["Brooks", "Christopher", ""]]}, {"id": "1802.06048", "submitter": "Yilei Wu", "authors": "Yilei Wu, Yingli Qin, Mu Zhu", "title": "High-dimensional covariance matrix estimation using a low-rank and\n  diagonal decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional covariance/precision matrix estimation under the\nassumption that the covariance/precision matrix can be decomposed into a\nlow-rank component L and a diagonal component D. The rank of L can either be\nchosen to be small or controlled by a penalty function. Under moderate\nconditions on the population covariance/precision matrix itself and on the\npenalty function, we prove some consistency results for our estimators. A\nblockwise coordinate descent algorithm, which iteratively updates L and D, is\nthen proposed to obtain the estimator in practice. Finally, various numerical\nexperiments are presented: using simulated data, we show that our estimator\nperforms quite well in terms of the Kullback-Leibler loss; using stock return\ndata, we show that our method can be applied to obtain enhanced solutions to\nthe Markowitz portfolio selection problem.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:43:44 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Wu", "Yilei", ""], ["Qin", "Yingli", ""], ["Zhu", "Mu", ""]]}, {"id": "1802.06054", "submitter": "James Sharpnack", "authors": "James Sharpnack", "title": "Learning Patterns for Detection with Multiscale Scan Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses detecting anomalous patterns in images, time-series, and\ntensor data when the location and scale of the pattern is unknown a priori. The\nmultiscale scan statistic convolves the proposed pattern with the image at\nvarious scales and returns the maximum of the resulting tensor. Scale corrected\nmultiscale scan statistics apply different standardizations at each scale, and\nthe limiting distribution under the null hypothesis---that the data is only\nnoise---is known for smooth patterns. We consider the problem of simultaneously\nlearning and detecting the anomalous pattern from a dictionary of smooth\npatterns and a database of many tensors. To this end, we show that the\nmultiscale scan statistic is a subexponential random variable, and prove a\nchaining lemma for standardized suprema, which may be of independent interest.\nThen by averaging the statistics over the database of tensors we can learn the\npattern and obtain Bernstein-type error bounds. We will also provide a\nconstruction of an $\\epsilon$-net of the location and scale parameters,\nproviding a computationally tractable approximation with similar error bounds.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:59:11 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 17:50:31 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 05:07:15 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Sharpnack", "James", ""]]}, {"id": "1802.06156", "submitter": "Ruoqing Zhu", "authors": "Wenzhuo Zhou, Ruoqing Zhu, Donglin Zeng", "title": "A Parsimonious Personalized Dose Finding Model via Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an individualized dose rule in personalized medicine is a\nchallenging statistical problem. Existing methods for estimating the optimal\nindividualized dose rule often suffer from the curse of dimensionality,\nespecially when the dose rule is learned nonparametrically using machine\nlearning approaches. To tackle this problem, we propose a dimension reduction\nframework that effectively reduces the estimation of dose rule in a\nlower-dimensional subspace of the covariates, leading to a more parsimonious\nmodel. To achieve this, the proposed methods exploit that the subspace is\nspanned by a few linear combinations of the covariates, which can be estimated\nefficiently by using an orthogonality constrained optimization approach. Using\nthis framework, we proposed a direct estimation of the value function under any\ngiven suggested dose rule with dimension reduction. This does not require an\ninverse probability of the propensity score, which distinguishes us from the\npopular outcome weighted learning framework. We further propose two approaches:\na direct learning approach that yields the dose rule as commonly desired in\npersonalized medicine, and a pseudo-direct learning approach that focuses more\non estimating the dimensionality-reduced subspace. Under mild regularity\nassumptions, the asymptotical normality of the proposed subspace estimators is\nestablished respectively. The consistency and convergence rate for the\nestimated optimal dose rule are also derived. For both approaches, we formulate\nan effectively numerical optimization problem as solving solutions on the\nStiefel manifold. The performance of the proposed approaches is evaluated\nthrough simulation studies and a warfarin pharmacogenetic dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 23:03:29 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 21:04:31 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 17:01:44 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 17:06:51 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Wenzhuo", ""], ["Zhu", "Ruoqing", ""], ["Zeng", "Donglin", ""]]}, {"id": "1802.06291", "submitter": "Enkelejd Hashorva", "authors": "Enkelejd Hashorva", "title": "Domination of Sample Maxima and Related Extremal Dependence Measures", "comments": "Accepted in Dependence Modelling", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given $d$-dimensional distribution function (df) $H$ we introduce the\nclass of dependence measures $ \\mu(H,Q) = - \\mathbb{E}\\{ \\ln H(Z_1, \\ldots,\nZ_d)\\},$ where the random vector $(Z_1, \\ldots, Z_d)$ has df $Q$ which has the\nsame marginal df's as $H$. If both $H$ and $Q$ are max-stable df's, we show\nthat for a df $F$ in the max-domain of attraction of $H$, this dependence\nmeasure explains the extremal dependence exhibited by $F$. Moreover we prove\nthat $\\mu(H,Q)$ is the limit of the probability that the maxima of a random\nsample from $F$ is marginally dominated by some random vector with df in the\nmax-domain of attraction of $Q$. We show a similar result for the complete\ndomination of the sample maxima which leads to another measure of dependence\ndenoted by $\\lambda(Q,H)$. In the literature $\\lambda(H,H)$ with $H$ a\nmax-stable df has been studied in the context of records, multiple maxima,\nconcomitants of order statistics and concurrence probabilities. It turns out\nthat both $\\mu(H,Q)$ and $\\lambda(Q,H)$ are closely related. If $H$ is\nmax-stable we derive useful representations for both $\\mu(H,Q)$ and\n$\\lambda(Q,H)$. Our applications include equivalent conditions for $H$ to be a\nproduct df and $F$ to have asymptotically independent components.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:54:28 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 15:56:08 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Hashorva", "Enkelejd", ""]]}, {"id": "1802.06308", "submitter": "Guang Cheng", "authors": "Meimei Liu and Zuofeng Shang and Guang Cheng", "title": "Nonparametric Testing under Random Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in nonparametric inference is its high computational\ncomplexity when data volume is large. In this paper, we develop computationally\nefficient nonparametric testing by employing a random projection strategy. In\nthe specific kernel ridge regression setup, a simple distance-based test\nstatistic is proposed. Notably, we derive the minimum number of random\nprojections that is sufficient for achieving testing optimality in terms of the\nminimax rate. An adaptive testing procedure is further established without\nprior knowledge of regularity. One technical contribution is to establish upper\nbounds for a range of tail sums of empirical kernel eigenvalues. Simulations\nand real data analysis are conducted to support our theory.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:22:34 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liu", "Meimei", ""], ["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1802.06310", "submitter": "Karren Yang", "authors": "Karren D. Yang, Abigail Katcoff and Caroline Uhler", "title": "Characterizing and Learning Equivalence Classes of Causal DAGs under\n  Interventions", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal DAGs in the setting where both\nobservational and interventional data is available. This setting is common in\nbiology, where gene regulatory networks can be intervened on using chemical\nreagents or gene deletions. Hauser and B\\\"uhlmann (2012) previously\ncharacterized the identifiability of causal DAGs under perfect interventions,\nwhich eliminate dependencies between targeted variables and their direct\ncauses. In this paper, we extend these identifiability results to general\ninterventions, which may modify the dependencies between targeted variables and\ntheir causes without eliminating them. We define and characterize the\ninterventional Markov equivalence class that can be identified from general\n(not necessarily perfect) intervention experiments. We also propose the first\nprovably consistent algorithm for learning DAGs in this setting and evaluate\nour algorithm on simulated and biological datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:42:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 23:27:53 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 17:27:13 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Yang", "Karren D.", ""], ["Katcoff", "Abigail", ""], ["Uhler", "Caroline", ""]]}, {"id": "1802.06332", "submitter": "Hailin Sang", "authors": "Jamye Curry, Xin Dang and Hailin Sang", "title": "A rank-based Cram\\'er-von-Mises-type test for two samples", "comments": "32 pages, 2 figures, to appear at Brazilian Journal of Probability\n  and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a rank based univariate two-sample distribution-free test. The test\nstatistic is the difference between the average of between-group rank distances\nand the average of within-group rank distances. This test statistic is closely\nrelated to the two-sample Cram\\'er-von Mises criterion. They are different\nempirical versions of a same quantity for testing the equality of two\npopulation distributions. Although they may be different for finite samples,\nthey share the same expected value, variance and asymptotic properties. The\nadvantage of the new rank based test over the classical one is its ease to\ngeneralize to the multivariate case. Rather than using the empirical process\napproach, we provide a different easier proof, bringing in a different\nperspective and insight. In particular, we apply the H\\'ajek projection and\northogonal decomposition technique in deriving the asymptotics of the proposed\nrank based statistic. A numerical study compares power performance of the rank\nformulation test with other commonly-used nonparametric tests and\nrecommendations on those tests are provided. Lastly, we propose a multivariate\nextension of the test based on the spatial rank.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 04:46:23 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 18:53:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Curry", "Jamye", ""], ["Dang", "Xin", ""], ["Sang", "Hailin", ""]]}, {"id": "1802.06340", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton and Ali Shojaie", "title": "Graphical Models for Non-Negative Data Using Generalized Score Matching", "comments": "10 pages, 8 figures, uses aistats2018.sty, to be published in the\n  Proceedings of the 21st International Conference on Artificial Intelligence\n  and Statistics (AISTATS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in estimating parameters of probability density functions\nis the intractability of the normalizing constant. While in such cases maximum\nlikelihood estimation may be implemented using numerical integration, the\napproach becomes computationally intensive. In contrast, the score matching\nmethod of Hyv\\\"arinen (2005) avoids direct calculation of the normalizing\nconstant and yields closed-form estimates for exponential families of\ncontinuous distributions over $\\mathbb{R}^m$. Hyv\\\"arinen (2007) extended the\napproach to distributions supported on the non-negative orthant\n$\\mathbb{R}_+^m$. In this paper, we give a generalized form of score matching\nfor non-negative data that improves estimation efficiency. We also generalize\nthe regularized score matching method of Lin et al. (2016) for non-negative\nGaussian graphical models, with improved theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 06:42:24 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "1802.06350", "submitter": "Haakon Bakka", "authors": "Haakon Bakka, H{\\aa}vard Rue, Geir-Arne Fuglstad, Andrea Riebler,\n  David Bolin, Elias Krainski, Daniel Simpson, Finn Lindgren", "title": "Spatial modelling with R-INLA: A review", "comments": "Extensive update, restructuring of sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coming up with Bayesian models for spatial data is easy, but performing\ninference with them can be challenging. Writing fast inference code for a\ncomplex spatial model with realistically-sized datasets from scratch is\ntime-consuming, and if changes are made to the model, there is little guarantee\nthat the code performs well. The key advantages of R-INLA are the ease with\nwhich complex models can be created and modified, without the need to write\ncomplex code, and the speed at which inference can be done even for spatial\nproblems with hundreds of thousands of observations.\n  R-INLA handles latent Gaussian models, where fixed effects, structured and\nunstructured Gaussian random effects are combined linearly in a linear\npredictor, and the elements of the linear predictor are observed through one or\nmore likelihoods. The structured random effects can be both standard areal\nmodel such as the Besag and the BYM models, and geostatistical models from a\nsubset of the Mat\\'ern Gaussian random fields. In this review, we discuss the\nlarge success of spatial modelling with R-INLA and the types of spatial models\nthat can be fitted, we give an overview of recent developments for areal\nmodels, and we give an overview of the stochastic partial differential equation\n(SPDE) approach and some of the ways it can be extended beyond the assumptions\nof isotropy and separability. In particular, we describe how slight changes to\nthe SPDE approach leads to straight-forward approaches for non-stationary\nspatial models and non-separable space-time models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 08:46:22 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 07:33:31 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bakka", "Haakon", ""], ["Rue", "H\u00e5vard", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Bolin", "David", ""], ["Krainski", "Elias", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""]]}, {"id": "1802.06359", "submitter": "Emanuele  Giorgi", "authors": "Emanuele Giorgi, Peter J. Diggle, Robert W. Snow and Abdisalan M. Noor", "title": "Geostatistical methods for disease mapping and visualization using data\n  from spatio-temporally referenced prevalence surveys", "comments": "Extended version of the paper in press on International Statistical\n  Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we set out general principles and develop geostatistical\nmethods for the analysis of data from spatio-temporally referenced prevalence\nsurveys. Our objective is to provide a tutorial guide that can be used in order\nto identify parsimonious geostatistical models for prevalence mapping. A\ngeneral variogram-based Monte Carlo procedure is proposed to check the validity\nof the modelling assumptions. We describe and contrast likelihood-based and\nBayesian methods of inference, showing how to account for parameter uncertainty\nunder each of the two paradigms. We also describe extensions of the standard\nmodel for disease prevalence that can be used when stationarity of the\nspatio-temporal covariance function is not supported by the data. We discuss\nhow to define predictive targets and argue that exceedance probabilities\nprovide one of the most effective ways to convey uncertainty in prevalence\nestimates. We describe statistical software for the visualization of\nspatio-temporal predictive summaries of prevalence through interactive\nanimations. Finally, we illustrate an application to historical malaria\nprevalence data from 1334 surveys conducted in Senegal between 1905 and 2014.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 10:19:53 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Giorgi", "Emanuele", ""], ["Diggle", "Peter J.", ""], ["Snow", "Robert W.", ""], ["Noor", "Abdisalan M.", ""]]}, {"id": "1802.06373", "submitter": "Mark Podolskij", "authors": "Stepan Mazur, Dmitry Otryakhin and Mark Podolskij", "title": "Estimation of the linear fractional stable motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the parametric inference for the linear\nfractional stable motion in high and low frequency setting. The symmetric\nlinear fractional stable motion is a three-parameter family, which constitutes\na natural non-Gaussian analogue of the scaled fractional Brownian motion. It is\nfully characterised by the scaling parameter $\\sigma>0$, the self-similarity\nparameter $H \\in (0,1)$ and the stability index $\\alpha \\in (0,2)$ of the\ndriving stable motion. The parametric estimation of the model is inspired by\nthe limit theory for stationary increments L\\'evy moving average processes that\nhas been recently studied in \\cite{BLP}. More specifically, we combine\n(negative) power variation statistics and empirical characteristic functions to\nobtain consistent estimates of $(\\sigma, \\alpha, H)$. We present the law of\nlarge numbers and some fully feasible weak limit theorems.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 13:21:24 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Mazur", "Stepan", ""], ["Otryakhin", "Dmitry", ""], ["Podolskij", "Mark", ""]]}, {"id": "1802.06710", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Dylan S. Small, Francesca Dominici", "title": "Discovering Effect Modification and Randomization Inference in Air\n  Pollution Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that exposure to air pollution, even at low levels,\nsignificantly increases mortality. As regulatory actions are becoming\nprohibitively expensive, robust evidence to guide the development of targeted\ninterventions to reduce air pollution exposure is needed. In this paper, we\nintroduce a novel statistical method that splits the data into two subsamples:\n(a) Using the first subsample, we consider a data-driven search for $\\textit{de\nnovo}$ discovery of subgroups that could have exposure effects that differ from\nthe population mean; and then (b) using the second subsample, we quantify\nevidence of effect modification among the subgroups with nonparametric\nrandomization-based tests. We also develop a sensitivity analysis method to\nassess the robustness of the conclusions to unmeasured confounding bias. Via\nsimulation studies and theoretical arguments, we demonstrate that since we\ndiscover the subgroups in the first subsample, hypothesis testing on the second\nsubsample can focus on theses subgroups only, thus substantially increasing the\nstatistical power of the test. We apply our method to the data of 1,612,414\nMedicare beneficiaries in New England region in the United States for the\nperiod 2000 to 2006. We find that seniors aged between 81-85 with low income\nand seniors aged above 85 have statistically significant higher causal effects\nof exposure to PM$_{2.5}$ on 5-year mortality rate compared to the population\nmean.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:11:16 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lee", "Kwonsang", ""], ["Small", "Dylan S.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1802.06711", "submitter": "Kwonsang Lee", "authors": "Kwonsang Lee, Scott A. Lorch, Dylan S. Small", "title": "Sensitivity analyses for average treatment effects when outcome is\n  censored by death in instrumental variable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two problems that arise in making causal inferences for non-mortality\noutcomes such as bronchopulmonary dysplasia (BPD) are unmeasured confounding\nand censoring by death, i.e., the outcome is only observed when subjects\nsurvive. In randomized experiments with noncompliance, instrumental variable\nmethods can be used to control for the unmeasured confounding without censoring\nby death. But when there is censoring by death, the average causal treatment\neffect cannot be identified under usual assumptions, but can be studied for a\nspecific subpopulation by using sensitivity analysis with additional\nassumptions. However, in observational studies, evaluation of the local average\ntreatment effect (LATE) in censoring by death problems with unmeasured\nconfounding is not well studied. We develop a novel sensitivity analysis method\nbased on instrumental variable models for studying the LATE. Specifically, we\npresent the identification results under an additional assumption, and propose\na three-step procedure for the LATE estimation. Also, we propose an improved\ntwo-step procedure by simultaneously estimating the instrument propensity score\n(i.e., the probability of instrument given covariates) and the parameters\ninduced by the assumption. We have shown with simulation studies that the\ntwo-step procedure can be more robust and efficient than the three-step\nprocedure. Finally, we apply our sensitivity analysis methods to a study of the\neffect of delivery at high-level neonatal intensive care units on the risk of\nBPD.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 17:13:09 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lee", "Kwonsang", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1802.06715", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu and Vahid Nekoukhou", "title": "Univariate and Bivariate Geometric Discrete Generalized Exponential\n  Distributions", "comments": "arXiv admin note: text overlap with arXiv:1701.03569", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marshall and Olkin (1997, Biometrika, 84, 641 - 652) introduced a very\npowerful method to introduce an additional parameter to a class of continuous\ndistribution functions and hence it brings more flexibility to the model. They\nhave demonstrated their method for the exponential and Weibull classes. In the\nsame paper they have briefly indicated regarding its bivariate extension. The\nmain aim of this paper is to introduce the same method, for the first time, to\nthe class of discrete generalized exponential distributions both for the\nunivariate and bivariate cases. We investigate several properties of the\nproposed univariate and bivariate classes. The univariate class has three\nparameters, whereas the bivariate class has five parameters. It is observed\nthat depending on the parameter values the univariate class can be both zero\ninflated as well as heavy tailed. We propose to use EM algorithm to estimate\nthe unknown parameters. Small simulation experiments have been performed to see\nthe effectiveness of the proposed EM algorithm, and a bivariate data set has\nbeen analyzed and it is observed that the proposed models and the EM algorithm\nwork quite well in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 05:58:10 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kundu", "Debasis", ""], ["Nekoukhou", "Vahid", ""]]}, {"id": "1802.06859", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A. Vsevolozhskaya, Gabriel Ruiz, Dmitri V. Zaykin", "title": "Maximum value of the standardized log of odds ratio and celestial\n  mechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The odds ratio (OR) is a widely used measure of the effect size in\nobservational research. ORs reflect statistical association between a binary\noutcome, such as the presence of a health condition, and a binary predictor,\nsuch as an exposure to a pollutant. Statistical significance and interval\nestimates are often computed for the logarithm of OR, ln(OR), and depend on the\nasymptotic standard error of ln(OR). For a sample of size N, the standard error\ncan be written as a ratio of sigma over square root of N, where sigma is the\npopulation standard deviation of ln(OR). The ratio of ln(OR) over sigma is a\nstandardized effect size. Unlike correlation, that is another familiar\nstandardized statistic, the standardized ln(OR) cannot reach values of minus\none or one. We find that its maximum possible value is given by the Laplace\nLimit Constant, (LLC=0.6627...), that appears as a condition in solutions to\nKepler equation -- one of the central equations in celestial mechanics. The\nrange of the standardized ln(OR) is bounded by minus LLC to LLC, reaching its\nmaximum for ln(OR)~4.7987. This range has implications for analysis of\nepidemiological associations, affecting the behavior of the reasonable prior\ndistribution for the standardized ln(OR).\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:18:53 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Vsevolozhskaya", "Olga A.", ""], ["Ruiz", "Gabriel", ""], ["Zaykin", "Dmitri V.", ""]]}, {"id": "1802.06931", "submitter": "Wei Wang", "authors": "Wei Wang and Matthew Stephens", "title": "Empirical Bayes Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization methods - including Factor analysis (FA), and Principal\nComponents Analysis (PCA) - are widely used for inferring and summarizing\nstructure in multivariate data. Many matrix factorization methods exist,\ncorresponding to different assumptions on the elements of the underlying matrix\nfactors. For example, many recent methods use a penalty or prior distribution\nto achieve sparse representations (\"Sparse FA/PCA\"). Here we introduce a\ngeneral Empirical Bayes approach to matrix factorization (EBMF), whose key\nfeature is that it uses the observed data to estimate prior distributions on\nmatrix elements. We derive a correspondingly-general variational fitting\nalgorithm, which reduces fitting EBMF to solving a simpler problem - the\nso-called \"normal means\" problem. We implement this general algorithm, but\nfocus particular attention on the use of sparsity-inducing priors that are\nuni-modal at 0. This yields a sparse EBMF approach - essentially a version of\nsparse FA/PCA - that automatically adapts the amount of sparsity to the data.\nWe demonstrate the benefits of our approach through both numerical comparisons\nwith competing methods and through analysis of data from the GTEx (Genotype\nTissue Expression) project on genetic associations across 44 human tissues. In\nnumerical comparisons EBMF often provides more accurate inferences than other\nmethods. In the GTEx data, EBMF identifies interpretable structure that\nconcords with known relationships among human tissues. Software implementing\nour approach is available at https://github.com/stephenslab/flashr\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 01:45:02 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 02:53:32 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 17:25:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Wei", ""], ["Stephens", "Matthew", ""]]}, {"id": "1802.06995", "submitter": "Maria Umlauft", "authors": "Maria Umlauft", "title": "How to analyze data in a factorial design? An extensive simulation study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs are frequently used in different fields of science, e.g.\npsychological, medical or biometric studies. Standard approaches, as the ANOVA\n$F$-test, make different assumptions on the distribution of the error terms,\nthe variances or the sample sizes in the different groups. Because of time\nconstraints or a lack of statistical background, many users do not check these\nassumptions; enhancing the risk of potentially inflated type-$I$ error rates or\na substantial loss of power. It is the aim of the present paper, to give an\noverview of different methods without such restrictive assumptions and to\nidentify situations in which one method is superior compared to others. In\nparticular, after summarizing their underlying assumptions, the different\napproaches are compared within extensive simulations. To also address the\ncurrent discussion about redefining the statistical significance level, we also\nincluded simulations for the 0.5\\% level.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 07:31:59 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Umlauft", "Maria", ""]]}, {"id": "1802.07330", "submitter": "Michail Tsagris", "authors": "Michail Tsagris and Connie Stewart", "title": "A folded model for compositional data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A folded type model is developed for analyzing compositional data. The\nproposed model involves an extension of the $\\alpha$-transformation for\ncompositional data and provides a new and flexible class of distributions for\nmodeling data defined on the simplex sample space. Despite its rather seemingly\ncomplex structure, employment of the EM algorithm guarantees efficient\nparameter estimation. The model is validated through simulation studies and\nexamples which illustrate that the proposed model performs better in terms of\ncapturing the data structure, when compared to the popular logistic normal\ndistribution, and can be advantageous over a similar model without folding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 21:16:09 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 10:41:19 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Tsagris", "Michail", ""], ["Stewart", "Connie", ""]]}, {"id": "1802.07380", "submitter": "Sean Jewell", "authors": "Sean Jewell, Toby Dylan Hocking, Paul Fearnhead, Daniela Witten", "title": "Fast Nonconvex Deconvolution of Calcium Imaging Data", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging data promises to transform the field of neuroscience by\nmaking it possible to record from large populations of neurons simultaneously.\nHowever, determining the exact moment in time at which a neuron spikes, from a\ncalcium imaging data set, amounts to a non-trivial deconvolution problem which\nis of critical importance for downstream analyses. While a number of\nformulations have been proposed for this task in the recent literature, in this\npaper we focus on a formulation recently proposed in Jewell and Witten (2017)\nwhich has shown initial promising results. However, this proposal is slow to\nrun on fluorescence traces of hundreds of thousands of timesteps.\n  Here we develop a much faster online algorithm for solving the optimization\nproblem of Jewell and Witten (2017) that can be used to deconvolve a\nfluorescence trace of 100,000 timesteps in less than a second. Furthermore,\nthis algorithm overcomes a technical challenge of Jewell and Witten (2017) by\navoiding the occurrence of so-called \"negative\" spikes. We demonstrate that\nthis algorithm has superior performance relative to existing methods for spike\ndeconvolution on calcium imaging datasets that were recently released as part\nof the spikefinder challenge (http://spikefinder.codeneuro.org/).\n  Our C++ implementation, along with R and python wrappers, is publicly\navailable on Github at https://github.com/jewellsean/FastLZeroSpikeInference.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 00:04:42 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Jewell", "Sean", ""], ["Hocking", "Toby Dylan", ""], ["Fearnhead", "Paul", ""], ["Witten", "Daniela", ""]]}, {"id": "1802.07434", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari, Joydeep Ghosh, Mingyuan Zhou", "title": "Nonparametric Bayesian Sparse Graph Linear Dynamical Systems", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric Bayesian sparse graph linear dynamical system (SGLDS) is\nproposed to model sequentially observed multivariate data. SGLDS uses the\nBernoulli-Poisson link together with a gamma process to generate an infinite\ndimensional sparse random graph to model state transitions. Depending on the\nsparsity pattern of the corresponding row and column of the graph affinity\nmatrix, a latent state of SGLDS can be categorized as either a non-dynamic\nstate or a dynamic one. A normal-gamma construction is used to shrink the\nenergy captured by the non-dynamic states, while the dynamic states can be\nfurther categorized into live, absorbing, or noise-injection states, which\ncapture different types of dynamical components of the underlying time series.\nThe state-of-the-art performance of SGLDS is demonstrated with experiments on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:15:21 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Kalantari", "Rahi", ""], ["Ghosh", "Joydeep", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1802.07444", "submitter": "Chen Luo", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and\npopular variants of MCMC for problems when an MCMC state consists of an unknown\nnumber of components. It is well known that state-of-the-art methods for\nsplit-merge MCMC do not scale well. Strategies for rapid mixing requires smart\nand informative proposals to reduce the rejection rate. However, all known\nsmart proposals involve expensive operations to suggest informative\ntransitions. As a result, the cost of each iteration is prohibitive for massive\nscale datasets. It is further known that uninformative but computationally\nefficient proposals, such as random split-merge, leads to extremely slow\nconvergence. This tradeoff between mixing time and per update cost seems hard\nto get around.\n  In this paper, we show a sweet spot. We leverage some unique properties of\nweighted MinHash, which is a popular LSH, to design a novel class of\nsplit-merge proposals which are significantly more informative than random\nsampling but at the same time efficient to compute. Overall, we obtain a\nsuperior tradeoff between convergence and per update cost. As a direct\nconsequence, our proposals are around 6X faster than the state-of-the-art\nsampling methods on two large real datasets KDDCUP and PubMed with several\nmillions of entities and thousands of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 07:03:32 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 20:36:49 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2018 05:06:40 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1802.07613", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "About Kendall's regression", "comments": "37 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Kendall's tau is a measure of dependence between two random\nvariables, conditionally on some covariates. We assume a regression-type\nrelationship between conditional Kendall's tau and some covariates, in a\nparametric setting with a large number of transformations of a small number of\nregressors. This model may be sparse, and the underlying parameter is estimated\nthrough a penalized criterion. We prove non-asymptotic bounds with explicit\nconstants that hold with high probabilities. We derive the consistency of a\ntwo-step estimator, its asymptotic law and some oracle properties. Some\nsimulations and applications to real data conclude the paper.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 15:28:37 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 09:13:01 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "1802.07696", "submitter": "Holger Dette", "authors": "Holger Dette, Josua G\\\"osmann", "title": "A likelihood ratio approach to sequential change point detection for a\n  general class of parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach for sequential monitoring of a\nparameter of a $d$-dimensional time series, which can be estimated by\napproximately linear functionals of the empirical distribution function. We\nconsider a closed-end-method, which is motivated by the likelihood ratio test\nprinciple and compare the new method with two alternative procedures. We also\nincorporate self-normalization such that estimation of the long-run variance is\nnot necessary. We prove that for a large class of testing problems the new\ndetection scheme has asymptotic level $\\alpha$ and is consistent. The\nasymptotic theory is illustrated for the important cases of monitoring a change\nin the mean, variance and correlation. By means of a simulation study it is\ndemonstrated that the new test performs better than the currently available\nprocedures for these problems.Finally the methodology is illustrated by a small\ndata example investigating index prices from the dot-com bubble.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 17:54:14 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 17:35:42 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Dette", "Holger", ""], ["G\u00f6smann", "Josua", ""]]}, {"id": "1802.07721", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Discussion on \"Sparse graphs using exchangeable random measures\" by\n  Francois Caron and Emily B. Fox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a discussion on \"Sparse graphs using exchangeable random measures\" by\nFrancois Caron and Emily B. Fox, published in Journal of the Royal Statistical\nSociety, Series B, 2017.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 06:23:29 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1802.07838", "submitter": "Carter Butts", "authors": "Francis Lee and Carter T Butts", "title": "Mutual Assent or Unilateral Nomination? A Performance Comparison of\n  Intersection and Union Rules for Integrating Self-reports of Social\n  Relationships", "comments": null, "journal-ref": "Social Networks, 55, 55-62 (2018)", "doi": "10.1016/j.socnet.2018.05.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection designs for social network studies frequently involve asking\nboth parties to a potential relationship to report on the presence of absence\nof that relationship, resulting in two measurements per potential tie. When\ninferring the underlying network, is it better to estimate the tie as present\nonly when both parties report it as present or do so when either reports it?\nEmploying several data sets in which network structure can be well-determined\nfrom large numbers of informant reports, we examine the performance of these\ntwo simple rules. Our analysis shows better results for mutual assent across\nall data sets examined. A theoretical analysis of estimator performance shows\nthat the best rule depends on both underlying error rates and the sparsity of\nthe underlying network, with sparsity driving the superiority of mutual assent\nin typical social network settings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:49:41 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Lee", "Francis", ""], ["Butts", "Carter T", ""]]}, {"id": "1802.07995", "submitter": "Frank Werner", "authors": "Claudia K\\\"onig and Axel Munk and Frank Werner", "title": "Multidimensional multiscale scanning in Exponential Families: Limit\n  theory and statistical consequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding anomalies in a $d$-dimensional field of\nindependent random variables $\\{Y_i\\}_{i \\in \\left\\{1,...,n\\right\\}^d}$, each\ndistributed according to a one-dimensional natural exponential family $\\mathcal\nF = \\left\\{F_\\theta\\right\\}_{\\theta \\in\\Theta}$. Given some baseline parameter\n$\\theta_0 \\in\\Theta$, the field is scanned using local likelihood ratio tests\nto detect from a (large) given system of regions $\\mathcal{R}$ those regions $R\n\\subset \\left\\{1,...,n\\right\\}^d$ with $\\theta_i \\neq \\theta_0$ for some $i \\in\nR$. We provide a unified methodology which controls the overall family wise\nerror (FWER) to make a wrong detection at a given error rate.\n  Fundamental to our method is a Gaussian approximation of the distribution of\nthe underlying multiscale test statistic with explicit rate of convergence.\nFrom this, we obtain a weak limit theorem which can be seen as a generalized\nweak invariance principle to non identically distributed data and is of\nindependent interest. Furthermore, we give an asymptotic expansion of the\nprocedures power, which yields minimax optimality in case of Gaussian\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 11:52:56 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 10:42:02 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 07:02:23 GMT"}, {"version": "v4", "created": "Sun, 24 Mar 2019 08:42:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["K\u00f6nig", "Claudia", ""], ["Munk", "Axel", ""], ["Werner", "Frank", ""]]}, {"id": "1802.08114", "submitter": "Thomas E Bartlett", "authors": "Thomas E. Bartlett, Ioannis Kosmidis, and Ricardo Silva", "title": "Two-way sparsity for time-varying networks, with applications in\n  genomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel way of modelling time-varying networks, by inducing\ntwo-way sparsity on local models of node connectivity. This two-way sparsity\nseparately promotes sparsity across time and sparsity across variables (within\ntime). Separation of these two types of sparsity is achieved through a novel\nprior structure, which draws on ideas from the Bayesian lasso and from copula\nmodelling. We provide an efficient implementation of the proposed model via a\nGibbs sampler, and we apply the model to data from neural development. In doing\nso, we demonstrate that the proposed model is able to identify changes in\ngenomic network structure that match current biological knowledge. Such changes\nin genomic network structure can then be used by neuro-biologists to identify\npotential targets for further experimental investigation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 15:53:55 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:35:48 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 16:37:20 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2019 12:25:35 GMT"}, {"version": "v5", "created": "Thu, 8 Oct 2020 19:41:34 GMT"}, {"version": "v6", "created": "Wed, 18 Nov 2020 15:28:30 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bartlett", "Thomas E.", ""], ["Kosmidis", "Ioannis", ""], ["Silva", "Ricardo", ""]]}, {"id": "1802.08161", "submitter": "Augustin Touron", "authors": "Augustin Touron (UP11, EDF R&D)", "title": "Consistency of the maximum likelihood estimator in seasonal hidden\n  Markov models", "comments": "arXiv admin note: text overlap with arXiv:1710.08112", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a variant of hidden Markov models in which the\ntransition probabilities between the states, as well as the emission\ndistributions, are not constant in time but vary in a periodic manner. This\nclass of models, that we will call seasonal hidden Markov models (SHMM) is\nparticularly useful in practice, as many applications involve a seasonal\nbehaviour. However, up to now, there is no theoretical result regarding this\nkind of model. We show that under mild assumptions, SHMM are identifiable: we\ncan identify the transition matrices and the emission distributions from the\njoint distribution of the observations on a period, up to state labelling. We\nalso give sufficient conditions for the strong consistency of the maximum\nlikelihood estimator (MLE). These results are applied to simulated data, using\nthe EM algorithm to compute the MLE. Finally, we show how SHMM can be used in\nreal world applications by applying our model to precipitation data, with\nmixtures of exponential distributions as emission distributions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:14:04 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Touron", "Augustin", "", "UP11, EDF R&D"]]}, {"id": "1802.08178", "submitter": "Thomas Welchowski M. Sc.", "authors": "Thomas Welchowski, Verena Zuber, Matthias Schmid", "title": "Correlation-Adjusted Regression Survival Scores for High-Dimensional\n  Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The development of classification methods for personalized\nmedicine is highly dependent on the identification of predictive genetic\nmarkers. In survival analysis it is often necessary to discriminate between\ninfluential and non-influential markers. Usually, the first step is to perform\na univariate screening step that ranks the markers according to their\nassociations with the outcome. It is common to perform screening using Cox\nscores, which quantify the associations between survival and each of the\nmarkers individually. Since Cox scores do not account for dependencies between\nthe markers, their use is suboptimal in the presence highly correlated markers.\nMethods: As an alternative to the Cox score, we propose the\ncorrelation-adjusted regression survival (CARS) score for right-censored\nsurvival outcomes. By removing the correlations between the markers, the CARS\nscore quantifies the associations between the outcome and the set of\n\"de-correlated\" marker values. Estimation of the scores is based on inverse\nprobability weighting, which is applied to log-transformed event times. For\nhigh-dimensional data, estimation is based on shrinkage techniques. Results:\nThe consistency of the CARS score is proven under mild regularity conditions.\nIn simulations, survival models based on CARS score rankings achieved higher\nareas under the precision-recall curve than competing methods. Two example\napplications on prostate and breast cancer confirmed these results. CARS scores\nare implemented in the R package carSurv. Conclusions: In research applications\ninvolving high-dimensional genetic data, the use of CARS scores for marker\nselection is a favorable alternative to Cox scores even when correlations\nbetween covariates are low. Having a straightforward interpretation and low\ncomputational requirements, CARS scores are an easy-to-use screening tool in\npersonalized medicine research.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:07:24 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 09:16:18 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Welchowski", "Thomas", ""], ["Zuber", "Verena", ""], ["Schmid", "Matthias", ""]]}, {"id": "1802.08229", "submitter": "Farouk Nathoo", "authors": "Farouk S. Nathoo, Robyn E. Kilshaw, Michael E. J. Masson", "title": "A Better (Bayesian) Interval Estimate for Within-Subject Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian highest-density interval (HDI) for use in\nwithin-subject designs. This credible interval is based on a standard\nnoninformative prior and a modified posterior distribution that conditions on\nboth the data and point estimates of the subject-specific random effects.\nConditioning on the estimated random effects removes between-subject variance\nand produces intervals that are the Bayesian analogue of the within-subject\nconfidence interval proposed in Loftus and Masson (1994). We show that the\nlatter interval can also be derived as a Bayesian within-subject HDI under a\ncertain improper prior. We argue that the proposed new interval is superior to\nthe original within-subject confidence interval, on the grounds of (a) it being\nbased on a more sensible prior, (b) it having a clear and intuitively appealing\ninterpretation, and (c) because its length is always smaller. A generalization\nof the new interval that can be applied to heteroscedastic data is also\nderived, and we show that the resulting interval is numerically equivalent to\nthe normalization method discussed in Franz and Loftus (2012); however, our\nwork provides a Bayesian formulation for the normalization method, and in doing\nso we identify the associated prior distribution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:31:57 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 17:22:07 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 18:01:11 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Nathoo", "Farouk S.", ""], ["Kilshaw", "Robyn E.", ""], ["Masson", "Michael E. J.", ""]]}, {"id": "1802.08242", "submitter": "Konstantin Usevich", "authors": "Jonathan Gillard and Konstantin Usevich", "title": "Structured low-rank matrix completion for forecasting in time series\n  analysis", "comments": "25 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the low-rank matrix completion problem with\nspecific application to forecasting in time series analysis. Briefly, the\nlow-rank matrix completion problem is the problem of imputing missing values of\na matrix under a rank constraint. We consider a matrix completion problem for\nHankel matrices and a convex relaxation based on the nuclear norm. Based on new\ntheoretical results and a number of numerical and real examples, we investigate\nthe cases when the proposed approach can work. Our results highlight the\nimportance of choosing a proper weighting scheme for the known observations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:56:27 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Gillard", "Jonathan", ""], ["Usevich", "Konstantin", ""]]}, {"id": "1802.08308", "submitter": "Qiwei Li", "authors": "Qiwei Li and Xinlei Wang and Faming Liang and Guanghua Xiao", "title": "A Bayesian Mark Interaction Model for Analysis of Tumor Pathology Images", "comments": "53 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of imaging technology, digital pathology imaging of tumor\ntissue slides is becoming a routine clinical procedure for cancer diagnosis.\nThis process produces massive imaging data that capture histological details in\nhigh resolution. Recent developments in deep-learning methods have enabled us\nto identify and classify individual cells from digital pathology images at\nlarge scale. The randomly distributed cells can be considered from a marked\npoint process, where each point is defined by its position and cell type.\nReliable statistical approaches to model such marked spatial point patterns can\nprovide new insight into tumor progression and shed light on the biological\nmechanisms of cancer. In this paper, we consider the problem of modeling\nspatial correlations among three commonly seen cells (i.e. lymphocyte, stromal,\nand tumor) observed in tumor pathology images. A novel marking model of marked\npoint processes, with interpretable underlying parameters (some of which are\nclinically meaningful), is proposed in a Bayesian framework. We use Markov\nchain Monte Carlo (MCMC) sampling techniques, combined with the double\nMetropolis-Hastings (DMH) algorithm, to sample from the posterior distribution\nwith an intractable normalizing constant. On the benchmark datasets, we\ndemonstrate how this model-based analysis can lead to sharper inferences than\nordinary exploratory analyses. Lastly, we conduct a case study on the pathology\nimages of 188 lung cancer patients from the National Lung Screening Trial. The\nresults show that the spatial correlation between tumor and stromal cells\npredicts patient prognosis. This statistical methodology not only presents a\nnew model for characterizing spatial correlations in a multi-type spatial point\npattern, but also provides a new perspective for understanding the role of\ncell-cell interactions in cancer progression.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 21:28:12 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Qiwei", ""], ["Wang", "Xinlei", ""], ["Liang", "Faming", ""], ["Xiao", "Guanghua", ""]]}, {"id": "1802.08339", "submitter": "Jan Terje Kval{\\o}y", "authors": "Jan Terje Kval{\\o}y and Bo Henry Lindqvist", "title": "A Class of Tests for Trend in Time Censored Recurrent Event Data", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical tests for trend in recurrent event data not following a Poisson\nprocess are generally constructed for event censored data. However, time\ncensored data are more frequently encountered in practice. In this paper we\ncontribute to filling an important gap in the literature on trend testing by\npresenting a class of statistical tests for trend in time censored recurrent\nevent data, based on the null hypothesis of a renewal process. The class of\ntests is constructed by an adaption of a functional central limit theorem for\nrenewal processes. By this approach a number of tests for time censored\nrecurrent event data can be constructed, including among others a version of\nthe classical Lewis-Robinson trend test and an Anderson-Darling type test. The\nlatter test turns out to have attractive properties for general use by having\ngood power properties against both monotonic and non-monotonic trends.\nExtensions to situations with several processes are considered. Properties of\nthe tests are studied by simulations, and the approach is illustrated in two\ndata examples.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 23:07:46 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Kval\u00f8y", "Jan Terje", ""], ["Lindqvist", "Bo Henry", ""]]}, {"id": "1802.08363", "submitter": "Ranjan Maitra", "authors": "Andrew Lithio and Ranjan Maitra", "title": "An efficient $k$-means-type algorithm for clustering datasets with\n  incomplete records", "comments": "21 pages, 12 figures, 3 tables, in press, Statistical Analysis and\n  Data Mining -- The ASA Data Science Journal, 2018", "journal-ref": null, "doi": "10.1002/sam.11392", "report-no": null, "categories": "stat.ML astro-ph.HE cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is arguably the most popular nonparametric clustering\nmethod but cannot generally be applied to datasets with incomplete records. The\nusual practice then is to either impute missing values under an assumed\nmissing-completely-at-random mechanism or to ignore the incomplete records, and\napply the algorithm on the resulting dataset. We develop an efficient version\nof the $k$-means algorithm that allows for clustering in the presence of\nincomplete records. Our extension is called $k_m$-means and reduces to the\n$k$-means algorithm when all records are complete. We also provide\ninitialization strategies for our algorithm and methods to estimate the number\nof groups in the dataset. Illustrations and simulations demonstrate the\nefficacy of our approach in a variety of settings and patterns of missing data.\nOur methods are also applied to the analysis of activation images obtained from\na functional Magnetic Resonance Imaging experiment.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:24:14 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 13:15:48 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lithio", "Andrew", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1802.08405", "submitter": "Yanjun Han", "authors": "Yanjun Han, Jiantao Jiao, Tsachy Weissman", "title": "Local moment matching: A unified methodology for symmetric functional\n  estimation and distribution estimation under Wasserstein distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\emph{Local Moment Matching (LMM)}, a unified methodology for\nsymmetric functional estimation and distribution estimation under Wasserstein\ndistance. We construct an efficiently computable estimator that achieves the\nminimax rates in estimating the distribution up to permutation, and show that\nthe plug-in approach of our unlabeled distribution estimator is \"universal\" in\nestimating symmetric functionals of discrete distributions. Instead of doing\nbest polynomial approximation explicitly as in existing literature of\nfunctional estimation, the plug-in approach conducts polynomial approximation\nimplicitly and attains the optimal sample complexity for the entropy, power sum\nand support size functionals.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:33:37 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 23:19:20 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Han", "Yanjun", ""], ["Jiao", "Jiantao", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1802.08417", "submitter": "Yanjun Han", "authors": "Yanjun Han, Ayfer \\\"Ozg\\\"ur, Tsachy Weissman", "title": "Geometric Lower Bounds for Distributed Parameter Estimation under\n  Communication Constraints", "comments": "This version (v4) added a new corollary on logistic regression, as\n  well as more discussions on sparse Gaussian mean estimation, compared to v3", "journal-ref": "published in COLT 2018", "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimation in distributed networks, where each sensor\nin the network observes an independent sample from an underlying distribution\nand has $k$ bits to communicate its sample to a centralized processor which\ncomputes an estimate of a desired parameter. We develop lower bounds for the\nminimax risk of estimating the underlying parameter for a large class of losses\nand distributions. Our results show that under mild regularity conditions, the\ncommunication constraint reduces the effective sample size by a factor of $d$\nwhen $k$ is small, where $d$ is the dimension of the estimated parameter.\nFurthermore, this penalty reduces at most exponentially with increasing $k$,\nwhich is the case for some models, e.g., estimating high-dimensional\ndistributions. For other models however, we show that the sample size reduction\nis re-mediated only linearly with increasing $k$, e.g. when some sub-Gaussian\nstructure is available. We apply our results to the distributed setting with\nproduct Bernoulli model, multinomial model, Gaussian location models, and\nlogistic regression which recover or strengthen existing results.\n  Our approach significantly deviates from existing approaches for developing\ninformation-theoretic lower bounds for communication-efficient estimation. We\ncircumvent the need for strong data processing inequalities used in prior work\nand develop a geometric approach which builds on a new representation of the\ncommunication constraint. This approach allows us to strengthen and generalize\nexisting results with simpler and more transparent proofs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 07:34:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 23:22:17 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 08:53:17 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 09:55:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Han", "Yanjun", ""], ["\u00d6zg\u00fcr", "Ayfer", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1802.08579", "submitter": "Carla Moreira", "authors": "Carla Moreira, Jacobo de U\\~na-\\'Alvarez and Roel Braekers", "title": "Nonparametric Estimation of a distribution function from doubly\n  truncated data under dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NPMLE of a distribution function from doubly truncated data was\nintroduced in the seminal paper of Efron and Petrosian. The consistency of the\nEfron-Petrosian estimator depends however on the assumption of independent\ntruncation. In this work we introduce an extension of the Efron-Petrosian NPMLE\nwhen the variable of interest and the truncation variables may be dependent.\nThe proposed estimator is constructed on the basis of a copula function which\nrepresents the dependence structure between the variable of interest and the\ntruncation variables. Two different iterative algorithms to compute the\nestimator in practice are introduced, and their performance is explored through\nan intensive Monte Carlo simulation study. We illustrate the use of the\nestimators on two real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:05:18 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 23:28:00 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 13:47:13 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Moreira", "Carla", ""], ["de U\u00f1a-\u00c1lvarez", "Jacobo", ""], ["Braekers", "Roel", ""]]}, {"id": "1802.08613", "submitter": "Dao Nguyen", "authors": "Dao Nguyen", "title": "Accelerate iterated filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In simulation-based inferences for partially observed Markov process models\n(POMP), the by-product of the Monte Carlo filtering is an approximation of the\nlog likelihood function. Recently, iterated filtering [14, 13] has originally\nbeen introduced and it has been shown that the gradient of the log likelihood\ncan also be approximated. Consequently, different stochastic optimization\nalgorithm can be applied to estimate the parameters of the underlying models.\nAs accelerated gradient is an efficient approach in the optimization\nliterature, we show that we can accelerate iterated filtering in the same\nmanner and inherit that high convergence rate while relaxing the restricted\nconditions of unbiased gradient approximation. We show that this novel\nalgorithm can be applied to both convex and non-convex log likelihood\nfunctions. In addition, this approach has substantially outperformed most of\nother previous approaches in a toy example and in a challenging scientific\nproblem of modeling infectious diseases.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:53:53 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Nguyen", "Dao", ""]]}, {"id": "1802.08622", "submitter": "Ngom Papa", "authors": "Jean Claude Utazirubanda, Tomas Leon, Papa Ngom", "title": "Variable selection via Group LASSO Approach : Application to the Cox\n  Regression and frailty model", "comments": "26 pages , 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survival outcome supplemented with both clinical\ninformation and high-dimensional gene expression data, use of the traditional\nCox proportional hazards model (1972) fails to meet some emerging needs in\nbiomedical research. First, the number of covariates is generally much larger\nthe sample size. Secondly, predicting an outcome based on individual gene\nexpression is inadequate because multiple biological processes and functional\npathways regulate the expression associated with a gene. Another challenge is\nthat the Cox model assumes that populations are homogenous, implying that all\nindividuals have the same risk of death, which is rarely true due to unmeasured\nrisk factors among populations. In this paper we propose group LASSO with\ngamma-distributed frailty for variable selection in Cox regression by extending\nprevious scholarship to account for heterogeneity among group structures\nrelated to exposure and susceptibility. The consistency property of the\nproposed method is established. This method is appropriate for addressing a\nwide variety of research questions from genetics to air pollution. Simulated\nanalysis shows promising performance by group LASSO compared with other\nmethods, including group SCAD and group MCP. Future directions include\nexpanding the use of frailty with adaptive group LASSO and sparse group LASS.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:28:15 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Utazirubanda", "Jean Claude", ""], ["Leon", "Tomas", ""], ["Ngom", "Papa", ""]]}, {"id": "1802.08727", "submitter": "Jeffrey Morris", "authors": "Wonyul Lee, Michelle F. Miranda, Phlip Rausch, Veerbhadran\n  Baladandayuthapani, Massimo Fazio, J. Crawford Downs, Jeffrey S. Morris", "title": "Bayesian Semiparametric Functional Mixed Models for Serially Correlated\n  Functional Data, with Application to Glaucoma Data", "comments": "paper accepted in Journal of the American Statistical Association,\n  2018 -- to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glaucoma, a leading cause of blindness, is characterized by optic nerve\ndamage related to intraocular pressure (IOP), but its full etiology is unknown.\nResearchers at UAB have devised a custom device to measure scleral strain\ncontinuously around the eye under fixed levels of IOP, which here is used to\nassess how strain varies around the posterior pole, with IOP, and across\nglaucoma risk factors such as age. The hypothesis is that scleral strain\ndecreases with age, which could alter biomechanics of the optic nerve head and\ncause damage that could eventually lead to glaucoma. To evaluate this\nhypothesis, we adapted Bayesian Functional Mixed Models to model these complex\ndata consisting of correlated functions on spherical scleral surface, with\nnonparametric age effects allowed to vary in magnitude and smoothness across\nthe scleral surface, multi-level random effect functions to capture\nwithin-subject correlation, and functional growth curve terms to capture serial\ncorrelation across IOPs that can vary around the scleral surface. Our method\nyields fully Bayesian inference on the scleral surface or any aggregation or\ntransformation thereof, and reveals interesting insights into the biomechanical\netiology of glaucoma. The general modeling framework described is very flexible\nand applicable to many complex, high-dimensional functional data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:30:44 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 14:51:11 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Lee", "Wonyul", ""], ["Miranda", "Michelle F.", ""], ["Rausch", "Phlip", ""], ["Baladandayuthapani", "Veerbhadran", ""], ["Fazio", "Massimo", ""], ["Downs", "J. Crawford", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "1802.08736", "submitter": "Dmitry Shemetov", "authors": "Kirill Paramonov, Dmitry Shemetov, James Sharpnack", "title": "Estimating Graphlet Statistics via Lifting", "comments": null, "journal-ref": "KDD '19 Proceedings of the 25th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining, July 2019, Pages 587-595", "doi": "10.1145/3292500.3330995", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory analysis over network data is often limited by the ability to\nefficiently calculate graph statistics, which can provide a model-free\nunderstanding of the macroscopic properties of a network. We introduce a\nframework for estimating the graphlet count---the number of occurrences of a\nsmall subgraph motif (e.g. a wedge or a triangle) in the network. For massive\ngraphs, where accessing the whole graph is not possible, the only viable\nalgorithms are those that make a limited number of vertex neighborhood queries.\nWe introduce a Monte Carlo sampling technique for graphlet counts, called {\\em\nLifting}, which can simultaneously sample all graphlets of size up to $k$\nvertices for arbitrary $k$. This is the first graphlet sampling method that can\nprovably sample every graphlet with positive probability and can sample\ngraphlets of arbitrary size $k$. We outline variants of lifted graphlet counts,\nincluding the ordered, unordered, and shotgun estimators, random walk starts,\nand parallel vertex starts. We prove that our graphlet count updates are\nunbiased for the true graphlet count and have a controlled variance for all\ngraphlets. We compare the experimental performance of lifted graphlet counts to\nthe state-of-the art graphlet sampling procedures: Waddling and the pairwise\nsubgraph random walk.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 20:57:28 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 20:39:48 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Paramonov", "Kirill", ""], ["Shemetov", "Dmitry", ""], ["Sharpnack", "James", ""]]}, {"id": "1802.08798", "submitter": "Dao Nguyen", "authors": "Dao Nguyen, Perry de Valpine, Yves Atchade, Daniel Turek, Nicholas\n  Michaud and Christopher Paciorek", "title": "Automatic adaptation of MCMC algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are ubiquitous tools for\nsimulation-based inference in many fields but designing and identifying good\nMCMC samplers is still an open question. This paper introduces a novel MCMC\nalgorithm, namely, Auto Adapt MCMC. For sampling variables or blocks of\nvariables, we use two levels of adaptation where the inner adaptation optimizes\nthe MCMC performance within each sampler, while the outer adaptation explores\nthe valid space of kernels to find the optimal samplers. We provide a\ntheoretical foundation for our approach. To show the generality and usefulness\nof the approach, we describe a framework using only standard MCMC samplers as\ncandidate samplers and some adaptation schemes for both inner and outer\niterations. In several benchmark problems, we show that our proposed approach\nsubstantially outperforms other approaches, including an automatic blocking\nalgorithm, in terms of MCMC efficiency and computational time.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 04:40:52 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 17:49:02 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Nguyen", "Dao", ""], ["de Valpine", "Perry", ""], ["Atchade", "Yves", ""], ["Turek", "Daniel", ""], ["Michaud", "Nicholas", ""], ["Paciorek", "Christopher", ""]]}, {"id": "1802.08895", "submitter": "Yueyong Shi", "authors": "Yueyong Shi, Jian Huang, Yuling Jiao, Qinglong Yang", "title": "A Semi-Smooth Newton Algorithm for High-Dimensional Nonconvex Sparse\n  Learning", "comments": "4th revision submitted to IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smoothly clipped absolute deviation (SCAD) and the minimax concave\npenalty (MCP) penalized regression models are two important and widely used\nnonconvex sparse learning tools that can handle variable selection and\nparameter estimation simultaneously, and thus have potential applications in\nvarious fields such as mining biological data in high-throughput biomedical\nstudies. Theoretically, these two models enjoy the oracle property even in the\nhigh-dimensional settings, where the number of predictors $p$ may be much\nlarger than the number of observations $n$. However, numerically, it is quite\nchallenging to develop fast and stable algorithms due to their non-convexity\nand non-smoothness. In this paper we develop a fast algorithm for SCAD and MCP\npenalized learning problems. First, we show that the global minimizers of both\nmodels are roots of the nonsmooth equations. Then, a semi-smooth Newton (SSN)\nalgorithm is employed to solve the equations. We prove that the SSN algorithm\nconverges locally and superlinearly to the Karush-Kuhn-Tucker (KKT) points.\nComputational complexity analysis shows that the cost of the SSN algorithm per\niteration is $O(np)$. Combined with the warm-start technique, the SSN algorithm\ncan be very efficient and accurate. Simulation studies and a real data example\nsuggest that our SSN algorithm, with comparable solution accuracy with the\ncoordinate descent (CD) and the difference of convex (DC) proximal Newton\nalgorithms, is more computationally efficient.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:08:22 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 01:31:58 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 08:51:01 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 19:29:31 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Shi", "Yueyong", ""], ["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Yang", "Qinglong", ""]]}, {"id": "1802.08952", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy", "title": "Efficient nonparametric causal inference with missing exposure\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing exposure information is a very common feature of many observational\nstudies. Here we study identifiability and efficient estimation of causal\neffects on vector outcomes, in such cases where treatment is unconfounded but\npartially missing. We consider a missing at random setting where missingness in\ntreatment can depend not only on complex covariates, but also on post-treatment\noutcomes. We give a new identifying expression for average treatment effects in\nthis setting, along with the efficient influence function for this parameter in\na nonparametric model, which yields a nonparametric efficiency bound. We use\nthis latter result to construct nonparametric estimators that are less\nsensitive to the curse of dimensionality than usual, e.g., by having faster\nrates of convergence than the complex nuisance estimators they rely on. Further\nwe show that these estimators can be root-n consistent and asymptotically\nnormal under weak nonparametric conditions, even when constructed using\nflexible machine learning. Finally we apply these results to the problem of\ncausal inference with a partially missing instrumental variable.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 04:13:51 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 01:47:50 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2020 03:23:15 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Kennedy", "Edward H.", ""]]}, {"id": "1802.08982", "submitter": "Adam Lund", "authors": "Adam Lund and Niels Richard Hansen", "title": "Sparse Network Estimation for Dynamical Spatio-temporal Array Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural field models represent neuronal communication on a population level\nvia synaptic weight functions. Using voltage sensitive dye (VSD) imaging it is\npossible to obtain measurements of neural fields with a relatively high spatial\nand temporal resolution. The synaptic weight functions represent functional\nconnectivity in the brain and give rise to a spatio-temporal dependence\nstructure. We present a stochastic functional differential equation for\nmodeling neural fields, which leads to a vector autoregressive model of the\ndata via basis expansions of the synaptic weight functions and time and space\ndiscretization. Fitting the model to data is a pratical challenge as this\nrepresents a large scale regression problem. By using a 1-norm penalty in\ncombination with localized basis functions it is possible to learn a sparse\nnetwork representation of the functional connectivity of the brain, but still,\nthe explicit construction of a design matrix can be computationally\nprohibitive. We demonstrate that by using tensor product basis expansions, the\ncomputation of the penalized estimator via a proximal gradient algorithm\nbecomes feasible. It is crucial for the computations that the data is organized\nin an array as is the case for the three dimensional VSD imaging data. This\nallows for the use of array arithmetic that is both memory and time\nefficient.The proposed method is implemented and showcased in the R package\ndynamo available from CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 10:30:34 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 21:01:30 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 09:42:22 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Lund", "Adam", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1802.09018", "submitter": "Daniel Zelterman", "authors": "Chang Yu and Daniel Zelterman", "title": "Distributions associated with simultaneous multiple hypothesis testing", "comments": "29 pages; 6 figures; 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the distribution of the number of hypotheses found to be\nstatistically significant using the rule from Benjamini and Hochberg (1995) for\ncontrolling the false discovery rate (FDR). This distribution has both a small\nsample form and an asymptotic expression for testing many independent\nhypotheses simultaneously. We propose a parametric distribution\n$\\,\\Psi_I(\\cdot)\\,$ to approximate the marginal distribution of p-values under\na non-uniform alternative hypothesis. This distribution is useful when there\nare many different alternative hypotheses and these are not individually well\nunderstood. We fit $\\,\\Psi_I\\,$ to data from three cancer studies and use it to\nillustrate the distribution of the number of notable hypotheses observed in\nthese examples. We model dependence of sampled p-values using a copula model\nand a latent variable approach. These methods can be combined to illustrate a\npower analysis in planning a large study on the basis of a smaller pilot study.\nWe show the number of statistically significant p-values behaves approximately\nas a mixture of a normal and the Borel-Tanner distribution.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 15:00:37 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Yu", "Chang", ""], ["Zelterman", "Daniel", ""]]}, {"id": "1802.09053", "submitter": "Yu Xiang", "authors": "Yu Xiang, Jie Ding, Vahid Tarokh", "title": "Estimation of the Evolutionary Spectra with Application to Stationarity\n  Test", "comments": "To appear in IEEE Transactions on Signal Processing. A short version\n  of this work appeared in ICASSP 2018", "journal-ref": null, "doi": "10.1109/TSP.2018.2890369", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new inference procedure for understanding\nnon-stationary processes, under the framework of evolutionary spectra developed\nby Priestley. Among various frameworks of modeling non-stationary processes,\nthe distinguishing feature of the evolutionary spectra is its focus on the\nphysical meaning of frequency. The classical estimate of the evolutionary\nspectral density is based on a double-window technique consisting of a\nshort-time Fourier transform and a smoothing. However, smoothing is known to\nsuffer from the so-called bias leakage problem. By incorporating Thomson's\nmultitaper method that was originally designed for stationary processes, we\npropose an improved estimate of the evolutionary spectral density, and analyze\nits bias/variance/resolution tradeoff. As an application of the new estimate,\nwe further propose a non-parametric rank-based stationarity test, and provide\nvarious experimental studies.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:11:03 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 02:11:46 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 06:22:57 GMT"}, {"version": "v4", "created": "Fri, 18 Jan 2019 04:22:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Xiang", "Yu", ""], ["Ding", "Jie", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1802.09057", "submitter": "Carlos Sevcik", "authors": "Carlos Sevcik", "title": "First derivatives at the optimum analysis (\\textit{fdao}): An approach\n  to estimate the uncertainty in nonlinear regression involving stochastically\n  independent variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem of optimization analysis surges when parameters such as\n$ \\{\\theta_j\\}_{j=1,\\, \\dots \\,,k }$, determining a function $\ny=f(x\\given\\{\\theta_j\\}) $, must be estimated from a set of observables $ \\{\nx_i,y_i\\}_{i=1,\\, \\dots \\,,m} $. Where $ \\{x_i\\} $ are independent variables\nassumed to be uncertainty-free. It is known that analytical solutions are\npossible if $ y=f(x\\given\\theta_j) $ is a linear combination of $\n\\{\\theta_{j=1,\\, \\dots \\,,k} \\}.$ Here it is proposed that determining the\nuncertainty of parameters that are not \\textit{linearly independent} may be\nachieved from derivatives $ \\tfrac{\\partial f(x \\given \\{\\theta_j\\})}{\\partial\n\\theta_j} $ at an optimum, if the parameters are \\textit{stochastically\nindependent}.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:37:57 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 22:02:50 GMT"}, {"version": "v3", "created": "Sun, 20 Jan 2019 15:35:24 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 11:13:21 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sevcik", "Carlos", ""]]}, {"id": "1802.09098", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, Michael Jordan", "title": "SAFFRON: an adaptive algorithm for online control of the false discovery\n  rate", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online false discovery rate (FDR) problem, one observes a possibly\ninfinite sequence of $p$-values $P_1,P_2,\\dots$, each testing a different null\nhypothesis, and an algorithm must pick a sequence of rejection thresholds\n$\\alpha_1,\\alpha_2,\\dots$ in an online fashion, effectively rejecting the\n$k$-th null hypothesis whenever $P_k \\leq \\alpha_k$. Importantly, $\\alpha_k$\nmust be a function of the past, and cannot depend on $P_k$ or any of the later\nunseen $p$-values, and must be chosen to guarantee that for any time $t$, the\nFDR up to time $t$ is less than some pre-determined quantity $\\alpha \\in\n(0,1)$. In this work, we present a powerful new framework for online FDR\ncontrol that we refer to as SAFFRON. Like older alpha-investing (AI)\nalgorithms, SAFFRON starts off with an error budget, called alpha-wealth, that\nit intelligently allocates to different tests over time, earning back some\nwealth on making a new discovery. However, unlike older methods, SAFFRON's\nthreshold sequence is based on a novel estimate of the alpha fraction that it\nallocates to true null hypotheses. In the offline setting, algorithms that\nemploy an estimate of the proportion of true nulls are called adaptive methods,\nand SAFFRON can be seen as an online analogue of the famous offline Storey-BH\nadaptive procedure. Just as Storey-BH is typically more powerful than the\nBenjamini-Hochberg (BH) procedure under independence, we demonstrate that\nSAFFRON is also more powerful than its non-adaptive counterparts, such as LORD\nand other generalized alpha-investing algorithms. Further, a monotone version\nof the original AI algorithm is recovered as a special case of SAFFRON, that is\noften more stable and powerful than the original. Lastly, the derivation of\nSAFFRON provides a novel template for deriving new online FDR rules.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 22:19:06 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 18:21:55 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Zrnic", "Tijana", ""], ["Wainwright", "Martin", ""], ["Jordan", "Michael", ""]]}, {"id": "1802.09116", "submitter": "Kashif Yousuf", "authors": "Kashif Yousuf and Yang Feng", "title": "Partial Distance Correlation Screening for High Dimensional Time Series", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional time series datasets are becoming increasingly common in\nvarious fields such as economics, finance, meteorology, and neuroscience. Given\nthis ubiquity of time series data, it is surprising that very few works on\nvariable screening discuss the time series setting, and even fewer works have\ndeveloped methods which utilize the unique features of time series data. This\npaper introduces several model free screening methods based on the partial\ndistance correlation and developed specifically to deal with time dependent\ndata. Methods are developed both for univariate models, such as nonlinear\nautoregressive models with exogenous predictors (NARX), and multivariate models\nsuch as linear or nonlinear VAR models. Sure screening properties are proved\nfor our methods, which depend on the moment conditions, and the strength of\ndependence in the response and covariate processes, amongst other factors.\nDependence is quantified by functional dependence measures (Wu [Proc. Natl.\nAcad. Sci. USA 102 (2005) 14150-14154]) and $\\beta$-mixing coefficients, and\nthe results rely on the use of Nagaev and Rosenthal type inequalities for\ndependent random variables. Finite sample performance of our methods is shown\nthrough extensive simulation studies, and we include an application to\nmacroeconomic forecasting.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 00:52:54 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 02:03:26 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 23:16:13 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yousuf", "Kashif", ""], ["Feng", "Yang", ""]]}, {"id": "1802.09117", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Jianqing Fan, Yinchu Zhu", "title": "Testability of high-dimensional linear models with non-sparse structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding statistical inference under possibly non-sparse\nhigh-dimensional models has gained much interest recently. For a given\ncomponent of the regression coefficient, we show that the difficulty of the\nproblem depends on the sparsity of the corresponding row of the precision\nmatrix of the covariates, not the sparsity of the regression coefficients. We\ndevelop new concepts of uniform and essentially uniform non-testability that\nallow the study of limitations of tests across a broad set of alternatives.\nUniform non-testability identifies a collection of alternatives such that the\npower of any test, against any alternative in the group, is asymptotically at\nmost equal to the nominal size. Implications of the new constructions include\nnew minimax testability results that, in sharp contrast to the current results,\ndo not depend on the sparsity of the regression parameters. We identify new\ntradeoffs between testability and feature correlation. In particular, we show\nthat, in models with weak feature correlations, minimax lower bound can be\nattained by a test whose power has the $\\sqrt{n}$ rate, regardless of the size\nof the model sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 01:00:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 18:49:17 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 02:15:24 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Bradic", "Jelena", ""], ["Fan", "Jianqing", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1802.09387", "submitter": "Whitney Huang", "authors": "Whitney K. Huang, Douglas W. Nychka, Hao Zhang", "title": "Estimating Precipitation Extremes using Log-Histospline", "comments": "32 pages, 13 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the commonly used approaches to modeling extremes is the\npeaks-over-threshold (POT) method. The POT method models exceedances over a\nthreshold that is sufficiently high or low so that the exceedance has\napproximately a generalized Pareto distribution (GPD). This method requires the\nselection of a threshold that might affect the estimates. Here we propose an\nalternative method, the Log-Histospline (LHSpline), to explore modeling the\ntail behavior and the remainder of the density in one step using the full range\nof the data. LHSpline applies a smoothing spline model to a finely binned\nhistogram of the log transformed data to estimate its log density. By\nconstruction, a LHSpline estimation is constrained to have polynomial tail\nbehavior, a feature commonly observed in daily rainfall observations. We\nillustrate the LHSpline method by analyzing precipitation data collected in\nHouston, Texas.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:16:18 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 20:50:58 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 17:23:55 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Huang", "Whitney K.", ""], ["Nychka", "Douglas W.", ""], ["Zhang", "Hao", ""]]}, {"id": "1802.09388", "submitter": "Peter Dutey-Magni PhD", "authors": "Peter Dutey-Magni", "title": "Bayesian Sample Size Determination for Planning Hierarchical Bayes Small\n  Area Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper devises a fully Bayesian sample size determination method for\nhierarchical model-based small area estimation with a decision risk approach. A\nnew loss function specified around a desired maximum posterior variance target\nimplements conventional official statistics criteria of estimator reliability\n(coefficient of variation of up to 20 per cent). This approach comes with an\nefficient binary search algorithm identifying the minimum effective sample size\nneeded to produce small area estimates under this threshold constraint.\nTraditional survey sampling design tools can then be used to plan appropriate\ndata collection using the resulting effective sample size target. This approach\nis illustrated in a case study on small area prevalence of life limiting health\nproblems for 6 age groups across 1,956 small areas in Northern England, using\nthe recently developed Integrated Nested Laplace Approximation method for\nspatial generalised linear mixed hierarchical models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:17:39 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Dutey-Magni", "Peter", ""]]}, {"id": "1802.09479", "submitter": "Weixin Cai", "authors": "Weixin Cai and Mark J. van der Laan", "title": "One-step Targeted Maximum Likelihood for Time-to-event Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current Targeted Maximum Likelihood Estimation (TMLE) methods used to analyze\ntime-to-event data estimate the survival probability for each time point\nseparately, which result in estimates that are not necessarily monotone. In\nthis paper, we present an extension of TMLE for observational time-to-event\ndata, the one-step Targeted Maximum Likelihood Estimator for the treatment-rule\nspecific survival curve. We construct a one-dimensional universal least\nfavorable submodel that targets the entire survival curve, and thereby requires\nminimal extra fitting with data to achieve its goal of solving the efficient\ninfluence curve equation. Through the use of a simulation study, we will show\nthat this method improves on previously proposed methods in both robustness and\nefficiency, and at the same time respects the monotone decreasing nature of the\nsurvival curve.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:55:10 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 19:55:53 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 18:43:02 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Cai", "Weixin", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1802.09509", "submitter": "Paulo Serra", "authors": "Paulo Serra, Michel Mandjes", "title": "Estimation of Local Degree Distributions via Local Weighted Averaging\n  and Monte Carlo Cross-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to their capability of summarising interactions between elements of a\nsystem, networks have become a common type of data in many fields. As networks\ncan be inhomogeneous, in that different regions of the network may exhibit\ndifferent topologies, an important topic concerns their local properties. This\npaper focuses on the estimation of the local degree distribution of a vertex in\nan inhomogeneous network. The contributions are twofold: we propose an\nestimator based on local weighted averaging, and we set up a Monte Carlo\ncross-validation procedure to pick the parameters of this estimator. Under a\nspecific modelling assumption we derive an oracle inequality that shows how the\nmodel parameters affect the precision of the estimator. We illustrate our\nmethod by several numerical experiments, on both real and synthetic data,\nshowing in particular that the approach considerably improves upon the natural,\nempirical estimator.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 18:52:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Serra", "Paulo", ""], ["Mandjes", "Michel", ""]]}, {"id": "1802.09565", "submitter": "Daniele Durante", "authors": "Daniele Durante", "title": "Conjugate Bayes for probit regression via unified skew-normal\n  distributions", "comments": null, "journal-ref": "Biometrika (2019). 106, 765-779", "doi": "10.1093/biomet/asz034", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models for dichotomous data are ubiquitous in statistics. Besides\nbeing useful for inference on binary responses, these methods serve also as\nbuilding blocks in more complex formulations, such as density regression,\nnonparametric classification and graphical models. Within the Bayesian\nframework, inference proceeds by updating the priors for the coefficients,\ntypically set to be Gaussians, with the likelihood induced by probit or logit\nregressions for the responses. In this updating, the apparent absence of a\ntractable posterior has motivated a variety of computational methods, including\nMarkov Chain Monte Carlo routines and algorithms which approximate the\nposterior. Despite being routinely implemented, Markov Chain Monte Carlo\nstrategies face mixing or time-inefficiency issues in large p and small n\nstudies, whereas approximate routines fail to capture the skewness typically\nobserved in the posterior. This article proves that the posterior distribution\nfor the probit coefficients has a unified skew-normal kernel, under Gaussian\npriors. Such a novel result allows efficient Bayesian inference for a wide\nclass of applications, especially in large p and small-to-moderate n studies\nwhere state-of-the-art computational methods face notable issues. These\nadvances are outlined in a genetic study, and further motivate the development\nof a wider class of conjugate priors for probit models along with methods to\nobtain independent and identically distributed samples from the unified\nskew-normal posterior.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 19:29:27 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 13:37:47 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 15:41:21 GMT"}, {"version": "v4", "created": "Tue, 23 Oct 2018 10:13:05 GMT"}, {"version": "v5", "created": "Sun, 17 Nov 2019 18:11:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""]]}, {"id": "1802.09582", "submitter": "Ben Parker", "authors": "Ben M. Parker, Steven G Gilmour and Vasiliki Koutra", "title": "A graph-theoretic framework for algorithmic design of experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate that considering experiments in a\ngraph-theoretic manner allows us to exploit automorphisms of the graph to\nreduce the number of evaluations of candidate designs for those experiments,\nand thus find optimal designs faster. We show that the use of automorphisms for\nreducing the number of evaluations required of an optimality criterion function\nis effective on designs where experimental units have a network structure.\nMoreover, we show that we can take block designs with no apparent network\nstructure, such as one-way blocked experiments, row-column experiments, and\ncrossover designs, and add block nodes to induce a network structure.\nConsidering automorphisms can thus reduce the amount of time it takes to find\noptimal designs for a wide class of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:13:14 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Parker", "Ben M.", ""], ["Gilmour", "Steven G", ""], ["Koutra", "Vasiliki", ""]]}, {"id": "1802.09585", "submitter": "Nicole Barthel", "authors": "Nicole Barthel, Claudia Czado and Yarema Okhrin", "title": "A partial correlation vine based approach for modeling and forecasting\n  multivariate volatility time-series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for dynamic modeling and forecasting of realized covariance\nmatrices is proposed. Realized variances and realized correlation matrices are\njointly estimated. The one-to-one relationship between a positive definite\ncorrelation matrix and its associated set of partial correlations corresponding\nto any vine specification is used for data transformation. The model components\ntherefore are realized variances as well as realized standard and partial\ncorrelations corresponding to a daily log-return series. As such, they have a\nclear practical interpretation. A method to select a regular vine structure,\nwhich allows for parsimonious time-series and dependence modeling of the model\ncomponents, is introduced. Being algebraically independent the latter do not\nunderlie any algebraic constraint. The proposed model approach is outlined in\ndetail and motivated along with a real data example on six highly liquid\nstocks. The forecasting performance is evaluated both with respect to\nstatistical precision and in the context of portfolio optimization. Comparisons\nwith Cholesky decomposition based benchmark models support the excellent\nprediction ability of the proposed model approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:15:16 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 14:20:25 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Barthel", "Nicole", ""], ["Czado", "Claudia", ""], ["Okhrin", "Yarema", ""]]}, {"id": "1802.09631", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi, Ian H. Jermyn, Jochen Einbeck", "title": "Bayesian shape modelling of cross-sectional geological data", "comments": "4 pages, 1 figure, In proceedings 29th International Workshop on\n  Statistical Modelling, 14-18 July 2014, Gottingen, Germany. Amsterdam:\n  Statistical Modelling Society, pp. 161-164", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape information is of great importance in many applications. For example,\nthe oil-bearing capacity of sand bodies, the subterranean remnants of ancient\nrivers, is related to their cross-sectional shapes. The analysis of these\nshapes is therefore of some interest, but current classifications are\nsimplistic and ad hoc. In this paper, we describe the first steps towards a\ncoherent statistical analysis of these shapes by deriving the integrated\nlikelihood for data shapes given class parameters. The result is of interest\nbeyond this particular application.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 22:28:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Tsiftsi", "Thomai", ""], ["Jermyn", "Ian H.", ""], ["Einbeck", "Jochen", ""]]}, {"id": "1802.09642", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele, Alex R. Luedtke, Mark J. van der Laan, Ronald C.\n  Kessler", "title": "Selecting optimal subgroups for treatment using many covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting the optimal subgroup to treat when data\non covariates is available from a randomized trial or observational study. We\ndistinguish between four different settings including (i) treatment selection\nwhen resources are constrained, (ii) treatment selection when resources are not\nconstrained, (iii) treatment selection in the presence of side effects and\ncosts, and (iv) treatment selection to maximize effect heterogeneity. We show\nthat, in each of these cases, the optimal treatment selection rule involves\ntreating those for whom the predicted mean difference in outcomes comparing\nthose with versus without treatment, conditional on covariates, exceeds a\ncertain threshold. The threshold varies across these four scenarios but the\nform of the optimal treatment selection rule does not. The results suggest a\nmove away from traditional subgroup analysis for personalized medicine. New\nrandomized trial designs are proposed so as to implement and make use of\noptimal treatment selection rules in health care practice.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 23:38:39 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Luedtke", "Alex R.", ""], ["van der Laan", "Mark J.", ""], ["Kessler", "Ronald C.", ""]]}, {"id": "1802.09650", "submitter": "Scott Sisson", "authors": "Y. Fan and S. A. Sisson", "title": "ABC Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"ABC Samplers\", is to appear in the forthcoming Handbook of\nApproximate Bayesian Computation (2018). It details the main ideas and\nalgorithms used to sample from the ABC approximation to the posterior\ndistribution, including methods based on rejection/importance sampling, MCMC\nand sequential Monte Carlo.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 23:57:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.09667", "submitter": "Menghao Xu", "authors": "Menghao Xu, Zhou Yu, Jun Shao", "title": "Sufficient variable screening via directional regression with censored\n  response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We in this paper propose a directional regression based approach for\nultrahigh dimensional sufficient variable screening with censored responses.\nThe new method is designed in a model-free manner and thus can be adapted to\nvarious complex model structures. Under some commonly used assumptions, we show\nthat the proposed method enjoys the sure screening property when the dimension\np diverges at an exponential rate of the sample size n. To improve the marginal\nscreening method, the corresponding iterative screening algorithm and stability\nscreening algorithm are further equipped. We demonstrate the effectiveness of\nthe proposed method through simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 01:15:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Xu", "Menghao", ""], ["Yu", "Zhou", ""], ["Shao", "Jun", ""]]}, {"id": "1802.09673", "submitter": "Daniel Zelterman", "authors": "Daniel Zelterman", "title": "The maximum negative hypergeometric distribution", "comments": "25 pages, 6 figures; 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An urn contains a known number of balls of two different colors. We describe\nthe random variable counting the smallest number of draws needed in order to\nobserve at least $\\,c\\,$ of both colors when sampling without replacement for a\npre-specified value of $\\,c=1,2,\\ldots\\,$. This distribution is the finite\nsample analogy to the maximum negative binomial distribution described by\nZhang, Burtness, and Zelterman (2000). We describe the modes, approximating\ndistributions, and estimation of the contents of the urn.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 15:31:22 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zelterman", "Daniel", ""]]}, {"id": "1802.09684", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Network Representation Using Graph Root Distributions", "comments": "47 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable random graphs serve as an important probabilistic framework for\nthe statistical analysis of network data. In this work we develop an\nalternative parameterization for a large class of exchangeable random graphs,\nwhere the nodes are independent random vectors in a linear space equipped with\nan indefinite inner product, and the edge probability between two nodes equals\nthe inner product of the corresponding node vectors. Therefore, the\ndistribution of exchangeable random graphs in this subclass can be represented\nby a node sampling distribution on this linear space, which we call the graph\nroot distribution. We study existence and identifiability of such\nrepresentations, the topological relationship between the graph root\ndistribution and the exchangeable random graph sampling distribution, and\nestimation of graph root distributions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 02:12:30 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:06:52 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1802.09720", "submitter": "Scott Sisson", "authors": "S. A. Sisson and Y. Fan and M. A. Beaumont", "title": "Overview of Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"Overview of Approximate Bayesian Computation\", is to appear as\nthe first chapter in the forthcoming Handbook of Approximate Bayesian\nComputation (2018). It details the main ideas and concepts behind ABC methods\nwith many examples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:18:12 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Sisson", "S. A.", ""], ["Fan", "Y.", ""], ["Beaumont", "M. A.", ""]]}, {"id": "1802.09725", "submitter": "Scott Sisson", "authors": "D. J. Nott and V. M.-H. Ong and Y. Fan and S. A. Sisson", "title": "High-dimensional ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"High-dimensional ABC\", is to appear in the forthcoming\nHandbook of Approximate Bayesian Computation (2018). It details the main ideas\nand concepts behind extending ABC methods to higher dimensions, with supporting\nexamples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:47:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Nott", "D. J.", ""], ["Ong", "V. M. -H.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.09977", "submitter": "Johan Segers", "authors": "Ma\\\"el Chiapino and Anne Sabourin and Johan Segers", "title": "Identifying groups of variables with the potential of being large\n  simultaneously", "comments": "23 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying groups of variables that may be large simultaneously amounts to\nfinding out which joint tail dependence coefficients of a multivariate\ndistribution are positive. The asymptotic distribution of a vector of\nnonparametric, rank-based estimators of these coefficients justifies a stopping\ncriterion in an algorithm that searches the collection of all possible groups\nof variables in a systematic way, from smaller groups to larger ones. The issue\nthat the tolerance level in the stopping criterion should depend on the size of\nthe groups is circumvented by the use of a conditional tail dependence\ncoefficient. Alternatively, such stopping criteria can be based on limit\ndistributions of rank-based estimators of the coefficient of tail dependence,\nquantifying the speed of decay of joint survival functions. Numerical\nexperiments indicate that the algorithm's effectiveness for detecting\ntail-dependent groups of variables is highest when paired with a criterion\nbased on a Hill-type estimator of the coefficient of tail dependence.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 15:48:59 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Chiapino", "Ma\u00ebl", ""], ["Sabourin", "Anne", ""], ["Segers", "Johan", ""]]}, {"id": "1802.09996", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai", "title": "Exact Simulation of reciprocal Archimedean copulas", "comments": null, "journal-ref": "Statistics and Probability Letters 141C, 68-73, 2018", "doi": "10.1016/j.spl.2018.05.020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decreasing enumeration of the points of a Poisson random measure whose\nmean measure has finite survival function on the positive half-axis can be\nrepresented as a non-increasing function of the jump times of a standard\nPoisson process. This observation allows to generalize the essential idea from\na well-known exact simulation algorithm for arbitrary extreme-value copulas to\ncopulas of a more general family of max-infinitely divisible distributions,\nwith reciprocal Archimedean copulas being a particular example.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:13:11 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 12:46:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mai", "Jan-Frederik", ""]]}, {"id": "1802.10024", "submitter": "Ryan Martin", "authors": "Yi Lin, Ryan Martin, Min Yang", "title": "On optimal designs for non-regular models", "comments": "23 pages of text; 3 figures; 11 pages of supplementary material", "journal-ref": "Annals of Statistics, 2019, volume 47, pages 3335--3359", "doi": "10.1214/18-AOS1780", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classically, Fisher information is the relevant object in defining optimal\nexperimental designs. However, for models that lack certain regularity, the\nFisher information does not exist and, hence, there is no notion of design\noptimality available in the literature. This article seeks to fill the gap by\nproposing a so-called Hellinger information, which generalizes Fisher\ninformation in the sense that the two measures agree in regular problems, but\nthe former also exists for certain types of non-regular problems. We derive a\nHellinger information inequality, showing that Hellinger information defines a\nlower bound on the local minimax risk of estimators. This provides a connection\nbetween features of the underlying model---in particular, the design---and the\nperformance of estimators, motivating the use of this new Hellinger information\nfor non-regular optimal design problems. Hellinger optimal designs are derived\nfor several non-regular regression problems, with numerical results empirically\ndemonstrating the efficiency of these designs compared to alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 17:06:56 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2019 17:26:14 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Lin", "Yi", ""], ["Martin", "Ryan", ""], ["Yang", "Min", ""]]}, {"id": "1802.10065", "submitter": "Marina Riabiz Ms.", "authors": "Marina Riabiz, Tohid Ardeshiri, Ioannis Kontoyiannis, Simon Godsill", "title": "Nonasymptotic Gaussian Approximation for Inference with Stable Noise", "comments": "V1: 41 pages, 16 figures. V2: Text typos fixed; redundant figures\n  from main text and appendices removed; added references in section I; changed\n  section VI and its proofs in Appendices C-D-E; improved section IX; removed\n  section X (Discussion and Conclusion), reference style changed. V3: title in\n  the metadata updated. V4: Updated Theorem 1 and its proof, global revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of a series of theoretical studies are reported, examining the\nconvergence rate for different approximate representations of $\\alpha$-stable\ndistributions. Although they play a key role in modelling random processes with\njumps and discontinuities, the use of $\\alpha$-stable distributions in\ninference often leads to analytically intractable problems. The LePage series,\nwhich is a probabilistic representation employed in this work, is used to\ntransform an intractable, infinite-dimensional inference problem into a\nconditionally Gaussian parametric problem. A major component of our approach is\nthe approximation of the tail of this series by a Gaussian random variable.\nStandard statistical techniques, such as Expectation-Maximization, Markov chain\nMonte Carlo, and Particle Filtering, can then be applied. In addition to the\nasymptotic normality of the tail of this series, we establish explicit,\nnonasymptotic bounds on the approximation error. Their proofs follow classical\nFourier-analytic arguments, using Ess\\'{e}en's smoothing lemma. Specifically,\nwe consider the distance between the distributions of: $(i)$~the tail of the\nseries and an appropriate Gaussian; $(ii)$~the full series and the truncated\nseries; and $(iii)$~the full series and the truncated series with an added\nGaussian term. In all three cases, sharp bounds are established, and the\ntheoretical results are compared with the actual distances (computed\nnumerically) in specific examples of symmetric $\\alpha$-stable distributions.\nThis analysis facilitates the selection of appropriate truncations in practice\nand offers theoretical guarantees for the accuracy of resulting estimates. One\nof the main conclusions obtained is that, for the purposes of inference, the\nuse of a truncated series together with an approximately Gaussian error term\nhas superior statistical properties and is likely a preferable choice in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 18:43:43 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 15:58:22 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 14:57:40 GMT"}, {"version": "v4", "created": "Wed, 1 Jan 2020 10:44:34 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Riabiz", "Marina", ""], ["Ardeshiri", "Tohid", ""], ["Kontoyiannis", "Ioannis", ""], ["Godsill", "Simon", ""]]}, {"id": "1802.10245", "submitter": "Zheng Chen", "authors": "Dong Han, Zheng Chen and Yawen Hou", "title": "Sample size for a non-inferiority clinical trial with time-to-event data\n  in the presence of competing risks", "comments": "24 pages, 1 figure", "journal-ref": "Journal of Biopharmaceutical Statistics, 2018, 28:797-807", "doi": "10.1080/10543406.2017.1399897", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis and planning methods for competing risks model have been\ndescribed in the literatures in recent decades, and non-inferiority clinical\ntrials are helpful in current pharmaceutical practice. Analytical methods for\nnon-inferiority clinical trials in the presence of competing risks were\ninvestigated by Parpia et al., who indicated that the proportional\nsub-distribution hazard model is appropriate in the context of biological\nstudies. However, the analytical methods of competing risks model differ from\nthose appropriate for analyzing non-inferiority clinical trials with a single\noutcome; thus, a corresponding method for planning such trials is necessary. A\nsample size formula for non-inferiority clinical trials in the presence of\ncompeting risks based on the proportional sub-distribution hazard model is\npresented in this paper. The primary endpoint relies on the sub-distribution\nhazard ratio. A total of 120 simulations and an example based on a randomized\ncontrolled trial verified the empirical performance of the presented formula.\nThe results demonstrate that the empirical power of sample size formulas based\non the Weibull distribution for non-inferiority clinical trials with competing\nrisks can reach the targeted power.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:11:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Dong", ""], ["Chen", "Zheng", ""], ["Hou", "Yawen", ""]]}, {"id": "1802.10254", "submitter": "Tomoyuki Obuchi", "authors": "Tomoyuki Obuchi, Yoshiyuki Kabashima", "title": "Semi-Analytic Resampling in Lasso", "comments": "33 pages, 10 figures, MATLAB codes implementing the proposed method\n  are distributed in https://github.com/T-Obuchi/AMPR_lasso_matlab", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate method for conducting resampling in Lasso, the $\\ell_1$\npenalized linear regression, in a semi-analytic manner is developed, whereby\nthe average over the resampled datasets is directly computed without repeated\nnumerical sampling, thus enabling an inference free of the statistical\nfluctuations due to sampling finiteness, as well as a significant reduction of\ncomputational time. The proposed method is based on a message passing type\nalgorithm, and its fast convergence is guaranteed by the state evolution\nanalysis, when covariates are provided as zero-mean independently and\nidentically distributed Gaussian random variables. It is employed to implement\nbootstrapped Lasso (Bolasso) and stability selection, both of which are\nvariable selection methods using resampling in conjunction with Lasso, and\nresolves their disadvantage regarding computational cost. To examine\napproximation accuracy and efficiency, numerical experiments were carried out\nusing simulated datasets. Moreover, an application to a real-world dataset, the\nwine quality dataset, is presented. To process such real-world datasets, an\nobjective criterion for determining the relevance of selected variables is also\nintroduced by the addition of noise variables and resampling.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 03:49:34 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 06:46:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Obuchi", "Tomoyuki", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1802.10311", "submitter": "Maksym Byshkin", "authors": "Maksym Byshkin, Alex Stivala, Antonietta Mira, Garry Robins and\n  Alessandro Lomi", "title": "Fast Maximum Likelihood estimation via Equilibrium Expectation for Large\n  Network Data", "comments": "Final version", "journal-ref": "Scientific Reports | (2018) 8:11509\n  https://www.nature.com/articles/s41598-018-29725-8", "doi": "10.1038/s41598-018-29725-8", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major line of contemporary research on complex networks is based on the\ndevelopment of statistical models that specify the local motifs associated with\nmacro-structural properties observed in actual networks. This statistical\napproach becomes increasingly problematic as network size increases. In the\ncontext of current research on efficient estimation of models for large network\ndata sets, we propose a fast algorithm for maximum likelihood estimation (MLE)\nthat afords a signifcant increase in the size of networks amenable to direct\nempirical analysis. The algorithm we propose in this paper relies on properties\nof Markov chains at equilibrium, and for this reason it is called equilibrium\nexpectation (EE). We demonstrate the performance of the EE algorithm in the\ncontext of exponential random graphmodels (ERGMs) a family of statistical\nmodels commonly used in empirical research based on network data observed at a\nsingle period in time. Thus far, the lack of efcient computational strategies\nhas limited the empirical scope of ERGMs to relatively small networks with a\nfew thousand nodes. The approach we propose allows a dramatic increase in the\nsize of networks that may be analyzed using ERGMs. This is illustrated in an\nanalysis of several biological networks and one social network with 104,103\nnodes\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:02:26 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:03:41 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Byshkin", "Maksym", ""], ["Stivala", "Alex", ""], ["Mira", "Antonietta", ""], ["Robins", "Garry", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1802.10330", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai", "title": "Extreme-value copulas associated with the expected scaled maximum of\n  independent random variables", "comments": null, "journal-ref": "Journal of Multivariate Analysis 166, 50-61, 2018", "doi": "10.1016/j.jmva.2018.02.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the expected scaled maximum of non-negative random\nvariables with unit mean defines a stable tail dependence function associated\nwith some extreme-value copula. In the special case when these random variables\nare independent and identically distributed, min-stable multivariate\nexponential random vectors with the associated survival extreme-value copulas\nare shown to arise as finite-dimensional margins of an infinite exchangeable\nsequence in the sense of De Finetti's Theorem. The associated latent factor is\na stochastic process which is strongly infinitely divisible with respect to\ntime, which induces a bijection from the set of distribution functions F of\nnon-negative random variables with finite mean to the set of L\\'evy measures on\nthe positive half-axis. Since the Gumbel and the Galambos copula are the most\npopular examples of this construction, the investigation of this bijection\ncontributes to a further understanding of their well-known analytical\nsimilarities. Furthermore, a simulation algorithm based on the latent factor\nrepresentation is developed, if the support of F is bounded. Especially in\nlarge dimensions, this algorithm is efficient because it makes use of the De\nFinetti structure.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 09:51:43 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Mai", "Jan-Frederik", ""]]}, {"id": "1802.10346", "submitter": "Rose Baker", "authors": "Rose Baker", "title": "A flexible and computationally tractable discrete distribution derived\n  from a stationary renewal process", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of discrete distributions can be derived from stationary renewal\nprocesses. They have the useful property that the mean is a simple function of\nthe model parameters. Thus regressions of the distribution mean on covariates\ncan be carried out and marginal effects of covariates calculated. Probabilities\ncan be easily computed in closed form for only two such distributions, when the\nevent interarrival times in the renewal process follow either a gamma or an\ninverse Gaussian distribution. The gamma-based distribution has more attractive\nproperties and is described and fitted to data. The inverse-Gaussian based\ndistribution is also briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 10:33:20 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Baker", "Rose", ""]]}, {"id": "1802.10490", "submitter": "Paul Novosad", "authors": "Sam Asher, Paul Novosad, Charlie Rafkin", "title": "Partial Identification of Expectations with Interval Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional expectation function (CEF) can at best be partially identified\nwhen the conditioning variable is interval censored. When the number of bins is\nsmall, existing methods often yield minimally informative bounds. We propose\nthree innovations that make meaningful inference possible in interval data\ncontexts. First, we prove novel nonparametric bounds for contexts where the\ndistribution of the censored variable is known. Second, we show that a class of\nmeasures that describe the conditional mean across a fixed interval of the\nconditioning space can often be bounded tightly even when the CEF itself\ncannot. Third, we show that a constraint on CEF curvature can either tighten\nbounds or can substitute for the monotonicity assumption often made in interval\ndata applications. We derive analytical bounds that use the first two\ninnovations, and develop a numerical method to calculate bounds under the\nthird. We show the performance of the method in simulations and then present\ntwo applications. First, we resolve a known problem in the estimation of\nmortality as a function of education: because individuals with high school or\nless are a smaller and thus more negatively selected group over time, estimates\nof their mortality change are likely to be biased. Our method makes it possible\nto hold education rank bins constant over time, revealing that current\nestimates of rising mortality for less educated women are biased upward in some\ncases by a factor of three. Second, we apply the method to the estimation of\nintergenerational mobility, where researchers frequently use coarsely measured\neducation data in the many contexts where matched parent-child income data are\nunavailable. Conventional measures like the rank-rank correlation may be\nuninformative once interval censoring is taken into account; CEF interval-based\nmeasures of mobility are bounded tightly.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 15:52:05 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Asher", "Sam", ""], ["Novosad", "Paul", ""], ["Rafkin", "Charlie", ""]]}, {"id": "1802.10570", "submitter": "Thomai Tsiftsi", "authors": "Thomai Tsiftsi", "title": "Statistical shape analysis in a Bayesian framework for shapes in two and\n  three dimensions", "comments": "6 pages, 1 figure", "journal-ref": "In proceedings 31st International Workshop on Statistical\n  Modelling, 4-8 July 2016, Rennes, France. Amsterdam: Statistical Modelling\n  Society, pp. 309-314", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel shape classification method which is\nembedded in the Bayesian paradigm. We discuss the modelling and the resulting\nshape classification algorithm for two and three dimensional data shapes. We\nconclude by evaluating the efficiency and efficacy of the proposed algorithm on\nthe Kimia shape database for the two dimensional case.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:20:12 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Tsiftsi", "Thomai", ""]]}]