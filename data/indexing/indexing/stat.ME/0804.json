[{"id": "0804.0031", "submitter": "Peter Hoff", "authors": "Peter Hoff", "title": "A hierarchical eigenmodel for pooled covariance estimation", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a set of covariance matrices corresponding to different populations are\nunlikely to be exactly equal they can still exhibit a high degree of\nsimilarity. For example, some pairs of variables may be positively correlated\nacross most groups, while the correlation between other pairs may be\nconsistently negative. In such cases much of the similarity across covariance\nmatrices can be described by similarities in their principal axes, the axes\ndefined by the eigenvectors of the covariance matrices. Estimating the degree\nof across-population eigenvector heterogeneity can be helpful for a variety of\nestimation tasks. Eigenvector matrices can be pooled to form a central set of\nprincipal axes, and to the extent that the axes are similar, covariance\nestimates for populations having small sample sizes can be stabilized by\nshrinking their principal axes towards the across-population center. To this\nend, this article develops a hierarchical model and estimation procedure for\npooling principal axes across several populations. The model for the\nacross-group heterogeneity is based on a matrix-valued antipodally symmetric\nBingham distribution that can flexibly describe notions of ``center'' and\n``spread'' for a population of orthonormal matrices.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2008 21:37:18 GMT"}], "update_date": "2008-04-02", "authors_parsed": [["Hoff", "Peter", ""]]}, {"id": "0804.0380", "submitter": "Frank Porter", "authors": "Frank C. Porter", "title": "Testing Consistency of Two Histograms", "comments": "35 pages, 15 figures, 5 tables. Plain TeX. Uses graphicx external\n  package. For better quality figures, go to:\n  http://www.hep.caltech.edu/~fcp/statistics/hypothesisTest/PoissonConsistency/PoissonConsistency.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an hep-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches to testing the hypothesis that two histograms are drawn\nfrom the same distribution are investigated. We note that single-sample\ncontinuous distribution tests may be adapted to this two-sample grouped data\nsituation. The difficulty of not having a fully-specified null hypothesis is an\nimportant consideration in the general case, and care is required in estimating\nprobabilities with ``toy'' Monte Carlo simulations. The performance of several\ncommon tests is compared; no single test performs best in all situations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2008 19:52:25 GMT"}], "update_date": "2008-04-03", "authors_parsed": [["Porter", "Frank C.", ""]]}, {"id": "0804.1038", "submitter": "Sofia Olhede Professor", "authors": "Heidi Hindberg and Sofia C. Olhede", "title": "Estimation of Ambiguity Functions With Limited Spread", "comments": "various small changes", "journal-ref": null, "doi": null, "report-no": "Stat Sci 293", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new estimation procedure for the ambiguity function of\na non-stationary time series. The stochastic properties of the empirical\nambiguity function calculated from a single sample in time are derived.\nDifferent thresholding procedures are introduced for the estimation of the\nambiguity function. Such estimation methods are suitable if the ambiguity\nfunction is only non-negligible in a limited region of the ambiguity plane. The\nthresholds of the procedures are formally derived for each point in the plane,\nand methods for the estimation of nuisance parameters that the thresholds\ndepend on are proposed. The estimation method is tested on several signals, and\nreductions in mean square error when estimating the ambiguity function by\nfactors of over a hundred are obtained. An estimator of the spread of the\nambiguity function is proposed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2008 14:24:37 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2009 16:12:49 GMT"}], "update_date": "2009-08-21", "authors_parsed": [["Hindberg", "Heidi", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "0804.1143", "submitter": "Jie Yang", "authors": "Zhishen Ye and Jie Yang", "title": "Sliced Inverse Moment Regression Using Weighted Chi-Squared Tests for\n  Dimension Reduction", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for dimension reduction in regression using the first\ntwo inverse moments. We develop corresponding weighted chi-squared tests for\nthe dimension of the regression. The proposed method considers linear\ncombinations of Sliced Inverse Regression (SIR) and the method using a new\ncandidate matrix which is designed to recover the entire inverse second moment\nsubspace. The optimal combination may be selected based on the p-values derived\nfrom the dimension tests. Theoretically, the proposed method, as well as Sliced\nAverage Variance Estimate (SAVE), are more capable of recovering the complete\ncentral dimension reduction subspace than SIR and Principle Hessian Directions\n(pHd). Therefore it can substitute for SIR, pHd, SAVE, or any linear\ncombination of them at a theoretical level. Simulation study indicates that the\nproposed method may have consistently greater power than SIR, pHd, and SAVE.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2008 20:50:53 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2009 13:30:34 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2013 16:22:18 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Ye", "Zhishen", ""], ["Yang", "Jie", ""]]}, {"id": "0804.1837", "submitter": "Tetsuya Hattori", "authors": "Kumiko Hattori, Tetsuya Hattori", "title": "Mathematical analysis of long tail economy using stochastic ranking\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of estimating the distribution of sales rates of,\ne.g., book titles at an online bookstore, from the time evolution of ranking\ndata found at websites of the store. The method is based on new mathematical\nresults on an infinite particle limit of the stochastic ranking process, and is\nsuitable for quantitative studies of the long tail structure of online retails.\nWe give an example of a fit to the actual data obtained from Amazon.co.jp,\nwhich gives the Pareto slope parameter of the distribution of sales rates of\nthe book titles in the store.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2008 06:21:23 GMT"}], "update_date": "2008-12-08", "authors_parsed": [["Hattori", "Kumiko", ""], ["Hattori", "Tetsuya", ""]]}, {"id": "0804.2413", "submitter": "Jean-Michel Marin", "authors": "Kate Lee, Jean-Michel Marin, Kerrie Mengersen and Christian P. Robert", "title": "Bayesian Inference on Mixtures of Distributions", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey covers state-of-the-art Bayesian techniques for the estimation of\nmixtures. It complements the earlier Marin, Mengersen and Robert (2005) by\nstudying new types of distributions, the multinomial, latent class and t\ndistributions. It also exhibits closed form solutions for Bayesian inference in\nsome discrete setups. Lastly, it sheds a new light on the computation of Bayes\nfactors via the approximation of Chib (1995).\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2008 15:45:03 GMT"}], "update_date": "2008-04-16", "authors_parsed": [["Lee", "Kate", ""], ["Marin", "Jean-Michel", ""], ["Mengersen", "Kerrie", ""], ["Robert", "Christian P.", ""]]}, {"id": "0804.2414", "submitter": "Christian Robert", "authors": "J.-M. Marin (CREST, Inria Saclay - Ile de France), Christian Robert\n  (CREST, Ceremade)", "title": "Approximating the marginal likelihood in mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Chib (1995), a method for approximating marginal densities in a Bayesian\nsetting is proposed, with one proeminent application being the estimation of\nthe number of components in a normal mixture. As pointed out in Neal (1999) and\nFruhwirth-Schnatter (2004), the approximation often fails short of providing a\nproper approximation to the true marginal densities because of the well-known\nlabel switching problem (Celeux et al., 2000). While there exist other\nalternatives to the derivation of approximate marginal densities, we reconsider\nthe original proposal here and show as in Berkhof et al. (2003) and Lee et al.\n(2008) that it truly approximates the marginal densities once the label\nswitching issue has been solved.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2008 15:39:46 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Marin", "J. -M.", "", "CREST, Inria Saclay - Ile de France"], ["Robert", "Christian", "", "CREST, Ceremade"]]}, {"id": "0804.2752", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann, Torsten Hothorn", "title": "Boosting Algorithms: Regularization, Prediction and Model Fitting", "comments": "This paper commented in: [arXiv:0804.2757], [arXiv:0804.2770].\n  Rejoinder in [arXiv:0804.2777]. Published in at\n  http://dx.doi.org/10.1214/07-STS242 the Statistical Science\n  (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 477-505", "doi": "10.1214/07-STS242", "report-no": "IMS-STS-STS242", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical perspective on boosting. Special emphasis is given\nto estimating potentially complex parametric or nonparametric models, including\ngeneralized linear and additive models as well as regression models for\nsurvival analysis. Concepts of degrees of freedom and corresponding Akaike or\nBayesian information criteria, particularly useful for regularization and\nvariable selection in high-dimensional covariate spaces, are discussed as well.\nThe practical aspects of boosting procedures for fitting statistical models are\nillustrated by means of the dedicated open-source software package mboost. This\npackage implements functions which can be used for model fitting, prediction\nand variable selection. It is flexible, allowing for the implementation of new\nboosting algorithms optimizing user-specified loss functions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2008 12:01:37 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Hothorn", "Torsten", ""]]}, {"id": "0804.2757", "submitter": "Andreas Buja", "authors": "Andreas Buja, David Mease, Abraham J. Wyner", "title": "Comment: Boosting Algorithms: Regularization, Prediction and Model\n  Fitting", "comments": "Published in at http://dx.doi.org/10.1214/07-STS242B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 506-512", "doi": "10.1214/07-STS242B", "report-no": "IMS-STS-STS242B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors are doing the readers of Statistical Science a true service with\na well-written and up-to-date overview of boosting that originated with the\nseminal algorithms of Freund and Schapire. Equally, we are grateful for\nhigh-level software that will permit a larger readership to experiment with, or\nsimply apply, boosting-inspired model fitting. The authors show us a world of\nmethodology that illustrates how a fundamental innovation can penetrate every\nnook and cranny of statistical thinking and practice. They introduce the reader\nto one particular interpretation of boosting and then give a display of its\npotential with extensions from classification (where it all started) to least\nsquares, exponential family models, survival analysis, to base-learners other\nthan trees such as smoothing splines, to degrees of freedom and regularization,\nand to fascinating recent work in model selection. The uninitiated reader will\nfind that the authors did a nice job of presenting a certain coherent and\nuseful interpretation of boosting. The other reader, though, who has watched\nthe business of boosting for a while, may have quibbles with the authors over\ndetails of the historic record and, more importantly, over their optimism about\nthe current state of theoretical knowledge. In fact, as much as ``the\nstatistical view'' has proven fruitful, it has also resulted in some ideas\nabout why boosting works that may be misconceived, and in some recommendations\nthat may be misguided. [arXiv:0804.2752]\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2008 10:20:41 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Buja", "Andreas", ""], ["Mease", "David", ""], ["Wyner", "Abraham J.", ""]]}, {"id": "0804.2770", "submitter": "Trevor Hastie", "authors": "Trevor Hastie", "title": "Comment: Boosting Algorithms: Regularization, Prediction and Model\n  Fitting", "comments": "Published in at http://dx.doi.org/10.1214/07-STS242A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 513-515", "doi": "10.1214/07-STS242A", "report-no": "IMS-STS-STS242A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on ``Boosting Algorithms: Regularization, Prediction and Model\nFitting'' [arXiv:0804.2752]\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2008 11:18:48 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Hastie", "Trevor", ""]]}, {"id": "0804.2777", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann, Torsten Hothorn", "title": "Rejoinder: Boosting Algorithms: Regularization, Prediction and Model\n  Fitting", "comments": "Published in at http://dx.doi.org/10.1214/07-STS242REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 516-522", "doi": "10.1214/07-STS242REJ", "report-no": "IMS-STS-STS242REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to ``Boosting Algorithms: Regularization, Prediction and Model\nFitting'' [arXiv:0804.2752]\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2008 11:41:50 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Hothorn", "Torsten", ""]]}, {"id": "0804.2958", "submitter": "Joseph L. Schafer", "authors": "Joseph D. Y. Kang, Joseph L. Schafer", "title": "Demystifying Double Robustness: A Comparison of Alternative Strategies\n  for Estimating a Population Mean from Incomplete Data", "comments": "This paper commented in: [arXiv:0804.2962], [arXiv:0804.2965],\n  [arXiv:0804.2969], [arXiv:0804.2970]. Rejoinder in [arXiv:0804.2973].\n  Published in at http://dx.doi.org/10.1214/07-STS227 the Statistical Science\n  (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 523-539", "doi": "10.1214/07-STS227", "report-no": "IMS-STS-STS227", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When outcomes are missing for reasons beyond an investigator's control, there\nare two different ways to adjust a parameter estimate for covariates that may\nbe related both to the outcome and to missingness. One approach is to model the\nrelationships between the covariates and the outcome and use those\nrelationships to predict the missing values. Another is to model the\nprobabilities of missingness given the covariates and incorporate them into a\nweighted or stratified estimate. Doubly robust (DR) procedures apply both types\nof model simultaneously and produce a consistent estimate of the parameter if\neither of the two models has been correctly specified. In this article, we show\nthat DR estimates can be constructed in many ways. We compare the performance\nof various DR and non-DR estimates of a population mean in a simulated example\nwhere both models are incorrect but neither is grossly misspecified. Methods\nthat use inverse-probabilities as weights, whether they are DR or not, are\nsensitive to misspecification of the propensity model when some estimated\npropensities are small. Many DR methods perform better than simple\ninverse-probability weighting. None of the DR methods we tried, however,\nimproved upon the performance of simple regression-based prediction of the\nmissing values. This study does not represent every missing-data problem that\nwill arise in practice. But it does demonstrate that, in at least some\nsettings, two wrong models are not better than one.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:38:39 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Kang", "Joseph D. Y.", ""], ["Schafer", "Joseph L.", ""]]}, {"id": "0804.2962", "submitter": "Greg Ridgeway", "authors": "Greg Ridgeway, Daniel F. McCaffrey", "title": "Comment: Demystifying Double Robustness: A Comparison of Alternative\n  Strategies for Estimating a Population Mean from Incomplete Data", "comments": "Published in at http://dx.doi.org/10.1214/07-STS227C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 540-543", "doi": "10.1214/07-STS227C", "report-no": "IMS-STS-STS227C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on ``Demystifying Double Robustness: A Comparison of Alternative\nStrategies for Estimating a Population Mean from Incomplete Data''\n[arXiv:0804.2958]\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:00:50 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Ridgeway", "Greg", ""], ["McCaffrey", "Daniel F.", ""]]}, {"id": "0804.2965", "submitter": "James Robins", "authors": "James Robins, Mariela Sued, Quanhong Lei-Gomez, Andrea Rotnitzky", "title": "Comment: Performance of Double-Robust Estimators When ``Inverse\n  Probability'' Weights Are Highly Variable", "comments": "Published in at http://dx.doi.org/10.1214/07-STS227D the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 544-559", "doi": "10.1214/07-STS227D", "report-no": "IMS-STS-STS227D", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on ``Performance of Double-Robust Estimators When ``Inverse\nProbability'' Weights Are Highly Variable'' [arXiv:0804.2958]\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:10:20 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Robins", "James", ""], ["Sued", "Mariela", ""], ["Lei-Gomez", "Quanhong", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "0804.2969", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Comment: Understanding OR, PS and DR", "comments": "Published in at http://dx.doi.org/10.1214/07-STS227A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 560-568", "doi": "10.1214/07-STS227A", "report-no": "IMS-STS-STS227A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on ``Understanding OR, PS and DR'' [arXiv:0804.2958]\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:17:33 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "0804.2970", "submitter": "Marie Davidian", "authors": "Anastasios A. Tsiatis, Marie Davidian", "title": "Comment: Demystifying Double Robustness: A Comparison of Alternative\n  Strategies for Estimating a Population Mean from Incomplete Data", "comments": "Published in at http://dx.doi.org/10.1214/07-STS227B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 569-573", "doi": "10.1214/07-STS227B", "report-no": "IMS-STS-STS227B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment on ``Demystifying Double Robustness: A Comparison of Alternative\nStrategies for Estimating a Population Mean from Incomplete Data''\n[arXiv:0804.2958]\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:24:14 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Tsiatis", "Anastasios A.", ""], ["Davidian", "Marie", ""]]}, {"id": "0804.2973", "submitter": "Joseph L. Schafer", "authors": "Joseph D. Y. Kang, Joseph L. Schafer", "title": "Rejoinder: Demystifying Double Robustness: A Comparison of Alternative\n  Strategies for Estimating a Population Mean from Incomplete Data", "comments": "Published in at http://dx.doi.org/10.1214/07-STS227REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 574-580", "doi": "10.1214/07-STS227REJ", "report-no": "IMS-STS-STS227REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to ``Demystifying Double Robustness: A Comparison of Alternative\nStrategies for Estimating a Population Mean from Incomplete Data''\n[arXiv:0804.2958]\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 08:31:29 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Kang", "Joseph D. Y.", ""], ["Schafer", "Joseph L.", ""]]}, {"id": "0804.2982", "submitter": "John Rice", "authors": "Peter J. Bickel, Chao Chen, Jaimyoung Kwon, John Rice, Erik van Zwet,\n  Pravin Varaiya", "title": "Measuring Traffic", "comments": "Published in at http://dx.doi.org/10.1214/07-STS238 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 581-597", "doi": "10.1214/07-STS238", "report-no": "IMS-STS-STS238", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A traffic performance measurement system, PeMS, currently functions as a\nstatewide repository for traffic data gathered by thousands of automatic\nsensors. It has integrated data collection, processing and communications\ninfrastructure with data storage and analytical tools. In this paper, we\ndiscuss statistical issues that have emerged as we attempt to process a data\nstream of 2 GB per day of wildly varying quality. In particular, we focus on\ndetecting sensor malfunction, imputation of missing or bad data, estimation of\nvelocity and forecasting of travel times on freeway networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 09:43:57 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Bickel", "Peter J.", ""], ["Chen", "Chao", ""], ["Kwon", "Jaimyoung", ""], ["Rice", "John", ""], ["van Zwet", "Erik", ""], ["Varaiya", "Pravin", ""]]}, {"id": "0804.2996", "submitter": "Stephen M. Stigler", "authors": "Stephen M. Stigler", "title": "The Epic Story of Maximum Likelihood", "comments": "Published in at http://dx.doi.org/10.1214/07-STS249 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 598-620", "doi": "10.1214/07-STS249", "report-no": "IMS-STS-STS249", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At a superficial level, the idea of maximum likelihood must be prehistoric:\nearly hunters and gatherers may not have used the words ``method of maximum\nlikelihood'' to describe their choice of where and how to hunt and gather, but\nit is hard to believe they would have been surprised if their method had been\ndescribed in those terms. It seems a simple, even unassailable idea: Who would\nrise to argue in favor of a method of minimum likelihood, or even mediocre\nlikelihood? And yet the mathematical history of the topic shows this ``simple\nidea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli,\nLeonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of\nthose who explored the topic, not always in ways we would sanction today. In\nthis article, that history is reviewed from back well before Fisher to the time\nof Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930\ncharacterization of conditions for the consistency and efficiency of maximum\nlikelihood estimates is presented, and the mathematical basis of his three\nproofs discussed. In particular, Fisher's derivation of the information\ninequality is seen to be derived from his work on the analysis of variance, and\nhis later approach via estimating functions was derived from Euler's Relation\nfor homogeneous functions. The reaction to Fisher's work is reviewed, and some\nlessons drawn.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2008 11:11:13 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Stigler", "Stephen M.", ""]]}, {"id": "0804.3010", "submitter": "Yonina C. Eldar", "authors": "Yonina C. Eldar", "title": "Generalized SURE for Exponential Families: Applications to\n  Regularization", "comments": "to appear in the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2008.2008212", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's unbiased risk estimate (SURE) was proposed by Stein for the\nindependent, identically distributed (iid) Gaussian model in order to derive\nestimates that dominate least-squares (LS). In recent years, the SURE criterion\nhas been employed in a variety of denoising problems for choosing\nregularization parameters that minimize an estimate of the mean-squared error\n(MSE). However, its use has been limited to the iid case which precludes many\nimportant applications. In this paper we begin by deriving a SURE counterpart\nfor general, not necessarily iid distributions from the exponential family.\nThis enables extending the SURE design technique to a much broader class of\nproblems. Based on this generalization we suggest a new method for choosing\nregularization parameters in penalized LS estimators. We then demonstrate its\nsuperior performance over the conventional generalized cross validation\napproach and the discrepancy method in the context of image deblurring and\ndeconvolution. The SURE technique can also be used to design estimates without\npredefining their structure. However, allowing for too many free parameters\nimpairs the performance of the resulting estimates. To address this inherent\ntradeoff we propose a regularized SURE objective. Based on this design\ncriterion, we derive a wavelet denoising strategy that is similar in sprit to\nthe standard soft-threshold approach but can lead to improved MSE performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2008 20:48:46 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2008 18:34:19 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Eldar", "Yonina C.", ""]]}, {"id": "0804.3152", "submitter": "Christian Robert P", "authors": "Yves Atchade, Nicolas Lartillot and Christian P. Robert", "title": "Bayesian computation for statistical models with intractable normalizing\n  constants", "comments": "20 pages, 4 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with some computational aspects in the Bayesian analysis of\nstatistical models with intractable normalizing constants. In the presence of\nintractable normalizing constants in the likelihood function, traditional MCMC\nmethods cannot be applied. We propose an approach to sample from such posterior\ndistributions. The method can be thought as a Bayesian version of the MCMC-MLE\napproach of Geyer and Thompson (1992). To the best of our knowledge, this is\nthe first general and asymptotically consistent Monte Carlo method for such\nproblems. We illustrate the method with examples from image segmentation and\nsocial network modeling. We study as well the asymptotic behavior of the\nalgorithm and obtain a strong law of large numbers for empirical averages.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2008 11:07:23 GMT"}], "update_date": "2008-04-22", "authors_parsed": [["Atchade", "Yves", ""], ["Lartillot", "Nicolas", ""], ["Robert", "Christian P.", ""]]}, {"id": "0804.3243", "submitter": "Ronald Christensen", "authors": "Ronald Christensen, Wesley Johnson", "title": "A Conversation with Seymour Geisser", "comments": "Published in at http://dx.doi.org/10.1214/088342307000000131 the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 621-636", "doi": "10.1214/088342307000000131", "report-no": "IMS-STS-STS203", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seymour Geisser received his bachelor's degree in Mathematics from the City\nCollege of New York in 1950, and his M.A. and Ph.D. degrees in Mathematical\nStatistics at the University of North Carolina in 1952 and 1955, respectively.\nHe then held positions at the National Bureau of Standards and the National\nInstitute of Mental Health until 1961. From 1961 until 1965, he was Chief of\nthe Biometry Section at the National Institute of Arthritis and Metabolic\nDiseases, and also held the position of Professorial Lecturer at the George\nWashington University from 1960 to 1965. From 1965 to 1970, he was the founding\nChair of the Department of Statistics at the State University of New York,\nBuffalo, and in 1971, he became the founding Director of the School of\nStatistics at the University of Minnesota, remaining in that position until\n2001. He held visiting professorships at Iowa State University, 1960;\nUniversity of Wisconsin, 1964; University of Tel-Aviv (Israel), 1971;\nUniversity of Waterloo (Canada), 1972; Stanford University, 1976, 1977, 1988;\nCarnegie Mellon University, 1976; University of the Orange Free State (South\nAfrica), 1978, 1993; Harvard University, 1981; University of Chicago, 1985;\nUniversity of Warwick (England), 1986; University of Modena (Italy), 1996; and\nNational Chiao Tung University (Taiwan), 1998. He was the Lady Davis Visiting\nProfessor, Hebrew University of Jerusalem, 1991, 1994, 1999, and the Schor\nScholar, Merck Research Laboratories, 2002-2003. He was a Fellow of the\nInstitute of Mathematical Statistics and the American Statistical Association.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2008 07:06:30 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Christensen", "Ronald", ""], ["Johnson", "Wesley", ""]]}, {"id": "0804.3244", "submitter": "Barry I. Graubard", "authors": "Barry I. Graubard, Paul S. Levy, Gordon B. Willis", "title": "A Conversation with Monroe Sirken", "comments": "Published in at http://dx.doi.org/10.1214/07-STS245 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 4, 637-650", "doi": "10.1214/07-STS245", "report-no": "IMS-STS-STS245", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Born January 11, 1921 in New York City, Monroe Sirken grew up in a suburb of\nPasadena, California. He earned B.A. and M.A. degrees in sociology at UCLA in\n1946 and 1947, and a Ph.D. in 1950 in sociology with a minor in mathematics at\nthe University of Washington in 1950 where Professor Z. W. Birnbaum was his\nmentor and thesis advisor. As a Post-Doctoral Fellow of the Social Science\nResearch Council, Monroe spent 1950--1951 at the Statistics Laboratory,\nUniversity of California at Berkeley and the Office of the Assistant Director\nfor Research, U.S. Bureau of the Census in Suitland, Maryland. Monroe visited\nthe Census Bureau at a time of great change in the use of sampling and survey\nmethods, and decided to remain. He began his government career there in 1951 as\na mathematical statistician, and moved to the National Office of Vital\nStatistics (NOVS) in 1953 where he was an actuarial mathematician and a\nmathematical statistician. He has held a variety of research and administrative\npositions at the National Center for Health Statistics (NCHS) and he was the\nAssociate Director, Research and Methodology and the Director, Office of\nResearch and Methodology until 1996 when he became a senior research scientist,\nthe title he currently holds. Aside from administrative responsibilities,\nMonroe's major professional interests have been conducting and fostering survey\nand statistical research responsive to the needs of federal statistics. His\ninterest in the design of rare and sensitive population surveys led to the\ndevelopment of network sampling which improves precision by linking multiple\nselection units to the same observation units. His interest in fostering\nresearch on the cognitive aspects of survey methods led to the establishment of\npermanent questionnaire design research laboratories, first at NCHS and later\nat other federal statistical agencies here and abroad.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2008 07:58:24 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Graubard", "Barry I.", ""], ["Levy", "Paul S.", ""], ["Willis", "Gordon B.", ""]]}, {"id": "0804.3367", "submitter": "Theodore Alexandrov", "authors": "Theodore Alexandrov", "title": "A Method of Trend Extraction Using Singular Spectrum Analysis", "comments": "9 pages, 3 figures, accepted to COMPSTAT'08, updated version is\n  published in RevStat, 7(1):1-22, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new method of trend extraction in the framework of the\nSingular Spectrum Analysis (SSA) approach. This method is easy to use, does not\nneed specification of models of time series and trend, allows to extract trend\nin the presence of noise and oscillations and has only two parameters (besides\nbasic SSA parameter called window length). One parameter manages scale of the\nextracted trend and another is a method specific threshold value. We propose\nprocedures for the choice of the parameters. The presented method is evaluated\non a simulated time series with a polynomial trend and an oscillating component\nwith unknown period and on the seasonally adjusted monthly data of unemployment\nlevel in Alaska for the period 1976/01-2006/09.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2008 17:28:36 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2008 14:54:05 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2009 22:55:28 GMT"}], "update_date": "2009-06-06", "authors_parsed": [["Alexandrov", "Theodore", ""]]}, {"id": "0804.3853", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Renate Meyer and Nelson Christensen", "title": "Modelling coloured residual noise in gravitational-wave signal\n  processing", "comments": "21 pages, 5 figures", "journal-ref": "Classical and Quantum Gravity 28:015010, 2011", "doi": "10.1088/0264-9381/28/1/015010", "report-no": "AEI-2008-046", "categories": "stat.ME gr-qc physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a signal processing model for signals in non-white noise, where\nthe exact noise spectrum is a priori unknown. The model is based on a Student's\nt distribution and constitutes a natural generalization of the widely used\nnormal (Gaussian) model. This way, it allows for uncertainty in the noise\nspectrum, or more generally is also able to accommodate outliers (heavy-tailed\nnoise) in the data. Examples are given pertaining to data from gravitational\nwave detectors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2008 07:03:25 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2008 13:00:18 GMT"}, {"version": "v3", "created": "Fri, 8 Aug 2008 08:16:26 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2010 12:48:13 GMT"}, {"version": "v5", "created": "Wed, 18 Aug 2010 15:38:20 GMT"}, {"version": "v6", "created": "Sun, 12 Dec 2010 10:55:41 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Meyer", "Renate", ""], ["Christensen", "Nelson", ""]]}, {"id": "0804.3926", "submitter": "Marian Grendar", "authors": "M. Grendar", "title": "Maximum Probability and Relative Entropy Maximization. Bayesian Maximum\n  Probability and Empirical Likelihood", "comments": "Intnl. Workshop on Applied Probability 2008, Compiegne, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Works, briefly surveyed here, are concerned with two basic methods: Maximum\nProbability and Bayesian Maximum Probability; as well as with their asymptotic\ninstances: Relative Entropy Maximization and Maximum Non-parametric Likelihood.\nParametric and empirical extensions of the latter methods - Empirical Maximum\nMaximum Entropy and Empirical Likelihood - are also mentioned. The methods are\nviewed as tools for solving certain ill-posed inverse problems, called\nPi-problem, Phi-problem, respectively. Within the two classes of problems,\nprobabilistic justification and interpretation of the respective methods are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2008 13:44:28 GMT"}], "update_date": "2008-04-25", "authors_parsed": [["Grendar", "M.", ""]]}, {"id": "0804.3989", "submitter": "Madeleine Cule", "authors": "Madeleine Cule, Richard Samworth and Michael Stewart", "title": "Maximum likelihood estimation of a multidimensional log-concave density", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X_1, ..., X_n be independent and identically distributed random vectors\nwith a log-concave (Lebesgue) density f. We first prove that, with probability\none, there exists a unique maximum likelihood estimator of f. The use of this\nestimator is attractive because, unlike kernel density estimation, the method\nis fully automatic, with no smoothing parameters to choose. Although the\nexistence proof is non-constructive, we are able to reformulate the issue of\ncomputation in terms of a non-differentiable convex optimisation problem, and\nthus combine techniques of computational geometry with Shor's r-algorithm to\nproduce a sequence that converges to the maximum likelihood estimate. For the\nmoderate or large sample sizes in our simulations, the maximum likelihood\nestimator is shown to provide an improvement in performance compared with\nkernel-based methods, even when we allow the use of a theoretical, optimal\nfixed bandwidth for the kernel estimator that would not be available in\npractice. We also present a real data clustering example, which shows that our\nmethodology can be used in conjunction with the Expectation--Maximisation (EM)\nalgorithm to fit finite mixtures of log-concave densities. An R version of the\nalgorithm is available in the package LogConcDEAD -- Log-Concave Density\nEstimation in Arbitrary Dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2008 19:32:25 GMT"}], "update_date": "2008-04-25", "authors_parsed": [["Cule", "Madeleine", ""], ["Samworth", "Richard", ""], ["Stewart", "Michael", ""]]}, {"id": "0804.4361", "submitter": "Stephen Lee", "authors": "Stephen M.S. Lee, P.Y. Lai", "title": "Improving Coverage Accuracy of Block Bootstrap Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": "Research Report No. 435. Department of Statistics and Actuarial\n  Science, The University of Hong Kong", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block bootstrap confidence interval based on dependent data can\noutperform the computationally more convenient normal approximation only with\nnon-trivial Studentization which, in the case of complicated statistics, calls\nfor highly specialist treatment. We propose two different approaches to\nimproving the accuracy of the block bootstrap confidence interval under very\ngeneral conditions. The first calibrates the coverage level by iterating the\nblock bootstrap. The second calculates Studentizing factors directly from block\nbootstrap series and requires no non-trivial analytic treatment. Both\napproaches involve two nested levels of block bootstrap resampling and yield\nhigh-order accuracy with simple tuning of block lengths at the two resampling\nlevels. A simulation study is reported to provide empirical support for our\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2008 10:04:07 GMT"}], "update_date": "2008-04-29", "authors_parsed": [["Lee", "Stephen M. S.", ""], ["Lai", "P. Y.", ""]]}, {"id": "0804.4451", "submitter": "Jian Ma", "authors": "Jian Ma and Zengqi Sun", "title": "Dependence Structure Estimation via Copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependence strucuture estimation is one of the important problems in machine\nlearning domain and has many applications in different scientific areas. In\nthis paper, a theoretical framework for such estimation based on copula and\ncopula entropy -- the probabilistic theory of representation and measurement of\nstatistical dependence, is proposed. Graphical models are considered as a\nspecial case of the copula framework. A method of the framework for estimating\nmaximum spanning copula is proposed. Due to copula, the method is irrelevant to\nthe properties of individual variables, insensitive to outlier and able to deal\nwith non-Gaussianity. Experiments on both simulated data and real dataset\ndemonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2008 17:14:53 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 00:29:28 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ma", "Jian", ""], ["Sun", "Zengqi", ""]]}, {"id": "0804.4685", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Gaussian Processes and Limiting Linear Models", "comments": "31 pages, 10 figures, 4 tables, accepted by CSDA, earlier version in\n  JSM06 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes retain the linear model either as a special case, or in\nthe limit. We show how this relationship can be exploited when the data are at\nleast partially linear. However from the perspective of the Bayesian posterior,\nthe Gaussian processes which encode the linear model either have probability of\nnearly zero or are otherwise unattainable without the explicit construction of\na prior with the limiting linear model in mind. We develop such a prior, and\nshow that its practical benefits extend well beyond the computational and\nconceptual simplicity of the linear model. For example, linearity can be\nextracted on a per-dimension basis, or can be combined with treed partition\nmodels to yield a highly efficient nonstationary model. Our approach is\ndemonstrated on synthetic and real datasets of varying linearity and\ndimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2008 19:43:43 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2008 20:02:26 GMT"}, {"version": "v3", "created": "Wed, 25 Jun 2008 09:43:32 GMT"}, {"version": "v4", "created": "Sun, 13 Jul 2008 09:19:09 GMT"}], "update_date": "2008-07-13", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}]