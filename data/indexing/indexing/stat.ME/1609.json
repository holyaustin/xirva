[{"id": "1609.00031", "submitter": "Jue Hou", "authors": "Jue Hou, Christina D. Chambers and Ronghui Xu", "title": "A Nonparametric Maximum Likelihood Approach for Partially Observed Cured\n  Data with Left Truncation and Right-Censoring", "comments": "48 pages including a 25-page appendix, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observed cured data occur in the analysis of spontaneous abortion\n(SAB) in observational studies in pregnancy. In contrast to the traditional\ncured data, such data has an observable `cured' portion as women who do not\nabort spontaneously. The data is also subject to left truncate in addition to\nright-censoring because women may enter or withdraw from a study any time\nduring their pregnancy. Left truncation in particular causes unique bias in the\npresence of a cured portion. In this paper, we study a cure rate model and\ndevelop a conditional nonparametric maximum likelihood approach. To tackle the\ncomputational challenge we adopt an EM algorithm making use of \"ghost copies\"\nof the data, and a closed form variance estimator is derived. Under suitable\nassumptions, we prove the consistency of the resulting estimator involving an\nunbounded cumulative baseline hazard function, as well as the asymptotic\nnormality. Simulation results are carried out to evaluate the finite sample\nperformance. We present the analysis of the motivating SAB study to illustrate\nthe power of our model addressing both occurrence and timing of SAB, as\ncompared to existing approaches in practice.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 20:38:31 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Hou", "Jue", ""], ["Chambers", "Christina D.", ""], ["Xu", "Ronghui", ""]]}, {"id": "1609.00046", "submitter": "Yan (Dora) Zhang", "authors": "Yan Dora Zhang, Brian P. Naughton, Howard D. Bondell, and Brian J.\n  Reich", "title": "Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage\n  Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior distributions for high-dimensional linear regression require specifying\na joint distribution for the unobserved regression coefficients, which is\ninherently difficult. We instead propose a new class of shrinkage priors for\nlinear regression via specifying a prior first on the model fit, in particular,\nthe coefficient of determination, and then distributing through to the\ncoefficients in a novel way. The proposed method compares favourably to\nprevious approaches in terms of both concentration around the origin and tail\nbehavior, which leads to improved performance both in posterior contraction and\nin empirical performance. The limiting behavior of the proposed prior is $1/x$,\nboth around the origin and in the tails. This behavior is optimal in the sense\nthat it simultaneously lies on the boundary of being an improper prior both in\nthe tails and around the origin. None of the existing shrinkage priors obtain\nthis behavior in both regions simultaneously. We also demonstrate that our\nproposed prior leads to the same near-minimax posterior contraction rate as the\nspike-and-slab prior.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:16:51 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 15:13:29 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 09:15:22 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Zhang", "Yan Dora", ""], ["Naughton", "Brian P.", ""], ["Bondell", "Howard D.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1609.00065", "submitter": "Anirban Bhattacharya", "authors": "Anirban Bhattacharya, Jeffrey D. Hart", "title": "Partitioned Cross-Validation for Divide-and-Conquer Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient method to estimate cross-validation bandwidth\nparameters for kernel density estimation in very large datasets where ordinary\ncross-validation is rendered highly inefficient, both statistically and\ncomputationally. Our approach relies on calculating multiple cross-validation\nbandwidths on partitions of the data, followed by suitable scaling and\naveraging to return a partitioned cross-validation bandwidth for the entire\ndataset. The partitioned cross-validation approach produces substantial\ncomputational gains over ordinary cross-validation. We additionally show that\npartitioned cross-validation can be statistically efficient compared to\nordinary cross-validation. We derive analytic expressions for the\nasymptotically optimal number of partitions and study its finite sample\naccuracy through a detailed simulation study. We additionally propose a\npermuted version of partitioned cross-validation which attains even higher\nefficiency. Theoretical properties of the estimators are studied and the\nmethodology is applied to the Higgs Boson dataset with 11 million observations\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 23:01:21 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Bhattacharya", "Anirban", ""], ["Hart", "Jeffrey D.", ""]]}, {"id": "1609.00066", "submitter": "David Inouye", "authors": "David I. Inouye, Eunho Yang, Genevera I. Allen, Pradeep Ravikumar", "title": "A Review of Multivariate Distributions for Count Data Derived from the\n  Poisson Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson distribution has been widely studied and used for modeling\nunivariate count-valued data. Multivariate generalizations of the Poisson\ndistribution that permit dependencies, however, have been far less popular.\nYet, real-world high-dimensional count-valued data found in word counts,\ngenomics, and crime statistics, for example, exhibit rich dependencies, and\nmotivate the need for multivariate distributions that can appropriately model\nthis data. We review multivariate distributions derived from the univariate\nPoisson, categorizing these models into three main classes: 1) where the\nmarginal distributions are Poisson, 2) where the joint distribution is a\nmixture of independent multivariate Poisson distributions, and 3) where the\nnode-conditional distributions are derived from the Poisson. We discuss the\ndevelopment of multiple instances of these classes and compare the models in\nterms of interpretability and theory. Then, we empirically compare multiple\nmodels from each class on three real-world datasets that have varying data\ncharacteristics from different domains, namely traffic accident data,\nbiological next generation sequencing data, and text data. These empirical\nexperiments develop intuition about the comparative advantages and\ndisadvantages of each class of multivariate distribution that was derived from\nthe Poisson. Finally, we suggest new research directions as explored in the\nsubsequent discussion section.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 23:08:02 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 13:42:25 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Inouye", "David I.", ""], ["Yang", "Eunho", ""], ["Allen", "Genevera I.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1609.00293", "submitter": "Yining Chen", "authors": "Rafal Baranowski, Yining Chen, Piotr Fryzlewicz", "title": "Narrowest-Over-Threshold Detection of Multiple Change-points and\n  Change-point-like Features", "comments": "62 pages, 10 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, generic and flexible methodology for nonparametric function\nestimation, in which we first estimate the number and locations of any features\nthat may be present in the function, and then estimate the function\nparametrically between each pair of neighbouring detected features. Examples of\nfeatures handled by our methodology include change-points in the\npiecewise-constant signal model, kinks in the piecewise-linear signal model,\nand other similar irregularities, which we also refer to as generalised\nchange-points.\n  Our methodology works with only minor modifications across a range of\ngeneralised change-point scenarios, and we achieve such a high degree of\ngenerality by proposing and using a new multiple generalised change-point\ndetection device, termed Narrowest-Over-Threshold (NOT). The key ingredient of\nNOT is its focus on the smallest local sections of the data on which the\nexistence of a feature is suspected. Crucially, this adaptive localisation\ntechnique prevents NOT from considering subsamples containing two or more\nfeatures, a key factor that ensures the general applicability of NOT.\n  For selected scenarios, we show the consistency and near-optimality of NOT in\ndetecting the number and locations of generalised change-points. Furthermore,\nwe propose to select NOT's threshold (automatically) via the strengthened\nSchwarz Information Criterion (sSIC) and give theoretical justifications. The\nNOT estimators are easy to implement and rapid to compute: the entire\nthreshold-indexed solution path can be computed in close-to-linear time.\nImportantly, the NOT approach is easy to extend by the user to tailor to their\nown needs. There is no single competitor, but we show that the performance of\nNOT matches or surpasses the state of the art in the scenarios tested. Our\nmethodology is implemented in the R package \\textbf{not}.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 15:56:45 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 07:17:33 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Baranowski", "Rafal", ""], ["Chen", "Yining", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1609.00451", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jing Lei, Larry Wasserman", "title": "Least Ambiguous Set-Valued Classifiers with Bounded Error Levels", "comments": "Final version to be published in the Journal of the American\n  Statistical Association at\n  https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1395341?journalCode=uasa20", "journal-ref": null, "doi": "10.1080/01621459.2017.1395341", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most classification tasks there are observations that are ambiguous and\ntherefore difficult to correctly label. Set-valued classifiers output sets of\nplausible labels rather than a single label, thereby giving a more appropriate\nand informative treatment to the labeling of ambiguous instances. We introduce\na framework for multiclass set-valued classification, where the classifiers\nguarantee user-defined levels of coverage or confidence (the probability that\nthe true label is contained in the set) while minimizing the ambiguity (the\nexpected size of the output). We first derive oracle classifiers assuming the\ntrue distribution to be known. We show that the oracle classifiers are obtained\nfrom level sets of the functions that define the conditional probability of\neach class. Then we develop estimators with good asymptotic and finite sample\nproperties. The proposed estimators build on existing single-label classifiers.\nThe optimal classifier can sometimes output the empty set, but we provide two\nsolutions to fix this issue that are suitable for various practical needs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 02:46:45 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 20:39:01 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1609.00606", "submitter": "Trang Nguyen", "authors": "Trang Quynh Nguyen, Allan Dafoe and Elizabeth L. Ogburn", "title": "The magnitude and direction of collider bias for binary variables", "comments": null, "journal-ref": "Epidemiologic Methods. 2019. Vol 8, Issue 1", "doi": "10.1515/em-2017-0013", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we are interested in the effect of variable $X$ on variable $Y$. If\n$X$ and $Y$ both influence, or are associated with variables that influence, a\ncommon outcome, called a collider, then conditioning on the collider (or on a\nvariable influenced by the collider -- its \"child\") induces a spurious\nassociation between $X$ and $Y$, which is known as collider bias.\nCharacterizing the magnitude and direction of collider bias is crucial for\nunderstanding the implications of selection bias and for adjudicating decisions\nabout whether to control for variables that are known to be associated with\nboth exposure and outcome but could be either confounders or colliders.\nConsidering a class of situations where all variables are binary, and where $X$\nand $Y$ either are, or are respectively influenced by, two marginally\nindependent causes of a collider, we derive collider bias that results from (i)\nconditioning on specific levels of, or (ii) linear regression adjustment for,\nthe collider (or its child). We also derive simple conditions that determine\nthe sign of such bias.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:09:48 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 23:10:27 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 20:01:30 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Nguyen", "Trang Quynh", ""], ["Dafoe", "Allan", ""], ["Ogburn", "Elizabeth L.", ""]]}, {"id": "1609.00635", "submitter": "Jonathan Law", "authors": "Jonathan Law, Darren Wilkinson", "title": "Composable Models for Online Bayesian Analysis of Streaming Data", "comments": "25 pages, 11 figures. For associated code repository, see\n  http://git.io/statespace", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is rapidly increasing in volume and velocity and the Internet of Things\n(IoT) is one important source of this data. The IoT is a collection of\nconnected devices (things) which are constantly recording data from their\nsurroundings using on-board sensors. These devices can record and stream data\nto the cloud at a very high rate, leading to high storage and analysis costs.\nIn order to ameliorate these costs, we can analyse the data as it arrives in a\nstream to learn about the underlying process, perform interpolation and\nsmoothing and make forecasts and predictions.\n  Conventional tools of state space modelling assume data on a fixed regular\ntime grid. However, many sensors change their sampling frequency, sometimes\nadaptively, or get interrupted and re-started out of sync with the previous\nsampling grid, or just generate event data at irregular times. It is therefore\ndesirable to model the system as a partially and irregularly observed Markov\nprocess which evolves in continuous time. Both the process and the observation\nmodel are potentially non-linear. Particle filters therefore represent the\nsimplest approach to online analysis. A functional Scala library of composable\ncontinuous time Markov process models has been developed in order to model the\nwide variety of data captured in the IoT.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 14:58:25 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Law", "Jonathan", ""], ["Wilkinson", "Darren", ""]]}, {"id": "1609.00656", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jerome P. Reiter", "title": "Itemwise conditionally independent nonresponse modeling for incomplete\n  multivariate data", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonresponse mechanism for multivariate missing data in which\neach study variable and its nonresponse indicator are conditionally independent\ngiven the remaining variables and their nonresponse indicators. This is a\nnonignorable missingness mechanism, in that nonresponse for any item can depend\non values of other items that are themselves missing. We show that, under this\nitemwise conditionally independent nonresponse assumption, one can define and\nidentify nonparametric saturated classes of joint multivariate models for the\nstudy variables and their missingness indicators. We also show how to perform\nsensitivity analysis to violations of the conditional independence assumptions\nencoded by this missingness mechanism. Throughout, we illustrate the use of\nthis modeling approach with data analyses.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 16:33:00 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1609.00672", "submitter": "Elie Wolfe", "authors": "Elie Wolfe, Robert W. Spekkens, Tobias Fritz", "title": "The Inflation Technique for Causal Inference with Latent Variables", "comments": "Minor final corrections, updated to match the published version as\n  closely as possible", "journal-ref": "J. Causal Inference 7(2), 2019", "doi": "10.1515/jci-2017-0020", "report-no": null, "categories": "quant-ph math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of causal inference is to determine if a given probability\ndistribution on observed variables is compatible with some causal structure.\nThe difficult case is when the causal structure includes latent variables. We\nhere introduce the $\\textit{inflation technique}$ for tackling this problem. An\ninflation of a causal structure is a new causal structure that can contain\nmultiple copies of each of the original variables, but where the ancestry of\neach copy mirrors that of the original. To every distribution of the observed\nvariables that is compatible with the original causal structure, we assign a\nfamily of marginal distributions on certain subsets of the copies that are\ncompatible with the inflated causal structure. It follows that compatibility\nconstraints for the inflation can be translated into compatibility constraints\nfor the original causal structure. Even if the constraints at the level of\ninflation are weak, such as observable statistical independences implied by\ndisjoint causal ancestry, the translated constraints can be strong. We apply\nthis method to derive new inequalities whose violation by a distribution\nwitnesses that distribution's incompatibility with the causal structure (of\nwhich Bell inequalities and Pearl's instrumental inequality are prominent\nexamples). We describe an algorithm for deriving all such inequalities for the\noriginal causal structure that follow from ancestral independences in the\ninflation. For three observed binary variables with pairwise common causes, it\nyields inequalities that are stronger in at least some aspects than those\nobtainable by existing methods. We also describe an algorithm that derives a\nweaker set of inequalities but is more efficient. Finally, we discuss which\ninflations are such that the inequalities one obtains from them remain valid\neven for quantum (and post-quantum) generalizations of the notion of a causal\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:21:25 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 10:55:55 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 16:22:04 GMT"}, {"version": "v4", "created": "Fri, 10 Aug 2018 03:39:13 GMT"}, {"version": "v5", "created": "Mon, 22 Jul 2019 20:08:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Wolfe", "Elie", ""], ["Spekkens", "Robert W.", ""], ["Fritz", "Tobias", ""]]}, {"id": "1609.00695", "submitter": "Nicholas Seewald", "authors": "Nicholas J. Seewald, Ji Sun, Peng Liao", "title": "MRT-SS Calculator: An R Shiny Application for Sample Size Calculation in\n  Micro-Randomized Trials", "comments": "20 pages. Source code for the application is available at\n  https://github.com/pengliao/mrt-ss-calculator", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The micro-randomized trial (MRT) is a new experimental design which allows\nfor the investigation of the proximal effects of a \"just-in-time\" treatment,\noften provided via a mobile device as part of a mobile health intervention. As\nwith a traditional randomized controlled trial, computing the minimum required\nsample size to achieve a desired power is a crucial step in designing an MRT.\nWe present MRT-SS Calculator, an online sample-size calculator for\nmicro-randomized trials, built with R Shiny. MRT-SS Calculator requires\nspecification of time-varying patterns for the proximal treatment effect and\nexpected treatment availability. We illustrate the implementation of MRT-SS\nCalculator using a mobile health trial, HeartSteps. The application can be\naccessed from https://pengliao.shinyapps.io/mrt-calculator.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 18:45:04 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 17:51:41 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 19:34:50 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Seewald", "Nicholas J.", ""], ["Sun", "Ji", ""], ["Liao", "Peng", ""]]}, {"id": "1609.00696", "submitter": "Scott Bruce", "authors": "Scott A. Bruce, Martica H. Hall, Daniel J. Buysse, and Robert T.\n  Krafty", "title": "Adaptive Bayesian Spectral Analysis of Nonstationary Biomedical Time\n  Series", "comments": "46 pages (not including title page), 6 figures at the end of the\n  article, 2 web supplements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies of biomedical time series signals aim to measure the association\nbetween frequency-domain properties of time series and clinical and behavioral\ncovariates. However, the time-varying dynamics of these associations are\nlargely ignored due to a lack of methods that can assess the changing nature of\nthe relationship through time. This article introduces a method for the\nsimultaneous and automatic analysis of the association between the time-varying\npower spectrum and covariates. The procedure adaptively partitions the grid of\ntime and covariate values into an unknown number of approximately stationary\nblocks and nonparametrically estimates local spectra within blocks through\npenalized splines. The approach is formulated in a fully Bayesian framework, in\nwhich the number and locations of partition points are random, and fit using\nreversible jump Markov chain Monte Carlo techniques. Estimation and inference\naveraged over the distribution of partitions allows for the accurate analysis\nof spectra with both smooth and abrupt changes. The proposed methodology is\nused to analyze the association between the time-varying spectrum of heart rate\nvariability and self-reported sleep quality in a study of older adults serving\nas the primary caregiver for their ill spouse.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 18:49:44 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 16:16:55 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bruce", "Scott A.", ""], ["Hall", "Martica H.", ""], ["Buysse", "Daniel J.", ""], ["Krafty", "Robert T.", ""]]}, {"id": "1609.00711", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid and Robert Garthoff", "title": "Generalized Spatial and Spatiotemporal Autoregressive Conditional\n  Heteroscedasticity", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2018.07.005", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new spatial model that incorporates\nheteroscedastic variance depending on neighboring locations. The proposed\nprocess is regarded as the spatial equivalent to the temporal autoregressive\nconditional heteroscedasticity (ARCH) model. We show additionally how the\nintroduced spatial ARCH model can be used in spatiotemporal settings. In\ncontrast to the temporal ARCH model, in which the distribution is known given\nthe full information set of the prior periods, the distribution is not\nstraightforward in the spatial and spatiotemporal setting. However, it is\npossible to estimate the parameters of the model using the maximum-likelihood\napproach. Via Monte Carlo simulations, we demonstrate the performance of the\nestimator for a specific spatial weighting matrix. Moreover, we combine the\nknown spatial autoregressive model with the spatial ARCH model assuming\nheteroscedastic errors. Eventually, the proposed autoregressive process is\nillustrated using an empirical example. Specifically, we model lung cancer\nmortality in 3108 U.S. counties and compare the introduced model with two\nbenchmark approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 19:38:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""], ["Garthoff", "Robert", ""]]}, {"id": "1609.00736", "submitter": "Yi-Hui Zhou", "authors": "Yi-Hui Zhou", "title": "A robust covariance testing approach for high-throughput data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing changes in covariance has received increasing\nattention in recent years, especially in the context of high-dimensional\ntesting. A number of approaches have been proposed, all limited to the\ntwo-sample problem and involving varying statistics and assumptions on the\nnumber of features $p$ vs. the sample size $n$. There are no general approaches\nto test association of covariances with a continuous outcome. We propose a\nuniform framework for testing association of covariances with an experimental\nvariable, whether discrete or continuous. The approach is not limited by the\ndata dimensions. Our test procedure (i) does not rely on parametric\nassumptions, (ii) works well for a range of $p$ and $n$ (e.g., does not require\n$n > p$), (iii) provides correct type I error control, and (iv) includes four\ndifferent statistics, to ensure power and flexibility under various settings,\nincluding a new \"connectivity\" statistic that is sensitive to changes in\noverall covariance magnitude. We demonstrate that, for the two-sample special\ncase, the proposed statistics are permutationally equivalent or similar to\nexisting proposed statistics. We demonstrate the power and utility of our\napproaches via simulation and analysis of real data. The approach is\nimplemented in an $R$ package.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 20:05:30 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Zhou", "Yi-Hui", ""]]}, {"id": "1609.00814", "submitter": "Stephan Huckemann", "authors": "Stephan F. Huckemann and Benjamin Eltzner", "title": "Backward Nested Descriptors Asymptotics with Inference on Stem Cell\n  Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sequences of random backward nested subspaces as occur, say, in dimension\nreduction for manifold or stratified space valued data, asymptotic results are\nderived. In fact, we formulate our results more generally for backward nested\nfamilies of descriptors (BNFD). Under rather general conditions, asymptotic\nstrong consistency holds. Under additional, still rather general hypotheses,\namong them existence of a.s. local twice differentiable charts, asymptotic\njoint normality of a BNFD can be shown. If charts factor suitably, this leads\nto individual asymptotic normality for the last element, a principal nested\nmean or a principal nested geodesic, say. It turns out that these results\npertain to principal nested spheres (PNS) and principal nested great subsphere\n(PNGS) analysis by Jung et al. (2010) as well as to the intrinsic mean on a\nfirst geodesic principal component (IMo1GPC) for manifolds and Kendall's shape\nspaces. A nested bootstrap two-sample test is derived and illustrated with\nsimulations. In a study on real data, PNGS is applied to track early human\nmesenchymal stem cell differentiation over a coarse time grid and, among\nothers, to locate a change point with direct consequences for the design of\nfurther studies.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 11:00:00 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Huckemann", "Stephan F.", ""], ["Eltzner", "Benjamin", ""]]}, {"id": "1609.00834", "submitter": "Marie-H\\'el\\`ene Descary", "authors": "Marie-H\\'el\\`ene Descary and Victor M. Panaretos", "title": "Functional Data Analysis by Matrix Completion", "comments": "To appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analyses typically proceed by smoothing, followed by\nfunctional PCA. This paradigm implicitly assumes that rough variation is due to\nnuisance noise. Nevertheless, relevant functional features such as\ntime-localised or short scale fluctuations may indeed be rough relative to the\nglobal scale, but still smooth at shorter scales. These may be confounded with\nthe global smooth components of variation by the smoothing and PCA, potentially\ndistorting the parsimony and interpretability of the analysis. The goal of this\npaper is to investigate how both smooth and rough variations can be recovered\non the basis of discretely observed functional data. Assuming that a functional\ndatum arises as the sum of two uncorrelated components, one smooth and one\nrough, we develop identifiability conditions for the recovery of the two\ncorresponding covariance operators. The key insight is that they should possess\ncomplementary forms of parsimony: one smooth and finite rank (large scale), and\nthe other banded and potentially infinite rank (small scale). Our conditions\nelucidate the precise interplay between rank, bandwidth, and grid resolution.\nUnder these conditions, we show that the recovery problem is equivalent to\nrank-constrained matrix completion, and exploit this to construct estimators of\nthe two covariances, without assuming knowledge of the true bandwidth or rank;\nwe study their asymptotic behaviour, and then use them to recover the smooth\nand rough components of each functional datum by best linear prediction. As a\nresult, we effectively produce separate functional PCAs for smooth and rough\nvariation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 15:03:02 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 06:24:20 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Descary", "Marie-H\u00e9l\u00e8ne", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1609.00908", "submitter": "Rasmus Waagepetersen", "authors": "Jesper M{\\o}ller and Rasmus Waagepetersen", "title": "Some recent developments in statistics for spatial point patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews developments in statistics for spatial point processes\nobtained within roughly the last decade. These developments include new classes\nof spatial point process models such as determinantal point processes, models\nincorporating both regularity and aggregation, and models where points are\nrandomly distributed around latent geometric structures. Regarding parametric\ninference the main focus is on various types of estimating functions derived\nfrom so-called innovation measures. Optimality of such estimating functions is\ndiscussed as well as computational issues. Maximum likelihood inference for\ndeterminantal point processes and Bayesian inference are briefly considered\ntoo. Concerning non-parametric inference, we consider extensions of functional\nsummary statistics to the case of inhomogeneous point processes as well as new\napproaches to simulation based inference.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 09:14:39 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1609.01118", "submitter": "David Amar", "authors": "David Amar, Ron Shamir, and Daniel Yekutieli", "title": "Extracting replicable associations across multiple studies: algorithms\n  for controlling the false discovery rate", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005700", "report-no": null, "categories": "stat.ME q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting associations that recur across multiple studies while controlling\nthe false discovery rate is a fundamental challenge. Here, we consider an\nextension of Efron's single-study two-groups model to allow joint analysis of\nmultiple studies. We assume that given a set of p-values obtained from each\nstudy, the researcher is interested in associations that recur in at least\n$k>1$ studies. We propose new algorithms that differ in how the study\ndependencies are modeled. We compared our new methods and others using various\nsimulated scenarios. The top performing algorithm, SCREEN (Scalable\nCluster-based REplicability ENhancement), is our new algorithm that is based on\nthree stages: (1) clustering an estimated correlation network of the studies,\n(2) learning replicability (e.g., of genes) within clusters, and (3) merging\nthe results across the clusters using dynamic programming. We applied SCREEN to\ntwo real datasets and demonstrated that it greatly outperforms the results\nobtained via standard meta-analysis. First, on a collection of 29 case-control\nlarge-scale gene expression cancer studies, we detected a large up-regulated\nmodule of genes related to proliferation and cell cycle regulation. These genes\nare both consistently up-regulated across many cancer studies, and are well\nconnected in known gene networks. Second, on a recent pan-cancer study that\nexamined the expression profiles of patients with or without mutations in the\nHLA complex, we detected an active module of up-regulated genes that are\nrelated to immune responses. Thanks to our ability to quantify the false\ndiscovery rate, we detected thrice more genes as compared to the original\nstudy. Our module contains most of the genes reported in the original study,\nand many new ones. Interestingly, the newly discovered genes are needed to\nestablish the connectivity of the module.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 12:00:07 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 12:40:26 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Amar", "David", ""], ["Shamir", "Ron", ""], ["Yekutieli", "Daniel", ""]]}, {"id": "1609.01336", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "Robust Particle Filter by Dynamic Averaging of Multiple Noise Models", "comments": "5 pages, 3 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State filtering is a key problem in many signal processing applications. From\na series of noisy measurement, one would like to estimate the state of some\ndynamic system. Existing techniques usually adopt a Gaussian noise assumption\nwhich may result in a major degradation in performance when the measurements\nare with the presence of outliers. A robust algorithm immune to the presence of\noutliers is desirable. To this end, a robust particle filter (PF) algorithm is\nproposed, in which the heavier tailed Student's t distributions are employed\ntogether with the Gaussian distribution to model the measurement noise. The\neffect of each model is automatically and dynamically adjusted via a Bayesian\nmodel averaging mechanism. The validity of the proposed algorithm is evaluated\nby illustrative simulations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 21:47:33 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 18:04:09 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1609.01396", "submitter": "Yi Shen", "authors": "Zhuan Pei, Yi Shen", "title": "The Devil is in the Tails: Regression Discontinuity Design with\n  Measurement Error in the Assignment Variable", "comments": "70 pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification in a regression discontinuity (RD) research design hinges on\nthe discontinuity in the probability of treatment when a covariate (assignment\nvariable) exceeds a known threshold. When the assignment variable is measured\nwith error, however, the discontinuity in the relationship between the\nprobability of treatment and the observed mismeasured assignment variable may\ndisappear. Therefore, the presence of measurement error in the assignment\nvariable poses a direct challenge to treatment effect identification. This\npaper provides sufficient conditions to identify the RD treatment effect using\nthe mismeasured assignment variable, the treatment status and the outcome\nvariable. We prove identification separately for discrete and continuous\nassignment variables and study the properties of various estimation procedures.\nWe illustrate the proposed methods in an empirical application, where we\nestimate Medicaid takeup and its crowdout effect on private health insurance\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 05:14:38 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Pei", "Zhuan", ""], ["Shen", "Yi", ""]]}, {"id": "1609.01547", "submitter": "Jaakko Reinikainen", "authors": "Jaakko Reinikainen and Juha Karvanen", "title": "Bayesian subcohort selection for longitudinal covariate measurements in\n  follow-up studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider planning longitudinal covariate measurements in follow-up studies\nwhere covariates are time-varying. We assume that the entire cohort cannot be\nselected for longitudinal measurements due to financial limitations and study\nhow a subset of the cohort should be selected optimally in order to obtain\nprecise estimates of covariate effects in a survival model. In our approach,\nthe study will be designed sequentially utilizing the data collected in\nprevious measurements of the individuals as prior information. We propose using\na Bayesian optimality criterion in the subcohort selections, which is compared\nwith simple random sampling using simulated and real follow-up data. This study\nextends previous results where optimal subcohort selection was studied with\nonly one re-measurement and one covariate, to more realistic cases where\nseveral covariates and measurement points are allowed. Our results support the\nconclusion that the precision of the estimates can be clearly improved by\noptimal design.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 13:39:42 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Reinikainen", "Jaakko", ""], ["Karvanen", "Juha", ""]]}, {"id": "1609.01553", "submitter": "Zimu Chen", "authors": "Zhanfeng Wang and Zimu Chen and Yaohua Wu", "title": "A relative error estimation approach for single index model", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A product relative error estimation method for single index regression model\nis proposed as an alternative to absolute error methods, such as the least\nsquare estimation and the least absolute deviation estimation. It is scale\ninvariant for outcome and covariates in the model. Regression coefficients are\nestimated via a two-stage procedure and their statistical properties such as\nconsistency and normality are studied. Numerical studies including simulation\nand a body fat example show that the proposed method performs well.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 13:51:51 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 14:10:42 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chen", "Zimu", ""], ["Wu", "Yaohua", ""]]}, {"id": "1609.01664", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A. Vsevolozhskaya, Gabriel Ruiz, Dmitri V. Zaykin", "title": "Assessment of P-value variability in the current replicability crisis", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased availability of data and accessibility of computational tools in\nrecent years have created unprecedented opportunities for scientific research\ndriven by statistical analysis. Inherent limitations of statistics impose\nconstrains on reliability of conclusions drawn from data but misuse of\nstatistical methods is a growing concern. Significance, hypothesis testing and\nthe accompanying P-values are being scrutinized as representing most widely\napplied and abused practices. One line of critique is that P-values are\ninherently unfit to fulfill their ostensible role as measures of scientific\nhypothesis's credibility. It has also been suggested that while P-values may\nhave their role as summary measures of effect, researchers underappreciate the\ndegree of randomness in the P-value. High variability of P-values would suggest\nthat having obtained a small P-value in one study, one is, nevertheless, likely\nto obtain a much larger P-value in a similarly powered replication study. Thus,\n\"replicability of P-value\" is itself questionable. To characterize P-value\nvariability one can use prediction intervals whose endpoints reflect the likely\nspread of P-values that could have been obtained by a replication study.\nUnfortunately, the intervals currently in use, the P-intervals, are based on\nunrealistic implicit assumptions. Namely, P-intervals are constructed with the\nassumptions that imply substantial chances of encountering large values of\neffect size in an observational study, which leads to bias. As an alternative\nto P-intervals, we develop a method that gives researchers flexibility by\nproviding them with the means to control these assumptions. Unlike endpoints of\nP-intervals, endpoints of our intervals are directly interpreted as\nprobabilistic bounds for replication P-values and are resistant to selection\nbias contingent upon approximate prior knowledge of the effect size\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 17:35:29 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 21:10:42 GMT"}, {"version": "v3", "created": "Sat, 10 Sep 2016 15:28:19 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Vsevolozhskaya", "Olga A.", ""], ["Ruiz", "Gabriel", ""], ["Zaykin", "Dmitri V.", ""]]}, {"id": "1609.01672", "submitter": "Daniel Sussman", "authors": "Runze Tang, Michael Ketcha, Alexandra Badea, Evan D. Calabrese, Daniel\n  S. Margulies, Joshua T. Vogelstein, Carey E. Priebe, Daniel L. Sussman", "title": "Connectome Smoothing via Low-rank Approximations", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical connectomics, the quantitative study of brain networks,\nestimating the mean of a population of graphs based on a sample is a core\nproblem. Often, this problem is especially difficult because the sample or\ncohort size is relatively small, sometimes even a single subject. While using\nthe element-wise sample mean of the adjacency matrices is a common approach,\nthis method does not exploit any underlying structural properties of the\ngraphs. We propose using a low-rank method which incorporates tools for\ndimension selection and diagonal augmentation to smooth the estimates and\nimprove performance over the naive methodology for small sample sizes.\nTheoretical results for the stochastic blockmodel show that this method offers\nmajor improvements when there are many vertices. Similarly, we demonstrate that\nthe low-rank methods outperform the standard sample mean for a variety of\nindependent edge distributions as well as human connectome data derived from\nmagnetic resonance imaging, especially when sample sizes are small. Moreover,\nthe low-rank methods yield \"eigen-connectomes\", which correlate with the\nlobe-structure of the human brain and superstructures of the mouse brain. These\nresults indicate that low-rank methods are an important part of the tool box\nfor researchers studying populations of graphs in general, and statistical\nconnectomics in particular.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 17:53:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 19:00:18 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 22:23:40 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tang", "Runze", ""], ["Ketcha", "Michael", ""], ["Badea", "Alexandra", ""], ["Calabrese", "Evan D.", ""], ["Margulies", "Daniel S.", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""], ["Sussman", "Daniel L.", ""]]}, {"id": "1609.01708", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco J. Rubio", "title": "Tractable Bayesian variable selection: beyond normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection often assumes normality, but the effects of model\nmisspecification are not sufficiently understood. There are sound reasons\nbehind this assumption, particularly for large $p$: ease of interpretation,\nanalytical and computational convenience. More flexible frameworks exist,\nincluding semi- or non-parametric models, often at the cost of some\ntractability. We propose a simple extension of the Normal model that allows for\nskewness and thicker-than-normal tails but preserves tractability. It leads to\neasy interpretation and a log-concave likelihood that facilitates optimization\nand integration. We characterize asymptotically parameter estimation and Bayes\nfactor rates, in particular studying the effects of model misspecification.\nUnder suitable conditions misspecified Bayes factors are consistent and induce\nsparsity at the same asymptotic rates than under the correct model. However,\nthe rates to detect signal are altered by an exponential factor, often\nresulting in a loss of sensitivity. These deficiencies can be ameliorated by\ninferring the error distribution from the data, a simple strategy that can\nimprove inference substantially. Our work focuses on the likelihood and can\nthus be combined with any likelihood penalty or prior, but here we focus on\nnon-local priors to induce extra sparsity and ameliorate finite-sample effects\ncaused by misspecification. Our results highlight the practical importance of\nfocusing on the likelihood rather than solely on the prior, when it comes to\nBayesian variable selection. The methodology is available in R package `mombf'.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 19:58:35 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 08:48:32 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 16:57:40 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco J.", ""]]}, {"id": "1609.01774", "submitter": "Peter Aronow", "authors": "Peter M. Aronow", "title": "A Note on \"How Robust Standard Errors Expose Methodological Problems\n  They Do Not Fix, and What to Do About It\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  King and Roberts (2015, KR) claim that a disagreement between robust and\nclassical standard errors exposes model misspecification. We emphasize that\nKR's claim only generally applies to parametric models: models that assume a\nrestrictive form of the distribution of the outcome. Many common models in use\nin political science, including the linear model, are not necessarily\nparametric -- rather they may be semiparametric. Common estimators of model\nparameters such as ordinary least squares have both robust (corresponding to a\nsemiparametric model) and classical (corresponding to a more restrictive model)\nstandard error estimates. Given a properly specified semiparametric model and\nmild regularity conditions, the classical standard errors are not generally\nconsistent, but the robust standard errors are. To illustrate this point, we\nconsider the case of the regression estimate of a semiparametric linear model\nwith no model misspecification, and show that robust standard errors may\nnevertheless systematically differ from classical standard errors. We show that\na disagreement between robust and classical standard errors is not generally\nsuitable as a diagnostic for regression estimators, and that KR's reanalyses of\nNeumayer (2003) and B\\\"uthe and Milner (2008) are predicated on strong\nassumptions that the original authors did not invoke nor require.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 21:57:08 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Aronow", "Peter M.", ""]]}, {"id": "1609.01788", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A. Vsevolozhskaya, Chia-Ling Kuo, Gabriel Ruiz, Luda Diatchenko,\n  Dmitri V. Zaykin", "title": "The more you test, the more you find: Smallest P-values become\n  increasingly enriched with real findings as more tests are conducted", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing accessibility of data to researchers makes it possible to conduct\nmassive amounts of statistical testing. Rather than follow a carefully crafted\nset of scientific hypotheses with statistical analysis, researchers can now\ntest many possible relations and let P-values or other statistical summaries\ngenerate hypotheses for them. Genetic epidemiology field is an illustrative\ncase in this paradigm shift. Driven by technological advances, testing a\nhandful of genetic variants in relation to a health outcome has been abandoned\nin favor of agnostic screening of the entire genome, followed by selection of\ntop hits, e.g., by selection of genetic variants with the smallest association\nP-values. At the same time, nearly total lack of replication of claimed\nassociations that has been shaming the field turned to a flow of reports whose\nfindings have been robustly replicating. Researchers may have adopted better\nstatistical practices by learning from past failures, but we suggest that a\nsteep increase in the amount of statistical testing itself is an important\nfactor. Regardless of whether statistical significance has been reached, an\nincreased number of tested hypotheses leads to enrichment of smallest P-values\nwith genuine associations. In this study, we quantify how the expected\nproportion of genuine signals (EPGS) among top hits changes with an increasing\nnumber of tests. When the rate of occurrence of genuine signals does not\ndecrease too sharply to zero as more tests are performed, the smallest P-values\nare increasingly more likely to represent genuine associations in studies with\nmore tests.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 00:02:09 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Vsevolozhskaya", "Olga A.", ""], ["Kuo", "Chia-Ling", ""], ["Ruiz", "Gabriel", ""], ["Diatchenko", "Luda", ""], ["Zaykin", "Dmitri V.", ""]]}, {"id": "1609.01811", "submitter": "Simon Mak", "authors": "Simon Mak, V. Roshan Joseph", "title": "Support points", "comments": "Accepted, Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new way to compact a continuous probability\ndistribution $F$ into a set of representative points called support points.\nThese points are obtained by minimizing the energy distance, a statistical\npotential measure initially proposed by Sz\\'ekely and Rizzo (2004) for testing\ngoodness-of-fit. The energy distance has two appealing features. First, its\ndistance-based structure allows us to exploit the duality between powers of the\nEuclidean distance and its Fourier transform for theoretical analysis. Using\nthis duality, we show that support points converge in distribution to $F$, and\nenjoy an improved error rate to Monte Carlo for integrating a large class of\nfunctions. Second, the minimization of the energy distance can be formulated as\na difference-of-convex program, which we manipulate using two algorithms to\nefficiently generate representative point sets. In simulation studies, support\npoints provide improved integration performance to both Monte Carlo and a\nspecific Quasi-Monte Carlo method. Two important applications of support points\nare then highlighted: (a) as a way to quantify the propagation of uncertainty\nin expensive simulations, and (b) as a method to optimally compact Markov chain\nMonte Carlo (MCMC) samples in Bayesian computation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 02:59:22 GMT"}, {"version": "v2", "created": "Sun, 2 Oct 2016 04:23:09 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 05:37:18 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 22:50:44 GMT"}, {"version": "v5", "created": "Wed, 23 Aug 2017 14:57:50 GMT"}, {"version": "v6", "created": "Fri, 8 Sep 2017 17:38:15 GMT"}, {"version": "v7", "created": "Sun, 9 Sep 2018 17:50:58 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Mak", "Simon", ""], ["Joseph", "V. Roshan", ""]]}, {"id": "1609.01910", "submitter": "Paolo Gorgi", "authors": "Paolo Gorgi", "title": "Integer-valued autoregressive models with survival probability driven by\n  a stochastic recurrence equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of integer-valued autoregressive models with dynamic survival\nprobability is proposed. The peculiarity of this class of models lies on the\nspecification of the survival probability through a stochastic recurrence\nequation. The estimation of the model can be performed by maximum likelihood\nand the consistency of the estimator is proved in a misspecified model setting.\nThe flexibility of the proposed specification is illustrated in a simulation\nstudy. An application to a time series of crime reports is presented. The\nresults show how the dynamic survival probability can enhance both in-sample\nand out-of-sample performances of integer-valued autoregressive models.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 09:43:07 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Gorgi", "Paolo", ""]]}, {"id": "1609.02112", "submitter": "Colin Fogarty", "authors": "Colin B. Fogarty", "title": "Studentized sensitivity analysis for the sample average treatment effect\n  in paired observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental limitation of causal inference in observational studies is that\nperceived evidence for an effect might instead be explained by factors not\naccounted for in the primary analysis. Methods for assessing the sensitivity of\na study's conclusions to unmeasured confounding have been established under the\nassumption that the treatment effect is constant across all individuals. In the\npotential presence of unmeasured confounding, it has been argued that certain\npatterns of effect heterogeneity may conspire with unobserved covariates to\nrender the performed sensitivity analysis inadequate. We present a new method\nfor conducting a sensitivity analysis for the sample average treatment effect\nin the presence of effect heterogeneity in paired observational studies. Our\nrecommended procedure, called the studentized sensitivity analysis, represents\nan extension of recent work on studentized permutation tests to the case of\nobservational studies, where randomizations are no longer drawn uniformly. The\nmethod naturally extends conventional tests for the sample average treatment\neffect in paired experiments to the case of unknown, but bounded, probabilities\nof assignment to treatment. In so doing, we illustrate that concerns about\ncertain sensitivity analyses operating under the presumption of constant\neffects are largely unwarranted.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 18:55:41 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 15:28:46 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 13:35:02 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 18:58:44 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Fogarty", "Colin B.", ""]]}, {"id": "1609.02186", "submitter": "Arrykrishna Mootoovaloo", "authors": "A. Mootoovaloo, Bruce A. Bassett, M.Kunz", "title": "Bayes Factors via Savage-Dickey Supermodels", "comments": "24 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a new method to compute the Bayes Factor for model selection which\nbypasses the Bayesian Evidence. Our method combines multiple models into a\nsingle, nested, Supermodel using one or more hyperparameters. Since the models\nare now nested the Bayes Factors between the models can be efficiently computed\nusing the Savage-Dickey Density Ratio (SDDR). In this way model selection\nbecomes a problem of parameter estimation. We consider two ways of constructing\nthe supermodel in detail: one based on combined models, and a second based on\ncombined likelihoods. We report on these two approaches for a Gaussian linear\nmodel for which the Bayesian evidence can be calculated analytically and a toy\nnonlinear problem. Unlike the combined model approach, where a standard Monte\nCarlo Markov Chain (MCMC) struggles, the combined-likelihood approach fares\nmuch better in providing a reliable estimate of the log-Bayes Factor. This\nscheme potentially opens the way to computationally efficient ways to compute\nBayes Factors in high dimensions that exploit the good scaling properties of\nMCMC, as compared to methods such as nested sampling that fail for high\ndimensions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 20:46:00 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Mootoovaloo", "A.", ""], ["Bassett", "Bruce A.", ""], ["Kunz", "M.", ""]]}, {"id": "1609.02463", "submitter": "Julien Flamant", "authors": "Julien Flamant, Nicolas Le Bihan, Pierre Chainais", "title": "Time-frequency analysis of bivariate signals", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many phenomena are described by bivariate signals or bidimensional vectors in\napplications ranging from radar to EEG, optics and oceanography. The\ntime-frequency analysis of bivariate signals is usually carried out by\nanalyzing two separate quantities, e.g. rotary components. We show that an\nadequate quaternion Fourier transform permits to build relevant time-frequency\nrepresentations of bivariate signals that naturally identify geometrical or\npolarization properties. First, the quaternion embedding of bivariate signals\nis introduced, similar to the usual analytic signal of real signals. Then two\nfundamental theorems ensure that a quaternion short term Fourier transform and\na quaternion continuous wavelet transform are well defined and obey desirable\nproperties such as conservation laws and reconstruction formulas. The resulting\nspectrograms and scalograms provide meaningful representations of both the\ntime-frequency and geometrical/polarization content of the signal. Moreover the\nnumerical implementation remains simply based on the use of FFT. A toolbox is\navailable for reproducibility. Synthetic and real-world examples illustrate the\nrelevance and efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 15:13:00 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Flamant", "Julien", ""], ["Bihan", "Nicolas Le", ""], ["Chainais", "Pierre", ""]]}, {"id": "1609.02629", "submitter": "Wesley Lee", "authors": "Wesley Lee, Bailey K. Fosdick, and Tyler H. McCormick", "title": "Inferring social structure from continuous-time interaction data", "comments": null, "journal-ref": "Applied Stochastic Models in Business and Industry 2018, Vol. 34,\n  No. 2, 87-104", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational event data, which consist of events involving pairs of actors over\ntime, are now commonly available at the finest of temporal resolutions.\nExisting continuous-time methods for modeling such data are based on point\nprocesses and directly model interaction \"contagion,\" whereby one interaction\nincreases the propensity of future interactions among actors, often as dictated\nby some latent variable structure. In this article, we present an alternative\napproach to using temporal-relational point process models for continuous-time\nevent data. We characterize interactions between a pair of actors as either\nspurious or that resulting from an underlying, persistent connection in a\nlatent social network. We argue that consistent deviations from expected\nbehavior, rather than solely high frequency counts, are crucial for identifying\nwell-established underlying social relationships. This study aims to explore\nthese latent network structures in two contexts: one comprising of college\nstudents and another involving barn swallows.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 01:08:49 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 23:45:00 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lee", "Wesley", ""], ["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1609.02661", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris, Erhan Bayraktar and Lifeng Lai", "title": "Efficient Byzantine Sequential Change Detection", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multisensor sequential change detection problem, a disruption occurs\nin an environment monitored by multiple sensors. This disruption induces a\nchange in the observations of an unknown subset of sensors. In the Byzantine\nversion of this problem, which is the focus of this work, it is further assumed\nthat the postulated change-point model may be misspecified for an unknown\nsubset of sensors. The problem then is to detect the change quickly and\nreliably, for any possible subset of affected sensors, even if the misspecified\nsensors are controlled by an adversary. Given a user-specified upper bound on\nthe number of compromised sensors, we propose and study three families of\nsequential change-detection rules for this problem. These are designed and\nevaluated under a generalization of Lorden's criterion, where conditional\nexpected detection delay and expected time to false alarm are both computed in\nthe worst-case scenario for the compromised sensors. The first-order asymptotic\nperformance of these procedures is characterized as the worst-case false alarm\nrate goes to 0. The insights from these theoretical results are corroborated by\na simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 05:32:57 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 03:23:07 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Fellouris", "Georgios", ""], ["Bayraktar", "Erhan", ""], ["Lai", "Lifeng", ""]]}, {"id": "1609.02686", "submitter": "Elisabeth Waldmann", "authors": "Elisabeth Waldmann, David Taylor-Robinson, Nadja Klein, Thomas Kneib,\n  Tania Pressler, Matthias Schmid and Andreas Mayr", "title": "Boosting Joint Models for Longitudinal and Time-to-Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint Models for longitudinal and time-to-event data have gained a lot of\nattention in the last few years as they are a helpful technique to approach\ncommon a data structure in clinical studies where longitudinal outcomes are\nrecorded alongside event times. Those two processes are often linked and the\ntwo outcomes should thus be modeled jointly in order to prevent the potential\nbias introduced by independent modelling. Commonly, joint models are estimated\nin likelihood based expectation maximization or Bayesian approaches using\nframeworks where variable selection is problematic and which do not immediately\nwork for high-dimensional data. In this paper, we propose a boosting algorithm\ntackling these challenges by being able to simultaneously estimate predictors\nfor joint models and automatically select the most influential variables even\nin high-dimensional data situations. We analyse the performance of the new\nalgorithm in a simulation study and apply it to the Danish cystic fibrosis\nregistry which collects longitudinal lung function data on patients with cystic\nfibrosis together with data regarding the onset of pulmonary infections. This\nis the first approach to combine state-of-the art algorithms from the field of\nmachine-learning with the model class of joint models, providing a fully\ndata-driven mechanism to select variables and predictor effects in a unified\nframework of boosting joint models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:17:54 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 17:04:04 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Waldmann", "Elisabeth", ""], ["Taylor-Robinson", "David", ""], ["Klein", "Nadja", ""], ["Kneib", "Thomas", ""], ["Pressler", "Tania", ""], ["Schmid", "Matthias", ""], ["Mayr", "Andreas", ""]]}, {"id": "1609.02688", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (ENSAI, IRMAR), Anne Ruiz-Gazen (TSE)", "title": "A comparison of pivotal sampling and unequal probability sampling with\n  replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any implementation of pivotal sampling is more efficient than\nmultinomial sampling. This property entails the weak consistency of the\nHorvitz-Thompson estimator and the existence of a conservative variance\nestimator. A small simulation study supports our findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:23:26 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 14:29:33 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Chauvet", "Guillaume", "", "ENSAI, IRMAR"], ["Ruiz-Gazen", "Anne", "", "TSE"]]}, {"id": "1609.02696", "submitter": "Elisabeth Waldmann", "authors": "Elisabeth Waldmann and David Taylor-Robinson", "title": "Bayesian Quantile-Based Joint Modelling of Repeated Measurement and\n  Time-to-Event data, with an Application to Lung Function Decline and Time to\n  Infection in Patients with Cystic Fibrosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The most widely used approach to joint modelling of repeated\nmeasurement and time to event data is to combine a linear Gaussian random\neffects model for the repeated measurements with a log-Gaussian frailty model\nfor the time-to-event outcome, linking the two through some form of correlation\nstructure between the random effects and the log-frailty. In this approach,\ncovariates are assumed to affect the mean response profile of the repeated\nmeasurement data. Objectives: Some applications raise substantive questions\nthat cannot be captured by this structure. For example, an important question\nin cystic fibrosis (CF) research is to understand the impact of a patient's\nlung function trajectory on their risk of acquiring a variety of infections,\nand how this varies at different quantiles of the lung function distribution.\nMethods: Motivated by this question, we develop a joint quantile modelling\nframework in this paper with an associated Markov Chain Monte Carlo algorithm\nfor Bayesian inference. Results: The translation from the common joint model\ntowards quantile regression succeeds and is applied to CF data from the United\nKingdom. The method helps detecting an overall difference in the relation\nbetween lung function decline and onset of infection in the different\nquantiles. Conclusions: Joint modelling without taking into account the special\nheteroscedastic structure is not sufficient in certain research question and\nthe extensions towards models beyond the mean is necessary.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:35:48 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Waldmann", "Elisabeth", ""], ["Taylor-Robinson", "David", ""]]}, {"id": "1609.02700", "submitter": "Clement Chevalier", "authors": "S\\'ebastien Marmin (IMSV, I2M), Cl\\'ement Chevalier, David Ginsbourger\n  (IMSV)", "title": "Efficient batch-sequential Bayesian optimization with moments of\n  truncated Gaussian vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the efficient parallelization of Bayesian global optimization\nalgorithms, and more specifically of those based on the expected improvement\ncriterion and its variants. A closed form formula relying on multivariate\nGaussian cumulative distribution functions is established for a generalized\nversion of the multipoint expected improvement criterion. In turn, the latter\nrelies on intermediate results that could be of independent interest concerning\nmoments of truncated Gaussian vectors. The obtained expansion of the criterion\nenables studying its differentiability with respect to point batches and\ncalculating the corresponding gradient in closed form. Furthermore , we derive\nfast numerical approximations of this gradient and propose efficient batch\noptimization strategies. Numerical experiments illustrate that the proposed\napproaches enable computational savings of between one and two order of\nmagnitudes, hence enabling derivative-based batch-sequential acquisition\nfunction maximization to become a practically implementable and efficient\nstandard.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:42:12 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Marmin", "S\u00e9bastien", "", "IMSV, I2M"], ["Chevalier", "Cl\u00e9ment", "", "IMSV"], ["Ginsbourger", "David", "", "IMSV"]]}, {"id": "1609.02818", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Gunter K. J. Maris, Lourens J. Waldorp and Denny\n  Borsboom", "title": "Network Psychometrics", "comments": "In Irwing, P., Hughes, D., and Booth, T. (2018). The Wiley Handbook\n  of Psychometric Testing, 2 Volume Set: A Multidisciplinary Reference on\n  Survey, Scale and Test Development. New York: Wiley", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides a general introduction of network modeling in\npsychometrics. The chapter starts with an introduction to the statistical model\nformulation of pairwise Markov random fields (PMRF), followed by an\nintroduction of the PMRF suitable for binary data: the Ising model. The Ising\nmodel is a model used in ferromagnetism to explain phase transitions in a field\nof particles. Following the description of the Ising model in statistical\nphysics, the chapter continues to show that the Ising model is closely related\nto models used in psychometrics. The Ising model can be shown to be equivalent\nto certain kinds of logistic regression models, loglinear models and\nmulti-dimensional item response theory (MIRT) models. The equivalence between\nthe Ising model and the MIRT model puts standard psychometrics in a new light\nand leads to a strikingly different interpretation of well-known latent\nvariable models. The chapter gives an overview of methods that can be used to\nestimate the Ising model, and concludes with a discussion on the interpretation\nof latent variables given the equivalence between the Ising model and MIRT.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 14:51:54 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 10:08:33 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Epskamp", "Sacha", ""], ["Maris", "Gunter K. J.", ""], ["Waldorp", "Lourens J.", ""], ["Borsboom", "Denny", ""]]}, {"id": "1609.02938", "submitter": "Hau-tieng Wu", "authors": "Su Li, Hau-tieng Wu", "title": "Extract fetal ECG from single-lead abdominal ECG by de-shape short time\n  Fourier transform and nonlocal median", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiple fundamental frequency detection problem and the source\nseparation problem from a single-channel signal containing multiple oscillatory\ncomponents and a nonstationary noise are both challenging tasks. To extract the\nfetal electrocardiogram (ECG) from a single-lead maternal abdominal ECG, we\nface both challenges. In this paper, we propose a novel method to extract the\nfetal ECG signal from the single channel maternal abdominal ECG signal, without\nany additional measurement. The algorithm is composed of three main\ningredients. First, the maternal and fetal heart rates are estimated by the\nde-shape short time Fourier transform, which is a recently proposed nonlinear\ntime-frequency analysis technique; second, the beat tracking technique is\napplied to accurately obtain the maternal and fetal R peaks; third, the\nmaternal and fetal ECG waveforms are established by the nonlocal median. The\nalgorithm is evaluated on a simulated fetal ECG signal database ({\\em fecgsyn}\ndatabase), and tested on two real databases with the annotation provided by\nexperts ({\\em adfecgdb} database and {\\em CinC2013} database). In general, the\nalgorithm could be applied to solve other detection and source separation\nproblems, and reconstruct the time-varying wave-shape function of each\noscillatory component.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 20:26:31 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Li", "Su", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1609.02950", "submitter": "Priyam Das", "authors": "Priyam Das and Subhashis Ghoshal", "title": "Bayesian Quantile Regression Using Random B-spline Series Prior", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2016.11.014", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian method for simultaneous quantile regression on a real\nvariable. By monotone transformation, we can make both the response variable\nand the predictor variable take values in the unit interval. A representation\nof quantile function is given by a convex combination of two monotone\nincreasing functions $\\xi_1$ and $\\xi_2$ not depending on the prediction\nvariables. In a Bayesian approach, a prior is put on quantile functions by\nputting prior distributions on $\\xi_1$ and $\\xi_2$. The monotonicity constraint\non the curves $\\xi_1$ and $\\xi_2$ are obtained through a spline basis expansion\nwith coefficients increasing and lying in the unit interval. We put a Dirichlet\nprior distribution on the spacings of the coefficient vector. A finite random\nseries based on splines obeys the shape restrictions. We compare our approach\nwith a Bayesian method using Gaussian process prior through an extensive\nsimulation study and some other Bayesian approaches proposed in the literature.\nAn application to a data on hurricane activities in the Atlantic region is\ngiven. We also apply our method on region-wise population data of USA for the\nperiod 1985--2010.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 21:36:43 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Das", "Priyam", ""], ["Ghoshal", "Subhashis", ""]]}, {"id": "1609.02980", "submitter": "Jiehuan Sun", "authors": "Jiehuan Sun, Jose D. Herazo-Maya, Naftali Kaminski, Hongyu Zhao,\n  Joshua L. Warren", "title": "A Dirichlet Process Mixture Model for Clustering Longitudinal Gene\n  Expression Data", "comments": "This paper has been withdrawn by the author because it was submitted\n  without consents of all authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgroup identification (clustering) is an important problem in biomedical\nresearch. Gene expression profiles are commonly utilized to define subgroups.\nLongitudinal gene expression profiles might provide additional information on\ndisease progression than what is captured by baseline profiles alone. Moreover,\nthe longitudinal gene expression data allows for intra-individual variability\nto be accounted for when grouping patients. Therefore, subgroup identification\ncould be more accurate and effective with the aid of longitudinal gene\nexpression data. However, existing statistical methods are unable to fully\nutilize these data for patient clustering. In this article, we introduce a\nnovel subgroup identification method in the Bayesian setting based on\nlongitudinal gene expression profiles. This method, called BClustLonG, adopts a\nlinear mixed-effects framework to model the trajectory of genes over time while\nclustering is jointly conducted based on the regression coefficients obtained\nfrom all genes. In order to account for the correlations among genes and\nalleviate the high dimensionality challenges, we adopt a factor analysis model\nfor the regression coefficients. The Dirichlet process prior distribution is\nutilized for the means of the regression coefficients to induce clustering.\nThrough extensive simulation studies, we show that BClustLonG has improved\nperformance over other clustering methods. When applied to a dataset of\nseverely injured (burn or trauma) patients, our model is able to distinguish\nburn patients from trauma patients and identify interesting subgroups in trauma\npatients.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 00:25:58 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 00:32:20 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Sun", "Jiehuan", ""], ["Herazo-Maya", "Jose D.", ""], ["Kaminski", "Naftali", ""], ["Zhao", "Hongyu", ""], ["Warren", "Joshua L.", ""]]}, {"id": "1609.02983", "submitter": "Jiehuan Sun", "authors": "Jiehuan Sun, Jose D. Herazo-Maya, Xiu Huang, Naftali Kaminski, Hongyu\n  Zhao", "title": "Distance-Correlation based Gene Set Analysis in Longitudinal Studies", "comments": "This paper has been withdrawn by the author because it was submitted\n  without consents of all authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal gene expression profiles of patients are collected in some\nclinical studies to monitor disease progression and understand disease\netiology. The identification of gene sets that have coordinated changes with\nrelevant clinical outcomes over time from these data could provide significant\ninsights into the molecular basis of disease progression and hence may lead to\nbetter treatments. In this article, we propose a Distance-Correlation based\nGene Set Analysis (dcGSA) method for longitudinal gene expression data. dcGSA\nis a non-parametric approach, statistically robust, and can capture both linear\nand nonlinear relationships between gene sets and clinical outcomes. In\naddition, dcGSA is able to identify related gene sets in cases where the\neffects of gene sets on clinical outcomes differ across patients due to the\npatient heterogeneity, alleviate the confounding effects of some unobserved\ncovariates, and allow the assessment of associations between gene sets and\nmultiple related outcomes simultaneously. Through extensive simulation studies,\nwe demonstrate that dcGSA is more powerful of detecting relevant genes than\nother commonly used gene set analysis methods. When dcGSA is applied to\nmultiple real datasets of different characteristics, we are able to identify\nmore disease related gene sets than other methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 01:02:08 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 00:32:15 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Sun", "Jiehuan", ""], ["Herazo-Maya", "Jose D.", ""], ["Huang", "Xiu", ""], ["Kaminski", "Naftali", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1609.02984", "submitter": "Jiehuan Sun", "authors": "Jiehuan Sun, Joshua L. Warren, Hongyu Zhao", "title": "A Bayesian Semiparametric Factor Analysis Model for Subtype\n  Identification", "comments": "This paper has been withdrawn by the author because it was submitted\n  without consents of all authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease subtype identification (clustering) is an important problem in\nbiomedical research. Gene expression profiles are commonly utilized to infer\ndisease subtypes, which often lead to biologically meaningful insights into\ndisease. Despite many successes, existing clustering methods may not perform\nwell when genes are highly correlated and many uninformative genes are included\nfor clustering due to the high dimensionality. In this article, we introduce a\nnovel subtype identification method in the Bayesian setting based on gene\nexpression profiles. This method, called BCSub, adopts an innovative\nsemiparametric Bayesian factor analysis model to reduce the dimension of the\ndata to a few factor scores for clustering. Specifically, the factor scores are\nassumed to follow the Dirichlet process mixture model in order to induce\nclustering. Through extensive simulation studies, we show that BCSub has\nimproved performance over commonly used clustering methods. When applied to two\ngene expression datasets, our model is able to identify subtypes that are\nclinically more relevant than those identified from the existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 01:30:28 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 00:32:06 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Sun", "Jiehuan", ""], ["Warren", "Joshua L.", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1609.02986", "submitter": "Jiehuan Sun", "authors": "Molei Liu, Jiehuan Sun, Jose D. Herazo-Maya, Naftali Kaminski, Hongyu\n  Zhao", "title": "Joint Models for Time-to-Event Data and Longitudinal Biomarkers of High\n  Dimension", "comments": "This paper has been withdrawn by the author because it was submitted\n  without consents of all authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models for longitudinal biomarkers and time-to-event data are widely\nused in longitudinal studies. Many joint modeling approaches have been proposed\nto deal with different types of longitudinal biomarkers and survival outcomes.\nHowever, most existing joint modeling methods cannot deal with a large number\nof longitudinal biomarkers simultaneously, such as the longitudinally collected\ngene expression profiles. In this article, we propose a new joint modeling\nmethod under the Bayesian framework, which is able to deal with longitudinal\nbiomarkers of high dimension. Specifically, we assume that only a few\nunobserved latent variables are related to the survival outcome and the latent\nvariables are inferred using a factor analysis model, which greatly reduces the\ndimensionality of the biomarkers and also accounts for the high correlations\namong the biomarkers. Through extensive simulation studies, we show that our\nproposed method has improved prediction accuracy over other joint modeling\nmethods. We illustrate the usefulness of our method on a dataset of idiopathic\npulmonary fibrosis patients in which we are interested in predicting the\npatients' time-to-death using their gene expression profiles.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 01:37:27 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 00:31:50 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Liu", "Molei", ""], ["Sun", "Jiehuan", ""], ["Herazo-Maya", "Jose D.", ""], ["Kaminski", "Naftali", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1609.03045", "submitter": "Tom Nye", "authors": "Tom M. W. Nye and Xiaoxian Tang and Grady Weyenberg and Ruriko Yoshida", "title": "Principal component analysis and the locus of the Frechet mean in the\n  space of phylogenetic trees", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most biological data are multidimensional, posing a major challenge to human\ncomprehension and computational analysis. Principal component analysis is the\nmost popular approach to rendering two- or three-dimensional representations of\nthe major trends in such multidimensional data. The problem of\nmultidimensionality is acute in the rapidly growing area of phylogenomics.\nEvolutionary relationships are represented by phylogenetic trees, and very\ntypically a phylogenomic analysis results in a collection of such trees, one\nfor each gene in the analysis. Principal component analysis offers a means of\nquantifying variation and summarizing a collection of phylogenies by\ndimensional reduction. However, the space of all possible phylogenies on a\nfixed set of species does not form a Euclidean vector space, so principal\ncomponent analysis must be reformulated in the geometry of tree-space, which is\na CAT(0) geodesic metric space. Previous work has focused on construction of\nthe first principal component, or principal geodesic. Here we propose a\ngeometric object which represents a $k$-th order principal component: the locus\nof the weighted Fr\\'echet mean of $k+1$ points in tree-space, where the weights\nvary over the standard $k$-dimensional simplex. We establish basic properties\nof these objects, in particular that locally they generically have dimension\n$k$, and we propose an efficient algorithm for projection onto these surfaces.\nCombined with a stochastic optimization algorithm, this projection algorithm\ngives a procedure for constructing a principal component of arbitrary order in\ntree-space. Simulation studies confirm these algorithms perform well, and they\nare applied to data sets of Apicomplexa gene trees and the African coelacanth\ngenome. The results enable visualizations of slices of tree-space, revealing\nstructure within these complex data sets.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 13:00:53 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Nye", "Tom M. W.", ""], ["Tang", "Xiaoxian", ""], ["Weyenberg", "Grady", ""], ["Yoshida", "Ruriko", ""]]}, {"id": "1609.03228", "submitter": "Eric Lock", "authors": "Eric F. Lock and Gen Li", "title": "Supervised multiway factorization", "comments": "31 pages, 6 figures, 7 tables", "journal-ref": "Electronic Journal of Statistics 2018, Vol. 12, No. 1, 1150-1180", "doi": "10.1214/18-EJS1421", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a probabilistic PARAFAC/CANDECOMP (CP) factorization for multiway\n(i.e., tensor) data that incorporates auxiliary covariates, SupCP. SupCP\ngeneralizes the supervised singular value decomposition (SupSVD) for\nvector-valued observations, to allow for observations that have the form of a\nmatrix or higher-order array. Such data are increasingly encountered in\nbiomedical research and other fields. We describe a likelihood-based latent\nvariable representation of the CP factorization, in which the latent variables\nare informed by additional covariates. We give conditions for identifiability,\nand develop an EM algorithm for simultaneous estimation of all model\nparameters. SupCP can be used for dimension reduction, capturing latent\nstructures that are more accurate and interpretable due to covariate\nsupervision. Moreover, SupCP specifies a full probability distribution for a\nmultiway data observation with given covariate values, which can be used for\npredictive modeling. We conduct comprehensive simulations to evaluate the SupCP\nalgorithm. We apply it to a facial image database with facial descriptors\n(e.g., smiling / not smiling) as covariates, and to a study of amino acid\nfluorescence. Software is available at https://github.com/lockEF/SupCP .\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 23:12:54 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 02:04:36 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Lock", "Eric F.", ""], ["Li", "Gen", ""]]}, {"id": "1609.03295", "submitter": "Manuel Batram", "authors": "Manuel Batram and Dietmar Bauer", "title": "New results on the asymptotic and finite sample properties of the MaCML\n  approach to multinomial probit model estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the properties of the maximum approximate composite marginal\nlikelihood (MaCML) approach to the estimation of multinomial probit models\n(MNP) proposed by Chandra Bhat and coworkers is investigated in finite samples\nas well as with respect to asymptotic properties. Using a small illustration\nexample it is proven that the approach does not necessarily lead to consistent\nestimators for four different types of approximation of the Gaussian cumulative\ndistribution function (including the Solow-Joe approach proposed by Bhat). It\nis shown that the bias of parameter estimates can be substantial (while\ntypically it is small) and the bias in the corresponding implied probabilities\nis small but non-negligible. Furthermore in finite sample it is demonstrated by\nsimulation that between two versions of the Solow-Joe method and two versions\nof the Mendell-Elston approximation no method dominates the others in terms of\naccuracy and numerical speed. Moreover the system to be estimated, the ordering\nof the components in the approximation method and even the tolerance used for\nstopping the numerical optimization routine all have an influence on the\nrelative performance of the procedures corresponding to the various\napproximation methods. Jointly the paper thus points towards eminent research\nneeds in order to decide on the method to use for a particular estimation\nproblem at hand.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 07:47:19 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 08:48:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Batram", "Manuel", ""], ["Bauer", "Dietmar", ""]]}, {"id": "1609.03297", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner H. Bonat and C\\'elestin C. Kokonendji", "title": "Flexible Tweedie regression models for continuous data", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": "10.1080/00949655.2017.1318876", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie regression models provide a flexible family of distributions to deal\nwith non-negative highly right-skewed data as well as symmetric and heavy\ntailed data and can handle continuous data with probability mass at zero. The\nestimation and inference of Tweedie regression models based on the maximum\nlikelihood method are challenged by the presence of an infinity sum in the\nprobability function and non-trivial restrictions on the power parameter space.\nIn this paper, we propose two approaches for fitting Tweedie regression models,\nnamely, quasi- and pseudo-likelihood. We discuss the asymptotic properties of\nthe two approaches and perform simulation studies to compare our methods with\nthe maximum likelihood method. In particular, we show that the quasi-likelihood\nmethod provides asymptotically efficient estimation for regression parameters.\nThe computational implementation of the alternative methods is faster and\neasier than the orthodox maximum likelihood, relying on a simple Newton scoring\nalgorithm. Simulation studies showed that the quasi- and pseudo-likelihood\napproaches present estimates, standard errors and coverage rates similar to the\nmaximum likelihood method. Furthermore, the second-moment assumptions required\nby the quasi- and pseudo-likelihood methods enables us to extend the Tweedie\nregression models to the class of quasi-Tweedie regression models in the\nWedderburn's style. Moreover, it allows to eliminate the non-trivial\nrestriction on the power parameter space, and thus provides a flexible\nregression model to deal with continuous data. We provide \\texttt{R}\nimplementation and illustrate the application of Tweedie regression models\nusing three data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 08:04:38 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner H.", ""], ["Kokonendji", "C\u00e9lestin C.", ""]]}, {"id": "1609.03320", "submitter": "Chenlei Leng", "authors": "Junlong Zhao, Chao Liu, Lu Niu, and Chenlei Leng", "title": "Multiple Influential Point Detection in High-Dimensional Spaces", "comments": "The paper has been substantially revised to make it more focused,\n  shorter and clearer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagnosis is an integrated component of data analysis, but is\nseverely under-investigated in a high-dimensional setting. One of the key\nchallenges, even in a fixed-dimensional setting, is how to deal with multiple\ninfluential points giving rise to the masking and swamping effects. This paper\nproposes a novel group deletion procedure referred to as MIP by studying two\nextreme statistics based on a marginal correlation based influence measure.\nNamed the Min and Max statistics, they have complimentary properties in that\nthe Max statistic is effective for overcoming the masking effect while the Min\nstatistic is useful for overcoming the swamping effect. Combining their\nstrengths, we further propose an efficient algorithm that can detect\ninfluential points with a prespecified false discovery rate. The proposed\ninfluential point detection procedure is simple to implement, efficient to run,\nand enjoys attractive theoretical properties. Its effectiveness is verified\nempirically via extensive simulation study and data analysis. An R package\nimplementing the procedure is freely available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:21:51 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 19:30:27 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Zhao", "Junlong", ""], ["Liu", "Chao", ""], ["Niu", "Lu", ""], ["Leng", "Chenlei", ""]]}, {"id": "1609.03333", "submitter": "Niek Tax", "authors": "Niek Tax, Emin Alasgarov, Natalia Sidorova, Reinder Haakma", "title": "On Generation of Time-based Label Refinements", "comments": "Accepted at CS&P workshop 2016 Overlap in preliminaries with\n  arXiv:1606.07259", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights in processes. Applying process mining techniques\non data from smart home environments has the potential to provide valuable\ninsights in (un)healthy habits and to contribute to ambient assisted living\nsolutions. Finding the right event labels to enable application of process\nmining techniques is however far from trivial, as simply using the triggering\nsensor as the label for sensor events results in uninformative models that\nallow for too much behavior (overgeneralizing). Refinements of sensor level\nevent labels suggested by domain experts have shown to enable discovery of more\nprecise and insightful process models. However, there exist no automated\napproach to generate refinements of event labels in the context of process\nmining. In this paper we propose a framework for automated generation of label\nrefinements based on the time attribute of events. We show on a case study with\nreal life smart home event data that behaviorally more specific, and therefore\nmore insightful, process models can be found by using automatically generated\nrefined labels in process discovery.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 10:25:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Tax", "Niek", ""], ["Alasgarov", "Emin", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""]]}, {"id": "1609.03367", "submitter": "Christian R\\\"over", "authors": "Simon Wandel, Beat Neuenschwander, Tim Friede, Christian R\\\"over", "title": "Using phase II data for the analysis of phase III studies: an\n  application in rare diseases", "comments": "25 pages, 3 tables, 2 figures", "journal-ref": "Clinical Trials, 14(3):277-285, 2017", "doi": "10.1177/1740774517699409", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical research and drug development in orphan diseases is challenging,\nsince large-scale randomized studies are difficult to conduct. Formally\nsynthesizing the evidence is therefore of great value, yet this is rarely done\nin the drug approval process. Phase III designs that make better use of phase\nII data can facilitate drug development in orphan diseases.\n  A Bayesian meta-analytic approach is used to inform the phase III study with\nphase II data. It is particularly attractive, since uncertainty of\nbetween-trial heterogeneity can be dealt with probabilistically, which is\ncritical if the number of studies is small. Furthermore, it allows quantifying\nand discounting the phase II data through the predictive distribution relevant\nfor phase III. A phase III design is proposed which uses the phase II data and\nconsiders approval based on a phase III interim analysis. The design is\nillustrated with a non-inferiority case study from an FDA approval in herpetic\nkeratitis (an orphan disease). Design operating characteristics are compared to\nthose of a traditional design, which ignores the phase II data.\n  An analysis of the phase II data reveals good but insufficient evidence for\nnon-inferiority, highlighting the need for a phase III study. For the phase III\nstudy supported by phase II data, the interim analysis is based on half of the\npatients. For this design, the meta-analytic interim results are conclusive and\nwould justify approval. In contrast, based on the phase III data only, interim\nresults are inconclusive and would require further evidence.\n  To accelerate drug development for orphan diseases, innovative study designs\nand appropriate methodology are needed. Taking advantage of randomized phase II\ndata when analyzing phase III studies looks promising because the evidence from\nphase II supports informed decision making. The implementation of the Bayesian\ndesign is straightforward.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 12:32:25 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Wandel", "Simon", ""], ["Neuenschwander", "Beat", ""], ["Friede", "Tim", ""], ["R\u00f6ver", "Christian", ""]]}, {"id": "1609.03436", "submitter": "Adam Johansen", "authors": "Murray Pollock and Paul Fearnhead and Adam M. Johansen and Gareth O.\n  Roberts", "title": "Quasi-stationary Monte Carlo and the ScaLE Algorithm", "comments": "Substantially revised with clearer presentation and more extensive\n  simulation study. 59 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a class of Monte Carlo algorithms which are based upon\nthe simulation of a Markov process whose quasi-stationary distribution\ncoincides with a distribution of interest. This differs fundamentally from,\nsay, current Markov chain Monte Carlo methods which simulate a Markov chain\nwhose stationary distribution is the target. We show how to approximate\ndistributions of interest by carefully combining sequential Monte Carlo methods\nwith methodology for the exact simulation of diffusions. The methodology\nintroduced here is particularly promising in that it is applicable to the same\nclass of problems as gradient based Markov chain Monte Carlo algorithms but\nentirely circumvents the need to conduct Metropolis-Hastings type accept/reject\nsteps whilst retaining exactness: the paper gives theoretical guarantees\nensuring the algorithm has the correct limiting target distribution.\nFurthermore, this methodology is highly amenable to big data problems. By\nemploying a modification to existing na{\\\"\\i}ve sub-sampling and control\nvariate techniques it is possible to obtain an algorithm which is still exact\nbut has sub-linear iterative cost as a function of data size.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 15:08:51 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 16:05:05 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:08:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Pollock", "Murray", ""], ["Fearnhead", "Paul", ""], ["Johansen", "Adam M.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1609.03479", "submitter": "Johan Sw\\\"ard", "authors": "Johan Sw\\\"ard, Stefan Ingi Adalbj\\\"ornsson, and Andreas Jakobsson", "title": "Generalized Sparse Covariance-based Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we extend the sparse iterative covariance-based estimator\n(SPICE), by generalizing the formulation to allow for different norm\nconstraints on the signal and noise parameters in the covariance model. For a\ngiven norm, the resulting extended SPICE method enjoys the same benefits as the\nregular SPICE method, including being hyper-parameter free, although the choice\nof norms are shown to govern the sparsity in the resulting solution.\nFurthermore, we show that solving the extended SPICE method is equivalent to\nsolving a penalized regression problem, which provides an alternative\ninterpretation of the proposed method and a deeper insight on the differences\nin sparsity between the extended and the original SPICE formulation. We examine\nthe performance of the method for different choices of norms, and compare the\nresults to the original SPICE method, showing the benefits of using the\nextended formulation. We also provide two ways of solving the extended SPICE\nmethod; one grid-based method, for which an efficient implementation is given,\nand a gridless method for the sinusoidal case, which results in a semi-definite\nprogramming problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 16:47:53 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 17:36:36 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Sw\u00e4rd", "Johan", ""], ["Adalbj\u00f6rnsson", "Stefan Ingi", ""], ["Jakobsson", "Andreas", ""]]}, {"id": "1609.03508", "submitter": "Umberto Picchini", "authors": "Umberto Picchini", "title": "Likelihood-free stochastic approximation EM for inference in complex\n  models", "comments": "Fixed a couple of typos, e.g. in algorithm 3, step (i), we now have\n  y*~p(Y|x*) instead of y*~p(Y|x). Similarly on page 7 (step 1 of Internal\n  SAEM-SL). Published in \"Communications in Statistics: Simulation and\n  Computation\" doi:10.1080/03610918.2017.1401082", "journal-ref": null, "doi": "10.1080/03610918.2017.1401082", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximum likelihood methodology for the parameters of models with an\nintractable likelihood is introduced. We produce a likelihood-free version of\nthe stochastic approximation expectation-maximization (SAEM) algorithm to\nmaximize the likelihood function of model parameters. While SAEM is best suited\nfor models having a tractable \"complete likelihood\" function, its application\nto moderately complex models is a difficult or even impossible task. We show\nhow to construct a likelihood-free version of SAEM by using the \"synthetic\nlikelihood\" paradigm. Our method is completely plug-and-play, requires almost\nno tuning and can be applied to both static and dynamic models. Four simulation\nstudies illustrate the method, including a stochastic differential equation\nmodel, a stochastic Lotka-Volterra model and data from $g$-and-$k$\ndistributions. MATLAB code is available as supplementary material.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 18:01:34 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 15:43:48 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 09:51:35 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 09:16:31 GMT"}, {"version": "v5", "created": "Tue, 16 Jan 2018 12:19:05 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Picchini", "Umberto", ""]]}, {"id": "1609.03686", "submitter": "Hao Chen", "authors": "Hao Chen and Dylan S. Small", "title": "New multivariate tests for assessing covariate balance in matched\n  observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new tests for assessing whether covariates in a treatment group\nand matched control group are balanced in observational studies. The tests\nexhibit high power under a wide range of multivariate alternatives, some of\nwhich existing tests have little power for. The asymptotic permutation null\ndistributions of the proposed tests are studied and the p-values calculated\nthrough the asymptotic results work well in finite samples, facilitating the\napplication of the test to large data sets. The tests are illustrated in a\nstudy of the effect of smoking on blood lead levels. The proposed tests are\nimplemented in an R package BalanceCheck.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 05:40:28 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 05:38:07 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Chen", "Hao", ""], ["Small", "Dylan S.", ""]]}, {"id": "1609.03692", "submitter": "Hyoung-Moon Kim", "authors": "Adelchi Azzalini, Hyoung-Moon Kim, Hea-Jung Kim", "title": "Sample selection models for discrete and other non-Gaussian response\n  variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observation of a phenomenon of interest subject to selective\nsampling due to a censoring mechanism regulated by some other variable. In this\ncontext, an extensive literature exists linked to the so-called Heckman\nselection model. A great deal of this work has been developed under Gaussian\nassumption of the underlying probability distributions; considerably less work\nhas dealt with other distributions. We examine a general construction which\nencompasses a variety of distributions and allows various options of the\nselection mechanism, focusing especially on the case of discrete response.\nInferential methods based on the pertaining likelihood function are developed.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 06:20:40 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Azzalini", "Adelchi", ""], ["Kim", "Hyoung-Moon", ""], ["Kim", "Hea-Jung", ""]]}, {"id": "1609.04156", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp, Lourens J. Waldorp, Ren\\'e M\\~ottus, Denny Borsboom", "title": "The Gaussian Graphical Model in Cross-sectional and Time-series Data", "comments": "Accepted pending revision in Multivariate Behavioral Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the Gaussian graphical model (GGM; an undirected network of\npartial correlation coefficients) and detail its utility as an exploratory data\nanalysis tool. The GGM shows which variables predict one-another, allows for\nsparse modeling of covariance structures, and may highlight potential causal\nrelationships between observed variables. We describe the utility in 3 kinds of\npsychological datasets: datasets in which consecutive cases are assumed\nindependent (e.g., cross-sectional data), temporally ordered datasets (e.g., n\n= 1 time series), and a mixture of the 2 (e.g., n > 1 time series). In\ntime-series analysis, the GGM can be used to model the residual structure of a\nvector-autoregression analysis (VAR), also termed graphical VAR. Two network\nmodels can then be obtained: a temporal network and a contemporaneous network.\nWhen analyzing data from multiple subjects, a GGM can also be formed on the\ncovariance structure of stationary means---the between-subjects network. We\ndiscuss the interpretation of these models and propose estimation methods to\nobtain these networks, which we implement in the R packages graphicalVAR and\nmlVAR. The methods are showcased in two empirical examples, and simulation\nstudies on these methods are included in the supplementary materials.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 07:41:34 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 16:01:35 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 08:47:24 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 11:11:04 GMT"}, {"version": "v5", "created": "Fri, 15 Sep 2017 22:07:17 GMT"}, {"version": "v6", "created": "Thu, 8 Feb 2018 14:25:38 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Epskamp", "Sacha", ""], ["Waldorp", "Lourens J.", ""], ["M\u00f5ttus", "Ren\u00e9", ""], ["Borsboom", "Denny", ""]]}, {"id": "1609.04231", "submitter": "Jia Guo", "authors": "Jia Guo and Jin-Ting Zhang", "title": "A Further Study of an $L^2$-norm Based Test for the Equality of Several\n  Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the multi-sample equal covariance function (ECF) testing problem, Zhang\n(2013) proposed an $L^{2}$-norm based test. However, its asymptotic power and\nfinite sample performance have not been studied. In this paper, its asymptotic\npower is investigated under some mild conditions. It is shown that the\n$L^2$-norm based test is root-$n$ consistent. In addition, intensive simulation\nstudies demonstrate that in terms of size-controlling and power, the\n$L^{2}$-norm based test outperforms the dimension-reduction based test proposed\nby Fremdt et al. (2013) when the functional data are less correlated or when\nthe effective signal information is located in high frequencies. Two real data\napplications are also presented to demonstrate the good performance of the\n$L^2$-norm based test.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:13:07 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guo", "Jia", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04232", "submitter": "Jia Guo", "authors": "Jia Guo, Bu Zhou and Jin-Ting Zhang", "title": "A Supremum-Norm Based Test for the Equality of Several Covariance\n  Functions", "comments": "arXiv admin note: text overlap with arXiv:1609.04231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new test for the equality of several covariance\nfunctions for functional data. Its test statistic is taken as the supremum\nvalue of the sum of the squared differences between the estimated individual\ncovariance functions and the pooled sample covariance function, hoping to\nobtain a more powerful test than some existing tests for the same testing\nproblem. The asymptotic random expression of this test statistic under the null\nhypothesis is obtained. To approximate the null distribution of the proposed\ntest statistic, we describe a parametric bootstrap method and a non-parametric\nbootstrap method. The asymptotic random expression of the proposed test is also\nstudied under a local alternative and it is shown that the proposed test is\nroot-$n$ consistent. Intensive simulation studies are conducted to demonstrate\nthe finite sample performance of the proposed test and it turns out that the\nproposed test is indeed more powerful than some existing tests when functional\ndata are highly correlated. The proposed test is illustrated with three real\ndata examples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:15:14 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Guo", "Jia", ""], ["Zhou", "Bu", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04234", "submitter": "Jia Guo", "authors": "Jia Guo and Jin-Ting Zhang", "title": "Two New Tests for Equality of Several Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two new tests for testing the equality of the\ncovariance functions of several functional populations, namely a quasi GPF test\nand a quasi $F_{\\max}$ test. The asymptotic random expressions of the two tests\nunder the null hypothesis are derived. We show that the asymptotic null\ndistribution of the quasi GPF test is a chi-squared-type mixture whose\ndistribution can be well approximated by a simple scaled chi-squared\ndistribution. We also adopt a random permutation method for approximating the\nnull distributions of the quasi GPF and $F_{\\max}$ tests. The random\npermutation method is applicable for both large and finite sample sizes. The\nasymptotic distributions of the two tests under a local alternative are\ninvestigated and they are shown to be root-n consistent. Simulation studies are\npresented to demonstrate the finite-sample performance of the new tests against\nthree existing tests. They show that our new tests are more powerful than the\nthree existing tests when the covariance functions at different time points\nhave different scales. An illustrative example is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:16:32 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guo", "Jia", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04340", "submitter": "Jack Murtagh", "authors": "Marco Gaboardi, James Honaker, Gary King, Jack Murtagh, Kobbi Nissim,\n  Jonathan Ullman, Salil Vadhan", "title": "PSI ({\\Psi}): a Private data Sharing Interface", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of PSI (\"a Private data Sharing Interface\"), a system\nwe are developing to enable researchers in the social sciences and other fields\nto share and explore privacy-sensitive datasets with the strong privacy\nprotections of differential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 16:45:09 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 05:02:59 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 02:24:30 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gaboardi", "Marco", ""], ["Honaker", "James", ""], ["King", "Gary", ""], ["Murtagh", "Jack", ""], ["Nissim", "Kobbi", ""], ["Ullman", "Jonathan", ""], ["Vadhan", "Salil", ""]]}, {"id": "1609.04365", "submitter": "Konstantinos Spiliopoulos", "authors": "Michael Salins and Konstantinos Spiliopoulos", "title": "Rare event simulation via importance sampling for linear SPDE's", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to develop provably efficient importance sampling\nMonte Carlo methods for the estimation of rare events within the class of\nlinear stochastic partial differential equations (SPDEs). We find that if a\nspectral gap of appropriate size exists, then one can identify a lower\ndimensional manifold where the rare event takes place. This allows one to build\nimportance sampling changes of measures that perform provably well even\npre-asymptotically (i.e. for small but non-zero size of the noise) without\ndegrading in performance due to infinite dimensionality or due to long\nsimulation time horizons. Simulation studies supplement and illustrate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 17:57:05 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 13:13:57 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Salins", "Michael", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1609.04383", "submitter": "David Sharrow", "authors": "David J. Sharrow, Jessica Godwin, Yanjun He, Samuel J. Clark, Adrian\n  E. Raftery", "title": "Probabilistic Population Projections for Countries with Generalized\n  HIV/AIDS Epidemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations (UN) issued official probabilistic population projections\nfor all countries to 2100 in July 2015. This was done by simulating future\nlevels of total fertility and life expectancy from Bayesian hierarchical\nmodels, and combining the results using a standard cohort-component projection\nmethod. The 40 countries with generalized HIV/AIDS epidemics were treated\ndifferently from others, in that the projections used the highly multistate\nSpectrum/EPP model, a complex 15-compartment model that was designed for\nshort-term projections of quantities relevant to policy for the epidemic. Here\nwe propose a simpler approach that is more compatible with the existing UN\nprobabilistic projection methodology for other countries. Changes in life\nexpectancy are projected probabilistically using a simple time series\nregression model on current life expectancy, HIV prevalence and ART coverage.\nThese are then converted to age- and sex-specific mortality rates using a new\nfamily of model life tables designed for countries with HIV/AIDS epidemics that\nreproduces the characteristic hump in middle adult mortality. These are then\ninput to the standard cohort-component method, as for other countries. The\nmethod performed well in an out-of-sample cross-validation experiment. It gives\nsimilar population projections to Spectrum/EPP in the short run, while being\nsimpler and avoiding multistate modeling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 19:11:03 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Sharrow", "David J.", ""], ["Godwin", "Jessica", ""], ["He", "Yanjun", ""], ["Clark", "Samuel J.", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1609.04464", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang and Guido Imbens", "title": "Peer Encouragement Designs in Causal Inference with Partial Interference\n  and Identification of Local Average Network Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-network settings, encouragement designs have been widely used to\nanalyze causal effects of a treatment, policy, or intervention on an outcome of\ninterest when randomizing the treatment was considered impractical or when\ncompliance to treatment cannot be perfectly enforced. Unfortunately, such\nquestions related to treatment compliance have received less attention in\nnetwork settings and the most well-studied experimental design in networks, the\ntwo-stage randomization design, requires perfect compliance with treatment. The\npaper proposes a new experimental design called peer encouragement design to\nstudy network treatment effects when enforcing treatment randomization is not\nfeasible. The key idea in peer encouragement design is the idea of personalized\nencouragement, which allows point-identification of familiar estimands in the\nencouragement design literature. The paper also defines new causal estimands,\nlocal average network effects, that can be identified under the new design and\nanalyzes the effect of non-compliance behavior in randomized experiments on\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 22:29:19 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Kang", "Hyunseung", ""], ["Imbens", "Guido", ""]]}, {"id": "1609.04522", "submitter": "Will Wei Sun", "authors": "Xiang Lyu, Will Wei Sun, Zhaoran Wang, Han Liu, Jian Yang, Guang Cheng", "title": "Tensor Graphical Model: Non-convex Optimization and Statistical\n  Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation and inference of graphical models that\ncharacterize the dependency structure of high-dimensional tensor-valued data.\nTo facilitate the estimation of the precision matrix corresponding to each way\nof the tensor, we assume the data follow a tensor normal distribution whose\ncovariance has a Kronecker product structure. A critical challenge in the\nestimation and inference of this model is the fact that its penalized maximum\nlikelihood estimation involves minimizing a non-convex objective function. To\naddress it, this paper makes two contributions: (i) In spite of the\nnon-convexity of this estimation problem, we prove that an alternating\nminimization algorithm, which iteratively estimates each sparse precision\nmatrix while fixing the others, attains an estimator with an optimal\nstatistical rate of convergence. (ii) We propose a de-biased statistical\ninference procedure for testing hypotheses on the true support of the sparse\nprecision matrices, and employ it for testing a growing number of hypothesis\nwith false discovery rate (FDR) control. The asymptotic normality of our test\nstatistic and the consistency of FDR control procedure are established. Our\ntheoretical results are backed up by thorough numerical studies and our real\napplications on neuroimaging studies of Autism spectrum disorder and users'\nadvertising click analysis bring new scientific findings and business insights.\nThe proposed methods are encoded into a publicly available R package Tlasso.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 06:41:11 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 19:47:07 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Lyu", "Xiang", ""], ["Sun", "Will Wei", ""], ["Wang", "Zhaoran", ""], ["Liu", "Han", ""], ["Yang", "Jian", ""], ["Cheng", "Guang", ""]]}, {"id": "1609.04523", "submitter": "Will Wei Sun", "authors": "Will Wei Sun and Lexin Li", "title": "STORE: Sparse Tensor Response Regression and Neuroimaging Analysis", "comments": "42 pages. To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in neuroimaging analysis, we propose a new\nregression model, Sparse TensOr REsponse regression (STORE), with a tensor\nresponse and a vector predictor. STORE embeds two key sparse structures:\nelement-wise sparsity and low-rankness. It can handle both a non-symmetric and\na symmetric tensor response, and thus is applicable to both structural and\nfunctional neuroimaging data. We formulate the parameter estimation as a\nnon-convex optimization problem, and develop an efficient alternating updating\nalgorithm. We establish a non-asymptotic estimation error bound for the actual\nestimator obtained from the proposed algorithm. This error bound reveals an\ninteresting interaction between the computational efficiency and the\nstatistical rate of convergence. When the distribution of the error tensor is\nGaussian, we further obtain a fast estimation error rate which allows the\ntensor dimension to grow exponentially with the sample size. We illustrate the\nefficacy of our model through intensive simulations and an analysis of the\nAutism spectrum disorder neuroimaging data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 06:51:51 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 03:32:26 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 23:01:54 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1609.04558", "submitter": "Ting Yan", "authors": "Ting Yan, Binyan Jiang, Stephen E. Fienberg, Chenlei Leng", "title": "Statistical Inference in a Directed Network Model with Covariates", "comments": "29 pages. minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are often characterized by node heterogeneity for which nodes\nexhibit different degrees of interaction and link homophily for which nodes\nsharing common features tend to associate with each other. In this paper, we\npropose a new directed network model to capture the former via node-specific\nparametrization and the latter by incorporating covariates. In particular, this\nmodel quantifies the extent of heterogeneity in terms of outgoingness and\nincomingness of each node by different parameters, thus allowing the number of\nheterogeneity parameters to be twice the number of nodes. We study the maximum\nlikelihood estimation of the model and establish the uniform consistency and\nasymptotic normality of the resulting estimators. Numerical studies demonstrate\nour theoretical findings and a data analysis confirms the usefulness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 09:54:37 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 07:50:57 GMT"}, {"version": "v3", "created": "Sat, 21 Jan 2017 08:02:53 GMT"}, {"version": "v4", "created": "Thu, 22 Feb 2018 08:59:14 GMT"}, {"version": "v5", "created": "Sun, 11 Mar 2018 01:55:46 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Yan", "Ting", ""], ["Jiang", "Binyan", ""], ["Fienberg", "Stephen E.", ""], ["Leng", "Chenlei", ""]]}, {"id": "1609.04595", "submitter": "Olivier Bouaziz", "authors": "O Bouaziz (MAP5), G Nuel (LPMA)", "title": "L0 regularisation for the estimation of piecewise constant hazard rates\n  in survival analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a survival analysis context we suggest a new method to estimate the\npiecewise constant hazard rate model. The method provides an automatic\nprocedure to find the number and location of cut points and to estimate the\nhazard on each cut interval. Estimation is performed through a penalized\nlikelihood using an adaptive ridge procedure. A bootstrap procedure is proposed\nin order to derive valid statistical inference taking both into account the\nvariability of the estimate and the variability in the choice of the cut\npoints. The new method is applied both to simulated data and to the Mayo Clinic\ntrial on primary biliary cirrhosis. The algorithm implementation is seen to\nwork well and to be of practical relevance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 12:28:36 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 16:00:02 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Bouaziz", "O", "", "MAP5"], ["Nuel", "G", "", "LPMA"]]}, {"id": "1609.04682", "submitter": "Michele Nguyen", "authors": "Michele Nguyen and Almut E. D. Veraart", "title": "Modelling spatial heteroskedasticity by volatility modulated moving\n  averages", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2017.03.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial heteroskedasticity refers to stochastically changing variances and\ncovariances in space. Such features have been observed in, for example, air\npollution and vegetation data. We study how volatility modulated moving\naverages can model this by developing theory, simulation and statistical\ninference methods. For illustration, we also apply our procedure to sea surface\ntemperature anomaly data from the International Research Institute for Climate\nand Society.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 14:53:55 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nguyen", "Michele", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1609.04778", "submitter": "Jichun Xie", "authors": "Jichun Xie and Ruosha Li", "title": "False Discovery Rate Control for High-Dimensional Networks of Quantile\n  Associations Conditioning on Covariates", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": "10.1111/rssb.12288", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motivated by the gene co-expression pattern analysis, we propose a novel\nsample quantile-based contingency (squac) statistic to infer quantile\nassociations conditioning on covariates. It features enhanced flexibility in\nhandling variables with both arbitrary distributions and complex association\npatterns conditioning on covariates. We first derive its asymptotic null\ndistribution, and then develop a multiple testing procedure based on squac to\nsimultaneously test the independence between one pair of variables conditioning\non covariates for all $p(p-1)/2$ pairs. Here, $p$ is the length of the outcomes\nand could exceed the sample size. The testing procedure does not require\nresampling or perturbation, and thus is computationally efficient. We prove by\ntheory and numerical experiments that this testing method asymptotically\ncontrols the false discovery rate (\\FDR). It outperforms all alternative\nmethods when the complex association panterns exist. Applied to a gastric\ncancer data, this testing method successfully inferred the gene co-expression\nnetworks of early and late stage patients. It identified more changes in the\nnetworks which are associated with cancer survivals. We extend our method to\nthe case that both the length of the outcomes and the length of covariates\nexceed the sample size, and show that the asymptotic theory still holds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 18:54:57 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 21:06:03 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Xie", "Jichun", ""], ["Li", "Ruosha", ""]]}, {"id": "1609.04843", "submitter": "Priyam Das", "authors": "Priyam Das and Subhashis Ghosal", "title": "Analyzing Ozone Concentration by Bayesian Spatio-temporal Quantile\n  Regression", "comments": null, "journal-ref": null, "doi": "10.1002/env.2443", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground level Ozone is one of the six common air-pollutants on which the EPA\nhas set national air quality standards. In order to capture the spatio-temporal\ntrend of 1-hour and 8-hour average ozone concentration in the US, we develop a\nmethod for spatio-temporal simultaneous quantile regression. Unlike existing\nprocedures, in the proposed method, smoothing across the sites is incorporated\nwithin modeling assumptions thus allowing borrowing of information across\nlocations, an essential step when the number of samples in each location is\nlow. The quantile function has been assumed to be linear in time and smooth\nover space and at any given site is given by a convex combination of two\nmonotone increasing functions $\\xi_1$ and $\\xi_2$ not depending on time. A\nB-spline basis expansion with increasing coefficients varying smoothly over the\nspace is used to put a prior and a Bayesian analysis is performed. We analyze\nthe average daily 1-hour maximum and 8-hour maximum ozone concentration level\ndata of US and California during 2006-2015 using the proposed method. It is\nnoted that in the last ten years, there is an overall decreasing trend in both\n1-hour maximum and 8-hour maximum ozone concentration level over the most parts\nof the US. In California, an overall a decreasing trend of 1-hour maximum ozone\nlevel is observed while no particular overall trend has been observed in the\ncase of 8-hour maximum ozone level.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 20:14:31 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 23:19:33 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Das", "Priyam", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1609.04862", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Reuben Aspden and Miles Padgett and Steve McLaughlin", "title": "A Bayesian approach to denoising of single-photon binary images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses new methods for processing images in the photon-limited\nregime where the number of photons per pixel is binary. We present a new\nBayesian denoising method for binary, single-photon images. Each pixel\nmeasurement is assumed to follow a Bernoulli distribution whose mean is related\nby a nonlinear function to the underlying intensity value to be recovered.\nAdopting a Bayesian approach, we assign the unknown intensity field a\nsmoothness promoting spatial and potentially temporal prior while enforcing the\npositivity of the intensity. A stochastic simulation method is then used to\nsample the resulting joint posterior distribution and estimate the unknown\nintensity, as well as the regularization parameters. We show that this new\nunsupervised denoising method can also be used to analyze images corrupted by\nPoisson noise. The proposed algorithm is compared to state-of-the art denoising\ntechniques dedicated to photon-limited images using synthetic and real\nsingle-photon measurements. The results presented illustrate the potential\nbenefits of the proposed methodology for photon-limited imaging, in particular\nwith non photonnumber resolving detectors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 21:19:51 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Altmann", "Yoann", ""], ["Aspden", "Reuben", ""], ["Padgett", "Miles", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1609.04902", "submitter": "Momiao Xiong", "authors": "Nan Lin, Yun Zhu, Ruzong Fan and Momiao Xiong", "title": "A Quadratically Regularized Functional Canonical Correlation Analysis\n  for Identifying the Global Structure of Pleiotropy with NGS Data", "comments": "64 pages including 12 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005788", "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the pleiotropic effects of genetic variants can increase\nstatistical power, provide important information to achieve deep understanding\nof the complex genetic structures of disease, and offer powerful tools for\ndesigning effective treatments with fewer side effects. However, the current\nmultiple phenotype association analysis paradigm lacks breadth (number of\nphenotypes and genetic variants jointly analyzed at the same time) and depth\n(hierarchical structure of phenotype and genotypes). A key issue for high\ndimensional pleiotropic analysis is to effectively extract informative internal\nrepresentation and features from high dimensional genotype and phenotype data.\nTo explore multiple levels of representations of genetic variants, learn their\ninternal patterns involved in the disease development, and overcome critical\nbarriers in advancing the development of novel statistical methods and\ncomputational algorithms for genetic pleiotropic analysis, we proposed a new\nframework referred to as a quadratically regularized functional CCA (QRFCCA)\nfor association analysis which combines three approaches: (1) quadratically\nregularized matrix factorization, (2) functional data analysis and (3)\ncanonical correlation analysis (CCA). Large-scale simulations show that the\nQRFCCA has a much higher power than that of the nine competing statistics while\nretaining the appropriate type 1 errors. To further evaluate performance, the\nQRFCCA and nine other statistics are applied to the whole genome sequencing\ndataset from the TwinsUK study. We identify a total of 79 genes with rare\nvariants and 67 genes with common variants significantly associated with the 46\ntraits using QRFCCA. The results show that the QRFCCA substantially outperforms\nthe nine other statistics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 03:18:44 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Lin", "Nan", ""], ["Zhu", "Yun", ""], ["Fan", "Ruzong", ""], ["Xiong", "Momiao", ""]]}, {"id": "1609.04967", "submitter": "Sven Buhl", "authors": "Sven Buhl, Richard A. Davis, Claudia Kl\\\"uppelberg and Christina\n  Steinkohl", "title": "Semiparametric estimation for isotropic max-stable space-time processes", "comments": "43 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularly varying space-time processes have proved useful to study extremal\ndependence in space-time data. We propose a semiparametric estimation procedure\nbased on a closed form expression of the extremogram to estimate parametric\nmodels of extremal dependence functions. We establish the asymptotic properties\nof the resulting parameter estimates and propose subsampling procedures to\nobtain asymptotically correct confidence intervals. A simulation study shows\nthat the proposed procedure works well for moderate sample sizes and is robust\nto small departures from the underlying model. Finally, we apply this\nestimation procedure to fitting a max-stable process to radar rainfall\nmeasurements in a region in Florida. Complementary results and some proofs of\nkey results are presented together with the simulation study in the supplement.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 09:33:32 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 06:25:57 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 09:20:30 GMT"}, {"version": "v4", "created": "Sun, 15 Jul 2018 19:07:28 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Buhl", "Sven", ""], ["Davis", "Richard A.", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Steinkohl", "Christina", ""]]}, {"id": "1609.04985", "submitter": "Hamed Haselimashhadi", "authors": "Hamed Haselimashhadi, Veronica Vinciotti", "title": "A Differentiable Alternative to the Lasso Penalty", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2018.1515362", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regularized regression has become very popular nowadays, particularly on\nhigh-dimensional problems where the addition of a penalty term to the\nlog-likelihood allows inference where traditional methods fail. A number of\npenalties have been proposed in the literature, such as lasso, SCAD, ridge and\nelastic net to name a few. Despite their advantages and remarkable performance\nin rather extreme settings, where $p \\gg n$, all these penalties, with the\nexception of ridge, are non-differentiable at zero. This can be a limitation in\ncertain cases, such as computational efficiency of parameter estimation in\nnon-linear models or derivation of estimators of the degrees of freedom for\nmodel selection criteria. With this paper, we provide the scientific community\nwith a differentiable penalty, which can be used in any situation, but\nparticularly where differentiability plays a key role. We show some desirable\nfeatures of this function and prove theoretical properties of the resulting\nestimators within a regularized regression context. A simulation study and the\nanalysis of a real dataset show overall a good performance under different\nscenarios. The method is implemented in the R package DLASSO freely available\nfrom CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 10:25:22 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Haselimashhadi", "Hamed", ""], ["Vinciotti", "Veronica", ""]]}, {"id": "1609.05108", "submitter": "Augustin Saucan", "authors": "Augustin-Alexandru Saucan, Mark Coates and Michael Rabbat", "title": "A multi-sensor multi-Bernoulli filter", "comments": "21 pages, 8 figures, 2 table", "journal-ref": null, "doi": "10.1109/TSP.2017.2723348", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive a multi-sensor multi-Bernoulli (MS-MeMBer) filter for\nmulti-target tracking. Measurements from multiple sensors are employed by the\nproposed filter to update a set of tracks modeled as a multi-Bernoulli random\nfinite set. An exact implementation of the MS-MeMBer update procedure is\ncomputationally intractable. We propose an efficient approximate implementation\nby using a greedy measurement partitioning mechanism. The proposed filter\nallows for Gaussian mixture or particle filter implementations. Numerical\nsimulations conducted for both linear-Gaussian and non-linear models highlight\nthe improved accuracy of the MS-MeMBer filter and its reduced computational\nload with respect to the multi-sensor cardinalized probability hypothesis\ndensity filter and the iterated-corrector cardinality-balanced multi-Bernoulli\nfilter especially for low probabilities of detection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 15:31:35 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 00:30:01 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 13:36:39 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 16:42:33 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Saucan", "Augustin-Alexandru", ""], ["Coates", "Mark", ""], ["Rabbat", "Michael", ""]]}, {"id": "1609.05186", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli, Joel Schwartz, Itai Kloog, Brent Coull", "title": "Spatial Multiresolution Analysis of the Effect of PM2.5 on Birth Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine particulate matter (PM2.5) measured at a given location is a mix of\npollution generated locally and pollution traveling long distances in the\natmosphere. Therefore, the identification of spatial scales associated with\nhealth effects can inform on pollution sources responsible for these effects,\nresulting in more targeted regulatory policy. Recently, prediction methods that\nyield high-resolution spatial estimates of PM2.5 exposures allow one to\nevaluate such scale-specific associations. We propose a two-dimensional wavelet\ndecomposition that alleviates restrictive assumptions required for standard\nwavelet decompositions. Using this method we decompose daily surfaces of PM2.5\nto identify which scales of pollution are most associated with adverse health\noutcomes. A key feature of the approach is that it can remove the purely\ntemporal component of variability in PM2.5 levels and calculate effect\nestimates derived solely from spatial contrasts. This eliminates the potential\nfor unmeasured confounding of the exposure - outcome associations by temporal\nfactors, such as season. We apply our method to a study of birth weights in\nMassachusetts, U.S.A from 2003-2008 and find that both local and urban sources\nof pollution are strongly negatively associated with birth weight. Results also\nsuggest that failure to eliminate temporal confounding in previous analyses\nattenuated the overall effect estimate towards zero, with the effect estimate\ngrowing in magnitude once this source of variability is removed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 19:26:29 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Antonelli", "Joseph", ""], ["Schwartz", "Joel", ""], ["Kloog", "Itai", ""], ["Coull", "Brent", ""]]}, {"id": "1609.05188", "submitter": "Jose Ameijeiras-Alonso", "authors": "Jose Ameijeiras-Alonso, Rosa M. Crujeiras and Alberto\n  Rodr\\'iguez-Casal", "title": "Mode testing, critical bandwidth and excess mass", "comments": null, "journal-ref": "Test 28, 900-919 (2019)", "doi": "10.1007/s11749-018-0611-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of peaks or maxima in probability densities, by mode\ntesting or bump hunting, has become an important problem in applied fields.\nThis task has been approached in the statistical literature from different\nperspectives, with the proposal of testing procedures which are based on kernel\ndensity estimators or on the quantification of excess mass. However, none of\nthe existing proposals provides a satisfactory performance in practice. In this\nwork, a new procedure which combines the previous approaches (smoothing and\nexcess mass) is presented and compared with the existing methods, showing a\nsuperior behaviour. A real data example on philatelic data is also included for\nillustration purposes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 19:33:57 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 16:35:00 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 15:35:47 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ameijeiras-Alonso", "Jose", ""], ["Crujeiras", "Rosa M.", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1609.05372", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Permutation and Grouping Methods for Sharpening Gaussian Process\n  Approximations", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2018.1437476", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vecchia's approximate likelihood for Gaussian process parameters depends on\nhow the observations are ordered, which can be viewed as a deficiency because\nthe exact likelihood is permutation-invariant. This article takes the\nalternative standpoint that the ordering of the observations can be tuned to\nsharpen the approximations. Advantageously chosen orderings can drastically\nimprove the approximations, and in fact, completely random orderings often\nproduce far more accurate approximations than default coordinate-based\norderings do. In addition to the permutation results, automatic methods for\ngrouping calculations of components of the approximation are introduced, having\nthe result of simultaneously improving the quality of the approximation and\nreducing its computational burden. In common settings, reordering combined with\ngrouping reduces Kullback-Leibler divergence from the target model by a factor\nof 80 and computation time by a factor of 2 compared to ungrouped\napproximations with default ordering. The claims are supported by theory and\nnumerical results with comparisons to other approximations, including tapered\ncovariances and stochastic partial differential equation approximations.\nComputational details are provided, including efficiently finding the orderings\nand ordered nearest neighbors, and profiling out linear mean parameters and\nusing the approximations for prediction and conditional simulation. An\napplication to space-time satellite data is presented.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 18:32:52 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 15:04:52 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1609.05519", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic", "title": "Some copula inference procedures adapted to the presence of ties", "comments": "31 pages, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling the distribution of a multivariate continuous random vector\nusing the so-called \\emph{copula approach}, it is not uncommon to have ties in\nthe coordinate samples of the available data because of rounding or lack of\nmeasurement precision. Yet, the vast majority of existing inference procedures\non the underlying copula were both theoretically derived and practically\nimplemented under the assumption of no ties. Applying them nonetheless can lead\nto strongly biased results. Some of the existing statistical tests can however\nbe adapted to provide meaningful results in the presence of ties. It is the\ncase of some tests of exchangeability, radial symmetry, extreme-value\ndependence and goodness of fit. Detailed algorithms for computing approximate\np-values for the modified tests are provided and their finite-sample behaviors\nare empirically investigated through extensive Monte Carlo experiments. An\nillustration on a real-world insurance data set concludes the work.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 17:28:55 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 14:54:01 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 14:36:41 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Kojadinovic", "Ivan", ""]]}, {"id": "1609.05530", "submitter": "Erin Conlon", "authors": "Zheng Wei, Daeyoung Kim and Erin Marie Conlon", "title": "Parallel Computing for Copula Parameter Estimation with Big Data: A\n  Simulation Study", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula-based modeling has seen rapid advances in recent years. However, in\nbig data applications, the lengthy computation time for estimating copula\nparameters is a major difficulty. Here, we develop a novel method to speed\ncomputation time in estimating copula parameters, using communication-free\nparallel computing. Our procedure partitions full data sets into disjoint\nindependent subsets, performs copula parameter estimation on the subsets, and\ncombines the results to produce an approximation to the full data copula\nparameter. We show in simulation studies that the computation time is greatly\nreduced through our method, using three well-known one-parameter bivariate\ncopulas within the elliptical and Archimedean families: Gaussian, Frank and\nGumbel. In addition, our simulation studies find small values for estimated\nbias, estimated mean squared error, and estimated relative L1 and L2 errors for\nour method, when compared to the full data parameter estimates.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 19:19:37 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wei", "Zheng", ""], ["Kim", "Daeyoung", ""], ["Conlon", "Erin Marie", ""]]}, {"id": "1609.05564", "submitter": "Paul Ginzberg", "authors": "Paul Ginzberg, Federico Giorgi, Andrea Califano", "title": "Searching for Gene Sets with Mutually Exclusive Mutations", "comments": "Submitted to the proceedings of the American Statistical Association\n  Joint Statistical Meetings (JSM), held in Chicago in 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer cells evolve through random somatic mutations. \"Beneficial\" mutations\nwhich disrupt key pathways (e.g. cell cycle regulation) are subject to natural\nselection. Multiple mutations may lead to the same \"beneficial\" effect, in\nwhich case there is no selective advantage to having more than one of these\nmutations. Hence we are interested in finding sets of genes whose mutations are\napproximately mutually exclusive (anti-co-occurring) within the TCGA Pancancer\ndataset. In principle, finding the best set is NP Hard. Nevertheless, we will\nshow how a new Mutation anti-co-OCcurrence Algorithm (MOCA) provides an\neffective greedy search and testing algorithm with guaranteed control of the\nfamilywise error rate or false discovery rate, by combining some\nunder-appreciated ideas from frequentist hypothesis testing. These ideas\ninclude: (a) A novel exact conditional test for the tendency of multiple sets\nto have a large/small union/intersection, which generalises Fisher's exact test\nof 2x2 tables. (b) Randomised hypothesis tests for discrete distributions. (c)\nStouffer's method for combining p-values. (d) Weighted multiple hypothesis\ntesting. A new approach to setting a-priori weights which generates additional\nimplicit hypothesis tests is suggested, and allows us to preserve almost all\nstatistical power when testing pairs despite introducing a combinatorially\nlarge number of additional hypotheses.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 23:06:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ginzberg", "Paul", ""], ["Giorgi", "Federico", ""], ["Califano", "Andrea", ""]]}, {"id": "1609.05615", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei", "title": "Discussion of \"Fast Approximate Inference for Arbitrarily Large\n  Semiparametric Regression Models via Message Passing\"", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion paper on \"Fast Approximate Inference for Arbitrarily Large\nSemiparametric Regression Models via Message Passing\" by Wand\n[arXiv:1602.07412].\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 07:24:21 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1609.05684", "submitter": "Francisco Javier Rubio", "authors": "F. J. Rubio and M. F. J. Steel", "title": "Flexible linear mixed models with improper priors for longitudinal and\n  survival data", "comments": "To appear in the Electronic Journal of Statistics. Additional R codes\n  can be found at: http://rpubs.com/FJRubio", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach using improper priors for hierarchical linear\nmixed models with flexible random effects and residual error distributions. The\nerror distribution is modelled using scale mixtures of normals, which can\ncapture tails heavier than those of the normal distribution. This\ngeneralisation is useful to produce models that are robust to the presence of\noutliers. The case of asymmetric residual errors is also studied. We present\ngeneral results for the propriety of the posterior that also cover cases with\ncensored observations, allowing for the use of these models in the contexts of\npopular longitudinal and survival analyses. We consider the use of copulas with\nflexible marginals for modelling the dependence between the random effects, but\nour results cover the use of any random effects distribution. Thus, our paper\nprovides a formal justification for Bayesian inference in a very wide class of\nmodels (covering virtually all of the literature) under attractive prior\nstructures that limit the amount of required user elicitation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 12:17:35 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 12:08:51 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 15:09:26 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rubio", "F. J.", ""], ["Steel", "M. F. J.", ""]]}, {"id": "1609.05805", "submitter": "Debasis Kundu Professor", "authors": "Shuvashree Mondal and Debasis Kundu", "title": "A New Two Sample Type-II Progressive Censoring Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive censoring scheme has received considerable attention in recent\nyears. In this paper we introduce a new type-II progressive censoring scheme\nfor two samples. It is observed that the proposed censoring scheme is\nanalytically more tractable than the existing joint progressive type-II\ncensoring scheme proposed by Rasouli and Balakrishnan \\cite{RB:2010}. It has\nsome other advantages also. We study the statistical inference of the unknown\nparameters based on the assumptions that the lifetime distribution of the\nexperimental units for the two samples follow exponential distribution with\ndifferent scale parameters. The maximum likelihood estimators of the unknown\nparameters are obtained and their exact distributions are derived. Based on the\nexact distributions of the maximum likelihood estimators exact confidence\nintervals are also constructed. For comparison purposes we have used bootstrap\nconfidence intervals also. It is observed that the bootstrap confidence\nintervals work very well and they are very easy to implement in practice. Some\nsimulation experiments are performed to compare the performances of the\nproposed method with the existing one, and the performances of the proposed\nmethod are quite satisfactory. One data analysis has been performed for\nillustrative purposes. Finally we propose some open problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 16:00:03 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Mondal", "Shuvashree", ""], ["Kundu", "Debasis", ""]]}, {"id": "1609.06031", "submitter": "Subhajit Dutta Dr.", "authors": "Minerva Mukhopadhyay and Subhajit Dutta", "title": "Bayesian Variable Selection for Ultrahigh-dimensional Sparse Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian variable selection procedure for ultrahigh-dimensional\nlinear regression models. The number of regressors involved in regression,\n$p_n$, is allowed to grow exponentially with $n$. Assuming the true model to be\nsparse, in the sense that only a small number of regressors contribute to this\nmodel, we propose a set of priors suitable for this regime. The model selection\nprocedure based on the proposed set of priors is shown to be variable selection\nconsistent when all the $2^{p_n}$ models are considered. In the\nultrahigh-dimensional setting, selection of the true model among all the\n$2^{p_n}$ possible ones involves prohibitive computation. To cope with this, we\npresent a two-step model selection algorithm based on screening and Gibbs\nsampling. The first step of screening discards a large set of unimportant\ncovariates, and retains a smaller set containing all the active covariates with\nprobability tending to one. In the next step, we search for the best model\namong the covariates obtained in the screening step. This procedure is\ncomputationally quite fast, simple and intuitive. We demonstrate competitive\nperformance of the proposed algorithm for a variety of simulated and real data\nsets when compared with several frequentist, as well as Bayesian methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 06:34:27 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Dutta", "Subhajit", ""]]}, {"id": "1609.06035", "submitter": "Lihua Lei", "authors": "Lihua Lei and William Fithian", "title": "AdaPT: An interactive procedure for multiple testing with side\n  information", "comments": "Accepted by JRSS-B; Develop an R package adaptMT\n  (https://github.com/lihualei71/adaptMT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multiple hypothesis testing with generic side\ninformation: for each hypothesis $H_i$ we observe both a p-value $p_i$ and some\npredictor $x_i$ encoding contextual information about the hypothesis. For\nlarge-scale problems, adaptively focusing power on the more promising\nhypotheses (those more likely to yield discoveries) can lead to much more\npowerful multiple testing procedures. We propose a general iterative framework\nfor this problem, called the Adaptive p-value Thresholding (AdaPT) procedure,\nwhich adaptively estimates a Bayes-optimal p-value rejection threshold and\ncontrols the false discovery rate (FDR) in finite samples. At each iteration of\nthe procedure, the analyst proposes a rejection threshold and observes\npartially censored p-values, estimates the false discovery proportion (FDP)\nbelow the threshold, and either stops to reject or proposes another threshold,\nuntil the estimated FDP is below $\\alpha$. Our procedure is adaptive in an\nunusually strong sense, permitting the analyst to use any statistical or\nmachine learning method she chooses to estimate the optimal threshold, and to\nswitch between different models at each iteration as information accrues. We\ndemonstrate the favorable performance of AdaPT by comparing it to\nstate-of-the-art methods in five real applications and two simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 06:49:04 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 07:09:10 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 09:14:29 GMT"}, {"version": "v4", "created": "Wed, 25 Jul 2018 02:48:09 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Lei", "Lihua", ""], ["Fithian", "William", ""]]}, {"id": "1609.06070", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Sarah Brockhaus, Kornelia Gentsch, Klaus Scherer and\n  Sonja Greven", "title": "Boosting Factor-Specific Functional Historical Models for the Detection\n  of Synchronisation in Bioelectrical Signals", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The link between different psychophysiological measures during emotion\nepisodes is not well understood. To analyse the functional relationship between\nelectroencephalography (EEG) and facial electromyography (EMG), we apply\nhistorical function-on-function regression models to EEG and EMG data that were\nsimultaneously recorded from 24 participants while they were playing a\ncomputerised gambling task. Given the complexity of the data structure for this\napplication, we extend simple functional historical models to models including\nrandom historical effects, factor-specific historical effects, and\nfactor-specific random historical effects. Estimation is conducted by a\ncomponent-wise gradient boosting algorithm, which scales well to large data\nsets and complex models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 09:42:32 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 09:31:56 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Brockhaus", "Sarah", ""], ["Gentsch", "Kornelia", ""], ["Scherer", "Klaus", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.06076", "submitter": "Nicolas Dobigeon", "authors": "Vinicius Ferraris, Nicolas Dobigeon, Qi Wei and Marie Chabert", "title": "Robust Fusion of Multi-Band Images with Different Spatial and Spectral\n  Resolutions for Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal scenarios for change detection generally consider two images\nacquired through sensors of the same modality. However, in some specific cases\nsuch as emergency situations, the only images available may be those acquired\nthrough different kinds of sensors. More precisely, this paper addresses the\nproblem of detecting changes between two multi-band optical images\ncharacterized by different spatial and spectral resolutions. This sensor\ndissimilarity introduces additional issues in the context of operational change\ndetection. To alleviate these issues, classical change detection methods are\napplied after independent preprocessing steps (e.g., resampling) used to get\nthe same spatial and spectral resolutions for the pair of observed images.\nNevertheless, these preprocessing steps tend to throw away relevant\ninformation. Conversely, in this paper, we propose a method that more\neffectively uses the available information by modeling the two observed images\nas spatial and spectral versions of two (unobserved) latent images\ncharacterized by the same high spatial and high spectral resolutions. As they\ncover the same scene, these latent images are expected to be globally similar\nexcept for possible changes in sparse spatial locations. Thus, the change\ndetection task is envisioned through a robust multi-band image fusion method\nwhich enforces the differences between the estimated latent images to be\nspatially sparse. This robust fusion problem is formulated as an inverse\nproblem which is iteratively solved using an efficient block-coordinate descent\nalgorithm. The proposed method is applied to real panchormatic/multispectral\nand hyperspectral images with simulated realistic changes. A comparison with\nstate-of-the-art change detection methods evidences the accuracy of the\nproposed strategy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 10:04:04 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Ferraris", "Vinicius", ""], ["Dobigeon", "Nicolas", ""], ["Wei", "Qi", ""], ["Chabert", "Marie", ""]]}, {"id": "1609.06245", "submitter": "Laura Forastiere", "authors": "Laura Forastiere, Edoardo M. Airoldi, Fabrizia Mealli", "title": "Identification and estimation of treatment and interference effects in\n  observational studies on networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference on a population of units connected through a network often\npresents technical challenges, including how to account for interference. In\nthe presence of local interference, for instance, potential outcomes of a unit\ndepend on its treatment as well as on the treatments of other local units, such\nas its neighbors according to the network. In observational studies, a further\ncomplication is that the typical unconfoundedness assumption must be extended -\nsay, to include the treatment of neighbors, and indi- vidual and neighborhood\ncovariates - to guarantee identification and valid inference. Here, we propose\nnew estimands that define treatment and interference effects. We then derive\nanalytical expressions for the bias of a naive estimator that wrongly assumes\naway interference. The bias depends on the level of interference but also on\nthe degree of association between individual and neighborhood treatments. We\npropose an extended unconfoundedness assumption that accounts for interference,\nand we develop new covariate-adjustment methods that lead to valid estimates of\ntreatment and interference effects in observational studies on networks.\nEstimation is based on a generalized propensity score that balances individual\nand neighborhood covariates across units under different levels of individual\ntreatment and of exposure to neighbors' treatment. We carry out simulations,\ncalibrated using friendship networks and covariates in a nationally\nrepresentative longitudinal study of adolescents in grades 7-12, in the United\nStates, to explore finite-sample performance in different realistic settings.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 16:38:26 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 19:24:55 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 04:24:12 GMT"}, {"version": "v4", "created": "Fri, 30 Mar 2018 01:25:31 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Forastiere", "Laura", ""], ["Airoldi", "Edoardo M.", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1609.06339", "submitter": "Mathias Trabs", "authors": "Tobias Niebuhr and Mathias Trabs", "title": "Profiting from correlations: Adjusted estimators for categorical data", "comments": "14 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To take sample biases and skewness in the observations into account,\npractitioners frequently weight their observations according to some marginal\ndistribution. The present paper demonstrates that such weighting can indeed\nimprove the estimation. Studying contingency tables, estimators for marginal\ndistributions are proposed under the assumption that another marginal is known.\nIt is shown that the weighted estimators have a strictly smaller asymptotic\nvariance whenever the two marginals are correlated. The finite sample\nperformance is illustrated in a simulation study. As an application to traffic\naccident data the method allows for correcting a well-known bias in the\nobserved injury severity distribution.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 20:21:37 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 14:55:25 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 06:57:28 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Niebuhr", "Tobias", ""], ["Trabs", "Mathias", ""]]}, {"id": "1609.06465", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci, Francesco Bartolucci, Leonardo Grilli, and Carla\n  Rampichini", "title": "Evaluation of student proficiency through a multidimensional finite\n  mixture IRT model", "comments": "24 pages, 9 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In certain academic systems, a student can enroll for an exam immediately\nafter the end of the teaching period or can postpone it to any later\nexamination session, so that the grade is missing until the exam is not\nattempted. We propose an approach for the evaluation in itinere of a student's\nproficiency accounting also for non-attempted exams. The approach is based on\nconsidering each exam as an item, so that responding to the item amounts to\nattempting the exam, and on an Item Response Theory model that includes two\nlatent variables corresponding to the student's ability and the propensity to\nattempt the exam. In this way, we explicitly account for non-ignorable missing\nobservations as the indicators of item response also contribute to measure the\nability. The two latent variables are assumed to have a discrete distribution\ndefining latent classes of students that are homogeneous in terms of ability\nand priority assigned to exams. The model, which also allows for individual\ncovariates in its structural part, is fitted by the Expectation-Maximization\nalgorithm. The approach is illustrated through the analysis of data about the\nfirst-year exams of freshmen of the School of Economics at the University of\nFlorence (Italy).\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 08:43:25 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""], ["Grilli", "Leonardo", ""], ["Rampichini", "Carla", ""]]}, {"id": "1609.06697", "submitter": "Felix Ballani", "authors": "Markus Baaske, Felix Ballani, Alexandra Illgen", "title": "On the estimation of parameters of a spheroid distribution from planar\n  sections", "comments": "20 pages, 5 figures (including 16 images)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two different methods for inferring the parameters of a spheroid\ndistribution from planar sections of a stationary spatial system of spheroids:\none method first unfolds non-parametrically the joint size-shape-orientation\ndistribution of the observable ellipses in the plane into the joint\nsize-shape-orientation distribution of the spheroids followed by a maximum\nlikelihood estimation of the parameters; the second method directly estimates\nthese parameters based on statistics of the observable ellipses using a\nquasi-likelihood approach. As an application we consider a metal-matrix\ncomposite with ceramic particles as reinforcing inclusions, model the\ninclusions as prolate spheroids and estimate the parameters of their\ndistribution from planar sections.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 19:40:22 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:00:41 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Baaske", "Markus", ""], ["Ballani", "Felix", ""], ["Illgen", "Alexandra", ""]]}, {"id": "1609.06789", "submitter": "Da Huang", "authors": "Da Huang, Qiwei Yao, Rongmao Zhang", "title": "Krigings Over Space and Time Based on Latent Low-Dimensional Structures", "comments": "35 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to represent nonparametrically the linear\ndependence structure of a spatio-temporal process in terms of latent common\nfactors. Though it is formally similar to the existing reduced rank\napproximation methods (Section 7.1.3 of Cressie and Wikle, 2011), the\nfundamental difference is that the low-dimensional structure is completely\nunknown in our setting, which is learned from the data collected irregularly\nover space but regularly over time. Furthermore a graph Laplacian is\nincorporated in the learning in order to take the advantage of the continuity\nover space, and a new aggregation method via randomly partitioning space is\nintroduced to improve the efficiency. We do not impose any stationarity\nconditions over space either, as the learning is facilitated by the\nstationarity in time. Krigings over space and time are carried out based on the\nlearned low-dimensional structure, which is scalable to the cases when the data\nare taken over a large number of locations and/or over a long time period.\nAsymptotic properties of the proposed methods are established. Illustration\nwith both simulated and real data sets is also reported.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 01:04:45 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 11:32:39 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Huang", "Da", ""], ["Yao", "Qiwei", ""], ["Zhang", "Rongmao", ""]]}, {"id": "1609.06874", "submitter": "Facundo Costa", "authors": "Facundo Costa, Hadj Batatia, Thomas Oberlin and Jean-Yves Tourneret", "title": "EEG reconstruction and skull conductivity estimation using a Bayesian\n  model promoting structured sparsity", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M/EEG source localization is an open research issue. To solve it, it is\nimportant to have good knowledge of several physical parameters to build a\nreliable head operator. Amongst them, the value of the conductivity of the\nhuman skull has remained controversial. This report introduces a novel\nhierarchical Bayesian framework to estimate the skull conductivity jointly with\nthe brain activity from the M/EEG measurements to improve the reconstruction\nquality. A partially collapsed Gibbs sampler is used to draw samples\nasymptotically distributed according to the associated posterior. The generated\nsamples are then used to estimate the brain activity and the model\nhyperparameters jointly in a completely unsupervised framework. We use\nsynthetic and real data to illustrate the improvement of the reconstruction.\nThe performance of our method is also compared with two optimization algorithms\nintroduced by Vallagh\\'e \\textit{et al.} and Gutierrez \\textit{et al.}\nrespectively, showing that our method is able to provide results of similar or\nbetter quality while remaining applicable in a wider array of situations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 09:02:01 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 20:56:38 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Costa", "Facundo", ""], ["Batatia", "Hadj", ""], ["Oberlin", "Thomas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1609.06926", "submitter": "Konstantinos Perrakis", "authors": "Dimitris Fouskakis, Ioannis Ntzoufras, Konstantinos Perrakis", "title": "Variations of Power-Expected-Posterior Priors in Normal Regression\n  Models", "comments": null, "journal-ref": "Computational Statistics and Data Analysis Volume 143, March 2020,\n  106836", "doi": "10.1016/j.csda.2019.106836", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power-expected-posterior (PEP) prior is an objective prior for Gaussian\nlinear models, which leads to consistent model selection inference, under the\nM-closed scenario, and tends to favor parsimonious models. Recently, two new\nforms of the PEP prior were proposed which generalize its applicability to a\nwider range of models. The properties of these two PEP variants within the\ncontext of the normal linear model are examined thoroughly, focusing on the\nprior dispersion and on the consistency of the induced model selection\nprocedure. Results show that both PEP variants have larger variances than the\nunit-information g-prior and that they are M-closed consistent as the limiting\nbehavior of the corresponding marginal likelihoods matches that of the BIC. The\nconsistency under the M-open case, using three different model misspecification\nscenarios is further investigated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 11:53:44 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 11:47:39 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 14:06:18 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 17:11:25 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Fouskakis", "Dimitris", ""], ["Ntzoufras", "Ioannis", ""], ["Perrakis", "Konstantinos", ""]]}, {"id": "1609.06968", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine PSL, University of\n  Warwick) and Judith Rousseau (Universit\\'e Paris-Dauphine PSL)", "title": "Some comments about \"Penalising model component complexity\" by Simpson\n  et al. (2017)", "comments": "7 pages, to appear as a discussion in Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note discusses the paper \"Penalising model component complexity\" by\nSimpson et al. (2017). While we acknowledge the highly novel approach to prior\nconstruction and commend the authors for setting new-encompassing principles\nthat will Bayesian modelling, and while we perceive the potential connection\nwith other branches of the literature, we remain uncertain as to what extent\nthe principles exposed in the paper can be developed outside specific models,\ngiven their lack of precision. The very notions of model component, base model,\noverfitting prior are for instance conceptual rather than mathematical and we\nthus fear the concept of penalised complexity may not further than extending\nfirst-guess priors into larger families, thus failing to establish reference\npriors on a novel sound ground.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 13:34:47 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine PSL, University of\n  Warwick"], ["Rousseau", "Judith", "", "Universit\u00e9 Paris-Dauphine PSL"]]}, {"id": "1609.07007", "submitter": "Jona Cederbaum", "authors": "Jona Cederbaum, Fabian Scheipl and Sonja Greven", "title": "Fast symmetric additive covariance smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast bivariate smoothing approach for symmetric surfaces that\nhas a wide range of applications. We show how it can be applied to estimate the\ncovariance function in longitudinal data as well as multiple additive\ncovariances in functional data with complex correlation structures. Our\nsymmetric smoother can handle (possibly noisy) data sampled on a common, dense\ngrid as well as irregularly or sparsely sampled data. Estimation is based on\nbivariate penalized spline smoothing using a mixed model representation and the\nsymmetry is used to reduce computation time compared to the usual non-symmetric\nsmoothers. We outline the application of our approach in functional principal\ncomponent analysis and demonstrate its practical value in two applications. The\napproach is evaluated in extensive simulations. We provide documented open\nsource software implementing our fast symmetric bivariate smoother building on\nestablished algorithms for additive models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:00:25 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cederbaum", "Jona", ""], ["Scheipl", "Fabian", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.07165", "submitter": "Jelena Bradic", "authors": "Jelena Bradic and Jiaqi Guo", "title": "Robust Confidence Intervals in High-Dimensional Left-Censored Regression", "comments": "62 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops robust confidence intervals in high-dimensional and\nleft-censored regression. Type-I censored regression models are extremely\ncommon in practice, where a competing event makes the variable of interest\nunobservable. However, techniques developed for entirely observed data do not\ndirectly apply to the censored observations. In this paper, we develop smoothed\nestimating equations that augment the de-biasing method, such that the\nresulting estimator is adaptive to censoring and is more robust to the\nmisspecification of the error distribution. We propose a unified class of\nrobust estimators, including Mallow's, Schweppe's and Hill-Ryan's one-step\nestimator.\n  In the ultra-high-dimensional setting, where the dimensionality can grow\nexponentially with the sample size, we show that as long as the preliminary\nestimator converges faster than $n^{-1/4}$, the one-step estimator inherits\nasymptotic distribution of fully iterated version. Moreover, we show that the\nsize of the residuals of the Bahadur representation matches those of the simple\nlinear models, $s^{3/4 } (\\log (p \\vee n))^{3/4} / n^{1/4}$ -- that is, the\neffects of censoring asymptotically disappear. Simulation studies demonstrate\nthat our method is adaptive to the censoring level and asymmetry in the error\ndistribution, and does not lose efficiency when the errors are from symmetric\ndistributions. Finally, we apply the developed method to a real data set from\nthe MAQC-II repository that is related to the HIV-1 study.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 21:09:43 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bradic", "Jelena", ""], ["Guo", "Jiaqi", ""]]}, {"id": "1609.07195", "submitter": "Johannes Lederer", "authors": "Mahsa Taheri, N\\'eh\\'emy Lim, and Johannes Lederer", "title": "Balancing Statistical and Computational Precision and Applications to\n  Penalized Linear Regression with Group Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to technological advances, large and high-dimensional data have become\nthe rule rather than the exception. Methods that allow for feature selection\nwith such data are thus highly sought after, in particular, since standard\nmethods, such as cross-validated lasso and group-lasso, can be challenging both\ncomputationally and mathematically. In this paper, we propose a novel approach\nto feature selection and group feature selection in linear regression. It\nconsists of simple optimization steps and tests, which makes it computationally\nmore efficient than standard approaches and suitable even for very large data\nsets. Moreover, it satisfies sharp guarantees for estimation and feature\nselection in terms of oracle inequalities. We thus expect that our contribution\ncan help to leverage the increasing volume of data in Biology, Public Health,\nAstronomy, Economics, and other fields.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 00:35:23 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 20:31:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Taheri", "Mahsa", ""], ["Lim", "N\u00e9h\u00e9my", ""], ["Lederer", "Johannes", ""]]}, {"id": "1609.07217", "submitter": "Xiao Liu", "authors": "Xiao Liu, Kyongmin Yeo, Jayant Kalagnanam", "title": "Statistical Modeling for Spatio-Temporal Degradation Data", "comments": "30 pages, 7 figures. Manuscript prepared for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the modeling of an important class of degradation\ndata, which are collected from a spatial domain over time; for example, the\nsurface quality degradation. Like many existing time-dependent stochastic\ndegradation models, a special random field is constructed for modeling the\nspatio-temporal degradation process. In particular, we express the degradation\nat any spatial location and time as an additive superposition of two stochastic\ncomponents: a dynamic spatial degradation generation process, and a\nspatio-temporal degradation propagation process. Some unique challenges are\naddressed, including the spatial heterogeneity of the degradation process, the\nspatial propagation of degradation to neighboring areas, the anisotropic and\nspace-time non-separable covariance structure often associated with a complex\nspatio-temporal degradation process, and the computational issue related to\nparameter estimation. When the spatial dependence is ignored, we show that the\nproposed spatio-temporal degradation model incorporates some existing pure\ntime-dependent degradation processes as its special cases. We also show the\nconnection, under special conditions, between the proposed model and general\nphysical degradation processes which are often defined by stochastic partial\ndifferential equations. A numerical example is presented to illustrate the\nmodeling approach and model validation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 03:16:10 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 21:27:33 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Liu", "Xiao", ""], ["Yeo", "Kyongmin", ""], ["Kalagnanam", "Jayant", ""]]}, {"id": "1609.07233", "submitter": "Norbert Remenyi", "authors": "Norbert Remenyi", "title": "Fully Bayesian Estimation and Variable Selection in Partially Linear\n  Wavelet Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a wavelet-based methodology for estimation and\nvariable selection in partially linear models. The inference is conducted in\nthe wavelet domain, which provides a sparse and localized decomposition\nappropriate for nonparametric components with various degrees of smoothness. A\nhierarchical Bayes model is formulated on the parameters of this\nrepresentation, where the estimation and variable selection is performed by a\nGibbs sampling procedure. For both the parametric and nonparametric part of the\nmodel we are using point-mass-at-zero contamination priors with a double\nexponential spread distribution. Only a few papers in the area of partially\nlinear wavelet models exist, and we show that the proposed methodology is often\nsuperior to the existing methods with respect to the task of estimating model\nparameters. Moreover, the method is able to perform Bayesian variable selection\nby a stochastic search for the parametric part of the model.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 05:20:09 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Remenyi", "Norbert", ""]]}, {"id": "1609.07330", "submitter": "Nirian Mart\\'in", "authors": "Juana M. Alonso-Revenga, Nirian Martin, Leandro Pardo", "title": "Semiparametric clustered overdispersed multinomial goodness-of-fit of\n  log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the Dirichlet-multinomial distribution has been recognized as\na key model for contingency tables generated by cluster sampling schemes. There\nare, however, other possible distributions appropriate for these contingency\ntables. This paper introduces new test-statistics capable to test log-linear\nmodeling hypotheses with no distributional specification, when the individuals\nof the clusters are possibly homogeneously correlated. The estimator for the\nintracluster correlation coefficient proposed in Alonso-Revenga et al. (2016),\nvalid for different cluster sizes, plays a crucial role in the construction of\nthe goodness-of-fit test-statistic.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 12:03:27 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Alonso-Revenga", "Juana M.", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1609.07363", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Guillem Rigaill", "title": "Changepoint Detection in the Presence of Outliers", "comments": "Updated to include a proof of consistency and accuracy of estimating\n  change points using the biweight loss function", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 10:56:16 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Fearnhead", "Paul", ""], ["Rigaill", "Guillem", ""]]}, {"id": "1609.07494", "submitter": "Kari Lock Morgan", "authors": "Fan Li, Kari Lock Morgan, and Alan M. Zaslavsky", "title": "Balancing Covariates via Propensity Score Weighting", "comments": "This is instead available as a new version of a previous submission,\n  arXiv:1404.1785", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate balance is crucial for unconfounded descriptive or causal\ncomparisons. However, lack of balance is common in observational studies. This\narticle considers weighting strategies for balancing covariates. We define a\ngeneral class of weights---the balancing weights---that balance the weighted\ndistributions of the covariates between treatment groups. These weights\nincorporate the propensity score to weight each group to an analyst-selected\ntarget population. This class unifies existing weighting methods, including\ncommonly used weights such as inverse-probability weights as special cases.\nGeneral large-sample results on nonparametric estimation based on these weights\nare derived. We further propose a new weighting scheme, the overlap weights, in\nwhich each unit's weight is proportional to the probability of that unit being\nassigned to the opposite group. The overlap weights are bounded, and minimize\nthe asymptotic variance of the weighted average treatment effect among the\nclass of balancing weights. The overlap weights also possess a desirable\nsmall-sample exact balance property, based on which we propose a new method\nthat achieves exact balance for means of any selected set of covariates. Two\napplications illustrate these methods and compare them with other approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 20:00:18 GMT"}, {"version": "v2", "created": "Thu, 29 Sep 2016 01:00:54 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Li", "Fan", ""], ["Morgan", "Kari Lock", ""], ["Zaslavsky", "Alan M.", ""]]}, {"id": "1609.07630", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake, V. A. Coutinho", "title": "Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform", "comments": "Fixed diagonal matrix, 11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2515378", "report-no": null, "categories": "cs.MM cs.CV cs.DS stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 14:49:31 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 21:18:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 17:05:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Coutinho", "V. A.", ""]]}, {"id": "1609.07637", "submitter": "V\\'eronique Maume-Deschamps", "authors": "V\\'eronique Maume-Deschamps and Didier Rulli\\`ere and Khalil Sa\\\"id", "title": "Multivariate extensions of expectiles risk measures", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the introduction and study of a new family of\nmultivariate elicitable risk measures. We call the obtained vector-valued\nmeasures multivariate expectiles. We present the different approaches used to\nconstruct our measures. We discuss the coherence properties of these\nmultivariate expectiles. Furthermore, we propose a stochastic approximation\ntool of these risk measures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 15:22:24 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Maume-Deschamps", "V\u00e9ronique", ""], ["Rulli\u00e8re", "Didier", ""], ["Sa\u00efd", "Khalil", ""]]}, {"id": "1609.07662", "submitter": "Evgeny Burnaev", "authors": "Alexey Artemov and Evgeny Burnaev", "title": "Detecting Performance Degradation of Software-Intensive Systems in the\n  Presence of Trends and Long-Range Dependence", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As contemporary software-intensive systems reach increasingly large scale, it\nis imperative that failure detection schemes be developed to help prevent\ncostly system downtimes. A promising direction towards the construction of such\nschemes is the exploitation of easily available measurements of system\nperformance characteristics such as average number of processed requests and\nqueue size per unit of time. In this work, we investigate a holistic\nmethodology for detection of abrupt changes in time series data in the presence\nof quasi-seasonal trends and long-range dependence with a focus on failure\ndetection in computer systems. We propose a trend estimation method enjoying\noptimality properties in the presence of long-range dependent noise to estimate\nwhat is considered \"normal\" system behaviour. To detect change-points and\nanomalies, we develop an approach based on the ensembles of \"weak\" detectors.\nWe demonstrate the performance of the proposed change-point detection scheme\nusing an artificial dataset, the publicly available Abilene dataset as well as\nthe proprietary geoinformation system dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 19:20:18 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1609.07690", "submitter": "Lazhi Wang", "authors": "Lazhi Wang, David E. Jones, and Xiao-Li Meng", "title": "Warp Bridge Sampling: The Next Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridge sampling is an effective Monte Carlo method for estimating the ratio\nof normalizing constants of two probability densities, a routine computational\nproblem in statistics, physics, chemistry, and other fields. The Monte Carlo\nerror of the bridge sampling estimator is determined by the amount of overlap\nbetween the two densities. In the case of uni-modal densities, Warp-I, II, and\nIII transformations (Meng and Schilling, 2002) are effective for increasing the\ninitial overlap, but they are less so for multi-modal densities. This paper\nintroduces Warp-U transformations that aim to transform multi-modal densities\ninto Uni-modal ones without altering their normalizing constants. The\nconstruction of a Warp-U transformation starts with a Normal (or other\nconvenient) mixture distribution $\\phi_{\\text{mix}}$ that has reasonable\noverlap with the target density $p$, whose normalizing constant is unknown. The\nstochastic transformation that maps $\\phi_{\\text{mix}}$ back to its generating\ndistribution $N(0,1)$ is then applied to $p$ yielding its Warp-U version, which\nwe denote $\\tilde{p}$. Typically, $\\tilde{p}$ is uni-modal and has\nsubstantially increased overlap with $N(0,1)$. Furthermore, we prove that the\noverlap between $\\tilde{p}$ and $N(0,1)$ is guaranteed to be no less than the\noverlap between $p$ and $\\phi_{\\text{mix}}$, in terms of any $f$-divergence. We\npropose a computationally efficient method to find an appropriate\n$\\phi_{\\text{mix}}$, and a simple but effective approach to remove the bias\nwhich results from estimating the normalizing constants and fitting\n$\\phi_{\\text{mix}}$ with the same data. We illustrate our findings using 10 and\n50 dimensional highly irregular multi-modal densities, and demonstrate how\nWarp-U sampling can be used to improve the final estimation step of the\nGeneralized Wang-Landau algorithm (Liang, 2005), a powerful sampling and\nestimation method.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 02:10:30 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 13:46:25 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Wang", "Lazhi", ""], ["Jones", "David E.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1609.07714", "submitter": "Ben Youngman", "authors": "Benjamin D. Youngman and David B. Stephenson", "title": "Inference for spatial processes using imperfect data from measurements\n  and numerical simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for inference for spatial processes that have actual\nvalues imperfectly represented by data. Environmental processes represented as\nspatial fields, either at fixed time points, or aggregated over fixed time\nperiods, are studied. Data from both measurements and simulations performed by\ncomplex computer models are used to infer actual values of the spatial fields.\n  Methods from geostatistics and statistical emulation are used to explicitly\ncapture discrepancies between a spatial field's actual and simulated values. A\ngeostatistical model captures spatial discrepancy: the difference in spatial\nstructure between simulated and actual values. An emulator represents the\nintensity discrepancy: the bias in simulated values of given intensity.\nMeasurement error is also represented. Gaussian process priors represent each\nsource of error, which gives an analytical expression for the posterior\ndistribution for the actual spatial field.\n  Actual footprints for 50 European windstorms, which represent maximum wind\ngust speeds on a grid over a 72-hour period, are derived from wind gust speed\nmeasurements taken at stations across Europe and output simulated from a\ndownscaled version of the Met Office Unified Model. The derived footprints have\nrealistic spatial structure, and gust speeds closer to the measurements than\noriginally simulated.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 08:32:08 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Youngman", "Benjamin D.", ""], ["Stephenson", "David B.", ""]]}, {"id": "1609.07792", "submitter": "Pierre-Antoine Thouvenin", "authors": "Pierre-Antoine Thouvenin, Nicolas Dobigeon, Jean-Yves Tourneret", "title": "A Hierarchical Bayesian Model Accounting for Endmember Variability and\n  Abrupt Spectral Changes to Unmix Multitemporal Hyperspectral Images", "comments": "14 pages, 13 figures, accepted for publication in IEEE Trans. Comput.\n  Imag., 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing is a blind source separation problem which consists in\nestimating the reference spectral signatures contained in a hyperspectral\nimage, as well as their relative contribution to each pixel according to a\ngiven mixture model. In practice, the process is further complexified by the\ninherent spectral variability of the observed scene and the possible presence\nof outliers. More specifically, multi-temporal hyperspectral images, i.e.,\nsequences of hyperspectral images acquired over the same area at different time\ninstants, are likely to simultaneously exhibit moderate endmember variability\nand abrupt spectral changes either due to outliers or to significant time\nintervals between consecutive acquisitions. Unless properly accounted for,\nthese two perturbations can significantly affect the unmixing process. In this\ncontext, we propose a new unmixing model for multitemporal hyperspectral images\naccounting for smooth temporal variations, construed as spectral variability,\nand abrupt spectral changes interpreted as outliers. The proposed hierarchical\nBayesian model is inferred using a Markov chain Monte-Carlo (MCMC) method\nallowing the posterior of interest to be sampled and Bayesian estimators to be\napproximated. A comparison with unmixing techniques from the literature on\nsynthetic and real data allows the interest of the proposed approach to be\nappreciated.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 20:02:25 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 12:59:32 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 15:56:57 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 16:55:31 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Thouvenin", "Pierre-Antoine", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1609.07844", "submitter": "Vladimir Minin", "authors": "Amrit Dhar and Vladimir N. Minin", "title": "Calculating higher-order moments of phylogenetic stochastic mapping\n  summaries in linear time", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic mapping is a simulation-based method for probabilistically mapping\nsubstitution histories onto phylogenies according to continuous-time Markov\nmodels of evolution. This technique can be used to infer properties of the\nevolutionary process on the phylogeny and, unlike parsimony-based mapping,\nconditions on the observed data to randomly draw substitution mappings that do\nnot necessarily require the minimum number of events on a tree. Most stochastic\nmapping applications simulate substitution mappings only to estimate the mean\nand/or variance of two commonly used mapping summaries: the number of\nparticular types of substitutions (labeled substitution counts) and the time\nspent in a particular group of states (labeled dwelling times) on the tree.\nFast, simulation-free algorithms for calculating the mean of stochastic mapping\nsummaries exist. Importantly, these algorithms scale linearly in the number of\ntips/leaves of the phylogenetic tree. However, to our knowledge, no such\nalgorithm exists for calculating higher-order moments of stochastic mapping\nsummaries. We present one such simulation-free dynamic programming algorithm\nthat calculates prior and posterior mapping variances and scales linearly in\nthe number of phylogeny tips. Our procedure suggests a general framework that\ncan be used to efficiently compute higher-order moments of stochastic mapping\nsummaries without simulations. We demonstrate the usefulness of our algorithm\nby extending previously developed statistical tests for rate variation across\nsites and for detecting evolutionarily conserved regions in genomic sequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 04:12:00 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 06:15:14 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dhar", "Amrit", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1609.07887", "submitter": "Karel Hron", "authors": "Ivo Muller, Karel Hron, Eva Fiserova, Jan Smahaj, Panajotis\n  Cakirpaloglu, Jana Vancakova", "title": "Interpretation of Compositional Regression with Application to Time\n  Budget Analysis", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression with compositional response or covariates, or even regression\nbetween parts of a composition, is frequently employed in social sciences.\nAmong other possible applications, it may help to reveal interesting features\nin time allocation analysis. As individual activities represent relative\ncontributions to the total amount of time, statistical processing of raw data\n(frequently represented directly as proportions or percentages) using standard\nmethods may lead to biased results. Specific geometrical features of time\nbudget variables are captured by the logratio methodology of compositional\ndata, whose aim is to build (preferably orthonormal) coordinates to be applied\nwith popular statistical methods. The aim of this paper is to present recent\ntools of regression analysis within the logratio methodology and apply them to\nreveal potential relationships among psychometric indicators in a real-world\ndata set. In particular, orthogonal logratio coordinates have been introduced\nto enhance the interpretability of coefficients in regression models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 08:58:38 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Muller", "Ivo", ""], ["Hron", "Karel", ""], ["Fiserova", "Eva", ""], ["Smahaj", "Jan", ""], ["Cakirpaloglu", "Panajotis", ""], ["Vancakova", "Jana", ""]]}, {"id": "1609.07958", "submitter": "Nir Halay", "authors": "Nir Halay, Koby Todros and Alfred O. Hero", "title": "Binary Hypothesis Testing via Measure Transformed Quasi Likelihood Ratio\n  Test", "comments": "Important notice - The paper: N. Halay and K. Todros, \"Plug-in\n  measure-transformed quasi likelihood ratio test for random signal detection,\"\n  IEEE Signal Processing Letters, vol. 24, no. 6, pp. 838-842, Jun. 2017,\n  refers to the first arxiv version of this article\n  https://arxiv.org/pdf/1609.07958v1.pdf", "journal-ref": null, "doi": "10.1109/TSP.2017.2752692", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Gaussian quasi likelihood ratio test (GQLRT) for\nnon-Bayesian binary hypothesis testing is generalized by applying a transform\nto the probability distribution of the data. The proposed generalization,\ncalled measure-transformed GQLRT (MT-GQLRT), selects a Gaussian probability\nmodel that best empirically fits a transformed probability measure of the data.\nBy judicious choice of the transform we show that, unlike the GQLRT, the\nproposed test is resilient to outliers and involves higher-order statistical\nmoments leading to significant mitigation of the model mismatch effect on the\ndecision performance. Under some mild regularity conditions we show that the\nMT-GQLRT is consistent and its corresponding test statistic is asymptotically\nnormal. A data driven procedure for optimal selection of the measure\ntransformation parameters is developed that maximizes an empirical estimate of\nthe asymptotic power given a fixed empirical asymptotic size. A Bayesian\nextension of the proposed MT-GQLRT is also developed that is based on selection\nof a Gaussian probability model that best empirically fits a transformed\nconditional probability distribution of the data. In the Bayesian MT-GQLRT the\nthreshold and the measure transformation parameters are selected via joint\nminimization of the empirical asymptotic Bayes risk. The non-Bayesian and\nBayesian MT-GQLRTs are applied to signal detection and classification, in\nsimulation examples that illustrate their advantages over the standard GQLRT\nand other robust alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 13:09:47 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 15:22:28 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Halay", "Nir", ""], ["Todros", "Koby", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1609.08043", "submitter": "Christian Weiss", "authors": "Christian Weiss and Abdelhak M. Zoubir", "title": "A Compressed Sampling and Dictionary Learning Framework for WDM-Based\n  Distributed Fiber Sensing", "comments": "Accepted for publication in Journal of the Optical Society of America\n  A [ \\copyright\\ 2017 Optical Society of America.]. One print or electronic\n  copy may be made for personal use only. Systematic reproduction and\n  distribution, duplication of any material in this paper for a fee or for\n  commercial purposes, or modifications of the content of this paper are\n  prohibited", "journal-ref": "Journal of the Optical Society of America A (JOSA A), Vol. 34,\n  Issue 5, pp. 783-797 (2017)", "doi": "10.1364/JOSAA.34.000783", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a compressed sampling and dictionary learning framework for\nfiber-optic sensing using wavelength-tunable lasers. A redundant dictionary is\ngenerated from a model for the reflected sensor signal. Imperfect prior\nknowledge is considered in terms of uncertain local and global parameters. To\nestimate a sparse representation and the dictionary parameters, we present an\nalternating minimization algorithm that is equipped with a pre-processing\nroutine to handle dictionary coherence. The support of the obtained sparse\nsignal indicates the reflection delays, which can be used to measure\nimpairments along the sensing fiber. The performance is evaluated by\nsimulations and experimental data for a fiber sensor system with common core\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 16:12:31 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 00:45:13 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Weiss", "Christian", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1609.08227", "submitter": "Larry Wasserman", "authors": "Larry Wasserman", "title": "Topological Data Analysis", "comments": "Submitted to Annual Reviews in Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) can broadly be described as a collection of\ndata analysis methods that find structure in data. This includes: clustering,\nmanifold estimation, nonlinear dimension reduction, mode estimation, ridge\nestimation and persistent homology. This paper reviews some of these methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 00:19:27 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Wasserman", "Larry", ""]]}, {"id": "1609.08347", "submitter": "Juha Karvanen", "authors": "Juha Karvanen, Jarno Vanhatalo, Kari Auranen, Sangita Kulathinal and\n  Samu M\\\"antyniemi", "title": "Optimal design of observational studies: overview and synthesis", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review typical design problems encountered in the planning of\nobservational studies and propose a unifying framework that allows us to use\nthe same concepts and notation for different problems. In the framework, the\ndesign is defined as a probability measure in the space of observational\nprocesses that determine whether the value of a variable is observed for a\nspecific unit at the given time. The optimal design is then defined, according\nto Bayesian decision theory, to be the one that maximizes the expected utility\nrelated to the design. We present examples on the use of the framework and\ndiscuss methods for deriving optimal or approximately optimal designs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 10:46:32 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 05:34:55 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 13:05:59 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Karvanen", "Juha", ""], ["Vanhatalo", "Jarno", ""], ["Auranen", "Kari", ""], ["Kulathinal", "Sangita", ""], ["M\u00e4ntyniemi", "Samu", ""]]}, {"id": "1609.08395", "submitter": "Juan Pablo Carbajal", "authors": "Juan Pablo Carbajal, Jo\\~ao Paulo Leit\\~ao, Carlo Albert, J\\\"org\n  Rieckermann", "title": "Appraisal of data-driven and mechanistic emulators of nonlinear\n  hydrodynamic urban drainage simulators", "comments": "This article was published in Environmental Modelling and Software, 7\n  figures, 4 tables", "journal-ref": null, "doi": "10.1016/j.envsoft.2017.02.006", "report-no": null, "categories": "stat.ME cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many model based scientific and engineering methodologies, such as system\nidentification, sensitivity analysis, optimization and control, require a large\nnumber of model evaluations. In particular, model based real-time control of\nurban water infrastructures and online flood alarm systems require fast\nprediction of the network response at different actuation and/or parameter\nvalues. General purpose urban drainage simulators are too slow for this\napplication. Fast surrogate models, so-called emulators, provide a solution to\nthis efficiency demand. Emulators are attractive, because they sacrifice\nunneeded accuracy in favor of speed. However, they have to be fine-tuned to\npredict the system behavior satisfactorily. Also, some emulators fail to\nextrapolate the system behavior beyond the training set. Although, there are\nmany strategies for developing emulators, up until now the selection of the\nemulation strategy remains subjective. In this paper, we therefore compare the\nperformance of two families of emulators for open channel flows in the context\nof urban drainage simulators. We compare emulators that explicitly use\nknowledge of the simulator's equations, i.e. mechanistic emulators based on\nGaussian Processes, with purely data-driven emulators using matrix\nfactorization. Our results suggest that in many urban applications, naive\ndata-driven emulation outperforms mechanistic emulation. Nevertheless, we\ndiscuss scenarios in which we think that mechanistic emulation might be\nfavorable for i) extrapolation in time and ii) dealing with sparse and unevenly\nsampled data. We also provide many references to advances in the field of\nMachine Learning that have not yet permeated into the Bayesian environmental\nscience community.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 21:57:34 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 12:30:52 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Carbajal", "Juan Pablo", ""], ["Leit\u00e3o", "Jo\u00e3o Paulo", ""], ["Albert", "Carlo", ""], ["Rieckermann", "J\u00f6rg", ""]]}, {"id": "1609.08725", "submitter": "Haoran Li", "authors": "Haoran Li, Alexander Aue, Debashis Paul, Jie Peng, Pei Wang", "title": "An adaptable generalization of Hotelling's $T^2$ test in high dimension", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-sample test for detecting the difference between mean\nvectors in a high-dimensional regime based on a ridge-regularized Hotelling's\n$T^2$. To choose the regularization parameter, a method is derived that aims at\nmaximizing power within a class of local alternatives. We also propose a\ncomposite test that combines the optimal tests corresponding to a specific\ncollection of local alternatives. Weak convergence of the stochastic process\ncorresponding to the ridge-regularized Hotelling's $T^2$ is established and\nused to derive the cut-off values of the proposed test. Large sample properties\nare verified for a class of sub-Gaussian distributions. Through an extensive\nsimulation study, the composite test is shown to compare favorably against a\nhost of existing two-sample test procedure in a wide range of settings. The\nperformance of the proposed test procedure is illustrated through an\napplication to a breast cancer data set where the goal is to detect the\npathways with different DNA copy number alterations across breast cancer\nsubtypes.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 01:55:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 23:04:06 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 06:18:44 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Li", "Haoran", ""], ["Aue", "Alexander", ""], ["Paul", "Debashis", ""], ["Peng", "Jie", ""], ["Wang", "Pei", ""]]}, {"id": "1609.08737", "submitter": "Yuan Ji", "authors": "Wentian Guo, Sue-Jane Wang, Shengjie Yang, Suiheng Lin, Yuan Ji", "title": "A Bayesian Interval Dose-Finding Design Addressing Ockham's Razor:\n  mTPI-2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There has been an increasing interest in using interval-based Bayesian\ndesigns for dose finding, one of which is the modified toxicity probability\ninterval (mTPI) method. We show that the decision rules in mTPI correspond to\nan optimal rule under a formal Bayesian decision theoretic framework. However,\nthe probability models in mTPI are overly sharpened by the Ockham's razor,\nwhich, while in general helps with parsimonious statistical inference, leads to\nsuboptimal decisions in small-sample inference such as dose finding. We propose\na new framework that blunts the Ockham's razor, and demonstrate the superior\nperformance of the new method, called mTPI-2. An online web tool is provided\nfor users who can generate the design, conduct clinical trials, and examine\noperating characteristics of the designs through big data and crowd sourcing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 02:28:17 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Guo", "Wentian", ""], ["Wang", "Sue-Jane", ""], ["Yang", "Shengjie", ""], ["Lin", "Suiheng", ""], ["Ji", "Yuan", ""]]}, {"id": "1609.08816", "submitter": "Wang Miao", "authors": "Wang Miao, Zhi Geng, and Eric Tchetgen Tchetgen", "title": "Identifying Causal Effects With Proxy Variables of an Unmeasured\n  Confounder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a causal effect that is confounded by an unobserved variable, but\nwith observed proxy variables of the confounder. We show that, with at least\ntwo independent proxy variables satisfying a certain rank condition, the causal\neffect is nonparametrically identified, even if the measurement error\nmechanism, i.e., the conditional distribution of the proxies given the con-\nfounder, may not be identified. Our result generalizes the identification\nstrategy of Kuroki & Pearl (2014) that rests on identification of the\nmeasurement error mechanism. When only one proxy for the confounder is\navailable, or the required rank condition is not met, we develop a strategy to\ntest the null hypothesis of no causal effect.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 08:40:25 GMT"}, {"version": "v2", "created": "Mon, 26 Dec 2016 08:27:44 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 17:51:57 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 17:33:30 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Miao", "Wang", ""], ["Geng", "Zhi", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1609.08886", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano, Hironori Fujisawa, Toyoyuki Takada, Toshihiko\n  Shiroishi", "title": "Sparse principal component regression for generalized linear models", "comments": "29 pages", "journal-ref": "Computational Statistics & Data Analysis 124 (2018) 180-196", "doi": "10.1016/j.csda.2018.03.008", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component regression (PCR) is a widely used two-stage procedure:\nprincipal component analysis (PCA), followed by regression in which the\nselected principal components are regarded as new explanatory variables in the\nmodel. Note that PCA is based only on the explanatory variables, so the\nprincipal components are not selected using the information on the response\nvariable. In this paper, we propose a one-stage procedure for PCR in the\nframework of generalized linear models. The basic loss function is based on a\ncombination of the regression loss and PCA loss. An estimate of the regression\nparameter is obtained as the minimizer of the basic loss function with a sparse\npenalty. We call the proposed method sparse principal component regression for\ngeneralized linear models (SPCR-glm). Taking the two loss function into\nconsideration simultaneously, SPCR-glm enables us to obtain sparse principal\ncomponent loadings that are related to a response variable. However, a\ncombination of loss functions may cause a parameter identification problem, but\nthis potential problem is avoided by virtue of the sparse penalty. Thus, the\nsparse penalty plays two roles in this method. The parameter estimation\nprocedure is proposed using various update algorithms with the coordinate\ndescent algorithm. We apply SPCR-glm to two real datasets, doctor visits data\nand mouse consomic strain data. SPCR-glm provides more easily interpretable\nprincipal component (PC) scores and clearer classification on PC plots than the\nusual PCA.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 12:33:42 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 05:59:25 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 03:51:02 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kawano", "Shuichi", ""], ["Fujisawa", "Hironori", ""], ["Takada", "Toyoyuki", ""], ["Shiroishi", "Toshihiko", ""]]}, {"id": "1609.08905", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Alessio Benavoli and Janez Dem\\v{s}ar and Francesca\n  Mangili and Marco Zaffalon", "title": "Statistical comparison of classifiers through Bayesian hierarchical\n  modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually one compares the accuracy of two competing classifiers via null\nhypothesis significance tests (nhst). Yet the nhst tests suffer from important\nshortcomings, which can be overcome by switching to Bayesian hypothesis\ntesting. We propose a Bayesian hierarchical model which jointly analyzes the\ncross-validation results obtained by two classifiers on multiple data sets. It\nreturns the posterior probability of the accuracies of the two classifiers\nbeing practically equivalent or significantly different. A further strength of\nthe hierarchical model is that, by jointly analyzing the results obtained on\nall data sets, it reduces the estimation error compared to the usual approach\nof averaging the cross-validation results obtained on a given data set.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 13:30:31 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 08:23:38 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 15:16:45 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Corani", "Giorgio", ""], ["Benavoli", "Alessio", ""], ["Dem\u0161ar", "Janez", ""], ["Mangili", "Francesca", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1609.08956", "submitter": "Lynn Roy LaMotte", "authors": "Lynn R. LaMotte", "title": "Yates's and Other Sums of Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that the sum of squares by Yates's method of weighted squares of\nmeans is equivalent to numerator sums of squares formulated by other methods.\nThese relations are established first for hypotheses about fixed effects in a\ngeneral linear model, in the process showing how Yates's method can be\nextended. They are then illustrated in the unequal-subclass-numbers model for\nmain effects and interaction effects of two factors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 15:14:00 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["LaMotte", "Lynn R.", ""]]}, {"id": "1609.08970", "submitter": "Moritz  Berger", "authors": "Stella Bollmann, Moritz Berger, Gerhard Tutz", "title": "Item-Focussed Trees for the Detection of Differential Item Functioning\n  in Partial Credit Models", "comments": "28 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods to detect differential item functioning (DIF) in item\nresponse models are available. However, most of the methods assume that the\nresponses are binary, for ordered response categories available methods are\nscarce. In the present paper DIF in the widely used partial credit model is\ninvestigated. An item-focussed tree is proposed that allows to detect\nDIF-items, which might affect the performance of the partial credit model. The\nmethod uses tree methodology yielding a tree for each item that is detected as\nDIF-item. The resulting trees show which variables induce DIF and in which way.\nThe visualization as trees makes the results easily accessible. The method is\ncompared to an alternative approach, simulations demonstrate the performance of\nthe method and an application illustrates how it works for real data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 15:40:50 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Bollmann", "Stella", ""], ["Berger", "Moritz", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1609.09033", "submitter": "David Kaplan", "authors": "David M. Kaplan and Yixiao Sun", "title": "Smoothed estimating equations for instrumental variables quantile\n  regression", "comments": "Authors' accepted manuscript; forthcoming in Econometric Theory,\n  copyright Cambridge University Press, published version at\n  http://dx.doi.org/10.1017/S0266466615000407", "journal-ref": "Econometric Theory 33 (2017) 105-157", "doi": "10.1017/S0266466615000407", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The moment conditions or estimating equations for instrumental variables\nquantile regression involve the discontinuous indicator function. We instead\nuse smoothed estimating equations (SEE), with bandwidth $h$. We show that the\nmean squared error (MSE) of the vector of the SEE is minimized for some $h>0$,\nleading to smaller asymptotic MSE of the estimating equations and associated\nparameter estimators. The same MSE-optimal $h$ also minimizes the higher-order\ntype I error of a SEE-based $\\chi^2$ test and increases size-adjusted power in\nlarge samples. Computation of the SEE estimator also becomes simpler and more\nreliable, especially with (more) endogenous regressors. Monte Carlo simulations\ndemonstrate all of these superior properties in finite samples, and we apply\nour estimator to JTPA data. Smoothing the estimating equations is not just a\ntechnical operation for establishing Edgeworth expansions and bootstrap\nrefinements; it also brings the real benefits of having more precise estimators\nand more powerful tests. Code for the estimator, simulations, and empirical\nexamples is available from the first author's website.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:33:54 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Kaplan", "David M.", ""], ["Sun", "Yixiao", ""]]}, {"id": "1609.09035", "submitter": "David Kaplan", "authors": "Matt Goldman and David M. Kaplan", "title": "Fractional order statistic approximation for nonparametric conditional\n  quantile inference", "comments": "Authors' accepted manuscript (Journal of Econometrics); DOI TBD", "journal-ref": "Journal of Econometrics 196 (2017) 331-346", "doi": "10.1016/j.jeconom.2016.09.015", "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using and extending fractional order statistic theory, we characterize the\n$O(n^{-1})$ coverage probability error of the previously proposed confidence\nintervals for population quantiles using $L$-statistics as endpoints in Hutson\n(1999). We derive an analytic expression for the $n^{-1}$ term, which may be\nused to calibrate the nominal coverage level to get\n$O\\bigl(n^{-3/2}[\\log(n)]^3\\bigr)$ coverage error. Asymptotic power is shown to\nbe optimal. Using kernel smoothing, we propose a related method for\nnonparametric inference on conditional quantiles. This new method compares\nfavorably with asymptotic normality and bootstrap methods in theory and in\nsimulations. Code is available from the second author's website for both\nunconditional and conditional methods, simulations, and empirical examples.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 18:34:27 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Goldman", "Matt", ""], ["Kaplan", "David M.", ""]]}, {"id": "1609.09272", "submitter": "Giorgio Picci", "authors": "Giorgio Picci and Bin Zhu", "title": "A New Algorithm for Circulant Rational Covariance Extension and\n  Applications to Finite-interval Smoothing", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial stochastic realization of periodic processes from finite\ncovariance data has recently been solved by Lindquist and Picci based on convex\noptimization of a generalized entropy functional. The meaning and the role of\nthis criterion have an unclear origin. In this paper we propose a solution\nbased on a nonlinear generalization of the classical Yule-Walker type equations\nand on a new iterative algorithm which is shown to converge to the same\n(unique) solution of the variational problem. This provides a conceptual link\nto the variational principles and at the same time yields a robust algorithm\nwhich can for example be successfully applied to finite-interval smoothing\nproblems providing a simpler procedure if compared with the classical\nRiccati-based calculations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 09:39:42 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Picci", "Giorgio", ""], ["Zhu", "Bin", ""]]}, {"id": "1609.09278", "submitter": "Dreamlee Sharma", "authors": "Dreamlee Sharma and Tapan Kumar Chakrabarty", "title": "On Size Biased Kumaraswamy Distribution", "comments": null, "journal-ref": "Statistics, Optimization, and Information Computing, Vol 4, Sep\n  2016, pp 252-264", "doi": "10.19139/soic.v4i3.217", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce and study the size-biased form of Kumaraswamy\ndistribution. The Kumaraswamy distribution which has drawn considerable\nattention in hydrology and related areas was proposed by Kumarswamy. The new\ndistribution is derived under size-biased probability of sampling taking the\nweights as the variate values. Various distributional and characterizing\nproperties of the model are studied. The methods of maximum likelihood and\nmatching quantiles estimation are employed to estimate the parameters of the\nproposed model. Finally, we apply the proposed model to simulated and real data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 09:51:55 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Sharma", "Dreamlee", ""], ["Chakrabarty", "Tapan Kumar", ""]]}, {"id": "1609.09380", "submitter": "Xianyang Zhang", "authors": "Shun Yao, Xianyang Zhang and Xiaofeng Shao", "title": "Testing mutual independence in high dimension via distance covariance", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a ${\\mathcal L}_2$ type test for testing mutual\nindependence and banded dependence structure for high dimensional data. The\ntest is constructed based on the pairwise distance covariance and it accounts\nfor the non-linear and non-monotone dependences among the data, which cannot be\nfully captured by the existing tests based on either Pearson correlation or\nrank correlation. Our test can be conveniently implemented in practice as the\nlimiting null distribution of the test statistic is shown to be standard\nnormal. It exhibits excellent finite sample performance in our simulation\nstudies even when the sample size is small albeit dimension is high, and is\nshown to successfully identify nonlinear dependence in empirical data analysis.\nOn the theory side, asymptotic normality of our test statistic is shown under\nquite mild moment assumptions and with little restriction on the growth rate of\nthe dimension as a function of sample size. As a demonstration of good power\nproperties for our distance covariance based test, we further show that an\ninfeasible version of our test statistic has the rate optimality in the class\nof Gaussian distribution with equal correlation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 15:11:43 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 18:39:11 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Yao", "Shun", ""], ["Zhang", "Xianyang", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1609.09529", "submitter": "Simon Stolarczyk", "authors": "Simon Stolarczyk, Manisha Bhardwaj, Kevin E. Bassler, Wei Ji Ma,\n  Kresimir Josic", "title": "Loss of information in feedforward social networks", "comments": null, "journal-ref": "Loss of information in feedforward social networks. Journal of\n  Complex Networks, cnx032 (2017)", "doi": "10.1093/comnet/cnx032", "report-no": null, "categories": "stat.ME cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model social networks in which information propagates\ndirectionally across layers of rational agents. Each agent makes a locally\noptimal estimate of the state of the world, and communicates this estimate to\nagents downstream. When agents receive information from the same source their\nestimates are correlated. We show that the resulting redundancy can lead to the\nloss of information about the state of the world across layers of the network,\neven when all agents have full knowledge of the network's structure. A simple\nalgebraic condition identifies networks in which information loss occurs, and\nwe show that all such networks must contain a particular network motif. We also\nstudy random networks asymptotically as the number of agents increases, and\nfind a sharp transition in the probability of information loss at the point at\nwhich the number of agents in one layer exceeds the number in the previous\nlayer.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 22:57:43 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Stolarczyk", "Simon", ""], ["Bhardwaj", "Manisha", ""], ["Bassler", "Kevin E.", ""], ["Ma", "Wei Ji", ""], ["Josic", "Kresimir", ""]]}, {"id": "1609.09803", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Beyond p values: practical methods for analyzing uncertainty in research", "comments": "18 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article explains, and discusses the merits of, three approaches for\nanalyzing the certainty with which statistical results can be extrapolated\nbeyond the data gathered. Sometimes it may be possible to use more than one of\nthese approaches. (1) If there is an exact null hypothesis which is credible\nand interesting (usually not the case), researchers should cite a p value\n(significance level), although jargon is best avoided. (2) If the research\nresult is a numerical value, researchers should cite a confidence interval. (3)\nIf there are one or more hypotheses of interest, it may be possible to adapt\nthe methods used for confidence intervals to derive an \"estimated probability\"\nfor each. Under certain circumstances these could be interpreted as Bayesian\nposterior probabilities. These estimated probabilities can easily be worked out\nfrom the p values and confidence intervals produced by packages such as SPSS.\nEstimating probabilities for hypotheses means researchers can give a direct\nanswer to the question \"How certain can we be that this hypothesis is right?\".\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 16:37:28 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "1609.09816", "submitter": "Xiao Liu", "authors": "Xiao Liu, Viknesswaran Gopal, Jayant Kalagnanam", "title": "A Spatio-Temporal Modeling Approach for Weather Radar Reflectivity Data\n  and Its Applications in Tropical Southeast Asia", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather radar echoes, correlated in both space and time, are the most\nimportant input data for short-term precipitation forecast. Motivated by real\ndatasets, this paper is concerned with the spatio-temporal modeling of\ntwo-dimensional radar reflectivity fields from a sequence of radar images.\nUnder a Lagrangian integration scheme, we model the radar reflectivity data by\na spatio-temporal conditional autoregressive process which is driven by two\nhidden sub-processes. The first sub-process is the dynamic velocity field which\ndetermines the motion of the weather system, while the second sub-process\ngoverns the growth or decay of the strength of radar reflectivity. The proposed\nmethod is demonstrated, and compared with existing methods, using the real\nradar data collected from the tropical southeast Asia. Note that, since the\ntropical storms are known to be highly chaotic and extremely difficult to be\npredicted, we only focus on the modeling of reflectivity data within a\nshort-period of time and consider the short-term prediction problem based on\nthe proposed model. This is often referred to as the nowcasting issue in the\nmeteorology society.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 17:08:32 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Liu", "Xiao", ""], ["Gopal", "Viknesswaran", ""], ["Kalagnanam", "Jayant", ""]]}]