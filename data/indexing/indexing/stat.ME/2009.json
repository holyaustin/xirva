[{"id": "2009.00131", "submitter": "Prasanth Shyamsundar", "authors": "Konstantin T. Matchev, Prasanth Shyamsundar", "title": "InClass Nets: Independent Classifier Networks for Nonparametric\n  Estimation of Conditional Independence Mixture Models and Unsupervised\n  Classification", "comments": "46 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM hep-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new machine-learning-based approach, which we call the\nIndependent Classifier networks (InClass nets) technique, for the\nnonparameteric estimation of conditional independence mixture models (CIMMs).\nWe approach the estimation of a CIMM as a multi-class classification problem,\nsince dividing the dataset into different categories naturally leads to the\nestimation of the mixture model. InClass nets consist of multiple independent\nclassifier neural networks (NNs), each of which handles one of the variates of\nthe CIMM. Fitting the CIMM to the data is performed by simultaneously training\nthe individual NNs using suitable cost functions. The ability of NNs to\napproximate arbitrary functions makes our technique nonparametric. Further\nleveraging the power of NNs, we allow the conditionally independent variates of\nthe model to be individually high-dimensional, which is the main advantage of\nour technique over existing non-machine-learning-based approaches. We derive\nsome new results on the nonparametric identifiability of bivariate CIMMs, in\nthe form of a necessary and a (different) sufficient condition for a bivariate\nCIMM to be identifiable. We provide a public implementation of InClass nets as\na Python package called RainDancesVI and validate our InClass nets technique\nwith several worked out examples. Our method also has applications in\nunsupervised and semi-supervised classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 22:24:09 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Matchev", "Konstantin T.", ""], ["Shyamsundar", "Prasanth", ""]]}, {"id": "2009.00148", "submitter": "Jinglong Zhao", "authors": "Iavor Bojinov, David Simchi-Levi, Jinglong Zhao", "title": "Design and Analysis of Switchback Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In switchback experiments, a firm sequentially exposes an experimental unit\nto a random treatment, measures its response, and repeats the procedure for\nseveral periods to determine which treatment leads to the best outcome.\nAlthough practitioners have widely adopted this experimental design technique,\nthe development of its theoretical properties and the derivation of optimal\ndesign procedures have been, to the best of our knowledge, elusive. In this\npaper, we address these limitations by establishing the necessary results to\nensure that practitioners can apply this powerful class of experiments with\nminimal assumptions. Our main result is the derivation of the optimal design of\nswitchback experiments under a range of different assumptions on the order of\ncarryover effect - that is, the length of time a treatment persists in\nimpacting the outcome. We cast the experimental design problem as a minimax\ndiscrete robust optimization problem, identify the worst-case adversarial\nstrategy, establish structural results for the optimal design, and finally\nsolve the problem via a continuous relaxation. For the optimal design, we\nderive two approaches for performing inference after running the experiment.\nThe first provides exact randomization based $p$-values and the second uses a\nfinite population central limit theorem to conduct conservative hypothesis\ntests and build confidence intervals. We further provide theoretical results\nfor our inferential procedures when the order of the carryover effect is\nmisspecified. For firms that possess the capability to run multiple switchback\nexperiments, we also provide a data-driven strategy to identify the likely\norder of carryover effect. To study the empirical properties of our results, we\nconduct extensive simulations. We conclude the paper by providing some\npractical suggestions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:40:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:00:49 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Bojinov", "Iavor", ""], ["Simchi-Levi", "David", ""], ["Zhao", "Jinglong", ""]]}, {"id": "2009.00208", "submitter": "Martin Wortman", "authors": "Martin Wortman, Ernest Kee and Paul Nelson", "title": "Characterizing the Probability Law on Time Until Core Damage With PRA", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain modeling assumptions underlying Probabilistic Risk Assessment (PRA)\nallow a simple computation of core damage frequency (CDF). These assumptions\nalso guarantee that the time remaining until a core damage event follows an\nexponential distribution having parameter value equal to that computed for the\nCDF. While it is commonly understood that these modeling assumptions lead to an\napproximate characterization of uncertainty, we offer a simple argument that\nexplains why the resulting exponential time until core damage distribution\nunder-estimates risk. Our explanation will first review operational physics\nproperties of hazard functions, and then offer a non-measure-theoretic argument\nto reveal the the consequences of these properties for PRA. The conclusions\noffered, here, hold for any possible operating history that respects the\nunderlying assumptions of PRA. Hence, the measure-theoretic constructs on\nfiltered probability spaces is unnecessary for our developments. We will then\nconclude with a brief discussion that connects intuition with our analytical\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:36:05 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Wortman", "Martin", ""], ["Kee", "Ernest", ""], ["Nelson", "Paul", ""]]}, {"id": "2009.00217", "submitter": "Jared Murray", "authors": "Jared S. Murray", "title": "Invited Discussion of \"A Unified Framework for De-Duplication and\n  Population Size Estimation\"", "comments": "Full paper and discussion available at\n  https://projecteuclid.org/download/pdfview_1/euclid.ba/1551949260", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invited Discussion of \"A Unified Framework for De-Duplication and Population\nSize Estimation\", published in Bayesian Analysis. My discussion focuses on two\nmain themes: Providing a more nuanced picture of the costs and benefits of\njoint models for record linkage and the \"downstream task\" (i.e. whatever we\nmight want to do with the linked and de-duplicated files), and how we should\nmeasure performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:19:29 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Murray", "Jared S.", ""]]}, {"id": "2009.00399", "submitter": "Qing Cheng", "authors": "Qing Cheng, Baoluo Sun, Yingcun Xia, Jin Liu", "title": "Accounting for correlated horizontal pleiotropy in two-sample Mendelian\n  randomization using correlated instrumental variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) is a powerful approach to examine the causal\nrelationships between health risk factors and outcomes from observational\nstudies. Due to the proliferation of genome-wide association studies (GWASs)\nand abundant fully accessible GWASs summary statistics, a variety of two-sample\nMR methods for summary data have been developed to either detect or account for\nhorizontal pleiotropy, primarily based on the assumption that the effects of\nvariants on exposure ({\\gamma}) and horizontal pleiotropy ({\\alpha}) are\nindependent. This assumption is too strict and can be easily violated because\nof the correlated horizontal pleiotropy (CHP). To account for this CHP, we\npropose a Bayesian approach, MR-Corr2, that uses the orthogonal projection to\nreparameterize the bivariate normal distribution for {\\gamma} and {\\alpha}, and\na spike-slab prior to mitigate the impact of CHP. We develop an efficient\nalgorithm with paralleled Gibbs sampling. To demonstrate the advantages of\nMR-Corr2 over existing methods, we conducted comprehensive simulation studies\nto compare for both type-I error control and point estimates in various\nscenarios. By applying MR-Corr2 to study the relationships between pairs in two\nsets of complex traits, we did not identify the contradictory causal\nrelationship between HDL-c and CAD. Moreover, the results provide a new\nperspective of the causal network among complex traits. The developed R package\nand code to reproduce all the results are available at\nhttps://github.com/QingCheng0218/MR.Corr2.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:03:51 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Cheng", "Qing", ""], ["Sun", "Baoluo", ""], ["Xia", "Yingcun", ""], ["Liu", "Jin", ""]]}, {"id": "2009.00471", "submitter": "Yuling Yao", "authors": "Yuling Yao, Collin Cademartori, Aki Vehtari, Andrew Gelman", "title": "Adaptive Path Sampling in Metastable Posterior Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalizing constant plays an important role in Bayesian computation, and\nthere is a large literature on methods for computing or approximating\nnormalizing constants that cannot be evaluated in closed form. When the\nnormalizing constant varies by orders of magnitude, methods based on importance\nsampling can require many rounds of tuning. We present an improved approach\nusing adaptive path sampling, iteratively reducing gaps between the base and\ntarget. Using this adaptive strategy, we develop two metastable sampling\nschemes. They are automated in Stan and require little tuning. For a multimodal\nposterior density, we equip simulated tempering with a continuous temperature.\nFor a funnel-shaped entropic barrier, we adaptively increase mass in bottleneck\nregions to form an implicit divide-and-conquer. Both approaches empirically\nperform better than existing methods for sampling from metastable\ndistributions, including higher accuracy and computation efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:28:31 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Yao", "Yuling", ""], ["Cademartori", "Collin", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2009.00503", "submitter": "Sara Algeri", "authors": "Sara Algeri", "title": "Informative Goodness-of-Fit for Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces an informative goodness-of-fit (iGOF) approach to\nstudy multivariate distributions. When the null model is rejected, iGOF allows\nus to identify the underlying sources of mismodeling and naturally equips\npractitioners with additional insights on the nature of the deviations from the\ntrue distribution. The informative character of the procedure is achieved by\nexploiting smooth tests and random fields theory to facilitate the analysis of\nmultivariate data. Simulation studies show that iGOF enjoys high power for\ndifferent types of alternatives. The methods presented here directly address\nthe problem of background mismodeling arising in physics and astronomy. It is\nin these areas that the motivation of this work is rooted.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 15:04:30 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 15:13:17 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 16:19:56 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 21:38:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Algeri", "Sara", ""]]}, {"id": "2009.00606", "submitter": "Oren Yuval", "authors": "Oren Yuval and Saharon Rosset", "title": "Semi-Supervised Empirical Risk Minimization: When can unlabeled data\n  improve prediction?", "comments": "36 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general methodology for using unlabeled data to design semi\nsupervised learning (SSL) variants of the Empirical Risk Minimization (ERM)\nlearning process. Focusing on generalized linear regression, we provide a\ncareful treatment of the effectiveness of the SSL to improve prediction\nperformance. The key ideas are carefully considering the null model as a\ncompetitor, and utilizing the unlabeled data to determine signal-noise\ncombinations where the SSL outperforms both the ERM learning and the null\nmodel. In the special case of linear regression with Gaussian covariates, we\nshow that the previously suggested semi-supervised estimator is in fact not\ncapable of improving on both the supervised estimator and the null model\nsimultaneously. However, the new estimator presented in this work, can achieve\nan improvement of $O(1/n)$ term over both competitors simultaneously. On the\nother hand, we show that in other scenarios, such as non-Gaussian covariates,\nmisspecified linear regression, or generalized linear regression with\nnon-linear link functions, having unlabeled data can derive substantial\nimprovement in practice by applying our suggested SSL approach. Moreover, it is\npossible to identify the situations where SSL improves prediction, by using the\nresults we establish throughout this work. This is shown empirically through\nextensive simulations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:55:51 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 11:49:52 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 14:58:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yuval", "Oren", ""], ["Rosset", "Saharon", ""]]}, {"id": "2009.00666", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka, Alejandro Catalina, Michael Riis Andersen, M{\\aa}ns\n  Magnusson, Jonathan H. Huggins, Aki Vehtari", "title": "Robust, Accurate Stochastic Optimization for Variational Inference", "comments": null, "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting variational posterior approximations using\nstochastic optimization methods. The performance of these approximations\ndepends on (1) how well the variational family matches the true posterior\ndistribution,(2) the choice of divergence, and (3) the optimization of the\nvariational objective. We show that even in the best-case scenario when the\nexact posterior belongs to the assumed variational family, common stochastic\noptimization methods lead to poor variational approximations if the problem\ndimension is moderately large. We also demonstrate that these methods are not\nrobust across diverse model types. Motivated by these findings, we develop a\nmore robust and accurate stochastic optimization framework by viewing the\nunderlying optimization algorithm as producing a Markov chain. Our approach is\ntheoretically motivated and includes a diagnostic for convergence and a novel\nstopping rule, both of which are robust to noisy evaluations of the objective\nfunction. We show empirically that the proposed framework works well on a\ndiverse set of models: it can automatically detect stochastic optimization\nfailure or inaccurate variational approximation\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 19:12:11 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 15:45:09 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Catalina", "Alejandro", ""], ["Andersen", "Michael Riis", ""], ["Magnusson", "M\u00e5ns", ""], ["Huggins", "Jonathan H.", ""], ["Vehtari", "Aki", ""]]}, {"id": "2009.00717", "submitter": "Zhi Wang", "authors": "Zhi Wang, Xueying Tang, Jingchen Liu and Zhiliang Ying", "title": "Subtask Analysis of Process Data Through a Predictive Model", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response process data collected from human-computer interactive items contain\nrich information about respondents' behavioral patterns and cognitive\nprocesses. Their irregular formats as well as their large sizes make standard\nstatistical tools difficult to apply. This paper develops a computationally\nefficient method for exploratory analysis of such process data. The new\napproach segments a lengthy individual process into a sequence of short\nsubprocesses to achieve complexity reduction, easy clustering and meaningful\ninterpretation. Each subprocess is considered a subtask. The segmentation is\nbased on sequential action predictability using a parsimonious predictive model\ncombined with the Shannon entropy. Simulation studies are conducted to assess\nperformance of the new methods. We use the process data from PIAAC 2012 to\ndemonstrate how exploratory analysis of process data can be done with the new\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:11:01 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wang", "Zhi", ""], ["Tang", "Xueying", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "2009.00729", "submitter": "Sina Khatami", "authors": "Sina Khatami, Timothy John Peterson, Murray C Peel, Andrew Western", "title": "Evaluating Catchment Models as Multiple Working Hypotheses: on the Role\n  of Error Metrics, Parameter Sampling, Model Structure, and Data Information\n  Content", "comments": null, "journal-ref": null, "doi": "10.1002/essoar.10504066.1", "report-no": null, "categories": "stat.ME physics.data-an physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To evaluate models as hypotheses, we developed the method of Flux Mapping to\nconstruct a hypothesis space based on dominant runoff generating mechanisms.\nAcceptable model runs, defined as total simulated flow with similar (and\nminimal) model error, are mapped to the hypothesis space given their simulated\nrunoff components. In each modeling case, the hypothesis space is the result of\nan interplay of factors: model structure and parameterization, chosen error\nmetric, and data information content. The aim of this study is to disentangle\nthe role of each factor in model evaluation. We used two model structures\n(SACRAMENTO and SIMHYD), two parameter sampling approaches (Latin Hypercube\nSampling of the parameter space and guided-search of the solution space), three\nwidely used error metrics (Nash-Sutcliffe Efficiency - NSE, Kling-Gupta\nEfficiency skill score - KGEss, and Willmott refined Index of Agreement - WIA),\nand hydrological data from a large sample of Australian catchments. First, we\ncharacterized how the three error metrics behave under different error types\nand magnitudes independent of any modeling. We then conducted a series of\ncontrolled experiments to unpack the role of each factor in runoff generation\nhypotheses. We show that KGEss is a more reliable metric compared to NSE and\nWIA for model evaluation. We further demonstrate that only changing the error\nmetric -- while other factors remain constant -- can change the model solution\nspace and hence vary model performance, parameter sampling sufficiency, and or\nthe flux map. We show how unreliable error metrics and insufficient parameter\nsampling impair model-based inferences, particularly runoff generation\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 22:24:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Khatami", "Sina", ""], ["Peterson", "Timothy John", ""], ["Peel", "Murray C", ""], ["Western", "Andrew", ""]]}, {"id": "2009.00791", "submitter": "Masahiro Takimoto", "authors": "Masahiro Takimoto", "title": "An Approximation Scheme for Multivariate Information based on Partial\n  Information Decomposition", "comments": "6 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an approximation scheme for multivariate information assuming\nthat synergistic information only appearing in higher order joint distributions\nis suppressed, which may hold in large classes of systems. Our approximation\nscheme gives a practical way to evaluate information among random variables and\nis expected to be applied to feature selection in machine learning. The\ntruncation order of our approximation scheme is given by the order of synergy.\nIn the classification of information, we use the partial information\ndecomposition of the original one. The resulting multivariate information is\nexpected to be reasonable if higher order synergy is suppressed in the system.\nIn addition, it is calculable in relatively easy way if the truncation order is\nnot so large. We also perform numerical experiments to check the validity of\nour approximation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 02:36:41 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Takimoto", "Masahiro", ""]]}, {"id": "2009.00827", "submitter": "Jiajin Wei", "authors": "Jiajin Wei, Ping He and Tiejun Tong", "title": "Estimating the reciprocal of a binomial proportion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classic parameter from the binomial distribution, the binomial\nproportion has been well studied in the literature owing to its wide range of\napplications. In contrast, the reciprocal of the binomial proportion, also\nknown as the inverse proportion, is often overlooked, even though it also plays\nan important role in various fields including clinical studies and random\nsampling. The maximum likelihood estimator of the inverse proportion suffers\nfrom the zero-event problem, and to overcome it, alternative methods have been\ndeveloped in the literature. Nevertheless, there is little work addressing the\noptimality of the existing estimators, as well as their practical performance\ncomparison. Inspired by this, we propose to further advance the literature by\ndeveloping an optimal estimator for the inverse proportion in a family of\nshrinkage estimators. We further derive the explicit and approximate formulas\nfor the optimal shrinkage parameter under different settings. Simulation\nstudies show that the performance of our new estimator performs better than, or\nas well as, the existing competitors in most practical settings. Finally, to\nillustrate the usefulness of our new method, we also revisit a recent\nmeta-analysis on COVID-19 data for assessing the relative risks of physical\ndistancing on the infection of coronavirus, in which six out of seven studies\nencounter the zero-event problem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 05:17:27 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wei", "Jiajin", ""], ["He", "Ping", ""], ["Tong", "Tiejun", ""]]}, {"id": "2009.00874", "submitter": "Takashi Goda", "authors": "Takashi Goda", "title": "A simple algorithm for global sensitivity analysis with Shapley effects", "comments": null, "journal-ref": "Reliability Engineering and System Safety, Volume 213, Article\n  107702, 2021", "doi": "10.1016/j.ress.2021.107702", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Global sensitivity analysis aims at measuring the relative importance of\ndifferent variables or groups of variables for the variability of a quantity of\ninterest. Among several sensitivity indices, so-called Shapley effects have\nrecently gained popularity mainly because the Shapley effects for all the\nindividual variables are summed up to the overall variance, which gives a\nbetter interpretability than the classical sensitivity indices called main\neffects and total effects. In this paper, assuming that all the input variables\nare independent, we introduce a quite simple Monte Carlo algorithm to estimate\nthe Shapley effects for all the individual variables simultaneously, which\ndrastically simplifies the existing algorithms proposed in the literature. We\npresent a short Matlab implementation of our algorithm and show some numerical\nresults. A possible extension to the case where the input variables are\ndependent is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:50:55 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 02:05:28 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 01:04:08 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 01:14:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Goda", "Takashi", ""]]}, {"id": "2009.00921", "submitter": "Christian Hennig", "authors": "Christian Hennig and Pietro Coretto", "title": "An adequacy approach for deciding the number of clusters for OTRIMLE\n  robust Gaussian mixture based clustering", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to deciding the number of clusters. The approach\nis applied to Optimally Tuned Robust Improper Maximum Likelihood Estimation\n(OTRIMLE; Coretto and Hennig 2016) of a Gaussian mixture model allowing for\nobservations to be classified as \"noise\", but it can be applied to other\nclustering methods as well. The quality of a clustering is assessed by a\nstatistic $Q$ that measures how close the within-cluster distributions are to\nelliptical unimodal distributions that have the only mode in the mean. This\nnonparametric measure allows for non-Gaussian clusters as long as they have a\ngood quality according to $Q$. The simplicity of a model is assessed by a\nmeasure $S$ that prefers a smaller number of clusters unless additional\nclusters can reduce the estimated noise proportion substantially. The simplest\nmodel is then chosen that is adequate for the data in the sense that its\nobserved value of $Q$ is not significantly larger than what is expected for\ndata truly generated from the fitted model, as can be assessed by parametric\nbootstrap. The approach is compared with model-based clustering using the\nBayesian Information Criterion (BIC) and the Integrated Complete Likelihood\n(ICL) in a simulation study and on two datasets of scientific interest.\nKeywords: parametric bootstrap; noise component; unimodality; model-based\nclustering\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 09:41:53 GMT"}, {"version": "v2", "created": "Sat, 26 Dec 2020 00:53:46 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hennig", "Christian", ""], ["Coretto", "Pietro", ""]]}, {"id": "2009.00936", "submitter": "Hajo Holzmann", "authors": "Katharina Proksch, Nicolai Bissantz and Hajo Holzmann", "title": "Simultaneous inference for Berkson errors-in-variables regression under\n  fixed design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various applications of regression analysis, in addition to errors in the\ndependent observations also errors in the predictor variables play a\nsubstantial role and need to be incorporated in the statistical modeling\nprocess. In this paper we consider a nonparametric measurement error model of\nBerkson type with fixed design regressors and centered random errors, which is\nin contrast to much existing work in which the predictors are taken as random\nobservations with random noise. Based on an estimator that takes the error in\nthe predictor into account and on a suitable Gaussian approximation, we derive\n%uniform confidence statements for the function of interest. In particular, we\nprovide finite sample bounds on the coverage error of uniform confidence bands,\nwhere we circumvent the use of extreme-value theory and rather rely on recent\nresults on anti-concentration of Gaussian processes. In a simulation study we\ninvestigate the performance of the uniform confidence sets for finite samples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:29:36 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Proksch", "Katharina", ""], ["Bissantz", "Nicolai", ""], ["Holzmann", "Hajo", ""]]}, {"id": "2009.01273", "submitter": "Ping Li", "authors": "Zhixin Zhou, Ping Li, Feifang Hu", "title": "Adaptive Randomization in Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data have appeared frequently in recent research. For example, in\ncomparing the effects of different types of treatment, network models have been\nproposed to improve the quality of estimation and hypothesis testing. In this\npaper, we focus on efficiently estimating the average treatment effect using an\nadaptive randomization procedure in networks. We work on models of causal\nframeworks, for which the treatment outcome of a subject is affected by its own\ncovariate as well as those of its neighbors. Moreover, we consider the case in\nwhich, when we assign treatments to the current subject, only the subnetwork of\nexisting subjects is revealed. New randomized procedures are proposed to\nminimize the mean squared error of the estimated differences between treatment\neffects. In network data, it is usually difficult to obtain theoretical\nproperties because the numbers of nodes and connections increase\nsimultaneously. Under mild assumptions, our proposed procedure is closely\nrelated to a time-varying inhomogeneous Markov chain. We then use Lyapunov\nfunctions to derive the theoretical properties of the proposed procedures. The\nadvantages of the proposed procedures are also demonstrated by extensive\nsimulations and experiments on real network data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:09:37 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhou", "Zhixin", ""], ["Li", "Ping", ""], ["Hu", "Feifang", ""]]}, {"id": "2009.01296", "submitter": "Manjunath B G", "authors": "Barry C. Arnold and B.G. Manjunath", "title": "Statistical Inference for distributions with one Poisson conditional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It will be recalled that the classical bivariate normal distributions have\nnormal marginals and normal conditionals. It is natural to ask whether a\nsimilar phenomenon can be encountered involving Poisson marginals and\nconditionals. Reference to Arnold, Castillo and Sarabia's (1999) book on\nconditionally specified models will confirm that Poisson marginals will be\nencountered, together with both conditionals being of the Poisson form, only in\nthe case in which the variables are independent. Instead, in the present\narticle we will be focusing on bivariate distributions with one marginal and\nthe other family of conditionals being of the Poisson form. Such distributions\nare called Pseudo-Poisson distributions. We discuss distributional features of\nsuch models, explore inferential aspects and include an example of applications\nof the Pseudo-Poisson model to sets of over-dispersed data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 18:49:54 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Arnold", "Barry C.", ""], ["Manjunath", "B. G.", ""]]}, {"id": "2009.01323", "submitter": "Tugba Akkaya Hocagil", "authors": "Tugba Akkaya Hocagil, Louise M. Ryan, Richard J. Cook, Gale A.\n  Richardson, Nancy L. Day, Claire D. Coles, Heather Carmichael Olson, Sandra\n  W. Jacobson, and Joseph L. Jacobson", "title": "A Hierarchical Meta-Analysis for Settings Involving Multiple Outcomes\n  across Multiple Cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence from animal models and epidemiological studies has linked prenatal\nalcohol exposure (PAE) to a broad range of long-term cognitive and behavioral\ndeficits. However, there is virtually no information in the scientific\nliterature regarding the levels of PAE associated with an increased risk of\nclinically significant adverse effects. During the period from 1975-1993,\nseveral prospective longitudinal cohort studies were conducted in the U.S., in\nwhich maternal reports regarding alcohol use were obtained during pregnancy and\nthe cognitive development of the offspring was assessed from early childhood\nthrough early adulthood. The sample sizes in these cohorts did not provide\nsufficient power to examine effects associated with different levels and\npatterns of PAE. To address this critical public health issue, we have\ndeveloped a hierarchical meta-analysis to synthesize information regarding the\neffects of PAE on cognition, integrating data on multiple endpoints from six\nU.S. longitudinal cohort studies. Our approach involves estimating the\ndose-response coefficients for each endpoint and then pooling these correlated\ndose-response coefficients to obtain an estimated `global' effect of exposure\non cognition. In the first stage, we use individual participant data to derive\nestimates of the effects of PAE by fitting regression models that adjust for\npotential confounding variables using propensity scores. The correlation matrix\ncharacterizing the dependence between the endpoint-specific dose-response\ncoefficients estimated within each cohort is then run, while accommodating\nincomplete information on some endpoints. We also compare and discuss\ninferences based on the proposed approach to inferences based on a full\nmultivariate analysis\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 19:48:23 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hocagil", "Tugba Akkaya", ""], ["Ryan", "Louise M.", ""], ["Cook", "Richard J.", ""], ["Richardson", "Gale A.", ""], ["Day", "Nancy L.", ""], ["Coles", "Claire D.", ""], ["Olson", "Heather Carmichael", ""], ["Jacobson", "Sandra W.", ""], ["Jacobson", "Joseph L.", ""]]}, {"id": "2009.01474", "submitter": "Tuomas Rajala", "authors": "Tuomas A. Rajala, Sofia C. Olhede and David J. Murrell", "title": "Spectral estimation for spatial point patterns", "comments": "29 pages + 23 pages of supplements, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article determines how to implement spatial spectral analysis of point\nprocesses (in two dimensions or more), by establishing the moments of raw\nspectral summaries of point processes. We establish the first moments of raw\ndirect spectral estimates such as the discrete Fourier transform of a point\npattern. These have a number of surprising features that departs from the\nproperties of raw spectral estimates of random fields and time series. As for\nrandom fields, the special case of isotropic processes warrants special\nattention, which we discuss. For time series and random fields white noise\nplays a special role, mirrored by the Poisson processes in the case of the\npoint process. For random fields bilinear estimators are prevalent in spectral\nanalysis. We discuss how to smooth any bilinear spectral estimator for a point\nprocess. We also determine how to taper this bilinear spectral estimator, how\nto calculate the periodogram, sample the wavenumbers and discuss the\ncorrelation of the periodogram. In parts this corresponds to recommending\nsuitable separable as well as isotropic tapers in d dimensions. This, in\naggregation, establishes the foundations for spectral analysis of point\nprocesses.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:29:20 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Rajala", "Tuomas A.", ""], ["Olhede", "Sofia C.", ""], ["Murrell", "David J.", ""]]}, {"id": "2009.01517", "submitter": "Enzo D'Innocenzo", "authors": "Enzo D'Innocenzo, Alessandra Luati, Mario Mazzocchi", "title": "A Robust Score-Driven Filter for Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate score-driven model is developed to extract signals from noisy\nvector processes. By assuming that the conditional location vector from a\nmultivariate Student's \\emph{t} distribution changes over time, we construct a\nrobust filter which is able to overcome several issues that naturally arise\nwhen modeling heavy-tailed phenomena and, more in general, vectors of dependent\nnon-Gaussian time series. We derive conditions for stationarity and\ninvertibility and estimate the unknown parameters by maximum likelihood (ML).\nStrong consistency and asymptotic normality of the estimator are proved and the\nfinite sample properties are illustrated by a Monte-Carlo study. From a\ncomputational point of view, analytical formulae are derived, which consent to\ndevelop estimation procedures based on the Fisher scoring method. The theory is\nsupported by a novel empirical illustration that shows how the model can be\neffectively applied to estimate consumer prices from home scanner data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:37:46 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:21:13 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["D'Innocenzo", "Enzo", ""], ["Luati", "Alessandra", ""], ["Mazzocchi", "Mario", ""]]}, {"id": "2009.01520", "submitter": "Samuel Pawel", "authors": "Samuel Pawel, Leonhard Held", "title": "The sceptical Bayes factor for the assessment of replication success", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an urgent need to develop new methodology for the design and\nanalysis of replication studies. Recently, a reverse-Bayes method called the\nsceptical $p$-value has been proposed for this purpose; the inversion of Bayes'\ntheorem allows us to mathematically formalise the notion of scepticism, which\nin turn can be used to assess the agreement between the findings of an original\nstudy and its replication. However, despite its Bayesian nature, the method\nrelies on tail probabilities as primary inference tools. Here, we present an\nextension that uses Bayes factors as an alternative means of quantifying\nevidence. This leads to a new measure for evaluating replication success, the\nsceptical Bayes factor: Conceptually, the sceptical Bayes factor provides a\nbound for the maximum level of evidence at which an advocate of the original\nfinding can convince a sceptic who does not trust it, in light of the\nreplication data. While the sceptical $p$-value can only quantify the conflict\nbetween the sceptical prior and the observed replication data, the sceptical\nBayes factor also takes into account how likely the data are under the\nposterior distribution of the effect conditional on the original study,\nallowing for stronger statements about replication success. Moreover, the\nproposed method elegantly combines traditional notions of replication success;\nit ensures that both studies need to show evidence against the null, while at\nthe same time penalising incompatibility of their effect estimates. Case\nstudies from the Reproducibility Project: Cancer Biology and the Social\nSciences Replication Project show the advantages of the method for the\nquantitative assessment of replicability.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:54:03 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Pawel", "Samuel", ""], ["Held", "Leonhard", ""]]}, {"id": "2009.01522", "submitter": "Thilo Welz", "authors": "Thilo Welz, Philipp Doebler, Markus Pauly", "title": "Fisher transformation based Confidence Intervals of Correlations in\n  Fixed- and Random-Effects Meta-Analysis", "comments": "28 pages, 4 figures, 17 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analyses of correlation coefficients are an important technique to\nintegrate results from many cross-sectional and longitudinal research designs.\nUncertainty in pooled estimates is typically assessed with the help of\nconfidence intervals, which can double as hypothesis tests for two-sided\nhypotheses about the underlying correlation. A standard approach to construct\nconfidence intervals for the main effect is the Hedges-Olkin-Vevea Fisher-z\n(HOVz) approach, which is based on the Fisher-z transformation. Results from\nprevious studies (Field, 2005; Hafdahl and Williams, 2009), however, indicate\nthat in random-effects models the performance of the HOVz confidence interval\ncan be unsatisfactory. To this end, we propose improvements of the HOVz\napproach, which are based on enhanced variance estimators for the main effect\nestimate. In order to study the coverage of the new confidence intervals in\nboth fixed- and random-effects meta-analysis models, we perform an extensive\nsimulation study, comparing them to established approaches. Data were generated\nvia a truncated normal and beta distribution model. The results show that our\nnewly proposed confidence intervals based on a Knapp-Hartung-type variance\nestimator or robust heteroscedasticity consistent sandwich estimators in\ncombination with the integral z-to-r transformation (Hafdahl, 2009) provide\nmore accurate coverage than existing approaches in most scenarios, especially\nin the more appropriate beta distribution simulation model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:58:18 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Welz", "Thilo", ""], ["Doebler", "Philipp", ""], ["Pauly", "Markus", ""]]}, {"id": "2009.01549", "submitter": "Kanchan Aggarwal", "authors": "Kanchan Aggarwal, Siddhartha Mukhopadhyay, Arun K Tangirala", "title": "Statistical characterization and time-series modeling of seismic noise", "comments": "21 pages, 6 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing statistical models for seismic noise is an exercise of high value\nin seismic data analysis since these models play a critical role in detecting\nthe onset of seismic events. A majority of these models are usually built on\ncertain critical assumptions, namely, stationarity, linearity, and Gaussianity.\nDespite their criticality, very little reported literature exists on validating\nthese assumptions on real seismic data. The objectives of this work are (i) to\ncritically study these long-held assumptions and (ii) to propose a systematic\nprocedure for developing appropriate time-series models. A rigorous statistical\nanalysis reveals that these standard assumptions do not hold for most of the\ndata sets under study; rather they exhibit additional special features such as\nheteroskedasticity and integrating effects. Resting on these novel discoveries,\nARIMA-GARCH models are developed for seismic noise. Studies are carried out on\n$185$ real-time data sets over different time intervals to study the daily and\nseasonal variations in noise characteristics and model structure. Nearly all\ndatasets tested positive for first-order non-stationarity, heteroskedasticity,\nand Gaussianity, while $19\\%$ tested negative for linearity. Analysis of the\nstructural uniformity of the developed models with respect to daily and\nseasonal variations is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:47:01 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Aggarwal", "Kanchan", ""], ["Mukhopadhyay", "Siddhartha", ""], ["Tangirala", "Arun K", ""]]}, {"id": "2009.01551", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Zhiliang Ying and Haoran Zhang", "title": "Unfolding-Model-Based Visualization: Theory, Method and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional unfolding methods are widely used for visualizing item\nresponse data. Such methods project respondents and items simultaneously onto a\nlow-dimensional Euclidian space, in which respondents and items are represented\nby ideal points, with person-person, item-item, and person-item similarities\nbeing captured by the Euclidian distances between the points. In this paper, we\nstudy the visualization of multidimensional unfolding from a statistical\nperspective. We cast multidimensional unfolding into an estimation problem,\nwhere the respondent and item ideal points are treated as parameters to be\nestimated. An estimator is then proposed for the simultaneous estimation of\nthese parameters. Asymptotic theory is provided for the recovery of the ideal\npoints, shedding lights on the validity of model-based visualization. An\nalternating projected gradient descent algorithm is proposed for the parameter\nestimation. We provide two illustrative examples, one on users' movie rating\nand the other on senate roll call voting.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 09:49:38 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Chen", "Yunxiao", ""], ["Ying", "Zhiliang", ""], ["Zhang", "Haoran", ""]]}, {"id": "2009.01595", "submitter": "Xiuqin Xu", "authors": "Xiuqin Xu, Ying Chen, Yannig Goude, Qiwei Yao", "title": "Probabilistic Forecasting for Daily Electricity Loads and Quantiles for\n  Curve-to-Curve Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting of electricity load curves is of fundamental\nimportance for effective scheduling and decision making in the increasingly\nvolatile and competitive energy markets. We propose a novel approach to\nconstruct probabilistic predictors for curves (PPC), which leads to a natural\nand new definition of quantiles in the context of curve-to-curve linear\nregression. There are three types of PPC: a predictive set, a predictive band\nand a predictive quantile, all of which are defined at a pre-specified nominal\nprobability level. In the simulation study, the PPC achieve promising coverage\nprobabilities under a variety of data generating mechanisms. When applying to\none day ahead forecasting for the French daily electricity load curves, PPC\noutperform several state-of-the-art predictive methods in terms of forecasting\naccuracy, coverage rate and average length of the predictive bands. The\npredictive quantile curves provide insightful information which is highly\nrelevant to hedging risks in electricity supply management.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 12:00:20 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 02:57:17 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Xu", "Xiuqin", ""], ["Chen", "Ying", ""], ["Goude", "Yannig", ""], ["Yao", "Qiwei", ""]]}, {"id": "2009.01799", "submitter": "Dootika Vats", "authors": "Medha Agarwal and Dootika Vats", "title": "Globally-centered autocovariances in MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autocovariances are a fundamental quantity of interest in Markov chain Monte\nCarlo (MCMC) simulations with autocorrelation function (ACF) plots being an\nintegral visualization tool for performance assessment. Unfortunately, for slow\nmixing Markov chains, the empirical autocovariance can highly underestimate the\ntruth. For multiple-chain MCMC sampling, we propose a globally-centered\nestimator of the autocovariance function (G-ACvF) that exhibits significant\ntheoretical and empirical improvements. We show that the bias of the G-ACvF\nestimator is smaller than the bias of the current state-of-the-art. The impact\nof this improved estimator is evident in three critical output analysis\napplications: (1) ACF plots, (2) estimates of the Monte Carlo asymptotic\ncovariance matrix, and (3) estimates of the effective sample size. Under weak\nconditions, we establish strong consistency of our improved asymptotic\ncovariance estimator, and obtain its large-sample bias and variance. The\nperformance of the new estimators is demonstrated through various examples.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 16:59:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Agarwal", "Medha", ""], ["Vats", "Dootika", ""]]}, {"id": "2009.01897", "submitter": "Olli Saarela", "authors": "Sangita Kulathinal, Minna S\\\"a\\\"av\\\"al\\\"a, Kari Auranen, Olli Saarela", "title": "Estimation of marriage incidence rates by combining two cross-sectional\n  retrospective designs: Event history analysis of two dependent processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to develop methods for studying the determinants of\nmarriage incidence using marriage histories collected under two different types\nof retrospective cross-sectional study designs. These designs are: sampling of\never married women before the cross-section, a prevalent cohort, and sampling\nof women irrespective of marital status, a general cross-sectional cohort.\nWhile retrospective histories from a prevalent cohort do not identify incidence\nrates without parametric modelling assumptions, the rates can be identified\nwhen combined with data from a general cohort. Moreover, education, a strong\nendogenous covariate, and marriage processes are correlated. Hence, they need\nto be modelled jointly in order to estimate the marriage incidence. For this\npurpose, we specify a multi-state model and propose a likelihood-based\nestimation method. We outline the assumptions under which a likelihood\nexpression involving only marriage incidence parameters can be derived. This is\nof particular interest when either retrospective education histories are not\navailable or related parameters are not of interest. Our simulation results\nconfirm the gain in efficiency by combining data from the two designs, while\ndemonstrating how the parameter estimates are affected by violations of the\nassumptions used in deriving the simplified likelihood expressions. Two Indian\nNational Family Health Surveys are used as motivation for the methodological\ndevelopment and to demonstrate the application of the methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:31:44 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Kulathinal", "Sangita", ""], ["S\u00e4\u00e4v\u00e4l\u00e4", "Minna", ""], ["Auranen", "Kari", ""], ["Saarela", "Olli", ""]]}, {"id": "2009.01940", "submitter": "Noah Haber", "authors": "Noah A Haber, Emma Clarke-Deelder, Joshua A Salomon, Avi Feller,\n  Elizabeth A Stuart", "title": "COVID-19 Policy Impact Evaluation: A guide to common design issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy responses to COVID-19, particularly those related to\nnon-pharmaceutical interventions, are unprecedented in scale and scope.\nEpidemiologists are more involved in policy decisions and evidence generation\nthan ever before. However, policy impact evaluations always require a complex\ncombination of circumstance, study design, data, statistics, and analysis.\nBeyond the issues that are faced for any policy, evaluation of COVID-19\npolicies is complicated by additional challenges related to infectious disease\ndynamics and lags, lack of direct observation of key outcomes, and a\nmultiplicity of interventions occurring on an accelerated time scale. The\nmethods needed for policy-level impact evaluation are not often used or taught\nin epidemiology, and differ in important ways that may not be obvious. The\nvolume and speed, and methodological complications of policy evaluations can\nmake it difficult for decision-makers and researchers to synthesize and\nevaluate strength of evidence in COVID-19 health policy papers.\n  In this paper, we (1) introduce the basic suite of policy impact evaluation\ndesigns for observational data, including cross-sectional analyses, pre/post,\ninterrupted time-series, and difference-in-differences analysis, (2)\ndemonstrate key ways in which the requirements and assumptions underlying these\ndesigns are often violated in the context of COVID-19, and (3) provide\ndecision-makers and reviewers a conceptual and graphical guide to identifying\nthese key violations. The overall goal of this paper is to help\nepidemiologists, policy-makers, journal editors, journalists, researchers, and\nother research consumers understand and weigh the strengths and limitations of\nevidence that is essential to decision-making.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 22:08:29 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 19:54:05 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 20:57:47 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 16:56:43 GMT"}, {"version": "v5", "created": "Thu, 31 Dec 2020 21:22:39 GMT"}, {"version": "v6", "created": "Sat, 6 Mar 2021 02:18:17 GMT"}, {"version": "v7", "created": "Fri, 16 Apr 2021 19:12:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Haber", "Noah A", ""], ["Clarke-Deelder", "Emma", ""], ["Salomon", "Joshua A", ""], ["Feller", "Avi", ""], ["Stuart", "Elizabeth A", ""]]}, {"id": "2009.01983", "submitter": "Didong Li", "authors": "Didong Li, Yulong Lu, Emmanuel Chevallier, David B. Dunson", "title": "Density estimation and modeling on symmetric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data and/or parameters are supported on non-Euclidean\nmanifolds. It is important to take into account the geometric structure of\nmanifolds in statistical analysis to avoid misleading results. Although there\nhas been a considerable focus on simple and specific manifolds, there is a lack\nof general and easy-to-implement statistical methods for density estimation and\nmodeling on manifolds. In this article, we consider a very broad class of\nmanifolds: non-compact Riemannian symmetric spaces. For this class, we provide\na very general mathematical result for easily calculating volume changes of the\nexponential and logarithm map between the tangent space and the manifold. This\nallows one to define statistical models on the tangent space, push these models\nforward onto the manifold, and easily calculate induced distributions by\nJacobians. To illustrate the statistical utility of this theoretical result, we\nprovide a general method to construct distributions on symmetric spaces. In\nparticular, we define the log-Gaussian distribution as an analogue of the\nmultivariate Gaussian distribution in Euclidean space. With these new kernels\non symmetric spaces, we also consider the problem of density estimation. Our\nproposed approach can use any existing density estimation approach designed for\nEuclidean spaces and push it forward to the manifold with an easy-to-calculate\nadjustment. We provide theorems showing that the induced density estimators on\nthe manifold inherit the statistical optimality properties of the parent\nEuclidean density estimator; this holds for both frequentist and Bayesian\nnonparametric methods. We illustrate the theory and practical utility of the\nproposed approach on the space of positive definite matrices.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 02:25:13 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 23:39:24 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 17:59:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Didong", ""], ["Lu", "Yulong", ""], ["Chevallier", "Emmanuel", ""], ["Dunson", "David B.", ""]]}, {"id": "2009.02112", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya, Shirshendu Chatterjee and Soumendu Sundar\n  Mukherjee", "title": "Consistent detection and optimal localization of all detectable change\n  points in piecewise stationary arbitrarily sparse network-sequences", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the offline change point detection and localization problem in\nthe context of piecewise stationary networks, where the observable is a finite\nsequence of networks. We develop algorithms involving some suitably modified\nCUSUM statistics based on adaptively trimmed adjacency matrices of the observed\nnetworks for both detection and localization of single or multiple change\npoints present in the input data. We provide rigorous theoretical analysis and\nfinite sample estimates evaluating the performance of the proposed methods when\nthe input (finite sequence of networks) is generated from an inhomogeneous\nrandom graph model, where the change points are characterized by the change in\nthe mean adjacency matrix. We show that the proposed algorithms can detect\n(resp. localize) all change points, where the change in the expected adjacency\nmatrix is above the minimax detectability (resp. localizability) threshold,\nconsistently without any a priori assumption about (a) a lower bound for the\nsparsity of the underlying networks, (b) an upper bound for the number of\nchange points, and (c) a lower bound for the separation between successive\nchange points, provided either the minimum separation between successive pairs\nof change points or the average degree of the underlying networks goes to\ninfinity arbitrarily slowly. We also prove that the above condition is\nnecessary to have consistency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:17:10 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Chatterjee", "Shirshendu", ""], ["Mukherjee", "Soumendu Sundar", ""]]}, {"id": "2009.02136", "submitter": "Kelly Van Lancker", "authors": "Kelly Van Lancker, An Vandebosch and Stijn Vansteelandt", "title": "Efficient, Doubly Robust Estimation of the Effect of Dose Switching for\n  Switchers in a Randomised Clinical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a clinical trial conducted by Janssen Pharmaceuticals in which a\nflexible dosing regimen is compared to placebo, we evaluate how switchers in\nthe treatment arm (i.e., patients who were switched to the higher dose) would\nhave fared had they been kept on the low dose. This in order to understand\nwhether flexible dosing is potentially beneficial for them. Simply comparing\nthese patients' responses with those of patients who stayed on the low dose is\nunsatisfactory because the latter patients are usually in a better health\ncondition. Because the available information in the considered trial is too\nscarce to enable a reliable adjustment, we will instead transport data from a\nfixed dosing trial that has been conducted concurrently on the same target,\nalbeit not in an identical patient population. In particular, we will propose\nan estimator which relies on an outcome model and a propensity score model for\nthe association between study and patient characteristics. The proposed\nestimator is asymptotically unbiased if at least one of both models is\ncorrectly specified, and efficient (under the model defined by the restrictions\non the propensity score) when both models are correctly specified. We show that\nthe proposed method for using results from an external study is generically\napplicable in studies where a classical confounding adjustment is not possible\ndue to positivity violation (e.g., studies where switching takes place in a\ndeterministic manner). Monte Carlo simulations and application to the\nmotivating study demonstrate adequate performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 12:26:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Van Lancker", "Kelly", ""], ["Vandebosch", "An", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2009.02287", "submitter": "Hanzhong Liu", "authors": "Wei Ma, Fuyi Tu, Hanzhong Liu", "title": "Regression analysis for covariate-adaptive randomization: A robust and\n  efficient inference perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is arguably the most fundamental statistical model;\nhowever, the validity of its use in randomized clinical trials, despite being\ncommon practice, has never been crystal clear, particularly when stratified or\ncovariate-adaptive randomization is used. In this paper, we investigate several\nof the most intuitive and commonly used regression models for estimating and\ninferring the treatment effect in randomized clinical trials. By allowing the\nregression model to be arbitrarily misspecified, we demonstrate that all these\nregression-based estimators robustly estimate the treatment effect, albeit with\npossibly different efficiency. We also propose consistent non-parametric\nvariance estimators and compare their performances to those of the model-based\nvariance estimators that are readily available in standard statistical\nsoftware. Based on the results and taking into account both theoretical\nefficiency and practical feasibility, we make recommendations for the effective\nuse of regression under various scenarios. For equal allocation, it suffices to\nuse the regression adjustment for the stratum covariates and additional\nbaseline covariates, if available, with the usual ordinary-least-squares\nvariance estimator. For unequal allocation, regression with\ntreatment-by-covariate interactions should be used, together with our proposed\nvariance estimators. These recommendations apply to simple and stratified\nrandomization, and minimization, among others. We hope this work helps to\nclarify and promote the usage of regression in randomized clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 16:27:54 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Ma", "Wei", ""], ["Tu", "Fuyi", ""], ["Liu", "Hanzhong", ""]]}, {"id": "2009.02297", "submitter": "Johan Ugander", "authors": "Johan Ugander, Hao Yin", "title": "Randomized Graph Cluster Randomization", "comments": "59 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global average treatment effect (GATE) is a primary quantity of interest\nin the study of causal inference under network interference. With a correctly\nspecified exposure model of the interference, the Horvitz-Thompson (HT) and\nH\\'ajek estimators of the GATE are unbiased and consistent, respectively, yet\nknown to exhibit extreme variance under many designs and in many settings of\ninterest. With a fixed clustering of the interference graph, graph cluster\nrandomization (GCR) designs have been shown to greatly reduce variance compared\nto node-level random assignment, but even so the variance is still often\nprohibitively large. In this work we propose a randomized version of the GCR\ndesign, descriptively named randomized graph cluster randomization (RGCR),\nwhich uses a random clustering rather than a single fixed clustering. By\nconsidering an ensemble of many different cluster assignments, this design\navoids a key problem with GCR where a given node is sometimes \"lucky\" or\n\"unlucky\" in a given clustering. We propose two randomized graph decomposition\nalgorithms for use with RGCR, randomized 3-net and 1-hop-max, adapted from\nprior work on multiway graph cut problems. When integrating over their own\nrandomness, these algorithms furnish network exposure probabilities that can be\nestimated efficiently. We develop upper bounds on the variance of the HT\nestimator of the GATE under assumptions on the metric structure of the\ninterference graph. Where the best known variance upper bound for the HT\nestimator under a GCR design is exponential in the parameters of the metric\nstructure, we give a comparable variance upper bound under RGCR that is instead\npolynomial in the same parameters. We provide extensive simulations comparing\nRGCR and GCR designs, observing substantial reductions in the mean squared\nerror for both HT and H\\'ajek estimators of the GATE in a variety of settings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 16:50:41 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Ugander", "Johan", ""], ["Yin", "Hao", ""]]}, {"id": "2009.02305", "submitter": "Chuang Wan", "authors": "Chuang Wan", "title": "Composite Estimation for Quantile Regression Kink Models with\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kink model is developed to analyze the data where the regression function is\ntwostage linear but intersects at an unknown threshold. In quantile regression\nwith longitudinal data, previous work assumed that the unknown threshold\nparameters or kink points are heterogeneous across different quantiles.\nHowever, the location where kink effect happens tend to be the same across\ndifferent quantiles, especially in a region of neighboring quantile levels.\nIgnoring such homogeneity information may lead to efficiency loss for\nestimation. In view of this, we propose a composite estimator for the common\nkink point by absorbing information from multiple quantiles. In addition, we\nalso develop a sup-likelihood-ratio test to check the kink effect at a given\nquantile level. A test-inversion confidence interval for the common kink point\nis also developed based on the quantile rank score test. The simulation study\nshows that the proposed composite kink estimator is more competitive with the\nleast square estimator and the single quantile estimator. We illustrate the\npractical value of this work through the analysis of a body mass index and\nblood pressure data set.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 17:08:14 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wan", "Chuang", ""]]}, {"id": "2009.02314", "submitter": "Sami Stouli", "authors": "Whitney K. Newey and Sami Stouli", "title": "Heterogeneous Coefficients, Control Variables, and Identification of\n  Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional heterogeneity and endogeneity are important features of\nmodels with multiple treatments. We consider a heterogeneous coefficients model\nwhere the outcome is a linear combination of dummy treatment variables, with\neach variable representing a different kind of treatment. We use control\nvariables to give necessary and sufficient conditions for identification of\naverage treatment effects. With mutually exclusive treatments we find that,\nprovided the generalized propensity scores (Imbens, 2000) are bounded away from\nzero with probability one, a simple identification condition is that their sum\nbe bounded away from one with probability one. These results generalize the\nclassical identification result of Rosenbaum and Rubin (1983) for binary\ntreatments.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 17:37:47 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Newey", "Whitney K.", ""], ["Stouli", "Sami", ""]]}, {"id": "2009.02362", "submitter": "Nirvik Sinha", "authors": "Nirvik Sinha", "title": "Bootstrap p-values reduce type 1 error of the robust rank-order test of\n  difference in medians", "comments": "22 pages, 1 table, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The robust rank-order test (Fligner and Policello, 1981) was designed as an\nimprovement of the non-parametric Wilcoxon-Mann-Whitney U-test to be more\nappropriate when the samples being compared have unequal variance. However, it\ntends to be excessively liberal when the samples are asymmetric. This is likely\nbecause the test statistic is assumed to have a standard normal distribution\nfor sample sizes > 12. This work proposes an on-the-fly method to obtain the\ndistribution of the test statistic from which the critical/p-value may be\ncomputed directly. The method of likelihood maximization is used to estimate\nthe parameters of the parent distributions of the samples being compared. Using\nthese estimated populations, the null distribution of the test statistic is\nobtained by the Monte-Carlo method. Simulations are performed to compare the\nproposed method with that of standard normal approximation of the test\nstatistic. For small sample sizes (<= 20), the Monte-Carlo method outperforms\nthe normal approximation method. This is especially true for low values of\nsignificance levels (< 5%). Additionally, when the smaller sample has the\nlarger standard deviation, the Monte-Carlo method outperforms the normal\napproximation method even for large sample sizes (= 40/60). The two methods do\nnot differ in power. Finally, a Monte-Carlo sample size of 10^4 is found to be\nsufficient to obtain the aforementioned relative improvements in performance.\nThus, the results of this study pave the way for development of a toolbox to\nperform the robust rank-order test in a distribution-free manner.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 18:59:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sinha", "Nirvik", ""]]}, {"id": "2009.02597", "submitter": "Kun Chen", "authors": "Wenjie Wang, Chongliang Luo, Robert H. Aseltine, Fei Wang, Jun Yan,\n  Kun Chen", "title": "Suicide Risk Modeling with Uncertain Diagnostic Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the pressing need for suicide prevention through improving\nbehavioral healthcare, we use medical claims data to study the risk of\nsubsequent suicide attempts for patients who were hospitalized due to suicide\nattempts and later discharged. Understanding the risk behaviors of such\npatients at elevated suicide risk is an important step towards the goal of\n\"Zero Suicide\". An immediate and unconventional challenge is that the\nidentification of suicide attempts from medical claims contains substantial\nuncertainty: almost 20\\% of \"suspected\" suicide attempts are identified from\ndiagnostic codes indicating external causes of injury and poisoning with\nundermined intent. It is thus of great interest to learn which of these\nundetermined events are more likely actual suicide attempts and how to properly\nutilize them in survival analysis with severe censoring. To tackle these\ninterrelated problems, we develop an integrative Cox cure model with\nregularization to perform survival regression with uncertain events and a\nlatent cure fraction. We apply the proposed approach to study the risk of\nsubsequent suicide attempt after suicide-related hospitalization for adolescent\nand young adult population, using medical claims data from Connecticut. The\nidentified risk factors are highly interpretable; more intriguingly, our method\ndistinguishes the risk factors that are most helpful in assessing either\nsusceptibility or timing of subsequent attempt. The predicted statuses of the\nuncertain attempts are further investigated, leading to several new insights on\nsuicide event identification.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 20:47:16 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Wenjie", ""], ["Luo", "Chongliang", ""], ["Aseltine", "Robert H.", ""], ["Wang", "Fei", ""], ["Yan", "Jun", ""], ["Chen", "Kun", ""]]}, {"id": "2009.02609", "submitter": "Ashwin Pananjady", "authors": "Ashwin Pananjady, Richard J. Samworth", "title": "Isotonic regression with unknown permutations: Statistics, computation,\n  and adaptation", "comments": "Version v2 contains reorganized material, one figure, and expanded\n  discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by models for multiway comparison data, we consider the problem of\nestimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from\nnoisy observations collected on a uniform lattice, but where the design points\nhave been permuted along each dimension. While the univariate and bivariate\nversions of this problem have received significant attention, our focus is on\nthe multivariate case $d \\geq 3$. We study both the minimax risk of estimation\n(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified\nby the adaptivity index) to a family of piecewise constant functions. We\nprovide a computationally efficient Mirsky partition estimator that is minimax\noptimal while also achieving the smallest adaptivity index possible for\npolynomial time procedures. Thus, from a worst-case perspective and in sharp\ncontrast to the bivariate case, the latent permutations in the model do not\nintroduce significant computational difficulties over and above vanilla\nisotonic regression. On the other hand, the fundamental limits of adaptation\nare significantly different with and without unknown permutations: Assuming a\nhardness conjecture from average-case complexity theory, a\nstatistical-computational gap manifests in the former case. In a complementary\ndirection, we show that natural modifications of existing estimators fail to\nsatisfy at least one of the desiderata of optimal worst-case statistical\nperformance, computational efficiency, and fast adaptation. Along the way to\nshowing our results, we improve adaptation results in the special case $d = 2$\nand establish some properties of estimators for vanilla isotonic regression,\nboth of which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 22:17:51 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 13:58:37 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pananjady", "Ashwin", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2009.02623", "submitter": "Zifeng Wang", "authors": "Zifeng Wang and Xi Chen and Rui Wen and Shao-Lun Huang and Ercan E.\n  Kuruoglu and Yefeng Zheng", "title": "Information Theoretic Counterfactual Learning from Missing-Not-At-Random\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual learning for dealing with missing-not-at-random data (MNAR) is\nan intriguing topic in the recommendation literature since MNAR data are\nubiquitous in modern recommender systems. Missing-at-random (MAR) data, namely\nrandomized controlled trials (RCTs), are usually required by most previous\ncounterfactual learning methods for debiasing learning. However, the execution\nof RCTs is extraordinarily expensive in practice. To circumvent the use of\nRCTs, we build an information-theoretic counterfactual variational information\nbottleneck (CVIB), as an alternative for debiasing learning without RCTs. By\nseparating the task-aware mutual information term in the original information\nbottleneck Lagrangian into factual and counterfactual parts, we derive a\ncontrastive information loss and an additional output confidence penalty, which\nfacilitates balanced learning between the factual and counterfactual domains.\nEmpirical evaluation on real-world datasets shows that our CVIB significantly\nenhances both shallow and deep models, which sheds light on counterfactual\nlearning in recommendation that goes beyond RCTs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 01:22:47 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 13:54:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Zifeng", ""], ["Chen", "Xi", ""], ["Wen", "Rui", ""], ["Huang", "Shao-Lun", ""], ["Kuruoglu", "Ercan E.", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2009.02695", "submitter": "Kohei Yoshikawa", "authors": "Kohei Yoshikawa, Shuichi Kawano", "title": "Multilinear Common Component Analysis via Kronecker Product\n  Representation", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extracting a common structure from multiple tensor\ndatasets. For this purpose, we propose multilinear common component analysis\n(MCCA) based on Kronecker products of mode-wise covariance matrices. MCCA\nconstructs a common basis represented by linear combinations of the original\nvariables which loses as little information of the multiple tensor datasets. We\nalso develop an estimation algorithm for MCCA that guarantees mode-wise global\nconvergence. Numerical studies are conducted to show the effectiveness of MCCA.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 10:03:17 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 08:06:05 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Yoshikawa", "Kohei", ""], ["Kawano", "Shuichi", ""]]}, {"id": "2009.02776", "submitter": "Marco Morucci", "authors": "Marco Morucci and Cynthia Rudin", "title": "Matching Bounds: How Choice of Matching Algorithm Impacts Treatment\n  Effects Estimates and What to Do about It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Different matches on the same data may produce different treatment effect\nestimates, even when they achieve similar balance or minimize the same loss\nfunction. We discuss reasons and consequences of this problem. We present\nevidence of this problem by replicating ten papers that use matching. We\nintroduce Matching Bounds: a finite-sample, non-stochastic method that allows\nanalysts to know whether a matched sample that produces different results with\nthe same levels of balance and overall match quality could be obtained from\ntheir data. We apply Matching Bounds to a replication of a matching study of\nthe effects of foreign aid on civil conflict and find that results could change\nbased on the matched sample chosen.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:03:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 23:35:26 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Morucci", "Marco", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2009.02841", "submitter": "Alan Mishler", "authors": "Alan Mishler (1), Edward H. Kennedy (1) ((1) Carnegie Mellon\n  University)", "title": "Fairness in Risk Assessment Instruments: Post-Processing to Achieve\n  Counterfactual Equalized Odds", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic fairness is a topic of increasing concern both within research\ncommunities and among the general public. Conventional fairness criteria place\nrestrictions on the joint distribution of a sensitive feature $A$, an outcome\n$Y$, and a predictor $S$. For example, the criterion of equalized odds requires\nthat $S$ be conditionally independent of $A$ given $Y$, or equivalently, when\nall three variables are binary, that the false positive and false negative\nrates of the predictor be the same for two levels of $A$.\n  However, fairness criteria based around observable $Y$ are misleading when\napplied to Risk Assessment Instruments (RAIs), such as predictors designed to\nestimate the risk of recidivism or child neglect. It has been argued instead\nthat RAIs ought to be trained and evaluated with respect to potential outcomes\n$Y^0$. Here, $Y^0$ represents the outcome that would be observed under no\nintervention--for example, whether recidivism would occur if a defendant were\nto be released pretrial.\n  In this paper, we develop a method to post-process an existing binary\npredictor to satisfy approximate counterfactual equalized odds, which requires\n$S$ to be nearly conditionally independent of $A$ given $Y^0$, within a\ntolerance specified by the user. Our predictor converges to an optimal fair\npredictor at $\\sqrt{n}$ rates under appropriate assumptions. We propose doubly\nrobust estimators of the risk and fairness properties of a fixed post-processed\npredictor, and we show that they are $\\sqrt{n}$-consistent and asymptotically\nnormal under appropriate assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 00:50:30 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Mishler", "Alan", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "2009.02921", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng", "title": "Penalized Maximum Likelihood Estimator for Mixture of von Mises-Fisher\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The von Mises-Fisher distribution is one of the most widely used probability\ndistributions to describe directional data. Finite mixtures of von Mises-Fisher\ndistributions have found numerous applications. However, the likelihood\nfunction for the finite mixture of von Mises-Fisher distributions is unbounded\nand consequently the maximum likelihood estimation is not well defined. To\naddress the problem of likelihood degeneracy, we consider a penalized maximum\nlikelihood approach whereby a penalty function is incorporated. We prove strong\nconsistency of the resulting estimator. An Expectation-Maximization algorithm\nfor the penalized likelihood function is developed and simulation studies are\nperformed to examine its performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 07:48:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ng", "Tin Lok James", ""]]}, {"id": "2009.03014", "submitter": "Samudra Herath", "authors": "Samudra Herath, Matthew Roughan, Gary Glonek", "title": "Simulating Name-like Vectors for Testing Large-scale Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient entity resolution (ER) has been a problem in data\nanalysis and data mining projects for decades. In our work, we are interested\nin developing ER methods to handle big data. Good public datasets are\nrestricted in this area and usually small in size. Simulation is one technique\nfor generating datasets for testing. Existing simulation tools have problems of\ncomplexity, scalability and limitations of resampling. We address these\nproblems by introducing a better way of simulating testing data for big data\nER. Our proposed simulation model is simple, inexpensive and fast. We focus on\navoiding the detail-level simulation of records using a simple vector\nrepresentation. In this paper, we will discuss how to simulate simple vectors\nthat approximate the properties of names (commonly used as identification\nkeys).\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 11:00:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Herath", "Samudra", ""], ["Roughan", "Matthew", ""], ["Glonek", "Gary", ""]]}, {"id": "2009.03058", "submitter": "Hans Van Houwelingen", "authors": "Hans C. van Houwelingen, Ronald Brand, Thomas A. Louis", "title": "Empirical Bayes methods for monitoring health care quality", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses empirical Bayes methodology for repeated quality\ncomparisons of health care institutions using data from the Dutch VOKS study\nthat annually monitors the relative performance and quality of nearly all Dutch\ngynecological centres with respect to different aspects of the childbirths\ntaking place in these centres. This paper can be seen as an extension of the\npioneering work of Thomas, Longford and Rolph and Goldstein and Spiegelhalter.\nFirst of all, this paper introduces a new simple crude estimate of the centre\neffect in a logistic regression setting. Next, a simple estimate of the\nexpected percentile of a centre given all data and a measure of rankability of\nthe centres based on the expected percentiles are presented. Finally, the\ntemporal dimension is explored and methods are discussed to predict next years\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:30:25 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["van Houwelingen", "Hans C.", ""], ["Brand", "Ronald", ""], ["Louis", "Thomas A.", ""]]}, {"id": "2009.03117", "submitter": "Ivo V. Stoepker", "authors": "Ivo V. Stoepker, Rui M. Castro, Ery Arias-Castro, Edwin van den Heuvel", "title": "Anomaly Detection for a Large Number of Streams: A Permutation-Based\n  Higher Criticism Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection when observing a large number of data streams is essential\nin a variety of applications, ranging from epidemiological studies to\nmonitoring of complex systems. High-dimensional scenarios are usually tackled\nwith scan-statistics and related methods, requiring stringent modeling\nassumptions for proper calibration. In this work we take a non-parametric\nstance, and propose a permutation-based variant of the higher criticism\nstatistic not requiring knowledge of the null distribution. This results in an\nexact test in finite samples which is asymptotically optimal in the wide class\nof exponential models. We demonstrate the power loss in finite samples is\nminimal with respect to the oracle test. Furthermore, since the proposed\nstatistic does not rely on asymptotic approximations it typically performs\nbetter than popular variants of higher criticism that rely on such\napproximations. We include recommendations such that the test can be readily\napplied in practice, and demonstrate its applicability in monitoring the daily\nnumber of COVID-19 cases in the Netherlands.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 14:07:31 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 15:15:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Stoepker", "Ivo V.", ""], ["Castro", "Rui M.", ""], ["Arias-Castro", "Ery", ""], ["Heuvel", "Edwin van den", ""]]}, {"id": "2009.03167", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Johannes Ruf, Martin Larsson, Wouter Koolen", "title": "Admissible anytime-valid sequential inference must rely on nonnegative\n  martingales", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wald's anytime-valid $p$-values and Robbins' confidence sequences enable\nsequential inference for composite and nonparametric classes of distributions\nat arbitrary stopping times, as do more recent proposals involving Vovk's\n`$e$-values' or Shafer's `betting scores'. Examining the literature, one finds\nthat at the heart of all these (quite different) approaches has been the\nidentification of composite nonnegative (super)martingales. Thus, informally,\nnonnegative (super)martingales are known to be sufficient for \\emph{valid}\nsequential inference. Our central contribution is to show that martingales are\nalso universal---all \\emph{admissible} constructions of (composite) anytime\n$p$-values, confidence sequences, or $e$-values must necessarily utilize\nnonnegative martingales (or so-called max-martingales in the case of\n$p$-values). Sufficient conditions for composite admissibility are also\nprovided. Our proofs utilize a plethora of modern mathematical tools for\ncomposite testing and estimation problems: max-martingales, Snell envelopes,\nand new Doob-L\\'evy martingales make appearances in previously unencountered\nways. Informally, if one wishes to perform anytime-valid sequential inference,\nthen any existing approach can be recovered or dominated using martingales. We\nprovide several sophisticated examples, with special focus on the nonparametric\nproblem of testing if a distribution is symmetric, where our new constructions\nrender past methods inadmissible.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 15:32:13 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 06:30:14 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Ruf", "Johannes", ""], ["Larsson", "Martin", ""], ["Koolen", "Wouter", ""]]}, {"id": "2009.03443", "submitter": "Sagar Kumar Tamang", "authors": "Sagar K. Tamang, Ardeshir Ebtehaj, Peter J. Van Leeuwen, Dongmian Zou,\n  and Gilad Lerman", "title": "Ensemble Riemannian Data Assimilation over the Wasserstein Space", "comments": null, "journal-ref": null, "doi": "10.5194/npg-28-295-2021", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an ensemble data assimilation paradigm over a\nRiemannian manifold equipped with the Wasserstein metric. Unlike the Eulerian\npenalization of error in the Euclidean space, the Wasserstein metric can\ncapture translation and difference between the shapes of square-integrable\nprobability distributions of the background state and observations -- enabling\nto formally penalize geophysical biases in state-space with non-Gaussian\ndistributions. The new approach is applied to dissipative and chaotic\nevolutionary dynamics and its potential advantages and limitations are\nhighlighted compared to the classic variational and filtering data assimilation\napproaches under systematic and random errors.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 22:12:50 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 18:42:55 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 18:12:00 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tamang", "Sagar K.", ""], ["Ebtehaj", "Ardeshir", ""], ["Van Leeuwen", "Peter J.", ""], ["Zou", "Dongmian", ""], ["Lerman", "Gilad", ""]]}, {"id": "2009.03449", "submitter": "Weijing Tang", "authors": "Weijing Tang, Kevin He, Gongjun Xu, Ji Zhu", "title": "Survival Analysis via Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general framework for survival analysis based on\nordinary differential equations (ODE). Specifically, this framework unifies\nmany existing survival models, including proportional hazards models, linear\ntransformation models, accelerated failure time models, and time-varying\ncoefficient models as special cases. Such a unified framework provides a novel\nperspective on modeling censored data and offers opportunities for designing\nnew and more flexible survival model structures. Further, the aforementioned\nexisting survival models are traditionally estimated by procedures that suffer\nfrom lack of scalability, statistical inefficiency, or implementation\ndifficulty. Based on well-established numerical solvers and sensitivity\nanalysis tools for ODEs, we propose a novel, scalable, and easy-to-implement\ngeneral estimation procedure that is applicable to a wide range of models. In\nparticular, we develop a sieve maximum likelihood estimator for a general\nsemi-parametric class of ODE models as an illustrative example. We also\nestablish a general sieve M-theorem for bundled parameters and show that the\nproposed sieve estimator is consistent and asymptotically normal, and achieves\nthe semi-parametric efficiency bound. The finite sample performance of the\nproposed estimator is examined in simulation studies and a real-world data\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 22:53:17 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Tang", "Weijing", ""], ["He", "Kevin", ""], ["Xu", "Gongjun", ""], ["Zhu", "Ji", ""]]}, {"id": "2009.03472", "submitter": "Hailin Sang", "authors": "Timothy Fortune and Hailin Sang", "title": "Shannon entropy estimation for linear processes", "comments": "14 pages, accepted by Journal of Risk and Financial Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we estimate the Shannon entropy $S(f) = -\\E[ \\log (f(x))]$ of\na one-sided linear process with probability density function $f(x)$. We employ\nthe integral estimator $S_n(f)$, which utilizes the standard kernel density\nestimator $f_n(x)$ of $f(x)$. We show that $S_n (f)$ converges to $S(f)$ almost\nsurely and in $\\L^2$ under reasonable conditions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 01:09:22 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 04:25:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Fortune", "Timothy", ""], ["Sang", "Hailin", ""]]}, {"id": "2009.03646", "submitter": "Maike Hohberg", "authors": "Maike Hohberg, Francesco Donat, Giampiero Marra and Thomas Kneib", "title": "Beyond unidimensional poverty analysis using distributional copula\n  models for mixed ordered-continuous outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poverty is a multidimensional concept often comprising a monetary outcome and\nother welfare dimensions such as education, subjective well-being or health,\nthat are measured on an ordinal scale. In applied research, multidimensional\npoverty is ubiquitously assessed by studying each poverty dimension\nindependently in univariate regression models or by combining several poverty\ndimensions into a scalar index. This inhibits a thorough analysis of the\npotentially varying interdependence between the poverty dimensions. We propose\na multivariate copula generalized additive model for location, scale and shape\n(copula GAMLSS or distributional copula model) to tackle this challenge. By\nrelating the copula parameter to covariates, we specifically examine if certain\nfactors determine the dependence between poverty dimensions. Furthermore,\nspecifying the full conditional bivariate distribution, allows us to derive\nseveral features such as poverty risks and dependence measures coherently from\none model for different individuals. We demonstrate the approach by studying\ntwo important poverty dimensions: income and education. Since the level of\neducation is measured on an ordinal scale while income is continuous, we extend\nthe bivariate copula GAMLSS to the case of mixed ordered-continuous outcomes.\nThe new model is integrated into the GJRM package in R and applied to data from\nIndonesia. Particular emphasis is given to the spatial variation of the\nincome-education dependence and groups of individuals at risk of being\nsimultaneously poor in both education and income dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 11:31:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hohberg", "Maike", ""], ["Donat", "Francesco", ""], ["Marra", "Giampiero", ""], ["Kneib", "Thomas", ""]]}, {"id": "2009.03699", "submitter": "Joshua Bon", "authors": "Joshua J Bon, Anthony Lee, Christopher Drovandi", "title": "Accelerating sequential Monte Carlo with surrogate likelihoods", "comments": "35 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delayed-acceptance is a technique for reducing computational effort for\nBayesian models with expensive likelihoods. Using a delayed-acceptance kernel\nfor Markov chain Monte Carlo can reduce the number of expensive likelihoods\nevaluations required to approximate a posterior expectation. Delayed-acceptance\nuses a surrogate, or approximate, likelihood to avoid evaluation of the\nexpensive likelihood when possible. Within the sequential Monte Carlo\nframework, we utilise the history of the sampler to adaptively tune the\nsurrogate likelihood to yield better approximations of the expensive\nlikelihood, and use a surrogate first annealing schedule to further increase\ncomputational efficiency. Moreover, we propose a framework for optimising\ncomputation time whilst avoiding particle degeneracy, which encapsulates\nexisting strategies in the literature. Overall, we develop a novel algorithm\nfor computationally efficient SMC with expensive likelihood functions. The\nmethod is applied to static Bayesian models, which we demonstrate on toy and\nreal examples, code for which is available at\nhttps://github.com/bonStats/smcdar.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:41:43 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 23:49:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bon", "Joshua J", ""], ["Lee", "Anthony", ""], ["Drovandi", "Christopher", ""]]}, {"id": "2009.03860", "submitter": "My Phan", "authors": "My Phan, David Arbour, Drew Dimmery, Anup B. Rao", "title": "Designing Transportable Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing a randomized experiment on a source\npopulation to estimate the Average Treatment Effect (ATE) on a target\npopulation. We propose a novel approach which explicitly considers the target\nwhen designing the experiment on the source. Under the covariate shift\nassumption, we design an unbiased importance-weighted estimator for the target\npopulation's ATE. To reduce the variance of our estimator, we design a\ncovariate balance condition (Target Balance) between the treatment and control\ngroups based on the target population. We show that Target Balance achieves a\nhigher variance reduction asymptotically than methods that do not consider the\ntarget population during the design phase. Our experiments illustrate that\nTarget Balance reduces the variance even for small sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:49:15 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:27:45 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 15:48:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Phan", "My", ""], ["Arbour", "David", ""], ["Dimmery", "Drew", ""], ["Rao", "Anup B.", ""]]}, {"id": "2009.03937", "submitter": "Alessandro Morandini", "authors": "Andrea De Simone, Alessandro Morandini", "title": "Nonparametric Density Estimation from Markov Chains", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "SISSA 22/2020/FISI", "categories": "stat.ME cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new nonparametric density estimator inspired by Markov Chains,\nand generalizing the well-known Kernel Density Estimator (KDE). Our estimator\npresents several benefits with respect to the usual ones and can be used\nstraightforwardly as a foundation in all density-based algorithms. We prove the\nconsistency of our estimator and we find it typically outperforms KDE in\nsituations of large sample size and high dimensionality. We also employ our\ndensity estimator to build a local outlier detector, showing very promising\nresults when applied to some realistic datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 18:33:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["De Simone", "Andrea", ""], ["Morandini", "Alessandro", ""]]}, {"id": "2009.04096", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "A Joint MLE Approach to Large-Scale Structured Latent Attribute Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Latent Attribute Models (SLAMs) are a family of discrete latent\nvariable models widely used in education, psychology, and epidemiology to model\nmultivariate categorical data. A SLAM assumes that multiple discrete latent\nattributes explain the dependence of observed variables in a highly structured\nfashion. Usually, the maximum marginal likelihood estimation approach is\nadopted for SLAMs, treating the latent attributes as random effects. The\nincreasing scope of modern assessment data involves large numbers of observed\nvariables and high-dimensional latent attributes. This poses challenges to\nclassical estimation methods and requires new methodology and understanding of\nlatent variable modeling. Motivated by this, we consider the joint maximum\nlikelihood estimation (MLE) approach to SLAMs, treating latent attributes as\nfixed unknown parameters. We investigate estimability, consistency, and\ncomputation in the regime where sample size, number of variables, and number of\nlatent attributes all can diverge. We establish the statistical consistency of\nthe joint MLE and propose efficient algorithms that scale well to large-scale\ndata for several popular SLAMs. Simulation studies demonstrate the superior\nempirical performance of the proposed methods. An application to real data from\nan international educational assessment gives interpretable findings of\ncognitive diagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 04:41:30 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 20:24:30 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 20:34:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "2009.04132", "submitter": "Christophe Reype", "authors": "Christophe Reype (IECL), Antonin Richard (IECL), Madalina Deaconu\n  (IECL), Radu Stoica (IECL)", "title": "Bayesian statistical analysis of hydrogeochemical data using point\n  processes: a new tool for source detection in multicomponent fluid mixtures", "comments": null, "journal-ref": "RING Meeting 2020, Sep 2020, Nancy, France", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydrogeochemical data may be seen as a point cloud in a multi-dimensional\nspace. Each dimension of this space represents a hydrogeochemical parameter\n(i.e. salinity, solute concentration, concentration ratio, isotopic\ncomposition...). While the composition of many geological fluids is controlled\nby mixing between multiple sources, a key question related to hydrogeochemical\ndata set is the detection of the sources. By looking at the hydrogeochemical\ndata as spatial data, this paper presents a new solution to the source\ndetection problem that is based on point processes. Results are shown on\nsimulated and real data from geothermal fluids.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:05:53 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Reype", "Christophe", "", "IECL"], ["Richard", "Antonin", "", "IECL"], ["Deaconu", "Madalina", "", "IECL"], ["Stoica", "Radu", "", "IECL"]]}, {"id": "2009.04136", "submitter": "Wei Ma", "authors": "Li Yang, Wei Ma, Yichen Qin, Feifang Hu", "title": "Testing for Treatment Effect in Covariate-Adaptive Randomized Clinical\n  Trials with Generalized Linear Models and Omitted Covariates", "comments": "Updated to the published version", "journal-ref": "Statistical Methods in Medical Research (2021)", "doi": "10.1177/09622802211008206", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns have been expressed over the validity of statistical inference under\ncovariate-adaptive randomization despite the extensive use in clinical trials.\nIn the literature, the inferential properties under covariate-adaptive\nrandomization have been mainly studied for continuous responses; in particular,\nit is well known that the usual two sample t-test for treatment effect is\ntypically conservative, in the sense that the actual test size is smaller than\nthe nominal level. This phenomenon of invalid tests has also been found for\ngeneralized linear models without adjusting for the covariates and are\nsometimes more worrisome due to inflated Type I error. The purpose of this\nstudy is to examine the unadjusted test for treatment effect under generalized\nlinear models and covariate-adaptive randomization. For a large class of\ncovariate-adaptive randomization methods, we obtain the asymptotic distribution\nof the test statistic under the null hypothesis and derive the conditions under\nwhich the test is conservative, valid, or anti-conservative. Several commonly\nused generalized linear models, such as logistic regression and Poisson\nregression, are discussed in detail. An adjustment method is also proposed to\nachieve a valid size based on the asymptotic results. Numerical studies confirm\nthe theoretical findings and demonstrate the effectiveness of the proposed\nadjustment method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:15:48 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 10:42:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yang", "Li", ""], ["Ma", "Wei", ""], ["Qin", "Yichen", ""], ["Hu", "Feifang", ""]]}, {"id": "2009.04137", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, T. Kypraios, P. D. O'Neill, T. J. Hagenaars", "title": "A Bayesian Nonparametric Analysis of the 2003 Outbreak of Highly\n  Pathogenic Avian Influenza in the Netherlands", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases on farms pose both public and animal health risks, so\nunderstanding how they spread between farms is crucial for developing disease\ncontrol strategies to prevent future outbreaks. We develop novel Bayesian\nnonparametric methodology to fit spatial stochastic transmission models in\nwhich the infection rate between any two farms is a function that depends on\nthe distance between them, but without assuming a specified parametric form.\nMaking nonparametric inference in this context is challenging since the\nlikelihood function of the observed data is intractable because the underlying\ntransmission process is unobserved. We adopt a fully Bayesian approach by\nassigning a transformed Gaussian Process prior distribution to the infection\nrate function, and then develop an efficient data augmentation Markov Chain\nMonte Carlo algorithm to perform Bayesian inference. We use the posterior\npredictive distribution to simulate the effect of different disease control\nmethods and their economic impact. We analyse a large outbreak of Avian\nInfluenza in the Netherlands and infer the between-farm infection rate, as well\nas the unknown infection status of farms which were pre-emptively culled. We\nuse our results to analyse ring-culling strategies, and conclude that although\neffective, ring-culling has limited impact in high density areas.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 07:19:49 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Seymour", "R. G.", ""], ["Kypraios", "T.", ""], ["O'Neill", "P. D.", ""], ["Hagenaars", "T. J.", ""]]}, {"id": "2009.04239", "submitter": "Jon Cockayne", "authors": "Jon Cockayne and Andrew B. Duncan", "title": "Probabilistic Gradients for Fast Calibration of Differential Equation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of large-scale differential equation models to observational or\nexperimental data is a widespread challenge throughout applied sciences and\nengineering. A crucial bottleneck in state-of-the art calibration methods is\nthe calculation of local sensitivities, i.e. derivatives of the loss function\nwith respect to the estimated parameters, which often necessitates several\nnumerical solves of the underlying system of partial or ordinary differential\nequations. In this paper we present a new probabilistic approach to computing\nlocal sensitivities. The proposed method has several advantages over classical\nmethods. Firstly, it operates within a constrained computational budget and\nprovides a probabilistic quantification of uncertainty incurred in the\nsensitivities from this constraint. Secondly, information from previous\nsensitivity estimates can be recycled in subsequent computations, reducing the\noverall computational effort for iterative gradient-based calibration methods.\nThe methodology presented is applied to two challenging test problems and\ncompared against classical methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 10:35:09 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 08:08:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cockayne", "Jon", ""], ["Duncan", "Andrew B.", ""]]}, {"id": "2009.04464", "submitter": "Steven Thompson", "authors": "Steve Thompson", "title": "Estimation for network snowball sampling: Preventing pandemics", "comments": "arXiv admin note: text overlap with arXiv:2002.01350", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snowball designs are the most natural of the network sampling designs. They\nhave many desirable properties for sampling hidden and hard-to reach\npopulations. They have been under-used in recent years because simple\ndesign-based estimators and confidence intervals have not been available for\nthem. The needed estimation methods are supplied in this paper. Snowball\nsampling methods and accurate estimators with them are needed for sampling of\nthe people exposed to the animals from which new coronavirus outbreaks\noriginate, and to sample the animal populations to which they are exposed.\nAccurate estimates are needed to evaluate the effectiveness of interventions to\nreduce the risk to the people exposed to the animals. In this way the\nfrequencies of major outbreaks and pandemics can be reduced. Snowball designs\nare needed in studies of sexual and opioid networks through which HIV can\nspread explosively, so that prevention intervention methods can be developed,\naccurately assessed, and effectively distributed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 19:43:34 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Thompson", "Steve", ""]]}, {"id": "2009.04710", "submitter": "Abhik Ghosh PhD", "authors": "Soumya Chakraborty, Ayanendranath Basu, Abhik Ghosh", "title": "Robust Clustering with Normal Mixture Models: A Pseudo\n  $\\beta$-Likelihood Approach", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As in other estimation scenarios, likelihood based estimation in the normal\nmixture set-up is highly non-robust against model misspecification and presence\nof outliers (apart from being an ill-posed optimization problem). We propose a\nrobust alternative to the ordinary likelihood approach for this estimation\nproblem which performs simultaneous estimation and data clustering and leads to\nsubsequent anomaly detection. To invoke robustness, we follow, in spirit, the\nmethodology based on the minimization of the density power divergence (or\nalternatively, the maximization of the $\\beta$-likelihood) under suitable\nconstraints. An iteratively reweighted least squares approach has been followed\nin order to compute our estimators for the component means (or equivalently\ncluster centers) and component dispersion matrices which leads to simultaneous\ndata clustering. Some exploratory techniques are also suggested for anomaly\ndetection, a problem of great importance in the domain of statistics and\nmachine learning. Existence and consistency of the estimators are established\nunder the aforesaid constraints. We validate our method with simulation studies\nunder different set-ups; it is seen to perform competitively or better compared\nto the popular existing methods like K-means and TCLUST, especially when the\nmixture components (i.e., the clusters) share regions with significant overlap\nor outlying clusters exist with small but non-negligible weights. Two real\ndatasets are also used to illustrate the performance of our method in\ncomparison with others along with an application in image processing. It is\nobserved that our method detects the clusters with lower misclassification\nrates and successfully points out the outlying (anomalous) observations from\nthese datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:50:08 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chakraborty", "Soumya", ""], ["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""]]}, {"id": "2009.04747", "submitter": "Mohammad Ghorbani Dr.", "authors": "Mohammad Ghorbani, Nafiseh Vafaei, Ji\\v{r}\\'i Dvo\\v{r}\\'ak, Mari\n  Myllym\\\"aki", "title": "Testing the first-order separability hypothesis for spatio-temporal\n  point patterns", "comments": "21 pages, 8 Figures (21 plots)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order separability of a spatio-temporal point process plays a\nfundamental role in the analysis of spatio-temporal point pattern data. While\nit is often a convenient assumption that simplifies the analysis greatly,\nexisting non-separable structures should be accounted for in the model\nconstruction. We propose three different tests to investigate this hypothesis\nas a step of preliminary data analysis. The first two tests are exact or\nasymptotically exact for Poisson processes. The first test based on\npermutations and global envelopes allows us to detect at which spatial and\ntemporal locations or lags the data deviate from the null hypothesis. The\nsecond test is a simple and computationally cheap $\\chi^2$-test. The third test\nis based on statistical reconstruction method and can be generally applied for\nnon-Poisson processes. The performance of the first two tests is studied in a\nsimulation study for Poisson and non-Poisson models. The third test is applied\nto the real data of the UK 2001 epidemic foot and mouth disease.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 09:35:16 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ghorbani", "Mohammad", ""], ["Vafaei", "Nafiseh", ""], ["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "2009.04793", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Rong Liu, Wolfgang Karl H\\\"ardle", "title": "Statistical Inference for Generalized Additive Partially Linear Model", "comments": null, "journal-ref": "Journal of Multivariate Analysis, Volume 162, November 2017, Pages\n  1-15", "doi": "10.1016/j.jmva.2017.07.011", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Generalized Additive Model (GAM) is a powerful tool and has been well\nstudied. This model class helps to identify additive regression structure. Via\navailable test procedures one may identify the regression structure even\nsharper if some component functions have parametric form. The Generalized\nAdditive Partially Linear Models (GAPLM) enjoy the simplicity of the GLM and\nthe flexibility of the GAM because they combine both parametric and\nnonparametric components. We use the hybrid spline-backfitted kernel estimation\nmethod, which combines the best features of both spline and kernel methods for\nmaking fast, efficient and reliable estimation under alpha-mixing condition. In\naddition, simultaneous confidence corridors (SCCs) for testing overall trends\nand empirical likelihood confidence region for parameters are provided under\nindependent condition. The asymptotic properties are obtained and simulation\nresults support the theoretical properties. For the application, we use the\nGAPLM to improve the accuracy ratio of the default predictions for $19610$\nGerman companies. The quantlets for this paper are available on\nhttps://github.com.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:51:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Liu", "Rong", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.04795", "submitter": "Federico Castelletti", "authors": "Federico Castelletti and Guido Consonni", "title": "Bayesian causal inference in probit graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary response which is potentially affected by a set of\ncontinuous variables. Of special interest is the causal effect on the response\ndue to an intervention on a specific variable. The latter can be meaningfully\ndetermined on the basis of observational data through suitable assumptions on\nthe data generating mechanism. In particular we assume that the joint\ndistribution obeys the conditional independencies (Markov properties) inherent\nin a Directed Acyclic Graph (DAG), and the DAG is given a causal interpretation\nthrough the notion of interventional distribution. We propose a DAG-probit\nmodel where the response is generated by discretization through a random\nthreshold of a continuous latent variable and the latter, jointly with the\nremaining continuous variables, has a distribution belonging to a zero-mean\nGaussian model whose covariance matrix is constrained to satisfy the Markov\nproperties of the DAG. Our model leads to a natural definition of causal effect\nconditionally on a given DAG. Since the DAG which generates the observations is\nunknown, we present an efficient MCMC algorithm whose target is the posterior\ndistribution on the space of DAGs, the Cholesky parameters of the concentration\nmatrix, and the threshold linking the response to the latent. Our end result is\na Bayesian Model Averaging estimate of the causal effect which incorporates\nparameter, as well as model, uncertainty. The methodology is assessed using\nsimulation experiments and applied to a gene expression data set originating\nfrom breast cancer stem cells.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:55:44 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Castelletti", "Federico", ""], ["Consonni", "Guido", ""]]}, {"id": "2009.04832", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Luke J Keele, Dylan S Small, Marshall M Joffe", "title": "A note on post-treatment selection in studying racial discrimination in\n  policing", "comments": "Accepted for publication in the American Political Science Review on\n  14th June, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss some causal estimands used to study racial discrimination in\npolicing. A central challenge is that not all police-civilian encounters are\nrecorded in administrative datasets and available to researchers. One possible\nsolution is to consider the average causal effect of race conditional on the\ncivilian already being detained by the police. We find that such an estimand\ncan be quite different from the more familiar ones in causal inference and\nneeds to be interpreted with caution. We propose using an estimand new for this\ncontext -- the causal risk ratio, which has more transparent interpretation and\nrequires weaker identification assumptions. We demonstrate this through a\nreanalysis of the NYPD Stop-and-Frisk dataset. Our reanalysis shows that the\nnaive estimator that ignores the post-treatment selection in administrative\nrecords may severely underestimate the disparity in police violence between\nminorities and whites in these and similar data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:17:37 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:26:53 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 21:11:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Keele", "Luke J", ""], ["Small", "Dylan S", ""], ["Joffe", "Marshall M", ""]]}, {"id": "2009.04837", "submitter": "Abhirup Datta", "authors": "Debangan Dey, Abhirup Datta, and Sudipto Banerjee", "title": "Graphical Gaussian Process Models for Highly Multivariate Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multivariate spatial (Gaussian) process models, common cross-covariance\nfunctions do not exploit graphical models to ensure process-level conditional\nindependence among the variables. This is undesirable, especially for highly\nmultivariate settings, where popular cross-covariance functions such as the\nmultivariate Mat\\'ern suffer from a \"curse of dimensionality\" as the number of\nparameters and floating point operations scale up in quadratic and cubic order,\nrespectively, in the number of variables. We propose a class of multivariate\n\"graphical Gaussian Processes\" using a general construction called \"stitching\"\nthat crafts cross-covariance functions from graphs and ensure process-level\nconditional independence among variables. For the Mat\\'ern family of functions,\nstitching yields a multivariate GP whose univariate components are exactly\nMat\\'ern GPs, and conforms to process-level conditional independence as\nspecified by the graphical model. For highly multivariate settings and\ndecomposable graphical models, stitching offers massive computational gains and\nparameter dimension reduction. We demonstrate the utility of the graphical\nMat\\'ern GP to jointly model highly multivariate spatial data using simulation\nexamples and an application to air-pollution modelling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 13:21:40 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 14:37:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Dey", "Debangan", ""], ["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2009.05007", "submitter": "Alessio Farcomeni", "authors": "Alessio Farcomeni, Marco Geraci, and Cinzia Viroli", "title": "Directional quantile classifiers", "comments": "23 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce classifiers based on directional quantiles. We derive\ntheoretical results for selecting optimal quantile levels given a direction,\nand, conversely, an optimal direction given a quantile level. We also show that\nthe misclassification rate is infinitesimal if population distributions differ\nby at most a location shift and if the number of directions is allowed to\ndiverge at the same rate of the problem's dimension. We illustrate the\nsatisfactory performance of our proposed classifiers in both small and high\ndimensional settings via a simulation study and a real data example. The code\nimplementing the proposed methods is publicly available in the R package\nQtools.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 17:15:25 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 07:03:17 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Farcomeni", "Alessio", ""], ["Geraci", "Marco", ""], ["Viroli", "Cinzia", ""]]}, {"id": "2009.05044", "submitter": "Madhuchhanda Bhattacharjee Prof.", "authors": "Madhuchhanda Bhattacharjee and Arup Bose", "title": "Modelling COVID-19 -- I A dynamic SIR(D) with application to Indian data", "comments": "Supplementary file available on request from authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an epidemiological model using an adaptive dynamic three\ncompartment (with four states) SIR(D) model. Our approach is similar to\nnon-parametric curve fitting in spirit and automatically adapts to key external\nfactors, such as interventions, while retaining the parsimonious nature of the\nstandard SIR(D) model. Initial dynamic temporal estimates of the model\nparameters are obtained by minimising the aggregate residual sum of squares\nacross the number of infections, recoveries, and fatalities, over a chosen lag\nperiod. Then a geometric smoother is applied to obtain the final time series of\nestimates. These estimates are used to obtain dynamic temporal robust estimates\nof the key feature of this pandemic, namely the \"reproduction number\". We\nillustrate our method on the Indian COVID-19 data for the period March 14 -\nAugust 31, 2020. The time series data plots of the 36 states and union\nterritories shows a clear presence of inter-regional variation in the prognosis\nof the epidemic. This is also bourne out by the estimates of the underlying\nparameters, including the reproduction numbers for the 36 regions. Due to this,\nan SIR(D) model, dynamic or otherwise, on the national aggregate data is not\nsuited for robust local predictions. The time series of estimates of the model\nenables us to carry out daily, weekly and also long term predictions, including\nconstruction of predictive bands. We obtain an excellent agreement between the\nactual data and the model predicted data at the regional level. Our estimates\nof the current reproduction number turn out to be more than 2 in three regions\n(Andhra Pradesh, Maharashtra and Uttar Pradesh) and between 1.5 and 2 in 13\nregions. Each of these regions have experienced an individual trajectory, which\ntypically involves initial phase of shock(s) followed by a relatively steady\nlower level of the reproduction number.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:26:08 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Bhattacharjee", "Madhuchhanda", ""], ["Bose", "Arup", ""]]}, {"id": "2009.05079", "submitter": "Miheer Dewaskar", "authors": "Miheer Dewaskar, John Palowitch, Mark He, Michael I. Love, Andrew\n  Nobel", "title": "Finding Stable Groups of Cross-Correlated Features in Multi-View data", "comments": "22 pages, 5 figures. R package: https://github.com/miheerdew/cbce", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data, in which data of different types are obtained from a common\nset of samples, is now common in many scientific problems. An important problem\nin the analysis of multi-view data is identifying interactions between groups\nof features from different data types. A bimodule is a pair $(A,B)$ of feature\nsets from two different data types such that the aggregate cross-correlation\nbetween the features in $A$ and those in $B$ is large. A bimodule $(A,B)$ is\nstable if $A$ coincides with the set of features having significant aggregate\ncorrelation with the features in $B$, and vice-versa. At the population level,\nstable bimodules correspond to connected components of the cross-correlation\nnetwork, which is the bipartite graph whose edges are pairs of features with\nnon-zero cross-correlations.\n  We develop an iterative, testing-based procedure, called BSP, to identify\nstable bimodules in two moderate- to high-dimensional data sets. BSP relies on\npermutation-based p-values for sums of squared cross-correlations. We\nefficiently approximate the p-values using tail probabilities of gamma\ndistributions that are fit using analytical estimates of the permutation\nmoments of the test statistic. Our moment estimates depend on the eigenvalues\nof the intra-correlation matrices of $A$ and $B$ and as a result, the\nsignificance of observed cross-correlations accounts for the correlations\nwithin each data type.\n  We carry out a thorough simulation study to assess the performance of BSP,\nand present an extended application of BSP to the problem of expression\nquantitative trait loci (eQTL) analysis using recent data from the GTEx\nproject. In addition, we apply BSP to climatology data in order to identify\nregions in North America where annual temperature variation affects\nprecipitation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 18:27:35 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Dewaskar", "Miheer", ""], ["Palowitch", "John", ""], ["He", "Mark", ""], ["Love", "Michael I.", ""], ["Nobel", "Andrew", ""]]}, {"id": "2009.05098", "submitter": "Sanjeena Subedi", "authors": "Wangshu Tu and Sanjeena Subedi", "title": "A Family of Mixture Models for Biclustering", "comments": "45", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering is used for simultaneous clustering of the observations and\nvariables when there is no group structure known \\textit{a priori}. It is being\nincreasingly used in bioinformatics, text analytics, etc. Previously,\nbiclustering has been introduced in a model-based clustering framework by\nutilizing a structure similar to a mixture of factor analyzers. In such models,\nobserved variables $\\mathbf{X}$ are modelled using a latent variable\n$\\mathbf{U}$ that is assumed to be from $N(\\mathbf{0}, \\mathbf{I})$. Clustering\nof variables is introduced by imposing constraints on the entries of the factor\nloading matrix to be 0 and 1 that results in a block diagonal covariance\nmatrices. However, this approach is overly restrictive as off-diagonal elements\nin the blocks of the covariance matrices can only be 1 which can lead to\nunsatisfactory model fit on complex data. Here, the latent variable\n$\\mathbf{U}$ is assumed to be from a $N(\\mathbf{0}, \\mathbf{T})$ where\n$\\mathbf{T}$ is a diagonal matrix. This ensures that the off-diagonal terms in\nthe block matrices within the covariance matrices are non-zero and not\nrestricted to be 1. This leads to a superior model fit on complex data. A\nfamily of models are developed by imposing constraints on the components of the\ncovariance matrix. For parameter estimation, an alternating expectation\nconditional maximization (AECM) algorithm is used. Finally, the proposed method\nis illustrated using simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:06:35 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Tu", "Wangshu", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2009.05247", "submitter": "Morteza Mohammadi", "authors": "Morteza Mohammadi, Mohammad Amini, and Mahdi Emadi", "title": "A simulation study of semiparametric estimation in copula models based\n  on minimum Alpha-Divergence", "comments": "14 pages", "journal-ref": null, "doi": "10.19139/soic-2310-5070-974", "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to introduce two semiparametric methods for the\nestimation of copula parameter. These methods are based on minimum\nAlpha-Divergence between a non-parametric estimation of copula density using\nlocal likelihood probit transformation method and a true copula density\nfunction. A Monte Carlo study is performed to measure the performance of these\nmethods based on Hellinger distance and Neyman divergence as special cases of\nAlpha-Divergence. Simulation results are compared to the Maximum\nPseudo-Likelihood (MPL) estimation as a conventional estimation method in\nwell-known bivariate copula models. These results show that the proposed method\nbased on Minimum Pseudo Hellinger Distance estimation has a good performance in\nsmall sample size and weak dependency situations. The parameter estimation\nmethods are applied to a real data set in Hydrology.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 06:22:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mohammadi", "Morteza", ""], ["Amini", "Mohammad", ""], ["Emadi", "Mahdi", ""]]}, {"id": "2009.05304", "submitter": "Luca Ganassali", "authors": "M. Akian, L. Ganassali, S. Gaubert, L. Massouli\\'e", "title": "Probabilistic and mean-field model of COVID-19 epidemics with user\n  mobility and contact tracing", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a detailed discrete-time model of COVID-19 epidemics coming in two\nflavours, mean-field and probabilistic. The main contribution lies in several\nextensions of the basic model that capture i) user mobility - distinguishing\nrouting, i.e. change of residence, from commuting, i.e. daily mobility - and\nii) contact tracing procedures. We confront this model to public data on daily\nhospitalizations, and discuss its application as well as underlying estimation\nprocedures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:29:07 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Akian", "M.", ""], ["Ganassali", "L.", ""], ["Gaubert", "S.", ""], ["Massouli\u00e9", "L.", ""]]}, {"id": "2009.05318", "submitter": "Andrew Golightly", "authors": "Andrew Golightly and Chris Sherlock", "title": "Augmented pseudo-marginal Metropolis-Hastings for partially observed\n  diffusion processes", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference for nonlinear, multivariate diffusion\nprocesses, satisfying It\\^o stochastic differential equations (SDEs), using\ndata at discrete times that may be incomplete and subject to measurement error.\nOur starting point is a state-of-the-art correlated pseudo-marginal\nMetropolis-Hastings scheme, that uses correlated particle filters to induce\nstrong and positive correlation between successive marginal likelihood\nestimates. However, unless the measurement error or the dimension of the SDE is\nsmall, correlation can be eroded by the resampling steps in the particle\nfilter. We therefore propose a novel augmentation scheme, that allows for\nconditioning on values of the latent process at the observation times,\ncompletely avoiding the need for resampling steps. We integrate over the\nuncertainty at the observation times with an additional Gibbs step. Connections\nbetween the resulting pseudo-marginal scheme and existing inference schemes for\ndiffusion processes are made. The methodology is applied in three examples of\nincreasing complexity. We find that our approach offers substantial increases\nin overall efficiency, compared to competing methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:02:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Golightly", "Andrew", ""], ["Sherlock", "Chris", ""]]}, {"id": "2009.05417", "submitter": "Antonio P. Ramos", "authors": "Antonio P. Ramos and Martin J. Flores and Leiwen Gao and Patrick\n  Heuveline and Robert E. Weiss", "title": "Explaining the Decline of Child Mortality in 44 Developing Countries: A\n  Bayesian Extension of Oaxaca Decomposition Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the decline of infant mortality in 42 low and middle income\ncountries (LMIC) using detailed micro data from 84 Demographic and Health\nSurveys. We estimate infant mortality risk for each infant in our data and\ndevelop a novel extension of Oaxaca decomposition to understand the sources of\nthese changes. We find that the decline in infant mortality is due to a\ndeclining propensity for parents with given characteristics to experience the\ndeath of an infant rather than due to changes in the distributions of these\ncharacteristics over time. Our results suggest that technical progress and\npolicy health interventions in the form of public goods are the main drivers of\nthe the recent decline in infant mortality in LMIC.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:04:28 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Ramos", "Antonio P.", ""], ["Flores", "Martin J.", ""], ["Gao", "Leiwen", ""], ["Heuveline", "Patrick", ""], ["Weiss", "Robert E.", ""]]}, {"id": "2009.05431", "submitter": "Piotr Fryzlewicz", "authors": "Piotr Fryzlewicz", "title": "Narrowest Significance Pursuit: inference for multiple change-points in\n  linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Narrowest Significance Pursuit (NSP), a general and flexible\nmethodology for automatically detecting localised regions in data sequences\nwhich each must contain a change-point, at a prescribed global significance\nlevel. Here, change-points are understood as abrupt changes in the parameters\nof an underlying linear model. NSP works by fitting the postulated linear model\nover many regions of the data, using a certain multiresolution sup-norm loss,\nand identifying the shortest interval on which the linearity is significantly\nviolated. The procedure then continues recursively to the left and to the right\nuntil no further intervals of significance can be found. The use of the\nmultiresolution sup-norm loss is a key feature of NSP, as it enables the\ntransfer of significance considerations to the domain of the unobserved true\nresiduals, a substantial simplification. It also guarantees important\nstochastic bounds which directly yield exact desired coverage probabilities,\nregardless of the form or number of the regressors.\n  NSP works with a wide range of distributional assumptions on the errors,\nincluding Gaussian with known or unknown variance, some light-tailed\ndistributions, and some heavy-tailed, possibly heterogeneous distributions via\nself-normalisation. It also works in the presence of autoregression. The\nmathematics of NSP is, by construction, uncomplicated, and its key\ncomputational component uses simple linear programming. In contrast to the\nwidely studied \"post-selection inference\" approach, NSP enables the opposite\nviewpoint and paves the way for the concept of \"post-inference selection\".\nPre-CRAN R code implementing NSP is available at https://github.com/pfryz/nsp.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 13:32:23 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:31:00 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Fryzlewicz", "Piotr", ""]]}, {"id": "2009.05482", "submitter": "Vartan Choulakian", "authors": "J. Allard, S. Champigny, V. Choulakian, S. Mahdi", "title": "TCA and TLRA: A comparison on contingency tables and compositional data", "comments": "22 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two popular general approaches for the analysis and visualization\nof a contingency table and a compositional data set: Correspondence analysis\n(CA) and log ratio analysis (LRA). LRA includes two independently well\ndeveloped methods: association models and compositional data analysis. The\napplication of either CA or LRA to a contingency table or to compositional data\nset includes a preprocessing centering step. In CA the centering step is\nmultiplicative, while in LRA it is log bi-additive. A preprocessed matrix is\ndouble-centered, so it is a residuel matrix; which implies that it affects the\nfinal results of the analysis. This paper introduces a novel index named the\nintrinsic measure of the quality of the signs of the residuals (QSR) for the\nchoice of the preprocessing, and consequently of the method. The criterion is\nbased on taxicab singular value decomposition (TSVD) on which the package\nTaxicabCA in R is developed. We present a minimal R script that can be executed\nto obtain the numerical results and the maps in this paper. Three relatively\nsmall sized data sets available freely on the web are used as examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:00:51 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Allard", "J.", ""], ["Champigny", "S.", ""], ["Choulakian", "V.", ""], ["Mahdi", "S.", ""]]}, {"id": "2009.05516", "submitter": "Andreas Groll", "authors": "Alexander Gerharz, Andreas Groll, Gunther Schauberger", "title": "Deducing neighborhoods of classes from a fitted model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays world the request for very complex models for huge data sets is\nrising steadily. The problem with these models is that by raising the\ncomplexity of the models, it gets much harder to interpret them. The growing\nfield of \\emph{interpretable machine learning} tries to make up for the lack of\ninterpretability in these complex (or even blackbox-)models by using specific\ntechniques that can help to understand those models better. In this article a\nnew kind of interpretable machine learning method is presented, which can help\nto understand the partitioning of the feature space into predicted classes in a\nclassification model using quantile shifts. To illustrate in which situations\nthis quantile shift method (QSM) could become beneficial, it is applied to a\ntheoretical medical example and a real data example. Basically, real data\npoints (or specific points of interest) are used and the changes of the\nprediction after slightly raising or decreasing specific features are observed.\nBy comparing the predictions before and after the manipulations, under certain\nconditions the observed changes in the predictions can be interpreted as\nneighborhoods of the classes with regard to the manipulated features.\nChordgraphs are used to visualize the observed changes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 16:35:53 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 09:47:20 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Gerharz", "Alexander", ""], ["Groll", "Andreas", ""], ["Schauberger", "Gunther", ""]]}, {"id": "2009.05641", "submitter": "Xu Shi", "authors": "Xu Shi, Wang Miao, Eric Tchetgen Tchetgen", "title": "A Selective Review of Negative Control Methods in Epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose of Review: Negative controls are a powerful tool to detect and adjust\nfor bias in epidemiological research. This paper introduces negative controls\nto a broader audience and provides guidance on principled design and causal\nanalysis based on a formal negative control framework.\n  Recent Findings: We review and summarize causal and statistical assumptions,\npractical strategies, and validation criteria that can be combined with subject\nmatter knowledge to perform negative control analyses. We also review existing\nstatistical methodologies for detection, reduction, and correction of\nconfounding bias, and briefly discuss recent advances towards nonparametric\nidentification of causal effects in a double negative control design.\n  Summary: There is great potential for valid and accurate causal inference\nleveraging contemporary healthcare data in which negative controls are\nroutinely available. Design and analysis of observational data leveraging\nnegative controls is an area of growing interest in health and social sciences.\nDespite these developments, further effort is needed to disseminate these novel\nmethods to ensure they are adopted by practicing epidemiologists.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 19:51:24 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Shi", "Xu", ""], ["Miao", "Wang", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2009.05642", "submitter": "Paul Parker", "authors": "Paul A. Parker, Scott H. Holan, and Ryan Janicki", "title": "Computationally Efficient Bayesian Unit-Level Models for Non-Gaussian\n  Data Under Informative Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical estimates from survey samples have traditionally been obtained\nvia design-based estimators. In many cases, these estimators tend to work well\nfor quantities such as population totals or means, but can fall short as sample\nsizes become small. In today's \"information age,\" there is a strong demand for\nmore granular estimates. To meet this demand, using a Bayesian\npseudo-likelihood, we propose a computationally efficient unit-level modeling\napproach for non-Gaussian data collected under informative sampling designs.\nSpecifically, we focus on binary and multinomial data. Our approach is both\nmultivariate and multiscale, incorporating spatial dependence at the\narea-level. We illustrate our approach through an empirical simulation study\nand through a motivating application to health insurance estimates using the\nAmerican Community Survey.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 19:53:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""], ["Janicki", "Ryan", ""]]}, {"id": "2009.05863", "submitter": "Bryan Wilder", "authors": "Bryan Wilder, Michael J. Mina, Milind Tambe", "title": "Tracking disease outbreaks from sparse data with Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": "Accepted at AAAI 2021", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic provides new motivation for a classic problem in\nepidemiology: estimating the empirical rate of transmission during an outbreak\n(formally, the time-varying reproduction number) from case counts. While\nstandard methods exist, they work best at coarse-grained national or state\nscales with abundant data, and struggle to accommodate the partial\nobservability and sparse data common at finer scales (e.g., individual schools\nor towns). For example, case counts may be sparse when only a small fraction of\ninfections are caught by a testing program. Or, whether an infected individual\ntests positive may depend on the kind of test and the point in time when they\nare tested. We propose a Bayesian framework which accommodates partial\nobservability in a principled manner. Our model places a Gaussian process prior\nover the unknown reproduction number at each time step and models observations\nsampled from the distribution of a specific testing program. For example, our\nframework can accommodate a variety of kinds of tests (viral RNA, antibody,\nantigen, etc.) and sampling schemes (e.g., longitudinal or cross-sectional\nscreening). Inference in this framework is complicated by the presence of tens\nor hundreds of thousands of discrete latent variables. To address this\nchallenge, we propose an efficient stochastic variational inference method\nwhich relies on a novel gradient estimator for the variational objective.\nExperimental results for an example motivated by COVID-19 show that our method\nproduces an accurate and well-calibrated posterior, while standard methods for\nestimating the reproduction number can fail badly.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 20:37:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wilder", "Bryan", ""], ["Mina", "Michael J.", ""], ["Tambe", "Milind", ""]]}, {"id": "2009.05892", "submitter": "Boyan Duan", "authors": "Boyan Duan, Aaditya Ramdas, Larry Wasserman", "title": "Which Wilcoxon should we use? An interactive rank test and other\n  alternatives", "comments": "36 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical nonparametric tests to compare multiple samples, such as the\nWilcoxon test, are often based on the ranks of observations. We design an\ninteractive rank test called i-Wilcoxon -- an analyst is allowed to adaptively\nguide the algorithm using observed outcomes, covariates, working models and\nprior knowledge -- that guarantees type-I error control using martingales.\nNumerical experiments demonstrate the advantage of (an automated version of)\nour algorithm under heterogeneous treatment effects. The i-Wilcoxon test is\nfirst proposed for two-sample comparison with unpaired data, and then extended\nto paired data, multi-sample comparison, and sequential settings, thus also\nextending the Kruskal-Wallis and Friedman tests. As alternatives, we\nnumerically investigate (non-interactive) covariance-adjusted variants of the\nWilcoxon test, and provide practical recommendations based on the anticipated\npopulation properties of the treatment effects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 01:20:49 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 03:51:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Duan", "Boyan", ""], ["Ramdas", "Aaditya", ""], ["Wasserman", "Larry", ""]]}, {"id": "2009.05894", "submitter": "Rob Hyndman", "authors": "Alexander Dokumentov, Rob J. Hyndman", "title": "STR: Seasonal-Trend Decomposition Using Regression", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new method for decomposing seasonal data: STR (a Seasonal-Trend\ndecomposition using Regression). Unlike other decomposition methods, STR allows\nfor multiple seasonal and cyclic components, covariates, seasonal patterns that\nmay have non-integer periods, and seasonality with complex topology. It can be\nused for time series with any regular time index including hourly, daily,\nweekly, monthly or quarterly data. It is competitive with existing methods when\nthey exist, but tackles many more decomposition problem than other methods\nallow.\n  STR is based on a regularized optimization, and so is somewhat related to\nridge regression. Because it is based on a statistical model, we can easily\ncompute confidence intervals for components, something that is not possible\nwith most existing decomposition methods (such as STL, X-12-ARIMA, SEATS-TRAMO,\netc.).\n  Our model is implemented in the R package stR, so can be applied by anyone to\ntheir own data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 01:41:31 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 01:11:07 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Dokumentov", "Alexander", ""], ["Hyndman", "Rob J.", ""]]}, {"id": "2009.06059", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston", "title": "Functional random effects modeling of brain shape and connectivity", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical framework that jointly models brain shape and\nfunctional connectivity, which are two complex aspects of the brain that have\nbeen classically studied independently. We adopt a Riemannian modeling approach\nto account for the non-Euclidean geometry of the space of shapes and the space\nof connectivity that constrains trajectories of co-variation to be valid\nstatistical estimates. In order to disentangle genetic sources of variability\nfrom those driven by unique environmental factors, we embed a functional random\neffects model in the Riemannian framework. We apply the proposed model to the\nHuman Connectome Project dataset to explore spontaneous co-variation between\nbrain shape and connectivity in young healthy individuals.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:33:25 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:30:50 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""]]}, {"id": "2009.06078", "submitter": "Andreas Groll", "authors": "Tobias Markus Krabel, Thi Ngoc Tien Tran, Andreas Groll, Daniel Horn,\n  Carsten Jentsch", "title": "Random boosting and random^2 forests -- A random tree depth injection\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The induction of additional randomness in parallel and sequential ensemble\nmethods has proven to be worthwhile in many aspects. In this manuscript, we\npropose and examine a novel random tree depth injection approach suitable for\nsequential and parallel tree-based approaches including Boosting and Random\nForests. The resulting methods are called \\emph{Random Boost} and\n\\emph{Random$^2$ Forest}. Both approaches serve as valuable extensions to the\nexisting literature on the gradient boosting framework and random forests. A\nMonte Carlo simulation, in which tree-shaped data sets with different numbers\nof final partitions are built, suggests that there are several scenarios where\n\\emph{Random Boost} and \\emph{Random$^2$ Forest} can improve the prediction\nperformance of conventional hierarchical boosting and random forest approaches.\nThe new algorithms appear to be especially successful in cases where there are\nmerely a few high-order interactions in the generated data. In addition, our\nsimulations suggest that our random tree depth injection approach can improve\ncomputation time by up to 40%, while at the same time the performance losses in\nterms of prediction accuracy turn out to be minor or even negligible in most\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 20:14:50 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Krabel", "Tobias Markus", ""], ["Tran", "Thi Ngoc Tien", ""], ["Groll", "Andreas", ""], ["Horn", "Daniel", ""], ["Jentsch", "Carsten", ""]]}, {"id": "2009.06083", "submitter": "Ying Yuan", "authors": "Liyun Jiang, Lei Nie, Ying Yuan", "title": "Elastic Priors to Dynamically Borrow Information from Historical Data in\n  Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of historical data and real-world evidence holds great potential to\nimprove the efficiency of clinical trials. One major challenge is how to\neffectively borrow information from historical data while maintaining a\nreasonable type I error. We propose the elastic prior approach to address this\nchallenge and achieve dynamic information borrowing. Unlike existing\napproaches, this method proactively controls the behavior of dynamic\ninformation borrowing and type I errors by incorporating a well-known concept\nof clinically meaningful difference through an elastic function, defined as a\nmonotonic function of a congruence measure between historical data and trial\ndata. The elastic function is constructed to satisfy a set of\ninformation-borrowing constraints prespecified by researchers or regulatory\nagencies, such that the prior will borrow information when historical and trial\ndata are congruent, but refrain from information borrowing when historical and\ntrial data are incongruent. In doing so, the elastic prior improves power and\nreduces the risk of data dredging and bias. The elastic prior is information\nborrowing consistent, i.e. asymptotically controls type I and II errors at the\nnominal values when historical data and trial data are not congruent, a unique\ncharacteristics of the elastic prior approach. Our simulation study that\nevaluates the finite sample characteristic confirms that, compared to existing\nmethods, the elastic prior has better type I error control and yields\ncompetitive or higher power.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 20:43:17 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 19:16:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jiang", "Liyun", ""], ["Nie", "Lei", ""], ["Yuan", "Ying", ""]]}, {"id": "2009.06094", "submitter": "Eni Musta", "authors": "Eni Musta and Ingrid Van Keilegom", "title": "A simulation-extrapolation approach for the mixture cure model with\n  mismeasured covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider survival data from a population with cured subjects in the\npresence of mismeasured covariates. We use the mixture cure model to account\nfor the individuals that will never experience the event and at the same time\ndistinguish between the effect of the covariates on the cure probabilities and\non survival times. In particular, for practical applications, it seems of\ninterest to assume a logistic form of the incidence and a Cox proportional\nhazards model for the latency. To correct the estimators for the bias\nintroduced by the measurement error, we use the simex algorithm, which is a\nvery general simulation based method. It essentially estimates this bias by\nintroducing additional error to the data and then recovers bias corrected\nestimators through an extrapolation approach. The estimators are shown to be\nconsistent and asymptotically normally distributed when the true extrapolation\nfunction is known. We investigate their finite sample performance through a\nsimulation study and apply the proposed method to analyse the effect of the\nprostate specific antigen (PSA) on patients with prostate cancer.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 21:58:07 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Musta", "Eni", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2009.06170", "submitter": "Qiaohui Lin", "authors": "Qiaohui Lin, Robert Lunde, Purnamrita Sarkar", "title": "Trading off Accuracy for Speedup: Multiplier Bootstraps for Subgraph\n  Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of multiplier bootstraps for count functionals. We\nconsider bootstrap procedures with linear and quadratic weights. These\ncorrespond to the first and second-order terms of the Hoeffding decomposition\nof the bootstrapped statistic arising from the multiplier bootstrap,\nrespectively. We show that the quadratic bootstrap procedure achieves\nhigher-order correctness for appropriately sparse graphs. The linear bootstrap\nprocedure requires fewer estimated network statistics, leading to improved\naccuracy over its higher-order correct counterpart in sparser regimes. To\nimprove the computational properties of the linear bootstrap further, we\nconsider fast sketching methods to conduct approximate subgraph counting and\nestablish consistency of the resulting bootstrap procedure. We complement our\ntheoretical results with a simulation study and real data analysis and verify\nthat our procedure offers state-of-the-art performance for several functionals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 03:17:10 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:19:56 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 21:51:37 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lin", "Qiaohui", ""], ["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "2009.06183", "submitter": "Andrew Herren", "authors": "Andrew Herren, P. Richard Hahn", "title": "Semi-supervised learning and the question of true versus estimated\n  propensity scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A straightforward application of semi-supervised machine learning to the\nproblem of treatment effect estimation would be to consider data as \"unlabeled\"\nif treatment assignment and covariates are observed but outcomes are\nunobserved. According to this formulation, large unlabeled data sets could be\nused to estimate a high dimensional propensity function and causal inference\nusing a much smaller labeled data set could proceed via weighted estimators\nusing the learned propensity scores. In the limiting case of infinite unlabeled\ndata, one may estimate the high dimensional propensity function exactly.\nHowever, longstanding advice in the causal inference community suggests that\nestimated propensity scores (from labeled data alone) are actually preferable\nto true propensity scores, implying that the unlabeled data is actually useless\nin this context. In this paper we examine this paradox and propose a simple\nprocedure that reconciles the strong intuition that a known propensity\nfunctions should be useful for estimating treatment effects with the previous\nliterature suggesting otherwise. Further, simulation studies suggest that\ndirect regression may be preferable to inverse-propensity weight estimators in\nmany circumstances.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:13:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Herren", "Andrew", ""], ["Hahn", "P. Richard", ""]]}, {"id": "2009.06203", "submitter": "Nima Hejazi", "authors": "Nima S. Hejazi, Kara E. Rudolph, Mark J. van der Laan, Iv\\'an D\\'iaz", "title": "Nonparametric causal mediation analysis for stochastic interventional\n  (in)direct effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis has historically been limited in two important\nways: (i) a focus has traditionally been placed on binary treatments and static\ninterventions, and (ii) direct and indirect effect decompositions have been\npursued that are only identifiable in the absence of intermediate confounders\naffected by treatment. We present a theoretical study of an (in)direct effect\ndecomposition of the population intervention effect, defined by stochastic\ninterventions jointly applied to the treatment and mediators. In contrast to\nexisting proposals, our causal effects can be evaluated regardless of whether a\ntreatment is categorical or continuous and remain well-defined even in the\npresence of intermediate confounders affected by treatment. Our (in)direct\neffects are identifiable without a restrictive assumption on cross-world\ncounterfactual independencies, allowing for substantive conclusions drawn from\nthem to be validated in randomized controlled trials. Beyond the novel effects\nintroduced, we provide a careful study of nonparametric efficiency theory\nrelevant for the construction of flexible, multiply robust estimators of our\n(in)direct effects, while avoiding undue restrictions induced by assuming\nparametric models of nuisance parameter functionals. To complement our\nnonparametric estimation strategy, we introduce inferential techniques for\nconstructing confidence intervals and hypothesis tests, and discuss open source\nsoftware implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 05:13:19 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:11:59 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 07:52:13 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Hejazi", "Nima S.", ""], ["Rudolph", "Kara E.", ""], ["van der Laan", "Mark J.", ""], ["D\u00edaz", "Iv\u00e1n", ""]]}, {"id": "2009.06305", "submitter": "Zhuozhao Zhan", "authors": "Edwin R. van den Heuvel, Osama Almalik, Zhuozhao Zhan", "title": "Simulation Models for Aggregated Data Meta-Analysis: Evaluation of\n  Pooling Effect Sizes and Publication Biases", "comments": "23 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies are commonly used to evaluate the performance of newly\ndeveloped meta-analysis methods. For methodology that is developed for an\naggregated data meta-analysis, researchers often resort to simulation of the\naggregated data directly, instead of simulating individual participant data\nfrom which the aggregated data would be calculated in reality. Clearly,\ndistributional characteristics of the aggregated data statistics may be derived\nfrom distributional assumptions of the underlying individual data, but they are\noften not made explicit in publications. This paper provides the distribution\nof the aggregated data statistics that were derived from a heteroscedastic\nmixed effects model for continuous individual data. As a result, we provide a\nprocedure for directly simulating the aggregated data statistics. We also\ncompare our distributional findings with other simulation approaches of\naggregated data used in literature by describing their theoretical differences\nand by conducting a simulation study for three meta-analysis methods:\nDerSimonian and Laird's pooled estimate and the Trim & Fill and PET-PEESE\nmethod for adjustment of publication bias. We demonstrate that the choices of\nsimulation model for aggregated data may have a relevant impact on (the\nconclusions of) the performance of the meta-analysis method. We recommend the\nuse of multiple aggregated data simulation models for investigation of new\nmethodology to determine sensitivity or otherwise make the individual\nparticipant data model explicit that would lead to the distributional choices\nof the aggregated data statistics used in the simulation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:07:40 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Heuvel", "Edwin R. van den", ""], ["Almalik", "Osama", ""], ["Zhan", "Zhuozhao", ""]]}, {"id": "2009.06383", "submitter": "Rico Krueger", "authors": "Rico Krueger and Michel Bierlaire and Thomas Gasos and Prateek Bansal", "title": "Robust discrete choice models with t-distributed kernel errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferences of robust behavioural and statistical models are insensitive to\noutlying observations resulting from aberrant behaviour, misreporting and\nmisclassification. Standard discrete choice models such as logit and probit\nlack robustness to outliers due to their rigid kernel error distributions. In\nthis paper, we analyse two robust alternatives to the multinomial probit (MNP)\nmodel. The two models belong to the family of robit models whose kernel error\ndistributions are heavy-tailed t-distributions which moderate the influence of\noutlying observations. The first model is the multinomial robit (MNR) model, in\nwhich a generic degrees of freedom parameter controls the heavy-tailedness of\nthe kernel error distribution. The second model, the generalised multinomial\nrobit (Gen-MNR) model, is more flexible than MNR, as it allows for distinct\nheavy-tailedness in each dimension of the kernel error distribution. For both\nmodels, we derive efficient Gibbs sampling schemes, which also allow for a\nstraightforward inclusion of random parameters. In a simulation study, we\nillustrate the excellent finite sample properties of the proposed Bayes\nestimators and show that MNR and Gen-MNR produce more exact elasticity\nestimates if the choice data contain outliers through the lens of the\nnon-robust MNP model. In a case study on transport mode choice behaviour, MNR\nand Gen-MNR outperform MNP by substantial margins in terms of in-sample fit and\nout-of-sample predictive accuracy. We also find that the benefits of the more\nflexible kernel error distributions underlying MNR and Gen-MNR are maintained\nin the presence of random heterogeneity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 12:36:28 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:52:16 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Krueger", "Rico", ""], ["Bierlaire", "Michel", ""], ["Gasos", "Thomas", ""], ["Bansal", "Prateek", ""]]}, {"id": "2009.06444", "submitter": "Debo Cheng", "authors": "Debo Cheng, Jiuyong Li, Lin Liu, Jixue Liu", "title": "Sufficient Dimension Reduction for Average Causal Effect Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a large number of covariates can have a negative impact on the quality\nof causal effect estimation since confounding adjustment becomes unreliable\nwhen the number of covariates is large relative to the samples available.\nPropensity score is a common way to deal with a large covariate set, but the\naccuracy of propensity score estimation (normally done by logistic regression)\nis also challenged by large number of covariates. In this paper, we prove that\na large covariate set can be reduced to a lower dimensional representation\nwhich captures the complete information for adjustment in causal effect\nestimation. The theoretical result enables effective data-driven algorithms for\ncausal effect estimation. We develop an algorithm which employs a supervised\nkernel dimension reduction method to search for a lower dimensional\nrepresentation for the original covariates, and then utilizes nearest neighbor\nmatching in the reduced covariate space to impute the counterfactual outcomes\nto avoid large-sized covariate set problem. The proposed algorithm is evaluated\non two semi-synthetic and three real-world datasets and the results have\ndemonstrated the effectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 13:58:57 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cheng", "Debo", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Liu", "Jixue", ""]]}, {"id": "2009.06460", "submitter": "Giorgio Paulon", "authors": "Giorgio Paulon, Peter M\\\"uller, Victor G. Sal Y Rosas", "title": "Bayesian Nonparametric Bivariate Survival Regression for Current Status\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric inference for event time distributions based on\ncurrent status data. We show that in this scenario conventional mixture priors,\nincluding the popular Dirichlet process mixture prior, lead to biologically\nuninterpretable results as they unnaturally skew the probability mass for the\nevent times toward the extremes of the observed data. Simple assumptions on\ndependent censoring can fix the problem. We then extend the discussion to\nbivariate current status data with partial ordering of the two outcomes. In\naddition to dependent censoring, we also exploit some minimal known structure\nrelating the two event times. We design a Markov chain Monte Carlo algorithm\nfor posterior simulation. Applied to a recurrent infection study, the method\nprovides novel insights into how symptoms-related hospital visits are affected\nby covariates.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:18:51 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 15:24:02 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Paulon", "Giorgio", ""], ["M\u00fcller", "Peter", ""], ["Rosas", "Victor G. Sal Y", ""]]}, {"id": "2009.06472", "submitter": "Alberto Caron", "authors": "Alberto Caron, Gianluca Baio and Ioanna Manolopoulou", "title": "Estimating Individual Treatment Effects using Non-Parametric Regression\n  Models: a Review", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large observational data are increasingly available in disciplines such as\nhealth, economic and social sciences, where researchers are interested in\ncausal questions rather than prediction. In this paper, we investigate the\nproblem of estimating heterogeneous treatment effects using non-parametric\nregression-based methods. Firstly, we introduce the setup and the issues\nrelated to conducting causal inference with observational or non-fully\nrandomized data, and how these issues can be tackled with the help of\nstatistical learning tools. Then, we provide a review of state-of-the-art\nmethods, with a particular focus on non-parametric modeling, and we cast them\nunder a unifying taxonomy. After presenting a brief overview on the problem of\nmodel selection, we illustrate the performance of some of the methods on three\ndifferent simulated studies and on a real world example to investigate the\neffect of participation in school meal programs on health indicators.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:26:55 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 10:14:48 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 20:50:50 GMT"}, {"version": "v4", "created": "Sun, 25 Apr 2021 23:29:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Caron", "Alberto", ""], ["Baio", "Gianluca", ""], ["Manolopoulou", "Ioanna", ""]]}, {"id": "2009.06501", "submitter": "Caterina May", "authors": "Jesus Lopez-Fidalgo, Caterina May, Jose Antonio Moler", "title": "Designing experiments for estimating an appropriate outlet size for a\n  silo type problem", "comments": "11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of jam formation during the discharge by gravity of granular\nmaterial through a two-dimensional silo has a number of practical applications.\nIn many problems the estimation of the minimum outlet size which guarantees\nthat the time to the next jamming event is long enough is crucial. Assuming\nthat the time is modeled by an exponential distribution with two unknown\nparameters, this goal translates to the optimal estimation of a non-linear\ntransformation of the parameters. We obtain $c$-optimum experimental designs\nwith that purpose, applying the graphic Elfving method. Since the optimal\ndesigns depend on the nominal values of the parameters, a sensitivity study is\nadditionally provided. Finally, a simulation study checks the performance of\nthe approximations made, first with the Fisher Information matrix, then with\nthe linearization of the function to be estimated. The results are useful for\nexperimenting in a laboratory and translating then the results to a larger\nscenario. Apart from the application a general methodology is developed in the\npaper for the problem of precise estimation of a one-dimensional parametric\ntransformation in a non-linear model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:04:54 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lopez-Fidalgo", "Jesus", ""], ["May", "Caterina", ""], ["Moler", "Jose Antonio", ""]]}, {"id": "2009.06570", "submitter": "Guy Tchuente", "authors": "Alexander Klein and Guy Tchuente", "title": "Spatial Differencing for Sample Selection Models with Unobserved\n  Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper derives identification, estimation, and inference results using\nspatial differencing in sample selection models with unobserved heterogeneity.\nWe show that under the assumption of smooth changes across space of the\nunobserved sub-location specific heterogeneities and inverse Mills ratio, key\nparameters of a sample selection model are identified. The smoothness of the\nsub-location specific heterogeneities implies a correlation in the outcomes. We\nassume that the correlation is restricted within a location or cluster and\nderive asymptotic results showing that as the number of independent clusters\nincreases, the estimators are consistent and asymptotically normal. We also\npropose a formula for standard error estimation. A Monte-Carlo experiment\nillustrates the small sample properties of our estimator. The application of\nour procedure to estimate the determinants of the municipality tax rate in\nFinland shows the importance of accounting for unobserved heterogeneity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:57:48 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Klein", "Alexander", ""], ["Tchuente", "Guy", ""]]}, {"id": "2009.06615", "submitter": "Ales Zahorski", "authors": "Ales Zahorski", "title": "Multilevel regression with poststratification for the national level\n  Viber/Street poll on the 2020 presidential election in Belarus", "comments": "45 pages, 23 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent sociological polls are forbidden in Belarus. Online polls\nperformed without sound scientific rigour do not yield representative results.\nYet, both inside and outside Belarus it is of great importance to obtain\nprecise estimates of the ratings of all candidates. These ratings could\nfunction as reliable proxies for the election's outcomes. We conduct an\nindependent poll based on the combination of the data collected via Viber and\non the streets of Belarus. The Viber and the street data samples consist of\nalmost 45000 and 1150 unique observations respectively. Bayesian regressions\nwith poststratification were build to estimate ratings of the candidates and\nrates of early voting turnout for the population as a whole and within various\nfocus subgroups. We show that both the officially announced results of the\nelection and early voting rates are highly improbable. With a probability of at\nleast 95%, Sviatlana Tikhanouskaya's rating lies between 75% and 80%, whereas\nAliaksandr Lukashenka's rating lies between 13% and 18% and early voting rate\npredicted by the method ranges from 9% to 13% of those who took part in the\nelection. These results contradict the officially announced outcomes, which are\n10.12%, 80.11%, and 49.54% respectively and lie far outside even the 99.9%\ncredible intervals predicted by our model. The only marginal groups of people\nwhere the upper bounds of the 99.9% credible intervals of the rating of\nLukashenka are above 50% are people older than 60 and uneducated people. For\nall other marginal subgroups, including rural residents, even the upper bounds\nof 99.9% credible intervals for Lukashenka are far below 50%. The same is true\nfor the population as a whole. Thus, with a probability of at least 99.9%\nLukashenka could not have had enough electoral support to win the 2020\npresidential election in Belarus.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:55:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zahorski", "Ales", ""]]}, {"id": "2009.06621", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "The Frisch--Waugh--Lovell Theorem for Standard Errors", "comments": null, "journal-ref": "Statistics and Probability Letters, 2020", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frisch--Waugh--Lovell Theorem states the equivalence of the coefficients\nfrom the full and partial regressions. I further show the equivalence between\nvarious standard errors. Applying the new result to stratified experiments\nreveals the discrepancy between model-based and design-based standard errors.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:59:17 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "2009.06670", "submitter": "Lawrence Bardwell", "authors": "Alexander T. M. Fisch, Lawrence Bardwell and Idris A. Eckley", "title": "Real Time Anomaly Detection And Categorisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to quickly and accurately detect anomalous structure within data\nsequences is an inference challenge of growing importance. This work extends\nrecently proposed post-hoc (offline) anomaly detection methodology to the\nsequential setting. The resultant procedure is capable of real-time analysis\nand categorisation between baseline and two forms of anomalous structure: point\nand collective anomalies. Various theoretical properties of the procedure are\nderived. These, together with an extensive simulation study, highlight that the\naverage run length to false alarm and the average detection delay of the\nproposed online algorithm are very close to that of the offline version.\nExperiments on simulated and real data are provided to demonstrate the benefits\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 18:11:19 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Bardwell", "Lawrence", ""], ["Eckley", "Idris A.", ""]]}, {"id": "2009.06699", "submitter": "Kathrin M\\\"ollenhoff", "authors": "Kathrin M\\\"ollenhoff, Achim Tresch", "title": "Survival analysis under non-proportional hazards: investigating\n  non-inferiority or equivalence in time-to-event data", "comments": "Supplemental Material available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to analyze time-to-event data, e.g. in clinical\ntrials, is to fit Kaplan-Meier curves yielding the treatment effect as the\nhazard ratio between treatment groups. Afterwards commonly a log-rank test is\nperformed in order to investigate whether there is a difference in survival,\nor, depending on additional covariates, a Cox proportional hazard model is\nused. However, in numerous trials these approaches fail due to the presence of\nnon-proportional hazards, resulting in difficulties of interpreting the hazard\nratio and a loss of power. When considering equivalence or non-inferiority\ntrials, the commonly performed log-rank based tests are similarly affected by a\nviolation of this assumption. Here we propose a parametric framework to assess\nequivalence or non-inferiority for survival data. We derive pointwise\nconfidence bands for both, the hazard ratio and the difference of the survival\ncurves. Further we propose a test procedure addressing non-inferiority and\nequivalence by directly comparing the survival functions at certain time points\nor over an entire range of time. We demonstrate the validity of the methods by\na clinical trial example and by numerous simulation results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 19:19:00 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["M\u00f6llenhoff", "Kathrin", ""], ["Tresch", "Achim", ""]]}, {"id": "2009.06828", "submitter": "Zhixuan Chu", "authors": "Zhixuan Chu, Stephen L. Rathbun, and Sheng Li", "title": "Matching in Selective and Balanced Representation Space for Treatment\n  Effects Estimation", "comments": "Proceedings of the 29th ACM International Conference on Information\n  and Knowledge Management (CIKM '20)", "journal-ref": null, "doi": "10.1145/3340531.3412037", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dramatically growing availability of observational data is being\nwitnessed in various domains of science and technology, which facilitates the\nstudy of causal inference. However, estimating treatment effects from\nobservational data is faced with two major challenges, missing counterfactual\noutcomes and treatment selection bias. Matching methods are among the most\nwidely used and fundamental approaches to estimating treatment effects, but\nexisting matching methods have poor performance when facing data with high\ndimensional and complicated variables. We propose a feature selection\nrepresentation matching (FSRM) method based on deep representation learning and\nmatching, which maps the original covariate space into a selective, nonlinear,\nand balanced representation space, and then conducts matching in the learned\nrepresentation space. FSRM adopts deep feature selection to minimize the\ninfluence of irrelevant variables for estimating treatment effects and\nincorporates a regularizer based on the Wasserstein distance to learn balanced\nrepresentations. We evaluate the performance of our FSRM method on three\ndatasets, and the results demonstrate superiority over the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 02:07:34 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 09:12:23 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chu", "Zhixuan", ""], ["Rathbun", "Stephen L.", ""], ["Li", "Sheng", ""]]}, {"id": "2009.06865", "submitter": "David Dewhurst", "authors": "David Rushing Dewhurst", "title": "Structural time series grammar over variable blocks", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A structural time series model additively decomposes into generative,\nsemantically-meaningful components, each of which depends on a vector of\nparameters. We demonstrate that considering each generative component together\nwith its vector of parameters as a single latent structural time series node\ncan simplify reasoning about collections of structural time series components.\nWe then introduce a formal grammar over structural time series nodes and\nparameter vectors. Valid sentences in the grammar can be interpreted as\ngenerative structural time series models. An extension of the grammar can also\nexpress structural time series models that include changepoints, though these\nmodels are necessarily not generative. We demonstrate a preliminary\nimplementation of the language generated by this grammar. We close with a\ndiscussion of possible future work.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 04:45:02 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Dewhurst", "David Rushing", ""]]}, {"id": "2009.06935", "submitter": "Pallavi Basu", "authors": "Pallavi Basu and Dylan S. Small", "title": "Constructing a More Closely Matched Control Group in a\n  Difference-in-Differences Analysis: Its Effect on History Interacting with\n  Group Bias", "comments": "Accepted to Observational Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences analysis with a control group that differs\nconsiderably from a treated group is vulnerable to bias from historical events\nthat have different effects on the groups. Constructing a more closely matched\ncontrol group by matching a subset of the overall control group to the treated\ngroup may result in less bias. We study this phenomenon in simulation studies.\nWe study the effect of mountaintop removal mining (MRM) on mortality using a\ndifference-in-differences analysis that makes use of the increase in MRM\nfollowing the 1990 Clean Air Act Amendments. For a difference-in-differences\nanalysis of the effect of MRM on mortality, we constructed a more closely\nmatched control group and found a 95\\% confidence interval that contains\nsubstantial adverse effects along with no effect and small beneficial effects.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 09:08:31 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Basu", "Pallavi", ""], ["Small", "Dylan S.", ""]]}, {"id": "2009.07055", "submitter": "Zheng Zhang", "authors": "Xiaohong Chen, Ying Liu, Shujie Ma, Zheng Zhang", "title": "Efficient Estimation of General Treatment Effects using Neural Networks\n  with A Diverging Number of Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of causal effects is a primary goal of behavioral, social,\neconomic and biomedical sciences. Under the unconfounded treatment assignment\ncondition, adjustment for confounders requires estimating the nuisance\nfunctions relating outcome and/or treatment to confounders. The conventional\napproaches rely on either a parametric or a nonparametric modeling strategy to\napproximate the nuisance functions. Parametric methods can introduce serious\nbias into casual effect estimation due to possible mis-specification, while\nnonparametric estimation suffers from the \"curse of dimensionality\". This paper\nproposes a new unified approach for efficient estimation of treatment effects\nusing feedforward artificial neural networks when the number of covariates is\nallowed to increase with the sample size. We consider a general optimization\nframework that includes the average, quantile and asymmetric least squares\ntreatment effects as special cases. Under this unified setup, we develop a\ngeneralized optimization estimator for the treatment effect with the nuisance\nfunction estimated by neural networks. We further establish the consistency and\nasymptotic normality of the proposed estimator and show that it attains the\nsemiparametric efficiency bound. The proposed methods are illustrated via\nsimulation studies and a real data application.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:07:24 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 02:38:23 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 03:56:02 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chen", "Xiaohong", ""], ["Liu", "Ying", ""], ["Ma", "Shujie", ""], ["Zhang", "Zheng", ""]]}, {"id": "2009.07312", "submitter": "Florian Heinrichs", "authors": "Axel B\\\"ucher, Holger Dette, Florian Heinrichs", "title": "A Portmanteau-type test for detecting serial correlation in locally\n  stationary functional time series", "comments": "Keywords: Autocovariance operator, Block multiplier bootstrap,\n  Functional white noise, Time domain test", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Portmanteau test provides the vanilla method for detecting serial\ncorrelations in classical univariate time series analysis. The method is\nextended to the case of observations from a locally stationary functional time\nseries. Asymptotic critical values are obtained by a suitable block multiplier\nbootstrap procedure. The test is shown to asymptotically hold its level and to\nbe consistent against general alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 18:25:02 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Dette", "Holger", ""], ["Heinrichs", "Florian", ""]]}, {"id": "2009.07427", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Lingxuan Shao and Fang Yao", "title": "Intrinsic Riemannian Functional Data Analysis for Sparse Longitudinal\n  Observations", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel framework is developed to intrinsically analyze sparsely observed\nRiemannian functional data. It features four innovative components: a\nframe-independent covariance function, a smooth vector bundle termed covariance\nvector bundle, a parallel transport and a smooth bundle metric on the\ncovariance vector bundle. The introduced intrinsic covariance function links\nestimation of covariance structure to smoothing problems that involve raw\ncovariance observations derived from sparsely observed Riemannian functional\ndata, while the covariance vector bundle provides a rigorous mathematical\nfoundation for formulating the smoothing problems. The parallel transport and\nthe bundle metric together make it possible to measure fidelity of fit to the\ncovariance function. They also plays a critical role in quantifying the quality\nof estimators for the covariance function. As an illustration, based on the\nproposed framework, we develop a local linear smoothing estimator for the\ncovariance function, analyze its theoretical properties, and provide numerical\ndemonstration via simulated and real datasets. The intrinsic feature of the\nframework makes it applicable to not only Euclidean submanifolds but also\nmanifolds without a canonical ambient space.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 02:29:07 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lin", "Zhenhua", ""], ["Shao", "Lingxuan", ""], ["Yao", "Fang", ""]]}, {"id": "2009.07444", "submitter": "Shihao Yang", "authors": "Shihao Yang, Samuel W. K. Wong, S. C. Kou", "title": "Inference of dynamic systems from noisy and sparse data via\n  manifold-constrained Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for nonlinear dynamic system models, represented by\nordinary differential equations (ODEs), using noisy and sparse data is a vital\ntask in many fields. We propose a fast and accurate method, MAGI\n(MAnifold-constrained Gaussian process Inference), for this task. MAGI uses a\nGaussian process model over time-series data, explicitly conditioned on the\nmanifold constraint that derivatives of the Gaussian process must satisfy the\nODE system. By doing so, we completely bypass the need for numerical\nintegration and achieve substantial savings in computational time. MAGI is also\nsuitable for inference with unobserved system components, which often occur in\nreal experiments. MAGI is distinct from existing approaches as we provide a\nprincipled statistical construction under a Bayesian framework, which\nincorporates the ODE system through the manifold constraint. We demonstrate the\naccuracy and speed of MAGI using realistic examples based on physical\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 03:13:59 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 02:05:17 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 00:13:56 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yang", "Shihao", ""], ["Wong", "Samuel W. K.", ""], ["Kou", "S. C.", ""]]}, {"id": "2009.07464", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Nishika Ranathunga", "title": "Confidence intervals in general regression models that utilize uncertain\n  prior information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general regression model, without a scale parameter. Our aim is\nto construct a confidence interval for a scalar parameter of interest $\\theta$\nthat utilizes the uncertain prior information that a distinct scalar parameter\n$\\tau$ takes the specified value $t$. This confidence interval should have good\ncoverage properties. It should also have scaled expected length, where the\nscaling is with respect to the usual confidence interval, that (a) is\nsubstantially less than 1 when the prior information is correct, (b) has a\nmaximum value that is not too large and (c) is close to 1 when the data and\nprior information are highly discordant. The asymptotic joint distribution of\nthe maximum likelihood estimators $\\theta$ and $\\tau$ is similar to the joint\ndistributions of these estimators in the particular case of a linear regression\nwith normally distributed errors having known variance. This similarity is used\nto construct a confidence interval with the desired properties by using the\nconfidence interval, computed using the R package ciuupi, that utilizes the\nuncertain prior information in this particular linear regression case. An\nimportant practical application of this confidence interval is to a quantal\nbioassay carried out to compare two similar compounds. In this context, the\nuncertain prior information is that the hypothesis of \"parallelism\" holds. We\nprovide extensive numerical results that illustrate the properties of this\nconfidence interval in this context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:31:28 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Kabaila", "Paul", ""], ["Ranathunga", "Nishika", ""]]}, {"id": "2009.07471", "submitter": "Angus Lewis", "authors": "Angus Lewis, Nigel Bean, Giang Nguyen", "title": "Bayesian estimation of trend components within Markovian\n  regime-switching models for wholesale electricity prices: an application to\n  the South Australian wholesale electricity market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss and extend methods for estimating Markovian-Regime-Switching (MRS)\nand trend models for wholesale electricity prices. We argue the existing\nmethods of trend estimation used in the electricity price modelling literature\neither require an ambiguous definition of an extreme price, or lead to issues\nwhen implementing model selection [23]. The first main contribution of this\npaper is to design and infer a model which has a model-based definition of\nextreme prices and permits the use of model selection criteria. Due to the\ncomplexity of the MRS models inference is not straightforward. In the existing\nliterature an approximate EM algorithm is used [26]. Another contribution of\nthis paper is to implement exact inference in a Bayesian setting. This also\nallows the use of posterior predictive checks to assess model fit. We\ndemonstrate the methodologies with South Australian electricity market.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 04:49:47 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lewis", "Angus", ""], ["Bean", "Nigel", ""], ["Nguyen", "Giang", ""]]}, {"id": "2009.07551", "submitter": "Masayuki Sawada", "authors": "Takuya Ishihara and Masayuki Sawada", "title": "Manipulation-Robust Regression Discontinuity Designs", "comments": "This work has been circulated as \"Harmless and Detectable\n  Manipulations of the Running Variable in Regression Discontinuity Designs:\n  Tests and Bounds.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression discontinuity (RD) design in a practical context is often\ncontaminated by units' behavior to manipulate their treatment assignment.\nHowever, we have no formal justification for point identification in such a\ncontaminated RD design. Diagnostic tests have been proposed to detect\nmanipulations, but they do not guarantee identification without some auxiliary\nassumptions, and the auxiliary assumptions have not been proposed. This study\nproposes a set of restrictions for possibly manipulated RD designs to validate\npoint identification and diagnostic tests. The same restrictions simultaneously\nvalidate worst-case bounds when the diagnostic tests are validated. Therefore,\nour designs are manipulation robust in testing and identification. The\nworst-case bounds have two shorter bounds as special cases, and we apply\nspecial-case bounds to a controversy regarding the incumbency margin study of\nthe U.S. House of Representatives elections studied in Lee (2008).\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 08:42:03 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:37:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ishihara", "Takuya", ""], ["Sawada", "Masayuki", ""]]}, {"id": "2009.07634", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Sayar Karmakar", "title": "Time-varying auto-regressive models for count time-series", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.02281,\n  text overlap with arXiv:2009.06007", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count-valued time series data are routinely collected in many application\nareas. We are particularly motivated to study the count time series of daily\nnew cases, arising from COVID-19 spread. We propose two Bayesian models, a\ntime-varying semiparametric AR(p) model for count and then a time-varying\nINGARCH model considering the rapid changes in the spread. We calculate\nposterior contraction rates of the proposed Bayesian methods with respect to\naverage Hellinger metric. Our proposed structures of the models are amenable to\nHamiltonian Monte Carlo (HMC) sampling for efficient computation. We\nsubstantiate our methods by simulations that show superiority compared to some\nof the close existing methods. Finally we analyze the daily time series data of\nnewly confirmed cases to study its spread through different government\ninterventions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 21:50:49 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 16:09:21 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Roy", "Arkaprava", ""], ["Karmakar", "Sayar", ""]]}, {"id": "2009.07662", "submitter": "Matthieu Marbac", "authors": "Marie Du Roy de Chaumaray and Matthieu Marbac", "title": "Clustering Data with Nonignorable Missingness using Semi-Parametric\n  Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We are concerned in clustering continuous data sets subject to non-ignorable\nmissingness. We perform clustering with a specific semi-parametric mixture,\nunder the assumption of conditional independence given the component. The\nmixture model isused for clustering and not for estimating the density of the\nfull variables (observed and unobserved), thus we do not need other assumptions\non the component distribution neither to specify the missingness mechanism.\nEstimation is performed by maximizing an extension of smoothed likelihood\nallowing missingness. This optimization is achieved by a\nMajorization-Minorization algorithm. We illustrate the relevance of our\napproach by numerical experiments. Under mild assumptions, we show the\nidentifiability of the model defining the distribution of the observed data and\nthe monotony of the algorithm. We also propose an extension of this new method\nto the case of mixed-type data that we illustrate on a real data set.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:01:09 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 15:03:08 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 19:49:46 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["de Chaumaray", "Marie Du Roy", ""], ["Marbac", "Matthieu", ""]]}, {"id": "2009.07703", "submitter": "Hang Yu", "authors": "Hang Yu, Songwei Wu, and Justin Dauwels", "title": "Efficient Variational Bayesian Structure Learning of Dynamic Graphical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating time-varying graphical models are of paramount importance in\nvarious social, financial, biological, and engineering systems, since the\nevolution of such networks can be utilized for example to spot trends, detect\nanomalies, predict vulnerability, and evaluate the impact of interventions.\nExisting methods require extensive tuning of parameters that control the graph\nsparsity and temporal smoothness. Furthermore, these methods are\ncomputationally burdensome with time complexity O(NP^3) for P variables and N\ntime points. As a remedy, we propose a low-complexity tuning-free Bayesian\napproach, named BADGE. Specifically, we impose temporally-dependent\nspike-and-slab priors on the graphs such that they are sparse and varying\nsmoothly across time. A variational inference algorithm is then derived to\nlearn the graph structures from the data automatically. Owning to the\npseudo-likelihood and the mean-field approximation, the time complexity of\nBADGE is only O(NP^2). Additionally, by identifying the frequency-domain\nresemblance to the time-varying graphical models, we show that BADGE can be\nextended to learning frequency-varying inverse spectral density matrices, and\nyields graphical models for multivariate stationary time series. Numerical\nresults on both synthetic and real data show that that BADGE can better recover\nthe underlying true graphs, while being more efficient than the existing\nmethods, especially for high-dimensional cases.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:19:23 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:59:08 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yu", "Hang", ""], ["Wu", "Songwei", ""], ["Dauwels", "Justin", ""]]}, {"id": "2009.07745", "submitter": "Marina Vannucci", "authors": "Cheng-Han Yu, Meng Li, Colin Noe, Simon Fischer-Baum and Marina\n  Vannucci", "title": "Bayesian Inference for Stationary Points in Gaussian Process Regression\n  Models for Event-Related Potentials Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary points embedded in the derivatives are often critical for a model\nto be interpretable and may be considered as key features of interest in many\napplications. We propose a semiparametric Bayesian model to efficiently infer\nthe locations of stationary points of a nonparametric function, while treating\nthe function itself as a nuisance parameter. We use Gaussian processes as a\nflexible prior for the underlying function and impose derivative constraints to\ncontrol the function's shape via conditioning. We develop an inferential\nstrategy that intentionally restricts estimation to the case of at least one\nstationary point, bypassing possible mis-specifications in the number of\nstationary points and avoiding the varying dimension problem that often brings\nin computational complexity. We illustrate the proposed methods using\nsimulations and then apply the method to the estimation of event-related\npotentials (ERP) derived from electroencephalography (EEG) signals. We show how\nthe proposed method automatically identifies characteristic components and\ntheir latencies at the individual level, which avoids the excessive averaging\nacross subjects which is routinely done in the field to obtain smooth curves.\nBy applying this approach to EEG data collected from younger and older adults\nduring a speech perception task, we are able to demonstrate how the time course\nof speech perception processes changes with age.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:24:28 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 18:09:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yu", "Cheng-Han", ""], ["Li", "Meng", ""], ["Noe", "Colin", ""], ["Fischer-Baum", "Simon", ""], ["Vannucci", "Marina", ""]]}, {"id": "2009.07782", "submitter": "Leonhard Held", "authors": "Leonhard Held, Charlotte Micheloud and Samuel Pawel", "title": "The assessment of replication success based on relative effect size", "comments": "revision, 16 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication studies are increasingly conducted in order to confirm original\nfindings. However, there is no established standard how to assess replication\nsuccess and in practice many different approaches are used. The purpose of this\npaper is to refine and extend a recently proposed reverse-Bayes approach for\nthe analysis of replication studies. We show how this method is directly\nrelated to the relative effect size, the ratio of the replication to the\noriginal effect estimate. This perspective leads to a new proposal to\nrecalibrate the assessment of replication success, the golden level. The\nrecalibration ensures that for borderline significant original studies\nreplication success can only be achieved if the replication effect estimate is\nlarger than the original one. Conditional power for replication success can\nthen take any desired value if the original study is significant and the\nreplication sample size is large enough. Compared to the standard approach to\nrequire statistical significance of both the original and replication study,\nreplication success at the golden level offers uniform gains in project power\nand controls the Type-I error rate if the replication sample size is not\nsmaller than the original one. An application to data from four large\nreplication projects shows that the new approach leads to more appropriate\ninferences, as it penalizes shrinkage of the replication estimate compared to\nthe original one, while ensuring that both effect estimates are sufficiently\nconvincing on their own.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:20:16 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 13:40:35 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 07:22:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Held", "Leonhard", ""], ["Micheloud", "Charlotte", ""], ["Pawel", "Samuel", ""]]}, {"id": "2009.07875", "submitter": "Jie Zhou", "authors": "Jie Zhou, Xun Jiang, H. Amy Xia, Peng Wei, Brian P. Hobbs", "title": "A Survival Mediation Model with Bayesian Model Averaging", "comments": "25 pages, 3 figures and 3 tables in the main manuscript.\n  Supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the extent to which a patient is benefiting from cancer therapy\nis challenging. Criteria for quantifying the extent of \"tumor response\"\nobserved within a few cycles of treatment have been established for various\ntypes of solid as well as hematologic malignancies. These measures comprise the\nprimary endpoints of phase II trials. Regulatory approvals of new cancer\ntherapies, however, are usually contingent upon the demonstration of superior\noverall survival with randomized evidence acquired with a phase III trial\ncomparing the novel therapy to an appropriate standard of care treatment. With\nnearly two thirds of phase III oncology trials failing to achieve statistically\nsignificant results, researchers continue to refine and propose new surrogate\nendpoints. This article presents a Bayesian framework for studying\nrelationships among treatment, patient subgroups, tumor response and survival.\nCombining classical components of mediation analysis with Bayesian model\naveraging (BMA), the methodology is robust to model mis-specification among\nvarious possible relationships among the observable entities. Posterior\ninference is demonstrated via application to a randomized controlled phase III\ntrial in metastatic colorectal cancer. Moreover, the article details posterior\npredictive distributions of survival and statistical metrics for quantifying\nthe extent of direct and indirect, or tumor response mediated, treatment\neffects.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 18:09:26 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhou", "Jie", ""], ["Jiang", "Xun", ""], ["Xia", "H. Amy", ""], ["Wei", "Peng", ""], ["Hobbs", "Brian P.", ""]]}, {"id": "2009.07910", "submitter": "Johannes Wieditz", "authors": "Johannes Wieditz, Yvo Pokern, Dominic Schuhmacher, Stephan Huckemann", "title": "Characteristic and Necessary Minutiae in Fingerprints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprints feature a ridge pattern with moderately varying ridge frequency\n(RF), following an orientation field (OF), which usually features some\nsingularities. Additionally at some points, called minutiae, ridge lines end or\nfork and this point pattern is usually used for fingerprint identification and\nauthentication. Whenever the OF features divergent ridge lines (e.g. near\nsingularities), a nearly constant RF necessitates the generation of more ridge\nlines, originating at minutiae. We call these the necessary minutiae. It turns\nout that fingerprints feature additional minutiae which occur at rather\narbitrary locations. We call these the random minutiae or, since they may\nconvey fingerprint individuality beyond the OF, the characteristic minutiae. In\nconsequence, the minutiae point pattern is assumed to be a realization of the\nsuperposition of two stochastic point processes: a Strauss point process (whose\nactivity function is given by the divergence field) with an additional hard\ncore, and a homogeneous Poisson point process, modelling the necessary and the\ncharacteristic minutiae, respectively. We perform Bayesian inference using an\nMCMC-based minutiae separating algorithm (MiSeal). In simulations, it provides\ngood mixing and good estimation of underlying parameters. In application to\nfingerprints, we can separate the two minutiae patterns and verify by example\nof two different prints with similar OF that characteristic minutiae convey\nfingerprint individuality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 19:38:36 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 11:18:10 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 16:50:01 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wieditz", "Johannes", ""], ["Pokern", "Yvo", ""], ["Schuhmacher", "Dominic", ""], ["Huckemann", "Stephan", ""]]}, {"id": "2009.07915", "submitter": "Massimiliano (Max) Bonamente", "authors": "Massimiliano Bonamente and David Spence", "title": "A semi-analytical solution to the maximum likelihood fit of Poisson data\n  to a linear model using the Cash statistic", "comments": "Accepted for publication in the Journal of Applied Statistics. Python\n  codes associated with this paper, including functions that can be customized\n  for individual use, are available at:\n  https://www.bonamente-statistics-data-analysis.com/home/cstat-linear-model", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [ABRIDGED] The Cash statistic, also known as the C stat, is commonly used for\nthe analysis of low-count Poisson data, including data with null counts for\ncertain values of the independent variable. The use of this statistic is\nespecially attractive for low-count data that cannot be combined, or re-binned,\nwithout loss of resolution. This paper presents a new maximum-likelihood\nsolution for the best-fit parameters of a linear model using the Poisson-based\nCash statistic. The solution presented in this paper provides a new and simple\nmethod to measure the best-fit parameters of a linear model for any\nPoisson-based data, including data with null counts. In particular, the method\nenforces the requirement that the best-fit linear model be non-negative\nthroughout the support of the independent variable. The method is summarized in\na simple algorithm to fit Poisson counting data of any size and counting rate\nwith a linear model, by-passing entirely the use of the traditional $\\chi^2$\nstatistic.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 20:04:00 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Bonamente", "Massimiliano", ""], ["Spence", "David", ""]]}, {"id": "2009.07934", "submitter": "Paul Parker", "authors": "Paul A. Parker and Scott H. Holan", "title": "Computationally Efficient Deep Bayesian Unit-Level Modeling of Survey\n  Data under Informative Sampling for Small Area Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of deep learning has seen a surge of interest in recent years both\nwithin and outside of the field of Statistics. Deep models leverage both\nnonlinearity and interaction effects to provide superior predictions in many\ncases when compared to linear or generalized linear models. However, one of the\nmain challenges with deep modeling approaches is quantification of uncertainty.\nThe use of random weight models, such as the popularized \"Extreme Learning\nMachine,\" offer a potential solution in this regard. In addition to uncertainty\nquantification, these models are extremely computationally efficient as they do\nnot require optimization through stochastic gradient descent, which is what is\ntypically done for deep learning. We show how the use of random weights in a\ndeep model can fit into a likelihood based framework to allow for uncertainty\nquantification of the model parameters and any desired estimates. Furthermore,\nwe show how this approach can be used to account for informative sampling of\nsurvey data through the use of a pseudo-likelihood. We illustrate the\neffectiveness of this methodology through simulation and with a real survey\ndata application involving American National Election Studies data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 21:05:21 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""]]}, {"id": "2009.08011", "submitter": "Xiang Lyu", "authors": "Xiang Lyu, Jian Kang, Lexin Li", "title": "Statistical Inference for High-Dimensional Vector Autoregression with\n  Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional vector autoregression with measurement error is frequently\nencountered in a large variety of scientific and business applications. In this\narticle, we study statistical inference of the transition matrix under this\nmodel. While there has been a large body of literature studying sparse\nestimation of the transition matrix, there is a paucity of inference solutions,\nespecially in the high-dimensional scenario. We develop inferential procedures\nfor both the global and simultaneous testing of the transition matrix. We first\ndevelop a new sparse expectation-maximization algorithm to estimate the model\nparameters, and carefully characterize their estimation precisions. We then\nconstruct a Gaussian matrix, after proper bias and variance corrections, from\nwhich we derive the test statistics. Finally, we develop the testing procedures\nand establish their asymptotic guarantees. We study the finite-sample\nperformance of our tests through intensive simulations, and illustrate with a\nbrain connectivity analysis example.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:57:45 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lyu", "Xiang", ""], ["Kang", "Jian", ""], ["Li", "Lexin", ""]]}, {"id": "2009.08165", "submitter": "Marta Regis", "authors": "Marta Regis, Paulo Serra, Edwin R. van den Heuvel", "title": "Random autoregressive models: A structured overview", "comments": "41 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models characterized by autoregressive structure and random coefficients are\npowerful tools for the analysis of high-frequency, high-dimensional and\nvolatile time series. The available literature on such models is broad, but\nalso sectorial, overlapping, and confusing. Most models focus on one property\nof the data, while much can be gained by combining the strength of various\nmodels and their sources of heterogeneity. We present a structured overview of\nthe literature on autoregressive models with random coefficients. We describe\nhierarchy and analogies among models, and for each we systematically list\nproperties, estimation methods, tests, software packages and typical\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 09:21:22 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Regis", "Marta", ""], ["Serra", "Paulo", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2009.08482", "submitter": "Takashi Arai", "authors": "Takashi Arai", "title": "Multivariate binary probability distribution in the Grassmann formalism", "comments": "24 pages, 3 figures", "journal-ref": "Phys. Rev. E 103, 062104 (2021)", "doi": "10.1103/PhysRevE.103.062104", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probability distribution for multivariate binary random\nvariables. For this purpose, we use the Grassmann number, an anti-commuting\nnumber. In our model, the partition function, the central moment, and the\nmarginal and conditional distributions are expressed analytically by the matrix\nof the parameters analogous to the covariance matrix in the multivariate\nGaussian distribution. That is, summation over all possible states is not\nnecessary for obtaining the partition function and various expected values,\nwhich is a problem with the conventional multivariate Bernoulli distribution.\nThe proposed model has many similarities to the multivariate Gaussian\ndistribution. For example, the marginal and conditional distributions are\nexpressed by the parameter matrix and its inverse matrix, respectively. That\nis, the inverse matrix expresses a sort of partial correlation. Analytical\nexpressions for the marginal and conditional distributions are also useful in\ngenerating random numbers for multivariate binary variables. Hence, we\nvalidated the proposed method using synthetic datasets. We observed that the\nsampling distributions of various statistics are consistent with the\ntheoretical predictions and estimates are consistent and asymptotically normal.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 18:03:17 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Arai", "Takashi", ""]]}, {"id": "2009.08543", "submitter": "Jon Wakefield", "authors": "Katie Wilson, Jon Wakefield", "title": "Estimation of Health and Demographic Indicators with Incomplete\n  Geographic Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In low and middle income countries, household surveys are a valuable source\nof information for a range of health and demographic indicators. Increasingly,\nsubnational estimates are required for targeting interventions and evaluating\nprogress towards targets. In the majority of cases, stratified cluster sampling\nis used, with clusters corresponding to enumeration areas. The reported\ngeographical information varies. A common procedure, to preserve\nconfidentiality, is to give a jittered location with the true centroid of the\ncluster is displaced under a known algorithm. An alternative situation, which\nwas used for older surveys in particular, is to report the geographical region\nwithin the cluster lies. In this paper, we describe a spatial hierarchical\nmodel in which we account for inaccuracies in the cluster locations. The\ncomputational algorithm we develop is fast and avoids the heavy computation of\na pure MCMC approach. We illustrate by simulation the benefits of the model,\nover naive alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 21:42:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wilson", "Katie", ""], ["Wakefield", "Jon", ""]]}, {"id": "2009.08564", "submitter": "Yifei Sun", "authors": "Guillaume Carlier, Arnaud Dupuy, Alfred Galichon, Yifei Sun", "title": "SISTA: learning optimal transport costs under sparsity constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a novel iterative procedure called SISTA to learn\nthe underlying cost in optimal transport problems. SISTA is a hybrid between\ntwo classical methods, coordinate descent (\"S\"-inkhorn) and proximal gradient\ndescent (\"ISTA\"). It alternates between a phase of exact minimization over the\ntransport potentials and a phase of proximal gradient descent over the\nparameters of the transport cost. We prove that this method converges linearly,\nand we illustrate on simulated examples that it is significantly faster than\nboth coordinate descent and ISTA. We apply it to estimating a model of\nmigration, which predicts the flow of migrants using country-specific\ncharacteristics and pairwise measures of dissimilarity between countries. This\napplication demonstrates the effectiveness of machine learning in quantitative\nsocial sciences.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 00:12:49 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 00:53:22 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Carlier", "Guillaume", ""], ["Dupuy", "Arnaud", ""], ["Galichon", "Alfred", ""], ["Sun", "Yifei", ""]]}, {"id": "2009.08573", "submitter": "Reza Mehrizi", "authors": "Reza V. Mehrizi and Shojaeddin Chenouri", "title": "Detection of Change Points in Piecewise Polynomial Signals Using Trend\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many approaches have been proposed for discovering abrupt changes in\npiecewise constant signals, few methods are available to capture these changes\nin piecewise polynomial signals. In this paper, we propose a change point\ndetection method, PRUTF, based on trend filtering. By providing a comprehensive\ndual solution path for trend filtering, PRUTF allows us to discover change\npoints of the underlying signal for either a given value of the regularization\nparameter or a specific number of steps of the algorithm. We demonstrate that\nthe dual solution path constitutes a Gaussian bridge process that enables us to\nderive an exact and efficient stopping rule for terminating the search\nalgorithm. We also prove that the estimates produced by this algorithm are\nasymptotically consistent in pattern recovery. This result holds even in the\ncase of staircases (consecutive change points of the same sign) in the signal.\nFinally, we investigate the performance of our proposed method for various\nsignals and then compare its performance against some state-of-the-art methods\nin the context of change point detection. We apply our method to three\nreal-world datasets including the UK House Price Index (HPI), the GISS surface\nTemperature Analysis (GISTEMP) and the Coronavirus disease (COVID-19) pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 00:47:00 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Mehrizi", "Reza V.", ""], ["Chenouri", "Shojaeddin", ""]]}, {"id": "2009.08592", "submitter": "Ciaran Evans", "authors": "Ciaran Evans and Max G'Sell", "title": "Sequential changepoint detection for label shift in classification", "comments": "34 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier predictions often rely on the assumption that new observations\ncome from the same distribution as training data. When the underlying\ndistribution changes, so does the optimal classifier rule, and predictions may\nno longer be valid. We consider the problem of detecting a change to the\noverall fraction of positive cases, known as label shift, in\nsequentially-observed binary classification data. We reduce this problem to the\nproblem of detecting a change in the one-dimensional classifier scores, which\nallows us to develop simple nonparametric sequential changepoint detection\nprocedures. Our procedures leverage classifier training data to estimate the\ndetection statistic, and converge to their parametric counterparts in the size\nof the training data. In simulations, we show that our method compares\nfavorably to other detection procedures in the label shift setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 02:26:46 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Evans", "Ciaran", ""], ["G'Sell", "Max", ""]]}, {"id": "2009.08789", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Hans-Georg M\\\"uller and Byeong U. Park", "title": "Additive Models for Symmetric Positive-Definite Matrices, Riemannian\n  Manifolds and Lie groups", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an additive regression model for a symmetric positive-definite\nmatrix valued response and multiple scalar predictors is proposed. The model\nexploits the abelian group structure inherited from either the Log-Cholesky\nmetric or the Log-Euclidean framework that turns the space of symmetric\npositive-definite matrices into a Riemannian manifold and further a\nbi-invariant Lie group. The additive model for responses in the space of\nsymmetric positive-definite matrices with either of these metrics is shown to\nconnect to an additive model on a tangent space. This connection not only\nentails an efficient algorithm to estimate the component functions but also\nallows to generalize the proposed additive model to general Riemannian\nmanifolds that might not have a Lie group structure. Optimal asymptotic\nconvergence rates and normality of the estimated component functions are also\nestablished. Numerical studies show that the proposed model enjoys superior\nnumerical performance, especially when there are multiple predictors. The\npractical merits of the proposed model are demonstrated by analyzing diffusion\ntensor brain imaging data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 12:29:31 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lin", "Zhenhua", ""], ["M\u00fcller", "Hans-Georg", ""], ["Park", "Byeong U.", ""]]}, {"id": "2009.08883", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian and Diego Fern\\'andez", "title": "An Independence Test Based on Recurrence Rates. An empirical study and\n  applications to real data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose several variants to perform the independence test\nbetween two random elements based on recurrence rates. We will show how to\ncalculate the test statistic in each one of these cases. From simulations we\nobtain that in high dimension, our test clearly outperforms, in almost all\ncases, the other widely used competitors. The test was performed on two data\nsets including small and large sample sizes and we show that in both ases the\napplication of the test allows us to obtain interesting conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 15:12:04 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kalemkerian", "Juan", ""], ["Fern\u00e1ndez", "Diego", ""]]}, {"id": "2009.08915", "submitter": "Paula Saavedra-Nieves", "authors": "Paula Saavedra-Nieves and Rosa Mar\\'ia Crujeiras", "title": "Nonparametric estimation of directional highest density regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of sets from a random sample of points intimately related to\nthem is the goal of set estimation theory. Within this context, a particular\nproblem is the one related with the reconstruction of density level sets and\nspecifically, those ones with a high probability content, namely highest\ndensity regions.\n  We define highest density regions for directional data and provide a plug-in\nestimator, based on kernel smoothing. A suitable bootstrap bandwidth selector\nis provided for the practical implementation of the proposal. An extensive\nsimulation study shows the performance of the plug-in estimator proposed with\nthe bootstrap bandwidth selector and with other bandwidth selectors\nspecifically designed for circular and spherical kernel density estimation. The\nmethodology is applied to analyze two real data sets in animal orientation and\nseismology.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 16:22:44 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 12:10:34 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Saavedra-Nieves", "Paula", ""], ["Crujeiras", "Rosa Mar\u00eda", ""]]}, {"id": "2009.08933", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Comment on Glenn Shafer's \"Testing by betting\"", "comments": "7 pages, 1 figure", "journal-ref": "J R Stat Soc Series A 184:432-478 (2021)", "doi": "10.1111/rssa.12656", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is my comment on Glenn Shafer's discussion paper \"Testing by\nbetting\", together with two online appendices comparing p-values and betting\nscores.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 11:50:03 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:49:58 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2009.09002", "submitter": "Qiaolan Deng", "authors": "Qiaolan Deng and Chi Song", "title": "Multiple-trait Adaptive Fisher's Method for Genome-wide Association\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association studies (GWASs), there is an increasing need for\ndetecting the associations between a genetic variant and multiple traits. In\nstudies of complex diseases, it is common to measure several potentially\ncorrelated traits in a single GWAS. Despite the multivariate nature of the\nstudies, single-trait-based methods remain the most widely-adopted analysis\nprocedure, owing to their simplicity for studies with multiple traits as their\noutcome. However, the association between a genetic variant and a single trait\nsometimes can be weak, and ignoring the actual correlation among traits may\nlose power. On the contrary, multiple-trait analysis, a method analyzes a group\nof traits simultaneously, has been proven to be more powerful by incorporating\ninformation from the correlated traits. Although existing methods have been\ndeveloped for multiple traits, several drawbacks limit their wide application\nin GWASs. In this paper, we propose a multiple-trait adaptive Fisher's (MTAF)\nmethod to test associations between a genetic variant and multiple traits at\nonce, by adaptively aggregating evidence from each trait. The proposed method\ncan accommodate both continuous and binary traits and it has reliable\nperformance under various scenarios. Using a simulation study, we compared our\nproposed method with several existing methods and demonstrated its\ncompetitiveness in terms of type I error control and statistical power. By\napplying the method to the Study of Addiction: Genetics and Environment (SAGE)\ndataset, we successfully identified several genes associated with substance\ndependence.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 18:22:03 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 18:43:15 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 01:23:38 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Deng", "Qiaolan", ""], ["Song", "Chi", ""]]}, {"id": "2009.09034", "submitter": "Marina Vannucci", "authors": "Matthew D. Koslovsky, Emily T. Hebert, Michael S. Businelle and Marina\n  Vannucci", "title": "A Bayesian Time-Varying Effect Model for Behavioral mHealth Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of mobile health (mHealth) devices into behavioral health\nresearch has fundamentally changed the way researchers and interventionalists\nare able to collect data as well as deploy and evaluate intervention\nstrategies. In these studies, researchers often collect intensive longitudinal\ndata (ILD) using ecological momentary assessment methods, which aim to capture\npsychological, emotional, and environmental factors that may relate to a\nbehavioral outcome in near real-time. In order to investigate ILD collected in\na novel, smartphone-based smoking cessation study, we propose a Bayesian\nvariable selection approach for time-varying effect models, designed to\nidentify dynamic relations between potential risk factors and smoking behaviors\nin the critical moments around a quit attempt. We use parameter-expansion and\ndata-augmentation techniques to efficiently explore how the underlying\nstructure of these relations varies over time and across subjects. We achieve\ndeeper insights into these relations by introducing nonparametric priors for\nregression coefficients that cluster similar effects for risk factors while\nsimultaneously determining their inclusion. Results indicate that our approach\nis well-positioned to help researchers effectively evaluate, design, and\ndeliver tailored intervention strategies in the critical moments surrounding a\nquit attempt.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 19:24:47 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Koslovsky", "Matthew D.", ""], ["Hebert", "Emily T.", ""], ["Businelle", "Michael S.", ""], ["Vannucci", "Marina", ""]]}, {"id": "2009.09036", "submitter": "Falco J. Bargagli Stoffi", "authors": "Kwonsang Lee, Falco J. Bargagli-Stoffi, Francesca Dominici", "title": "Causal Rule Ensemble: Interpretable Inference of Heterogeneous Treatment\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social and health sciences, it is critically important to identify\nsubgroups of the study population where a treatment has a notably larger or\nsmaller causal effect compared to the population average. In recent years,\nthere have been many methodological developments for addressing heterogeneity\nof causal effects. A common approach is to estimate the conditional average\ntreatment effect (CATE) given a pre-specified set of covariates. However, this\napproach does not allow to discover new subgroups. Recent causal machine\nlearning (ML) approaches estimate the CATE at an individual level in presence\nof large number of observations and covariates with great accuracy.\nNevertheless, the bulk of these ML approaches do not provide an interpretable\ncharacterization of the heterogeneous subgroups. In this paper, we propose a\nnew Causal Rule Ensemble (CRE) method that: 1) discovers de novo subgroups with\nsignificantly heterogeneous treatment effects (causal rules); 2) ensures\ninterpretability of these subgroups because they are defined in terms of\ndecision rules; and 3) estimates the CATE for each of these newly discovered\nsubgroups with small bias and high statistical precision. We provide\ntheoretical results that guarantee consistency of the estimated causal effects\nfor the newly discovered causal rules. A nice feature of CRE is that it is\nagnostic to the choices of the ML algorithms that can be used to discover the\ncausal rules, and the estimation methods for the causal effects within the\ndiscovered causal rules. Via simulations, we show that the CRE method has\ncompetitive performance as compared to existing approaches while providing\nenhanced interpretability. We also introduce a new sensitivity analysis to\nunmeasured confounding bias. We apply the CRE method to discover subgroups that\nare more vulnerable to the causal effects of long-term exposure to air\npollution on mortality.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 19:28:57 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 13:18:06 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 20:28:40 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Kwonsang", ""], ["Bargagli-Stoffi", "Falco J.", ""], ["Dominici", "Francesca", ""]]}, {"id": "2009.09177", "submitter": "Shengming Luo", "authors": "Jiashun Jin and Zheng Tracy Ke and Shengming Luo and Minzhe Wang", "title": "Estimating the number of communities by Stepwise Goodness-of-fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric network with $n$ nodes, how to estimate the number of\ncommunities $K$ is a fundamental problem. We propose Stepwise Goodness-of-Fit\n(StGoF) as a new approach to estimating $K$. For $m = 1, 2, \\ldots$, StGoF\nalternately uses a community detection step (pretending $m$ is the correct\nnumber of communities) and a goodness-of-fit step. We use SCORE \\cite{SCORE}\nfor community detection, and propose a new goodness-of-fit measure. Denote the\ngoodness-of-fit statistic in step $m$ by $\\psi_n^{(m)}$. We show that as $n\n\\rightarrow \\infty$, $\\psi_n^{(m)} \\rightarrow N(0,1)$ when $m = K$ and\n$\\psi_n^{(m)} \\rightarrow \\infty$ in probability when $m < K$. Therefore, with\na proper threshold, StGoF terminates at $m = K$ as desired.\n  We consider a broad setting where we allow severe degree heterogeneity, a\nwide range of sparsity, and especially weak signals. In particular, we propose\na measure for signal-to-noise ratio (SNR) and show that there is a phase\ntransition: when $\\mathrm{SNR} \\rightarrow 0$ as $n \\rightarrow \\infty$,\nconsistent estimates for $K$ do not exist, and when $\\mathrm{SNR} \\rightarrow\n\\infty$, StGoF is consistent, uniformly for a broad class of settings. In this\nsense, StGoF achieves the optimal phase transition. Stepwise testing algorithms\nof similar kind (e.g., \\cite{wang2017likelihood, ma2018determining}) are known\nto face analytical challenges. We overcome the challenges by using a different\ndesign in the stepwise algorithm and by deriving sharp results in the\nunder-fitting case $(m < K)$ and the null case ($m = K$). The key to our\nanalysis is to show that SCORE has the {\\it Non-Splitting Property (NSP)}. The\nNSP is non-obvious, so additional to rigorous proofs, we also provide an\nintuitive explanation.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 06:44:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""], ["Wang", "Minzhe", ""]]}, {"id": "2009.09200", "submitter": "Olga Mula", "authors": "Athmane Bakhta, Thomas Boiveau, Yvon Maday, Olga Mula", "title": "Epidemiological Forecasting with Model Reduction of Compartmental\n  Models. Application to the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a forecasting method for predicting epidemiological health series\non a two-week horizon at the regional and interregional resolution. The\napproach is based on model order reduction of parametric compartmental models,\nand is designed to accommodate small amount of sanitary data. The efficiency of\nthe method is shown in the case of the prediction of the number of infected and\nremoved people during the two pandemic waves of COVID-19 in France, which have\ntaken place approximately between February and November 2020. Numerical results\nillustrate the promising potential of the approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 09:47:19 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 17:49:03 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 09:13:19 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Bakhta", "Athmane", ""], ["Boiveau", "Thomas", ""], ["Maday", "Yvon", ""], ["Mula", "Olga", ""]]}, {"id": "2009.09248", "submitter": "Shouhao Zhou", "authors": "Shouhao Zhou", "title": "Posterior Averaging Information Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model selection method, the posterior averaging information\ncriterion, for Bayesian model assessment from a predictive perspective. The\ntheoretical foundation is built on the Kullback-Leibler divergence to quantify\nthe similarity between the proposed candidate model and the underlying true\nmodel. From a Bayesian perspective, our method evaluates the candidate models\nover the entire posterior distribution in terms of predicting a future\nindependent observation. Without assuming that the true distribution is\ncontained in the candidate models, the new criterion is developed by correcting\nthe asymptotic bias of the posterior mean of the log-likelihood against its\nexpected log-likelihood. It can be generally applied even for Bayesian models\nwith degenerate non-informative prior. The simulation in both normal and\nbinomial settings demonstrates decent small sample performance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 14:58:34 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhou", "Shouhao", ""]]}, {"id": "2009.09286", "submitter": "Zhiqiang Tan", "authors": "Baoluo Sun, Zhiqiang Tan", "title": "High-dimensional Model-assisted Inference for Local Average Treatment\n  Effects with Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating the local average treatment effect with an\ninstrument variable, where the instrument unconfoundedness holds after\nadjusting for a set of measured covariates. Several unknown functions of the\ncovariates need to be estimated through regression models, such as instrument\npropensity score and treatment and outcome regression models. We develop a\ncomputationally tractable method in high-dimensional settings where the numbers\nof regression terms are close to or larger than the sample size. Our method\nexploits regularized calibrated estimation, which involves Lasso penalties but\ncarefully chosen loss functions for estimating coefficient vectors in these\nregression models, and then employs a doubly robust estimator for the treatment\nparameter through augmented inverse probability weighting. We provide rigorous\ntheoretical analysis to show that the resulting Wald confidence intervals are\nvalid for the treatment parameter under suitable sparsity conditions if the\ninstrument propensity score model is correctly specified, but the treatment and\noutcome regression models may be misspecified. For existing high-dimensional\nmethods, valid confidence intervals are obtained for the treatment parameter if\nall three models are correctly specified. We evaluate the proposed methods via\nextensive simulation studies and an empirical application to estimate the\nreturns to education.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:41:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sun", "Baoluo", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "2009.09414", "submitter": "Janet van Niekerk Dr", "authors": "Janet van Niekerk and Haavard Rue", "title": "Skewed probit regression -- Identifiability, contraction and\n  reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skewed probit regression is but one example of a statistical model that\ngeneralizes a simpler model, like probit regression. All skew-symmetric\ndistributions and link functions arise from symmetric distributions by\nincorporating a skewness parameter through some skewing mechanism. In this work\nwe address some fundamental issues in skewed probit regression, and more\ngenreally skew-symmetric distributions or skew-symmetric link functions. We\naddress the issue of identifiability of the skewed probit model parameters by\nreformulating the intercept from first principles. A new standardization of the\nskew link function is given to provide and anchored interpretation of the\ninference. Possible skewness parameters are investigated and the penalizing\ncomplexity priors of these are derived. This prior is invariant under\nreparameterization of the skewness parameter and quantifies the contraction of\nthe skewed probit model to the probit model. The proposed results are available\nin the R-INLA package and we illustrate the use and effects of this work using\nsimulated data, and well-known datasets using the link as well as the\nlikelihood.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 11:58:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["van Niekerk", "Janet", ""], ["Rue", "Haavard", ""]]}, {"id": "2009.09420", "submitter": "Emiko Dupont", "authors": "Emiko Dupont, Simon N. Wood, Nicole Augustin", "title": "Spatial+: a novel approach to spatial confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial regression models, collinearity between covariates and spatial\neffects can lead to significant bias in effect estimates. This problem, known\nas spatial confounding, is encountered modelling forestry data to assess the\neffect of temperature on tree health. Reliable inference is difficult as\nresults depend on whether or not spatial effects are included in the model. The\nmechanism behind spatial confounding is poorly understood and methods for\ndealing with it are limited. We propose a novel approach, spatial+, in which\ncollinearity is reduced by replacing the covariates in the spatial model by\ntheir residuals after spatial dependence has been regressed away. Using a thin\nplate spline model formulation, we recognise spatial confounding as a\nsmoothing-induced bias identified by Rice (1986), and through asymptotic\nanalysis of the effect estimates, we show that spatial+ avoids the bias\nproblems of the spatial model. This is also demonstrated in a simulation study.\nSpatial+ is straight-forward to implement using existing software and, as the\nresponse variable is the same as that of the spatial model, standard model\nselection criteria can be used for comparisons. A major advantage of the method\nis also that it extends to models with non-Gaussian response distributions.\nFinally, while our results are derived in a thin plate spline setting, the\nspatial+ methodology transfers easily to other spatial model formulations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 12:11:35 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Dupont", "Emiko", ""], ["Wood", "Simon N.", ""], ["Augustin", "Nicole", ""]]}, {"id": "2009.09440", "submitter": "Erik van Zwet", "authors": "Erik van Zwet and Eric Cator", "title": "The Significance Filter, the Winner's Curse and the Need to Shrink", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"significance filter\" refers to focusing exclusively on statistically\nsignificant results. Since frequentist properties such as unbiasedness and\ncoverage are valid only before the data have been observed, there are no\nguarantees if we condition on significance. In fact, the significance filter\nleads to overestimation of the magnitude of the parameter, which has been\ncalled the \"winner's curse\". It can also lead to undercoverage of the\nconfidence interval. Moreover, these problems become more severe if the power\nis low. While these issues clearly deserve our attention, they have been\nstudied only informally and mathematical results are lacking. Here we study\nthem from the frequentist and the Bayesian perspective. We prove that the\nrelative bias of the magnitude is a decreasing function of the power and that\nthe usual confidence interval undercovers when the power is less than 50%. We\nconclude that failure to apply the appropriate amount of shrinkage can lead to\nmisleading inferences.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 14:30:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["van Zwet", "Erik", ""], ["Cator", "Eric", ""]]}, {"id": "2009.09462", "submitter": "Ke Zhu", "authors": "Ke Zhu and Hanzhong Liu", "title": "Confidence intervals for parameters in high-dimensional sparse vector\n  autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector autoregression (VAR) models are widely used to analyze the\ninterrelationship between multiple variables over time. Estimation and\ninference for the transition matrices of VAR models are crucial for\npractitioners to make decisions in fields such as economics and finance.\nHowever, when the number of variables is larger than the sample size, it\nremains a challenge to perform statistical inference of the model parameters.\nIn this article, we propose the de-biased Lasso and two bootstrap de-biased\nLasso methods to construct confidence intervals for the elements of the\ntransition matrices of high-dimensional VAR models. We show that the proposed\nmethods are asymptotically valid under appropriate sparsity and other\nregularity conditions. To implement our methods, we develop feasible and\nparallelizable algorithms, which save a large amount of computation required by\nthe nodewise Lasso and bootstrap. A simulation study illustrates that our\nmethods perform well in finite samples. Finally, we apply our methods to\nanalyze the price data of stocks in the S&P 500 index in 2019. We find that\nsome stocks, such as the largest producer of gold in the world, Newmont\nCorporation, have significant predictive power over the most stocks.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 16:05:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhu", "Ke", ""], ["Liu", "Hanzhong", ""]]}, {"id": "2009.09592", "submitter": "David Frazier", "authors": "Gael M. Martin, Rub\\'en Loaiza-Maya, David T. Frazier, Worapree\n  Maneesoonthorn, Andr\\'es Ram\\'irez Hassan", "title": "Optimal probabilistic forecasts: When do they work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper scoring rules are used to assess the out-of-sample accuracy of\nprobabilistic forecasts, with different scoring rules rewarding distinct\naspects of forecast performance. Herein, we re-investigate the practice of\nusing proper scoring rules to produce probabilistic forecasts that are\n`optimal' according to a given score, and assess when their out-of-sample\naccuracy is superior to alternative forecasts, according to that score.\nParticular attention is paid to relative predictive performance under\nmisspecification of the predictive model. Using numerical illustrations, we\ndocument several novel findings within this paradigm that highlight the\nimportant interplay between the true data generating process, the assumed\npredictive model and the scoring rule. Notably, we show that only when a\npredictive model is sufficiently compatible with the true process to allow a\nparticular score criterion to reward what it is designed to reward, will this\napproach to forecasting reap benefits. Subject to this compatibility however,\nthe superiority of the optimal forecast will be greater, the greater is the\ndegree of misspecification. We explore these issues under a range of different\nscenarios, and using both artificially simulated and empirical data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 03:07:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Martin", "Gael M.", ""], ["Loaiza-Maya", "Rub\u00e9n", ""], ["Frazier", "David T.", ""], ["Maneesoonthorn", "Worapree", ""], ["Hassan", "Andr\u00e9s Ram\u00edrez", ""]]}, {"id": "2009.09974", "submitter": "Francesca Romana Crucinio", "authors": "Francesca R Crucinio, Arnaud Doucet, Adam M Johansen", "title": "A Particle Method for Solving Fredholm Equations of the First Kind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fredholm integral equations of the first kind are the prototypical example of\nill-posed linear inverse problems. They model, among other things,\nreconstruction of distorted noisy observations and indirect density estimation\nand also appear in instrumental variable regression. However, their numerical\nsolution remains a challenging problem. Many techniques currently available\nrequire a preliminary discretization of the domain of the solution and make\nstrong assumptions about its regularity. For example, the popular expectation\nmaximization smoothing (EMS) scheme requires the assumption of piecewise\nconstant solutions which is inappropriate for most applications. We propose\nhere a novel particle method that circumvents these two issues. This algorithm\ncan be thought of as a Monte Carlo approximation of the EMS scheme which not\nonly performs an adaptive stochastic discretization of the domain but also\nresults in smooth approximate solutions. We analyze the theoretical properties\nof the EMS iteration and of the corresponding particle algorithm. Compared to\nstandard EMS, we show experimentally that our novel particle method provides\nstate-of-the-art performance for realistic systems, including motion deblurring\nand reconstruction of cross-section images of the brain from positron emission\ntomography.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:57:16 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 13:34:55 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Crucinio", "Francesca R", ""], ["Doucet", "Arnaud", ""], ["Johansen", "Adam M", ""]]}, {"id": "2009.10015", "submitter": "Pedro Mediano", "authors": "Pedro A.M. Mediano, Fernando E. Rosas, Adam B. Barrett, Daniel Bor", "title": "Decomposing spectral and phasic differences in non-linear features\n  between datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When employing non-linear methods to characterise complex systems, it is\nimportant to determine to what extent they are capturing genuine non-linear\nphenomena that could not be assessed by simpler spectral methods. Specifically,\nwe are concerned with the problem of quantifying spectral and phasic effects on\nan observed difference in a non-linear feature between two systems (or two\nstates of the same system). Here we derive, from a sequence of null models, a\ndecomposition of the difference in an observable into spectral, phasic, and\nspectrum-phase interaction components. Our approach makes no assumptions about\nthe structure of the data and adds nuance to a wide range of time series\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:46:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mediano", "Pedro A. M.", ""], ["Rosas", "Fernando E.", ""], ["Barrett", "Adam B.", ""], ["Bor", "Daniel", ""]]}, {"id": "2009.10029", "submitter": "Sen Tian", "authors": "Sen Tian, Clifford M. Hurvich and Jeffrey S. Simonoff", "title": "Selection of Regression Models under Linear Restrictions for Fixed and\n  Random Designs", "comments": "The computer code to reproduce the results for this article and the\n  complete set of simulation results are available online at\n  https://github.com/sentian/RAICc", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important modeling tasks in linear regression, including variable\nselection (in which slopes of some predictors are set equal to zero) and\nsimplified models based on sums or differences of predictors (in which slopes\nof those predictors are set equal to each other, or the negative of each other,\nrespectively), can be viewed as being based on imposing linear restrictions on\nregression parameters. In this paper, we discuss how such models can be\ncompared using information criteria designed to estimate predictive measures\nlike squared error and Kullback-Leibler (KL) discrepancy, in the presence of\neither deterministic predictors (fixed-X) or random predictors (random-X). We\nextend the justifications for existing fixed-X criteria Cp, FPE and AICc, and\nrandom-X criteria Sp and RCp, to general linear restrictions. We further\npropose and justify a KL-based criterion, RAICc, under random-X for variable\nselection and general linear restrictions. We show in simulations that the use\nof the KL-based criteria AICc and RAICc results in better predictive\nperformance and sparser solutions than the use of squared error-based criteria,\nincluding cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 17:11:55 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tian", "Sen", ""], ["Hurvich", "Clifford M.", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "2009.10117", "submitter": "Zhengyang Zhou", "authors": "Zhengyang Zhou, Dateng Li, Song Zhang", "title": "Sample Size Calculation for Cluster Randomized Trials with Zero-inflated\n  Count Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster randomized trails (CRT) have been widely employed in medical and\npublic health research. Many clinical count outcomes, such as the number of\nfalls in nursing homes, exhibit excessive zero values. In the presence of zero\ninflation, traditional power analysis methods for count data based on Poisson\nor negative binomial distribution may be inadequate. In this study, we present\na sample size method for CRTs with zero-inflated count outcomes. It is\ndeveloped based on GEE regression directly modeling the marginal mean of a ZIP\noutcome, which avoids the challenge of testing two intervention effects under\ntraditional modeling approaches. A closed-form sample size formula is derived\nwhich properly accounts for zero inflation, ICCs due to clustering, unbalanced\nrandomization, and variability in cluster size. Robust approaches, including\nt-distribution-based approximation and Jackknife re-sampling variance\nestimator, are employed to enhance trial properties under small sample sizes.\nExtensive simulations are conducted to evaluate the performance of the proposed\nmethod. An application example is presented in a real clinical trial setting.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:22:16 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Zhou", "Zhengyang", ""], ["Li", "Dateng", ""], ["Zhang", "Song", ""]]}, {"id": "2009.10126", "submitter": "Hamed Honari", "authors": "Hamed Honari (1), Ann S. Choe (2 and 3 and 4), Martin A. Lindquist (5)\n  ((1) Department of Electrical and Computer Engineering, Johns Hopkins\n  University, USA (2) F. M. Kirby Research Center for Functional Brain Imaging,\n  Kennedy Krieger Institute, USA (3) International Center for Spinal Cord\n  Injury, Kennedy Krieger Institute, USA (4) Russell H. Morgan Department of\n  Radiology and Radiological Science, Johns Hopkins School of Medicine, USA (5)\n  Department of Biostatistics, Johns Hopkins University, USA)", "title": "Evaluating phase synchronization methods in fMRI: a comparison study and\n  new approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been growing interest in measuring time-varying\nfunctional connectivity between different brain regions using resting-state\nfunctional magnetic resonance imaging (rs-fMRI) data. One way to assess the\nrelationship between signals from different brain regions is to measure their\nphase synchronization (PS) across time. There are several ways to perform such\nanalyses, and here we compare methods that utilize a PS metric together with a\nsliding window, referred to here as windowed phase synchronization (WPS), with\nthose that directly measure the instantaneous phase synchronization (IPS). In\nparticular, IPS has recently gained popularity as it offers single time-point\nresolution of time-resolved fMRI connectivity. In this paper, we discuss the\nunderlying assumptions required for performing PS analyses and emphasize the\nnecessity of band-pass filtering the data to obtain valid results. We review\nvarious methods for evaluating PS and introduce a new approach within the IPS\nframework denoted the cosine of the relative phase (CRP). We contrast methods\nthrough a series of simulations and application to rs-fMRI data. Our results\nindicate that CRP outperforms other tested methods and overcomes issues related\nto undetected temporal transitions from positive to negative associations\ncommon in IPS analysis. Further, in contrast to phase coherence, CRP unfolds\nthe distribution of PS measures, which benefits subsequent clustering of PS\nmatrices into recurring brain states.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:38:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Honari", "Hamed", "", "2 and 3 and 4"], ["Choe", "Ann S.", "", "2 and 3 and 4"], ["Lindquist", "Martin A.", ""]]}, {"id": "2009.10264", "submitter": "Sahir Bhatnagar", "authors": "Sahir Rai Bhatnagar, Maxime Turgeon, Jesse Islam, James A. Hanley,\n  Olli Saarela", "title": "casebase: An Alternative Framework For Survival Analysis and Comparison\n  of Event Rates", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In epidemiological studies of time-to-event data, a quantity of interest to\nthe clinician and the patient is the risk of an event given a covariate\nprofile. However, methods relying on time matching or risk-set sampling\n(including Cox regression) eliminate the baseline hazard from the likelihood\nexpression or the estimating function. The baseline hazard then needs to be\nestimated separately using a non-parametric approach. This leads to step-wise\nestimates of the cumulative incidence that are difficult to interpret. Using\ncase-base sampling, Hanley & Miettinen (2009) explained how the parametric\nhazard functions can be estimated using logistic regression. Their approach\nnaturally leads to estimates of the cumulative incidence that are\nsmooth-in-time. In this paper, we present the casebase R package, a\ncomprehensive and flexible toolkit for parametric survival analysis. We\ndescribe how the case-base framework can also be used in more complex settings:\ncompeting risks, time-varying exposure, and variable selection. Our package\nalso includes an extensive array of visualization tools to complement the\nanalysis of time-to-event data. We illustrate all these features through four\ndifferent case studies. *SRB and MT contributed equally to this work.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:30:48 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Bhatnagar", "Sahir Rai", ""], ["Turgeon", "Maxime", ""], ["Islam", "Jesse", ""], ["Hanley", "James A.", ""], ["Saarela", "Olli", ""]]}, {"id": "2009.10265", "submitter": "Zhengyang Zhou", "authors": "Zhengyang Zhou, Minge Xie, David Huh, Eun-Young Mun", "title": "A Bias Correction Method in Meta-analysis of Randomized Clinical Trials\n  with no Adjustments for Zero-inflated Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clinical endpoint measures, such as the number of standard drinks\nconsumed per week or the number of days that patients stayed in the hospital,\nare count data with excessive zeros. However, the zero-inflated nature of such\noutcomes is sometimes ignored in analyses of clinical trials. This leads to\nbiased estimates of study-level intervention effect and, consequently, a biased\nestimate of the overall intervention effect in a meta-analysis. The current\nstudy proposes a novel statistical approach, the Zero-inflation Bias Correction\n(ZIBC) method, that can account for the bias introduced when using the Poisson\nregression model, despite a high rate of inflated zeros in the outcome\ndistribution of a randomized clinical trial. This correction method only\nrequires summary information from individual studies to correct intervention\neffect estimates as if they were appropriately estimated using the\nzero-inflated Poisson regression model, thus it is attractive for meta-analysis\nwhen individual participant-level data are not available in some studies.\nSimulation studies and real data analyses showed that the ZIBC method performed\nwell in correcting zero-inflation bias in most situations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:32:15 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 02:05:23 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 18:32:53 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Zhengyang", ""], ["Xie", "Minge", ""], ["Huh", "David", ""], ["Mun", "Eun-Young", ""]]}, {"id": "2009.10289", "submitter": "Mylene Bedard", "authors": "D.A.S. Fraser and Myl\\`ene B\\'edard", "title": "The Linear Lasso: a location model resolution", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use location model methodology to guide the least squares analysis of the\nLasso problem of variable selection and inference. The nuisance parameter is\ntaken to be an indicator for the selection of explanatory variables and the\ninterest parameter is the response variable itself. Recent theory eliminates\nthe nuisance parameter by marginalization on the data space and then uses the\nresulting distribution for inference concerning the interest parameter. We\ndevelop this approach and find: that primary inference is essentially\none-dimensional rather than $n$-dimensional; that inference focuses on the\nresponse variable itself rather than the least squares estimate (as variables\nare removed); that first order probabilities are available; that computation is\nrelatively easy; that a scalar marginal model is available; and that\nineffective variables can be removed by distributional tilt or shift.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:47:52 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Fraser", "D. A. S.", ""], ["B\u00e9dard", "Myl\u00e8ne", ""]]}, {"id": "2009.10291", "submitter": "Xuejun Ma X.J. Ma", "authors": "Xuejun Ma, Yue Du and Jingli Wang", "title": "Model detection and variable selection for mode varying coefficient\n  model", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient model is often used in statistical modeling since it is\nmore flexible than the parametric model. However, model detection and variable\nselection of varying coefficient model are poorly understood in mode\nregression. Existing methods in the literature for these problems often based\non mean regression and quantile regression. In this paper, we propose a novel\nmethod to solve these problems for mode varying coefficient model based on the\nB-spline approximation and SCAD penalty. Moreover, we present a new algorithm\nto estimate the parameters of interest, and discuss the parameters selection\nfor the tuning parameters and bandwidth. We also establish the asymptotic\nproperties of estimated coefficients under some regular conditions. Finally, we\nillustrate the proposed method by some simulation studies and an empirical\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 02:48:53 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ma", "Xuejun", ""], ["Du", "Yue", ""], ["Wang", "Jingli", ""]]}, {"id": "2009.10303", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Olivier Zahm, Youssef Marzouk", "title": "An adaptive transport framework for joint and conditional density\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework to robustly characterize joint and conditional\nprobability distributions via transport maps. Transport maps or \"flows\"\ndeterministically couple two distributions via an expressive monotone\ntransformation. Yet, learning the parameters of such transformations in high\ndimensions is challenging given few samples from the unknown target\ndistribution, and structural choices for these transformations can have a\nsignificant impact on performance. Here we formulate a systematic framework for\nrepresenting and learning monotone maps, via invertible transformations of\nsmooth functions, and demonstrate that the associated minimization problem has\na unique global optimum. Given a hierarchical basis for the appropriate\nfunction space, we propose a sample-efficient adaptive algorithm that estimates\na sparse approximation for the map. We demonstrate how this framework can learn\ndensities with stable generalization performance across a wide range of sample\nsizes on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 03:41:45 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Baptista", "Ricardo", ""], ["Zahm", "Olivier", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2009.10332", "submitter": "Maxwell Cairns", "authors": "Maxwell Cairns and Luke Prendergast", "title": "On ratio measures of population heterogeneity for meta-analyses", "comments": "Corrections made to Table 5 (V2), Updates to boxplot and table 4 (V3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Popular measures of meta-analysis heterogeneity, such as $I^2$, cannot be\nconsidered measures of population heterogeneity since they are dependant on\nsamples sizes within studies. The coefficient of variation (CV) recently\nintroduced and defined to be the heterogeneity variance divided by the absolute\nvalue of the overall mean effect does not suffer such shortcomings. However,\nvery large CV values can occur when the effect is small making interpretation\ndifficult. The purpose of this paper is two-fold. Firstly, we consider variants\nof the CV that exist in the interval (0, 1] making interpretation simpler.\nSecondly, we provide interval estimators for the CV and its variants with\nexcellent coverage properties. We perform simulation studies based on simulated\nand real data sets and draw comparisons between the methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 05:55:15 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 10:45:49 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 02:26:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Cairns", "Maxwell", ""], ["Prendergast", "Luke", ""]]}, {"id": "2009.10622", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Hien D Nguyen, Faicel Chamroukhi and Geoffrey J\n  McLachlan", "title": "An $l_1$-oracle inequality for the Lasso in mixture-of-experts\n  regression models", "comments": "Corrected typos. Added new Section 4. Discussion and comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture-of-experts (MoE) models are a popular framework for modeling\nheterogeneity in data, for both regression and classification problems in\nstatistics and machine learning, due to their flexibility and the abundance of\nstatistical estimation and model choice tools. Such flexibility comes from\nallowing the mixture weights (or gating functions) in the MoE model to depend\non the explanatory variables, along with the experts (or component densities).\nThis permits the modeling of data arising from more complex data generating\nprocesses, compared to the classical finite mixtures and finite mixtures of\nregression models, whose mixing parameters are independent of the covariates.\nThe use of MoE models in a high-dimensional setting, when the number of\nexplanatory variables can be much larger than the sample size (i.e., $p\\gg n$),\nis challenging from a computational point of view, and in particular from a\ntheoretical point of view, where the literature is still lacking results in\ndealing with the curse of dimensionality, in both the statistical estimation\nand feature selection. We consider the finite mixture-of-experts model with\nsoft-max gating functions and Gaussian experts for high-dimensional regression\non heterogeneous data, and its $l_1$-regularized estimation via the Lasso. We\nfocus on the Lasso estimation properties rather than its feature selection\nproperties. We provide a lower bound on the regularization parameter of the\nLasso function that ensures an $l_1$-oracle inequality satisfied by the Lasso\nestimator according to the Kullback-Leibler loss.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:23:35 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 17:28:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Nguyen", "Hien D", ""], ["Chamroukhi", "Faicel", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "2009.10780", "submitter": "Tin Nguyen", "authors": "Tin D. Nguyen, Jonathan Huggins, Lorenzo Masoero, Lester Mackey,\n  Tamara Broderick", "title": "Independent finite approximations for Bayesian nonparametric inference", "comments": "Updated funding acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric priors based on completely random measures (CRMs)\noffer a flexible modeling approach when the number of latent components in a\ndataset is unknown. However, managing the infinite dimensionality of CRMs\ntypically requires practitioners to derive ad-hoc algorithms, preventing the\nuse of general-purpose inference methods and often leading to long compute\ntimes. We propose a general but explicit recipe to construct a simple\nfinite-dimensional approximation that can replace the infinite-dimensional\nCRMs. Our independent finite approximation (IFA) is a generalization of\nimportant cases that are used in practice. The independence of atom weights in\nour approximation (i) makes the construction well-suited for parallel and\ndistributed computation and (ii) facilitates more convenient inference schemes.\nWe quantify the approximation error between IFAs and the target nonparametric\nprior. We compare IFAs with an alternative approximation scheme -- truncated\nfinite approximations (TFAs), where the atom weights are constructed\nsequentially. We prove that, for worst-case choices of observation likelihoods,\nTFAs are a more efficient approximation than IFAs. However, in real-data\nexperiments with image denoising and topic modeling, we find that IFAs perform\nvery similarly to TFAs in terms of task-specific accuracy metrics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:37:21 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 20:33:53 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 22:36:57 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Nguyen", "Tin D.", ""], ["Huggins", "Jonathan", ""], ["Masoero", "Lorenzo", ""], ["Mackey", "Lester", ""], ["Broderick", "Tamara", ""]]}, {"id": "2009.10826", "submitter": "Larissa Matos", "authors": "Francisco H. C. de Alencar and Christian E. Galarza and Larissa A.\n  Matos and Victor H. Lachos", "title": "Finite mixture modeling of censored and missing data using the\n  multivariate skew-normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture models have been widely used to model and analyze data from a\nheterogeneous populations. Moreover, data of this kind can be missing or\nsubject to some upper and/or lower detection limits because of the restriction\nof experimental apparatuses. Another complication arises when measures of each\npopulation depart significantly from normality, for instance, asymmetric\nbehavior. For such data structures, we propose a robust model for censored\nand/or missing data based on finite mixtures of multivariate skew-normal\ndistributions. This approach allows us to model data with great flexibility,\naccommodating multimodality and skewness, simultaneously, depending on the\nstructure of the mixture components. We develop an analytically simple, yet\nefficient, EM- type algorithm for conducting maximum likelihood estimation of\nthe parameters. The algorithm has closed-form expressions at the E-step that\nrely on formulas for the mean and variance of the truncated multivariate\nskew-normal distributions. Furthermore, a general information-based method for\napproximating the asymptotic covariance matrix of the estimators is also\npresented. Results obtained from the analysis of both simulated and real\ndatasets are reported to demonstrate the effectiveness of the proposed method.\nThe proposed algorithm and method are implemented in the new R package CensMFM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 21:26:44 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["de Alencar", "Francisco H. C.", ""], ["Galarza", "Christian E.", ""], ["Matos", "Larissa A.", ""], ["Lachos", "Victor H.", ""]]}, {"id": "2009.10839", "submitter": "Arman Oganisian", "authors": "Arman Oganisian and Nandita Mitra and Jason Roy", "title": "Hierarchical Bayesian Bootstrap for Heterogeneous Treatment Effect\n  Estimation", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major focus of causal inference is the estimation of heterogeneous average\ntreatment effects (HTE) - average treatment effects within strata of another\nvariable of interest. This involves estimating a stratum-specific regression\nand integrating it over the distribution of confounders in that stratum - which\nitself must be estimated. Standard practice in the Bayesian causal literature\nis to use Rubin's Bayesian bootstrap to estimate these stratum-specific\nconfounder distributions independently. However, this becomes problematic for\nsparsely populated strata with few unique observed confounder vectors. By\nconstruction, the Bayesian bootstrap allocates no prior mass on confounder\nvalues unobserved within each stratum - even if these values are observed in\nother strata and we think they are a priori plausible. We propose causal\nestimation via a hierarchical Bayesian bootstrap (HBB) prior over the\nstratum-specific confounder distributions. Based on the Hierarchical Dirichlet\nProcess, the HBB partially pools the stratum-specific confounder distributions\nby assuming all confounder vectors seen in the overall sample are plausible. In\nlarge strata, estimates allocate much of the mass to values seen within the\nstrata, while placing small non-zero mass on unseen values. However, for sparse\nstrata, more weight is given to values unseen in that stratum but seen\nelsewhere - thus shrinking the distribution towards the marginal. This allows\nus to borrow information across strata when estimating HTEs - leading to\nefficiency gains over standard marginalization approaches while avoiding strong\nparametric modeling assumptions about the confounder distribution when\nestimating HTEs. Moreover, the HBB is computationally efficient (due to\nconjugacy) and compatible with arbitrary outcome models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 22:14:18 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Oganisian", "Arman", ""], ["Mitra", "Nandita", ""], ["Roy", "Jason", ""]]}, {"id": "2009.10922", "submitter": "Ximing Xu", "authors": "Libai Xu, Ximing Xu, Dehan Kong, Hong Gu, Toby Kenney", "title": "Stochastic Generalized Lotka-Volterra Model with An Application to\n  Learning Microbial Community Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring microbial community structure based on temporal metagenomics data\nis an important goal in microbiome studies. The deterministic generalized\nLotka-Volterra differential (GLV) equations have been used to model the\ndynamics of microbial data. However, these approaches fail to take random\nenvironmental fluctuations into account, which may negatively impact the\nestimates. We propose a new stochastic GLV (SGLV) differential equation model,\nwhere the random perturbations of Brownian motion in the model can naturally\naccount for the external environmental effects on the microbial community. We\nestablish new conditions and show various mathematical properties of the\nsolutions including general existence and uniqueness, stationary distribution,\nand ergodicity. We further develop approximate maximum likelihood estimators\nbased on discrete observations and systematically investigate the consistency\nand asymptotic normality of the proposed estimators. Our method is demonstrated\nthrough simulation studies and an application to the well-known \"moving\npicture\" temporal microbial dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 03:56:33 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Xu", "Libai", ""], ["Xu", "Ximing", ""], ["Kong", "Dehan", ""], ["Gu", "Hong", ""], ["Kenney", "Toby", ""]]}, {"id": "2009.10982", "submitter": "Eric Tchetgen Tchetgen", "authors": "Eric J Tchetgen Tchetgen, Andrew Ying, Yifan Cui, Xu Shi, Wang Miao", "title": "An Introduction to Proximal Causal Learning", "comments": "This paper was originally presented by the first author at the 2020\n  Myrto Lefkopoulou Distinguished Lectureship at the Harvard T. H. Chan School\n  of Public Health on September 17th 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard assumption for causal inference from observational data is that\none has measured a sufficiently rich set of covariates to ensure that within\ncovariate strata, subjects are exchangeable across observed treatment values.\nSkepticism about the exchangeability assumption in observational studies is\noften warranted because it hinges on investigators' ability to accurately\nmeasure covariates capturing all potential sources of confounding.\nRealistically, confounding mechanisms can rarely if ever, be learned with\ncertainty from measured covariates. One can therefore only ever hope that\ncovariate measurements are at best proxies of true underlying confounding\nmechanisms operating in an observational study, thus invalidating causal claims\nmade on basis of standard exchangeability conditions. Causal learning from\nproxies is a challenging inverse problem which has to date remained unresolved.\nIn this paper, we introduce a formal potential outcome framework for proximal\ncausal learning, which while explicitly acknowledging covariate measurements as\nimperfect proxies of confounding mechanisms, offers an opportunity to learn\nabout causal effects in settings where exchangeability on the basis of measured\ncovariates fails. Sufficient conditions for nonparametric identification are\ngiven, leading to the proximal g-formula and corresponding proximal\ng-computation algorithm for estimation. These may be viewed as generalizations\nof Robins' foundational g-formula and g-computation algorithm, which account\nexplicitly for bias due to unmeasured confounding. Both point treatment and\ntime-varying treatment settings are considered, and an application of proximal\ng-computation of causal effects is given for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:46:53 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Tchetgen", "Eric J Tchetgen", ""], ["Ying", "Andrew", ""], ["Cui", "Yifan", ""], ["Shi", "Xu", ""], ["Miao", "Wang", ""]]}, {"id": "2009.11060", "submitter": "Luke Oakden-Rayner", "authors": "Luke Oakden-Rayner, Lyle Palmer", "title": "Docs are ROCs: A simple off-the-shelf approach for estimating average\n  human performance in diagnostic studies", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating average human performance has been performed inconsistently in\nresearch in diagnostic medicine. This has been particularly apparent in the\nfield of medical artificial intelligence, where humans are often compared\nagainst AI models in multi-reader multi-case studies, and commonly reported\nmetrics such as the pooled or average human sensitivity and specificity will\nsystematically underestimate the performance of human experts. We present the\nuse of summary receiver operating characteristic curve analysis, a technique\ncommonly used in the meta-analysis of diagnostic test accuracy studies, as a\nsensible and methodologically robust alternative. We describe the motivation\nfor using these methods and present results where we apply these meta-analytic\ntechniques to a handful of prominent medical AI studies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 11:27:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 03:44:16 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Oakden-Rayner", "Luke", ""], ["Palmer", "Lyle", ""]]}, {"id": "2009.11098", "submitter": "Whitney Huang", "authors": "Brook T. Russell and Whitney K. Huang", "title": "Modeling short-ranged dependence in block extrema with application to\n  polar temperature data", "comments": "40 pages, 8 figures, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block maxima approach is an important method in univariate extreme value\nanalysis. While assuming that block maxima are independent results in\nstraightforward analysis, the resulting inferences maybe invalid when a series\nof block maxima exhibits dependence. We propose a model, based on a first-order\nMarkov assumption, that incorporates dependence between successive block maxima\nthrough the use of a bivariate logistic dependence structure while maintaining\ngeneralized extreme value (GEV) marginal distributions. Modeling dependence in\nthis manner allows us to better estimate extreme quantiles when block maxima\nexhibit short-ranged dependence. We demonstrate via a simulation study that our\nfirst-order Markov GEV model performs well when successive block maxima are\ndependent, while still being reasonably robust when maxima are independent. We\napply our method to two polar annual minimum air temperature data sets that\nexhibit short-ranged dependence structures, and find that the proposed model\nyields modified estimates of high quantiles.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 12:30:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Russell", "Brook T.", ""], ["Huang", "Whitney K.", ""]]}, {"id": "2009.11134", "submitter": "Chris McKennan", "authors": "Chris McKennan", "title": "Factor analysis in high dimensional biological data with dependent\n  observations", "comments": "21 pages of main text, 85 with supplement; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis is a critical component of high dimensional biological data\nanalysis. However, modern biological data contain two key features that\nirrevocably corrupt existing methods. First, these data, which include\nlongitudinal, multi-treatment and multi-tissue data, contain samples that break\ncritical independence requirements necessary for the utilization of prevailing\nmethods. Second, biological data contain factors with large, moderate and small\nsignal strengths, and therefore violate the ubiquitous \"pervasive factor\"\nassumption essential to the performance of many methods. In this work, I\ndevelop a novel statistical framework to perform factor analysis and interpret\nits results in data with dependent observations and factors whose signal\nstrengths span several orders of magnitude. I then prove that my methodology\ncan be used to solve many important and previously unsolved problems that\nroutinely arise when analyzing dependent biological data, including high\ndimensional covariance estimation, subspace recovery, latent factor\ninterpretation and data denoising. Additionally, I show that my estimator for\nthe number of factors overcomes both the notorious \"eigenvalue shadowing\"\nproblem, as well as the biases due to the pervasive factor assumption that\nplague existing estimators. Simulated and real data demonstrate the superior\nperformance of my methodology in practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:23:29 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["McKennan", "Chris", ""]]}, {"id": "2009.11242", "submitter": "Chieh Wu T", "authors": "Shi Dong, Zlatan Feric, Guangyu Li, Chieh Wu, April Z. Gu, Jennifer\n  Dy, John Meeker, Ingrid Y. Padilla, Jose Cordero, Carmen Velez Vega, Zaira\n  Rosario, Akram Alshawabkeh, David Kaeli", "title": "Using Undersampling with Ensemble Learning to Identify Factors\n  Contributing to Preterm Birth", "comments": null, "journal-ref": "ICMLA 2020", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Ensemble Learning models to identify factors\ncontributing to preterm birth. Our work leverages a rich dataset collected by a\nNIEHS P42 Center that is trying to identify the dominant factors responsible\nfor the high rate of premature births in northern Puerto Rico. We investigate\nanalytical models addressing two major challenges present in the dataset: 1)\nthe significant amount of incomplete data in the dataset, and 2) class\nimbalance in the dataset. First, we leverage and compare two types of missing\ndata imputation methods: 1) mean-based and 2) similarity-based, increasing the\ncompleteness of this dataset. Second, we propose a feature selection and\nevaluation model based on using undersampling with Ensemble Learning to address\nclass imbalance present in the dataset. We leverage and compare multiple\nEnsemble Feature selection methods, including Complete Linear Aggregation\n(CLA), Weighted Mean Aggregation (WMA), Feature Occurrence Frequency (OFA), and\nClassification Accuracy Based Aggregation (CAA). To further address missing\ndata present in each feature, we propose two novel methods: 1) Missing Data\nRate and Accuracy Based Aggregation (MAA), and 2) Entropy and Accuracy Based\nAggregation (EAA). Both proposed models balance the degree of data variance\nintroduced by the missing data handling during the feature selection process\nwhile maintaining model performance. Our results show a 42\\% improvement in\nsensitivity versus fallout over previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 16:32:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Dong", "Shi", ""], ["Feric", "Zlatan", ""], ["Li", "Guangyu", ""], ["Wu", "Chieh", ""], ["Gu", "April Z.", ""], ["Dy", "Jennifer", ""], ["Meeker", "John", ""], ["Padilla", "Ingrid Y.", ""], ["Cordero", "Jose", ""], ["Vega", "Carmen Velez", ""], ["Rosario", "Zaira", ""], ["Alshawabkeh", "Akram", ""], ["Kaeli", "David", ""]]}, {"id": "2009.11257", "submitter": "Federico Camerlenghi", "authors": "Fadhel Ayed, Marco Battiston, Federico Camerlenghi", "title": "An Information Theoretic approach to Post Randomization Methods under\n  Differential Privacy", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-020-09949-3", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post Randomization Methods (PRAM) are among the most popular disclosure\nlimitation techniques for both categorical and continuous data. In the\ncategorical case, given a stochastic matrix $M$ and a specified variable, an\nindividual belonging to category $i$ is changed to category $j$ with\nprobability $M_{i,j}$. Every approach to choose the randomization matrix $M$\nhas to balance between two desiderata: 1) preserving as much statistical\ninformation from the raw data as possible; 2) guaranteeing the privacy of\nindividuals in the dataset. This trade-off has generally been shown to be very\nchallenging to solve. In this work, we use recent tools from the computer\nscience literature and propose to choose $M$ as the solution of a constrained\nmaximization problems. Specifically, $M$ is chosen as the solution of a\nconstrained maximization problem, where we maximize the Mutual Information\nbetween raw and transformed data, given the constraint that the transformation\nsatisfies the notion of Differential Privacy. For the general Categorical\nmodel, it is shown how this maximization problem reduces to a convex linear\nprogramming and can be therefore solved with known optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 17:08:09 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ayed", "Fadhel", ""], ["Battiston", "Marco", ""], ["Camerlenghi", "Federico", ""]]}, {"id": "2009.11401", "submitter": "Sharmistha Guha", "authors": "Sharmistha Guha and Abel Rodriguez", "title": "High Dimensional Bayesian Network Classification with Network\n  Global-Local Shrinkage Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel Bayesian classification framework for networks\nwith labeled nodes. While literature on statistical modeling of network data\ntypically involves analysis of a single network, the recent emergence of\ncomplex data in several biological applications, including brain imaging\nstudies, presents a need to devise a network classifier for subjects. This\narticle considers an application from a brain connectome study, where the\noverarching goal is to classify subjects into two separate groups based on\ntheir brain network data, along with identifying influential regions of\ninterest (ROIs) (referred to as nodes). Existing approaches either treat all\nedge weights as a long vector or summarize the network information with a few\nsummary measures. Both these approaches ignore the full network structure, may\nlead to less desirable inference in small samples and are not designed to\nidentify significant network nodes. We propose a novel binary logistic\nregression framework with the network as the predictor and a binary response,\nthe network predictor coefficient being modeled using a novel class\nglobal-local shrinkage priors. The framework is able to accurately detect nodes\nand edges in the network influencing the classification. Our framework is\nimplemented using an efficient Markov Chain Monte Carlo algorithm.\nTheoretically, we show asymptotically optimal classification for the proposed\nframework when the number of network edges grows faster than the sample size.\nThe framework is empirically validated by extensive simulation studies and\nanalysis of a brain connectome data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:21:07 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Guha", "Sharmistha", ""], ["Rodriguez", "Abel", ""]]}, {"id": "2009.11428", "submitter": "Shiqing Yu", "authors": "Shiqing Yu, Mathias Drton, Ali Shojaie", "title": "Generalized Score Matching for General Domains", "comments": "50 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of density functions supported on general domains arises when the\ndata is naturally restricted to a proper subset of the real space. This problem\nis complicated by typically intractable normalizing constants. Score matching\nprovides a powerful tool for estimating densities with such intractable\nnormalizing constants, but as originally proposed is limited to densities on\n$\\mathbb{R}^m$ and $\\mathbb{R}_+^m$. In this paper, we offer a natural\ngeneralization of score matching that accommodates densities supported on a\nvery general class of domains. We apply the framework to truncated graphical\nand pairwise interaction models, and provide theoretical guarantees for the\nresulting estimators. We also generalize a recently proposed method from\nbounded to unbounded domains, and empirically demonstrate the advantages of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 00:53:04 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Yu", "Shiqing", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2009.11452", "submitter": "Xiaoke Zhang", "authors": "Rui Miao, Xiaoke Zhang, Raymond K. W. Wong", "title": "A Wavelet-Based Independence Test for Functional Data with an\n  Application to MEG Functional Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring and testing the dependency between multiple random functions is\noften an important task in functional data analysis. In the literature, a\nmodel-based method relies on a model which is subject to the risk of model\nmisspecification, while a model-free method only provides a correlation measure\nwhich is inadequate to test independence. In this paper, we adopt the\nHilbert-Schmidt Independence Criterion (HSIC) to measure the dependency between\ntwo random functions. We develop a two-step procedure by first pre-smoothing\neach function based on its discrete and noisy measurements and then applying\nthe HSIC to recovered functions. To ensure the compatibility between the two\nsteps such that the effect of the pre-smoothing error on the subsequent HSIC is\nasymptotically negligible, we propose to use wavelet soft-thresholding for\npre-smoothing and Besov-norm-induced kernels for HSIC. We also provide the\ncorresponding asymptotic analysis. The superior numerical performance of the\nproposed method over existing ones is demonstrated in a simulation study.\nMoreover, in an magnetoencephalography (MEG) data application, the functional\nconnectivity patterns identified by the proposed method are more anatomically\ninterpretable than those by existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 02:21:37 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Miao", "Rui", ""], ["Zhang", "Xiaoke", ""], ["Wong", "Raymond K. W.", ""]]}, {"id": "2009.11499", "submitter": "Dorota Toczydlowska", "authors": "Dorota Toczydlowska, Gareth W. Peters, Pavel V. Shevchenko", "title": "Parsimonious Feature Extraction Methods: Extending Robust Probabilistic\n  Projections with Generalized Skew-t", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generalisation to the Student-t Probabilistic Principal\nComponent methodology which: (1) accounts for an asymmetric distribution of the\nobservation data; (2) is a framework for grouped and generalised\nmultiple-degree-of-freedom structures, which provides a more flexible approach\nto modelling groups of marginal tail dependence in the observation data; and\n(3) separates the tail effect of the error terms and factors. The new feature\nextraction methods are derived in an incomplete data setting to efficiently\nhandle the presence of missing values in the observation vector. We discuss\nvarious special cases of the algorithm being a result of simplified assumptions\non the process generating the data. The applicability of the new framework is\nillustrated on a data set that consists of crypto currencies with the highest\nmarket capitalisation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 05:53:41 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Toczydlowska", "Dorota", ""], ["Peters", "Gareth W.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "2009.11747", "submitter": "Shihao Wu", "authors": "Shihao Wu, Zhe Li and Xuening Zhu", "title": "Distributed Community Detection for Large Scale Networks Using\n  Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid developments of information and technology, large scale network\ndata are ubiquitous. In this work we develop a distributed spectral clustering\nalgorithm for community detection in large scale networks. To handle the\nproblem, we distribute l pilot network nodes on the master server and the\nothers on worker servers. A spectral clustering algorithm is first conducted on\nthe master to select pseudo centers. The indexes of the pseudo centers are then\nbroadcasted to workers to complete distributed community detection task using a\nSVD type algorithm. The proposed distributed algorithm has three merits. First,\nthe communication cost is low since only the indexes of pseudo centers are\ncommunicated. Second, no further iteration algorithm is needed on workers and\nhence it does not suffer from problems as initialization and non-robustness.\nThird, both the computational complexity and the storage requirements are much\nlower compared to using the whole adjacency matrix. A Python package DCD\n(www.github.com/Ikerlz/dcd) is developed to implement the distributed algorithm\nfor a Spark system. Theoretical properties are provided with respect to the\nestimation accuracy and mis-clustering rates. Lastly, the advantages of the\nproposed methodology are illustrated by experiments on a variety of synthetic\nand empirical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:17:23 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:57:39 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 09:46:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Shihao", ""], ["Li", "Zhe", ""], ["Zhu", "Xuening", ""]]}, {"id": "2009.11772", "submitter": "Farhad Pishgar", "authors": "Farhad Pishgar, Noah Greifer, Cl\\'emence Leyrat, Elizabeth Stuart", "title": "MatchThem:: Matching and Weighting after Multiple Imputation", "comments": "23 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing the distributions of the confounders across the exposure levels in\nan observational study through matching or weighting is an accepted method to\ncontrol for confounding due to these variables when estimating the association\nbetween an exposure and outcome and to reduce the degree of dependence on\ncertain modeling assumptions. Despite the increasing popularity in practice,\nthese procedures cannot be immediately applied to datasets with missing values.\nMultiple imputation of the missing data is a popular approach to account for\nmissing values while preserving the number of units in the dataset and\naccounting for the uncertainty in the missing values. However, to the best of\nour knowledge, there is no comprehensive matching and weighting software that\ncan be easily implemented with multiply imputed datasets. In this paper, we\nreview this problem and suggest a framework to map out the matching and\nweighting multiply imputed datasets to 5 actions as well as the best practices\nto assess balance in these datasets after matching and weighting. We also\nillustrate these approaches using a companion package for R, MatchThem.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:58:16 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Pishgar", "Farhad", ""], ["Greifer", "Noah", ""], ["Leyrat", "Cl\u00e9mence", ""], ["Stuart", "Elizabeth", ""]]}, {"id": "2009.11797", "submitter": "Rebecca Bergee", "authors": "Rebecca E. Atanga, Edward L. Boone, Ryad A. Ghanam, Ben Stewart-Koster", "title": "Optimal Sampling Regimes for Estimating Population Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecologists are interested in modeling the population growth of species in\nvarious ecosystems. Studying population dynamics can assist environmental\nmanagers in making better decisions for the environment. Traditionally, the\nsampling of species and tracking of populations have been recorded on a regular\ntime frequency. However, sampling can be an expensive process due to available\nresources, money and time. Limiting sampling makes it challenging to properly\ntrack the growth of a population. Thus, we propose a new and novel approach to\ndesigning sampling regimes based on the dynamics associated with population\ngrowth models. This design study minimizes the amount of time ecologists spend\nin the field, while maximizing the information provided by the data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:39:15 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 21:53:45 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Atanga", "Rebecca E.", ""], ["Boone", "Edward L.", ""], ["Ghanam", "Ryad A.", ""], ["Stewart-Koster", "Ben", ""]]}, {"id": "2009.11808", "submitter": "Christopher James Rose", "authors": "Christopher James Rose, Unni Olsen, Maren Falch Lindberg, Eva\n  Marie-Louise Denison, Arild Aamodt, Anners Lerdal", "title": "A new multivariate meta-analysis model for many variates and few studies", "comments": "This version adds a simulation study comparing bias, variance, and\n  coverage to univariate meta-analysis, and adds a decision tool to help\n  analysts choose between methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies often estimate associations between an outcome and multiple variates.\nFor example, studies of diagnostic test accuracy estimate sensitivity and\nspecificity, and studies of predictive and prognostic factors typically\nestimate associations for multiple factors. Meta-analysis is a family of\nstatistical methods for synthesizing estimates across multiple studies.\nMultivariate models exist that account for within-study correlations and\nbetween-study heterogeneity. The number of parameters that must be estimated in\nexisting models is quadratic in the number of variates (e.g., risk factors).\nThis means they may not be usable if data are sparse with many variates and few\nstudies. We propose a new model that addresses this problem by approximating a\nvariance-covariance matrix that models within-study correlation and\nbetween-study heterogeneity in a low-dimensional space using random projection.\nThe number of parameters that must be estimated in this model scales linearly\nin the number of variates and quadratically in the dimension of the\napproximating space, making estimation more tractable. We performed a\nsimulation study to compare coverage, bias, and precision of estimates made\nusing the proposed model to those from univariate meta-analyses. We demonstrate\nthe method using data from an ongoing systematic review on predictors of pain\nand function after total knee arthroplasty. Finally, we suggest a decision tool\nto help analysts choose among available models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:49:29 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:38:58 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 12:27:49 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 11:07:33 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Rose", "Christopher James", ""], ["Olsen", "Unni", ""], ["Lindberg", "Maren Falch", ""], ["Denison", "Eva Marie-Louise", ""], ["Aamodt", "Arild", ""], ["Lerdal", "Anners", ""]]}, {"id": "2009.11828", "submitter": "Ting Ye", "authors": "Ting Ye and Jun Shao and Yanyao Yi and Qingyuan Zhao", "title": "Toward Better Practice of Covariate Adjustment in Analyzing Randomized\n  Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In randomized clinical trials, adjustments for baseline covariates at both\ndesign and analysis stages are highly encouraged by regulatory agencies. A\nrecent trend is to use a model-assisted approach for covariate adjustment to\ngain credibility and efficiency while producing asymptotically valid inference\neven when the model is incorrect. In this article we present three\nconsiderations for better practice when model-assisted inference is applied to\nadjust for covariates under simple or covariate-adaptive randomized trials: (1)\nguaranteed efficiency gain: a model-assisted method should often gain but never\nhurt efficiency; (2) wide applicability: a valid procedure should be\napplicable, and preferably universally applicable, to all commonly used\nrandomization schemes; (3) robust standard error: variance estimation should be\nrobust to model misspecification and heteroscedasticity. To achieve these, we\nrecommend a model-assisted estimator under an analysis of heterogeneous\ncovariance working model including all covariates utilized in randomization.\nOur conclusions are based on an asymptotic theory that provides a clear picture\nof how covariate-adaptive randomization and regression adjustment alter\nstatistical efficiency. Our theory is more general than the existing ones in\nterms of studying arbitrary functions of response means (including linear\ncontrasts, ratios, and odds ratios), multiple arms, guaranteed efficiency gain,\noptimality, and universal applicability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 17:22:30 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 04:57:17 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ye", "Ting", ""], ["Shao", "Jun", ""], ["Yi", "Yanyao", ""], ["Zhao", "Qingyuan", ""]]}, {"id": "2009.11891", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Yajun Mei", "title": "Bandit Change-Point Detection for Real-Time Monitoring High-Dimensional\n  Data Under Sampling Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems of real-time monitoring high-dimensional\nstreaming data, one wants to detect an undesired event or change quickly once\nit occurs, but under the sampling control constraint in the sense that one\nmight be able to only observe or use selected components data for\ndecision-making per time step in the resource-constrained environments. In this\npaper, we propose to incorporate multi-armed bandit approaches into sequential\nchange-point detection to develop an efficient bandit change-point detection\nalgorithm. Our proposed algorithm, termed\nThompson-Sampling-Shiryaev-Roberts-Pollak (TSSRP), consists of two policies per\ntime step: the adaptive sampling policy applies the Thompson Sampling algorithm\nto balance between exploration for acquiring long-term knowledge and\nexploitation for immediate reward gain, and the statistical decision policy\nfuses the local Shiryaev-Roberts-Pollak statistics to determine whether to\nraise a global alarm by sum shrinkage techniques. Extensive numerical\nsimulations and case studies demonstrate the statistical and computational\nefficiency of our proposed TSSRP algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 18:30:55 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zhang", "Wanrong", ""], ["Mei", "Yajun", ""]]}, {"id": "2009.11993", "submitter": "Paul Gustafson", "authors": "Paul Gustafson", "title": "Parameter Restrictions for the Sake of Identification: Is there Utility\n  in Asserting that Perhaps a Restriction Holds?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling can involve a tension between assumptions and\nstatistical identification. The law of the observable data may not uniquely\ndetermine the value of a target parameter without invoking a key assumption,\nand, while plausible, this assumption may not be obviously true in the\nscientific context at hand. Moreover, there are many instances of key\nassumptions which are untestable, hence we cannot rely on the data to resolve\nthe question of whether the target is legitimately identified. Working in the\nBayesian paradigm, we consider the grey zone of situations where a key\nassumption, in the form of a parameter space restriction, is scientifically\nreasonable but not incontrovertible for the problem being tackled.\nSpecifically, we investigate statistical properties that ensue if we structure\na prior distribution to assert that `maybe' or `perhaps' the assumption holds.\nTechnically this simply devolves to using a mixture prior distribution putting\njust some prior weight on the assumption, or one of several assumptions,\nholding. However, while the construct is straightforward, there is very little\nliterature discussing situations where Bayesian model averaging is employed\nacross a mix of fully identified and partially identified models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 01:14:21 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Gustafson", "Paul", ""]]}, {"id": "2009.12033", "submitter": "Zhiqiang Tan", "authors": "Satyajit Ghosh and Zhiqiang Tan", "title": "Doubly Robust Semiparametric Inference Using Regularized Calibrated\n  Estimation with High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider semiparametric estimation where a doubly robust estimating function\nfor a low-dimensional parameter is available, depending on two working models.\nWith high-dimensional data, we develop regularized calibrated estimation as a\ngeneral method for estimating the parameters in the two working models, such\nthat valid Wald confidence intervals can be obtained for the parameter of\ninterest under suitable sparsity conditions if either of the two working models\nis correctly specified. We propose a computationally tractable two-step\nalgorithm and provide rigorous theoretical analysis which justifies\nsufficiently fast rates of convergence for the regularized calibrated\nestimators in spite of sequential construction and establishes a desired\nasymptotic expansion for the doubly robust estimator. As concrete examples, we\ndiscuss applications to partially linear, log-linear, and logistic models and\nestimation of average treatment effects. Numerical studies in the former three\nexamples demonstrate superior performance of our method, compared with debiased\nLasso.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 04:30:33 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Ghosh", "Satyajit", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "2009.12052", "submitter": "Hege Michiels", "authors": "Hege Michiels, Cristina Sotto, An Vandebosch, Stijn Vansteelandt", "title": "A novel estimand to adjust for rescue treatment in clinical trials", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8901", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of randomised clinical trial results is often complicated\nby intercurrent events. For instance, rescue medication is sometimes given to\npatients in response to worsening of their disease, either in addition to the\nrandomised treatment or in its place. The use of such medication complicates\nthe interpretation of the intention-to-treat analysis. In view of this, we\npropose a novel estimand defined as the intention-to-treat effect that would\nhave been observed, had patients on the active arm been switched to rescue\nmedication if and only if they would have been switched when randomised to\ncontrol. This enables us to disentangle the treatment effect from the effect of\nrescue medication on a patient's outcome, while avoiding the strong\nextrapolations that are typically needed when inferring what the\nintention-to-treat effect would have been in the absence of rescue medication.\nWe develop an inverse probability weighting method to estimate this estimand\nunder specific untestable assumptions, in view of which we propose a\nsensitivity analysis. We use the method for the analysis of a clinical trial\nconducted by Janssen Pharmaceuticals, in which chronically ill patients can\nswitch to rescue medication for ethical reasons. Monte Carlo simulations\nconfirm that the proposed estimator is unbiased in moderate sample sizes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 06:37:28 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Michiels", "Hege", ""], ["Sotto", "Cristina", ""], ["Vandebosch", "An", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2009.12113", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Lenka Zbo\\v{n}\\'akov\\'a, Ricardo Pio Monti, Wolfgang Karl H\\\"ardle", "title": "Towards the interpretation of time-varying regularization parameters in\n  streaming penalized regression models", "comments": null, "journal-ref": "Pattern Recognition Letters, Volume 125, 1 July 2019, Pages 542 to\n  548", "doi": "10.1016/j.patrec.2019.06.021", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-dimensional, streaming datasets are ubiquitous in modern applications.\nExamples range from finance and e-commerce to the study of biomedical and\nneuroimaging data. As a result, many novel algorithms have been proposed to\naddress challenges posed by such datasets. In this work, we focus on the use of\n$\\ell_1$ regularized linear models in the context of (possibly non-stationary)\nstreaming data Recently, it has been noted that the choice of the\nregularization parameter is fundamental in such models and several methods have\nbeen proposed which iteratively tune such a parameter in a~time-varying manner;\nthereby allowing the underlying sparsity of estimated models to vary. Moreover,\nin many applications, inference on the regularization parameter may itself be\nof interest, as such a parameter is related to the underlying \\textit{sparsity}\nof the model. However, in this work, we highlight and provide extensive\nempirical evidence regarding how various (often unrelated) statistical\nproperties in the data can lead to changes in the regularization parameter. In\nparticular, through various synthetic experiments, we demonstrate that changes\nin the regularization parameter may be driven by changes in the true underlying\nsparsity, signal-to-noise ratio or even model misspecification. The purpose of\nthis letter is, therefore, to highlight and catalog various statistical\nproperties which induce changes in the associated regularization parameter. We\nconclude by presenting two applications: one relating to financial data and\nanother to neuroimaging data, where the aforementioned discussion is relevant.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 10:12:08 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zbo\u0148\u00e1kov\u00e1", "Lenka", ""], ["Monti", "Ricardo Pio", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2009.12217", "submitter": "Fui Swen Kuh", "authors": "F. Swen Kuh, Grace S. Chiu, Anton H. Westveld", "title": "Latent Causal Socioeconomic Health Index", "comments": "31 pages. arXiv admin note: substantial text overlap with\n  arXiv:1911.00512", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research develops a model-based LAtent Causal Socioeconomic Health\n(LACSH) index at the national level. We build upon the latent health factor\nindex (LHFI) approach that has been used to assess the unobservable\necological/ecosystem health. This framework integratively models the\nrelationship between metrics, the latent health, and the covariates that drive\nthe notion of health. In this paper, the LHFI structure is integrated with\nspatial modeling and statistical causal modeling, so as to evaluate the impact\nof a continuous policy variable (mandatory maternity leave days and\ngovernment's expenditure on healthcare, respectively) on a nation's\nsocioeconomic health, while formally accounting for spatial dependency among\nthe nations. A novel visualization technique for evaluating covariate balance\nis also introduced for the case of a continuous policy (treatment) variable. We\napply our LACSH model to countries around the world using data on various\nmetrics and potential covariates pertaining to different aspects of societal\nhealth. The approach is structured in a Bayesian hierarchical framework and\nresults are obtained by Markov chain Monte Carlo techniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 07:00:46 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Kuh", "F. Swen", ""], ["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "2009.12267", "submitter": "Daniele Durante", "authors": "Sirio Legramanti, Tommaso Rigon, Daniele Durante", "title": "Bayesian Testing for Exogenous Partition Structures in Stochastic Block\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data often exhibit block structures characterized by clusters of\nnodes with similar patterns of edge formation. When such relational data are\ncomplemented by additional information on exogenous node partitions, these\nsources of knowledge are typically included in the model to supervise the\ncluster assignment mechanism or to improve inference on edge probabilities.\nAlthough these solutions are routinely implemented, there is a lack of formal\napproaches to test if a given external node partition is in line with the\nendogenous clustering structure encoding stochastic equivalence patterns among\nthe nodes in the network. To fill this gap, we develop a formal Bayesian\ntesting procedure which relies on the calculation of the Bayes factor between a\nstochastic block model with known grouping structure defined by the exogenous\nnode partition and an infinite relational model that allows the endogenous\nclustering configurations to be unknown, random and fully revealed by the\nblock-connectivity patterns in the network. A simple Markov chain Monte Carlo\nmethod for computing the Bayes factor and quantifying uncertainty in the\nendogenous groups is proposed. This routine is evaluated in simulations and in\nan application to study exogenous equivalence structures in brain networks of\nAlzheimer's patients.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:33:59 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Legramanti", "Sirio", ""], ["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""]]}, {"id": "2009.12297", "submitter": "Elad Romanov", "authors": "David L. Donoho, Matan Gavish and Elad Romanov", "title": "ScreeNOT: Exact MSE-Optimal Singular Value Thresholding in Correlated\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a formula for optimal hard thresholding of the singular value\ndecomposition in the presence of correlated additive noise; although it\nnominally involves unobservables, we show how to apply it even where the noise\ncovariance structure is not a-priori known or is not independently estimable.\n  The proposed method, which we call ScreeNOT, is a mathematically solid\nalternative to Cattell's ever-popular but vague Scree Plot heuristic from 1966.\n  ScreeNOT has a surprising oracle property: it typically achieves exactly, in\nlarge finite samples, the lowest possible MSE for matrix recovery, on each\ngiven problem instance - i.e. the specific threshold it selects gives exactly\nthe smallest achievable MSE loss among all possible threshold choices for that\nnoisy dataset and that unknown underlying true low rank model. The method is\ncomputationally efficient and robust against perturbations of the underlying\ncovariance structure.\n  Our results depend on the assumption that the singular values of the noise\nhave a limiting empirical distribution of compact support; this model, which is\nstandard in random matrix theory, is satisfied by many models exhibiting either\ncross-row correlation structure or cross-column correlation structure, and also\nby many situations where there is inter-element correlation structure.\nSimulations demonstrate the effectiveness of the method even at moderate matrix\nsizes. The paper is supplemented by ready-to-use software packages implementing\nthe proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 15:40:05 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 01:00:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Donoho", "David L.", ""], ["Gavish", "Matan", ""], ["Romanov", "Elad", ""]]}, {"id": "2009.12453", "submitter": "MaryLena Bleile", "authors": "MaryLena Bleile", "title": "Theoretical Justification of the Bi Error Method", "comments": "6 figures, 5 pages", "journal-ref": null, "doi": "10.1007/978-3-030-80126-7_15", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorrect usage of $p$-values, particularly within the context of\nsignificance testing using the arbitrary .05 threshold, has become a major\nproblem in modern statistical practice. The prevalence of this problem can be\ntraced back to the context-free 5-step method commonly taught to\nundergraduates: we teach it because it is what is done, and we do it because it\nis what we are taught. This hold particularly true for practitioners of\nstatistics who are not formal statisticians. Thus, in order to improve\nscientific practice and overcome statistical dichotomania, an accessible\nreplacement for the 5-step method is warranted. We propose a method\nfoundational on the utilization of the Youden Index as a potential decision\nthreshold, which has been shown in the literature to be effective in\nconjunction with neutral zones. Unlike the traditional 5-step method, our\n5-step method (the Bi Error method) allows for neutral results, does not\nrequire $p$-values, and does not provide any default threshold values. Instead,\nour method explicitly requires contextual error analysis as well as\nquantification of statistical power. Furthermore, and in part due to its lack\nof usage of p-values, the method sports improved accessibility. This\naccessibility is supported by a generalized analytical derivation of the Youden\nIndex.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 22:07:32 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bleile", "MaryLena", ""]]}, {"id": "2009.12487", "submitter": "Yisha Yao", "authors": "Yisha Yao", "title": "Constructing Confidence Intervals for the Signals in Sparse Phase\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a general methodology to draw statistical\ninferences on individual signal coordinates or linear combinations of them in\nsparse phase retrieval. Given an initial estimator for the targeting parameter\n(some simple function of the signal), which is generated by some existing\nalgorithm, we can modify it in a way that the modified version is\nasymptotically normal and unbiased. Then confidence intervals and hypothesis\ntestings can be constructed based on this asymptotic normality. For\nconciseness, we focus on confidence intervals in this work, while a similar\nprocedure can be adopted for hypothesis testings. Under some mild assumptions\non the signal and sample size, we establish theoretical guarantees for the\nproposed method. These assumptions are generally weak in the sense that the\ndimension could exceed the sample size and many non-zero small coordinates are\nallowed. Furthermore, theoretical analysis reveals that the modified estimators\nfor individual coordinates have uniformly bounded variance, and hence\nsimultaneous interval estimation is possible. Numerical simulations in a wide\nrange of settings are supportive of our theoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 00:39:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yao", "Yisha", ""]]}, {"id": "2009.12523", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung and Ying Hung", "title": "Efficient calibration for imperfect epidemic models with applications to\n  the analysis of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of unknown parameters in simulations, also known as\ncalibration, is crucial for practical management of epidemics and prediction of\npandemic risk. A simple yet widely used approach is to estimate the parameters\nby minimizing the sum of the squared distances between actual observations and\nsimulation outputs. It is shown in this paper that this method is inefficient,\nparticularly when the epidemic models are developed based on certain\nsimplifications of reality, also known as imperfect models which are commonly\nused in practice. To address this issue, a new estimator is introduced that is\nasymptotically consistent, has a smaller estimation variance than the least\nsquares estimator, and achieves the semiparametric efficiency. Numerical\nstudies are performed to examine the finite sample performance. The proposed\nmethod is applied to the analysis of the COVID-19 pandemic for 20 countries\nbased on the SEIR (Susceptible-Exposed-Infectious-Recovered) model with both\ndeterministic and stochastic simulations. The estimation of the parameters,\nincluding the basic reproduction number and the average incubation period,\nreveal the risk of disease outbreaks in each country and provide insights to\nthe design of public health interventions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 06:00:52 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sung", "Chih-Li", ""], ["Hung", "Ying", ""]]}, {"id": "2009.12528", "submitter": "Takahiro Hoshino", "authors": "Takahiro Hoshino", "title": "Identification of causal direct-indirect effects without untestable\n  assumptions", "comments": "The paper has been submitted and the discussion paper will appear in\n  SSRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal mediation analysis, identification of existing causal direct or\nindirect effects requires untestable assumptions in which potential outcomes\nand potential mediators are independent. This paper defines a new causal direct\nand indirect effect that does not require the untestable assumptions. We show\nthat the proposed measure is identifiable from the observed data, even if\npotential outcomes and potential mediators are dependent, while the existing\nnatural direct or indirect effects may find a pseudo-indirect effect when the\nuntestable assumptions are violated.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 07:48:37 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Hoshino", "Takahiro", ""]]}, {"id": "2009.12665", "submitter": "Christoph Breunig", "authors": "Christoph Breunig and Stephan Martin", "title": "Nonclassical Measurement Error in the Outcome Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a semi-/nonparametric regression model with a general form of\nnonclassical measurement error in the outcome variable. We show equivalence of\nthis model to a generalized regression model. Our main identifying assumptions\nare a special regressor type restriction and monotonicity in the nonlinear\nrelationship between the observed and unobserved true outcome. Nonparametric\nidentification is then obtained under a normalization of the unknown link\nfunction, which is a natural extension of the classical measurement error case.\nWe propose a novel sieve rank estimator for the regression function and\nestablish its rate of convergence.\n  In Monte Carlo simulations, we find that our estimator corrects for biases\ninduced by nonclassical measurement error and provides numerically stable\nresults. We apply our method to analyze belief formation of stock market\nexpectations with survey data from the German Socio-Economic Panel (SOEP) and\nfind evidence for nonclassical measurement error in subjective belief data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 18:44:18 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 09:46:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Breunig", "Christoph", ""], ["Martin", "Stephan", ""]]}, {"id": "2009.12686", "submitter": "Abhik Ghosh PhD", "authors": "Amarnath Nandy, Abhik Ghosh, Ayanendranath Basu, Leandro Pardo", "title": "Robust Hypothesis Testing and Model Selection for Parametric\n  Proportional Hazard Regression Models", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semi-parametric Cox proportional hazards regression model has been widely\nused for many years in several applied sciences. However, a fully parametric\nproportional hazards model, if appropriately assumed, can often lead to more\nefficient inference. To tackle the extreme non-robustness of the traditional\nmaximum likelihood estimator in the presence of outliers in the data under such\nfully parametric proportional hazard models, a robust estimation procedure has\nrecently been proposed extending the concept of the minimum density power\ndivergence estimator (MDPDE) under this set-up. In this paper, we consider the\nproblem of statistical inference under the parametric proportional hazards\nmodel and develop robust Wald-type hypothesis testing and model selection\nprocedures using the MDPDEs. We have also derived the necessary asymptotic\nresults which are used to construct the testing procedure for general composite\nhypothesis and study its asymptotic powers. The claimed robustness properties\nare studied theoretically via appropriate influence function analyses. We have\nstudied the finite sample level and power of the proposed MDPDE based Wald type\ntest through extensive simulations where comparisons are also made with the\nexisting semi-parametric methods. The important issue of the selection of\nappropriate robustness tuning parameter is also discussed. The practical\nusefulness of the proposed robust testing and model selection procedures is\nfinally illustrated through three interesting real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 21:01:33 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nandy", "Amarnath", ""], ["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "2009.12852", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Sylvia Richardson, Paul D. W. Kirk", "title": "Kernel learning approaches for summarising and combining posterior\n  similarity matrices", "comments": "Manuscript: 27 pages, 8 figures. Supplement: 62 pages, 68 figures.\n  For associated R code, see https://github.com/acabassi/combine-psms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When using Markov chain Monte Carlo (MCMC) algorithms to perform inference\nfor Bayesian clustering models, such as mixture models, the output is typically\na sample of clusterings (partitions) drawn from the posterior distribution. In\npractice, a key challenge is how to summarise this output. Here we build upon\nthe notion of the posterior similarity matrix (PSM) in order to suggest new\napproaches for summarising the output of MCMC algorithms for Bayesian\nclustering models. A key contribution of our work is the observation that PSMs\nare positive semi-definite, and hence can be used to define\nprobabilistically-motivated kernel matrices that capture the clustering\nstructure present in the data. This observation enables us to employ a range of\nkernel methods to obtain summary clusterings, and otherwise exploit the\ninformation summarised by PSMs. For example, if we have multiple PSMs, each\ncorresponding to a different dataset on a common set of statistical units, we\nmay use standard methods for combining kernels in order to perform integrative\nclustering. We may moreover embed PSMs within predictive kernel models in order\nto perform outcome-guided data integration. We demonstrate the performances of\nthe proposed methods through a range of simulation studies as well as two real\ndata applications. R code is available at\nhttps://github.com/acabassi/combine-psms.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 14:16:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Richardson", "Sylvia", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "2009.13129", "submitter": "Yueh Wang", "authors": "Yueh Wang and Hung Hung", "title": "Statistical Inference on the Cure Time", "comments": "77 pages, 29 figures, PhD dissertation, Ins. of Epidemiology and\n  Preventive Medicine, National Taiwan University", "journal-ref": null, "doi": "10.6342/NTU201904221", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population-based cancer survival analysis, the net survival is important\nfor government to assess health care programs. For decades, it is observed that\nthe net survival reaches a plateau after long-term follow-up, this is so called\n``statistical cure''. Several methods were proposed to address the statistical\ncure. Besides, the cure time can be used to evaluate the time period of a\nhealth care program for a specific patient population, and it also can be\nhelpful for a clinician to explain the prognosis for patients, therefore the\ncure time is an important health care index. However, those proposed methods\nassume the cure time to be infinity, thus it is inconvenient to make inference\non the cure time. In this dissertation, we define a more general concept of\nstatistical cure via conditional survival. Based on the newly defined\nstatistical cure, the cure time is well defined. We develop cure time model\nmethodologies and show a variety of properties through simulation. In data\nanalysis, cure times are estimated for 22 major cancers in Taiwan, we further\nuse colorectal cancer data as an example to conduct statistical inference via\ncure time model with covariate sex, age group, and stage. This dissertation\nprovides a methodology to obtain cure time estimate, which can contribute to\npublic health policy making.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:20:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Yueh", ""], ["Hung", "Hung", ""]]}, {"id": "2009.13384", "submitter": "Michael B\\\"ucker", "authors": "Michael B\\\"ucker and Gero Szepannek and Alicja Gosiewska and\n  Przemyslaw Biecek", "title": "Transparency, Auditability and eXplainability of Machine Learning Models\n  in Credit Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.GN q-fin.EC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major requirement for credit scoring models is to provide a maximally\naccurate risk prediction. Additionally, regulators demand these models to be\ntransparent and auditable. Thus, in credit scoring, very simple predictive\nmodels such as logistic regression or decision trees are still widely used and\nthe superior predictive power of modern machine learning algorithms cannot be\nfully leveraged. Significant potential is therefore missed, leading to higher\nreserves or more credit defaults. This paper works out different dimensions\nthat have to be considered for making credit scoring models understandable and\npresents a framework for making ``black box'' machine learning models\ntransparent, auditable and explainable. Following this framework, we present an\noverview of techniques, demonstrate how they can be applied in credit scoring\nand how results compare to the interpretability of score cards. A real world\ncase study shows that a comparable degree of interpretability can be achieved\nwhile machine learning techniques keep their ability to improve predictive\npower.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:00:13 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["B\u00fccker", "Michael", ""], ["Szepannek", "Gero", ""], ["Gosiewska", "Alicja", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2009.13404", "submitter": "Soichiro Yamauchi", "authors": "Soichiro Yamauchi", "title": "Difference-in-Differences for Ordinal Outcomes: Application to the\n  Effect of Mass Shootings on Attitudes toward Gun Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difference-in-differences (DID) design is widely used in observational\nstudies to estimate the causal effect of a treatment when repeated observations\nover time are available. Yet, almost all existing methods assume linearity in\nthe potential outcome (parallel trends assumption) and target the additive\neffect. In social science research, however, many outcomes of interest are\nmeasured on an ordinal scale. This makes the linearity assumption inappropriate\nbecause the difference between two ordinal potential outcomes is not well\ndefined. In this paper, I propose a method to draw causal inferences for\nordinal outcomes under the DID design. Unlike existing methods, the proposed\nmethod utilizes the latent variable framework to handle the non-numeric nature\nof the outcome, enabling identification and estimation of causal effects based\non the assumption on the quantile of the latent continuous variable. The paper\nalso proposes an equivalence-based test to assess the plausibility of the key\nidentification assumption when additional pre-treatment periods are available.\nThe proposed method is applied to a study estimating the causal effect of mass\nshootings on the public's support for gun control. I find little evidence for a\nuniform shift toward pro-gun control policies as found in the previous study,\nbut find that the effect is concentrated on left-leaning respondents who\nexperienced the shooting for the first time in more than a decade.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 15:22:23 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yamauchi", "Soichiro", ""]]}, {"id": "2009.13484", "submitter": "Marcelo Medeiros", "authors": "Carlos B. Carneiro, I\\'uri H. Ferreira, Marcelo C. Medeiros, Henrique\n  F. Pires and Eduardo Zilberman", "title": "Lockdown effects in US states: an artificial counterfactual approach", "comments": "Updated versions of this paper will be available on\n  http://139.82.34.174/mcm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM econ.GN q-fin.EC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adopt an artificial counterfactual approach to assess the impact of\nlockdowns on the short-run evolution of the number of cases and deaths in some\nUS states. To do so, we explore the different timing in which US states adopted\nlockdown policies, and divide them among treated and control groups. For each\ntreated state, we construct an artificial counterfactual. On average, and in\nthe very short-run, the counterfactual accumulated number of cases would be two\ntimes larger if lockdown policies were not implemented.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 17:21:03 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 18:27:55 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Carneiro", "Carlos B.", ""], ["Ferreira", "I\u00fari H.", ""], ["Medeiros", "Marcelo C.", ""], ["Pires", "Henrique F.", ""], ["Zilberman", "Eduardo", ""]]}, {"id": "2009.13591", "submitter": "Sanket Jantre", "authors": "Sanket R. Jantre, Shrijita Bhattacharya, Tapabrata Maiti", "title": "Quantile Regression Neural Networks: A Bayesian Approach", "comments": null, "journal-ref": "J Stat Theory Pract 15 (3), 1-34, 2021", "doi": "10.1007/s42519-021-00189-w", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a Bayesian neural network estimation method for\nquantile regression assuming an asymmetric Laplace distribution (ALD) for the\nresponse variable. It is shown that the posterior distribution for feedforward\nneural network quantile regression is asymptotically consistent under a\nmisspecified ALD model. This consistency proof embeds the problem from density\nestimation domain and uses bounds on the bracketing entropy to derive the\nposterior consistency over Hellinger neighborhoods. This consistency result is\nshown in the setting where the number of hidden nodes grow with the sample\nsize. The Bayesian implementation utilizes the normal-exponential mixture\nrepresentation of the ALD density. The algorithm uses Markov chain Monte Carlo\n(MCMC) simulation technique - Gibbs sampling coupled with Metropolis-Hastings\nalgorithm. We have addressed the issue of complexity associated with the\nafore-mentioned MCMC implementation in the context of chain convergence, choice\nof starting values, and step sizes. We have illustrated the proposed method\nwith simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 19:21:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Jantre", "Sanket R.", ""], ["Bhattacharya", "Shrijita", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2009.13636", "submitter": "Paul Parker", "authors": "Paul A. Parker, Scott H. Holan, and Skye A. Wills", "title": "A General Bayesian Model for Heteroskedastic Data with Fully Conjugate\n  Full-Conditional Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for heteroskedastic data are relevant in a wide variety of\napplications ranging from financial time series to environmental statistics.\nHowever, the topic of modeling the variance function conditionally has not seen\nnear as much attention as modeling the mean. Volatility models have been used\nin specific applications, but these models can be difficult to fit in a\nBayesian setting due to posterior distributions that are challenging to sample\nfrom efficiently. In this work, we introduce a general model for\nheteroskedastic data. This approach models the conditional variance in a mixed\nmodel approach as a function of any desired covariates or random effects. We\nrely on new distribution theory in order to construct priors that yield fully\nconjugate full conditional distributions. Thus, our approach can easily be fit\nvia Gibbs sampling. Furthermore, we extend the model to a deep learning\napproach that can provide highly accurate estimates for time dependent data. We\nalso provide an extension for heavy-tailed data. We illustrate our methodology\nvia three applications. The first application utilizes a high dimensional soil\ndataset with inherent spatial dependence. The second application involves\nmodeling of asset volatility. The third application focuses on clinical trial\ndata for creatinine.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 21:25:44 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Parker", "Paul A.", ""], ["Holan", "Scott H.", ""], ["Wills", "Skye A.", ""]]}, {"id": "2009.13831", "submitter": "Milo\\v{s} Simi\\'c", "authors": "Milo\\v{s} Simi\\'c", "title": "Testing for Normality with Neural Networks", "comments": "49 pages, 10 figures; corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we treat the problem of testing for normality as a binary\nclassification problem and construct a feedforward neural network that can\nsuccessfully detect normal distributions by inspecting small samples from them.\nThe numerical experiments conducted on small samples with no more than 100\nelements indicated that the neural network which we trained was more accurate\nand far more powerful than the most frequently used and most powerful standard\ntests of normality: Shapiro-Wilk, Anderson-Darling, Lilliefors and\nJarque-Berra, as well as the kernel tests of goodness-of-fit. The neural\nnetwork had the AUROC score of almost 1, which corresponds to the perfect\nbinary classifier. Additionally, the network's accuracy was higher than 96% on\na set of larger samples with 250-1000 elements. Since the normality of data is\nan assumption of numerous techniques for analysis and inference, the neural\nnetwork constructed in this study has a very high potential for use in everyday\npractice of statistics, data analysis and machine learning in both science and\nindustry.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:35:40 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 07:47:22 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Simi\u0107", "Milo\u0161", ""]]}, {"id": "2009.13921", "submitter": "M Bitan", "authors": "Michal Bitan, Malka Gorfine, Laura Rosen and David M. Steinberg", "title": "Efficient Study Design with Multiple Measurement Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcomes from studies assessing exposure often use multiple measurements. In\nprevious work, using a model first proposed by Buonoccorsi (1991), we showed\nthat combining direct (e.g. biomarkers) and indirect (e.g. self-report)\nmeasurements provides a more accurate picture of true exposure than estimates\nobtained when using a single type of measurement. In this article, we propose a\nvaluable tool for efficient design of studies that include both direct and\nindirect measurements of a relevant outcome. Based on data from a pilot or\npreliminary study, the tool, which is available online as a shiny app\n\\citep{shinyR}, can be used to compute: (1) the sample size required for a\nstatistical power analysis, while optimizing the percent of participants who\nshould provide direct measures of exposure (biomarkers) in addition to the\nindirect (self-report) measures provided by all participants; (2) the ideal\nnumber of replicates; and (3) the allocation of resources to intervention and\ncontrol arms. In addition we show how to examine the sensitivity of results to\nunderlying assumptions. We illustrate our analysis using studies of tobacco\nsmoke exposure and nutrition. In these examples, a near-optimal allocation of\nthe resources can be found even if the assumptions are not precise.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 10:38:19 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Bitan", "Michal", ""], ["Gorfine", "Malka", ""], ["Rosen", "Laura", ""], ["Steinberg", "David M.", ""]]}, {"id": "2009.13974", "submitter": "Marion Hoffman", "authors": "Marion Hoffman, Per Block, Tom A.B. Snijders", "title": "Modeling partitions of individuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the central role of self-assembled groups in animal and human\nsocieties, statistical tools to explain their composition are limited. We\nintroduce a statistical framework for cross-sectional observations of groups\nwith exclusive membership to illuminate the social and organizational\nmechanisms that bring people together. Drawing from stochastic models for\nnetworks and partitions, the proposed framework introduces an exponential\nfamily of distributions for partitions. We derive its main mathematical\nproperties and suggest strategies to specify and estimate such models. A case\nstudy on hackathon events applies the developed framework to the study of\nmechanisms underlying the formation of self-assembled project teams.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 12:49:11 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hoffman", "Marion", ""], ["Block", "Per", ""], ["Snijders", "Tom A. B.", ""]]}, {"id": "2009.13995", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Shawn C. Liebenberg", "title": "On a new test of fit to the beta distribution", "comments": "10 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new $L^2$-type goodness-of-fit test for the family of beta\ndistributions based on a conditional moment characterisation. The asymptotic\nnull distribution is identified, and since it depends on the underlying\nparameters, a parametric bootstrap procedure is proposed. Consistency against\nall alternatives that satisfy a convergence criterion is shown, and a Monte\nCarlo simulation study indicates that the new procedure outperforms most of the\nclassical tests. Finally, the procedure is applied to a real data set related\nto air humidity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 13:29:16 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Ebner", "Bruno", ""], ["Liebenberg", "Shawn C.", ""]]}, {"id": "2009.14131", "submitter": "Hedibert Lopes", "authors": "Paloma W. Uribe and Hedibert F. Lopes", "title": "Dynamic sparsity on dynamic regression models", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we consider variable selection and shrinkage for the\nGaussian dynamic linear regression within a Bayesian framework. In particular,\nwe propose a novel method that allows for time-varying sparsity, based on an\nextension of spike-and-slab priors for dynamic models. This is done by\nassigning appropriate Markov switching priors for the time-varying\ncoefficients' variances, extending the previous work of Ishwaran and Rao\n(2005). Furthermore, we investigate different priors, including the common\nInverted gamma prior for the process variances, and other mixture prior\ndistributions such as Gamma priors for both the spike and the slab, which leads\nto a mixture of Normal-Gammas priors (Griffin ad Brown, 2010) for the\ncoefficients. In this sense, our prior can be view as a dynamic variable\nselection prior which induces either smoothness (through the slab) or shrinkage\ntowards zero (through the spike) at each time point. The MCMC method used for\nposterior computation uses Markov latent variables that can assume binary\nregimes at each time point to generate the coefficients' variances. In that\nway, our model is a dynamic mixture model, thus, we could use the algorithm of\nGerlach et al (2000) to generate the latent processes without conditioning on\nthe states. Finally, our approach is exemplified through simulated examples and\na real data application.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:26:08 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Uribe", "Paloma W.", ""], ["Lopes", "Hedibert F.", ""]]}, {"id": "2009.14150", "submitter": "Fernando Castro-Prado", "authors": "Fernando Castro-Prado and Wenceslao Gonz\\'alez-Manteiga", "title": "Nonparametric independence tests in metric spaces: What is known and\n  what is not", "comments": "18 pages with no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distance correlation is a recent extension of Pearson's correlation, that\ncharacterises general statistical independence between Euclidean-space-valued\nrandom variables, not only linear relations. This review delves into how and\nwhen distance correlation can be extended to metric spaces, combining the\ninformation that is available in the literature with some original remarks and\nproofs, in a way that is comprehensible for any mathematical statistician.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:47:56 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Castro-Prado", "Fernando", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "2009.14296", "submitter": "Hedibert Lopes", "authors": "Bruno Fava and Hedibert F. Lopes", "title": "The Illusion of the Illusion of Sparsity: An exercise in prior\n  sensitivity", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Big Data raises the question of how to model economic\nrelations when there is a large number of possible explanatory variables. We\nrevisit the issue by comparing the possibility of using dense or sparse models\nin a Bayesian approach, allowing for variable selection and shrinkage. More\nspecifically, we discuss the results reached by Giannone, Lenza, and Primiceri\n(2020) through a \"Spike-and-Slab\" prior, which suggest an \"illusion of\nsparsity\" in economic data, as no clear patterns of sparsity could be detected.\nWe make a further revision of the posterior distributions of the model, and\npropose three experiments to evaluate the robustness of the adopted prior\ndistribution. We find that the pattern of sparsity is sensitive to the prior\ndistribution of the regression coefficients, and present evidence that the\nmodel indirectly induces variable selection and shrinkage, which suggests that\nthe \"illusion of sparsity\" could be, itself, an illusion. Code is available on\ngithub.com/bfava/IllusionOfIllusion.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 20:39:13 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Fava", "Bruno", ""], ["Lopes", "Hedibert F.", ""]]}, {"id": "2009.14367", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo and Michael Jansson and Xinwei Ma", "title": "Local Regression Distribution Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the large sample properties of local regression\ndistribution estimators, which include a class of boundary adaptive density\nestimators as a prime example. First, we establish a pointwise Gaussian large\nsample distributional approximation in a unified way, allowing for both\nboundary and interior evaluation points simultaneously. Using this result, we\nstudy the asymptotic efficiency of the estimators, and show that a carefully\ncrafted minimum distance implementation based on \"redundant\" regressors can\nlead to efficiency gains. Second, we establish uniform linearizations and\nstrong approximations for the estimators, and employ these results to construct\nvalid confidence bands. Third, we develop extensions to weighted distributions\nwith estimated weights and to local $L^{2}$ least squares estimation. Finally,\nwe illustrate our methods with two applications in program evaluation:\ncounterfactual density testing, and IV specification and heterogeneity density\nanalysis. Companion software packages in Stata and R are available.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:10:44 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:46:10 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Ma", "Xinwei", ""]]}, {"id": "2009.14371", "submitter": "Osafu Augustine Egbon Mr.", "authors": "Francisco Louzada and Diego C. Nascimento and Osafu Augustine Egbon", "title": "Spatial Statistical Models: an overview under the Bayesian Approach", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial documentation is exponentially increasing given the availability of\nBig IoT Data, enabled by the devices miniaturization and data storage capacity.\nBayesian spatial statistics is a useful statistical tool to determine the\ndependence structure and hidden patterns over space through prior knowledge and\ndata likelihood. Nevertheless, this modeling class is not well explored as the\nclassification and regression machine learning models given their simplicity\nand often weak (data) independence supposition. In this manner, this systematic\nreview aimed to unravel the main models presented in the literature in the past\n20 years, identify gaps, and research opportunities. Elements such as random\nfields, spatial domains, prior specification, covariance function, and\nnumerical approximations were discussed. This work explored the two subclasses\nof spatial smoothing global and local.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:19:22 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Louzada", "Francisco", ""], ["Nascimento", "Diego C.", ""], ["Egbon", "Osafu Augustine", ""]]}, {"id": "2009.14461", "submitter": "Molei Liu", "authors": "Molei Liu, Yi Zhang, Doudou Zhou", "title": "Double/Debiased Machine Learning for Logistic Partially Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose double/debiased machine learning approaches to infer (at the\nparametric rate) the parametric component of a logistic partially linear model\nwith the binary response following a conditional logistic model of a low\ndimensional linear parametric function of some key (exposure) covariates and a\nnonparametric function adjusting for the confounding effect of other\ncovariates. We consider a Neyman orthogonal (doubly robust) score equation\nconsisting of two nuisance functions: nonparametric component in the logistic\nmodel and conditional mean of the exposure on the other covariates and with the\nresponse fixed. To estimate the nuisance models, we separately consider the use\nof high dimensional (HD) sparse parametric models and more general (typically\nnonparametric) machine learning (ML) methods. In the HD case, we derive certain\nmoment equations to calibrate the first-order bias of the nuisance models and\ngrant our method a model double robustness property in the sense that our\nestimator achieves the desirable rate when at least one of the nuisance models\nis correctly specified and both of them are ultra-sparse. In the ML case, the\nnon-linearity of the logit link makes it substantially harder than the\npartially linear setting to use an arbitrary conditional mean learning\nalgorithm to estimate the nuisance component of the logistic model. We handle\nthis obstacle through a novel full model refitting procedure that is\neasy-to-implement and facilitates the use of nonparametric ML algorithms in our\nframework. Our ML estimator is rate doubly robust in the same sense as\nChernozhukov et al. (2018a). We evaluate our methods through simulation studies\nand apply them in assessing the effect of emergency contraceptive (EC) pill on\nearly gestation foetal with a policy reform in Chile in 2008 (Bentancor and\nClarke, 2017).\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 05:55:29 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Liu", "Molei", ""], ["Zhang", "Yi", ""], ["Zhou", "Doudou", ""]]}, {"id": "2009.14484", "submitter": "Zhonghua Liu", "authors": "Zhonghua Liu and Ting Ye and Baoluo Sun and Mary Schooling and Eric\n  Tchetgen Tchetgen", "title": "On Mendelian Randomization Mixed-Scale Treatment Effect Robust\n  Identification (MR MiSTERI) and Estimation for Causal Inference", "comments": "2 figures, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Mendelian randomization analysis can produce biased results if the\ngenetic variant defining the instrumental variable (IV) is confounded and/or\nhas a horizontal pleiotropic effect on the outcome of interest not mediated by\nthe treatment. We provide novel identification conditions for the causal effect\nof a treatment in presence of unmeasured confounding, by leveraging an invalid\nIV for which both the IV independence and exclusion restriction assumptions may\nbe violated. The proposed Mendelian Randomization Mixed-Scale Treatment Effect\nRobust Identification (MR MiSTERI) approach relies on (i) an assumption that\nthe treatment effect does not vary with the invalid IV on the additive scale;\nand (ii) that the selection bias due to confounding does not vary with the\ninvalid IV on the odds ratio scale; and (iii) that the residual variance for\nthe outcome is heteroscedastic and thus varies with the invalid IV. We formally\nestablish that their conjunction can identify a causal effect even with an\ninvalid IV subject to pleiotropy. MiSTERI is shown to be particularly\nadvantageous in presence of pervasive heterogeneity of pleiotropic effects on\nadditive scale, a setting in which two recently proposed robust estimation\nmethods MR GxE and MR GENIUS can be severely biased. In order to incorporate\nmultiple, possibly correlated and weak IVs, a common challenge in MR studies,\nwe develop a MAny Weak Invalid Instruments (MR MaWII MiSTERI) approach for\nstrengthened identification and improved accuracy MaWII MiSTERI is shown to be\nrobust to horizontal pleiotropy, violation of IV independence assumption and\nweak IV bias. Both simulation studies and real data analysis results\ndemonstrate the robustness of the proposed MR MiSTERI methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 07:41:58 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 03:22:55 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 03:11:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Zhonghua", ""], ["Ye", "Ting", ""], ["Sun", "Baoluo", ""], ["Schooling", "Mary", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2009.14745", "submitter": "Jun Tang", "authors": "Jun Tang and Dale Zimmerman", "title": "Space-Time Covariance Models on Networks with An Application on Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-order, small-scale dependence structure of a stochastic process\ndefined in the space-time domain is key to prediction (or kriging). While great\nefforts have been dedicated to developing models for cases in which the spatial\ndomain is either a finite-dimensional Euclidean space or a unit sphere,\ncounterpart developments on a generalized linear network are practically\nnon-existent. To fill this gap, we develop a broad range of parametric,\nnon-separable space-time covariance models on generalized linear networks and\nthen an important subgroup -- Euclidean trees by the space embedding technique\n-- in concert with the generalized Gneiting class of models and 1-symmetric\ncharacteristic functions in the literature, and the scale mixture approach. We\ngive examples from each class of models and investigate the geometric features\nof these covariance functions near the origin and at infinity. We also show the\nlinkage between different classes of space-time covariance models on Euclidean\ntrees. We illustrate the use of models constructed by different methodologies\non a daily stream temperature data set and compare model predictive performance\nby cross validation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 15:32:42 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tang", "Jun", ""], ["Zimmerman", "Dale", ""]]}, {"id": "2009.14797", "submitter": "Jae-Kwang Kim", "authors": "Danhyang Lee and Li-Chun Zhang and Jae-Kwang Kim", "title": "Maximum Entropy classification for record linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By record linkage one joins records residing in separate files which are\nbelieved to be related to the same entity. In this paper we approach record\nlinkage as a classification problem, and adapt the maximum entropy\nclassification method in text mining to record linkage, both in the supervised\nand unsupervised settings of machine learning. The set of links will be chosen\naccording to the associated uncertainty. On the one hand, our framework\novercomes some persistent theoretical flaws of the classical approach pioneered\nby Fellegi and Sunter (1969); on the other hand, the proposed algorithm is\nscalable and fully automatic, unlike the classical approach that generally\nrequires clerical review to resolve the undecided cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 17:11:28 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Lee", "Danhyang", ""], ["Zhang", "Li-Chun", ""], ["Kim", "Jae-Kwang", ""]]}]