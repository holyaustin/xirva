[{"id": "1703.00253", "submitter": "Chien-Ju Lin", "authors": "Chien-Ju Lin and James Wason", "title": "Improving phase II oncology trials using best observed RECIST response\n  as an endpoint by modelling continuous tumour measurements", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many phase II trials in solid tumours, patients are assessed using\nendpoints based on the Response Evaluation Criteria in Solid Tumours (RECIST)\nscale. Often, analyses are based on the response rate. This is the proportion\nof patients who have an observed tumour shrinkage above a pre-defined level and\nno new tumour lesions. The augmented binary method has been proposed to improve\nthe precision of the estimator of the response rate. The method involves\nmodelling the tumour shrinkage to avoid dichotomising it. However, in many\ntrials the best observed response is used as the primary outcome. In such\ntrials, patients are followed until progression, and their best observed RECIST\noutcome is used as the primary endpoint. In this paper, we propose a method\nthat extends the augmented binary method so that it can be used when the\noutcome is best observed response. We show through simulated data and data from\na real phase II cancer trial that this method improves power in both single-arm\nand randomised trials. The average gain in power compared to the traditional\nanalysis is equivalent to approximately a 35% increase in sample size. A\nmodified version of the method is proposed to reduce the computational effort\nrequired. We show this modified method maintains much of the efficiency\nadvantages.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 12:03:54 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Lin", "Chien-Ju", ""], ["Wason", "James", ""]]}, {"id": "1703.00604", "submitter": "Jinghang Lin", "authors": "Lixin Zhang, Jinghang Lin", "title": "Marcinkiewicz's strong law of large numbers for non-additive expectation", "comments": "13 pages, 10 reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sub-linear expectation space is a nonlinear expectation space having\nadvantages of modelling the uncertainty of probability and distribution. In the\nsub-linear expectation space, we use capacity and sub-linear expectation to\nreplace probability and expectation of classical probability theory. In this\npaper, the method of selecting subsequence is used to prove Marcinkiewicz type\nstrong law of large numbers under sub-linear expectation space. This result is\na natural extension of the classical Marcinkiewicz's strong law of large\nnumbers to the case where the expectation is nonadditive. In addition, this\npaper also gives a theorem about convergence of a random series.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 03:45:27 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Zhang", "Lixin", ""], ["Lin", "Jinghang", ""]]}, {"id": "1703.00654", "submitter": "Sylvain Sardy", "authors": "Jairo Diaz-Rodriguez, Dominique Eckert, Hatef Monajemi, St\\'ephane\n  Paltani and Sylvain Sardy", "title": "Nonparametric estimation of galaxy cluster's emissivity and point source\n  detection in astrophysics with two lasso penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysicists are interested in recovering the 3D gas emissivity of a\ngalaxy cluster from a 2D image taken by a telescope. A blurring phenomenon and\npresence of point sources make this inverse problem even harder to solve. The\ncurrent state-of-the-art technique is two step: first identify the location of\npotential point sources, then mask these locations and deproject the data.\n  We instead model the data as a Poisson generalized linear model (involving\nblurring, Abel and wavelets operators) regularized by two lasso penalties to\ninduce sparse wavelet representation and sparse point sources. The amount of\nsparsity is controlled by two quantile universal thresholds. As a result, our\nmethod outperforms the existing one.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 07:57:20 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Diaz-Rodriguez", "Jairo", ""], ["Eckert", "Dominique", ""], ["Monajemi", "Hatef", ""], ["Paltani", "St\u00e9phane", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1703.00734", "submitter": "Xiangju Qin", "authors": "Xiangju Qin, Paul Blomstedt, Eemeli Lepp\\\"aaho, Pekka Parviainen,\n  Samuel Kaski", "title": "Distributed Bayesian Matrix Factorization with Limited Communication", "comments": "28 pages, 8 figures. The paper is published in Machine Learning\n  journal. An implementation of the method is is available in SMURFF software\n  on github (bmfpp branch): https://github.com/ExaScience/smurff", "journal-ref": "Machine Learning, 2019", "doi": "10.1007/s10994-019-05778-2", "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix factorization (BMF) is a powerful tool for producing low-rank\nrepresentations of matrices and for predicting missing values and providing\nconfidence intervals. Scaling up the posterior inference for massive-scale\nmatrices is challenging and requires distributing both data and computation\nover many workers, making communication the main computational bottleneck.\nEmbarrassingly parallel inference would remove the communication needed, by\nusing completely independent computations on different data subsets, but it\nsuffers from the inherent unidentifiability of BMF solutions. We introduce a\nhierarchical decomposition of the joint posterior distribution, which couples\nthe subset inferences, allowing for embarrassingly parallel computations in a\nsequence of at most three stages. Using an efficient approximate\nimplementation, we show improvements empirically on both real and simulated\ndata. Our distributed approach is able to achieve a speed-up of almost an order\nof magnitude over the full posterior, with a negligible effect on predictive\naccuracy. Our method outperforms state-of-the-art embarrassingly parallel MCMC\nmethods in accuracy, and achieves results competitive to other available\ndistributed and parallel implementations of BMF.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:48:24 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 09:47:09 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 18:58:59 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 17:07:21 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Qin", "Xiangju", ""], ["Blomstedt", "Paul", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Parviainen", "Pekka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1703.00884", "submitter": "Guido Lagos Barrios", "authors": "Ton Dieker and Guido Lagos", "title": "A Dichotomy for Sampling Barrier-Crossing Events of Random Walks with\n  Regularly Varying Tails", "comments": null, "journal-ref": null, "doi": "10.1017/jpr.2017.60", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to sample paths of a random walk up to the first time it crosses\na fixed barrier, in the setting where the step sizes are iid with negative mean\nand have a regularly varying right tail. We introduce a desirable property for\na change of measure to be suitable for exact simulation. We study whether the\nchange of measure of Blanchet and Glynn (2008) satisfies this property and show\nthat it does so if and only if the tail index $\\alpha$ of the right tail lies\nin the interval $(1, \\, 3/2)$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:20:18 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Dieker", "Ton", ""], ["Lagos", "Guido", ""]]}, {"id": "1703.00968", "submitter": "Bo Ning", "authors": "Bo Ning and Peter Bloomfield", "title": "Bayesian inference for generalized extreme value distribution with\n  Gaussian copula dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dependent generalized extreme value (dGEV) models have attracted much\nattention due to the dependency structure that often appears in real datasets.\nTo construct a dGEV model, a natural approach is to assume that some parameters\nin the model are time-varying. A previous study has shown that a dependent\nGumbel process can be naturally incorporated into a GEV model. The model is a\nnonlinear state space model with a hidden state that follows a Markov process,\nwith its innovation following a Gumbel distribution. Inference may be made for\nthe model using Bayesian methods, sampling the hidden process from a mixture\nnormal distribution, used to approximate the Gumbel distribution. Thus the\nresponse follows an approximate GEV model. We propose a new model in which each\nmarginal distribution is an exact GEV distribution. We use a variable\ntransformation to combine the marginal CDF of a Gumbel distribution with the\nstandard normal copula. Then our model is a nonlinear state space model in\nwhich the hidden state equation is Gaussian. We analyze this model using\nBayesian methods, and sample the elements of the state vector using particle\nGibbs with ancestor sampling (PGAS). The PGAS algorithm turns out to be very\nefficient in solving nonlinear state space models. We also show our model is\nflexible enough to incorporate seasonality.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 22:17:52 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Ning", "Bo", ""], ["Bloomfield", "Peter", ""]]}, {"id": "1703.01102", "submitter": "Zhidong Bai", "authors": "Zhidong Bai and Yongchang Hui and Zhihui Lv and Wing-Keung Wong and\n  Shurong Zheng and Zhenzhen Zhu", "title": "A New Test of Multivariate Nonlinear Causality", "comments": "20 pages. arXiv admin note: substantial text overlap with\n  arXiv:1701.03992", "journal-ref": null, "doi": "10.1371/journal.pone.0185155", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The multivariate nonlinear Granger causality developed by Bai et al. (2010)\nplays an important role in detecting the dynamic interrelationships between two\ngroups of variables. Following the idea of Hiemstra-Jones (HJ) test proposed by\nHiemstra and Jones (1994), they attempt to establish a central limit theorem\n(CLT) of their test statistic by applying the asymptotical property of\nmultivariate $U$-statistic. However, Bai et al. (2016) revisit the HJ test and\nfind that the test statistic given by HJ is NOT a function of $U$-statistics\nwhich implies that the CLT neither proposed by Hiemstra and Jones (1994) nor\nthe one extended by Bai et al. (2010) is valid for statistical inference. In\nthis paper, we re-estimate the probabilities and reestablish the CLT of the new\ntest statistic. Numerical simulation shows that our new estimates are\nconsistent and our new test performs decent size and power.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 10:30:11 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Bai", "Zhidong", ""], ["Hui", "Yongchang", ""], ["Lv", "Zhihui", ""], ["Wong", "Wing-Keung", ""], ["Zheng", "Shurong", ""], ["Zhu", "Zhenzhen", ""]]}, {"id": "1703.01234", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and John Paul Gosling", "title": "A Bayesian computer model analysis of Robust Bayesian analyses", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We harness the power of Bayesian emulation techniques, designed to aid the\nanalysis of complex computer models, to examine the structure of complex\nBayesian analyses themselves. These techniques facilitate robust Bayesian\nanalyses and/or sensitivity analyses of complex problems, and hence allow\nglobal exploration of the impacts of choices made in both the likelihood and\nprior specification. We show how previously intractable problems in robustness\nstudies can be overcome using emulation techniques, and how these methods allow\nother scientists to quickly extract approximations to posterior results\ncorresponding to their own particular subjective specification. The utility and\nflexibility of our method is demonstrated on a reanalysis of a real application\nwhere Bayesian methods were employed to capture beliefs about river flow. We\ndiscuss the obvious extensions and directions of future research that such an\napproach opens up.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 16:29:21 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Vernon", "Ian", ""], ["Gosling", "John Paul", ""]]}, {"id": "1703.01364", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "A Matrix Variate Skew-t Distribution", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.143", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is ample work in the literature dealing with skewness in the\nmultivariate setting, there is a relative paucity of work in the matrix variate\nparadigm. Such work is, for example, useful for modelling three-way data. A\nmatrix variate skew-t distribution is derived based on a mean-variance matrix\nnormal mixture. An expectation-conditional maximization algorithm is developed\nfor parameter estimation. Simulated data are used for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 00:16:54 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 20:57:57 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 01:29:16 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.01421", "submitter": "Zhou Fan", "authors": "Zhou Fan and Leying Guan", "title": "Approximate $l_0$-penalized estimation of piecewise-constant signals on\n  graphs", "comments": "v2: Title change, renumbering of sections and theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study recovery of piecewise-constant signals on graphs by the estimator\nminimizing an $l_0$-edge-penalized objective. Although exact minimization of\nthis objective may be computationally intractable, we show that the same\nstatistical risk guarantees are achieved by the $\\alpha$-expansion algorithm\nwhich computes an approximate minimizer in polynomial time. We establish that\nfor graphs with small average vertex degree, these guarantees are minimax\nrate-optimal over classes of edge-sparse signals. For spatially inhomogeneous\ngraphs, we propose minimization of an edge-weighted objective where each edge\nis weighted by its effective resistance or another measure of its contribution\nto the graph's connectivity. We establish minimax optimality of the resulting\nestimators over corresponding edge-weighted sparsity classes. We show\ntheoretically that these risk guarantees are not always achieved by the\nestimator minimizing the $l_1$/total-variation relaxation, and empirically that\nthe $l_0$-based estimates are more accurate in high signal-to-noise settings.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 09:12:42 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 17:16:01 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Fan", "Zhou", ""], ["Guan", "Leying", ""]]}, {"id": "1703.01518", "submitter": "David N. Levin", "authors": "David N. Levin", "title": "Model-Independent Analytic Nonlinear Blind Source Separation", "comments": "21 pages, 4 figures. This paper contains a more complete and didactic\n  explanation of the ideas introduced in http://arxiv.org/abs/1601.03410", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a time series of measurements of the state of an evolving system,\nx(t), where x has two or more components. This paper shows how to perform\nnonlinear blind source separation; i.e., how to determine if these signals are\nequal to linear or nonlinear mixtures of the state variables of two or more\nstatistically independent subsystems. First, the local distributions of\nmeasurement velocities are processed in order to derive vectors at each point\nin x-space. If the data are separable, each of these vectors must be directed\nalong a subspace of x-space that is traversed by varying the state variable of\none subsystem, while all other subsystems are kept constant. Because of this\nproperty, these vectors can be used to construct a small set of mappings, which\nmust contain the unmixing function, if it exists. Therefore, nonlinear blind\nsource separation can be performed by examining the separability of the data\nafter it has been transformed by each of these mappings. The method is\nanalytic, constructive, and model-independent. It is illustrated by blindly\nrecovering the separate utterances of two speakers from nonlinear combinations\nof their audio waveforms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 20:27:02 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Levin", "David N.", ""]]}, {"id": "1703.01665", "submitter": "Marianna Pensky", "authors": "Rida Benhaddou, Marianna Pensky and Rasika Rajapakshage", "title": "Anisotropic functional Laplace deconvolution", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider the problem of estimating a\nthree-dimensional function $f$ based on observations from its noisy Laplace\nconvolution. Our study is motivated by the analysis of Dynamic Contrast\nEnhanced (DCE) imaging data. We construct an adaptive wavelet-Laguerre\nestimator of $f$, derive minimax lower bounds for the $L^2$-risk when $f$\nbelongs to a three-dimensional Laguerre-Sobolev ball and demonstrate that the\nwavelet-Laguerre estimator is adaptive and asymptotically near-optimal in a\nwide range of Laguerre-Sobolev spaces. We carry out a limited simulations study\nand show that the estimator performs well in a finite sample setting. Finally,\nwe use the technique for the solution of the Laplace deconvolution problem on\nthe basis of DCE Computerized Tomography data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Mar 2017 21:14:28 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 15:51:07 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Benhaddou", "Rida", ""], ["Pensky", "Marianna", ""], ["Rajapakshage", "Rasika", ""]]}, {"id": "1703.01692", "submitter": "Robert Grossman", "authors": "Maria T Patterson and Robert L Grossman", "title": "Detecting Spatial Patterns of Disease in Large Collections of Electronic\n  Medical Records Using Neighbor-Based Bootstrapping (NB2)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method called neighbor-based bootstrapping (NB2) that can be\nused to quantify the geospatial variation of a variable. We applied this method\nto an analysis of the incidence rates of disease from electronic medical record\ndata (ICD-9 codes) for approximately 100 million individuals in the US over a\nperiod of 8 years. We considered the incidence rate of disease in each county\nand its geospatially contiguous neighbors and rank ordered diseases in terms of\ntheir degree of geospatial variation as quantified by the NB2 method.\n  We show that this method yields results in good agreement with established\nmethods for detecting spatial autocorrelation (Moran's I method and kriging).\nMoreover, the NB2 method can be tuned to identify both large area and small\narea geospatial variations. This method also applies more generally in any\nparameter space that can be partitioned to consist of regions and their\nneighbors.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 00:14:36 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Patterson", "Maria T", ""], ["Grossman", "Robert L", ""]]}, {"id": "1703.01776", "submitter": "Sylvain Le Corff", "authors": "Pierre Gloaguen (MIA-Paris), Marie-Pierre Etienne (MIA-Paris), Sylvain\n  Le Corff", "title": "Online Sequential Monte Carlo smoother for partially observed stochastic\n  differential equations", "comments": null, "journal-ref": null, "doi": "10.1186/s13634-018-0530-3", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm to approximate smoothed additive\nfunctionals for partially observed stochastic differential equations. This\nmethod relies on a recent procedure which allows to compute such approximations\nonline, i.e. as the observations are received, and with a computational\ncomplexity growing linearly with the number of Monte Carlo samples. This online\nsmoother cannot be used directly in the case of partially observed stochastic\ndifferential equations since the transition density of the latent data is\nusually unknown. We prove that a similar algorithm may still be defined for\npartially observed continuous processes by replacing this unknown quantity by\nan unbiased estimator obtained for instance using general Poisson estimators.\nWe prove that this estimator is consistent and its performance are illustrated\nusing data from two models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 09:24:07 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Gloaguen", "Pierre", "", "MIA-Paris"], ["Etienne", "Marie-Pierre", "", "MIA-Paris"], ["Corff", "Sylvain Le", ""]]}, {"id": "1703.01805", "submitter": "Johnny van Doorn", "authors": "Johnny van Doorn, Alexander Ly, Maarten Marsman, Eric-Jan Wagenmakers", "title": "Bayesian Estimation of Kendall's tau Using a Latent Normal Approach", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rank-based association between two variables can be modeled by\nintroducing a latent normal level to ordinal data. We demonstrate how this\napproach yields Bayesian inference for Kendall's rank correlation coefficient,\nimproving on a recent Bayesian solution from asymptotic properties of the test\nstatistic.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 10:32:54 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 15:50:03 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["van Doorn", "Johnny", ""], ["Ly", "Alexander", ""], ["Marsman", "Maarten", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1703.01866", "submitter": "Xiaohui Yuan", "authors": "Xiaohui Yuan and Xiaogang Dong", "title": "Weighted empirical likelihood for quantile regression with nonignorable\n  missing covariates", "comments": "20 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an empirical likelihood-based weighted estimator of\nregression parameter in quantile regression model with nonignorable missing\ncovariates. The proposed estimator is computationally simple and achieves\nsemiparametric efficiency if the probability of missingness on the fully\nobserved variables is correctly specified. The efficiency gain of the proposed\nestimator over the complete-case-analysis estimator is quantified theoretically\nand illustrated via simulation and a real data application.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 13:41:33 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 11:13:03 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Yuan", "Xiaohui", ""], ["Dong", "Xiaogang", ""]]}, {"id": "1703.01977", "submitter": "Bohdan Pavlyshenko", "authors": "B.M. Pavlyshenko", "title": "Linear, Machine Learning and Probabilistic Approaches for Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study different approaches for time series modeling. The\nforecasting approaches using linear models, ARIMA alpgorithm, XGBoost machine\nlearning algorithm are described. Results of different model combinations are\nshown. For probabilistic modeling the approaches using copulas and Bayesian\ninference are considered.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 10:41:26 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Pavlyshenko", "B. M.", ""]]}, {"id": "1703.02078", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Dylan S. Small, Paul R. Rosenbaum", "title": "Cross-screening in observational studies that test many hypotheses", "comments": "33 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss observational studies that test many causal hypotheses, either\nhypotheses about many outcomes or many treatments. To be credible an\nobservational study that tests many causal hypotheses must demonstrate that its\nconclusions are neither artifacts of multiple testing nor of small biases from\nnonrandom treatment assignment. In a sense that needs to be defined carefully,\nhidden within a sensitivity analysis for nonrandom assignment is an enormous\ncorrection for multiple testing: in the absence of bias, it is extremely\nimprobable that multiple testing alone would create an association insensitive\nto moderate biases. We propose a new strategy called \"cross-screening\",\ndifferent from but motivated by recent work of Bogomolov and Heller on\nreplicability. Cross-screening splits the data in half at random, uses the\nfirst half to plan a study carried out on the second half, then uses the second\nhalf to plan a study carried out on the first half, and reports the more\nfavorable conclusions of the two studies correcting using the Bonferroni\ninequality for having done two studies. If the two studies happen to concur,\nthen they achieve Bogomolov-Heller replicability; however, importantly,\nreplicability is not required for strong control of the family-wise error rate,\nand either study alone suffices for firm conclusions. In randomized studies\nwith a few hypotheses, cross-split screening is not an attractive method when\ncompared with conventional methods of multiplicity control, but it can become\nattractive when hundreds or thousands of hypotheses are subjected to\nsensitivity analyses in an observational study. We illustrate the technique by\ncomparing 46 biomarkers in individuals who consume large quantities of fish\nversus little or no fish.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 19:33:24 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1703.02089", "submitter": "Jean Daunizeau", "authors": "Jean Daunizeau", "title": "The variational Laplace approach to approximate Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approaches to approximate Bayesian inference provide very\nefficient means of performing parameter estimation and model selection. Among\nthese, so-called variational-Laplace or VL schemes rely on Gaussian\napproximations to posterior densities on model parameters. In this note, we\nreview the main variants of VL approaches, that follow from considering\nnonlinear models of continuous and/or categorical data. En passant, we also\nderive a few novel theoretical results that complete the portfolio of existing\nanalyses of variational Bayesian approaches, including investigations of their\nasymptotic convergence. We also suggest practical ways of extending existing VL\napproaches to hierarchical generative models that include (e.g., precision)\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 11:02:22 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 17:45:14 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Daunizeau", "Jean", ""]]}, {"id": "1703.02112", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Devin S. Johnson, John W. Durban", "title": "Process convolution approaches for modeling interacting trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are a fundamental statistical tool used in a wide range of\napplications. In the spatio-temporal setting, several families of covariance\nfunctions exist to accommodate a wide variety of dependence structures arising\nin different applications. These parametric families can be restrictive and are\ninsufficient in some situations. In contrast, process convolutions represent a\nflexible, interpretable approach to defining the covariance of a Gaussian\nprocess and have modest requirements to ensure validity. We introduce a\ngeneralization of the process convolution approach that employs multiple\nconvolutions sequentially to form a \"process convolution chain.\" In our\nproposed multi-stage framework, complex dependencies that arise from a\ncombination of different interacting mechanisms are decomposed into a series of\ninterpretable kernel smoothers. We demonstrate an application of process\nconvolution chains to model killer whale movement, in which the paths taken by\nmultiple individuals are not independent, but reflect dynamic social\ninteractions within the population. Our proposed model for dependent movement\nprovides inference for the latent dynamic social structure in the study\npopulation. Additionally, by leveraging the positive dependence among\nindividual paths, we achieve a reduction in uncertainty for the estimated\nlocations of the whales, compared to a model that treats paths as independent.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 21:17:42 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 18:16:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""], ["Durban", "John W.", ""]]}, {"id": "1703.02113", "submitter": "Jeff Wu", "authors": "C. F. Jeff Wu", "title": "A fresh look at effect aliasing and interactions: some new wine in old\n  bottles", "comments": "There is an error in Acknowledgements section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions and effect aliasing are among the fundamental concepts in\nexperimental design. In this paper, some new insights and approaches are\nprovided on these subjects. In the literature, the \"de-aliasing\" of aliased\neffects is deemed to be impossible. We argue that this \"impossibility\" can\nindeed be resolved by employing a new approach which consists of\nreparametrization of effects and exploitation of effect non-orthogonality. This\napproach is successfully applied to three classes of designs: regular and\nnonregular two-level fractional factorial designs, and three-level fractional\nfactorial designs. For reparametrization, the notion of conditional main\neffects (cme's) is employed for two-level regular designs, while the\nlinear-quadratic system is used for three-level designs. For nonregular\ntwo-level designs, reparametrization is not needed because the partial aliasing\nof their effects already induces non-orthogonality. The approach can be\nextended to general observational data by using a new bi-level variable\nselection technique based on the cme's. A historical recollection is given on\nhow these ideas were discovered.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 21:19:20 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 19:45:45 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Wu", "C. F. Jeff", ""]]}, {"id": "1703.02177", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang and Paul D. McNicholas", "title": "Mixtures of Generalized Hyperbolic Distributions and Mixtures of Skew-t\n  Distributions for Model-Based Clustering with Incomplete Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2018.08.016", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust clustering from incomplete data is an important topic because, in many\npractical situations, real data sets are heavy-tailed, asymmetric, and/or have\narbitrary patterns of missing observations. Flexible methods and algorithms for\nmodel-based clustering are presented via mixture of the generalized hyperbolic\ndistributions and its limiting case, the mixture of multivariate skew-t\ndistributions. An analytically feasible EM algorithm is formulated for\nparameter estimation and imputation of missing values for mixture models\nemploying missing at random mechanisms. The proposed methodologies are\ninvestigated through a simulation study with varying proportions of synthetic\nmissing values and illustrated using a real dataset. Comparisons are made with\nthose obtained from the traditional mixture of generalized hyperbolic\ndistribution counterparts by filling in the missing data using the mean\nimputation method.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 02:14:38 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 17:47:33 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 18:37:31 GMT"}, {"version": "v4", "created": "Fri, 27 Apr 2018 18:06:09 GMT"}, {"version": "v5", "created": "Sun, 19 Aug 2018 21:50:15 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.02237", "submitter": "Cheng Ju", "authors": "Cheng Ju, Susan Gruber, Samuel D. Lendle, Antoine Chambaz, Jessica M.\n  Franklin, Richard Wyss, Sebastian Schneeweiss, Mark J. van der Laan", "title": "Scalable Collaborative Targeted Learning for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference of a low-dimensional parameter in a large semi-parametric\nmodel relies on external estimators of infinite-dimensional features of the\ndistribution of the data. Typically, only one of the latter is optimized for\nthe sake of constructing a well behaved estimator of the low-dimensional\nparameter of interest. Optimizing more than one of them for the sake of\nachieving a better bias-variance trade-off in the estimation of the parameter\nof interest is the core idea driving the general template of the collaborative\ntargeted minimum loss-based estimation (C-TMLE) procedure. The original\nimplementation/instantiation of the C-TMLE template can be presented as a\ngreedy forward stepwise C-TMLE algorithm. It does not scale well when the\nnumber $p$ of covariates increases drastically. This motivates the introduction\nof a novel instantiation of the C-TMLE template where the covariates are\npre-ordered. Its time complexity is $\\mathcal{O}(p)$ as opposed to the original\n$\\mathcal{O}(p^2)$, a remarkable gain. We propose two pre-ordering strategies\nand suggest a rule of thumb to develop other meaningful strategies. Because it\nis usually unclear a priori which pre-ordering strategy to choose, we also\nintroduce another implementation/instantiation called SL-C-TMLE algorithm that\nenables the data-driven choice of the better pre-ordering strategy given the\nproblem at hand. Its time complexity is $\\mathcal{O}(p)$ as well. The\ncomputational burden and relative performance of these algorithms were compared\nin simulation studies involving fully synthetic data or partially synthetic\ndata based on a real world large electronic health database; and in analyses of\nthree real, large electronic health databases. In all analyses involving\nelectronic health databases, the greedy C-TMLE algorithm is unacceptably slow.\nSimulation studies indicate our scalable C-TMLE and SL-C-TMLE algorithms work\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:40:44 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Ju", "Cheng", ""], ["Gruber", "Susan", ""], ["Lendle", "Samuel D.", ""], ["Chambaz", "Antoine", ""], ["Franklin", "Jessica M.", ""], ["Wyss", "Richard", ""], ["Schneeweiss", "Sebastian", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1703.02296", "submitter": "Genevieve Robin", "authors": "Genevi\\`eve Robin (CMAP, XPOP), Julie Josse (CMAP, XPOP), Eric\n  Moulines (CMAP, LTCI, XPOP), Sylvain Sardy", "title": "Low-rank model with covariates for count data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data are collected in many scientific and engineering tasks including\nimage processing, single-cell RNA sequencing and ecological studies. Such data\nsets often contain missing values, for example because some ecological sites\ncannot be reached in a certain year. In addition, in many instances, side\ninformation is also available, for example covariates about ecological sites or\nspecies. Low-rank methods are popular to denoise and impute count data, and\nbenefit from a substantial theoretical background. Extensions accounting for\ncovariates have been proposed, but to the best of our knowledge their\ntheoretical and empirical properties have not been thoroughly studied, and few\nsoftwares are available for practitioners. We propose a complete methodology\ncalled LORI (Low-Rank Interaction), including a Poisson model, an algorithm,\nand automatic selection of the regularization parameter, to analyze count\ntables with covariates. We also derive an upper bound on the estimation error.\nWe provide a simulation study with synthetic data, revealing empirically that\nLORI improves on state of the art methods in terms of estimation and imputation\nof the missing values. We illustrate how the method can be interpreted through\nvisual displays with the analysis of a well-know plant abundance data set, and\nshow that the LORI outputs are consistent with known results. Finally we\ndemonstrate the relevance of the methodology by analyzing a water-birds\nabundance table from the French national agency for wildlife and hunting\nmanagement (ONCFS). The method is available in the R package lori on the\nComprehensive Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 09:39:00 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 13:04:38 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 10:48:07 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 11:41:54 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Robin", "Genevi\u00e8ve", "", "CMAP, XPOP"], ["Josse", "Julie", "", "CMAP, XPOP"], ["Moulines", "Eric", "", "CMAP, LTCI, XPOP"], ["Sardy", "Sylvain", ""]]}, {"id": "1703.02428", "submitter": "Michael Roth", "authors": "Michael Roth, Tohid Ardeshiri, Emre \\\"Ozkan, Fredrik Gustafsson", "title": "Robust Bayesian Filtering and Smoothing Using Student's t Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State estimation in heavy-tailed process and measurement noise is an\nimportant challenge that must be addressed in, e.g., tracking scenarios with\nagile targets and outlier-corrupted measurements. The performance of the Kalman\nfilter (KF) can deteriorate in such applications because of the close relation\nto the Gaussian distribution. Therefore, this paper describes the use of\nStudent's t distribution to develop robust, scalable, and simple filtering and\nsmoothing algorithms.\n  After a discussion of Student's t distribution, exact filtering in linear\nstate-space models with t noise is analyzed. Intermediate approximation steps\nare used to arrive at filtering and smoothing algorithms that closely resemble\nthe KF and the Rauch-Tung-Striebel (RTS) smoother except for a nonlinear\nmeasurement-dependent matrix update. The required approximations are discussed\nand an undesirable behavior of moment matching for t densities is revealed. A\nfavorable approximation based on minimization of the Kullback-Leibler\ndivergence is presented. Because of its relation to the KF, some properties and\nalgorithmic extensions are inherited by the t filter. Instructive simulation\nexamples demonstrate the performance and robustness of the novel algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 15:13:08 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Roth", "Michael", ""], ["Ardeshiri", "Tohid", ""], ["\u00d6zkan", "Emre", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1703.02462", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin, Jean-Fran\\c{c}ois Coeurjolly, and Fr\\'ed\\'erique\n  Letu\\'e", "title": "Convex and non-convex regularization methods for spatial point processes\n  intensity estimation", "comments": null, "journal-ref": null, "doi": "10.1214/18-EJS1408", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with feature selection procedures for spatial point\nprocesses intensity estimation. We consider regularized versions of estimating\nequations based on Campbell theorem derived from two classical functions:\nPoisson likelihood and logistic regression likelihood. We provide general\nconditions on the spatial point processes and on penalty functions which ensure\nconsistency, sparsity and asymptotic normality. We discuss the numerical\nimplementation and assess finite sample properties in a simulation study.\nFinally, an application to tropical forestry datasets illustrates the use of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 16:38:11 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Letu\u00e9", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1703.02468", "submitter": "Rakesh Malladi", "authors": "Rakesh Malladi, Don H Johnson, and Behnaam Aazhang", "title": "Data-Driven Estimation Of Mutual Information Between Dependent Data", "comments": "Submitted to International Symposium on Information Theory (ISIT)\n  2017. 5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating mutual information between dependent\ndata, an important problem in many science and engineering applications. We\npropose a data-driven, non-parametric estimator of mutual information in this\npaper. The main novelty of our solution lies in transforming the data to\nfrequency domain to make the problem tractable. We define a novel\nmetric--mutual information in frequency--to detect and quantify the dependence\nbetween two random processes across frequency using Cram\\'{e}r's spectral\nrepresentation. Our solution calculates mutual information as a function of\nfrequency to estimate the mutual information between the dependent data over\ntime. We validate its performance on linear and nonlinear models. In addition,\nmutual information in frequency estimated as a part of our solution can also be\nused to infer cross-frequency coupling in the data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 16:50:38 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Malladi", "Rakesh", ""], ["Johnson", "Don H", ""], ["Aazhang", "Behnaam", ""]]}, {"id": "1703.02679", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Matt Barnes, Willie Neiswanger", "title": "Performance Bounds for Graphical Record Linkage", "comments": "11 pages with supplement; 4 figures and 2 tables; to appear in\n  AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage involves merging records in large, noisy databases to remove\nduplicate entities. It has become an important area because of its widespread\noccurrence in bibliometrics, public health, official statistics production,\npolitical science, and beyond. Traditional linkage methods directly linking\nrecords to one another are computationally infeasible as the number of records\ngrows. As a result, it is increasingly common for researchers to treat record\nlinkage as a clustering task, in which each latent entity is associated with\none or more noisy database records. We critically assess performance bounds\nusing the Kullback-Leibler (KL) divergence under a Bayesian record linkage\nframework, making connections to Kolchin partition models. We provide an upper\nbound using the KL divergence and a lower bound on the minimum probability of\nmisclassifying a latent entity. We give insights for when our bounds hold using\nsimulated data and provide practical user guidance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 03:07:37 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Barnes", "Matt", ""], ["Neiswanger", "Willie", ""]]}, {"id": "1703.02724", "submitter": "Anru Zhang", "authors": "Anru Zhang and Dong Xia", "title": "Tensor SVD: Statistical and Computational Limits", "comments": "Typos fixed", "journal-ref": null, "doi": null, "report-no": "IEEE Transactions on Information Theory 64 (11), 7311-7338", "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a general framework for tensor singular value\ndecomposition (tensor SVD), which focuses on the methodology and theory for\nextracting the hidden low-rank structure from high-dimensional tensor data.\nComprehensive results are developed on both the statistical and computational\nlimits for tensor SVD. This problem exhibits three different phases according\nto the signal-to-noise ratio (SNR). In particular, with strong SNR, we show\nthat the classical higher-order orthogonal iteration achieves the minimax\noptimal rate of convergence in estimation; with weak SNR, the\ninformation-theoretical lower bound implies that it is impossible to have\nconsistent estimation in general; with moderate SNR, we show that the\nnon-convex maximum likelihood estimation provides optimal solution, but with\nNP-hard computational cost; moreover, under the hardness hypothesis of\nhypergraphic planted clique detection, there are no polynomial-time algorithms\nperforming consistently in general.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 06:22:56 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 18:54:51 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 17:29:56 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2020 12:35:34 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Anru", ""], ["Xia", "Dong", ""]]}, {"id": "1703.02736", "submitter": "Linglong Kong", "authors": "Qingguo Tang, Linglong Kong, David Ruppert and Rohana J. Karunamuni", "title": "Profile Estimation for Partial Functional Partially Linear Single-Index\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a \\textit{partial functional partially linear single-index\nmodel} that consists of a functional linear component as well as a linear\nsingle-index component. This model generalizes many well-known existing models\nand is suitable for more complicated data structures. However, its estimation\ninherits the difficulties and complexities from both components and makes it a\nchallenging problem, which calls for new methodology. We propose a novel\nprofile B-spline method to estimate the parameters by approximating the unknown\nnonparametric link function in the single-index component part with B-spline,\nwhile the linear slope function in the functional component part is estimated\nby the functional principal component basis. The consistency and asymptotic\nnormality of the parametric estimators are derived, and the global convergence\nof the proposed estimator of the linear slope function is also established.\nMore excitingly, the latter convergence is optimal in the minimax sense. A\ntwo-stage procedure is implemented to estimate the nonparametric link function,\nand the resulting estimator possesses the optimal global rate of convergence.\nFurthermore, the convergence rate of the mean squared prediction error for a\npredictor is also obtained. Empirical properties of the proposed procedures are\nstudied through Monte Carlo simulations. A real data example is also analyzed\nto illustrate the power and flexibility of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 07:41:13 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tang", "Qingguo", ""], ["Kong", "Linglong", ""], ["Ruppert", "David", ""], ["Karunamuni", "Rohana J.", ""]]}, {"id": "1703.02834", "submitter": "Pierre-Alexandre Mattei", "authors": "Charles Bouveyron (EPIONE, JAD), Pierre Latouche (MAP5 - UMR 8145),\n  Pierre-Alexandre Mattei", "title": "Exact Dimensionality Selection for Bayesian PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian model selection approach to estimate the intrinsic\ndimensionality of a high-dimensional dataset. To this end, we introduce a novel\nformulation of the probabilisitic principal component analysis model based on a\nnormal-gamma prior distribution. In this context, we exhibit a closed-form\nexpression of the marginal likelihood which allows to infer an optimal number\nof components. We also propose a heuristic based on the expected shape of the\nmarginal likelihood curve in order to choose the hyperparameters. In\nnon-asymptotic frameworks, we show on simulated data that this exact\ndimensionality selection approach is competitive with both Bayesian and\nfrequentist state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 13:47:17 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 13:00:55 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Bouveyron", "Charles", "", "EPIONE, JAD"], ["Latouche", "Pierre", "", "MAP5 - UMR 8145"], ["Mattei", "Pierre-Alexandre", ""]]}, {"id": "1703.03022", "submitter": "Kiranmoy Chatterjee Dr.", "authors": "Kiranmoy Chatterjee and Prajamitra Bhuyan", "title": "On the estimation of population size from a post-stratified two sample\n  capture-recapture data under dependence", "comments": "26 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation based on two sample capture-recapture type\nexperiment is an interesting problem in various fields including epidemiology,\npubic health, population studies, etc. The Lincoln-Petersen estimate is\npopularly used under the assumption that capture and recapture status of each\nindividual is independent. However, in many real life scenarios, there is an\ninherent dependency between capture and recapture attempts which is not\nwell-studied in the literature of the dual system or two sample\ncapture-recapture method. In this article, we propose a novel model that\nsuccessfully incorporates the possible causal dependency and provide\ncorresponding estimation methodologies for the associated model parameters\nbased on post-stratified two sample capture-recapture data. The superiority of\nthe performance of the proposed model over the existing competitors is\nestablished through an extensive simulation study. The method is illustrated\nthrough analysis of some real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 20:14:54 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 20:10:01 GMT"}, {"version": "v3", "created": "Sat, 20 May 2017 10:51:07 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 17:13:13 GMT"}, {"version": "v5", "created": "Thu, 17 Jan 2019 19:31:05 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "1703.03023", "submitter": "Michael Evans", "authors": "Michael Evans, Irwin Guttman, Peiying Li", "title": "Elicitation, measuring bias, checking for prior-data conflict and\n  inference with a Dirichlet prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods are developed for eliciting a Dirichlet prior based upon bounds on\nthe individual probabilities that hold with virtual certainty. This approach to\nselecting a prior is applied to a contingency table problem where it is\ndemonstrated how to assess the bias in the prior as well as how to check for\nprior-data conflict. It is shown that the assessment of a hypothesis via\nrelative belief can easily take into account what it means for the falsity of\nthe hypothesis to correspond to a difference of practical importance and\nprovide evidence in favor of a hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 20:32:20 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Evans", "Michael", ""], ["Guttman", "Irwin", ""], ["Li", "Peiying", ""]]}, {"id": "1703.03043", "submitter": "Konrad Menzel", "authors": "Konrad Menzel", "title": "Bootstrap with Clustering in Two or More Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a bootstrap procedure for data that may exhibit clustering in two\nor more dimensions. We use insights from the theory of generalized U-statistics\nto analyze the large-sample properties of statistics that are sample averages\nfrom the observations pooled across clusters. The asymptotic distribution of\nthese statistics may be non-standard if there is no clustering in means. We\nshow that the proposed bootstrap procedure is (a) point-wise consistent for any\nfixed data-generating process (DGP), (b) uniformly consistent if we exclude the\ncase of clustering without clustering in means, and (c) provides refinements\nfor any DGP such that the limiting distribution is Gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 21:40:57 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 22:35:58 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 22:40:59 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Menzel", "Konrad", ""]]}, {"id": "1703.03095", "submitter": "Phyllis Wan", "authors": "Phyllis Wan, Tiandong Wang, Richard A. Davis, Sidney I. Resnick", "title": "Fitting the Linear Preferential Attachment Model", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preferential attachment is an appealing mechanism for modeling power-law\nbehavior of the degree distributions in directed social networks. In this\npaper, we consider methods for fitting a 5-parameter linear preferential model\nto network data under two data scenarios. In the case where full history of the\nnetwork formation is given, we derive the maximum likelihood estimator of the\nparameters and show that it is strongly consistent and asymptotically normal.\nIn the case where only a single-time snapshot of the network is available, we\npropose an estimation method which combines method of moments with an\napproximation to the likelihood. The resulting estimator is also strongly\nconsistent and performs quite well compared to the MLE estimator. We illustrate\nboth estimation procedures through simulated data, and explore the usage of\nthis model in a real data example.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 01:19:39 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 17:21:50 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wan", "Phyllis", ""], ["Wang", "Tiandong", ""], ["Davis", "Richard A.", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "1703.03123", "submitter": "Leo Duan", "authors": "Leo L. Duan, James E. Johndrow, David B. Dunson", "title": "Scaling up Data Augmentation MCMC via Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable interest in making Bayesian inference more\nscalable. In big data settings, most literature focuses on reducing the\ncomputing time per iteration, with less focused on reducing the number of\niterations needed in Markov chain Monte Carlo (MCMC). This article focuses on\ndata augmentation MCMC (DA-MCMC), a widely used technique. DA-MCMC samples tend\nto become highly autocorrelated in large data samples, due to a miscalibration\nproblem in which conditional posterior distributions given augmented data are\ntoo concentrated. This makes it necessary to collect very long MCMC paths to\nobtain acceptably low MC error. To combat this inefficiency, we propose a\nfamily of calibrated data augmentation algorithms, which appropriately adjust\nthe variance of conditional posterior distributions. A Metropolis-Hastings step\nis used to eliminate bias in the stationary distribution of the resulting\nsampler. Compared to existing alternatives, this approach can dramatically\nreduce MC error by reducing autocorrelation and increasing the effective number\nof DA-MCMC samples per computing time. The approach is simple and applicable to\na broad variety of existing data augmentation algorithms, and we focus on three\npopular models: probit, logistic and Poisson log-linear. Dramatic gains in\ncomputational efficiency are shown in applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 03:49:33 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 22:39:45 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Duan", "Leo L.", ""], ["Johndrow", "James E.", ""], ["Dunson", "David B.", ""]]}, {"id": "1703.03165", "submitter": "Debraj Das", "authors": "Debraj Das, Karl Gregory, S. N. Lahiri", "title": "Perturbation Bootstrap in Adaptive Lasso", "comments": "43 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Adaptive Lasso(Alasso) was proposed by Zou [\\textit{J. Amer. Statist.\nAssoc. \\textbf{101} (2006) 1418-1429}] as a modification of the Lasso for the\npurpose of simultaneous variable selection and estimation of the parameters in\na linear regression model. Zou (2006) established that the Alasso estimator is\nvariable-selection consistent as well as asymptotically Normal in the indices\ncorresponding to the nonzero regression coefficients in certain\nfixed-dimensional settings. In an influential paper, Minnier, Tian and Cai\n[\\textit{J. Amer. Statist. Assoc. \\textbf{106} (2011) 1371-1382}] proposed a\nperturbation bootstrap method and established its distributional consistency\nfor the Alasso estimator in the fixed-dimensional setting. In this paper,\nhowever, we show that this (naive) perturbation bootstrap fails to achieve\nsecond order correctness in approximating the distribution of the Alasso\nestimator. We propose a modification to the perturbation bootstrap objective\nfunction and show that a suitably studentized version of our modified\nperturbation bootstrap Alasso estimator achieves second-order correctness even\nwhen the dimension of the model is allowed to grow to infinity with the sample\nsize. As a consequence, inferences based on the modified perturbation bootstrap\nwill be more accurate than the inferences based on the oracle Normal\napproximation. We give simulation studies demonstrating good finite-sample\nproperties of our modified perturbation bootstrap method as well as an\nillustration of our method on a real data set.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 07:27:33 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 19:55:40 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Das", "Debraj", ""], ["Gregory", "Karl", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1703.03213", "submitter": "M.I. Borrajo", "authors": "M.I. Borrajo, W. Gonz\\'alez-Manteiga and M.D. Mart\\'inez-Miranda", "title": "Bootstrapping kernel intensity estimation for nonhomogeneous point\n  processes depending on spatial covariates", "comments": "32 pages, 7 figures (15 images), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the spatial point process context, kernel intensity estimation has been\nmainly restricted to exploratory analysis due to its lack of consistency.\nDifferent methods have been analysed to overcome this problem, and the\ninclusion of covariates resulted to be one possible solution. In this paper we\nfocus on de\\-fi\\-ning a theoretical framework to derive a consistent kernel\nintensity estimator using covariates, as well as a consistent smooth bootstrap\nprocedure. We define two new data-driven bandwidth selectors specifically\ndesigned for our estimator: a rule-of-thumb and a plug-in bandwidth based on\nour consistent bootstrap method. A simulation study is accomplished to\nunderstand the performance of our proposals in finite samples. Finally, we\ndescribe an application to a real data set consisting of the wildfires in\nCanada during June 2015, using meteorological information as covariates.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 10:19:53 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 18:35:33 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 14:07:22 GMT"}, {"version": "v4", "created": "Fri, 18 May 2018 14:56:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Borrajo", "M. I.", ""], ["Gonz\u00e1lez-Manteiga", "W.", ""], ["Mart\u00ednez-Miranda", "M. D.", ""]]}, {"id": "1703.03312", "submitter": "Adam Jaeger", "authors": "Adam Jaeger and Nicole Lazar", "title": "Split Sample Empirical Likelihood", "comments": "Paper has been rejected on grounds it is trivial", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach that combines multiple non-parametric\nlikelihood-type components to build a data-driven approximation of the true\nlikelihood function. Our approach is built on empirical likelihood, a\nnon-parametric approximation of the likelihood function. We show the asymptotic\nbehaviors of our approach are identical to those seen in empirical likelihood.\nWe demonstrate that our method performs comparably to empirical likelihood\nwhile significantly decreasing computational time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 15:56:58 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 18:40:02 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Jaeger", "Adam", ""], ["Lazar", "Nicole", ""]]}, {"id": "1703.03882", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje, Michael J. Higgins and Jasjeet S. Sekhon", "title": "Generalized full matching and extrapolation of the results from a\n  large-scale voter mobilization experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching is an important tool in causal inference. The method provides a\nconceptually straightforward way to make groups of units comparable on observed\ncharacteristics. The use of the method is, however, limited to situations where\nthe study design is fairly simple and the sample is moderately sized. We\nillustrate the issue by revisiting a large-scale voter mobilization experiment\nthat took place in Michigan for the 2006 election. We ask what the causal\neffects would have been if the treatments in the experiment were scaled up to\nthe full population. Matching could help us answer this question, but no\nexisting matching method can accommodate the six treatment arms and the\n6,762,701 observations involved in the study. To offer a solution this and\nsimilar empirical problems, we introduce a generalization of the full matching\nmethod and an associated algorithm. The method can be used with any number of\ntreatment conditions, and it is shown to produce near-optimal matchings. The\nworst case maximum within-group dissimilarity is no worse than four times the\noptimal solution, and simulation results indicate that its performance is\nconsiderably closer to the optimal solution on average. Despite its\nperformance, the algorithm is fast and uses little memory. It terminates, on\naverage, in linearithmic time using linear space. This enables investigators to\nconstruct well-performing matchings within minutes even in complex studies with\nsamples of several million units.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 00:54:13 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 03:37:11 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["S\u00e4vje", "Fredrik", ""], ["Higgins", "Michael J.", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1703.04025", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Jiaying Gu, Qing Zhou", "title": "Learning Large-Scale Bayesian Networks with the sparsebn Package", "comments": "To appear in the Journal of Statistical Software, 39 pages, 7 figures", "journal-ref": "Journal of Statistical Software, 91(11), 1-38, 2019", "doi": "10.18637/jss.v091.i11", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning graphical models from data is an important problem with wide\napplications, ranging from genomics to the social sciences. Nowadays datasets\noften have upwards of thousands---sometimes tens or hundreds of thousands---of\nvariables and far fewer samples. To meet this challenge, we have developed a\nnew R package called sparsebn for learning the structure of large, sparse\ngraphical models with a focus on Bayesian networks. While there are many\nexisting software packages for this task, this package focuses on the unique\nsetting of learning large networks from high-dimensional data, possibly with\ninterventions. As such, the methods provided place a premium on scalability and\nconsistency in a high-dimensional setting. Furthermore, in the presence of\ninterventions, the methods implemented here achieve the goal of learning a\ncausal network from data. Additionally, the sparsebn package is fully\ncompatible with existing software packages for network analysis.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 20:07:06 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 23:22:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Aragam", "Bryon", ""], ["Gu", "Jiaying", ""], ["Zhou", "Qing", ""]]}, {"id": "1703.04157", "submitter": "Tyler McCormick", "authors": "Emily Breza, Arun G. Chandrasekhar, Tyler H. McCormick, Mengjie Pan", "title": "Using Aggregated Relational Data to feasibly identify network structure\n  without network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network data is often prohibitively expensive to collect, limiting\nempirical network research. Typical economic network mapping requires (1)\nenumerating a census, (2) eliciting the names of all network links for each\nindividual, (3) matching the list of social connections to the census, and (4)\nrepeating (1)-(3) across many networks. In settings requiring field surveys,\nsteps (2)-(3) can be very expensive. In other network populations such as\nfinancial intermediaries or high-risk groups, proprietary data and privacy\nconcerns may render (2)-(3) impossible. Both restrict the accessibility of\nhigh-quality networks research to investigators with considerable resources.\n  We propose an inexpensive and feasible strategy for network elicitation using\nAggregated Relational Data (ARD) -- responses to questions of the form \"How\nmany of your social connections have trait k?\" Our method uses ARD to recover\nthe parameters of a general network formation model, which in turn, permits the\nestimation of any arbitrary node- or graph-level statistic. The method works\nwell in simulations and in matching a range of network characteristics in\nreal-world graphs from 75 Indian villages. Moreover, we replicate the results\nof two field experiments that involved collecting network data. We show that\nthe researchers would have drawn similar conclusions using ARD alone. Finally,\nusing calculations from J-PAL fieldwork, we show that in rural India, for\nexample, ARD surveys are 80% cheaper than full network surveys.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 18:29:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 17:46:50 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 06:24:54 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Breza", "Emily", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""], ["Pan", "Mengjie", ""]]}, {"id": "1703.04180", "submitter": "Minkyoung Kang", "authors": "Minkyoung Kang and Brani Vidakovic", "title": "MEDL and MEDLA: Methods for Assessment of Scaling by Medians of\n  Log-Squared Nondecimated Wavelet Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-frequency measurements and images acquired from various sources in the\nreal world often possess a degree of self-similarity and inherent regular\nscaling. When data look like a noise, the scaling exponent may be the only\ninformative feature that summarizes such data. Methods for the assessment of\nself-similarity by estimating Hurst exponent often involve analysis of rate of\ndecay in a spectrum defined in various multiresolution domains. When this\nspectrum is calculated using discrete non-decimated wavelet transforms, due to\nincreased autocorrelation in wavelet coefficients, the estimators of $H$ show\nincreased bias compared to the estimators that use traditional orthogonal\ntransforms. At the same time, non-decimated transforms have a number of\nadvantages when employed for calculation of wavelet spectra and estimation of\nHurst exponents: the variance of the estimator is smaller, input signals and\nimages could be of arbitrary size, and due to the shift-invariance, the local\nscaling can be assessed as well. We propose two methods based on robust\nestimation and resampling that alleviate the effect of increased\nautocorrelation while maintaining all advantages of non-decimated wavelet\ntransforms. The proposed methods extend the approaches in existing literature\nwhere the logarithmic transformation and pairing of wavelet coefficients are\nused for lowering the bias. In a simulation study we use fractional Brownian\nmotions with a range of theoretical Hurst exponents. For such signals for which\n\"true\" $H$ is known, we demonstrate bias reduction and overall reduction of the\nmean-squared error by the two proposed estimators. For fractional Brownian\nmotions, both proposed methods yield estimators of $H$ that are asymptotically\nnormal and unbiased.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 21:19:29 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Kang", "Minkyoung", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1703.04264", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Jason L. Williams, Karl Granstr\\\"om,\n  Lennart Svensson", "title": "Poisson multi-Bernoulli mixture filter: direct derivation and\n  implementation", "comments": null, "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, vol. 54,\n  no. 4, pp. 1883-1901, Aug. 2018", "doi": "10.1109/TAES.2018.2805153", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a derivation of the Poisson multi-Bernoulli mixture (PMBM) filter\nfor multi-target tracking with the standard point target measurements without\nusing probability generating functionals or functional derivatives. We also\nestablish the connection with the \\delta-generalised labelled multi-Bernoulli\n(\\delta-GLMB) filter, showing that a \\delta-GLMB density represents a\nmulti-Bernoulli mixture with labelled targets so it can be seen as a special\ncase of PMBM. In addition, we propose an implementation for linear/Gaussian\ndynamic and measurement models and how to efficiently obtain typical estimators\nin the literature from the PMBM. The PMBM filter is shown to outperform other\nfilters in the literature in a challenging scenario.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 06:08:51 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 11:23:37 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 17:42:30 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 14:35:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Williams", "Jason L.", ""], ["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""]]}, {"id": "1703.04334", "submitter": "Fani Tsapeli", "authors": "Fani Tsapeli, Peter Tino, Mirco Musolesi", "title": "Probabilistic Matching: Causal Inference under Measurement Errors", "comments": "In Proceedings of International Joint Conference Of Neural Networks\n  (IJCNN) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of data produced daily from large variety of sources has\nboosted the need of novel approaches on causal inference analysis from\nobservational data. Observational data often contain noisy or missing entries.\nMoreover, causal inference studies may require unobserved high-level\ninformation which needs to be inferred from other observed attributes. In such\ncases, inaccuracies of the applied inference methods will result in noisy\noutputs. In this study, we propose a novel approach for causal inference when\none or more key variables are noisy. Our method utilizes the knowledge about\nthe uncertainty of the real values of key variables in order to reduce the bias\ninduced by noisy measurements. We evaluate our approach in comparison with\nexisting methods both on simulated and real scenarios and we demonstrate that\nour method reduces the bias and avoids false causal inference conclusions in\nmost cases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:28 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Tsapeli", "Fani", ""], ["Tino", "Peter", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1703.04882", "submitter": "Jonathan Lilly", "authors": "J. M. Lilly", "title": "Element analysis: a wavelet-based method for analyzing time-localized\n  events in noisy time series", "comments": "In press", "journal-ref": null, "doi": "10.1098/rspa.2016.0776", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is derived for the quantitative analysis of signals that are\ncomposed of superpositions of isolated, time-localized \"events\". Here these\nevents are taken to be well represented as rescaled and phase-rotated versions\nof generalized Morse wavelets, a broad family of continuous analytic functions.\nAnalyzing a signal composed of replicates of such a function using another\nMorse wavelet allows one to directly estimate the properties of events from the\nvalues of the wavelet transform at its own maxima. The distribution of events\nin general power-law noise is determined in order to establish significance\nbased on an expected false detection rate. Finally, an expression for an\nevent's \"region of influence\" within the wavelet transform permits the\nformation of a criterion for rejecting spurious maxima due to numerical\nartifacts or other unsuitable events. Signals can then be reconstructed based\non a small number of isolated points on the time/scale plane. This method,\ntermed element analysis, is applied to the identification of long-lived eddy\nstructures in ocean currents as observed by along-track measurements of sea\nsurface elevation from satellite altimetry\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:08:07 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 19:08:28 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Lilly", "J. M.", ""]]}, {"id": "1703.04951", "submitter": "Fatma Sevinc Kurnaz", "authors": "Fatma Sevinc Kurnaz, Irene Hoffmann, Peter Filzmoser", "title": "Robust and sparse estimation methods for high dimensional linear and\n  logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully robust versions of the elastic net estimator are introduced for linear\nand logistic regression. The algorithms to compute the estimators are based on\nthe idea of repeatedly applying the non-robust classical estimators to data\nsubsets only. It is shown how outlier-free subsets can be identified\nefficiently, and how appropriate tuning parameters for the elastic net\npenalties can be selected. A final reweighting step improves the efficiency of\nthe estimators. Simulation studies compare with non-robust and other competing\nrobust estimators and reveal the superiority of the newly proposed methods.\nThis is also supported by a reasonable computation time and by good performance\nin real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:17:40 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kurnaz", "Fatma Sevinc", ""], ["Hoffmann", "Irene", ""], ["Filzmoser", "Peter", ""]]}, {"id": "1703.04956", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee, Trisha Maitra and Sourabh Bhattacharya", "title": "A Short Note on Almost Sure Convergence of Bayes Factors in the General\n  Set-Up", "comments": "To appear in The American Statistician", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is a significant literature on the asymptotic theory of Bayes\nfactor, the set-ups considered are usually specialized and often involves\nindependent and identically distributed data. Even in such specialized cases,\nmostly weak consistency results are available. In this article, for the first\ntime ever, we derive the almost sure convergence theory of Bayes factor in the\ngeneral set-up that includes even dependent data and misspecified models.\nSomewhat surprisingly, the key to the proof of such a general theory is a\nsimple application of a result of Shalizi (2009) to a well-known identity\nsatisfied by the Bayes factor.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:34:38 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 17:14:14 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 13:46:13 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 11:30:01 GMT"}, {"version": "v5", "created": "Tue, 17 Apr 2018 11:09:53 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1703.05109", "submitter": "Yuya Sasaki", "authors": "Heng Chen and Harold D. Chiang and Yuya Sasaki", "title": "Quantile Treatment Effects in Regression Kink Designs", "comments": null, "journal-ref": "Econom. Theory 36 (2020) 1167-1191", "doi": "10.1017/S0266466619000409", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on regression kink designs develops identification results for\naverage effects of continuous treatments (Card, Lee, Pei, and Weber, 2015),\naverage effects of binary treatments (Dong, 2018), and quantile-wise effects of\ncontinuous treatments (Chiang and Sasaki, 2019), but there has been no\nidentification result for quantile-wise effects of binary treatments to date.\nIn this paper, we fill this void in the literature by providing an\nidentification of quantile treatment effects in regression kink designs with\nbinary treatment variables. For completeness, we also develop large sample\ntheories for statistical inference and a practical guideline on estimation and\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:18:21 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 15:45:45 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Heng", ""], ["Chiang", "Harold D.", ""], ["Sasaki", "Yuya", ""]]}, {"id": "1703.05157", "submitter": "Olga Savchuk Y", "authors": "Olga Y. Savchuk", "title": "One-Sided Cross-Validation for Nonsmooth Density Functions", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-sided cross-validation (OSCV) is a bandwidth selection method initially\nintroduced by Hart and Yi (1998) in the context of smooth regression functions.\nMart\\'{\\i}nez-Miranda et al. (2009) developed a version of OSCV for smooth\ndensity functions. This article extends the method for nonsmooth densities. It\nalso introduces the fully robust OSCV modification that produces consistent\nOSCV bandwidths for both smooth and nonsmooth cases. Practical implementations\nof the OSCV method for smooth and nonsmooth densities are discussed. One of the\nconsidered cross-validation kernels has potential for improving the OSCV\nmethod's implementation in the regression context.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 13:51:52 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Savchuk", "Olga Y.", ""]]}, {"id": "1703.05189", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Filip Tronarp, Toni Karvonen, Simo S\\\"arkk\\\"a and\n  Ond\\v{r}ej Straka", "title": "Student-t Process Quadratures for Filtering of Non-Linear Systems with\n  Heavy-Tailed Noise", "comments": "15 pages, 3 figures, submitted to 20th International Conference on\n  Information Fusion, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to design a moment transformation for Student- t\ndistributed random variables, which is able to account for the error in the\nnumerically computed mean. We employ Student-t process quadrature, an instance\nof Bayesian quadrature, which allows us to treat the integral itself as a\nrandom variable whose variance provides information about the incurred\nintegration error. Advantage of the Student- t process quadrature over the\ntraditional Gaussian process quadrature, is that the integral variance depends\nalso on the function values, allowing for a more robust modelling of the\nintegration error. The moment transform is applied in nonlinear sigma-point\nfiltering and evaluated on two numerical examples, where it is shown to\noutperform the state-of-the-art moment transforms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 14:47:02 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 09:36:40 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Tronarp", "Filip", ""], ["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Straka", "Ond\u0159ej", ""]]}, {"id": "1703.05203", "submitter": "Daniel Kraus", "authors": "Daniel Kraus and Claudia Czado", "title": "Growing simplified vine copula trees: improving Di{\\ss}mann's algorithm", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are pair-copula constructions enabling multivariate dependence\nmodeling in terms of bivariate building blocks. One of the main tasks of\nfitting a vine copula is the selection of a suitable tree structure. For this\nthe prevalent method is a heuristic called Di{\\ss}mann's algorithm. It\nsequentially constructs the vine's trees by maximizing dependence at each tree\nlevel, where dependence is measured in terms of absolute Kendall's $\\tau$.\nHowever, the algorithm disregards any implications of the tree structure on the\nsimplifying assumption that is usually made for vine copulas to keep inference\ntractable. We develop two new algorithms that select tree structures focused on\nproducing simplified vine copulas for which the simplifying assumption is\nviolated as little as possible. For this we make use of a recently developed\nstatistical test of the simplifying assumption. In a simulation study we show\nthat our proposed methods outperform the benchmark given by Di{\\ss}mann's\nalgorithm by a great margin. Several real data applications emphasize their\npractical relevance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 15:19:14 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Kraus", "Daniel", ""], ["Czado", "Claudia", ""]]}, {"id": "1703.05208", "submitter": "Dorian Cazau", "authors": "D. Cazau and G. Nuel", "title": "Understanding the Probabilistic Latent Component Analysis Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Component Latent Analysis (PLCA) is a statistical modeling\nmethod for feature extraction from non-negative data. It has been fruitfully\napplied to various research fields of information retrieval. However, the\nEM-solved optimization problem coming with the parameter estimation of\nPLCA-based models has never been properly posed and justified. We then propose\nin this short paper to re-define the theoretical framework of this problem,\nwith the motivation of making it clearer to understand, and more admissible for\nfurther developments of PLCA-based computational systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 15:27:46 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Cazau", "D.", ""], ["Nuel", "G.", ""]]}, {"id": "1703.05264", "submitter": "Bowei Yan", "authors": "Ying Liu, Bowei Yan, Kathleen Merikangas and Haochang Shou", "title": "Total Variation Regularized Tensor-on-scalar Regression", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Total Variation Regularized Tensor-on-scalar\nRegression(TVTR), a novel method for estimating the association between a\ntensor outcome (a one dimensional or multidimensional array) and scalar\npredictors. While the statistical developments proposed here were motivated by\nthe brain mapping and activity tracking, the methodology is designed and\npresented in generality and is applicable to many other areas of scientific\nresearch. The estimator is the solution of a penalized regression problem where\nthe objective is the sum of square error plus a total variation (TV)\nregularization on the predicted mean across all subjects. We propose an\nalgorithm for the parameter estimation, which is efficient and scalable in\ndistributed computing platform. Proof of the algorithm convergence is provided\nand the statistical consistency of the estimator is presented via an oracle\ninequality. We presented 1D and 2D simulation results, and demonstrate that\nTVTR outperforms existing methods in most cases. We also demonstrate the\ngeneral applicability of the method by two real data examples including the\nanalysis of the 1D accelerometry subsample of a large community-based study for\nmood disorders and the analysis of the 3D MRI data from the attention\ndeficient/hyperactive deficient (ADHD) 200 consortium.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 17:03:56 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 22:37:24 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 22:48:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Liu", "Ying", ""], ["Yan", "Bowei", ""], ["Merikangas", "Kathleen", ""], ["Shou", "Haochang", ""]]}, {"id": "1703.05312", "submitter": "Bruno Sudret", "authors": "N. Fajraoui, S. Marelli, B. Sudret", "title": "On optimal experimental designs for Sparse Polynomial Chaos Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-001", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification (UQ) has received much attention in the literature\nin the past decade. In this context, Sparse Polynomial chaos expansions (PCE)\nhave been shown to be among the most promising methods because of their ability\nto model highly complex models at relatively low computational costs. A\nleast-square minimization technique may be used to determine the coefficients\nof the sparse PCE by relying on the so called experimental design (ED), i.e.\nthe sample points where the original computational model is evaluated. An\nefficient sampling strategy is then needed to generate an accurate PCE at low\ncomputational cost. This paper is concerned with the problem of identifying an\noptimal experimental design that maximizes the accuracy of the surrogate model\nover the whole input space within a given computational budget. A novel\nsequential adaptive strategy where the ED is enriched sequentially by\ncapitalizing on the sparsity of the underlying metamodel is introduced. A\ncomparative study between several state-of-the-art methods is performed on four\nnumerical models with varying input dimensionality and computational\ncomplexity. It is shown that the optimal sequential design based on the S-value\ncriterion yields accurate, stable and computationally efficient PCE.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 08:09:27 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Fajraoui", "N.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1703.05782", "submitter": "Lala Khadidja Hamaidi", "authors": "Mohamad Hasan Bahari, L. Khadidja Hamaidi, Michael Muma, Jorge\n  Plata-Chaves, Marc Moonen, Abdelhak M. Zoubir, Alexander Bertrand", "title": "Distributed Multi-Speaker Voice Activity Detection for Wireless Acoustic\n  Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A distributed multi-speaker voice activity detection (DM-VAD) method for\nwireless acoustic sensor networks (WASNs) is proposed. DM-VAD is required in\nmany signal processing applications, e.g. distributed speech enhancement based\non multi-channel Wiener filtering, but is non-existent up to date. The proposed\nmethod neither requires a fusion center nor prior knowledge about the node\npositions, microphone array orientations or the number of observed sources. It\nconsists of two steps: (i) distributed source-specific energy signal unmixing\n(ii) energy signal based voice activity detection. Existing computationally\nefficient methods to extract source-specific energy signals from the mixed\nobservations, e.g., multiplicative non-negative independent component analysis\n(MNICA) quickly loose performance with an increasing number of sources, and\nrequire a fusion center. To overcome these limitations, we introduce a\ndistributed energy signal unmixing method based on a source-specific node\nclustering method to locate the nodes around each source. To determine the\nnumber of sources that are observed in the WASN, a source enumeration method\nthat uses a Lasso penalized Poisson generalized linear model is developed. Each\nidentified cluster estimates the energy signal of a single (dominant) source by\napplying a two-component MNICA. The VAD problem is transformed into a\nclustering task, by extracting features from the energy signals and applying\nK-means type clustering algorithms. All steps of the proposed method are\nevaluated using numerical experiments. A VAD accuracy of $> 85 \\%$ is achieved\nfor a challenging scenario where 20 nodes observe 7 sources in a simulated\nreverberant rectangular room.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 18:14:59 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Bahari", "Mohamad Hasan", ""], ["Hamaidi", "L. Khadidja", ""], ["Muma", "Michael", ""], ["Plata-Chaves", "Jorge", ""], ["Moonen", "Marc", ""], ["Zoubir", "Abdelhak M.", ""], ["Bertrand", "Alexander", ""]]}, {"id": "1703.05794", "submitter": "Gen Li", "authors": "Gen Li and Sungkyu Jung", "title": "Incorporating Covariates into Integrated Factor Analysis of Multi-View\n  Data", "comments": "To appear in Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern biomedical research, it is ubiquitous to have multiple data sets\nmeasured on the same set of samples from different views (i.e., multi-view\ndata). For example, in genetic studies, multiple genomic data sets at different\nmolecular levels or from different cell types are measured for a common set of\nindividuals to investigate genetic regulation. Integration and reduction of\nmulti-view data have the potential to leverage information in different data\nsets, and to reduce the magnitude and complexity of data for further\nstatistical analysis and interpretation. In this paper, we develop a novel\nstatistical model, called supervised integrated factor analysis (SIFA), for\nintegrative dimension reduction of multi-view data while incorporating\nauxiliary covariates. The model decomposes data into joint and individual\nfactors, capturing the joint variation across multiple data sets and the\nindividual variation specific to each set respectively. Moreover, both joint\nand individual factors are partially informed by auxiliary covariates via\nnonparametric models. We devise a computationally efficient\nExpectation-Maximization (EM) algorithm to fit the model under some\nidentifiability conditions. We apply the method to the Genotype-Tissue\nExpression (GTEx) data, and provide new insights into the variation\ndecomposition of gene expression in multiple tissues. Extensive simulation\nstudies and an additional application to a pediatric growth study demonstrate\nthe advantage of the proposed method over competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 19:04:22 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Li", "Gen", ""], ["Jung", "Sungkyu", ""]]}, {"id": "1703.05849", "submitter": "Marc Ratkovic", "authors": "Marc Ratkovic and Dustin Tingley", "title": "Estimation and Inference on Nonlinear and Heterogeneous Effects", "comments": "Unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple regression has been the go-to method for data analysis for\ngenerations of scholars due to its transparency, interpretability, and\ndesirable theoretical properties. However, the method's simplicity precludes\nthe discovery of complex heterogeneities in the data. We introduce the Method\nof Direct Estimation and Inference (MDEI) that embraces these potential\ncomplexities, is interpretable, has desirable theoretical guarantees, and,\nunlike some existing methods, returns appropriate uncertainty estimates. The\nproposed method uses a machine learning regression methodology to estimate the\nobservation-level effect of a treatment variable. Importantly, we introduce a\nrobust approach to uncertainty estimates. We provide simulation evidence and an\napplication illustrating the performance of the method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 23:33:24 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 20:29:07 GMT"}, {"version": "v3", "created": "Sat, 30 Jan 2021 20:30:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ratkovic", "Marc", ""], ["Tingley", "Dustin", ""]]}, {"id": "1703.05899", "submitter": "John W Jackson", "authors": "John W. Jackson, Tyler J. VanderWeele", "title": "Decomposition analysis to identify intervention targets for reducing\n  disparities", "comments": "John Jackson is Assistant Professor in the Departments of\n  Epidemiology and Mental Health at the Johns Hopkins Bloomberg School of\n  Public Health and Tyler VanderWeele is Professor in the Departments of\n  Epidemiology and Biostatistics at the Harvard T.H. Chan School of Public\n  Health. Correspondence to john.jackson@jhu.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable interest in using decomposition methods in\nepidemiology (mediation analysis) and economics (Oaxaca-Blinder decomposition)\nto understand how health disparities arise and how they might change upon\nintervention. It has not been clear when estimates from the Oaxaca-Blinder\ndecomposition can be interpreted causally because its implementation does not\nexplicitly address potential confounding of target variables. While mediation\nanalysis does explicitly adjust for confounders of target variables, it does so\nin a way that entails equalizing confounders across racial groups, which may\nnot reflect the intended intervention. Revisiting prior analyses in the\nNational Longitudinal Survey of Youth on disparities in wages, unemployment,\nincarceration, and overall health with test scores, taken as a proxy for\neducational attainment, as a target intervention, we propose and demonstrate a\nnovel decomposition that controls for confounders of test scores (measures of\nchildhood SES) while leaving their association with race intact. We compare\nthis decomposition with others that use standardization (to equalize childhood\nSES alone), mediation analysis (to equalize test scores within levels of\nchildhood SES), and one that equalizes both childhood SES and test scores. We\nalso show how these decompositions, including our novel proposals, are\nequivalent to causal implementations of the Oaxaca-Blinder decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 05:38:07 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Jackson", "John W.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1703.06001", "submitter": "Linda Altieri", "authors": "Linda Altieri, Daniela Cocchi, Giulia Roli", "title": "The use of spatial information in entropy measures", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of entropy, firstly introduced in information theory, rapidly\nbecame popular in many applied sciences via Shannon's formula to measure the\ndegree of heterogeneity among observations. A rather recent research field aims\nat accounting for space in entropy measures, as a generalization when the\nspatial location of occurrences ought to be accounted for. The main limit of\nthese developments is that all indices are computed conditional on a chosen\ndistance. This work follows and extends the route for including spatial\ncomponents in entropy measures. Starting from the probabilistic properties of\nShannon's entropy for categorical variables, it investigates the\ncharacteristics of the quantities known as residual entropy and mutual\ninformation, when space is included as a second dimension. This way, the\nproposal of entropy measures based on univariate distributions is extended to\nthe consideration of bivariate distributions, in a setting where the\nprobabilistic meaning of all components is well defined. As a direct\nconsequence, a spatial entropy measure satisfying the additivity property is\nobtained, as global residual entropy is a sum of partial entropies based on\ndifferent distance classes. Moreover, the quantity known as mutual information\nmeasures the information brought by the inclusion of space, and also has the\nproperty of additivity. A thorough comparative study illustrates the\nsuperiority of the proposed indices.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 13:19:23 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Altieri", "Linda", ""], ["Cocchi", "Daniela", ""], ["Roli", "Giulia", ""]]}, {"id": "1703.06031", "submitter": "Jennifer Wadsworth", "authors": "Rapha\\\"el G. Huser and Jennifer L. Wadsworth", "title": "Modeling spatial processes with unknown extremal dependence class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many environmental processes exhibit weakening spatial dependence as events\nbecome more extreme. Well-known limiting models, such as max-stable or\ngeneralized Pareto processes, cannot capture this, which can lead to a\npreference for models that exhibit a property known as asymptotic independence.\nHowever, weakening dependence does not automatically imply asymptotic\nindependence, and whether the process is truly asymptotically (in)dependent is\nusually far from clear. The distinction is key as it can have a large impact\nupon extrapolation, i.e., the estimated probabilities of events more extreme\nthan those observed. In this work, we present a single spatial model that is\nable to capture both dependence classes in a parsimonious manner, and with a\nsmooth transition between the two cases. The model covers a wide range of\npossibilities from asymptotic independence through to complete dependence, and\npermits weakening dependence of extremes even under asymptotic dependence.\nCensored likelihood-based inference for the implied copula is feasible in\nmoderate dimensions due to closed-form margins. The model is applied to\noceanographic datasets with ambiguous true limiting dependence structure.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 14:34:01 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 15:32:52 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Huser", "Rapha\u00ebl G.", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1703.06086", "submitter": "Shu Yang", "authors": "Shu Yang", "title": "Propensity score weighting for causal inference with clustered data", "comments": "37 pages, 1 figure and 3 tables. arXiv admin note: text overlap with\n  arXiv:1607.07521", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score weighting is a tool for causal inference to adjust for\nmeasured confounders in observational studies. In practice, data often present\ncomplex structures, such as clustering, which make propensity score modeling\nand estimation challenging. In addition, for clustered data, there may be\nunmeasured cluster-specific variables that are related to both the treatment\nassignment and the outcome. When such unmeasured cluster-specific confounders\nexist and are omitted in the propensity score model, the subsequent propensity\nscore adjustment may be biased. In this article, we propose a calibration\ntechnique for propensity score estimation under the latent ignorable treatment\nassignment mechanism, i.e., the treatment-outcome relationship is unconfounded\ngiven the observed covariates and the latent cluster effects. We then provide a\nconsistent propensity score weighting estimator of the average treatment effect\nwhen the propensity score and outcome follow generalized linear mixed effects\nmodels. The proposed propensity score weighting estimator is attractive,\nbecause it does not require specification of functional forms of the propensity\nscore and outcome models, and therefore is robust to model misspecification.\nThe proposed weighting method can be combined with sampling weights for an\nintegrated solution to handle confounding and sampling designs for causal\ninference with clustered survey data. In simulation studies, we show that the\nproposed estimator is superior to other competitors. We estimate the effect of\nSchool Body Mass Index Screening on prevalence of overweight and obesity for\nelementary schools in Pennsylvania.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 16:30:08 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Yang", "Shu", ""]]}, {"id": "1703.06098", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella, Gareth Roberts", "title": "Multilevel linear models, Gibbs samplers and multigrid decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of the Gibbs Sampler in the context of\nposterior distributions arising from Bayesian analysis of conditionally\nGaussian hierarchical models. We develop a multigrid approach to derive\nanalytic expressions for the convergence rates of the algorithm for various\nwidely used model structures, including nested and crossed random effects. Our\nresults apply to multilevel models with an arbitrary number of layers in the\nhierarchy, while most previous work was limited to the two-level nested case.\nThe theoretical results provide explicit and easy-to-implement guidelines to\noptimize practical implementations of the Gibbs Sampler, such as indications on\nwhich parametrization to choose (e.g. centred and non-centred), which\nconstraint to impose to guarantee statistical identifiability, and which\nparameters to monitor in the diagnostic process. Simulations suggest that the\nresults are informative also in the context of non-Gaussian distributions and\nmore general MCMC schemes, such as gradient-based ones.implementation of Gibbs\nsamplers on conditionally Gaussian hierarchical models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:00:53 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 10:40:26 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zanella", "Giacomo", ""], ["Roberts", "Gareth", ""]]}, {"id": "1703.06131", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Daniele Bigoni, Youssef Marzouk", "title": "Inference via low-dimensional couplings", "comments": "78 pages, 25 figures", "journal-ref": "Journal of Machine Learning Research, volume 19 (66): 1-71, 2018", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the low-dimensional structure of deterministic transformations\nbetween random variables, i.e., transport maps between probability measures. In\nthe context of statistics and machine learning, these transformations can be\nused to couple a tractable \"reference\" measure (e.g., a standard Gaussian) with\na target measure of interest. Direct simulation from the desired measure can\nthen be achieved by pushing forward reference samples through the map. Yet\ncharacterizing such a map---e.g., representing and evaluating it---grows\nchallenging in high dimensions. The central contribution of this paper is to\nestablish a link between the Markov properties of the target measure and the\nexistence of low-dimensional couplings, induced by transport maps that are\nsparse and/or decomposable. Our analysis not only facilitates the construction\nof transformations in high-dimensional settings, but also suggests new\ninference methodologies for continuous non-Gaussian graphical models. For\ninstance, in the context of nonlinear state-space models, we describe new\nvariational algorithms for filtering, smoothing, and sequential parameter\ninference. These algorithms can be understood as the natural\ngeneralization---to the non-Gaussian case---of the square-root\nRauch-Tung-Striebel Gaussian smoother.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:50:44 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 13:07:43 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 15:41:07 GMT"}, {"version": "v4", "created": "Sun, 1 Jul 2018 23:28:16 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Spantini", "Alessio", ""], ["Bigoni", "Daniele", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1703.06154", "submitter": "Jelena Markovic", "authors": "Snigdha Panigrahi, Jelena Markovic, Jonathan Taylor", "title": "An MCMC-free approach to post-selective inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Monte Carlo-free approach to inference post output from\nrandomized algorithms with a convex loss and a convex penalty. The pivotal\nstatistic based on a truncated law, called the selective pivot, usually lacks\nclosed form expressions. Inference in these settings relies upon standard Monte\nCarlo sampling techniques at a reference parameter followed by an exponential\ntilting at the reference. Tilting can however be unstable for parameters that\nare far off from the reference parameter. We offer in this paper an alternative\napproach to construction of intervals and point estimates by proposing an\napproximation to the intractable selective pivot. Such an approximation solves\na convex optimization problem in |E| dimensions, where |E| is the size of the\nactive set observed from selection. We empirically show that the confidence\nintervals obtained by inverting the approximate pivot have valid coverage.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 18:31:49 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 17:57:35 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Markovic", "Jelena", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1703.06176", "submitter": "Snigdha Panigrahi", "authors": "Snigdha Panigrahi, Jonathan Taylor", "title": "Scalable methods for Bayesian selective inference", "comments": "48 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeled along the truncated approach in Panigrahi (2016), selection-adjusted\ninference in a Bayesian regime is based on a selective posterior. Such a\nposterior is determined together by a generative model imposed on data and the\nselection event that enforces a truncation on the assumed law. The effective\ndifference between the selective posterior and the usual Bayesian framework is\nreflected in the use of a truncated likelihood. The normalizer of the truncated\nlaw in the adjusted framework is the probability of the selection event; this\nis typically intractable and it leads to the computational bottleneck in\nsampling from such a posterior. The current work lays out a primal-dual\napproach of solving an approximating optimization problem to provide valid\npost-selective Bayesian inference. The selection procedures are posed as\ndata-queries that solve a randomized version of a convex learning program which\nhave the advantage of preserving more left-over information for inference. We\npropose a randomization scheme under which the optimization has separable\nconstraints that result in a partially separable objective in lower dimensions\nfor many commonly used selective queries to approximate the otherwise\nintractable selective posterior. We show that the approximating optimization\nunder a Gaussian randomization gives a valid exponential rate of decay for the\nselection probability on a large deviation scale. We offer a primal-dual method\nto solve the optimization problem leading to an approximate posterior; this\nallows us to exploit the usual merits of a Bayesian machinery in both low and\nhigh dimensional regimes where the underlying signal is effectively sparse. We\nshow that the adjusted estimates empirically demonstrate better frequentist\nproperties in comparison to the unadjusted estimates based on the usual\nposterior, when applied to a wide range of constrained, convex data queries.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 19:23:10 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 18:55:38 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 07:57:59 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Panigrahi", "Snigdha", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1703.06222", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Rina Foygel Barber, Martin J. Wainwright, Michael I.\n  Jordan", "title": "A unified treatment of multiple testing with prior knowledge using the\n  p-filter", "comments": "36 pages, 1 figure, accepted for publication at the Annals of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a significant literature on methods for incorporating knowledge into\nmultiple testing procedures so as to improve their power and precision. Some\ncommon forms of prior knowledge include (a) beliefs about which hypotheses are\nnull, modeled by non-uniform prior weights; (b) differing importances of\nhypotheses, modeled by differing penalties for false discoveries; (c) multiple\narbitrary partitions of the hypotheses into (possibly overlapping) groups; and\n(d) knowledge of independence, positive or arbitrary dependence between\nhypotheses or groups, suggesting the use of more aggressive or conservative\nprocedures. We present a unified algorithmic framework called p-filter for\nglobal null testing and false discovery rate (FDR) control that allows the\nscientist to incorporate all four types of prior knowledge (a)-(d)\nsimultaneously, recovering a variety of known algorithms as special cases.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:08:59 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 01:30:58 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 06:10:07 GMT"}, {"version": "v4", "created": "Tue, 4 Dec 2018 20:17:20 GMT"}, {"version": "v5", "created": "Tue, 6 Aug 2019 07:25:11 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Barber", "Rina Foygel", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1703.06226", "submitter": "Jiyao Kou", "authors": "Jiyao Kou", "title": "Identifying the Support of Rectangular Signals in Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the support of the block signal in a\nsequence when both the length and the location of the block signal are unknown.\nThe multivariate version of this problem is also considered, in which we try to\nidentify the support of the rectangular signal in the hyper- rectangle. We\nallow the length of the block signal to grow polynomially with the length of\nthe sequence, which greatly generalizes the previous results in [16]. A\nstatistical boundary above which the identification is possible is presented\nand an asymptotically optimal and computationally efficient procedure is\nproposed under Gaussian white noise in both the univariate and multivariate\nsettings. The problem of block signal identification is shown to have the same\nstatistical difficulty as the corresponding problem of detection in both the\nunivariate and multivariate cases, in the sense that whenever we can detect the\nsignal, we can identify the support of the signal. Some generalizations are\nalso considered here: (1) We ex- tend our theory to the case of multiple block\nsignals. (2) We also discuss about the robust identification problem when the\nnoise distribution is un- specified and the block signal identification problem\nunder the exponential family setting.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 00:37:18 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Kou", "Jiyao", ""]]}, {"id": "1703.06277", "submitter": "Peirong Xu", "authors": "Peirong Xu, Heng Peng, Tao Huang", "title": "Unsupervised Learning of Mixture Regression Models for Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with learning of mixture regression models for\nindividuals that are measured repeatedly. The adjective \"unsupervised\" implies\nthat the number of mixing components is unknown and has to be determined,\nideally by data driven tools. For this purpose, a novel penalized method is\nproposed to simultaneously select the number of mixing components and to\nestimate the mixing proportions and unknown parameters in the models. The\nproposed method is capable of handling both continuous and discrete responses\nby only requiring the first two moment conditions of the model distribution. It\nis shown to be consistent in both selecting the number of components and\nestimating the mixing proportions and unknown regression parameters. Further, a\nmodified EM algorithm is developed to seamlessly integrate model selection and\nestimation. Simulation studies are conducted to evaluate the finite sample\nperformance of the proposed procedure. And it is further illustrated via an\nanalysis of a primary biliary cirrhosis data set.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 09:10:51 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 01:21:15 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Xu", "Peirong", ""], ["Peng", "Heng", ""], ["Huang", "Tao", ""]]}, {"id": "1703.06336", "submitter": "Wenge Guo", "authors": "Wenge Guo and Joseph P. Romano", "title": "Analysis of error control in large scale two-stage multiple hypothesis\n  testing", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with the problem of simultaneously testing a large number of\nnull hypotheses, a natural testing strategy is to first reduce the number of\ntested hypotheses by some selection (screening or filtering) process, and then\nto simultaneously test the selected hypotheses. The main advantage of this\nstrategy is to greatly reduce the severe effect of high dimensions. However,\nthe first screening or selection stage must be properly accounted for in order\nto maintain some type of error control. In this paper, we will introduce a\nselection rule based on a selection statistic that is independent of the test\nstatistic when the tested hypothesis is true. Combining this selection rule and\nthe conventional Bonferroni procedure, we can develop a powerful and valid\ntwo-stage procedure. The introduced procedure has several nice properties: (i)\nit completely removes the selection effect; (ii) it reduces the multiplicity\neffect; (iii) it does not \"waste\" data while carrying out both selection and\ntesting. Asymptotic power analysis and simulation studies illustrate that this\nproposed method can provide higher power compared to usual multiple testing\nmethods while controlling the Type 1 error rate. Optimal selection thresholds\nare also derived based on our asymptotic analysis.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 19:00:23 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Guo", "Wenge", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1703.06379", "submitter": "Jiwei Zhao", "authors": "Jiwei Zhao and Yang Yang and Yang Ning", "title": "Penalized pairwise pseudo likelihood for variable selection with\n  nonignorable missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regularization approach for variable selection was well developed for a\ncompletely observed data set in the past two decades. In the presence of\nmissing values, this approach needs to be tailored to different missing data\nmechanisms. In this paper, we focus on a flexible and generally applicable\nmissing data mechanism, which contains both ignorable and nonignorable missing\ndata mechanism assumptions. We show how the regularization approach for\nvariable selection can be adapted to the situation under this missing data\nmechanism. The computational and theoretical properties for variable selection\nconsistency are established. The proposed method is further illustrated by\ncomprehensive simulation studies and real data analyses, for both low and high\ndimensional settings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 01:29:12 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 14:42:16 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Zhao", "Jiwei", ""], ["Yang", "Yang", ""], ["Ning", "Yang", ""]]}, {"id": "1703.06417", "submitter": "Julien Flamant", "authors": "Julien Flamant, Nicolas Le Bihan, Pierre Chainais", "title": "Spectral analysis of stationary random bivariate signals", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": "10.1109/TSP.2017.2736494", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach towards the spectral analysis of stationary random bivariate\nsignals is proposed. Using the Quaternion Fourier Transform, we introduce a\nquaternion-valued spectral representation of random bivariate signals seen as\ncomplex-valued sequences. This makes possible the definition of a scalar\nquaternion-valued spectral density for bivariate signals. This spectral density\ncan be meaningfully interpreted in terms of frequency-dependent polarization\nattributes. A natural decomposition of any random bivariate signal in terms of\nunpolarized and polarized components is introduced. Nonparametric spectral\ndensity estimation is investigated, and we introduce the polarization\nperiodogram of a random bivariate signal. Numerical experiments support our\ntheoretical analysis, illustrating the relevance of the approach on synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 10:36:53 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Flamant", "Julien", ""], ["Bihan", "Nicolas Le", ""], ["Chainais", "Pierre", ""]]}, {"id": "1703.06419", "submitter": "Wenlin Dai", "authors": "Wenlin Dai and Marc G. Genton", "title": "Multivariate Functional Data Visualization and Outlier Detection", "comments": "30 pages, 10 figures, 1 table, Journal of Computational and Graphical\n  Statistics, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new graphical tool, the magnitude-shape (MS) plot,\nfor visualizing both the magnitude and shape outlyingness of multivariate\nfunctional data. The proposed tool builds on the recent notion of functional\ndirectional outlyingness, which measures the centrality of functional data by\nsimultaneously considering the level and the direction of their deviation from\nthe central region. The MS-plot intuitively presents not only levels but also\ndirections of magnitude outlyingness on the horizontal axis or plane, and\ndemonstrates shape outlyingness on the vertical axis. A dividing curve or\nsurface is provided to separate non-outlying data from the outliers. Both the\nsimulated data and the practical examples confirm that the MS-plot is superior\nto existing tools for visualizing centrality and detecting outliers for\nfunctional data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 10:51:41 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 14:00:31 GMT"}, {"version": "v3", "created": "Sun, 22 Apr 2018 06:39:31 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1703.06558", "submitter": "Jianwei Hu", "authors": "Jianwei Hu, Jingfei Zhang, Hong Qin, Ting Yan, and Ji Zhu", "title": "Using Maximum Entry-Wise Deviation to Test the Goodness-of-Fit for\n  Stochastic Block Models", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model is widely used for detecting community structures\nin network data. How to test the goodness-of-fit of the model is one of the\nfundamental problems and has gained growing interests in recent years. In this\narticle, we propose a novel goodness-of-fit test based on the maximum entry of\nthe centered and re-scaled adjacency matrix for the stochastic block model. One\nnoticeable advantage of the proposed test is that the number of communities can\nbe allowed to grow linearly with the number of nodes ignoring a logarithmic\nfactor. We prove that the null distribution of the test statistic converges in\ndistribution to a Gumbel distribution, and we show that both the number of\ncommunities and the membership vector can be tested via the proposed method.\nFurther, we show that the proposed test has asymptotic power guarantee against\na class of alternatives. We also demonstrate that the proposed method can be\nextended to the degree-corrected stochastic block model. Both simulation\nstudies and real-world data examples indicate that the proposed method works\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 02:01:02 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 12:11:29 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 03:41:50 GMT"}, {"version": "v4", "created": "Mon, 26 Aug 2019 08:49:26 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hu", "Jianwei", ""], ["Zhang", "Jingfei", ""], ["Qin", "Hong", ""], ["Yan", "Ting", ""], ["Zhu", "Ji", ""]]}, {"id": "1703.06559", "submitter": "Jelena Markovic", "authors": "Jelena Markovic, Lucy Xia, Jonathan Taylor", "title": "Unifying approach to selective inference with applications to\n  cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop tools to do valid post-selective inference for a family of model\nselection procedures, including choosing a model via cross-validated Lasso. The\ntools apply universally when the following random vectors are jointly\nasymptotically multivariate Gaussian: 1. the vector composed of each model's\nquality value evaluated under certain model selection criteria (e.g.\ncross-validation errors across folds, AIC, prediction errors etc.) 2. the test\nstatistics from which we make inference on the parameters; it is worth noting\nthat the parameters here are chosen after model selection methods are\nperformed. Under these assumptions, we derive a pivotal quantity that has an\nasymptotically Unif(0,1) distribution which can be used to perform tests and\nconstruct confidence intervals. Both the tests and confidence intervals are\nselectively valid for the chosen parameter. While the above assumptions may not\nbe satisfied in some applications, we propose a novel variation to these model\nselection procedures by adding Gaussian randomizations to either one of the two\nvectors. As a result, the joint distribution of the above random vectors is\nmultivariate Gaussian and our general tools apply. We illustrate our method by\napplying it to four important procedures for which very few selective inference\nresults have been developed: cross-validated Lasso, cross-validated randomized\nLasso, AIC-based model selection among a fixed set of models and inference for\na newly introduced novel marginal LOCO parameter, inspired by the LOCO\nparameter of Rinaldo et al (2016); and we provide complete results for these\ncases. For randomized model selection procedures, we develop Markov chain Monte\nCarlo sampling scheme to construct valid post-selective confidence intervals\nempirically.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 02:04:24 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 05:04:03 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 06:59:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Markovic", "Jelena", ""], ["Xia", "Lucy", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1703.06633", "submitter": "Julien  Chiquet Dr.", "authors": "Julien Chiquet, Mahendra Mariadassou and St\\'ephane Robin", "title": "Variational inference for probabilistic Poisson PCA", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many application domains such as ecology or genomics have to deal with\nmultivariate non Gaussian observations. A typical example is the joint\nobservation of the respective abundances of a set of species in a series of\nsites, aiming to understand the co-variations between these species. The\nGaussian setting provides a canonical way to model such dependencies, but does\nnot apply in general. We consider here the multivariate exponential family\nframework for which we introduce a generic model with multivariate Gaussian\nlatent variables. We show that approximate maximum likelihood inference can be\nachieved via a variational algorithm for which gradient descent easily applies.\nWe show that this setting enables us to account for covariates and offsets. We\nthen focus on the case of the Poisson-lognormal model in the context of\ncommunity ecology. We demonstrate the efficiency of our algorithm on microbial\necology datasets. We illustrate the importance of accounting for the effects of\ncovariates to better understand interactions between species.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 08:51:47 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 15:52:04 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 12:44:36 GMT"}, {"version": "v4", "created": "Tue, 6 Feb 2018 06:31:41 GMT"}, {"version": "v5", "created": "Mon, 30 Apr 2018 13:35:48 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chiquet", "Julien", ""], ["Mariadassou", "Mahendra", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1703.06808", "submitter": "Luis Fernando Campos", "authors": "Luke W. Miratrix, Jasjeet S. Sekhon, Alexander G. Theodoridis, Luis F.\n  Campos", "title": "Worth Weighting? How to Think About and Use Weights in Survey\n  Experiments", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of online surveys has increased the prominence of using\nweights that capture units' probabilities of inclusion for claims of\nrepresentativeness. Yet, much uncertainty remains regarding how these weights\nshould be employed in the analysis of survey experiments: Should they be used\nor ignored? If they are used, which estimators are preferred? We offer\npractical advice, rooted in the Neyman-Rubin model, for researchers producing\nand working with survey experimental data. We examine simple, efficient\nestimators for analyzing these data, and give formulae for their biases and\nvariances. We provide simulations that examine these estimators as well as real\nexamples from experiments administered online through YouGov. We find that for\nexamining the existence of population treatment effects using high-quality,\nbroadly representative samples recruited by top online survey firms, sample\nquantities, which do not rely on weights, are often sufficient. We found that\nSample Average Treatment Effect (SATE) estimates did not appear to differ\nsubstantially from their weighted counterparts, and they avoided the\nsubstantial loss of statistical power that accompanies weighting. When precise\nestimates of Population Average Treatment Effects (PATE) are essential, we\nanalytically show post-stratifying on survey weights and/or covariates highly\ncorrelated with the outcome to be a conservative choice. While we show these\nsubstantial gains in simulations, we find limited evidence of them in practice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 15:45:44 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 15:05:44 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 22:55:41 GMT"}, {"version": "v4", "created": "Tue, 15 Aug 2017 13:48:15 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Miratrix", "Luke W.", ""], ["Sekhon", "Jasjeet S.", ""], ["Theodoridis", "Alexander G.", ""], ["Campos", "Luis F.", ""]]}, {"id": "1703.06978", "submitter": "Bani Mallick", "authors": "Richard D. Payne, Nilabja Guha, Yu Ding, and Bani K. Mallick", "title": "A Conditional Density Estimation Partition Model Using Logistic Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional density estimation (density regression) estimates the\ndistribution of a response variable y conditional on covariates x. Utilizing a\npartition model framework, a conditional density estimation method is proposed\nusing logistic Gaussian processes. The partition is created using a Voronoi\ntessellation and is learned from the data using a reversible jump Markov chain\nMonte Carlo algorithm. The Markov chain Monte Carlo algorithm is made possible\nthrough a Laplace approximation on the latent variables of the logistic\nGaussian process model. This approximation marginalizes the parameters in each\npartition element, allowing an efficient search of the posterior distribution\nof the tessellation. The method has desirable consistency properties. In\nsimulation and applications, the model successfully estimates the partition\nstructure and conditional distribution of y.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 21:41:20 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Payne", "Richard D.", ""], ["Guha", "Nilabja", ""], ["Ding", "Yu", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1703.07009", "submitter": "Alexander Y. Shestopaloff", "authors": "Yuri K. Shestopaloff, Alexander Y. Shestopaloff", "title": "New reconstruction and data processing methods for regression and\n  interpolation analysis of multidimensional big data", "comments": "30 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems of computational data processing involving regression,\ninterpolation, reconstruction and imputation for multidimensional big datasets\nare becoming more important these days, because of the availability of data and\ntheir widely spread usage in business, technological, scientific and other\napplications. The existing methods often have limitations, which either do not\nallow, or make it difficult to accomplish many data processing tasks. The\nproblems usually relate to algorithm accuracy, applicability, performance\n(computational and algorithmic), demands for computational resources, both in\nterms of power and memory, and difficulty working with high dimensions. Here,\nwe propose a new concept and introduce two methods, which use local area\npredictors (input data) for finding outcomes. One method uses the gradient\nbased approach, while the second one employs an introduced family of smooth\napproximating functions. The new methods are free from many drawbacks of\nexisting approaches. They are practical, have very wide range of applicability,\nprovide high accuracy, excellent computational performance, fit for parallel\ncomputing, and very well suited for processing high dimension big data. The\nmethods also provide multidimensional outcome, when needed. We present\nnumerical examples of up to one hundred dimensions, and report in detail\nperformance characteristics and various properties of new methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 00:11:45 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Shestopaloff", "Yuri K.", ""], ["Shestopaloff", "Alexander Y.", ""]]}, {"id": "1703.07169", "submitter": "Patrick Flaherty", "authors": "Hachem Saddiki, Andrew C. Trapp, Patrick Flaherty", "title": "A Deterministic Global Optimization Method for Variational Inference", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference methods for latent variable statistical models have\ngained popularity because they are relatively fast, can handle large data sets,\nand have deterministic convergence guarantees. However, in practice it is\nunclear whether the fixed point identified by the variational inference\nalgorithm is a local or a global optimum. Here, we propose a method for\nconstructing iterative optimization algorithms for variational inference\nproblems that are guaranteed to converge to the $\\epsilon$-global variational\nlower bound on the log-likelihood. We derive inference algorithms for two\nvariational approximations to a standard Bayesian Gaussian mixture model\n(BGMM). We present a minimal data set for empirically testing convergence and\nshow that a variational inference algorithm frequently converges to a local\noptimum while our algorithm always converges to the globally optimal\nvariational lower bound. We characterize the loss incurred by choosing a\nnon-optimal variational approximation distribution suggesting that selection of\nthe approximating variational distribution deserves as much attention as the\nselection of the original statistical model for a given data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 12:33:19 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Saddiki", "Hachem", ""], ["Trapp", "Andrew C.", ""], ["Flaherty", "Patrick", ""]]}, {"id": "1703.07198", "submitter": "George Mathews", "authors": "George M. Mathews and John Vial", "title": "Overcoming model simplifications when quantifying predictive uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.PR physics.comp-ph physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally accepted that all models are wrong -- the difficulty is\ndetermining which are useful. Here, a useful model is considered as one that is\ncapable of combining data and expert knowledge, through an inversion or\ncalibration process, to adequately characterize the uncertainty in predictions\nof interest. This paper derives conditions that specify which simplified models\nare useful and how they should be calibrated. To start, the notion of an\noptimal simplification is defined. This relates the model simplifications to\nthe nature of the data and predictions, and determines when a standard\nprobabilistic calibration scheme is capable of accurately characterizing\nuncertainty. Furthermore, two additional conditions are defined for suboptimal\nmodels that determine when the simplifications can be safely ignored. The first\nallows a suboptimally simplified model to be used in a way that replicates the\nperformance of an optimal model. This is achieved through the judicial\nselection of a prior term for the calibration process that explicitly includes\nthe nature of the data, predictions and modelling simplifications. The second\nconsiders the dependency structure between the predictions and the available\ndata to gain insights into when the simplifications can be overcome by using\nthe right calibration data. Furthermore, the derived conditions are related to\nthe commonly used calibration schemes based on Tikhonov and subspace\nregularization. To allow concrete insights to be obtained, the analysis is\nperformed under a linear expansion of the model equations and where the\npredictive uncertainty is characterized via second order moments only.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 13:02:35 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Mathews", "George M.", ""], ["Vial", "John", ""]]}, {"id": "1703.07246", "submitter": "Hung Hung", "authors": "Hung Hung and Su-Yun Huang", "title": "Sufficient Dimension Reduction via Random-Partitions for Large-p-Small-n\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction (SDR) is continuing an active research field\nnowadays for high dimensional data. It aims to estimate the central subspace\n(CS) without making distributional assumption. To overcome the\nlarge-$p$-small-$n$ problem we propose a new approach for SDR. Our method\ncombines the following ideas for high dimensional data analysis: (1) Randomly\npartition the covariates into subsets and use distance correlation (DC) to\nconstruct a sketch of envelope subspace with low dimension. (2) Obtain a sketch\nof the CS by applying conventional SDR method within the constructed envelope\nsubspace. (3) Repeat the above two steps for a few times and integrate these\nmultiple sketches to form the final estimate of the CS. We name the proposed\nSDR procedure \"integrated random-partition SDR (iRP-SDR)\". Comparing with\nexisting methods, iRP-SDR is less affected by the selection of tuning\nparameters. Moreover, the estimation procedure of iRP-SDR does not involve the\ndetermination of the structural dimension until at the last stage, which makes\nthe method more robust in a high-dimensional setting. Asymptotic properties of\niRP-SDR are also established. The advantageous performance of the proposed\nmethod is demonstrated via simulation studies and the EEG data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 14:41:52 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Hung", "Hung", ""], ["Huang", "Su-Yun", ""]]}, {"id": "1703.07305", "submitter": "Andrej Aderhold", "authors": "Marco Grzegorczyk, Andrej Aderhold, and Dirk Husmeier", "title": "Targeting Bayes factors with direct-path non-equilibrium thermodynamic\n  integration", "comments": "Accepted for publication in Computational Statistics, 38 pages, 17\n  figures", "journal-ref": "Computational Statistics, 1-45", "doi": "10.1007/s00180-017-0721-7", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermodynamic integration (TI) for computing marginal likelihoods is based on\nan inverse annealing path from the prior to the posterior distribution. In many\ncases, the resulting estimator suffers from high variability, which\nparticularly stems from the prior regime. When comparing complex models with\ndifferences in a comparatively small number of parameters, intrinsic errors\nfrom sampling fluctuations may outweigh the differences in the log marginal\nlikelihood estimates. In the present article, we propose a thermodynamic\nintegration scheme that directly targets the log Bayes factor. The method is\nbased on a modified annealing path between the posterior distributions of the\ntwo models compared, which systematically avoids the high variance prior\nregime. We combine this scheme with the concept of non-equilibrium TI to\nminimise discretisation errors from numerical integration. Results obtained on\nBayesian regression models applied to standard benchmark data, and a complex\nhierarchical model applied to biopathway inference, demonstrate a significant\nreduction in estimator variance over state-of-the-art TI methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 16:39:28 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Grzegorczyk", "Marco", ""], ["Aderhold", "Andrej", ""], ["Husmeier", "Dirk", ""]]}, {"id": "1703.07603", "submitter": "Gertraud Malsiner-Walli", "authors": "Gertraud Malsiner-Walli and Daniela Pauger and Helga Wagner", "title": "Effect fusion using model-based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social and economic studies many of the collected variables are measured\non a nominal scale, often with a large number of categories. The definition of\ncategories is usually not unambiguous and different classification schemes\nusing either a finer or a coarser grid are possible. Categorisation has an\nimpact when such a variable is included as covariate in a regression model: a\ntoo fine grid will result in imprecise estimates of the corresponding effects,\nwhereas with a too coarse grid important effects will be missed, resulting in\nbiased effect estimates and poor predictive performance.\n  To achieve automatic grouping of levels with essentially the same effect, we\nadopt a Bayesian approach and specify the prior on the level effects as a\nlocation mixture of spiky normal components. Fusion of level effects is induced\nby a prior on the mixture weights which encourages empty components.\nModel-based clustering of the effects during MCMC sampling allows to\nsimultaneously detect categories which have essentially the same effect size\nand identify variables with no effect at all. The properties of this approach\nare investigated in simulation studies. Finally, the method is applied to\nanalyse effects of high-dimensional categorical predictors on income in\nAustria.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 11:20:00 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Malsiner-Walli", "Gertraud", ""], ["Pauger", "Daniela", ""], ["Wagner", "Helga", ""]]}, {"id": "1703.07747", "submitter": "Neal Grantham", "authors": "Neal S. Grantham, Brian J. Reich, Elizabeth T. Borer, and Kevin Gross", "title": "MIMIX: a Bayesian Mixed-Effects Model for Microbiome Data from Designed\n  Experiments", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in bioinformatics have made high-throughput microbiome data\nwidely available, and new statistical tools are required to maximize the\ninformation gained from these data. For example, analysis of high-dimensional\nmicrobiome data from designed experiments remains an open area in microbiome\nresearch. Contemporary analyses work on metrics that summarize collective\nproperties of the microbiome, but such reductions preclude inference on the\nfine-scale effects of environmental stimuli on individual microbial taxa. Other\napproaches model the proportions or counts of individual taxa as response\nvariables in mixed models, but these methods fail to account for complex\ncorrelation patterns among microbial communities. In this paper, we propose a\nnovel Bayesian mixed-effects model that exploits cross-taxa correlations within\nthe microbiome, a model we call MIMIX (MIcrobiome MIXed model). MIMIX offers\nglobal tests for treatment effects, local tests and estimation of treatment\neffects on individual taxa, quantification of the relative contribution from\nheterogeneous sources to microbiome variability, and identification of latent\necological subcommunities in the microbiome. MIMIX is tailored to large\nmicrobiome experiments using a combination of Bayesian factor analysis to\nefficiently represent dependence between taxa and Bayesian variable selection\nmethods to achieve sparsity. We demonstrate the model using a simulation\nexperiment and on a 2x2 factorial experiment of the effects of nutrient\nsupplement and herbivore exclusion on the foliar fungal microbiome of\n$\\textit{Andropogon gerardii}$, a perennial bunchgrass, as part of the global\nNutrient Network research initiative.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:56:48 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Grantham", "Neal S.", ""], ["Reich", "Brian J.", ""], ["Borer", "Elizabeth T.", ""], ["Gross", "Kevin", ""]]}, {"id": "1703.07856", "submitter": "Rutie Guo", "authors": "Ruite Guo and Vic Patrangenaru", "title": "Testing for the Equality of two Distributions on High Dimensional Object\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy statistics are estimators of the energy distance that depend on the\ndistances between observations. The idea behind energy statistics is to\nconsider a statistical potential energy that would parallel Newton's\ngravitational potential energy. This statistical potential energy is zero if\nand only if a certain null hypothesis relating two distributions holds true. In\nSzekely and Rizzo(2004), a nonparametric test for equality of two multivariate\ndistributions was given, based on the Euclidean distance between observations.\nThis test was shown to be effective for high dimensional multivariate data, and\nwas implemented by an appropriate distribution free permutation test. As an\nextension of Szekely and Rizzo (2013), here we consider the energy distance\nbetween to independent random objects X and Y on the object space M, that\nadmits an embedding into an Euclidean space. In the case of a Kendall shape\nspace, we can use its VW-embedding into an Euclidean space of matrices and\ndefine the extrinsic distance between two shapes as their VW associated\ndistance. The corresponding energy distance between two distributions of\nKendall shapes of k-ads will be called VW-energy distance We test our\nmethodology on, to compare the distributions of Kendall shape of the contour of\nthe midsagittal section of the Corpus Callossum in normal vs ADHD diagnosed\nindividuals. Here we use the VW distance between the shapes of two children CC\nmidsections. Using the CC data coming originally from http://fcon\n1000.projects.nitrc.org/indi/adhd200/ it appears that the two Kendall shape\ndistributions are not significantly different.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 21:13:02 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Guo", "Ruite", ""], ["Patrangenaru", "Vic", ""]]}, {"id": "1703.07879", "submitter": "Simone Carlo Surace", "authors": "Simone Carlo Surace and Anna Kutschireiter and Jean-Pascal Pfister", "title": "How to avoid the curse of dimensionality: scalability of particle\n  filters with and without importance weights", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are a popular and flexible class of numerical algorithms to\nsolve a large class of nonlinear filtering problems. However, standard particle\nfilters with importance weights have been shown to require a sample size that\nincreases exponentially with the dimension D of the state space in order to\nachieve a certain performance, which precludes their use in very\nhigh-dimensional filtering problems. Here, we focus on the dynamic aspect of\nthis curse of dimensionality (COD) in continuous time filtering, which is\ncaused by the degeneracy of importance weights over time. We show that the\ndegeneracy occurs on a time-scale that decreases with increasing D. In order to\nsoften the effects of weight degeneracy, most particle filters use particle\nresampling and improved proposal functions for the particle motion. We explain\nwhy neither of the two can prevent the COD in general. In order to address this\nfundamental problem, we investigate an existing filtering algorithm based on\noptimal feedback control that sidesteps the use of importance weights. We use\nnumerical experiments to show that this Feedback Particle Filter (FPF) by Yang\net al. (2013) does not exhibit a COD.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 22:59:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 14:04:40 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Surace", "Simone Carlo", ""], ["Kutschireiter", "Anna", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1703.07904", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Cross-Validation with Confidence", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is one of the most popular model selection methods in\nstatistics and machine learning. Despite its wide applicability, traditional\ncross validation methods tend to select overfitting models, due to the\nignorance of the uncertainty in the testing sample. We develop a new,\nstatistically principled inference tool based on cross-validation that takes\ninto account the uncertainty in the testing sample. This new method outputs a\nset of highly competitive candidate models containing the best one with\nguaranteed probability. As a consequence, our method can achieve consistent\nvariable selection in a classical linear regression setting, for which existing\ncross-validation methods require unconventional split ratios. When used for\nregularizing tuning parameter selection, the method can provide a further\ntrade-off between prediction accuracy and model interpretability. We\ndemonstrate the performance of the proposed method in several simulated and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 01:30:17 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 14:09:10 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1703.07975", "submitter": "Mickael De Backer", "authors": "Micka\\\"el De Backer, Anouar El Ghouch, Ingrid Van Keilegom", "title": "An Adapted Loss Function for Censored Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a novel approach for the estimation of quantiles when\nfacing potential right censoring of the responses. Contrary to the existing\nliterature on the subject, the adopted strategy of this paper is to tackle\ncensoring at the very level of the loss function usually employed for the\ncomputation of quantiles, the so-called \"check\" function. For interpretation\npurposes, a simple comparison with the latter reveals how censoring is\naccounted for in the newly proposed loss function. Subsequently, when\nconsidering the inclusion of covariates for conditional quantile estimation, by\ndefining a new general loss function, the proposed methodology opens the gate\nto numerous parametric, semiparametric and nonparametric modelling techniques.\nIn order to illustrate this statement, we consider the well-studied linear\nregression under the usual assumption of conditional independence between the\ntrue response and the censoring variable. For practical minimization of the\nstudied loss function, we also provide a simple algorithmic procedure shown to\nyield satisfactory results for the proposed estimator with respect to the\nexisting literature in an extensive simulation study. From a more theoretical\nprospect, consistency of the estimator for linear regression is obtained using\nvery recent results on non-smooth semiparametric estimation equations with an\ninfinite-dimensional nuisance parameter, while numerical examples illustrate\nthe adequateness of a simple bootstrap procedure for inferential purposes.\nLastly, an application to a real dataset is used to further illustrate the\nvalidity and finite sample performance of the proposed estimator.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 09:20:36 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["De Backer", "Micka\u00ebl", ""], ["Ghouch", "Anouar El", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1703.08045", "submitter": "Eric Adjakossa", "authors": "Eric Adjakossa (LPMA, UAC), Gr\\'egory Nuel (LPMA)", "title": "Profiled deviance for the multivariate linear mixed-effects model\n  fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the multivariate linear mixed-effects model, including\nall the correlations between the random effects when the marginal residual\nterms are assumed uncorrelated and homoscedastic with possibly different\nstandard deviations. The random effects covariance matrix is Cholesky\nfactorized to directly estimate the variance components of these random\neffects. This strategy enables a consistent estimate of the random effects\ncovariance matrix which, generally, has a poor estimate when it is grossly (or\ndirectly) estimated, using the estimating methods such as the EM algorithm. By\nusing simulated data sets, we compare the estimates based on the present method\nwith the EM algorithm-based estimates. We provide an illustration by using the\nreal-life data concerning the study of the child's immune against malaria in\nBenin (West Africa).\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 12:43:23 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 08:49:48 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Adjakossa", "Eric", "", "LPMA, UAC"], ["Nuel", "Gr\u00e9gory", "", "LPMA"]]}, {"id": "1703.08090", "submitter": "Robson Machado", "authors": "Robson J. M. Machado, Ardo van den Hout", "title": "Flexible multi-state models for interval-censored data: specification,\n  estimation, and an application to ageing research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time multi-state survival models can be used to describe\nhealth-related processes over time. In the presence of interval-censored times\nfor transitions between the living states, the likelihood is constructed using\ntransition probabilities. Models can be specified using parametric or\nsemi-parametric shapes for the hazards. Semi-parametric hazards can be fitted\nusing $P$-splines and penalised maximum likelihood estimation. This paper\npresents a method to estimate flexible multi-state models which allows for\nparametric and semi-parametric hazard specifications. The estimation is based\non a scoring algorithm. The method is illustrated with data from the English\nLongitudinal Study of Ageing.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 14:47:29 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Machado", "Robson J. M.", ""], ["Hout", "Ardo van den", ""]]}, {"id": "1703.08202", "submitter": "Frederic Schoenberg", "authors": "Frederic Schoenberg, Marc Hoffmann, and Ryan Harrigan", "title": "A recursive point process model for infectious diseases", "comments": "32 pages including 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of point process model to describe the incidence of\ncontagious diseases. The model is a variant of the Hawkes self-exciting process\nand exhibits similar clustering but without the restriction that the component\ndescribing the contagion must remain static over time. Instead, our proposed\nmodel prescribes that the degree of contagion (or productivity) changes as a\nfunction of the conditional intensity; of particular interest is the special\ncase where the productivity is inversely proportional to the conditional\nintensity. The model incorporates the premise that when the disease occurs at\nvery low frequency in the population, such as in the primary stages of an\noutbreak, then anyone with the disease is likely to have a high rate of\ntransmission to others, whereas when the disease is prevalent in the\npopulation, then the transmission rate is lower due to human mitigation actions\nand prevention measures and a relatively high percentage of previous exposure\nin the total population. The model is said to be recursive, in the sense that\nthe conditional intensity at any particular time depends on the productivity\nassociated with previous points, and this productivity in turn depends on the\nconditional intensity at those points. Some basic properties of the model are\nderived, estimation and simulation are discussed, and the recursive model is\nshown to fit well to historic data on measles in Los Angeles, California, a\nrelevant example given the 2017 outbreak of this disease in the same region.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 18:57:39 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Schoenberg", "Frederic", ""], ["Hoffmann", "Marc", ""], ["Harrigan", "Ryan", ""]]}, {"id": "1703.08487", "submitter": "Daniele Marinazzo", "authors": "Luca Faes, Giandomenico Nollo, Sebastiano Stramaglia, Daniele\n  Marinazzo", "title": "Multiscale Granger causality", "comments": null, "journal-ref": "Phys. Rev. E 96, 042150 (2017)", "doi": "10.1103/PhysRevE.96.042150", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the study of complex physical and biological systems represented by\nmultivariate stochastic processes, an issue of great relevance is the\ndescription of the system dynamics spanning multiple temporal scales. While\nmethods to assess the dynamic complexity of individual processes at different\ntime scales are well-established, multiscale analysis of directed interactions\nhas never been formalized theoretically, and empirical evaluations are\ncomplicated by practical issues such as filtering and downsampling. Here we\nextend the very popular measure of Granger causality (GC), a prominent tool for\nassessing directed lagged interactions between joint processes, to quantify\ninformation transfer across multiple time scales. We show that the multiscale\nprocessing of a vector autoregressive (AR) process introduces a moving average\n(MA) component, and describe how to represent the resulting ARMA process using\nstate space (SS) models and to combine the SS model parameters for computing\nexact GC values at arbitrarily large time scales. We exploit the theoretical\nformulation to identify peculiar features of multiscale GC in basic AR\nprocesses, and demonstrate with numerical simulations the much larger\nestimation accuracy of the SS approach compared with pure AR modeling of\nfiltered and downsampled data. The improved computational reliability is\nexploited to disclose meaningful multiscale patterns of information transfer\nbetween global temperature and carbon dioxide concentration time series, both\nin paleoclimate and in recent years.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:08:21 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:02:12 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Faes", "Luca", ""], ["Nollo", "Giandomenico", ""], ["Stramaglia", "Sebastiano", ""], ["Marinazzo", "Daniele", ""]]}, {"id": "1703.08489", "submitter": "Ross Jacobucci", "authors": "Ross Jacobucci", "title": "regsem: Regularized Structural Equation Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regsem package in R, an implementation of regularized structural equation\nmodeling (RegSEM; Jacobucci, Grimm, and McArdle 2016), was recently developed\nwith the goal of incorporating various forms of penalized likelihood estimation\nin a broad array of structural equations models. The forms of regularization\ninclude both the ridge (Hoerl and Kennard 1970) and the least absolute\nshrinkage and selection operator (lasso; Tibshirani 1996), along with sparser\nextensions. RegSEM is particularly useful for structural equation models that\nhave a small parameter to sample size ratio, as the addition of penalties can\nreduce the complexity, thus reducing the bias of the parameter estimates. The\npaper covers the algorithmic details and an overview of the use of regsem with\nthe application of both factor analysis and latent growth curve models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 16:11:19 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 15:58:14 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Jacobucci", "Ross", ""]]}, {"id": "1703.08520", "submitter": "Kaspar M\\\"artens", "authors": "Kaspar M\\\"artens, Michalis K Titsias, Christopher Yau", "title": "Augmented Ensemble MCMC sampling in Factorial Hidden Markov Models", "comments": null, "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha,Okinawa, Japan. PMLR: Volume\n  89", "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for factorial hidden Markov models is challenging due to\nthe exponentially sized latent variable space. Standard Monte Carlo samplers\ncan have difficulties effectively exploring the posterior landscape and are\noften restricted to exploration around localised regions that depend on\ninitialisation. We introduce a general purpose ensemble Markov Chain Monte\nCarlo (MCMC) technique to improve on existing poorly mixing samplers. This is\nachieved by combining parallel tempering and an auxiliary variable scheme to\nexchange information between the chains in an efficient way. The latter\nexploits a genetic algorithm within an augmented Gibbs sampler. We compare our\ntechnique with various existing samplers in a simulation study as well as in a\ncancer genomics application, demonstrating the improvements obtained by our\naugmented ensemble approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 17:17:45 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 20:46:32 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["M\u00e4rtens", "Kaspar", ""], ["Titsias", "Michalis K", ""], ["Yau", "Christopher", ""]]}, {"id": "1703.08596", "submitter": "David N. Levin", "authors": "David N. Levin (University of Chicago, Chicago, IL)", "title": "The Inner Structure of Time-Dependent Signals", "comments": "22 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SD math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how a time series of measurements of an evolving system can\nbe processed to create an inner time series that is unaffected by any\ninstantaneous invertible, possibly nonlinear transformation of the\nmeasurements. An inner time series contains information that does not depend on\nthe nature of the sensors, which the observer chose to monitor the system.\nInstead, it encodes information that is intrinsic to the evolution of the\nobserved system. Because of its sensor-independence, an inner time series may\nproduce fewer false negatives when it is used to detect events in the presence\nof sensor drift. Furthermore, if the observed physical system is comprised of\nnon-interacting subsystems, its inner time series is separable; i.e., it\nconsists of a collection of time series, each one being the inner time series\nof an isolated subsystem. Because of this property, an inner time series can be\nused to detect a specific behavior of one of the independent subsystems without\nusing blind source separation to disentangle that subsystem from the others.\nThe method is illustrated by applying it to: 1) an analytic example; 2) the\naudio waveform of one speaker; 3) video images from a moving camera; 4)\nmixtures of audio waveforms of two speakers.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 20:59:52 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Levin", "David N.", "", "University of Chicago, Chicago, IL"]]}, {"id": "1703.08620", "submitter": "Maryclare Griffin", "authors": "Maryclare Griffin and Peter D. Hoff", "title": "Lasso ANOVA Decompositions for Matrix and Tensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating the entries of an unknown mean matrix or\ntensor given a single noisy realization. In the matrix case, this problem can\nbe addressed by decomposing the mean matrix into a component that is additive\nin the rows and columns, i.e.\\ the additive ANOVA decomposition of the mean\nmatrix, plus a matrix of elementwise effects, and assuming that the elementwise\neffects may be sparse. Accordingly, the mean matrix can be estimated by solving\na penalized regression problem, applying a lasso penalty to the elementwise\neffects. Although solving this penalized regression problem is straightforward,\nspecifying appropriate values of the penalty parameters is not. Leveraging the\nposterior mode interpretation of the penalized regression problem, moment-based\nempirical Bayes estimators of the penalty parameters can be defined. Estimation\nof the mean matrix using these these moment-based empirical Bayes estimators\ncan be called LANOVA penalization, and the corresponding estimate of the mean\nmatrix can be called the LANOVA estimate. The empirical Bayes estimators are\nshown to be consistent. Additionally, LANOVA penalization is extended to\naccommodate sparsity of row and column effects and to estimate an unknown mean\ntensor. The behavior of the LANOVA estimate is examined under misspecification\nof the distribution of the elementwise effects, and LANOVA penalization is\napplied to several datasets, including a matrix of microarray data, a three-way\ntensor of fMRI data and a three-way tensor of wheat infection data.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 23:04:17 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 02:55:49 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Griffin", "Maryclare", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1703.08665", "submitter": "Rapha\\\"el Huser", "authors": "Rapha\\\"el Huser, Cl\\'ement Dombry, Mathieu Ribatet and Marc G. Genton", "title": "Full likelihood inference for max-stable data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to perform full likelihood inference for max-stable multivariate\ndistributions or processes based on a stochastic Expectation-Maximisation\nalgorithm, which combines statistical and computational efficiency in\nhigh-dimensions. The good performance of this methodology is demonstrated by\nsimulation based on the popular logistic and Brown--Resnick models, and it is\nshown to provide dramatic computational time improvements with respect to a\ndirect computation of the likelihood. Strategies to further reduce the\ncomputational burden are also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 09:11:28 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 19:02:06 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Huser", "Rapha\u00ebl", ""], ["Dombry", "Cl\u00e9ment", ""], ["Ribatet", "Mathieu", ""], ["Genton", "Marc G.", ""]]}, {"id": "1703.08723", "submitter": "Paul McNicholas", "authors": "Yuhong Wei, Yang Tang, Emilie Shireman, Paul D. McNicholas and Douglas\n  L. Steinley", "title": "Extending Growth Mixture Models Using Continuous Non-Elliptical\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth mixture models (GMMs) incorporate both conventional random effects\ngrowth modeling and latent trajectory classes as in finite mixture modeling;\ntherefore, they offer a way to handle the unobserved heterogeneity between\nsubjects in their development. GMMs with Gaussian random effects dominate the\nliterature. When the data are asymmetric and/or have heavier tails, more than\none latent class is required to capture the observed variable distribution.\nTherefore, a GMM with continuous non-elliptical distributions is proposed to\ncapture skewness and heavier tails in the data set. Specifically, multivariate\nskew-t distributions and generalized hyperbolic distributions are introduced to\nextend GMMs. When extending GMMs, four statistical models are considered with\ndiffering distributions of measurement errors and random effects. The\nmathematical development of GMMs with non-elliptical distributions relies on\ntheir expression as normal variance-mean mixtures and the resultant\nrelationship with the generalized inverse Gaussian distribution. Parameter\nestimation is outlined within the expectation-maximization framework before the\nperformance of our GMMs with non-elliptical distributions is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 17:57:31 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 00:14:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wei", "Yuhong", ""], ["Tang", "Yang", ""], ["Shireman", "Emilie", ""], ["McNicholas", "Paul D.", ""], ["Steinley", "Douglas L.", ""]]}, {"id": "1703.08741", "submitter": "Curtis Storlie", "authors": "Curtis Storlie, Scott Myers, S Katusic, Amy Weaver, Robert Voigt,\n  Robert Colligan, Paul Croarkin, Ruth Stoeckel, John Port", "title": "Clustering and Variable Selection in the Presence of Mixed Variable\n  Types and Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model-based clustering in the presence of many\ncorrelated, mixed continuous and discrete variables, some of which may have\nmissing values. Discrete variables are treated with a latent continuous\nvariable approach and the Dirichlet process is used to construct a mixture\nmodel with an unknown number of components. Variable selection is also\nperformed to identify the variables that are most influential for determining\ncluster membership. The work is motivated by the need to cluster patients\nthought to potentially have autism spectrum disorder (ASD) on the basis of many\ncognitive and/or behavioral test scores. There are a modest number of patients\n(~480) in the data set along with many (~100) test score variables (many of\nwhich are discrete valued and/or missing). The goal of the work is to (i)\ncluster these patients into similar groups to help identify those with similar\nclinical presentation, and (ii) identify a sparse subset of tests that inform\nthe clusters in order to eliminate unnecessary testing. The proposed approach\ncompares very favorably to other methods via simulation of problems of this\ntype. The results of the ASD analysis suggested three clusters to be most\nlikely, while only four test scores had high (>0.5) posterior probability of\nbeing informative. This will result in much more efficient and informative\ntesting. The need to cluster observations on the basis of many correlated,\ncontinuous/discrete variables with missing values, is a common problem in the\nhealth sciences as well as in many other disciplines.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 20:57:56 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 19:24:20 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Storlie", "Curtis", ""], ["Myers", "Scott", ""], ["Katusic", "S", ""], ["Weaver", "Amy", ""], ["Voigt", "Robert", ""], ["Colligan", "Robert", ""], ["Croarkin", "Paul", ""], ["Stoeckel", "Ruth", ""], ["Port", "John", ""]]}, {"id": "1703.08882", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Finite Mixtures of Skewed Matrix Variate Distributions", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.143", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the process of finding underlying group structures in data.\nAlthough mixture model-based clustering is firmly established in the\nmultivariate case, there is a relative paucity of work on matrix variate\ndistributions and none for clustering with mixtures of skewed matrix variate\ndistributions. Four finite mixtures of skewed matrix variate distributions are\nconsidered. Parameter estimation is carried out using an\nexpectation-conditional maximization algorithm, and both simulated and real\ndata are used for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 22:49:31 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 19:44:17 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 18:28:33 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1703.08995", "submitter": "Vincent Runge", "authors": "Vincent Runge", "title": "On the Limit Imbalanced Logistic Regression by Binary Predictors", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a modified (rescaled) likelihood for imbalanced\nlogistic regression. This new approach makes easier the use of exponential\npriors and the computation of lasso regularization path. Precisely, we study a\nlimiting behavior for which class imbalance is artificially increased by\nreplication of the majority class observations. If some strong overlap\nconditions are satisfied, the maximum likelihood estimate converges towards a\nfinite value close to the initial one (intercept excluded) as shown by\nsimulations with binary predictors. This solution corresponds to the extremum\nof a concave function that we refer to as \"rescaled\" likelihood. In this\ncontext, the use of exponential priors has a clear interpretation as a shift on\nthe predictor means for the minority class. Thanks to the simple binary\nstructure, some random designs give analytic path estimators for the lasso\nregularization problem. An effective approximate path algorithm by piecewise\nlogarithmic functions based on matrix inversions is also presented. This work\nwas motivated by its potential application to spontaneous reports databases in\na pharmacovigilance context.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 10:17:00 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 10:46:23 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Runge", "Vincent", ""]]}, {"id": "1703.09061", "submitter": "Yanxun Xu", "authors": "Fangzheng Xie and Yanxun Xu", "title": "Bayesian Repulsive Gaussian Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general class of Bayesian repulsive Gaussian mixture models that\nencourage well-separated clusters, aiming at reducing potentially redundant\ncomponents produced by independent priors for locations (such as the Dirichlet\nprocess). The asymptotic results for the posterior distribution of the proposed\nmodels are derived, including posterior consistency and posterior contraction\nrate in the context of nonparametric density estimation. More importantly, we\nshow that compared to the independent prior on the component centers, the\nrepulsive prior introduces additional shrinkage effect on the tail probability\nof the posterior number of components, which serves as a measurement of the\nmodel complexity. In addition, an efficient and easy-to-implement\nblocked-collapsed Gibbs sampler is developed based on the exchangeable\npartition distribution and the corresponding urn model. We evaluate the\nperformance and demonstrate the advantages of the proposed model through\nextensive simulation studies and real data analysis. The R code is available at\nhttps://drive.google.com/open?id=0B_zFse0eqxBHZnF5cEhsUFk0cVE.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:33:09 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 13:49:45 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1703.09062", "submitter": "Stella Hadjiantoni", "authors": "Stella Hadjiantoni and Erricos J. Kontoghiorghes", "title": "A numerical method for the estimation of time-varying parameter models\n  in large dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel numerical method for the estimation of large time-varying parameter\n(TVP) models is proposed. The updating and smoothing estimates of the TVP model\nare derived within the context of generalised linear least squares and through\nnumerically stable orthogonal transformations. The method developed is based on\ncomputationally efficient strategies. The computational cost is reduced by\nexploiting the special sparse structure of the TVP model and by utilising\nprevious computations. The proposed method is also extended to the rolling\nwindow estimation of the TVP model. Experimental results show the effectiveness\nof the new updating, window and smoothing strategies in high dimensions when a\nlarge number of covariates and regressions are included in the TVP model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 13:33:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 18:09:20 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Hadjiantoni", "Stella", ""], ["Kontoghiorghes", "Erricos J.", ""]]}, {"id": "1703.09124", "submitter": "Kemi Ding", "authors": "Kemi Ding, Yuzhe Li, Subhrakanti Dey, Ling Shi", "title": "Multi-sensor Transmission Management for Remote State Estimation under\n  Coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the remote state estimation in a cyber-physical system\n(CPS) using multiple sensors. The measurements of each sensor are transmitted\nto a remote estimator over a shared channel, where simultaneous transmissions\nfrom other sensors are regarded as interference signals. In such a competitive\nenvironment, each sensor needs to choose its transmission power for sending\ndata packets taking into account of other sensors' behavior. To model this\ninteractive decision-making process among the sensors, we introduce a\nmulti-player non-cooperative game framework. To overcome the inefficiency\narising from the Nash equilibrium (NE) solution, we propose a correlation\npolicy, along with the notion of correlation equilibrium (CE). An analytical\ncomparison of the game value between the NE and the CE is provided,\nwith/without the power expenditure constraints for each sensor. Also, numerical\nsimulations demonstrate the comparison results.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:52:09 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ding", "Kemi", ""], ["Li", "Yuzhe", ""], ["Dey", "Subhrakanti", ""], ["Shi", "Ling", ""]]}, {"id": "1703.09282", "submitter": "Christian Hennig", "authors": "Christian Hennig", "title": "Cluster validation by measurement of clustering characteristics relevant\n  to the user", "comments": "20 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many cluster analysis methods that can produce quite different\nclusterings on the same dataset. Cluster validation is about the evaluation of\nthe quality of a clustering; \"relative cluster validation\" is about using such\ncriteria to compare clusterings. This can be used to select one of a set of\nclusterings from different methods, or from the same method ran with different\nparameters such as different numbers of clusters.\n  There are many cluster validation indexes in the literature. Most of them\nattempt to measure the overall quality of a clustering by a single number, but\nthis can be inappropriate. There are various different characteristics of a\nclustering that can be relevant in practice, depending on the aim of\nclustering, such as low within-cluster distances and high between-cluster\nseparation.\n  In this paper, a number of validation criteria will be introduced that refer\nto different desirable characteristics of a clustering, and that characterise a\nclustering in a multidimensional way. In specific applications the user may be\ninterested in some of these criteria rather than others. A focus of the paper\nis on methodology to standardise the different characteristics so that users\ncan aggregate them in a suitable way specifying weights for the various\ncriteria that are relevant in the clustering application at hand.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 19:42:16 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 21:49:23 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Hennig", "Christian", ""]]}, {"id": "1703.09301", "submitter": "Michael Schweinberger", "authors": "Sergii Babkin, Jonathan Stewart, Xiaochen Long, Michael Schweinberger", "title": "Large-scale estimation of random graph models with local dependence", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2020.107029", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of random graph models is considered, combining features of\nexponential-family models and latent structure models, with the goal of\nretaining the strengths of both of them while reducing the weaknesses of each\nof them. An open problem is how to estimate such models from large networks. A\nnovel approach to large-scale estimation is proposed, taking advantage of the\nlocal structure of such models for the purpose of local computing. The main\nidea is that random graphs with local dependence can be decomposed into\nsubgraphs, which enables parallel computing on subgraphs and suggests a\ntwo-step estimation approach. The first step estimates the local structure\nunderlying random graphs. The second step estimates parameters given the\nestimated local structure of random graphs. Both steps can be implemented in\nparallel, which enables large-scale estimation. The advantages of the two-step\nestimation approach are demonstrated by simulation studies with up to 10,000\nnodes and an application to a large Amazon product recommendation network with\nmore than 10,000 products.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 20:37:04 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 00:54:54 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 19:59:29 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Babkin", "Sergii", ""], ["Stewart", "Jonathan", ""], ["Long", "Xiaochen", ""], ["Schweinberger", "Michael", ""]]}, {"id": "1703.09305", "submitter": "Georg Hahn", "authors": "Axel Gandy, Georg Hahn, Dong Ding", "title": "Implementing Monte Carlo Tests with P-value Buckets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software packages usually report the results of statistical tests using\np-values. Users often interpret these by comparing them to standard thresholds,\ne.g. 0.1%, 1% and 5%, which is sometimes reinforced by a star rating (***, **,\n*). We consider an arbitrary statistical test whose p-value p is not available\nexplicitly, but can be approximated by Monte Carlo samples, e.g. by bootstrap\nor permutation tests. The standard implementation of such tests usually draws a\nfixed number of samples to approximate p. However, the probability that the\nexact and the approximated p-value lie on different sides of a threshold (the\nresampling risk) can be high, particularly for p-values close to a threshold.\nWe present a method to overcome this. We consider a finite set of\nuser-specified intervals which cover [0,1] and which can be overlapping. We\ncall these p-value buckets. We present algorithms that, with arbitrarily high\nprobability, return a p-value bucket containing p. We prove that for both a\nbounded resampling risk and a finite runtime, overlapping buckets need to be\nemployed, and that our methods both bound the resampling risk and guarantee a\nfinite runtime for such overlapping buckets. To interpret decisions with\noverlapping buckets, we propose an extension of the star rating system. We\ndemonstrate that our methods are suitable for use in standard software,\nincluding for low p-value thresholds occurring in multiple testing settings,\nand that they can be computationally more efficient than standard\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 20:47:25 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 00:23:16 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 16:24:40 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 04:43:44 GMT"}, {"version": "v5", "created": "Mon, 4 Nov 2019 15:32:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Gandy", "Axel", ""], ["Hahn", "Georg", ""], ["Ding", "Dong", ""]]}, {"id": "1703.09626", "submitter": "Alfio Marazzi", "authors": "Michael Amiguet, Alfio Marazzi, Marina Valdora, Victor Yohai", "title": "Robust estimators for generalized linear models with a dispersion\n  parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly robust and efficient estimators for the generalized linear model with\na dispersion parameter are proposed. The estimators are based on three steps.\nIn the first step the maximum rank correlation estimator is used to\nconsistently estimate the slopes up to a scale factor. In the second step, the\nscale factor, the intercept, and the dispersion parameter are consistently\nestimated using a MT-estimator of a simple regression model. The combined\nestimator is highly robust but inefficient. Then, randomized quantile residuals\nbased on the initial estimators are used to detect outliers to be rejected and\nto define a set S of observations to be retained. Finally, a conditional\nmaximum likelihood (CML) estimator given the observations in S is computed. We\nshow that, under the model, S tends to the complete sample for increasing\nsample size. Therefore, the CML tends to the unconditional maximum likelihood\nestimator. It is therefore highly efficient, while maintaining the high degree\nof robustness of the initial estimator. The case of the negative binomial\nregression model is studied in detail.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 15:09:31 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Amiguet", "Michael", ""], ["Marazzi", "Alfio", ""], ["Valdora", "Marina", ""], ["Yohai", "Victor", ""]]}, {"id": "1703.09701", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Mike Hobson and Anthony Lasenby", "title": "Sampling Errors in Nested Sampling Parameter Estimation", "comments": "Very minor changes. 22 pages + appendix, 10 figures. Accepted by\n  Bayesian Analysis", "journal-ref": "Bayesian Analysis 13(3):873-896 (2018)", "doi": "10.1214/17-BA1075", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling errors in nested sampling parameter estimation differ from those in\nBayesian evidence calculation, but have been little studied in the literature.\nThis paper provides the first explanation of the two main sources of sampling\nerrors in nested sampling parameter estimation, and presents a new diagrammatic\nrepresentation for the process. We find no current method can accurately\nmeasure the parameter estimation errors of a single nested sampling run, and\npropose a method for doing so using a new algorithm for dividing nested\nsampling runs. We empirically verify our conclusions and the accuracy of our\nnew method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 18:00:00 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 18:37:45 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1703.09787", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao and Dylan S. Small and Weijie Su", "title": "Multiple testing when many $p$-values are uniformly conservative, with\n  application to testing qualitative interaction in educational interventions", "comments": "31 pages, 2 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the evaluation of treatment effects, it is of major policy interest to\nknow if the treatment is beneficial for some and harmful for others, a\nphenomenon known as qualitative interaction. We formulate this question as a\nmultiple testing problem with many conservative null $p$-values, in which the\nclassical multiple testing methods may lose power substantially. We propose a\nsimple technique---conditioning---to improve the power. A crucial assumption we\nneed is uniform conservativeness, meaning for any conservative $p$-value $p$,\nthe conditional distribution $(p/\\tau)\\,|\\,p \\le \\tau$ is stochastically larger\nthan the uniform distribution on $(0,1)$ for any $\\tau$. We show this property\nholds for one-sided tests in a one-dimensional exponential family (e.g.\\\ntesting for qualitative interaction) as well as testing $|\\mu|\\le\\eta$ using a\nstatistic $X \\sim \\mathrm{N}(\\mu,1)$ (e.g.\\ testing for practical importance\nwith threshold $\\eta$). We propose an adaptive method to select the threshold\n$\\tau$. Our theoretical and simulation results suggest the proposed tests gain\nsignificant power when many $p$-values are uniformly conservative and lose\nlittle power when no $p$-value is uniformly conservative. We apply our method\nto two educational intervention datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 20:23:40 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 21:58:00 GMT"}, {"version": "v3", "created": "Sat, 26 Aug 2017 21:21:31 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Su", "Weijie", ""]]}, {"id": "1703.09906", "submitter": "Yuhan Chen", "authors": "Yuhan Chen, David B. Dunson", "title": "Modular Bayes screening for high-dimensional predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the routine collection of massive-dimensional predictors in many\napplication areas, screening methods that rapidly identify a small subset of\npromising predictors have become commonplace. We propose a new MOdular Bayes\nScreening (MOBS) approach, which involves several novel characteristics that\ncan potentially lead to improved performance. MOBS first applies a Bayesian\nmixture model to the marginal distribution of the response, obtaining posterior\nsamples of mixture weights, cluster-specific parameters, and cluster\nallocations for each subject. Hypothesis tests are then introduced,\ncorresponding to whether or not to include a given predictor, with posterior\nprobabilities for each hypothesis available analytically conditionally on\nunknowns sampled in the first stage and tuning parameters controlling borrowing\nof information across tests. By marginalizing over the first stage posterior\nsamples, we avoid under-estimation of uncertainty typical of two-stage methods.\nWe greatly simplify the model specification and reduce computational complexity\nby using {\\em modularization}. We provide basic theoretical support for this\napproach, and illustrate excellent performance relative to competitors in\nsimulation studies and the ability to capture complex shifts beyond simple\ndifferences in means. The method is illustrated with applications to genomics\nby using a very high-dimensional cis-eQTL dataset with roughly 38 million SNPs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 06:58:30 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Chen", "Yuhan", ""], ["Dunson", "David B.", ""]]}, {"id": "1703.10136", "submitter": "Youjin Lee", "authors": "Youjin Lee, Cencheng Shen, Carey E. Priebe, and Joshua T. Vogelstein", "title": "Network Dependence Testing via Diffusion Maps and Distance-Based\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciphering the associations between network connectivity and nodal\nattributes is one of the core problems in network science. The dependency\nstructure and high-dimensionality of networks pose unique challenges to\ntraditional dependency tests in terms of theoretical guarantees and empirical\nperformance. We propose an approach to test network dependence via diffusion\nmaps and distance-based correlations. We prove that the new method yields a\nconsistent test statistic under mild distributional assumptions on the graph\nstructure, and demonstrate that it is able to efficiently identify the most\ninformative graph embedding with respect to the diffusion time. The methodology\nis illustrated on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 17:00:42 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 16:05:36 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 21:13:26 GMT"}, {"version": "v4", "created": "Sun, 12 Aug 2018 03:23:54 GMT"}, {"version": "v5", "created": "Thu, 14 Feb 2019 20:02:31 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Lee", "Youjin", ""], ["Shen", "Cencheng", ""], ["Priebe", "Carey E.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1703.10143", "submitter": "Richard Samworth", "authors": "Richard A. Lockhart and Richard J. Samworth", "title": "Comments on `High-dimensional simultaneous inference with the bootstrap'", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide some comments on the article `High-dimensional simultaneous\ninference with the bootstrap' by Ruben Dezeure, Peter Buhlmann and Cun-Hui\nZhang.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 17:18:59 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Lockhart", "Richard A.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1703.10192", "submitter": "Romain Aza\\\"is", "authors": "Romain Aza\\\"is and Alexandre Genadot", "title": "Estimation of the average number of continuous crossings for\n  non-stationary non-diffusion processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that you observe trajectories of a non-diffusive non-stationary\nprocess and that you are interested in the average number of times where the\nprocess crosses some threshold (in dimension $d=1$) or hypersurface (in\ndimension $d\\geq2$). Of course, you can actually estimate this quantity by its\nempirical version counting the number of observed crossings. But is there a\nbetter way? In this paper, for a wide class of piecewise smooth processes, we\npropose estimators of the average number of continuous crossings of an\nhypersurface based on Kac-Rice formulae. We revisit these formulae in the uni-\nand multivariate framework in order to be able to handle non-stationary\nprocesses. Our statistical method is tested on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 18:34:48 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 10:15:10 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 16:46:15 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Aza\u00efs", "Romain", ""], ["Genadot", "Alexandre", ""]]}, {"id": "1703.10210", "submitter": "Brian Lynch", "authors": "Brian Lynch, Kehui Chen", "title": "A test of weak separability for multi-way functional data, with\n  application to brain connectivity studies", "comments": null, "journal-ref": "Biometrika, Volume 105, Issue 4, 1 December 2018, Pages 815-831", "doi": "10.1093/biomet/asy048", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the modeling of multi-way functional data where double or\nmultiple indices are involved. We introduce a concept of weak separability. The\nweakly separable structure supports the use of factorization methods that\ndecompose the signal into its spatial and temporal components. The analysis\nreveals interesting connections to the usual strongly separable covariance\nstructure, and provides insights into tensor methods for multi-way functional\ndata. We propose a formal test for the weak separability hypothesis, where the\nasymptotic null distribution of the test statistic is a chi-square type\nmixture. The method is applied to study brain functional connectivity derived\nfrom source localized magnetoencephalography signals during motor tasks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 19:36:07 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 17:53:58 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 19:22:59 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Lynch", "Brian", ""], ["Chen", "Kehui", ""]]}, {"id": "1703.10256", "submitter": "Shu Yang", "authors": "Shu Yang and Jae Kwang Kim", "title": "Predictive mean matching imputation in survey sampling", "comments": "20 pages, 0 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive mean matching imputation is popular for handling item nonresponse\nin survey sampling. In this article, we study the asymptotic properties of the\npredictive mean matching estimator of the population mean. For variance\nestimation, the conventional bootstrap inference for matching estimators with\nfixed matches has been shown to be invalid due to the nonsmoothness nature of\nthe matching estimator. We propose asymptotically valid replication variance\nestimation. The key strategy is to construct replicates of the estimator\ndirectly based on linear terms, instead of individual records of variables.\nExtension to nearest neighbor imputation is also discussed. A simulation study\nconfirms that the new procedure provides valid variance estimation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 22:33:31 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 14:37:03 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 21:58:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1703.10266", "submitter": "Michael Donohue", "authors": "Dan Li, Samuel Iddi, Wesley K. Thompson, Michael C. Donohue", "title": "Bayesian latent time joint mixed effect models for multicohort\n  longitudinal data", "comments": null, "journal-ref": "Stat.Methods.Med.Res. (2017)", "doi": "10.1177/0962280217737566", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Characterization of long-term disease dynamics, from disease-free to\nend-stage, is integral to understanding the course of neurodegenerative\ndiseases such as Parkinson's and Alzheimer's; and ultimately, how best to\nintervene. Natural history studies typically recruit multiple cohorts at\ndifferent stages of disease and follow them longitudinally for a relatively\nshort period of time. We propose a latent time joint mixed effects model to\ncharacterize long-term disease dynamics using this short-term data. Markov\nchain Monte Carlo methods are proposed for estimation, model selection, and\ninference. We apply the model to detailed simulation studies and data from the\nAlzheimer's Disease Neuroimaging Initiative.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 23:19:34 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 21:55:34 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Li", "Dan", ""], ["Iddi", "Samuel", ""], ["Thompson", "Wesley K.", ""], ["Donohue", "Michael C.", ""]]}, {"id": "1703.10364", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck, Antony M. Overstall, Quentin F. Gronau, Eric-Jan\n  Wagenmakers", "title": "Quantifying Uncertainty in Transdimensional Markov Chain Monte Carlo\n  Using Discrete Markov Models", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-018-9828-0", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis often concerns an evaluation of models with different\ndimensionality as is necessary in, for example, model selection or mixture\nmodels. To facilitate this evaluation, transdimensional Markov chain Monte\nCarlo (MCMC) relies on sampling a discrete indexing variable to estimate the\nposterior model probabilities. However, little attention has been paid to the\nprecision of these estimates. If only few switches occur between the models in\nthe transdimensional MCMC output, precision may be low and assessment based on\nthe assumption of independent samples misleading. Here, we propose a new method\nto estimate the precision based on the observed transition matrix of the\nmodel-indexing variable. Assuming a first order Markov model, the method\nsamples from the posterior of the stationary distribution. This allows\nassessment of the uncertainty in the estimated posterior model probabilities,\nmodel ranks, and Bayes factors. Moreover, the method provides an estimate for\nthe effective sample size of the MCMC output. In two model-selection examples,\nwe show that the proposed approach provides a good assessment of the\nuncertainty associated with the estimated posterior model probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:54:34 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 10:37:50 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 14:00:02 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Heck", "Daniel W.", ""], ["Overstall", "Antony M.", ""], ["Gronau", "Quentin F.", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1703.10534", "submitter": "Zhaoqiang Liu", "authors": "Zhaoqiang Liu, Vincent Y. F. Tan", "title": "The Informativeness of $k$-Means for Learning Mixture Models", "comments": "Accepted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of mixture models can be viewed as a clustering problem. Indeed,\ngiven data samples independently generated from a mixture of distributions, we\noften would like to find the correct target clustering of the samples according\nto which component distribution they were generated from. For a clustering\nproblem, practitioners often choose to use the simple k-means algorithm.\nk-means attempts to find an optimal clustering which minimizes the\nsum-of-squared distance between each point and its cluster center. In this\npaper, we provide sufficient conditions for the closeness of any optimal\nclustering and the correct target clustering assuming that the data samples are\ngenerated from a mixture of log-concave distributions. Moreover, we show that\nunder similar or even weaker conditions on the mixture model, any optimal\nclustering for the samples with reduced dimensionality is also close to the\ncorrect target clustering. These results provide intuition for the\ninformativeness of k-means (with and without dimensionality reduction) as an\nalgorithm for learning mixture models. We verify the correctness of our\ntheorems using numerical experiments and demonstrate using datasets with\nreduced dimensionality significant speed ups for the time required to perform\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 15:41:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 06:54:35 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 13:48:52 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Liu", "Zhaoqiang", ""], ["Tan", "Vincent Y. F.", ""]]}]