[{"id": "1010.0173", "submitter": "Pierre Courrieu", "authors": "Pierre Courrieu (LPC), Muriele Brand-D'Abrescia (LEAD), Ronald\n  Peereman (LPNC), Daniel Spieler, Arnaud Rey (LPC)", "title": "Validated Intraclass Correlation Statistics to Test Item Performance\n  Models", "comments": null, "journal-ref": "Behavior Research Methods 43, 1 (2011) pp. 37-55, DOI\n  10.3758/s13428-010-0020-5", "doi": "10.3758/s13428-010-0020-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method, with an application program in Matlab code, is proposed for\ntesting item performance models on empirical databases. This method uses data\nintraclass correlation statistics as expected correlations to which one\ncompares simple functions of correlations between model predictions and\nobserved item performance. The method rests on a data population model whose\nvalidity for the considered data is suitably tested, and has been verified for\nthree behavioural measure databases. Contrarily to usual model selection\ncriteria, this method provides an effective way of testing under-fitting and\nover-fitting, answering the usually neglected question \"does this model\nsuitably account for these data?\"\n", "versions": [{"version": "v1", "created": "Fri, 1 Oct 2010 14:33:45 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2011 12:56:31 GMT"}], "update_date": "2011-04-13", "authors_parsed": [["Courrieu", "Pierre", "", "LPC"], ["Brand-D'Abrescia", "Muriele", "", "LEAD"], ["Peereman", "Ronald", "", "LPNC"], ["Spieler", "Daniel", "", "LPC"], ["Rey", "Arnaud", "", "LPC"]]}, {"id": "1010.0274", "submitter": "Juan Andres Bazerque", "authors": "Juan A. Bazerque, Gonzalo Mateos and Georgios B. Giannakis", "title": "Group-Lasso on Splines for Spectrum Cartography", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2011.2160858", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unceasing demand for continuous situational awareness calls for\ninnovative and large-scale signal processing algorithms, complemented by\ncollaborative and adaptive sensing platforms to accomplish the objectives of\nlayered sensing and control. Towards this goal, the present paper develops a\nspline-based approach to field estimation, which relies on a basis expansion\nmodel of the field of interest. The model entails known bases, weighted by\ngeneric functions estimated from the field's noisy samples. A novel field\nestimator is developed based on a regularized variational least-squares (LS)\ncriterion that yields finitely-parameterized (function) estimates spanned by\nthin-plate splines. Robustness considerations motivate well the adoption of an\novercomplete set of (possibly overlapping) basis functions, while a sparsifying\nregularizer augmenting the LS cost endows the estimator with the ability to\nselect a few of these bases that ``better'' explain the data. This parsimonious\nfield representation becomes possible, because the sparsity-aware spline-based\nmethod of this paper induces a group-Lasso estimator for the coefficients of\nthe thin-plate spline expansions per basis. A distributed algorithm is also\ndeveloped to obtain the group-Lasso estimator using a network of wireless\nsensors, or, using multiple processors to balance the load of a single\ncomputational unit. The novel spline-based approach is motivated by a spectrum\ncartography application, in which a set of sensing cognitive radios collaborate\nto estimate the distribution of RF power in space and frequency. Simulated\ntests corroborate that the estimated power spectrum density atlas yields the\ndesired RF state awareness, since the maps reveal spatial locations where idle\nfrequency bands can be reused for transmission, even when fading and shadowing\neffects are pronounced.\n", "versions": [{"version": "v1", "created": "Fri, 1 Oct 2010 22:59:07 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Bazerque", "Juan A.", ""], ["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1010.0300", "submitter": "Jean-Michel Marin", "authors": "Gilles Celeux, Mohammed El Anbari, Jean-Michel Marin and Christian P.\n  Robert", "title": "Regularization in regression: comparing Bayesian and frequentist methods\n  in a poorly informative situation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a collection of simulated an real benchmarks, we compare Bayesian and\nfrequentist regularization approaches under a low informative constraint when\nthe number of variables is almost equal to the number of observations on\nsimulated and real datasets. This comparison includes new global noninformative\napproaches for Bayesian variable selection built on Zellner's g-priors that are\nsimilar to Liang et al. (2008). The interest of those calibration-free\nproposals is discussed. The numerical experiments we present highlight the\nappeal of Bayesian regularization methods, when compared with non-Bayesian\nalternatives. They dominate frequentist methods in the sense that they provide\nsmaller prediction errors while selecting the most relevant variables in a\nparsimonious way.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 07:48:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2011 06:20:24 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2011 08:37:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Celeux", "Gilles", ""], ["Anbari", "Mohammed El", ""], ["Marin", "Jean-Michel", ""], ["Robert", "Christian P.", ""]]}, {"id": "1010.0303", "submitter": "Youngjo Lee", "authors": "Youngjo Lee, John A. Nelder", "title": "Likelihood Inference for Models with Unobservables: Another View", "comments": "This paper discussed in: [arXiv:1010.0804], [arXiv:1010.0807],\n  [arXiv:1010.0810]. Rejoinder at [arXiv:1010.0814]. Published in at\n  http://dx.doi.org/10.1214/09-STS277 the Statistical Science\n  (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 255-269", "doi": "10.1214/09-STS277", "report-no": "IMS-STS-STS277", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been controversies among statisticians on (i) what to model and\n(ii) how to make inferences from models with unobservables. One such\ncontroversy concerns the difference between estimation methods for the marginal\nmeans not necessarily having a probabilistic basis and statistical models\nhaving unobservables with a probabilistic basis. Another concerns\nlikelihood-based inference for statistical models with unobservables. This\nneeds an extended-likelihood framework, and we show how one such extension,\nhierarchical likelihood, allows this to be done. Modeling of unobservables\nleads to rich classes of new probabilistic models from which likelihood-type\ninferences can be made naturally with hierarchical likelihood.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 07:59:59 GMT"}, {"version": "v2", "created": "Wed, 6 Oct 2010 06:45:23 GMT"}], "update_date": "2010-10-07", "authors_parsed": [["Lee", "Youngjo", ""], ["Nelder", "John A.", ""]]}, {"id": "1010.0304", "submitter": "Jiawei Liu", "authors": "Bruce Lindsay, Jiawei Liu", "title": "Model Assessment Tools for a Model False World", "comments": "Published in at http://dx.doi.org/10.1214/09-STS302 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 303-318", "doi": "10.1214/09-STS302", "report-no": "IMS-STS-STS302", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard goal of model evaluation and selection is to find a model that\napproximates the truth well while at the same time is as parsimonious as\npossible. In this paper we emphasize the point of view that the models under\nconsideration are almost always false, if viewed realistically, and so we\nshould analyze model adequacy from that point of view. We investigate this\nissue in large samples by looking at a model credibility index, which is\ndesigned to serve as a one-number summary measure of model adequacy. We define\nthe index to be the maximum sample size at which samples from the model and\nthose from the true data generating mechanism are nearly indistinguishable. We\nuse standard notions from hypothesis testing to make this definition precise.\nWe use data subsampling to estimate the index. We show that the definition\nleads us to some new ways of viewing models as flawed but useful. The concept\nis an extension of the work of Davies [Statist. Neerlandica 49 (1995)\n185--245].\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 08:11:53 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Lindsay", "Bruce", ""], ["Liu", "Jiawei", ""]]}, {"id": "1010.0305", "submitter": "Guenther Walther", "authors": "Guenther Walther", "title": "Inference and Modeling with Log-concave Distributions", "comments": "Published in at http://dx.doi.org/10.1214/09-STS303 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 319-327", "doi": "10.1214/09-STS303", "report-no": "IMS-STS-STS303", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log-concave distributions are an attractive choice for modeling and\ninference, for several reasons: The class of log-concave distributions contains\nmost of the commonly used parametric distributions and thus is a rich and\nflexible nonparametric class of distributions. Further, the MLE exists and can\nbe computed with readily available algorithms. Thus, no tuning parameter, such\nas a bandwidth, is necessary for estimation. Due to these attractive\nproperties, there has been considerable recent research activity concerning the\ntheory and applications of log-concave distributions. This article gives a\nreview of these results.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 08:17:17 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Walther", "Guenther", ""]]}, {"id": "1010.0306", "submitter": "Paul Gustafson", "authors": "Paul Gustafson, Sander Greenland", "title": "Interval Estimation for Messy Observational Data", "comments": "Published in at http://dx.doi.org/10.1214/09-STS305 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 328-342", "doi": "10.1214/09-STS305", "report-no": "IMS-STS-STS305", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some aspects of Bayesian and frequentist interval estimation,\nfocusing first on their relative strengths and weaknesses when used in \"clean\"\nor \"textbook\" contexts. We then turn attention to observational-data situations\nwhich are \"messy,\" where modeling that acknowledges the limitations of study\ndesign and data collection leads to nonidentifiability. We argue, via a series\nof examples, that Bayesian interval estimation is an attractive way to proceed\nin this context even for frequentists, because it can be supplied with a\ndiagnostic in the form of a calibration-sensitivity simulation analysis. We\nillustrate the basis for this approach in a series of theoretical\nconsiderations, simulations and an application to a study of silica exposure\nand lung cancer.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 08:32:00 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Gustafson", "Paul", ""], ["Greenland", "Sander", ""]]}, {"id": "1010.0308", "submitter": "Joseph L. Gastwirth", "authors": "Joseph L. Gastwirth, Yulia R. Gel, Weiwen Miao", "title": "The Impact of Levene's Test of Equality of Variances on Statistical\n  Theory and Practice", "comments": "Published in at http://dx.doi.org/10.1214/09-STS301 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 343-360", "doi": "10.1214/09-STS301", "report-no": "IMS-STS-STS301", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, the underlying scientific question concerns whether the\nvariances of $k$ samples are equal. There are a substantial number of tests for\nthis problem. Many of them rely on the assumption of normality and are not\nrobust to its violation. In 1960 Professor Howard Levene proposed a new\napproach to this problem by applying the $F$-test to the absolute deviations of\nthe observations from their group means. Levene's approach is powerful and\nrobust to nonnormality and became a very popular tool for checking the\nhomogeneity of variances. This paper reviews the original method proposed by\nLevene and subsequent robust modifications. A modification of Levene-type tests\nto increase their power to detect monotonic trends in variances is discussed.\nThis procedure is useful when one is concerned with an alternative of\nincreasing or decreasing variability, for example, increasing volatility of\nstocks prices or \"open or closed gramophones\" in regression residual analysis.\nA major section of the paper is devoted to discussion of various scientific\nproblems where Levene-type tests have been used, for example, economic\nanthropology, accuracy of medical measurements, volatility of the price of oil,\nstudies of the consistency of jury awards in legal cases and the effect of\nhurricanes on ecological systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 08:38:14 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Gastwirth", "Joseph L.", ""], ["Gel", "Yulia R.", ""], ["Miao", "Weiwen", ""]]}, {"id": "1010.0310", "submitter": "Mark P. Becker", "authors": "Mark P. Becker", "title": "A Conversation with Leo Goodman", "comments": "Published in at http://dx.doi.org/10.1214/08-STS276 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 361-385", "doi": "10.1214/08-STS276", "report-no": "IMS-STS-STS276", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leo A. Goodman was born on August 7, 1928 in New York City. He received his\nA.B. degree, summa cum laude, in 1948 from Syracuse University, majoring in\nmathematics and sociology. He went on to pursue graduate studies in\nmathematics, with an emphasis on mathematical statistics, in the Mathematics\nDepartment at Princeton University, and in 1950 he was awarded the M.A. and\nPh.D. degrees. His statistics professors at Princeton were the late Sam Wilks\nand John Tukey. Goodman then began his academic career as a statistician, and\nalso as a statistician bridging sociology and statistics, with an appointment\nin 1950 as assistant professor in the Statistics Department and the Sociology\nDepartment at the University of Chicago, where he remained, except for various\nleaves, until 1987. He was promoted to associate professor in 1953, and to\nprofessor in 1955. Goodman was at Cambridge University in 1953--1954 and\n1959--1960 as visiting professor at Clare College and in the Statistical\nLaboratory. And he spent 1960--1961 as a visiting professor of mathematical\nstatistics and sociology at Columbia University. He was also a research\nassociate in the University of Chicago Population Research Center from 1967 to\n1987. In 1970 he was appointed the Charles L. Hutchinson Distinguished Service\nProfessor at the University of Chicago, a title that he held until 1987. He\nspent 1984--1985 at the Center for Advanced Study in the Behavioral Sciences in\nStanford. In 1987 he was appointed the Class of 1938 Professor at the\nUniversity of California, Berkeley, in the Sociology Department and the\nStatistics Department. Goodman's numerous honors include honorary D.Sc. degrees\nfrom the University of Michigan and Syracuse University, and membership in the\nNational Academy of Sciences, the American Academy of Arts and Sciences, and\nthe American Philosophical Society.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 08:46:46 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Becker", "Mark P.", ""]]}, {"id": "1010.0694", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Statistical inference optimized with respect to the observed sample for\n  single or multiple comparisons", "comments": "Typo in equation (7) of v2 corrected in equation (6) of v3; clarity\n  improved", "journal-ref": "Bickel, D. R. (2011). A predictive approach to measuring the\n  strength of statistical evidence for single and multiple comparisons.\n  Canadian Journal of Statistics, 39, 610-631", "doi": "10.1002/cjs.10109", "report-no": null, "categories": "math.ST cs.IT math.IT q-bio.BM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized maximum likelihood (NML) is a recent penalized likelihood that\nhas properties that justify defining the amount of discrimination information\n(DI) in the data supporting an alternative hypothesis over a null hypothesis as\nthe logarithm of an NML ratio, namely, the alternative hypothesis NML divided\nby the null hypothesis NML. The resulting DI, like the Bayes factor but unlike\nthe p-value, measures the strength of evidence for an alternative hypothesis\nover a null hypothesis such that the probability of misleading evidence\nvanishes asymptotically under weak regularity conditions and such that evidence\ncan support a simple null hypothesis. Unlike the Bayes factor, the DI does not\nrequire a prior distribution and is minimax optimal in a sense that does not\ninvolve averaging over outcomes that did not occur. Replacing a (possibly\npseudo-) likelihood function with its weighted counterpart extends the scope of\nthe DI to models for which the unweighted NML is undefined. The likelihood\nweights leverage side information, either in data associated with comparisons\nother than the comparison at hand or in the parameter value of a simple null\nhypothesis. Two case studies, one involving multiple populations and the other\ninvolving multiple biological features, indicate that the DI is robust to the\ntype of side information used when that information is assigned the weight of a\nsingle observation. Such robustness suggests that very little adjustment for\nmultiple comparisons is warranted if the sample size is at least moderate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 20:21:49 GMT"}, {"version": "v2", "created": "Wed, 6 Oct 2010 21:46:59 GMT"}, {"version": "v3", "created": "Tue, 2 Nov 2010 10:33:12 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1010.0804", "submitter": "Thomas A. Louis", "authors": "Thomas A. Louis", "title": "Discussion of Likelihood Inference for Models with Unobservables:\n  Another View", "comments": "Published in at http://dx.doi.org/10.1214/09-STS277A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 270-272", "doi": "10.1214/09-STS277A", "report-no": "IMS-STS-STS277A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Likelihood Inference for Models with Unobservables: Another\nView\" by Youngjo Lee and John A. Nelder [arXiv:1010.0303]\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 09:05:30 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Louis", "Thomas A.", ""]]}, {"id": "1010.0807", "submitter": "Geert Molenberghs", "authors": "Geert Molenberghs, Michael G. Kenward, Geert Verbeke", "title": "Discussion of Likelihood Inference for Models with Unobservables:\n  Another View", "comments": "Published in at http://dx.doi.org/10.1214/09-STS277B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 273-279", "doi": "10.1214/09-STS277B", "report-no": "IMS-STS-STS277B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Likelihood Inference for Models with Unobservables: Another\nView\" by Youngjo Lee and John A. Nelder [arXiv:1010.0303]\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 09:15:38 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Molenberghs", "Geert", ""], ["Kenward", "Michael G.", ""], ["Verbeke", "Geert", ""]]}, {"id": "1010.0810", "submitter": "Xiao-Li Meng", "authors": "Xiao-Li Meng", "title": "Decoding the H-likelihood", "comments": "Published in at http://dx.doi.org/10.1214/09-STS277C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 280-293", "doi": "10.1214/09-STS277C", "report-no": "IMS-STS-STS277C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Likelihood Inference for Models with Unobservables: Another\nView\" by Youngjo Lee and John A. Nelder [arXiv:1010.0303]\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 09:22:17 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Meng", "Xiao-Li", ""]]}, {"id": "1010.0814", "submitter": "Youngjo Lee", "authors": "Youngjo Lee, John A. Nelder", "title": "Rejoinder: Likelihood Inference for Models with Unobservables Another\n  View", "comments": "Published in at http://dx.doi.org/10.1214/09-STS277REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 3, 294-302", "doi": "10.1214/09-STS277REJ", "report-no": "IMS-STS-STS277REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Likelihood Inference for Models with Unobservables: Another\nView\" by Youngjo Lee and John A. Nelder [arXiv:1010.0303]\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 09:29:40 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Lee", "Youngjo", ""], ["Nelder", "John A.", ""]]}, {"id": "1010.1224", "submitter": "Sean Simpson", "authors": "Lloyd J. Edwards and Sean L. Simpson", "title": "Analysis of 24-Hour Ambulatory Blood Pressure Monitoring Data using\n  Orthonormal Polynomials in the Linear Mixed Model", "comments": null, "journal-ref": "Blood Pressure Monitoring 19: 153-163, 2014", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of 24-hour ambulatory blood pressure monitoring (ABPM) in clinical\npractice and observational epidemiological studies has grown considerably in\nthe past 25 years. ABPM is a very effective technique for assessing biological,\nenvironmental, and drug effects on blood pressure. In order to enhance the\neffectiveness of ABPM for clinical and observational research studies via\nanalytical and graphical results, developing alternative data analysis\napproaches are important. The linear mixed model for the analysis of\nlongitudinal data is particularly well-suited for the estimation of, inference\nabout, and interpretation of both population and subject-specific trajectories\nfor ABPM data. Subject-specific trajectories are of great importance in ABPM\nstudies, especially in clinical research, but little emphasis has been placed\non this dimension of the problem in the statistical analyses of the data. We\npropose using a linear mixed model with orthonormal polynomials across time in\nboth the fixed and random effects to analyze ABPM data. Orthonormal polynomials\nin the linear mixed model may be used to develop model-based, subject-specific\n24-hour ABPM correlates of cardiovascular disease outcomes. We demonstrate the\nproposed analysis technique using data from the Dietary Approaches to Stop\nHypertension (DASH) study, a multicenter, randomized, parallel arm feeding\nstudy that tested the effects of dietary patterns on blood pressure.\n", "versions": [{"version": "v1", "created": "Wed, 6 Oct 2010 18:35:45 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 00:04:59 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Edwards", "Lloyd J.", ""], ["Simpson", "Sean L.", ""]]}, {"id": "1010.1437", "submitter": "Mahdi Shafiei", "authors": "Mahdi Shafiei and Hugh Chipman", "title": "Mixed-Membership Stochastic Block-Models for Transactional Networks", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional network data can be thought of as a list of one-to-many\ncommunications(e.g., email) between nodes in a social network. Most social\nnetwork models convert this type of data into binary relations between pairs of\nnodes. We develop a latent mixed membership model capable of modeling richer\nforms of transactional network data, including relations between more than two\nnodes. The model can cluster nodes and predict transactions. The block-model\nnature of the model implies that groups can be characterized in very general\nways. This flexible notion of group structure enables discovery of rich\nstructure in transactional networks. Estimation and inference are accomplished\nvia a variational EM algorithm. Simulations indicate that the learning\nalgorithm can recover the correct generative model. Interesting structure is\ndiscovered in the Enron email dataset and another dataset extracted from the\nReddit website. Analysis of the Reddit data is facilitated by a novel\nperformance measure for comparing two soft clusterings. The new model is\nsuperior at discovering mixed membership in groups and in predicting\ntransactions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Oct 2010 14:16:38 GMT"}], "update_date": "2010-10-08", "authors_parsed": [["Shafiei", "Mahdi", ""], ["Chipman", "Hugh", ""]]}, {"id": "1010.1868", "submitter": "Qirong Ho", "authors": "Qirong Ho, Ankur P. Parikh, Le Song and Eric P. Xing", "title": "Infinite Hierarchical MMSB Model for Nested Communities/Groups in Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actors in realistic social networks play not one but a number of diverse\nroles depending on whom they interact with, and a large number of such\nrole-specific interactions collectively determine social communities and their\norganizations. Methods for analyzing social networks should capture these\nmulti-faceted role-specific interactions, and, more interestingly, discover the\nlatent organization or hierarchy of social communities. We propose a\nhierarchical Mixed Membership Stochastic Blockmodel to model the generation of\nhierarchies in social communities, selective membership of actors to subsets of\nthese communities, and the resultant networks due to within- and\ncross-community interactions. Furthermore, to automatically discover these\nlatent structures from social networks, we develop a Gibbs sampling algorithm\nfor our model. We conduct extensive validation of our model using synthetic\nnetworks, and demonstrate the utility of our model in real-world datasets such\nas predator-prey networks and citation networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Oct 2010 19:43:56 GMT"}], "update_date": "2010-10-12", "authors_parsed": [["Ho", "Qirong", ""], ["Parikh", "Ankur P.", ""], ["Song", "Le", ""], ["Xing", "Eric P.", ""]]}, {"id": "1010.1935", "submitter": "David Degras", "authors": "David Degras, Zhiwei Xu, Ting Zhang and Wei Biao Wu", "title": "Testing for Parallelism Between Trends in Multiple Time Series", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the inference of trends in multiple, nonstationary time\nseries. To test whether trends are parallel to each other, we use a parallelism\nindex based on the L2-distances between nonparametric trend estimators and\ntheir average. A central limit theorem is obtained for the test statistic and\nthe test's consistency is established. We propose a simulation-based\napproximation to the distribution of the test statistic, which significantly\nimproves upon the normal approximation. The test is also applied to devise a\nclustering algorithm. Finally, the finite-sample properties of the test are\nassessed through simulations and the test methodology is illustrated with time\nseries from Motorola cell phone activity in the United States.\n", "versions": [{"version": "v1", "created": "Sun, 10 Oct 2010 16:23:23 GMT"}, {"version": "v2", "created": "Tue, 24 May 2011 15:37:13 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Degras", "David", ""], ["Xu", "Zhiwei", ""], ["Zhang", "Ting", ""], ["Wu", "Wei Biao", ""]]}, {"id": "1010.2265", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "The Lambert Way to Gaussianize heavy tailed data with the inverse of\n  Tukey's h as a special case", "comments": "38 + 14 pages; 4 tables; 8 figures. Submitted for publication.\n  Keywords: Gaussianizing, family of heavy-tailed distributions, Tukey's $h$\n  distribution, Lambert W, kurtosis, transformation of random variables; latent\n  variables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a parametric, bijective transformation to generate heavy tail\nversions Y of arbitrary RVs X ~ F. The tail behavior of the so-called 'heavy\ntail Lambert W x F' RV Y depends on a tail parameter delta >= 0: for delta = 0,\nY = X, for delta > 0 Y has heavier tails than X. For X being Gaussian, this\nmeta-family of heavy-tailed distributions reduces to Tukey's h distribution.\nLambert's W function provides an explicit inverse transformation, which can be\nestimated by maximum likelihood. This inverse can remove heavy tails from data,\nand also provide analytical expressions for the cumulative distribution (cdf)\nand probability density function (pdf). As a special case, these yield explicit\nformulas for Tukey's h pdf and cdf - to the author's knowledge for the first\ntime in the literature. Simulations and applications to S&P 500 log-returns and\nsolar flares data demonstrate the usefulness of the introduced methodology. The\nR package \"LambertW\" (cran.r-project.org/web/packages/LambertW) implementing\nthe presented methodology is publicly available at CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 11 Oct 2010 23:42:41 GMT"}, {"version": "v2", "created": "Fri, 29 Oct 2010 02:13:03 GMT"}, {"version": "v3", "created": "Wed, 2 Feb 2011 23:51:41 GMT"}, {"version": "v4", "created": "Wed, 20 Jul 2011 22:45:13 GMT"}, {"version": "v5", "created": "Sun, 30 Dec 2012 21:02:18 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1010.2310", "submitter": "Cinzia Viroli", "authors": "Cinzia Viroli", "title": "Stochastic model selection for Mixtures of Matrix-Normals", "comments": "This paper has been withdrawn by the author. Some content of the work\n  paper has been extended", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of matrix normal distributions are a powerful tool for\nclassifying three-way data in unsupervised problems. The distribution of each\ncomponent is assumed to be a matrix variate normal density. The mixture model\ncan be estimated through the EM algorithm under the assumption that the number\nof components is known and fixed. In this work we introduce, develop and\nexplore a Bayesian analysis of the model in order to provide a tool for\nsimultaneous model estimation and model selection. The effectiveness of the\nproposed method is illustrated on a simulation study and on a real example.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 07:32:42 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 16:07:57 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2013 08:43:10 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Viroli", "Cinzia", ""]]}, {"id": "1010.2314", "submitter": "Cinzia Viroli", "authors": "Silvia Cagnone and Cinzia Viroli", "title": "A factor mixture analysis model for multivariate binary data", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a latent variable model for binary data coming from an\nunobserved heterogeneous population. The heterogeneity is taken into account by\nreplacing the traditional assumption of Gaussian distributed factors by a\nfinite mixture of multivariate Gaussians. The aim of the proposed model is\ntwofold: it allows to achieve dimension reduction when the data are dichotomous\nand, simultaneously, it performs model based clustering in the latent space.\nModel estimation is obtained by means of a maximum likelihood method via a\ngeneralized version of the EM algorithm. In order to evaluate the performance\nof the model a simulation study and two real applications are illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 07:43:05 GMT"}], "update_date": "2010-10-13", "authors_parsed": [["Cagnone", "Silvia", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1010.2457", "submitter": "Yohann de Castro", "authors": "Yohann de Castro (LM-Orsay)", "title": "Optimal designs for Lasso and Dantzig selector using Expander Codes", "comments": "Last version with optimal bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the high-dimensional regression problem using adjacency\nmatrices of unbalanced expander graphs. In this frame, we prove that the\n$\\ell_{2}$-prediction error and the $\\ell_{1}$-risk of the lasso and the\nDantzig selector are optimal up to an explicit multiplicative constant. Thus we\ncan estimate a high-dimensional target vector with an error term similar to the\none obtained in a situation where one knows the support of the largest\ncoordinates in advance.\n  Moreover, we show that these design matrices have an explicit restricted\neigenvalue. Precisely, they satisfy the restricted eigenvalue assumption and\nthe compatibility condition with an explicit constant.\n  Eventually, we capitalize on the recent construction of unbalanced expander\ngraphs due to Guruswami, Umans, and Vadhan, to provide a deterministic\npolynomial time construction of these design matrices.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 18:03:23 GMT"}, {"version": "v2", "created": "Fri, 5 Nov 2010 10:43:31 GMT"}, {"version": "v3", "created": "Thu, 18 Nov 2010 09:19:54 GMT"}, {"version": "v4", "created": "Thu, 14 Apr 2011 09:57:04 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2013 15:07:26 GMT"}, {"version": "v6", "created": "Tue, 22 Jul 2014 08:56:44 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["de Castro", "Yohann", "", "LM-Orsay"]]}, {"id": "1010.2731", "submitter": "Sahand N. Negahban", "authors": "Sahand N. Negahban, Pradeep Ravikumar, Martin J. Wainwright, Bin Yu", "title": "A Unified Framework for High-Dimensional Analysis of M-Estimators with\n  Decomposable Regularizers", "comments": "Published in at http://dx.doi.org/10.1214/12-STS400 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 538-557", "doi": "10.1214/12-STS400", "report-no": "IMS-STS-STS400", "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional statistical inference deals with models in which the the\nnumber of parameters p is comparable to or larger than the sample size n. Since\nit is usually impossible to obtain consistent procedures unless\n$p/n\\rightarrow0$, a line of recent work has studied models with various types\nof low-dimensional structure, including sparse vectors, sparse and structured\nmatrices, low-rank matrices and combinations thereof. In such settings, a\ngeneral approach to estimation is to solve a regularized optimization problem,\nwhich combines a loss function measuring how well the model fits the data with\nsome regularization function that encourages the assumed structure. This paper\nprovides a unified framework for establishing consistency and convergence rates\nfor such regularized M-estimators under high-dimensional scaling. We state one\nmain theorem and show how it can be used to re-derive some existing results,\nand also to obtain a number of new results on consistency and convergence\nrates, in both $\\ell_2$-error and related norms. Our analysis also identifies\ntwo key properties of loss and regularization functions, referred to as\nrestricted strong convexity and decomposability, that ensure corresponding\nregularized M-estimators have fast convergence rates and which are optimal in\nmany well-studied cases.\n", "versions": [{"version": "v1", "created": "Wed, 13 Oct 2010 19:05:27 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2012 14:28:14 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2013 11:17:04 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Negahban", "Sahand N.", ""], ["Ravikumar", "Pradeep", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1010.2737", "submitter": "Victoria Zinde-Walsh", "authors": "Victoria Zinde-Walsh", "title": "Identification and well-posedness in a class of nonparametric problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a companion note to Zinde-Walsh (2010), arXiv:1009.4217v1[MATH.ST],\nto clarify and extend results on identification in a number of problems that\nlead to a system of convolution equations. Examples include identification of\nthe distribution of mismeasured variables, of a nonparametric regression\nfunction under Berkson type measurement error, some nonparametric panel data\nmodels, etc. The reason that identification in different problems can be\nconsidered in one approach is that they lead to the same system of convolution\nequations; moreover the solution can be given under more general assumptions\nthan those usually considered, by examining these equations in spaces of\ngeneralized functions. An important issue that did not receive sufficient\nattention is that of well-posedness. This note gives conditions under which\nwell-posedness obtains, an example that demonstrates that when well-posedness\ndoes not hold functions that are far apart can give rise to observable\narbitrarily close functions and discusses misspecification and estimation from\nthe stand-point of well-posedness.\n", "versions": [{"version": "v1", "created": "Wed, 13 Oct 2010 19:19:49 GMT"}], "update_date": "2010-10-14", "authors_parsed": [["Zinde-Walsh", "Victoria", ""]]}, {"id": "1010.3043", "submitter": "Tamara Kolda", "authors": "Eric C. Chi and Tamara G. Kolda", "title": "Making Tensor Factorizations Robust to Non-Gaussian Noise", "comments": "Contributed presentation at the NIPS Workshop on Tensors, Kernels,\n  and Machine Learning, Whistler, BC, Canada, December 10, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are multi-way arrays, and the Candecomp/Parafac (CP) tensor\nfactorization has found application in many different domains. The CP model is\ntypically fit using a least squares objective function, which is a maximum\nlikelihood estimate under the assumption of i.i.d. Gaussian noise. We\ndemonstrate that this loss function can actually be highly sensitive to\nnon-Gaussian noise. Therefore, we propose a loss function based on the 1-norm\nbecause it can accommodate both Gaussian and grossly non-Gaussian\nperturbations. We also present an alternating majorization-minimization\nalgorithm for fitting a CP model using our proposed loss function.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 23:21:30 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Chi", "Eric C.", ""], ["Kolda", "Tamara G.", ""]]}, {"id": "1010.3317", "submitter": "Ronald Barry", "authors": "Ronald P. Barry and Julie McIntyre", "title": "Estimating animal densities and home range in regions with irregular\n  boundaries and holes: a lattice-based alternative to the kernel density\n  estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimates based on point processes are often restrained to regions\nwith irregular boundaries or holes. We propose a density estimator, the\nlattice-based density estimator, which produces reasonable density estimates\nunder these circumstances. The estimation process starts with overlaying the\nregion with nodes, linking these together in a lattice and then computing the\ndensity of random walks of length k on the lattice. We use an approximation to\nthe unbiased crossvalidation criterion to find the optimal walk length k. The\ntechnique is illustrated using walleye (Sander vitreus) radiotelemetry\nrelocations in Lake Monroe, Indiana. We also use simulation to compare the\ntechnique to the traditional kernel density estimate in the situation where\nthere are no significant boundary effects.\n", "versions": [{"version": "v1", "created": "Sat, 16 Oct 2010 04:59:37 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Barry", "Ronald P.", ""], ["McIntyre", "Julie", ""]]}, {"id": "1010.3390", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Local shrinkage rules, Levy processes, and regularized regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Levy processes to generate joint prior distributions, and therefore\npenalty functions, for a location parameter as p grows large. This generalizes\nthe class of local-global shrinkage rules based on scale mixtures of normals,\nilluminates new connections among disparate methods, and leads to new results\nfor computing posterior means and modes under a wide class of priors. We extend\nthis framework to large-scale regularized regression problems where p>n, and\nprovide comparisons with other methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 17 Oct 2010 02:34:51 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2011 22:38:27 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1010.3501", "submitter": "Noor Amin Muhammad Noor-ul-Amin", "authors": "Muhammad Noor-Ul-Amin", "title": "Forecasting with Neural Networks: A comparative study using the data of\n  emergency service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a case study discussing the supervised artificial neural network for\nthe purpose of forecasting with comparison of the Box-Jenkins methodology by\nusing the data of well known emergency service Rescue 1122. We fits a variety\nof neural network (NN) models and many problems were revealed while fitting the\nANNs model to achieve the local minima. Moreover ANNs model is giving much\nbetter out of sample forecasts as compare to the ARIMA model. However we use\ndiagnostic checks for the comparison of models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Oct 2010 07:04:28 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Noor-Ul-Amin", "Muhammad", ""]]}, {"id": "1010.3955", "submitter": "Ian Dryden", "authors": "Ian L. Dryden, Alexey Kolydenko, Diwei Zhou and Bai Li", "title": "Non-Euclidean statistical analysis of covariance matrices and diffusion\n  tensors", "comments": "The paper was presented at the 57th session of the International\n  Statistical Institute, 16-22 August, 2009, Durban, South Africa", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of covariance matrices occurs in many important\napplications, e.g. in diffusion tensor imaging and longitudinal data analysis.\nWe consider the situation where it is of interest to estimate an average\ncovariance matrix, describe its anisotropy, to carry out principal geodesic\nanalysis and to interpolate between covariance matrices. There are many choices\nof metric available, each with its advantages. The particular choice of what is\nbest will depend on the particular application. The use of the Procrustes\nsize-and-shape metric is particularly appropriate when the covariance matrices\nare close to being deficient in rank. We discuss the use of different metrics\nfor diffusion tensor analysis, and we also introduce certain types of\nregularization for tensors.\n", "versions": [{"version": "v1", "created": "Sun, 10 Oct 2010 21:39:31 GMT"}], "update_date": "2010-10-20", "authors_parsed": [["Dryden", "Ian L.", ""], ["Kolydenko", "Alexey", ""], ["Zhou", "Diwei", ""], ["Li", "Bai", ""]]}, {"id": "1010.4345", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Daniel Chen, Victor Chernozhukov, Christian Hansen", "title": "Sparse Models and Methods for Optimal Instruments with an Application to\n  Eminent Domain", "comments": null, "journal-ref": "Econometrica 80, no. 6 (2012): 2369-2429", "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop results for the use of Lasso and Post-Lasso methods to form\nfirst-stage predictions and estimate optimal instruments in linear instrumental\nvariables (IV) models with many instruments, $p$. Our results apply even when\n$p$ is much larger than the sample size, $n$. We show that the IV estimator\nbased on using Lasso or Post-Lasso in the first stage is root-n consistent and\nasymptotically normal when the first-stage is approximately sparse; i.e. when\nthe conditional expectation of the endogenous variables given the instruments\ncan be well-approximated by a relatively small set of variables whose\nidentities may be unknown. We also show the estimator is semi-parametrically\nefficient when the structural error is homoscedastic. Notably our results allow\nfor imperfect model selection, and do not rely upon the unrealistic \"beta-min\"\nconditions that are widely used to establish validity of inference following\nmodel selection. In simulation experiments, the Lasso-based IV estimator with a\ndata-driven penalty performs well compared to recently advocated\nmany-instrument-robust procedures. In an empirical example dealing with the\neffect of judicial eminent domain decisions on economic outcomes, the\nLasso-based IV estimator outperforms an intuitive benchmark.\n  In developing the IV results, we establish a series of new results for Lasso\nand Post-Lasso estimators of nonparametric conditional expectation functions\nwhich are of independent theoretical and practical interest. We construct a\nmodification of Lasso designed to deal with non-Gaussian, heteroscedastic\ndisturbances which uses a data-weighted $\\ell_1$-penalty function. Using\nmoderate deviation theory for self-normalized sums, we provide convergence\nrates for the resulting Lasso and Post-Lasso estimators that are as sharp as\nthe corresponding rates in the homoscedastic Gaussian case under the condition\nthat $\\log p = o(n^{1/3})$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 00:49:43 GMT"}, {"version": "v2", "created": "Wed, 27 Oct 2010 18:56:47 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2012 15:20:40 GMT"}, {"version": "v4", "created": "Sat, 8 Sep 2012 19:26:58 GMT"}, {"version": "v5", "created": "Sun, 19 Apr 2015 19:39:38 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chen", "Daniel", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1010.4406", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Aaron D. Byrnes and Pavel V. Shevchenko", "title": "Impact of Insurance for Operational Risk: Is it worthwhile to insure or\n  be insured for severe losses?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach allows a provision for reduction of capital as a result of\ninsurance mitigation of up to 20%. This paper studies the behaviour of\ndifferent insurance policies in the context of capital reduction for a range of\npossible extreme loss models and insurance policy scenarios in a multi-period,\nmultiple risk settings. A Loss Distributional Approach (LDA) for modelling of\nthe annual loss process, involving homogeneous compound Poisson processes for\nthe annual losses, with heavy tailed severity models comprised of alpha-stable\nseverities is considered. There has been little analysis of such models to date\nand it is believed, insurance models will play more of a role in OpRisk\nmitigation and capital reduction in future. The first question of interest is\nwhen would it be equitable for a bank or financial institution to purchase\ninsurance for heavy tailed OpRisk losses under different insurance policy\nscenarios? The second question then pertains to Solvency II and addresses what\nthe insurers capital would be for such operational risk scenarios under\ndifferent policy offerings. In addition we consider the insurers perspective\nwith respect to fair premium as a percentage above the expected annual claim\nfor each insurance policy. The intention being to address questions related to\nVaR reduction under Basel II, SCR under Solvency II and fair insurance premiums\nin OpRisk for different extreme loss scenarios. In the process we provide\nclosed form solutions for the distribution of loss process and claims process\nin an LDA structure as well as closed form analytic solutions for the Expected\nShortfall, SCR and MCR under Basel II and Solvency II. We also provide closed\nform analytic solutions for the annual loss distribution of multiple risks\nincluding insurance mitigation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 09:42:57 GMT"}, {"version": "v2", "created": "Wed, 3 Nov 2010 00:19:24 GMT"}], "update_date": "2010-11-04", "authors_parsed": [["Peters", "Gareth W.", ""], ["Byrnes", "Aaron D.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1010.4436", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "A Comparison of Two Proximity Catch Digraph Families in Testing Spatial\n  Clustering", "comments": "56 pages, 42 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-10-3", "categories": "math.CO math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two parametrized random digraph families, namely,\nproportional-edge and central similarity proximity catch digraphs (PCDs) and\ncompare the performance of these two PCD families in testing spatial point\npatterns. These PCD families are based on relative positions of data points\nfrom two classes and the relative density of the PCDs is used as a statistic\nfor testing segregation and association against complete spatial randomness.\nWhen scaled properly, the relative density of a PCD is a U-statistic. We extend\nthe distribution of the relative density of central similarity PCDs for\nexpansion parameter being larger than one. We compare the asymptotic\ndistribution of the statistic for the two PCD families, using the standard\ncentral limit theory of U-statistics. We compare finite sample performance of\nthe tests by Monte Carlo simulations and prove the consistency of the tests\nunder the alternatives. The asymptotic performance of the tests under the\nalternatives is assessed by Pitman's asymptotic efficiency. We find the optimal\nexpansion parameters of the PCDs for testing each of the segregation and\nassociation alternatives in finite samples and in the limit. We demonstrate\nthat in terms of empirical power (i.e., for finite samples) relative density of\ncentral similarity PCD has better performance (which occurs for expansion\nparameter values larger than one) under segregation alternative, while relative\ndensity of proportional-edge PCD has better performance under association\nalternative. The methods are illustrated in a real-life example from plant\necology.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 11:32:49 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "1010.4621", "submitter": "Gang Zheng", "authors": "Gang Zheng, Jonathan Marchini, Nancy L. Geller", "title": "Introduction to the Special Issue: Genome-Wide Association Studies", "comments": "Published in at http://dx.doi.org/10.1214/09-STS310 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 387-387", "doi": "10.1214/09-STS310", "report-no": "IMS-STS-STS310", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction to the Special Issue: Genome-Wide Association Studies\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 06:37:32 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Zheng", "Gang", ""], ["Marchini", "Jonathan", ""], ["Geller", "Nancy L.", ""]]}, {"id": "1010.4629", "submitter": "Nan M. Laird", "authors": "Nan M. Laird, Christoph Lange", "title": "The Role of Family-Based Designs in Genome-Wide Association Studies", "comments": "Published in at http://dx.doi.org/10.1214/08-STS280 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 388-397", "doi": "10.1214/08-STS280", "report-no": "IMS-STS-STS280", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-Wide Association Studies (GWAS) offer an exciting and promising new\nresearch avenue for finding genes for complex diseases. Traditional\ncase-control and cohort studies offer many advantages for such designs.\nFamily-based association designs have long been attractive for their robustness\nproperties, but robustness can mean a loss of power. In this paper we discuss\nsome of the special features of family designs and their relevance in the era\nof GWAS.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 07:57:16 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Laird", "Nan M.", ""], ["Lange", "Christoph", ""]]}, {"id": "1010.4637", "submitter": "Kathryn Roeder", "authors": "Kathryn Roeder, Larry Wasserman", "title": "Genome-Wide Significance Levels and Weighted Hypothesis Testing", "comments": "Published in at http://dx.doi.org/10.1214/09-STS289 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 398-413", "doi": "10.1214/09-STS289", "report-no": "IMS-STS-STS289", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic investigations often involve the testing of vast numbers of related\nhypotheses simultaneously. To control the overall error rate, a substantial\npenalty is required, making it difficult to detect signals of moderate\nstrength. To improve the power in this setting, a number of authors have\nconsidered using weighted $p$-values, with the motivation often based upon the\nscientific plausibility of the hypotheses. We review this literature, derive\noptimal weights and show that the power is remarkably robust to\nmisspecification of these weights. We consider two methods for choosing weights\nin practice. The first, external weighting, is based on prior information. The\nsecond, estimated weighting, uses the data to choose weights.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 08:42:27 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Roeder", "Kathryn", ""], ["Wasserman", "Larry", ""]]}, {"id": "1010.4659", "submitter": "Duncan C. Thomas", "authors": "Duncan C. Thomas, Graham Casey, David V. Conti, Robert W. Haile, Juan\n  Pablo Lewinger, Daniel O. Stram", "title": "Methodological Issues in Multistage Genome-Wide Association Studies", "comments": "Published in at http://dx.doi.org/10.1214/09-STS288 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 414-429", "doi": "10.1214/09-STS288", "report-no": "IMS-STS-STS288", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the high cost of commercial genotyping chip technologies, many\ninvestigations have used a two-stage design for genome-wide association\nstudies, using part of the sample for an initial discovery of ``promising''\nSNPs at a less stringent significance level and the remainder in a joint\nanalysis of just these SNPs using custom genotyping. Typical cost savings of\nabout 50% are possible with this design to obtain comparable levels of overall\ntype I error and power by using about half the sample for stage I and carrying\nabout 0.1% of SNPs forward to the second stage, the optimal design depending\nprimarily upon the ratio of costs per genotype for stages I and II. However,\nwith the rapidly declining costs of the commercial panels, the generally low\nobserved ORs of current studies, and many studies aiming to test multiple\nhypotheses and multiple endpoints, many investigators are abandoning the\ntwo-stage design in favor of simply genotyping all available subjects using a\nstandard high-density panel. Concern is sometimes raised about the absence of a\n``replication'' panel in this approach, as required by some high-profile\njournals, but it must be appreciated that the two-stage design is not a\ndiscovery/replication design but simply a more efficient design for discovery\nusing a joint analysis of the data from both stages. Once a subset of\nhighly-significant associations has been discovered, a truly independent\n``exact replication'' study is needed in a similar population of the same\npromising SNPs using similar methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 09:55:31 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Thomas", "Duncan C.", ""], ["Casey", "Graham", ""], ["Conti", "David V.", ""], ["Haile", "Robert W.", ""], ["Lewinger", "Juan Pablo", ""], ["Stram", "Daniel O.", ""]]}, {"id": "1010.4670", "submitter": "Jonathan Marchini", "authors": "Zhan Su, Niall Cardin, the Wellcome Trust Case Control Consortium,\n  Peter Donnelly, Jonathan Marchini", "title": "A Bayesian Method for Detecting and Characterizing Allelic Heterogeneity\n  and Boosting Signals in Genome-Wide Association Studies", "comments": "Published in at http://dx.doi.org/10.1214/09-STS311 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 430-450", "doi": "10.1214/09-STS311", "report-no": "IMS-STS-STS311", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard paradigm for the analysis of genome-wide association studies\ninvolves carrying out association tests at both typed and imputed SNPs. These\nmethods will not be optimal for detecting the signal of association at SNPs\nthat are not currently known or in regions where allelic heterogeneity occurs.\nWe propose a novel association test, complementary to the SNP-based approaches,\nthat attempts to extract further signals of association by explicitly modeling\nand estimating both unknown SNPs and allelic heterogeneity at a locus. At each\nsite we estimate the genealogy of the case-control sample by taking advantage\nof the HapMap haplotypes across the genome. Allelic heterogeneity is modeled by\nallowing more than one mutation on the branches of the genealogy. Our use of\nBayesian methods allows us to assess directly the evidence for a causative SNP\nnot well correlated with known SNPs and for allelic heterogeneity at each\nlocus. Using simulated data and real data from the WTCCC project, we show that\nour method (i) produces a significant boost in signal and accurately identifies\nthe form of the allelic heterogeneity in regions where it is known to exist,\n(ii) can suggest new signals that are not found by testing typed or imputed\nSNPs and (iii) can provide more accurate estimates of effect sizes in regions\nof association.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 11:00:18 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Su", "Zhan", ""], ["Cardin", "Niall", ""], ["Consortium", "the Wellcome Trust Case Control", ""], ["Donnelly", "Peter", ""], ["Marchini", "Jonathan", ""]]}, {"id": "1010.4681", "submitter": "William Astle", "authors": "William Astle, David J. Balding", "title": "Population Structure and Cryptic Relatedness in Genetic Association\n  Studies", "comments": "Published in at http://dx.doi.org/10.1214/09-STS307 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 451-471", "doi": "10.1214/09-STS307", "report-no": "IMS-STS-STS307", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the problem of confounding in genetic association studies, which\narises principally because of population structure and cryptic relatedness.\nMany treatments of the problem consider only a simple ``island'' model of\npopulation structure. We take a broader approach, which views population\nstructure and cryptic relatedness as different aspects of a single confounder:\nthe unobserved pedigree defining the (often distant) relationships among the\nstudy subjects. Kinship is therefore a central concept, and we review methods\nof defining and estimating kinship coefficients, both pedigree-based and\nmarker-based. In this unified framework we review solutions to the problem of\npopulation structure, including family-based study designs, genomic control,\nstructured association, regression control, principal components adjustment and\nlinear mixed models. The last solution makes the most explicit use of the\nkinships among the study subjects, and has an established role in the analysis\nof animal and plant breeding studies. Recent computational developments mean\nthat analyses of human genetic association data are beginning to benefit from\nits powerful tests for association, which protect against population structure\nand cryptic kinship, as well as intermediate levels of confounding by the\npedigree.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 11:39:55 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Astle", "William", ""], ["Balding", "David J.", ""]]}, {"id": "1010.4686", "submitter": "Charles Kooperberg", "authors": "Charles Kooperberg, Michael LeBlanc, James Y. Dai, Indika Rajapakse", "title": "Structures and Assumptions: Strategies to Harness Gene $\\times$ Gene and\n  Gene $\\times$ Environment Interactions in GWAS", "comments": "Published in at http://dx.doi.org/10.1214/09-STS287 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 472-488", "doi": "10.1214/09-STS287", "report-no": "IMS-STS-STS287", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies, in which as many as a million single\nnucleotide polymorphisms (SNP) are measured on several thousand samples, are\nquickly becoming a common type of study for identifying genetic factors\nassociated with many phenotypes. There is a strong assumption that interactions\nbetween SNPs or genes and interactions between genes and environmental factors\nsubstantially contribute to the genetic risk of a disease. Identification of\nsuch interactions could potentially lead to increased understanding about\ndisease mechanisms; drug $\\times$ gene interactions could have profound\napplications for personalized medicine; strong interaction effects could be\nbeneficial for risk prediction models. In this paper we provide an overview of\ndifferent approaches to model interactions, emphasizing approaches that make\nspecific use of the structure of genetic data, and those that make specific\nmodeling assumptions that may (or may not) be reasonable to make. We conclude\nthat to identify interactions it is often necessary to do some selection of\nSNPs, for example, based on prior hypothesis or marginal significance, but that\nto identify SNPs that are marginally associated with a disease it may also be\nuseful to consider larger numbers of interactions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 12:23:10 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Kooperberg", "Charles", ""], ["LeBlanc", "Michael", ""], ["Dai", "James Y.", ""], ["Rajapakse", "Indika", ""]]}, {"id": "1010.4700", "submitter": "Nilanjan Chatterjee", "authors": "Nilanjan Chatterjee, Yi-Hau Chen, Sheng Luo, Raymond J. Carroll", "title": "Analysis of Case-Control Association Studies: SNPs, Imputation and\n  Haplotypes", "comments": "Published in at http://dx.doi.org/10.1214/09-STS297 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 489-502", "doi": "10.1214/09-STS297", "report-no": "IMS-STS-STS297", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although prospective logistic regression is the standard method of analysis\nfor case-control data, it has been recently noted that in genetic epidemiologic\nstudies one can use the ``retrospective'' likelihood to gain major power by\nincorporating various population genetics model assumptions such as\nHardy-Weinberg-Equilibrium (HWE), gene-gene and gene-environment independence.\nIn this article we review these modern methods and contrast them with the more\nclassical approaches through two types of applications (i) association tests\nfor typed and untyped single nucleotide polymorphisms (SNPs) and (ii)\nestimation of haplotype effects and haplotype-environment interactions in the\npresence of haplotype-phase ambiguity. We provide novel insights to existing\nmethods by construction of various score-tests and pseudo-likelihoods. In\naddition, we describe a novel two-stage method for analysis of untyped SNPs\nthat can use any flexible external algorithm for genotype imputation followed\nby a powerful association test based on the retrospective likelihood. We\nillustrate applications of the methods using simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 13:08:13 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Chatterjee", "Nilanjan", ""], ["Chen", "Yi-Hau", ""], ["Luo", "Sheng", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "1010.4710", "submitter": "Peter M. Visscher", "authors": "Michael E. Goddard, Naomi R. Wray, Klara Verbyla, Peter M. Visscher", "title": "Estimating Effects and Making Predictions from Genome-Wide Marker Data", "comments": "Published in at http://dx.doi.org/10.1214/09-STS306 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 517-529", "doi": "10.1214/09-STS306", "report-no": "IMS-STS-STS306", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association studies (GWAS), hundreds of thousands of genetic\nmarkers (SNPs) are tested for association with a trait or phenotype. Reported\neffects tend to be larger in magnitude than the true effects of these markers,\nthe so-called ``winner's curse.'' We argue that the classical definition of\nunbiasedness is not useful in this context and propose to use a different\ndefinition of unbiasedness that is a property of the estimator we advocate. We\nsuggest an integrated approach to the estimation of the SNP effects and to the\nprediction of trait values, treating SNP effects as random instead of fixed\neffects. Statistical methods traditionally used in the prediction of trait\nvalues in the genetics of livestock, which predates the availability of SNP\ndata, can be applied to analysis of GWAS, giving better estimates of the SNP\neffects and predictions of phenotypic and genetic values in individuals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 13:37:29 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Goddard", "Michael E.", ""], ["Wray", "Naomi R.", ""], ["Verbyla", "Klara", ""], ["Visscher", "Peter M.", ""]]}, {"id": "1010.5040", "submitter": "Sebastian Z\\\"{o}llner", "authors": "Sebastian Z\\\"ollner, Tanya M. Teslovich", "title": "Using GWAS Data to Identify Copy Number Variants Contributing to Common\n  Complex Diseases", "comments": "Published in at http://dx.doi.org/10.1214/09-STS304 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 530-546", "doi": "10.1214/09-STS304", "report-no": "IMS-STS-STS304", "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copy number variants (CNVs) account for more polymorphic base pairs in the\nhuman genome than do single nucleotide polymorphisms (SNPs). CNVs encompass\ngenes as well as noncoding DNA, making these polymorphisms good candidates for\nfunctional variation. Consequently, most modern genome-wide association studies\ntest CNVs along with SNPs, after inferring copy number status from the data\ngenerated by high-throughput genotyping platforms. Here we give an overview of\nCNV genomics in humans, highlighting patterns that inform methods for\nidentifying CNVs. We describe how genotyping signals are used to identify CNVs\nand provide an overview of existing statistical models and methods used to\ninfer location and carrier status from such data, especially the most commonly\nused methods exploring hybridization intensity. We compare the power of such\nmethods with the alternative method of using tag SNPs to identify CNV carriers.\nAs such methods are only powerful when applied to common CNVs, we describe two\nalternative approaches that can be informative for identifying rare CNVs\ncontributing to disease risk. We focus particularly on methods identifying de\nnovo CNVs and show that such methods can be more powerful than case-control\ndesigns. Finally we present some recommendations for identifying CNVs\ncontributing to common complex disorders.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 06:15:52 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Z\u00f6llner", "Sebastian", ""], ["Teslovich", "Tanya M.", ""]]}, {"id": "1010.5046", "submitter": "Ruth M. Pfeiffer", "authors": "Ruth M. Pfeiffer, Mitchell H. Gail, David Pee", "title": "On Combining Data From Genome-Wide Association Studies to Discover\n  Disease-Associated SNPs", "comments": "Published in at http://dx.doi.org/10.1214/09-STS286 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 547-560", "doi": "10.1214/09-STS286", "report-no": "IMS-STS-STS286", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining data from several case-control genome-wide association (GWA)\nstudies can yield greater efficiency for detecting associations of disease with\nsingle nucleotide polymorphisms (SNPs) than separate analyses of the component\nstudies. We compared several procedures to combine GWA study data both in terms\nof the power to detect a disease-associated SNP while controlling the\ngenome-wide significance level, and in terms of the detection probability\n($\\mathit{DP}$). The $\\mathit{DP}$ is the probability that a particular\ndisease-associated SNP will be among the $T$ most promising SNPs selected on\nthe basis of low $p$-values. We studied both fixed effects and random effects\nmodels in which associations varied across studies. In settings of practical\nrelevance, meta-analytic approaches that focus on a single degree of freedom\nhad higher power and $\\mathit{DP}$ than global tests such as summing chi-square\ntest-statistics across studies, Fisher's combination of $p$-values, and forming\na combined list of the best SNPs from within each study.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 07:00:52 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Pfeiffer", "Ruth M.", ""], ["Gail", "Mitchell H.", ""], ["Pee", "David", ""]]}, {"id": "1010.5091", "submitter": "Gang Zheng", "authors": "Gang Zheng, Jungnam Joo, Dmitri Zaykin, Colin Wu, Nancy Geller", "title": "Robust Tests in Genome-Wide Scans under Incomplete Linkage\n  Disequilibrium", "comments": "Published in at http://dx.doi.org/10.1214/09-STS314 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 503-516", "doi": "10.1214/09-STS314", "report-no": "IMS-STS-STS314", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under complete linkage disequilibrium (LD), robust tests often have greater\npower than Pearson's chi-square test and trend tests for the analysis of\ncase-control genetic association studies. Robust statistics have been used in\ncandidate-gene and genome-wide association studies (GWAS) when the genetic\nmodel is unknown. We consider here a more general incomplete LD model, and\nexamine the impact of penetrances at the marker locus when the genetic models\nare defined at the disease locus. Robust statistics are then reviewed and their\nefficiency and robustness are compared through simulations in GWAS of 300,000\nmarkers under the incomplete LD model. Applications of several robust tests to\nthe Wellcome Trust Case-Control Consortium [Nature 447 (2007) 661--678] are\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 11:19:57 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Zheng", "Gang", ""], ["Joo", "Jungnam", ""], ["Zaykin", "Dmitri", ""], ["Wu", "Colin", ""], ["Geller", "Nancy", ""]]}, {"id": "1010.5095", "submitter": "John P. A. Ioannidis", "authors": "Peter Kraft, Eleftheria Zeggini, John P. A. Ioannidis", "title": "Replication in Genome-Wide Association Studies", "comments": "Published in at http://dx.doi.org/10.1214/09-STS290 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2009, Vol. 24, No. 4, 561-573", "doi": "10.1214/09-STS290", "report-no": "IMS-STS-STS290", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication helps ensure that a genotype-phenotype association observed in a\ngenome-wide association (GWA) study represents a credible association and is\nnot a chance finding or an artifact due to uncontrolled biases. We discuss\nprerequisites for exact replication, issues of heterogeneity, advantages and\ndisadvantages of different methods of data synthesis across multiple studies,\nfrequentist vs. Bayesian inferences for replication, and challenges that arise\nfrom multi-team collaborations. While consistent replication can greatly\nimprove the credibility of a genotype-phenotype association, it may not\neliminate spurious associations due to biases shared by many studies.\nConversely, lack of replication in well-powered follow-up studies usually\ninvalidates the initially proposed association, although occasionally it may\npoint to differences in linkage disequilibrium or effect modifiers across\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 12:02:52 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Kraft", "Peter", ""], ["Zeggini", "Eleftheria", ""], ["Ioannidis", "John P. A.", ""]]}, {"id": "1010.5223", "submitter": "Nicholas G. Polson", "authors": "Nicholas G. Polson, James G. Scott", "title": "Good, great, or lucky? Screening for firms with sustained superior\n  performance using heavy-tailed priors", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS512 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 161-185", "doi": "10.1214/11-AOAS512", "report-no": "IMS-AOAS-AOAS512", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines historical patterns of ROA (return on assets) for a\ncohort of 53,038 publicly traded firms across 93 countries, measured over the\npast 45 years. Our goal is to screen for firms whose ROA trajectories suggest\nthat they have systematically outperformed their peer groups over time. Such a\nproject faces at least three statistical difficulties: adjustment for relevant\ncovariates, massive multiplicity, and longitudinal dependence. We conclude\nthat, once these difficulties are taken into account, demonstrably superior\nperformance appears to be quite rare. We compare our findings with other recent\nmanagement studies on the same subject, and with the popular literature on\ncorporate success. Our methodological contribution is to propose a new class of\npriors for use in large-scale simultaneous testing. These priors are based on\nthe hypergeometric inverted-beta family, and have two main attractive features:\nheavy tails and computational tractability. The family is a four-parameter\ngeneralization of the normal/inverted-beta prior, and is the natural conjugate\nprior for shrinkage coefficients in a hierarchical normal model. Our results\nemphasize the usefulness of these heavy-tailed priors in large multiple-testing\nproblems, as they have a mild rate of tail decay in the marginal likelihood\n$m(y)$---a property long recognized to be important in testing.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 19:10:47 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 02:53:23 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2012 06:24:53 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1010.5233", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Jianqing Fan, Jiancheng Jiang", "title": "Regularization for Cox's proportional hazards model with\n  NP-dimensionality", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS911 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 6, 3092-3120", "doi": "10.1214/11-AOS911", "report-no": "IMS-AOS-AOS911", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput genetic sequencing arrays with thousands of measurements per\nsample and a great amount of related censored clinical data have increased\ndemanding need for better measurement specific model selection. In this paper\nwe establish strong oracle properties of nonconcave penalized methods for\nnonpolynomial (NP) dimensional data with censoring in the framework of Cox's\nproportional hazards model. A class of folded-concave penalties are employed\nand both LASSO and SCAD are discussed specifically. We unveil the question\nunder which dimensionality and correlation restrictions can an oracle estimator\nbe constructed and grasped. It is demonstrated that nonconcave penalties lead\nto significant reduction of the \"irrepresentable condition\" needed for LASSO\nmodel selection consistency. The large deviation result for martingales,\nbearing interests of its own, is developed for characterizing the strong oracle\nproperty. Moreover, the nonconcave regularized estimator, is shown to achieve\nasymptotically the information bound of the oracle estimator. A coordinate-wise\nalgorithm is developed for finding the grid of solution paths for penalized\nhazard regression problems, and its performance is evaluated on simulated and\ngene association study examples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 19:51:46 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 00:24:07 GMT"}, {"version": "v3", "created": "Fri, 25 May 2012 10:39:39 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""], ["Fan", "Jianqing", ""], ["Jiang", "Jiancheng", ""]]}, {"id": "1010.5265", "submitter": "James Scott", "authors": "James G. Scott", "title": "Parameter expansion in local-shrinkage models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of using MCMC to fit sparse Bayesian models\nbased on normal scale-mixture priors. Examples of this framework include the\nBayesian LASSO and the horseshoe prior. We study the usefulness of parameter\nexpansion (PX) for improving convergence in such models, which is notoriously\nslow when the global variance component is near zero. Our conclusion is that\nparameter expansion does improve matters in LASSO-type models, but only\nmodestly. In most cases this improvement, while noticeable, is less than what\nmight be expected, especially compared to the improvements that PX makes\npossible for models very similar to those considered here. We give some\nexamples, and we attempt to provide some intuition as to why this is so. We\nalso describe how slice sampling may be used to update the global variance\ncomponent. In practice, this approach seems to perform almost as well as\nparameter expansion. As a practical matter, however, it is perhaps best viewed\nnot as a replacement for PX, but as a tool for expanding the class of models to\nwhich PX is applicable.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 21:09:59 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Scott", "James G.", ""]]}, {"id": "1010.5389", "submitter": "Merlin Keller", "authors": "Merlin Keller and Marc Lavielle", "title": "Random threshold for linear model selection, revisited", "comments": "22 pages, 7 figures. Submitted to Statistics and its Interface (SII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [Lavielle and Ludena 07], a random thresholding metho d is intro duced to\nselect the significant, or non null, mean terms among a collection of\nindependent random variables, and applied to the problem of recovering the\nsignificant coefficients in non ordered model selection. We intro duce a simple\nmodification which removes the dep endency of the proposed estimator on a\nwindow parameter while maintaining its asymptotic properties. A simulation\nstudy suggests that both procedures compare favorably to standard thresholding\napproaches, such as multiple testing or model-based clustering, in terms of the\nbinary classification risk. An application of the method to the problem of\nactivation detection on functional magnetic resonance imaging (fMRI) data is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 13:16:15 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Keller", "Merlin", ""], ["Lavielle", "Marc", ""]]}, {"id": "1010.5586", "submitter": "Elizabeth A. Stuart", "authors": "Elizabeth A. Stuart", "title": "Matching Methods for Causal Inference: A Review and a Look Forward", "comments": "Published in at http://dx.doi.org/10.1214/09-STS313 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 1, 1-21", "doi": "10.1214/09-STS313", "report-no": "IMS-STS-STS313", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating causal effects using observational data, it is desirable to\nreplicate a randomized experiment as closely as possible by obtaining treated\nand control groups with similar covariate distributions. This goal can often be\nachieved by choosing well-matched samples of the original treated and control\ngroups, thereby reducing bias due to the covariates. Since the 1970s, work on\nmatching methods has examined how to best choose treated and control subjects\nfor comparison. Matching methods are gaining popularity in fields such as\neconomics, epidemiology, medicine and political science. However, until now the\nliterature and related advice has been scattered across disciplines.\nResearchers who are interested in using matching methods---or developing\nmethods related to matching---do not have a single place to turn to learn about\npast and current research. This paper provides a structure for thinking about\nmatching methods and guidance on their use, coalescing the existing research\n(both old and new) and providing a summary of where the literature on matching\nmethods is now and where it should be headed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Oct 2010 06:47:24 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Stuart", "Elizabeth A.", ""]]}, {"id": "1010.6056", "submitter": "Xu Han", "authors": "Jianqing Fan, Xu Han and Weijie Gu", "title": "Estimating False Discovery Proportion Under Arbitrary Covariance\n  Dependence", "comments": "51 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1012.4397", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Multiple hypothesis testing is a fundamental problem in high dimensional\ninference, with wide applications in many scientific fields. In genome-wide\nassociation studies, tens of thousands of tests are performed simultaneously to\nfind if any SNPs are associated with some traits and those tests are\ncorrelated. When test statistics are correlated, false discovery control\nbecomes very challenging under arbitrary dependence. In the current paper, we\npropose a novel method based on principal factor approximation, which\nsuccessfully subtracts the common dependence and weakens significantly the\ncorrelation structure, to deal with an arbitrary dependence structure. We\nderive an approximate expression for false discovery proportion (FDP) in large\nscale multiple testing when a common threshold is used and provide a consistent\nestimate of realized FDP. This result has important applications in controlling\nFDR and FDP. Our estimate of realized FDP compares favorably with Efron\n(2007)'s approach, as demonstrated in the simulated examples. Our approach is\nfurther illustrated by some real data applications. We also propose a\ndependence-adjusted procedure, which is more powerful than the fixed threshold\nprocedure.\n", "versions": [{"version": "v1", "created": "Thu, 28 Oct 2010 19:11:55 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2011 05:05:50 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["Fan", "Jianqing", ""], ["Han", "Xu", ""], ["Gu", "Weijie", ""]]}, {"id": "1010.6113", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine and CREST) and Kerrie\n  L. Mengersen (Queensland University of Technology, Brisbane)", "title": "Exact Bayesian Analysis of Mixtures", "comments": "2 figures, 3 tables, 2 R codes. Chapter to appear in Mixtures:\n  Estimation and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how a complete and exact Bayesian analysis of a\nparametric mixture model is possible in some cases when components of the\nmixture are taken from exponential families and when conjugate priors are used.\nThis restricted set-up allows us to show the relevance of the Bayesian approach\nas well as to exhibit the limitations of a complete analysis, namely that it is\nimpossible to conduct this analysis when the sample size is too large, when the\ndata are not from an exponential family, or when priors that are more complex\nthan conjugate priors are used.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 00:18:55 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine and CREST"], ["Mengersen", "Kerrie L.", "", "Queensland University of Technology, Brisbane"]]}, {"id": "1010.6202", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Sequential Data-Adaptive Bandwidth Selection by Cross-Validation for\n  Nonparametric Prediction", "comments": "26 pages", "journal-ref": "Communications in Statistics - Simulation and Computation, Volume\n  41, 2012 - Issue 7, 1195-1219,", "doi": "10.1080/03610918.2012.625853", "report-no": null, "categories": "math.ST math.PR stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of bandwidth selection by cross-validation from a\nsequential point of view in a nonparametric regression model. Having in mind\nthat in applications one often aims at estimation, prediction and change\ndetection simultaneously, we investigate that approach for sequential kernel\nsmoothers in order to base these tasks on a single statistic. We provide\nuniform weak laws of large numbers and weak consistency results for the\ncross-validated bandwidth. Extensions to weakly dependent error terms are\ndiscussed as well. The errors may be {\\alpha}-mixing or L2-near epoch\ndependent, which guarantees that the uniform convergence of the cross\nvalidation sum and the consistency of the cross-validated bandwidth hold true\nfor a large class of time series. The method is illustrated by analyzing\nphotovoltaic data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 13:29:55 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Steland", "Ansgar", ""]]}]