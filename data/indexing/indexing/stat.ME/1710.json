[{"id": "1710.00019", "submitter": "Luis Leon-Novelo", "authors": "Luis G. Leon-Novelo and Terrance D. Savitsky", "title": "Fully Bayesian Estimation Under Informative Sampling", "comments": "Pages 1-29 conform the main paper and they include seven figures and\n  three tables. Pages 30-36 contain Supplementary Material and pages 36-37\n  contain references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian estimation is increasingly popular for performing model based\ninference to support policymaking. These data are often collected from surveys\nunder informative sampling designs where subject inclusion probabilities are\ndesigned to be correlated with the response variable of interest. Sampling\nweights constructed from marginal inclusion probabilities are typically used to\nform an exponentiated pseudo likelihood that adjusts the population likelihood\nfor estimation on the sample due to ease-of-estimation. We propose an\nalternative adjustment based on a Bayes rule construction that simultaneously\nperforms weight smoothing and estimates the population model parameters in a\nfully Bayesian construction. We formulate conditions on known marginal and\npairwise inclusion probabilities that define a class of sampling designs where\n$L_{1}$ consistency of the joint posterior is guaranteed. We compare\nperformances between the two approaches on synthetic data, which reveals that\nour fully Bayesian approach better estimates posterior uncertainty without a\nrequirement to calibrate the normalization of the sampling weights. We\ndemonstrate our method on an application concerning the National Health and\nNutrition Examination Survey exploring the relationship between caffeine\nconsumption and systolic blood pressure.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 18:26:03 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 01:29:20 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 00:59:58 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Leon-Novelo", "Luis G.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1710.00153", "submitter": "Minjie Fan", "authors": "Minjie Fan, Debashis Paul, Thomas C. M. Lee, Tomoko Matsuo", "title": "A Multi-Resolution Model for Non-Gaussian Random Fields on a Sphere with\n  Application to Ionospheric Electrostatic Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields have been one of the most popular tools for analyzing\nspatial data. However, many geophysical and environmental processes often\ndisplay non-Gaussian characteristics. In this paper, we propose a new class of\nspatial models for non-Gaussian random fields on a sphere based on a\nmulti-resolution analysis. Using a special wavelet frame, named spherical\nneedlets, as building blocks, the proposed model is constructed in the form of\na sparse random effects model. The spatial localization of needlets, together\nwith carefully chosen random coefficients, ensure the model to be non-Gaussian\nand isotropic. The model can also be expanded to include a spatially varying\nvariance profile. The special formulation of the model enables us to develop\nefficient estimation and prediction procedures, in which an adaptive MCMC\nalgorithm is used. We investigate the accuracy of parameter estimation of the\nproposed model, and compare its predictive performance with that of two\nGaussian models by extensive numerical experiments. Practical utility of the\nproposed model is demonstrated through an application of the methodology to a\ndata set of high-latitude ionospheric electrostatic potentials, generated from\nthe LFM-MIX model of the magnetosphere-ionosphere system.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 06:02:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Fan", "Minjie", ""], ["Paul", "Debashis", ""], ["Lee", "Thomas C. M.", ""], ["Matsuo", "Tomoko", ""]]}, {"id": "1710.00190", "submitter": "Stanislav Kolenikov", "authors": "Stanislav Kolenikov, Heather Hammer", "title": "Power analysis for a linear regression model when regressors are matrix\n  sampled", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple matrix sampling is a survey methodology technique that randomly\nchooses a relatively small subset of items to be presented to survey\nrespondents for the purpose of reducing respondent burden. The data produced\nare missing completely at random (MCAR), and special missing data techniques\nshould be used in linear regression and other multivariate statistical\nanalysis. We derive asymptotic variances of regression parameter estimates that\nallow us to conduct power analysis for linear regression models fit to the data\nobtained via a multiple matrix sampling design. The ideas are demonstrated with\na variation of the Big Five Inventory of psychological traits. An exploration\nof the regression parameter space demonstrates instability of the sample size\nrequirements, and substantial losses of precision with matrix-sampled\nregressors. A simulation with non-normal data demonstrates the advantages of a\nsemi-parametric multiple imputation scheme.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 12:36:17 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Kolenikov", "Stanislav", ""], ["Hammer", "Heather", ""]]}, {"id": "1710.00253", "submitter": "Gyorgy Terdik DR", "authors": "S. Rao Jammalamadaka and Gyorgy Terdik", "title": "Harmonic analysis and distribution-free inference for spherical\n  distributions", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier analysis and representation of circular distributions in terms of\ntheir Fourier coefficients, is quite commonly discussed and used for model-free\ninference such as testing uniformity and symmetry etc. in dealing with\n2-dimensional directions. However a similar discussion for spherical\ndistributions, which are used to model 3-dimensional directional data, has not\nbeen fully developed in the literature in terms of their harmonics. This paper,\nin what we believe is the first such attempt, looks at the probability\ndistributions on a unit sphere, through the perspective of spherical harmonics,\nanalogous to the Fourier analysis for distributions on a unit circle. Harmonic\nrepresentations of many currently used spherical models are presented and\ndiscussed. A very general family of spherical distributions is then introduced,\nspecial cases of which yield many known spherical models. Through the prism of\nharmonic analysis, one can look at the mean direction, dispersion, and various\nforms of symmetry for these models in a generic setting. Aspects of\ndistribution free inference such as estimation and large-sample tests for these\nsymmetries, are provided. The paper concludes with a real-data example\nanalyzing the longitudinal sunspot activity.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 19:40:40 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 15:39:41 GMT"}, {"version": "v3", "created": "Sat, 24 Feb 2018 08:21:59 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jammalamadaka", "S. Rao", ""], ["Terdik", "Gyorgy", ""]]}, {"id": "1710.00473", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Youngjun Choe", "title": "Importance Sampling and its Optimality for Stochastic Simulation Models", "comments": "37 pages, 6 figures, 2 tables. Accepted to the Electronic Journal of\n  Statistics", "journal-ref": null, "doi": "10.1214/19-EJS1604", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an expected outcome from a stochastic\nsimulation model. Our goal is to develop a theoretical framework on importance\nsampling for such estimation. By investigating the variance of an importance\nsampling estimator, we propose a two-stage procedure that involves a regression\nstage and a sampling stage to construct the final estimator. We introduce a\nparametric and a nonparametric regression estimator in the first stage and\nstudy how the allocation between the two stages affects the performance of the\nfinal estimator. We analyze the variance reduction rates and derive oracle\nproperties of both methods. We evaluate the empirical performances of the\nmethods using two numerical examples and a case study on wind turbine\nreliability evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 03:53:30 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 18:56:25 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Choe", "Youngjun", ""]]}, {"id": "1710.00479", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Permutation methods for factor analysis and PCA", "comments": "To appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often have datasets measuring features $x_{ij}$ of samples, such\nas test scores of students. In factor analysis and PCA, these features are\nthought to be influenced by unobserved factors, such as skills. Can we\ndetermine how many components affect the data? This is an important problem,\nbecause it has a large impact on all downstream data analysis. Consequently,\nmany approaches have been developed to address it. Parallel Analysis is a\npopular permutation method. It works by randomly scrambling each feature of the\ndata. It selects components if their singular values are larger than those of\nthe permuted data. Despite widespread use in leading textbooks and scientific\npublications, as well as empirical evidence for its accuracy, it currently has\nno theoretical justification.\n  In this paper, we show that the parallel analysis permutation method\nconsistently selects the large components in certain high-dimensional factor\nmodels. However, it does not select the smaller components. The intuition is\nthat permutations keep the noise invariant, while \"destroying\" the low-rank\nsignal. This provides justification for permutation methods in PCA and factor\nmodels under some conditions. Our work uncovers drawbacks of permutation\nmethods, and paves the way to improvements.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 04:29:22 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 15:44:39 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 13:25:11 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "1710.00499", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Fanny Yang, Martin J. Wainwright, Michael I. Jordan", "title": "Online control of the false discovery rate with decaying memory", "comments": "20 pages, 4 figures. Published in the proceedings of NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online multiple testing problem, p-values corresponding to different\nnull hypotheses are observed one by one, and the decision of whether or not to\nreject the current hypothesis must be made immediately, after which the next\np-value is observed. Alpha-investing algorithms to control the false discovery\nrate (FDR), formulated by Foster and Stine, have been generalized and applied\nto many settings, including quality-preserving databases in science and\nmultiple A/B or multi-armed bandit tests for internet commerce. This paper\nimproves the class of generalized alpha-investing algorithms (GAI) in four\nways: (a) we show how to uniformly improve the power of the entire class of\nmonotone GAI procedures by awarding more alpha-wealth for each rejection,\ngiving a win-win resolution to a recent dilemma raised by Javanmard and\nMontanari, (b) we demonstrate how to incorporate prior weights to indicate\ndomain knowledge of which hypotheses are likely to be non-null, (c) we allow\nfor differing penalties for false discoveries to indicate that some hypotheses\nmay be more important than others, (d) we define a new quantity called the\ndecaying memory false discovery rate (mem-FDR) that may be more meaningful for\ntruly temporal applications, and which alleviates problems that we describe and\nrefer to as \"piggybacking\" and \"alpha-death\". Our GAI++ algorithms incorporate\nall four generalizations simultaneously, and reduce to more powerful variants\nof earlier algorithms when the weights and decay are all set to unity. Finally,\nwe also describe a simple method to derive new online FDR rules based on an\nestimated false discovery proportion.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 06:13:37 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Yang", "Fanny", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1710.00596", "submitter": "Konstantinos Perrakis", "authors": "Konstantinos Perrakis, Sach Mukherjee and the Alzheimers Disease\n  Neuroimaging Initiative", "title": "Scalable Bayesian regression in high dimensions with multiple data\n  sources", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1624294", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of high-dimensional regression often involve multiple sources or\ntypes of covariates. We propose methodology for this setting, emphasizing the\n\"wide data\" regime with large total dimensionality p and sample size n<<p. We\nfocus on a flexible ridge-type prior with shrinkage levels that are specific to\neach data type or source and that are set automatically by empirical Bayes. All\nestimation, including setting of shrinkage levels, is formulated mainly in\nterms of inner product matrices of size n x n. This renders computation\nefficient in the wide data regime and allows scaling to problems with millions\nof features. Furthermore, the proposed procedures are free of user-set tuning\nparameters. We show how sparsity can be achieved by post-processing of the\nBayesian output via constrained minimization of a certain Kullback-Leibler\ndivergence. This yields sparse solutions with adaptive, source-specific\nshrinkage, including a closed-form variant that scales to very large p. We\npresent empirical results from a simulation study based on real data and a case\nstudy in Alzheimer's disease involving millions of features and multiple data\nsources.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 12:00:23 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 14:09:30 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 12:36:54 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 10:23:39 GMT"}, {"version": "v5", "created": "Fri, 19 Jan 2018 14:28:24 GMT"}, {"version": "v6", "created": "Wed, 5 Jun 2019 08:44:31 GMT"}, {"version": "v7", "created": "Wed, 21 Aug 2019 12:00:29 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Perrakis", "Konstantinos", ""], ["Mukherjee", "Sach", ""], ["Initiative", "the Alzheimers Disease Neuroimaging", ""]]}, {"id": "1710.00688", "submitter": "Dario Azzimonti", "authors": "Dario Azzimonti (IDSIA), David Ginsbourger (Idiap, IMSV), J\\'er\\'emy\n  Rohmer (BRGM), D\\'eborah Idier (BRGM)", "title": "Profile extrema for visualizing and quantifying uncertainties on\n  excursion regions. Application to coastal flooding", "comments": null, "journal-ref": "Technometrics, 2019", "doi": "10.1080/00401706.2018.1562987", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of describing excursion sets of a real-valued\nfunction $f$, i.e. the set of inputs where $f$ is above a fixed threshold. Such\nregions are hard to visualize if the input space dimension, $d$, is higher than\n2. For a given projection matrix from the input space to a lower dimensional\n(usually $1,2$) subspace, we introduce profile sup (inf) functions that\nassociate to each point in the projection's image the sup (inf) of the function\nconstrained over the pre-image of this point by the considered projection.\nPlots of profile extrema functions convey a simple, although intrinsically\npartial, visualization of the set. We consider expensive to evaluate functions\nwhere only a very limited number of evaluations, $n$, is available, e.g.\n$n<100d$, and we surrogate $f$ with a posterior quantity of a Gaussian process\n(GP) model. We first compute profile extrema functions for the posterior mean\ngiven $n$ evaluations of $f$. We quantify the uncertainty on such estimates by\nstudying the distribution of GP profile extrema with posterior\nquasi-realizations obtained from an approximating process. We control such\napproximation with a bound inherited from the Borell-TIS inequality. The\ntechnique is applied to analytical functions ($d=2,3$) and to a $5$-dimensional\ncoastal flooding test case for a site located on the Atlantic French coast.\nHere $f$ is a numerical model returning the area of flooded surface in the\ncoastal region given some offshore conditions. Profile extrema functions\nallowed us to better understand which offshore conditions impact large flooding\nevents.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:34:19 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:12:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Azzimonti", "Dario", "", "IDSIA"], ["Ginsbourger", "David", "", "Idiap, IMSV"], ["Rohmer", "J\u00e9r\u00e9my", "", "BRGM"], ["Idier", "D\u00e9borah", "", "BRGM"]]}, {"id": "1710.00720", "submitter": "Marco Geraci", "authors": "Marco Geraci and Alessandra Mattei", "title": "A novel quantile-based decomposition of the indirect effect in mediation\n  analysis with an application to infant mortality in the US population", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mediation analysis, the effect of an exposure (or treatment) on an outcome\nvariable is decomposed into two components: a direct effect, which pertains to\nan immediate influence of the exposure on the outcome, and an indirect effect,\nwhich the exposure exerts on the outcome through a third variable called\nmediator. Our motivating example concerns the relationship between maternal\nsmoking (the exposure, $X$), birthweight (the mediator, $M$), and infant\nmortality (the outcome, $Y$), which has attracted the interest of\nepidemiologists and statisticians for many years. We introduce new causal\nestimands, named $u$-specific direct and indirect effects, which describe the\ndirect and indirect effects of the exposure on the outcome at a specific\nquantile $u$ of the mediator, $0 < u < 1$. Under sequential ignorability we\nderive an interesting and novel decomposition of $u$-specific indirect effects.\nThe components of this decomposition have a straightforward interpretation and\ncan provide new insights into the complexity of the mechanisms underlying the\nindirect effect. We illustrate the proposed methods using data on infant\nmortality in the US population. We provide analytical evidence that supports\nthe hypothesis that the risk of sudden infant death syndrome is not predicted\nby changes in the birthweight distribution.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 15:18:41 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 12:08:35 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Geraci", "Marco", ""], ["Mattei", "Alessandra", ""]]}, {"id": "1710.00862", "submitter": "Chao Gao", "authors": "Chao Gao and John Lafferty", "title": "Testing for Global Network Structure Using Small Subgraph Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing for community structure in networks using\nrelations between the observed frequencies of small subgraphs. We propose a\nsimple test for the existence of communities based only on the frequencies of\nthree-node subgraphs. The test statistic is shown to be asymptotically normal\nunder a null assumption of no community structure, and to have power\napproaching one under a composite alternative hypothesis of a degree-corrected\nstochastic block model. We also derive a version of the test that applies to\nmultivariate Gaussian data. Our approach achieves near-optimal detection rates\nfor the presence of community structure, in regimes where the signal-to-noise\nis too weak to explicitly estimate the communities themselves, using existing\ncomputationally efficient algorithms. We demonstrate how the method can be\neffective for detecting structure in social networks, citation networks for\nscientific articles, and correlations of stock returns between companies on the\nS\\&P 500.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:39:20 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 04:57:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gao", "Chao", ""], ["Lafferty", "John", ""]]}, {"id": "1710.00894", "submitter": "Pariya Behrouzi", "authors": "P. Behrouzi and E.C. Wit", "title": "Detecting Epistatic Selection with Partially Observed Genotype Data\n  Using Copula Graphical Models", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.MN q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recombinant Inbred Lines derived from divergent parental lines can display\nextensive segregation distortion and long-range linkage disequilibrium (LD)\nbetween distant loci. These genomic signatures are consistent with epistatic\nselection during inbreeding. Epistatic interactions affect growth and fertility\ntraits or even cause complete lethality. Detecting epistasis is challenging as\nmultiple testing approaches are under-powered and true long-range LD is\ndifficult to distinguish from drift.\n  Here we develop a method for reconstructing an underlying network of genomic\nsignatures of high-dimensional epistatic selection from multi-locus genotype\ndata. The network captures the conditionally dependent short- and long-range LD\nstructure and thus reveals \"aberrant\" marker-marker associations that are due\nto epistatic selection rather than gametic linkage. The network estimation\nrelies on penalized Gaussian copula graphical models, which accounts for a\nlarge number of markers p and a small number of individuals n.\n  A multi-core implementation of our algorithm makes it feasible to estimate\nthe graph in high-dimensions also in the presence of significant portions of\nmissing data. We demonstrate the efficiency of the proposed method on simulated\ndatasets as well as on genotyping data in A.thaliana and maize. In addition, we\nimplemented the method in the R package netgwas which is freely available at\nhttps://CRAN.R-project.org/package=netgwas.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:18:51 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 01:21:34 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Behrouzi", "P.", ""], ["Wit", "E. C.", ""]]}, {"id": "1710.00915", "submitter": "Yanglei Song", "authors": "Yanglei Song and Georgios Fellouris", "title": "Change Acceleration and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel sequential change detection problem is proposed, in which the change\nshould be not only detected but also accelerated. Specifically, it is assumed\nthat the sequentially collected observations are responses to treatments\nselected in real time. The assigned treatments not only determine the\npre-change and post-change distributions of the responses, but also influence\nwhen the change happens. The problem is to find a treatment assignment rule and\na stopping rule that minimize the expected total number of observations subject\nto a user-specified bound on the false alarm probability. The optimal solution\nto this problem is obtained under a general Markovian change-point model.\nMoreover, an alternative procedure is proposed, whose applicability is not\nrestricted to Markovian change-point models and whose design requires minimal\ncomputation. For a large class of change-point models, the proposed procedure\nis shown to achieve the optimal performance in an asymptotic sense. Finally,\nits performance is found in two simulation studies to be close to the optimal,\nuniformly with respect to the error probability.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 21:23:18 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 13:48:55 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Song", "Yanglei", ""], ["Fellouris", "Georgios", ""]]}, {"id": "1710.00959", "submitter": "Yajuan Si", "authors": "Susanna Makela, Yajuan Si and Andrew Gelman", "title": "Bayesian Inference under Cluster Sampling with Probability Proportional\n  to Size", "comments": null, "journal-ref": null, "doi": "10.1002/sim.7892", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster sampling is common in survey practice, and the corresponding\ninference has been predominantly design-based. We develop a Bayesian framework\nfor cluster sampling and account for the design effect in the outcome modeling.\nWe consider a two-stage cluster sampling design where the clusters are first\nselected with probability proportional to cluster size, and then units are\nrandomly sampled inside selected clusters. Challenges arise when the sizes of\nnonsampled cluster are unknown. We propose nonparametric and parametric\nBayesian approaches for predicting the unknown cluster sizes, with this\ninference performed simultaneously with the model for survey outcome.\nSimulation studies show that the integrated Bayesian approach outperforms\nclassical methods with efficiency gains. We use Stan for computing and apply\nthe proposal to the Fragile Families and Child Wellbeing study as an\nillustration of complex survey inference in health surveys.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 02:06:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Makela", "Susanna", ""], ["Si", "Yajuan", ""], ["Gelman", "Andrew", ""]]}, {"id": "1710.00965", "submitter": "Shulei Wang", "authors": "Shulei Wang, Ellen T. Arena, Kevin W. Eliceiri, Ming Yuan", "title": "Automated and Robust Quantification of Colocalization in Dual-Color\n  Fluorescence Microscopy: A Nonparametric Statistical Approach", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2763821", "report-no": null, "categories": "stat.ME eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colocalization is a powerful tool to study the interactions between\nfluorescently labeled molecules in biological fluorescence microscopy. However,\nexisting techniques for colocalization analysis have not undergone continued\ndevelopment especially in regards to robust statistical support. In this paper,\nwe examine two of the most popular quantification techniques for colocalization\nand argue that they could be improved upon using ideas from nonparametric\nstatistics and scan statistics. In particular, we propose a new colocalization\nmetric that is robust, easily implementable, and optimal in a rigorous\nstatistical testing framework. Application to several benchmark datasets, as\nwell as biological examples, further demonstrates the usefulness of the\nproposed technique.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 02:45:15 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Wang", "Shulei", ""], ["Arena", "Ellen T.", ""], ["Eliceiri", "Kevin W.", ""], ["Yuan", "Ming", ""]]}, {"id": "1710.01011", "submitter": "Shahla Faisal", "authors": "Shahla Faisal, Gerhard Tutz", "title": "Nearest Neighbor Imputation for Categorical Data by Weighting of\n  Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values are a common phenomenon in all areas of applied research.\nWhile various imputation methods are available for metrically scaled variables,\nmethods for categorical data are scarce. An imputation method that has been\nshown to work well for high dimensional metrically scaled variables is the\nimputation by nearest neighbor methods. In this paper, we extend the weighted\nnearest neighbors approach to impute missing values in categorical variables.\nThe proposed method, called $\\mathtt{wNNSel_{cat}}$, explicitly uses the\ninformation on association among attributes. The performance of different\nimputation methods is compared in terms of the proportion of falsely imputed\nvalues. Simulation results show that the weighting of attributes yields smaller\nimputation errors than existing approaches. A variety of real data sets is used\nto support the results obtained by simulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 07:18:42 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Faisal", "Shahla", ""], ["Tutz", "Gerhard", ""]]}, {"id": "1710.01063", "submitter": "Pariya Behrouzi", "authors": "Pariya Behrouzi and Ernst C. Wit", "title": "De novo construction of polyploid linkage maps using discrete graphical\n  models", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN q-bio.MN q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linkage maps are used to identify the location of genes responsible for\ntraits and diseases. New sequencing techniques have created opportunities to\nsubstantially increase the density of genetic markers. Such revolutionary\nadvances in technology have given rise to new challenges, such as creating\nhigh-density linkage maps. Current multiple testing approaches based on\npairwise recombination fractions are underpowered in the high-dimensional\nsetting and do not extend easily to polyploid species. We propose to construct\nlinkage maps using graphical models either via a sparse Gaussian copula or a\nnonparanormal skeptic approach. Linkage groups (LGs), typically chromosomes,\nand the order of markers in each LG are determined by inferring the conditional\nindependence relationships among large numbers of markers in the genome.\nThrough simulations, we illustrate the utility of our map construction method\nand compare its performance with other available methods, both when the data\nare clean and contain no missing observations and when data contain genotyping\nerrors and are incomplete. We apply the proposed method to two genotype\ndatasets: barley and potato from diploid and polypoid populations,\nrespectively. Our comprehensive map construction method makes full use of the\ndosage SNP data to reconstruct linkage map for any bi-parental diploid and\npolyploid species. We have implemented the method in the R package netgwas.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 10:30:51 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 10:31:25 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 00:52:29 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 14:32:47 GMT"}, {"version": "v5", "created": "Mon, 2 Apr 2018 22:36:52 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Behrouzi", "Pariya", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1710.01085", "submitter": "Florent Guinot", "authors": "Florent Guinot, Marie Szafranski, Christophe Ambroise (LaMME), Franck\n  Samson (MIG)", "title": "Learning the optimal scale for GWAS through hierarchical SNP aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Genome-Wide Association Studies (GWAS) seek to identify causal\ngenomic variants associated with rare human diseases. The classical statistical\napproach for detecting these variants is based on univariate hypothesis\ntesting, with healthy individuals being tested against affected individuals at\neach locus. Given that an individual's genotype is characterized by up to one\nmillion SNPs, this approach lacks precision, since it may yield a large number\nof false positives that can lead to erroneous conclusions about genetic\nassociations with the disease. One way to improve the detection of true genetic\nassociations is to reduce the number of hypotheses to be tested by grouping\nSNPs. Results: We propose a dimension-reduction approach which can be applied\nin the context of GWAS by making use of the haplotype structure of the human\ngenome. We compare our method with standard univariate and multivariate\napproaches on both synthetic and real GWAS data, and we show that reducing the\ndimension of the predictor matrix by aggregating SNPs gives a greater precision\nin the detection of associations between the phenotype and genomic regions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:36:35 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 14:15:56 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 13:20:15 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Guinot", "Florent", "", "LaMME"], ["Szafranski", "Marie", "", "LaMME"], ["Ambroise", "Christophe", "", "LaMME"], ["Samson", "Franck", "", "MIG"]]}, {"id": "1710.01146", "submitter": "Dominic Edelmann", "authors": "Dominic Edelmann, Konstantinos Fokianos and Maria Pitsillou", "title": "An Updated Literature Review of Distance Correlation and its\n  Applications to Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of distance covariance/correlation was introduced recently to\ncharacterize dependence among vectors of random variables. We review some\nstatistical aspects of distance covariance/correlation function and we\ndemonstrate its applicability to time series analysis. We will see that the\nauto-distance covariance/correlation function is able to identify nonlinear\nrelationships and can be employed for testing the i.i.d.\\ hypothesis.\nComparisons with other measures of dependence are included.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:38:47 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 16:03:25 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Edelmann", "Dominic", ""], ["Fokianos", "Konstantinos", ""], ["Pitsillou", "Maria", ""]]}, {"id": "1710.01285", "submitter": "Ian Fellows", "authors": "Ian E. Fellows", "title": "Analysis of Large Scale Web Experiments Using Sequences of Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Experimental testing is vital in the optimization of web applications, and as\nsuch A/B testing has been widely adopted as a methodology for determining\noptimal content for many web applications. While some testing platforms provide\nsequentially valid inferences, a large proportion of online tests still utilize\ntraditional statistical tests that do not allow for interim \"peeking\" at the\ndata or extending the test past its proposed sample size.\n  In this paper we develop results useful for the sequential analysis of large\nscale experiments. In particular, the properties of sequences of maximum\nlikelihood and generalized method of moments estimators are examined. This\nleads to new tests of odds ratios and relative risks for binary outcomes. For\ncontinuous and ordinal outcome we develop a test of mean difference and a\nnon-parametric test of Area Under the Curve (AUC). Additionally, multivariate\nversions of these tests are proposed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:26:14 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Fellows", "Ian E.", ""]]}, {"id": "1710.01490", "submitter": "Mohamed Laib", "authors": "Mohamed Laib, Jean Golay, Luciano Telesca, Mikhail Kanevski", "title": "Multifractal analysis of the time series of daily means of wind speed in\n  complex regions", "comments": null, "journal-ref": null, "doi": "10.1016/j.chaos.2018.02.024", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we applied the multifractal detrended fluctuation analysis to\nthe daily means of wind speed measured by 119 weather stations distributed over\nthe territory of Switzerland. The analysis was focused on the inner time\nfluctuations of wind speed, which could be more linked with the local\nconditions of the highly varying topography of Switzerland. Our findings point\nout to a persistent behaviour of all the measured wind speed series (indicated\nby a Hurst exponent significantly larger than 0.5), and to a high\nmultifractality degree indicating a relative dominance of the large\nfluctuations in the dynamics of wind speed, especially in the Swiss plateau,\nwhich is comprised between the Jura and Alp mountain ranges. The study\nrepresents a contribution to the understanding of the dynamical mechanisms of\nwind speed variability in mountainous regions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 07:46:53 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Laib", "Mohamed", ""], ["Golay", "Jean", ""], ["Telesca", "Luciano", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1710.01523", "submitter": "Jian Shi", "authors": "A'yunin Sofro, Jian Qing Shi and Chunzheng Cao", "title": "Regression Analysis for Multivariate Dependent Count Data Using\n  Convolved Gaussian Processes", "comments": "28 pages and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on Poisson regression analysis for dependent data has been developed\nrapidly in the last decade. One of difficult problems in a multivariate case is\nhow to construct a cross-correlation structure and at the meantime make sure\nthat the covariance matrix is positive definite. To address the issue, we\npropose to use convolved Gaussian process (CGP) in this paper. The approach\nprovides a semi-parametric model and offers a natural framework for modeling\ncommon mean structure and covariance structure simultaneously. The CGP enables\nthe model to define different covariance structure for each component of the\nresponse variables. This flexibility ensures the model to cope with data coming\nfrom different resources or having different data structures, and thus to\nprovide accurate estimation and prediction. In addition, the model is able to\naccommodate large-dimensional covariates. The definition of the model, the\ninference and the implementation, as well as its asymptotic properties, are\ndiscussed. Comprehensive numerical examples with both simulation studies and\nreal data are presented.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 10:00:19 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Sofro", "A'yunin", ""], ["Shi", "Jian Qing", ""], ["Cao", "Chunzheng", ""]]}, {"id": "1710.01619", "submitter": "Matthew Reimherr", "authors": "Hyun Bin Kang, Matthew Reimherr, Mark Shriver and Peter Claes", "title": "Manifold Data Analysis with Applications to High-Frequency 3D Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific areas are faced with the challenge of extracting information\nfrom large, complex, and highly structured data sets. A great deal of modern\nstatistical work focuses on developing tools for handling such data. This paper\npresents a new subfield of functional data analysis, FDA, which we call\nManifold Data Analysis, or MDA. MDA is concerned with the statistical analysis\nof samples where one or more variables measured on each unit is a manifold,\nthus resulting in as many manifolds as we have units. We propose a framework\nthat converts manifolds into functional objects, an efficient 2-step functional\nprincipal component method, and a manifold-on-scalar regression model. This\nwork is motivated by an anthropological application involving 3D facial imaging\ndata, which is discussed extensively throughout the paper. The proposed\nframework is used to understand how individual characteristics, such as age and\ngenetic ancestry, influence the shape of the human face.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 14:30:54 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Kang", "Hyun Bin", ""], ["Reimherr", "Matthew", ""], ["Shriver", "Mark", ""], ["Claes", "Peter", ""]]}, {"id": "1710.01702", "submitter": "Li Ma", "authors": "Jonathan Christensen and Li Ma", "title": "A Bayesian hierarchical model for related densities using Polya trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are used to share information between related\nsamples and obtain more accurate estimates of sample-level parameters, common\nstructure, and variation between samples. When the parameter of interest is the\ndistribution or density of a continuous variable, a hierarchical model for\ncontinuous distributions is required. A number of such models have been\ndescribed in the literature using extensions of the Dirichlet process and\nrelated processes, typically as a distribution on the parameters of a mixing\nkernel. We propose a new hierarchical model based on the P\\'olya tree, which\nallows direct modeling of densities and enjoys some computational advantages\nover the Dirichlet process. The P\\'olya tree also allows more flexible modeling\nof the variation between samples, providing more informed shrinkage and\npermitting posterior inference on the dispersion function, which quantifies the\nvariation among sample densities. We also show how the model can be extended to\ncluster samples in situations where the observed samples are believed to have\nbeen drawn from several latent populations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 17:11:54 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 14:57:10 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 20:51:00 GMT"}, {"version": "v4", "created": "Sat, 15 Jun 2019 21:22:41 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Christensen", "Jonathan", ""], ["Ma", "Li", ""]]}, {"id": "1710.01771", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Paul Gustafson", "title": "Conditional Equivalence Testing: an alternative remedy for publication\n  bias", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0195145", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a publication policy that incorporates conditional equivalence\ntesting (CET), a two-stage testing scheme in which standard NHST is followed\nconditionally by testing for equivalence. The idea of CET is carefully\nconsidered as it has the potential to address recent concerns about\nreproducibility and the limited publication of null results. In this paper we\ndetail the implementation of CET, investigate similarities with a Bayesian\ntesting scheme, and outline the basis for how a scientific journal could\nproceed to reduce publication bias while remaining relevant.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:19:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Campbell", "Harlan", ""], ["Gustafson", "Paul", ""]]}, {"id": "1710.01821", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, John Choi, Bijan Pesaran, Demba Ba, and Vahid Tarokh", "title": "Classification of Local Field Potentials using Gaussian Sequence Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem of classification of local field potentials (LFPs), recorded from\nthe prefrontal cortex of a macaque monkey, is considered. An adult macaque\nmonkey is trained to perform a memory-based saccade. The objective is to decode\nthe eye movement goals from the LFP collected during a memory period. The LFP\nclassification problem is modeled as that of classification of smooth functions\nembedded in Gaussian noise. It is then argued that using minimax function\nestimators as features would lead to consistent LFP classifiers. The theory of\nGaussian sequence models allows us to represent minimax estimators as finite\ndimensional objects. The LFP classifier resulting from this mathematical\nendeavor is a spectrum based technique, where Fourier series coefficients of\nthe LFP data, followed by appropriate shrinkage and thresholding, are used as\nfeatures in a linear discriminant classifier. The classifier is then applied to\nthe LFP data to achieve high decoding accuracy. The function classification\napproach taken in the paper also provides a systematic justification for using\nFourier series, with shrinkage and thresholding, as features for the problem,\nas opposed to using the power spectrum. It also suggests that phase information\nis crucial to the decision making.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 22:53:58 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 18:41:06 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 18:24:02 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 17:18:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Banerjee", "Taposh", ""], ["Choi", "John", ""], ["Pesaran", "Bijan", ""], ["Ba", "Demba", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1710.01894", "submitter": "Andrew Karl", "authors": "Andrew T. Karl, Yan Yang, Sharon L. Lohr", "title": "A Correlated Random Effects Model for Nonignorable Missing Data in\n  Value-Added Assessment of Teacher Effects", "comments": null, "journal-ref": "Journal of Educational and Behavioral Statistics, 2013, Vol 38,\n  Issue 6, pp. 577 - 603", "doi": "10.3102/1076998613494819", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value-added models have been widely used to assess the contributions of\nindividual teachers and schools to students' academic growth based on\nlongitudinal student achievement outcomes. There is concern, however, that\nignoring the presence of missing values, which are common in longitudinal\nstudies, can bias teachers' value-added scores. In this article, a flexible\ncorrelated random effects model is developed that jointly models the student\nresponses and the student missing data indicators. Both the student responses\nand the missing data mechanism depend on latent teacher effects as well as\nlatent student effects, and the correlation between the sets of random effects\nadjusts teachers' value-added scores for informative missing data. The methods\nare illustrated with data from calculus classes at a large public university\nand with data from an elementary school district.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 06:32:38 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Karl", "Andrew T.", ""], ["Yang", "Yan", ""], ["Lohr", "Sharon L.", ""]]}, {"id": "1710.01912", "submitter": "Nurgazy Sulaimanov", "authors": "Nurgazy Sulaimanov, Sunil Kumar, Fr\\'ed\\'eric Burdet, Mark Ibberson,\n  Marco Pagni, Heinz Koeppl", "title": "Inferring gene expression networks with hubs using a degree weighted\n  Lasso approach", "comments": "15 pages, 8 figures. Submitted to Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-scale gene networks contain regulatory genes called hubs that have\nmany interaction partners. These genes usually play an essential role in gene\nregulation and cellular processes. Despite recent advancements in\nhigh-throughput technology, inferring gene networks with hub genes from\nhigh-dimensional data still remains a challenging problem. Novel statistical\nnetwork inference methods are needed for efficient and accurate reconstruction\nof hub networks from high-dimensional data. To address this challenge we\npropose DW-Lasso, a degree weighted Lasso (least absolute shrinkage and\nselection operator) method which infers gene networks with hubs efficiently\nunder the low sample size setting. Our network reconstruction approach is\nformulated as a two stage procedure: first, the degree of networks is estimated\niteratively, and second, the gene regulatory network is reconstructed using\ndegree information. A useful property of the proposed method is that it\nnaturally favors the accumulation of neighbors around hub genes and thereby\nhelps in accurate modeling of the high-throughput data under the assumption\nthat the underlying network exhibits hub structure. In a simulation study, we\ndemonstrate good predictive performance of the proposed method in comparison to\ntraditional Lasso type methods in inferring hub and scale-free graphs. We show\nthe effectiveness of our method in an application to microarray data of\n\\textit{E.coli} and RNA sequencing data of Kidney Clear Cell Carcinoma from The\nCancer Genome Atlas datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 08:30:12 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Sulaimanov", "Nurgazy", ""], ["Kumar", "Sunil", ""], ["Burdet", "Fr\u00e9d\u00e9ric", ""], ["Ibberson", "Mark", ""], ["Pagni", "Marco", ""], ["Koeppl", "Heinz", ""]]}, {"id": "1710.02011", "submitter": "Caleb Miles", "authors": "Caleb H. Miles, Ilya Shpitser, Phyllis Kanki, Seema Meloni, Eric J.\n  Tchetgen Tchetgen", "title": "On semiparametric estimation of a path-specific effect in the presence\n  of mediator-outcome confounding", "comments": "17 pages + 9 pages of supplementary material + 4 pages of references,\n  3 figures, 1 table. arXiv admin note: text overlap with arXiv:1411.6028", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path-specific effects are a broad class of mediated effects from an exposure\nto an outcome via one or more causal pathways with respect to some subset of\nintermediate variables. The majority of the literature concerning estimation of\nmediated effects has focused on parametric models with stringent assumptions\nregarding unmeasured confounding. We consider semiparametric inference of a\npath-specific effect when these assumptions are relaxed. In particular, we\ndevelop a suite of semiparametric estimators for the effect along a pathway\nthrough a mediator, but not some exposure-induced confounder of that mediator.\nThese estimators have different robustness properties, as each depends on\ndifferent parts of the observed data likelihood. One of our estimators may be\nviewed as combining the others, because it is locally semiparametric efficient\nand multiply robust. The latter property is illustrated in a simulation study.\nWe apply our methodology to an HIV study, in which we estimate the effect\ncomparing two drug treatments on a patient's average log CD4 count mediated by\nthe patient's level of adherence, but not by previous experience of toxicity,\nwhich is clearly affected by which treatment the patient is assigned to, and\nmay confound the effect of the patient's level of adherence on their virologic\noutcome.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 19:04:05 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Miles", "Caleb H.", ""], ["Shpitser", "Ilya", ""], ["Kanki", "Phyllis", ""], ["Meloni", "Seema", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1710.02078", "submitter": "Haslifah Hasim Dr", "authors": "Yong K. Goh, Haslifah M. Hasim, Chris G. Antonopoulos", "title": "Inference of forex and stock-index financial networks based on the\n  normalised mutual information rate", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0192160", "report-no": null, "categories": "stat.ME cs.IT math.DS math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study data from financial markets using an\ninformation-theory tool that we call the normalised Mutual Information Rate and\nshow how to use it to infer the underlying network structure of interrelations\nin foreign currency exchange rates and stock indices of 14 countries world-wide\nand the European Union. We first present the mathematical method and discuss\nabout its computational aspects, and then apply it to artificial data from\nchaotic dynamics and to correlated random variates. Next, we apply the method\nto infer the network structure of the financial data. Particularly, we study\nand reveal the interrelations among the various foreign currency exchange rates\nand stock indices in two separate networks for which we also perform an\nanalysis to identify their structural properties. Our results show that both\nare small-world networks sharing similar properties but also having distinct\ndifferences in terms of assortativity. Finally, the consistent relationships\ndepicted among the 15 economies are further supported by a discussion from the\neconomics view point.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 15:40:21 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Goh", "Yong K.", ""], ["Hasim", "Haslifah M.", ""], ["Antonopoulos", "Chris G.", ""]]}, {"id": "1710.02175", "submitter": "Daryl DeFord", "authors": "Daryl DeFord and Katherine Moore", "title": "Random Walk Null Models for Time Series Data", "comments": "21 pages, 12 figures, and 2 tables", "journal-ref": null, "doi": "10.3390/e19110615", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation entropy has become a standard tool for time series analysis that\nexploits the temporal properties of these data sets. Many current applications\nuse an approach based on Shannon entropy, which implicitly assumes an\nunderlying uniform distribution of patterns. In this paper, we analyze random\nwalk null models for time series and determine the corresponding permutation\ndistributions. These new techniques allow us to explicitly describe the\nbehavior of real world data in terms of more complex generative processes.\nAdditionally, building on recent results of Martinez, we define a validation\nmeasure that allows us to determine when a random walk is an appropriate model\nfor a time series. We demonstrate the usefulness of our methods using empirical\ndata drawn from a variety of fields.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 18:34:25 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["DeFord", "Daryl", ""], ["Moore", "Katherine", ""]]}, {"id": "1710.02223", "submitter": "Michael Crowther", "authors": "Michael J. Crowther", "title": "Extended multivariate generalised linear and non-linear mixed effects\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate data occurs in a wide range of fields, with ever more flexible\nmodel specifications being proposed, often within a multivariate generalised\nlinear mixed effects (MGLME) framework. In this article, we describe an\nextended framework, encompassing multiple outcomes of any type, each of which\ncould be repeatedly measured (longitudinal), with any number of levels, and\nwith any number of random effects at each level. Many standard distributions\nare described, as well as non-standard user-defined non-linear models. The\nextension focuses on a complex linear predictor for each outcome model,\nallowing sharing and linking between outcome models in an extremely flexible\nway, either by linking random effects directly, or the expected value of one\noutcome (or function of it) within the linear predictor of another. Non-linear\nand time-dependent effects are also seamlessly incorporated to the linear\npredictor through the use of splines or fractional polynomials. We further\npropose level-specific random effect distributions and numerical integration\ntechniques to improve usability, relaxing the normally distributed random\neffects assumption to allow multivariate $t$-distributed random effects. We\nconsider some special cases of the general framework, describing some new\nmodels in the fields of clustered survival data, joint longitudinal-survival\nmodels, and discuss various potential uses of the implementation. User\nfriendly, and easily extendable, software is provided.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 21:27:43 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Crowther", "Michael J.", ""]]}, {"id": "1710.02269", "submitter": "Simeng Qu", "authors": "Simeng Qu and Xiao Wang", "title": "Optimal Global Test for Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the optimal testing for the nullity of the slope function\nin the functional linear model using smoothing splines. We propose a\ngeneralized likelihood ratio test based on an easily implementable data-driven\nestimate. The quality of the test is measured by the minimal distance between\nthe null and the alternative set that still allows a possible test. The lower\nbound of the minimax decay rate of this distance is derived, and test with a\ndistance that decays faster than the lower bound would be impossible. We show\nthat the minimax optimal rate is jointly determined by the smoothing spline\nkernel and the covariance kernel. It is shown that our test attains this\noptimal rate. Simulations are carried out to confirm the finite-sample\nperformance of our test as well as to illustrate the theoretical results.\nFinally, we apply our test to study the effect of the trajectories of oxides of\nnitrogen ($\\text{NO}_{\\text{x}}$) on the level of ozone ($\\text{O}_3$).\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 03:46:38 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Qu", "Simeng", ""], ["Wang", "Xiao", ""]]}, {"id": "1710.02290", "submitter": "Yasin Asar", "authors": "Jibo Wu, Yasin Asar", "title": "On the stochastic restricted Liu-type maximum likelihood estimator in\n  logistic regression", "comments": "8 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to overcome multicollinearity, we propose a stochastic restricted\nLiu-type max- imum likelihood estimator by incorporating Liu-type maximum\nlikelihood estimator (Inan and Erdo- gan, 2013) to the logistic regression\nmodel when the linear restrictions are stochastic. We also discuss the\nproperties of the new estimator. Moreover, we give a method to choose the\nbiasing parameter in the new estimator. Finally, a simulation study is given to\nshow the performance of the new estimator.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 07:02:55 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Wu", "Jibo", ""], ["Asar", "Yasin", ""]]}, {"id": "1710.02333", "submitter": "Bruno Ebner", "authors": "Bruno Ebner, Norbert Henze, Michael A. Klatt and Klaus Mecke", "title": "Goodness-of-fit tests for complete spatial randomness based on Minkowski\n  functionals of binary images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of goodness-of-fit tests for complete spatial randomness\n(CSR). In contrast to standard tests, our procedure utilizes a transformation\nof the data to a binary image, which is then characterized by geometric\nfunctionals. Under a suitable limiting regime, we derive the asymptotic\ndistribution of the test statistics under the null hypothesis and almost sure\nlimits under certain alternatives. The new tests are computationally efficient,\nand simulations show that they are strong competitors to other tests of CSR.\nThe tests are applied to a real data set in gamma-ray astronomy, and immediate\nextensions are presented to encourage further work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:59:58 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""], ["Klatt", "Michael A.", ""], ["Mecke", "Klaus", ""]]}, {"id": "1710.02351", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "Approximating Bayes factors from minimal ANOVA summaries: An extension\n  of the BIC method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, I extend a method of Masson (2011) to develop an easy-to-use\nformula for performing Bayesian hypothesis tests from minimal ANOVA summaries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 11:12:57 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1710.02385", "submitter": "Timo Adam", "authors": "Timo Adam, Andreas Mayr, Thomas Kneib", "title": "Gradient boosting in Markov-switching generalized additive models for\n  location, scale and shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of flexible latent-state time series regression\nmodels which we call Markov-switching generalized additive models for location,\nscale and shape. In contrast to conventional Markov-switching regression\nmodels, the presented methodology allows us to model different state-dependent\nparameters of the response distribution - not only the mean, but also variance,\nskewness and kurtosis parameters - as potentially smooth functions of a given\nset of explanatory variables. In addition, the set of possible distributions\nthat can be specified for the response is not limited to the exponential family\nbut additionally includes, for instance, a variety of Box-Cox-transformed,\nzero-inflated and mixture distributions. We propose an estimation approach\nbased on the EM algorithm, where we use the gradient boosting framework to\nprevent overfitting while simultaneously performing variable selection. The\nfeasibility of the suggested approach is assessed in simulation experiments and\nillustrated in a real-data setting, where we model the conditional distribution\nof the daily average price of energy in Spain over time.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 13:05:19 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 12:28:45 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Adam", "Timo", ""], ["Mayr", "Andreas", ""], ["Kneib", "Thomas", ""]]}, {"id": "1710.02616", "submitter": "Tao Wang", "authors": "Tao Wang, Can Yang, Hongyu Zhao", "title": "Prediction analysis for microbiome sequencing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One primary goal of human microbiome studies is to predict host traits based\non human microbiota. However, microbial community sequencing data present\nsignificant challenges to the development of statistical methods. In\nparticular, the samples have different library sizes, the data contain many\nzeros and are often over-dispersed. To address these challenges, we introduce a\nnew statistical framework, called predictive analysis in metagenomics via\ninverse regression (PAMIR). An inverse regression model is developed for\nover-dispersed microbiota counts given the trait, and then a prediction rule is\nconstructed by taking advantage of the dimension-reduction structure in the\nmodel. An efficient Monte Carlo expectation-maximization algorithm is designed\nfor carrying out maximum likelihood estimation. We demonstrate the advantages\nof PAMIR through simulations and a real data example.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 01:25:54 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Wang", "Tao", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1710.02642", "submitter": "Haihui Shen", "authors": "Haihui Shen, L. Jeff Hong, Xiaowei Zhang", "title": "Ranking and Selection with Covariates for Personalized Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of ranking and selection via simulation in the context\nof personalized decision making, where the best alternative is not universal\nbut varies as a function of some observable covariates. The goal of ranking and\nselection with covariates (R&S-C) is to use simulation samples to obtain a\nselection policy that specifies the best alternative with certain statistical\nguarantee for subsequent individuals upon observing their covariates. A linear\nmodel is proposed to capture the relationship between the mean performance of\nan alternative and the covariates. Under the indifference-zone formulation, we\ndevelop two-stage procedures for both homoscedastic and heteroscedastic\nsimulation errors, respectively, and prove their statistical validity in terms\nof average probability of correct selection. We also generalize the well-known\nslippage configuration, and prove that the generalized slippage configuration\nis the least favorable configuration for our procedures. Extensive numerical\nexperiments are conducted to investigate the performance of the proposed\nprocedures, the experimental design issue, and the robustness to the linearity\nassumption. Finally, we demonstrate the usefulness of R&S-C via a case study of\nselecting the best treatment regimen in the prevention of esophageal cancer. We\nfind that by leveraging disease-related personal information, R&S-C can\nsubstantially improve patients' expected quality-adjusted life years by\nproviding patient-specific treatment regimen.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 06:58:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 09:23:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Shen", "Haihui", ""], ["Hong", "L. Jeff", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1710.02683", "submitter": "Michael Grayling", "authors": "Michael Grayling, Adrian Mander, James Wason", "title": "Blinded and unblinded sample size re-estimation procedures for\n  stepped-wedge cluster randomized trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately estimate the sample size required by a\nstepped-wedge (SW) cluster randomized trial (CRT) routinely depends upon the\nspecification of several nuisance parameters. If these parameters are\nmis-specified, the trial could be over-powered, leading to increased cost, or\nunder-powered, enhancing the likelihood of a false negative. We address this\nissue here for cross-sectional SW-CRTs, analyzed with a particular linear mixed\nmodel, by proposing methods for blinded and unblinded sample size re-estimation\n(SSRE). Blinded estimators for the variance parameters of a SW-CRT analyzed\nusing the Hussey and Hughes model are derived. Then, procedures for blinded and\nunblinded SSRE after any time period in a SW-CRT are detailed. The performance\nof these procedures is then examined and contrasted using two example trial\ndesign scenarios. We find that if the two key variance parameters were\nunder-specified by 50%, the SSRE procedures were able to increase power over\nthe conventional SW-CRT design by up to 29%, resulting in an empirical power\nabove the desired level. Moreover, the performance of the re-estimation\nprocedures was relatively insensitive to the timing of the interim assessment.\nThus, the considered SSRE procedures can bring substantial gains in power when\nthe underlying variance parameters are mis-specified. Though there are\npractical issues to consider, the procedure's performance means researchers\nshould consider incorporating SSRE in to future SW-CRTs.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 13:27:05 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Grayling", "Michael", ""], ["Mander", "Adrian", ""], ["Wason", "James", ""]]}, {"id": "1710.02704", "submitter": "Zemin Zheng", "authors": "Zemin Zheng, Jinchi Lv and Wei Lin", "title": "Nonsparse learning with latent variables", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a popular tool for producing meaningful and interpretable models,\nlarge-scale sparse learning works efficiently when the underlying structures\nare indeed or close to sparse. However, naively applying the existing\nregularization methods can result in misleading outcomes due to model\nmisspecification. In particular, the direct sparsity assumption on coefficient\nvectors has been questioned in real applications. Therefore, we consider\nnonsparse learning with the conditional sparsity structure that the coefficient\nvector becomes sparse after taking out the impacts of certain unobservable\nlatent variables. A new methodology of nonsparse learning with latent variables\n(NSL) is proposed to simultaneously recover the significant observable\npredictors and latent factors as well as their effects. We explore a common\nlatent family incorporating population principal components and derive the\nconvergence rates of both sample principal components and their score vectors\nthat hold for a wide class of distributions. With the properly estimated latent\nvariables, properties including model selection consistency and oracle\ninequalities under various prediction and estimation losses are established for\nthe proposed methodology. Our new methodology and results are evidenced by\nsimulation and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 16:14:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zheng", "Zemin", ""], ["Lv", "Jinchi", ""], ["Lin", "Wei", ""]]}, {"id": "1710.02773", "submitter": "Carter Butts", "authors": "Carter T. Butts", "title": "Baseline Mixture Models for Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous mixtures of distributions are widely employed in the statistical\nliterature as models for phenomena with highly divergent outcomes; in\nparticular, many familiar heavy-tailed distributions arise naturally as\nmixtures of light-tailed distributions (e.g., Gaussians), and play an important\nrole in applications as diverse as modeling of extreme values and robust\ninference. In the case of social networks, continuous mixtures of graph\ndistributions can likewise be employed to model social processes with\nheterogeneous outcomes, or as robust priors for network inference. Here, we\nintroduce some simple families of network models based on continuous mixtures\nof baseline distributions. While analytically and computationally tractable,\nthese models allow more flexible modeling of cross-graph heterogeneity than is\npossible with conventional baseline (e.g., Bernoulli or $U|man$ distributions).\nWe illustrate the utility of these baseline mixture models with application to\nproblems of multiple-network ERGMs, network evolution, and efficient network\ninference. Our results underscore the potential ubiquity of network processes\nwith nontrivial mixture behavior in natural settings, and raise some\npotentially disturbing questions regarding the adequacy of current network data\ncollection practices.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 03:00:24 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Butts", "Carter T.", ""]]}, {"id": "1710.02776", "submitter": "Lihua Lei", "authors": "Lihua Lei, Aaditya Ramdas, and William Fithian", "title": "STAR: A general interactive framework for FDR control under structural\n  constraints", "comments": "To appear in Biometrika", "journal-ref": null, "doi": "10.1093/biomet/asaa064", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework based on selectively traversed accumulation\nrules (STAR) for interactive multiple testing with generic structural\nconstraints on the rejection set. It combines accumulation tests from ordered\nmultiple testing with data-carving ideas from post-selection inference,\nallowing for highly flexible adaptation to generic structural information. Our\nprocedure defines an interactive protocol for gradually pruning a candidate\nrejection set, beginning with the set of all hypotheses and shrinking with each\nstep. By restricting the information at each step via a technique we call\nmasking, our protocol enables interaction while controlling the false discovery\nrate (FDR) in finite samples for any data-adaptive update rule that the analyst\nmay choose. We suggest update rules for a variety of applications with complex\nstructural constraints, show that STAR performs well for problems ranging from\nconvex region detection to FDR control on directed acyclic graphs, and show how\nto extend it to regression problems where knockoff statistics are available in\nlieu of $p$-values.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 04:00:56 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 16:46:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lei", "Lihua", ""], ["Ramdas", "Aaditya", ""], ["Fithian", "William", ""]]}, {"id": "1710.02864", "submitter": "Tonglin Zhang", "authors": "Tonglin Zhang and Jorge Mateu", "title": "Substationarity in Spatial Point Processes", "comments": "20 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the article is to develop the approach of substationarity to\nspatial point processes (SPPs). Substationarity is a new concept, which has\nnever been studied in the literature. It means that the distribution of SPPs\ncan only be invariant under location shifts within a linear subspace of the\ndomain. Theoretically, substationarity is a concept between stationariy and\nnonstationarity, but it belongs to nonstationarity. To formally propose the\napproach, the article provides the definition of substationarity and an\nestimation method for the first-order intensity function. As the linear\nsubspace may be unknown, it recommends using a parametric way to estimate the\nlinear subspace and a nonparametric way to estimate the first-order intensity\nfunction, indicating that it is a semiparametric approach. The simulation\nstudies show that both the estimators of the linear subspace and the\nfirst-order intensity function are reliable. In an application to a forest\nwildfire data set, the article concludes that substationarity of wildfire\noccurrences may be assumed along the longitude, indicating that latitude is a\nmore important factor than longitude in forest wildfire studies.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 17:53:52 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhang", "Tonglin", ""], ["Mateu", "Jorge", ""]]}, {"id": "1710.02931", "submitter": "Eric Lock", "authors": "Michael J. O'Connell and Eric F. Lock", "title": "Linked Matrix Factorization", "comments": "24 pages, 4 figures", "journal-ref": "Biometrics 75 (2): 582-592, 2019", "doi": "10.1111/biom.13010", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a number of methods have been developed for the dimension\nreduction and decomposition of multiple linked high-content data matrices.\nTypically these methods assume that just one dimension, rows or columns, is\nshared among the data sources. This shared dimension may represent common\nfeatures that are measured for different sample sets (i.e., horizontal\nintegration) or a common set of samples with measurements for different feature\nsets (i.e., vertical integration). In this article we introduce an approach for\nsimultaneous horizontal and vertical integration, termed Linked Matrix\nFactorization (LMF), for the more general situation where some matrices share\nrows (e.g., features) and some share columns (e.g., samples). Our motivating\napplication is a cytotoxicity study with accompanying genomic and molecular\nchemical attribute data. In this data set, the toxicity matrix (cell lines\n$\\times$ chemicals) shares its sample set with a genotype matrix (cell lines\n$\\times$ SNPs), and shares its feature set with a chemical molecular attribute\nmatrix (chemicals $\\times$ attributes). LMF gives a unified low-rank\nfactorization of these three matrices, which allows for the decomposition of\nsystematic variation that is shared among the three matrices and systematic\nvariation that is specific to each matrix. This may be used for efficient\ndimension reduction, exploratory visualization, and the imputation of missing\ndata even when entire rows or columns are missing from a constituent data\nmatrix. We present theoretical results concerning the uniqueness,\nidentifiability, and minimal parametrization of LMF, and evaluate it with\nextensive simulation studies.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 04:11:49 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["O'Connell", "Michael J.", ""], ["Lock", "Eric F.", ""]]}, {"id": "1710.02944", "submitter": "Zhongwen Liang", "authors": "Zhongwen Liang", "title": "A Unified Approach on the Local Power of Panel Unit Root Tests", "comments": "67 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a unified approach is proposed to derive the exact local\nasymptotic power for panel unit root tests, which is one of the most important\nissues in nonstationary panel data literature. Two most widely used panel unit\nroot tests known as Levin-Lin-Chu (LLC, Levin, Lin and Chu (2002)) and\nIm-Pesaran-Shin (IPS, Im, Pesaran and Shin (2003)) tests are systematically\nstudied for various situations to illustrate our method. Our approach is\ncharacteristic function based, and can be used directly in deriving the moments\nof the asymptotic distributions of these test statistics under the null and the\nlocal-to-unity alternatives. For the LLC test, the approach provides an\nalternative way to obtain the results that can be derived by the existing\nmethod. For the IPS test, the new results are obtained, which fills the gap in\nthe literature where few results exist, since the IPS test is non-admissible.\nMoreover, our approach has the advantage in deriving Edgeworth expansions of\nthese tests, which are also given in the paper. The simulations are presented\nto illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 05:57:06 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Liang", "Zhongwen", ""]]}, {"id": "1710.02954", "submitter": "Kirk Bansak", "authors": "Kirk Bansak", "title": "Estimating Causal Moderation Effects with Randomized Treatments and\n  Non-Randomized Moderators", "comments": "Forthcoming, Journal of the Royal Statistical Society: Series A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in analyzing conditional treatment effects.\nOne variant of this is \"causal moderation,\" which implies that intervention\nupon a third (moderator) variable would alter the treatment effect. This study\nconsiders the conditions under which causal moderation can be identified and\npresents a generalized framework for estimating causal moderation effects given\nrandomized treatments and non-randomized moderators. As part of the estimation\nprocess, it allows researchers to implement their preferred method of covariate\nadjustment, including parametric and non-parametric methods, or alternative\nidentification strategies of their choosing. In addition, it provides a set-up\nwhereby sensitivity analysis designed for the average-treatment-effect context\ncan be extended to the moderation context. To illustrate the methods, the study\npresents two applications: one dealing with the effect of using the term\n\"welfare\" to describe public assistance in the United States, and one dealing\nwith the effect of asylum seekers' religion on European attitudes toward asylum\nseekers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 06:34:01 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:27:03 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 07:54:34 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Bansak", "Kirk", ""]]}, {"id": "1710.03133", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi, David J Nott and Daniel E Pagendam", "title": "A Semi-Automatic Method for History Matching using Sequential Monte\n  Carlo", "comments": "Accepted by SIAM/ASA Journal on Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the history matching method is to locate non-implausible regions\nof the parameter space of complex deterministic or stochastic models by\nmatching model outputs with data. It does this via a series of waves where at\neach wave an emulator is fitted to a small number of training samples. An\nimplausibility measure is defined which takes into account the closeness of\nsimulated and observed outputs as well as emulator uncertainty. As the waves\nprogress, the emulator becomes more accurate so that training samples are more\nconcentrated on promising regions of the space and poorer parts of the space\nare rejected with more confidence. Whilst history matching has proved to be\nuseful, existing implementations are not fully automated and some ad-hoc\nchoices are made during the process, which involves user intervention and is\ntime consuming. This occurs especially when the non-implausible region becomes\nsmall and it is difficult to sample this space uniformly to generate new\ntraining points. In this article we develop a sequential Monte Carlo (SMC)\nalgorithm for implementing history matching that is semi-automated. Our novel\nSMC approach reveals that the history matching method yields a non-implausible\nregion that can be multi-modal, highly irregular and very difficult to sample\nuniformly. Our SMC approach offers a much more reliable sampling of the\nnon-implausible space, which requires additional computation compared to other\napproaches used in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:02:01 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 12:42:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Drovandi", "Christopher C", ""], ["Nott", "David J", ""], ["Pagendam", "Daniel E", ""]]}, {"id": "1710.03138", "submitter": "Jacob Spertus", "authors": "Jacob Spertus and Sharon-Lise Normand", "title": "Bayesian Propensity Scores for High-Dimensional Causal Inference: A\n  Comparison of Drug-Eluting to Bare-Metal Coronary Stents", "comments": "17 pages (without references/appendix), 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data can be useful for causal inference by providing many\nconfounders that may bolster the plausibility of the ignorability assumption.\nPropensity score methods are powerful tools for causal inference, are popular\nin health care research, and are particularly useful for high-dimensional data.\nRecent interest has surrounded a Bayesian formulation of these methods in order\nto flexibly estimate propensity scores and summarize posterior quantities while\nincorporating variance from the (potentially high-dimensional) treatment model.\nWe discuss methods for Bayesian propensity score analysis of binary treatments,\nfocusing on modern methods for high-dimensional Bayesian regression and the\npropagation of uncertainty from the treatment regression. We introduce a novel\nand simple estimator for the average treatment effect that capitalizes on\nconjugancy of the beta and binomial distributions. Through simulations, we show\nthe utility of horseshoe priors and Bayesian additive regression trees paired\nwith our new estimator, while demonstrating the importance of including\nvariance from the treatment and outcome models. Cardiac stent data with almost\n500 confounders and 9000 patients illustrate approaches and compare among\nexisting frequentist alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:07:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Spertus", "Jacob", ""], ["Normand", "Sharon-Lise", ""]]}, {"id": "1710.03206", "submitter": "Mickael Binois", "authors": "Mickael Binois and Jiangeng Huang and Robert B Gramacy and Mike\n  Ludkovski", "title": "Replication or exploration? Sequential design for stochastic simulation\n  experiments", "comments": "34 pages, 9 figures", "journal-ref": null, "doi": "10.1080/00401706.2018.1469433", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the merits of replication, and provide methods for optimal\ndesign (including replicates), with the goal of obtaining globally accurate\nemulation of noisy computer simulation experiments. We first show that\nreplication can be beneficial from both design and computational perspectives,\nin the context of Gaussian process surrogate modeling. We then develop a\nlookahead based sequential design scheme that can determine if a new run should\nbe at an existing input location (i.e., replicate) or at a new one (explore).\nWhen paired with a newly developed heteroskedastic Gaussian process model, our\ndynamic design scheme facilitates learning of signal and noise relationships\nwhich can vary throughout the input space. We show that it does so efficiently,\non both computational and statistical grounds. In addition to illustrative\nsynthetic examples, we demonstrate performance on two challenging real-data\nsimulation experiments, from inventory management and epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 17:38:04 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 14:58:59 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Binois", "Mickael", ""], ["Huang", "Jiangeng", ""], ["Gramacy", "Robert B", ""], ["Ludkovski", "Mike", ""]]}, {"id": "1710.03266", "submitter": "Yun Yang", "authors": "Yun Yang and Debdeep Pati and Anirban Bhattacharya", "title": "$\\alpha$-Variational Inference with Statistical Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a family of variational approximations to Bayesian posterior\ndistributions, called $\\alpha$-VB, with provable statistical guarantees. The\nstandard variational approximation is a special case of $\\alpha$-VB with\n$\\alpha=1$. When $\\alpha \\in(0,1]$, a novel class of variational inequalities\nare developed for linking the Bayes risk under the variational approximation to\nthe objective function in the variational optimization problem, implying that\nmaximizing the evidence lower bound in variational inference has the effect of\nminimizing the Bayes risk within the variational density family. Operating in a\nfrequentist setup, the variational inequalities imply that point estimates\nconstructed from the $\\alpha$-VB procedure converge at an optimal rate to the\ntrue parameter in a wide range of problems. We illustrate our general theory\nwith a number of examples, including the mean-field variational approximation\nto (low)-high-dimensional Bayesian linear regression with spike and slab\npriors, mixture of Gaussian models, latent Dirichlet allocation, and (mixture\nof) Gaussian variational approximation in regular parametric models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:10:14 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 20:00:48 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Yang", "Yun", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1710.03283", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri", "title": "On decomposable random graphs", "comments": "46 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decomposable graphs are known for their tedious and complicated Markov update\nsteps. Instead of modelling them directly, this work introduces a class of\ntree-dependent bipartite graphs that span the projective space of decomposable\ngraphs. This is achieved through dimensionality expansion that causes the graph\nnodes to be conditionally independent given a latent tree. The Markov update\nsteps are thus remarkably simplified. Structural modelling with tree-dependent\nbipartite graphs has additional benefits. For example, certain properties that\nare hardly attainable in the decomposable form are now easily accessible.\nMoreover, tree-dependent bipartite graphs can extract and model extra\ninformation related to sub-clustering dynamics, while currently known models\nfor decomposable graphs do not. Properties of decomposable graphs are also\ntransferable to the expanded dimension, such as the attractive likelihood\nfactorization property. As a result of using the bipartite representation,\ntools developed for random graphs can be used. Hence, a framework for random\ntree-dependent bipartite graphs, thereupon for random decomposable graphs, is\nproposed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:42:09 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Elmasri", "Mohamad", ""]]}, {"id": "1710.03294", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Michael D. Shields", "title": "The effect of prior probabilities on quantification and propagation of\n  imprecise probabilities resulting from small datasets", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.cma.2018.01.045", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a methodology for Bayesian multimodel uncertainty\nquantification (UQ) and propagation and presents an investigation into the\neffect of prior probabilities on the resulting uncertainties. The UQ\nmethodology is adapted from the information-theoretic method previously\npresented by the authors (Zhang and Shields, 2018) to a fully Bayesian\nconstruction that enables greater flexibility in quantifying uncertainty in\nprobability model form. Being Bayesian in nature and rooted in UQ from small\ndatasets, prior probabilities in both probability model form and model\nparameters are shown to have a significant impact on quantified uncertainties\nand, consequently, on the uncertainties propagated through a physics-based\nmodel. These effects are specifically investigated for a simplified plate\nbuckling problem with uncertainties in material properties derived from a small\nnumber of experiments using noninformative priors and priors derived from past\nstudies of varying appropriateness. It is illustrated that prior probabilities\ncan have a significant impact on multimodel UQ for small datasets and\ninappropriate (but seemingly reasonable) priors may even have lingering effects\nthat bias probabilities even for large datasets. When applied to uncertainty\npropagation, this may result in probability bounds on response quantities that\ndo not include the true probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 20:08:48 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 16:16:12 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Shields", "Michael D.", ""]]}, {"id": "1710.03355", "submitter": "Yue Zhao", "authors": "Yue Zhao", "title": "An Extension of Deep Pathway Analysis: A Pathway Route Analysis\n  Framework Incorporating Multi-dimensional Cancer Genomics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.OH stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in cancer research have come via the up-and-coming field\nof pathway analysis. By applying statistical methods to prior known gene and\nprotein regulatory information, pathway analysis provides a meaningful way to\ninterpret genomic data. While many gene/protein regulatory relationships have\nbeen studied, never before has such a significant amount data been made\navailable in organized forms of gene/protein regulatory networks and pathways.\nHowever, pathway analysis research is still in its infancy, especially when\napplying it to solve practical problems.\n  In this paper we propose a new method of studying biological pathways, one\nthat cross analyzes mutation information, transcriptome and proteomics data.\nUsing this outcome, we identify routes of aberrant pathways potentially\nresponsible for the etiology of disease. Each pathway route is encoded as a\nbayesian network which is initialized with a sequence of conditional\nprobabilities specifically designed to encode directionality of regulatory\nrelationships encoded in the pathways. Far more complex interactions, such as\nphosphorylation and methylation, among others, in the pathways can be modeled\nusing this approach. The effectiveness of our model is demonstrated through its\nability to distinguish real pathways from decoys on TCGA mRNA-seq, mutation,\nCopy Number Variation and phosphorylation data for both Breast cancer and\nOvarian cancer study. The majority of pathways distinguished can be confirmed\nby biological literature. Moreover, the proportion of correctly indentified\npathways is \\% higher than previous work where only mRNA-seq mutation data is\nincorporated for breast cancer patients. Consequently, such an in-depth pathway\nanalysis incorporating more diverse data can give rise to the accuracy of\nperturbed pathway detection.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 00:04:04 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhao", "Yue", ""]]}, {"id": "1710.03410", "submitter": "James Johndrow", "authors": "David Goldberg and James E. Johndrow", "title": "A Decision Theoretic Approach to A/B Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing is ubiquitous within the machine learning and data science\noperations of internet companies. Generically, the idea is to perform a\nstatistical test of the hypothesis that a new feature is better than the\nexisting platform---for example, it results in higher revenue. If the p value\nfor the test is below some pre-defined threshold---often, 0.05---the new\nfeature is implemented. The difficulty of choosing an appropriate threshold has\nbeen noted before, particularly because dependent tests are often done\nsequentially, leading some to propose control of the false discovery rate (FDR)\nrather than use of a single, universal threshold. However, it is still\nnecessary to make an arbitrary choice of the level at which to control FDR.\nHere we suggest a decision-theoretic approach to determining whether to adopt a\nnew feature, which enables automated selection of an appropriate threshold. Our\nmethod has the basic ingredients of any decision-theory problem: a loss\nfunction, action space, and a notion of optimality, for which we choose Bayes\nrisk. However, the loss function and the action space differ from the typical\nchoices made in the literature, which has focused on the theory of point\nestimation. We give some basic results for Bayes-optimal thresholding rules for\nthe feature adoption decision, and give some examples using eBay data. The\nresults suggest that the 0.05 p-value threshold may be too conservative in some\nsettings, but that its widespread use may reflect an ad-hoc means of\ncontrolling multiplicity in the common case of repeatedly testing variants of\nan experiment when the threshold is not reached.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 05:59:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Goldberg", "David", ""], ["Johndrow", "James E.", ""]]}, {"id": "1710.03453", "submitter": "Mauro Bernardi", "authors": "Mauro Bernardi and Lea Petrella and Paola Stolfi", "title": "The Sparse Multivariate Method of Simulated Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the method of simulated quantiles (MSQ) of Dominicy and Veredas\n(2013) and Dominick et al. (2013) is extended to a general multivariate\nframework (MMSQ) and to provide a sparse estimator of the scale matrix\n(sparse-MMSQ). The MSQ, like alternative likelihood-free procedures, is based\non the minimisation of the distance between appropriate statistics evaluated on\nthe true and synthetic data simulated from the postulated model. Those\nstatistics are functions of the quantiles providing an effective way to deal\nwith distributions that do not admit moments of any order like the\n$\\alpha$-Stable or the Tukey lambda distribution. The lack of a natural\nordering represents the major challenge for the extension of the method to the\nmultivariate framework. Here, we rely on the notion of projectional quantile\nrecently introduced by Hallin etal. (2010) and Kong Mizera (2012). We establish\nconsistency and asymptotic normality of the proposed estimator. The smoothly\nclipped absolute deviation (SCAD) $\\ell_1$--penalty of Fan and Li (2001) is\nthen introduced into the MMSQ objective function in order to achieve sparse\nestimation of the scaling matrix which is the major responsible for the curse\nof dimensionality problem. We extend the asymptotic theory and we show that the\nsparse-MMSQ estimator enjoys the oracle properties under mild regularity\nconditions. The method is illustrated and its effectiveness is tested using\nseveral synthetic datasets simulated from the Elliptical Stable distribution\n(ESD) for which alternative methods are recognised to perform poorly. The\nmethod is then applied to build a new network-based systemic risk measurement\nframework. The proposed methodology to build the network relies on a new\nsystemic risk measure and on a parametric test of statistical dominance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:50:40 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Bernardi", "Mauro", ""], ["Petrella", "Lea", ""], ["Stolfi", "Paola", ""]]}, {"id": "1710.03490", "submitter": "Michael Grayling", "authors": "Michael Grayling, James Wason, Adrian Mander", "title": "An optimised multi-arm multi-stage clinical trial design for unknown\n  variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-arm multi-stage trial designs can bring notable gains in efficiency to\nthe drug development process. However, for normally distributed endpoints, the\ndetermination of a design typically depends on the assumption that the patient\nvariance in response is known. In practice, this will not usually be the case.\nTo allow for unknown variance, previous research explored the performance of\nt-test statistics, coupled with a quantile substitution procedure for modifying\nthe stopping boundaries, at controlling the familywise error-rate to the\nnominal level. Here, we discuss an alternative method based on Monte Carlo\nsimulation that allows the group size and stopping boundaries of a multi-arm\nmulti-stage t-test to be optimised according to some nominated optimality\ncriteria. We consider several examples, provide R code for general\nimplementation, and show that our designs confer a familywise error-rate and\npower close to the desired level. Consequently, this methodology will provide\nutility in future multi-arm multi-stage trials.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 10:09:42 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Grayling", "Michael", ""], ["Wason", "James", ""], ["Mander", "Adrian", ""]]}, {"id": "1710.03492", "submitter": "Michael Grayling", "authors": "Michael Grayling, James Wason, Adrian Mander", "title": "Group Sequential Crossover Trial Designs with Strong Control of the\n  Familywise Error Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crossover designs are an extremely useful tool to investigators, whilst group\nsequential methods have proven highly proficient at improving the efficiency of\nparallel group trials. Yet, group sequential methods and crossover designs have\nrarely been paired together. One possible explanation for this could be the\nabsence of a formal proof of how to strongly control the familywise error rate\nin the case when multiple comparisons will be made. Here, we provide this\nproof, valid for any number of initial experimental treatments and any number\nof stages, when results are analysed using a linear mixed model. We then\nestablish formulae for the expected sample size and expected number of\nobservations of such a trial, given any choice of stopping boundaries. Finally,\nutilising the four-treatment, four-period TOMADO trial as an example, we\ndemonstrate group sequential methods in this setting could have reduced the\ntrials expected number of observations under the global null hypothesis by over\n33%.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 10:14:15 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Grayling", "Michael", ""], ["Wason", "James", ""], ["Mander", "Adrian", ""]]}, {"id": "1710.03496", "submitter": "Michael Grayling", "authors": "Michael Grayling, Adrian Mander, James Wason", "title": "Admissible multi-arm stepped-wedge cluster randomized trial designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous publications have now addressed the principles of designing,\nanalyzing, and reporting the results of, stepped-wedge cluster randomized\ntrials. In contrast, there is little research available pertaining to the\ndesign and analysis of multi-arm stepped-wedge cluster randomized trials,\nutilized to evaluate the effectiveness of multiple experimental interventions.\nIn this paper, we address this by explaining how the required sample size in\nthese multi-arm trials can be ascertained when data are to be analyzed using a\nlinear mixed model. We then go on to describe how the design of such trials can\nbe optimized to balance between minimizing the cost of the trial, and\nminimizing some function of the covariance matrix of the treatment effect\nestimates. Using a recently commenced trial that will evaluate the\neffectiveness of sensor monitoring in an occupational therapy rehabilitation\nprogram for older persons after hip fracture as an example, we demonstrate that\nour designs could reduce the number of observations required for a fixed power\nlevel by up to 58%. Consequently, when logistical constraints permit the\nutilization of any one of a range of possible multi-arm stepped-wedge cluster\nrandomized trial designs, researchers should consider employing our approach to\noptimize their trials efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 10:21:37 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 14:32:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Grayling", "Michael", ""], ["Mander", "Adrian", ""], ["Wason", "James", ""]]}, {"id": "1710.03519", "submitter": "Markus Bibinger", "authors": "Markus Bibinger and Mathias Trabs", "title": "Volatility estimation for stochastic PDEs using high-frequency\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the parameter estimation for parabolic, linear, second-order,\nstochastic partial differential equations (SPDEs) observing a mild solution on\na discrete grid in time and space. A high-frequency regime is considered where\nthe mesh of the grid in the time variable goes to zero. Focusing on volatility\nestimation, we provide an explicit and easy to implement method of moments\nestimator based on squared increments. The estimator is consistent and admits a\ncentral limit theorem. This is established moreover for the joint estimation of\nthe integrated volatility and parameters in the differential operator in a\nsemi-parametric framework. Starting from a representation of the solution of\nthe SPDE with Dirichlet boundary conditions as an infinite factor model and\nexploiting mixing-type properties of time series, the theory considerably\ndiffers from the statistics for semi-martingales literature. The performance of\nthe method is illustrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 11:33:19 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 13:38:41 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 06:18:33 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Bibinger", "Markus", ""], ["Trabs", "Mathias", ""]]}, {"id": "1710.03551", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli", "title": "Exact integrated completed likelihood maximisation in a stochastic block\n  transition model for dynamic networks", "comments": "23 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent stochastic block model is a flexible and widely used statistical\nmodel for the analysis of network data. Extensions of this model to a dynamic\ncontext often fail to capture the persistence of edges in contiguous network\nsnapshots. The recently introduced stochastic block transition model addresses\nprecisely this issue, by modelling the probabilities of creating a new edge and\nof maintaining an edge over time. Using a model-based clustering approach, this\npaper illustrates a methodology to fit stochastic block transition models under\na Bayesian framework. The method relies on a greedy optimisation procedure to\nmaximise the exact integrated completed likelihood. The computational\nefficiency of the algorithm used makes the methodology scalable and appropriate\nfor the analysis of large network datasets. Crucially, the optimal number of\nlatent groups is automatically selected at no additional computing cost. The\nefficacy of the method is demonstrated through applications to both artificial\nand real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 12:59:29 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 10:05:08 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Rastelli", "Riccardo", ""]]}, {"id": "1710.03892", "submitter": "Tianzhou Ma", "authors": "Tianzhou Ma, Zhao Ren and George C. Tseng", "title": "Variable screening with multiple studies", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancement in technology has generated abundant high-dimensional data that\nallows integration of multiple relevant studies. Due to their huge\ncomputational advantage, variable screening methods based on marginal\ncorrelation have become promising alternatives to the popular regularization\nmethods for variable selection. However, all these screening methods are\nlimited to single study so far. In this paper, we consider a general framework\nfor variable screening with multiple related studies, and further propose a\nnovel two-step screening procedure using a self-normalized estimator for\nhigh-dimensional regression analysis in this framework. Compared to the\none-step procedure and rank-based sure independence screening (SIS) procedure,\nour procedure greatly reduces false negative errors while keeping a low false\npositive rate. Theoretically, we show that our procedure possesses the sure\nscreening property with weaker assumptions on signal strengths and allows the\nnumber of features to grow at an exponential rate of the sample size. In\naddition, we relax the commonly used normality assumption and allow\nsub-Gaussian distributions. Simulations and a real transcriptomic application\nillustrate the advantage of our method as compared to the rank-based SIS\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 03:12:32 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Ma", "Tianzhou", ""], ["Ren", "Zhao", ""], ["Tseng", "George C.", ""]]}, {"id": "1710.03897", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu", "title": "On similarity of the sample depth contours", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the similarity property of the sample\nprojection depth contours. It turns out that some of these contours are of\n\\emph{the same shape} with different sizes, following a similar fashion to the\nMahalanobis depth contours. One advantage of this investigation is the\npotential of bringing convenience to the computation of the projection depth\ncontours; the other one is that we may utilize this idea to extend both the\nhalfspace depth and zonoid depth to versions that do not vanish outside the\nconvex hull of the data cloud, aiming at overcoming the so-called `outside\nproblem'. Examples are also provided to illustrate the main results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 04:04:26 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Liu", "Xiaohui", ""]]}, {"id": "1710.03904", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu and Yuanyuan Li", "title": "General notions of regression depth function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a measure for the centrality of a point in a set of multivariate data,\nstatistical depth functions play important roles in multivariate analysis,\nbecause one may conveniently construct descriptive as well as inferential\nprocedures relying on them. Many depth notions have been proposed in the\nliterature to fit to different applications. However, most of them are mainly\ndeveloped for the location setting. In this paper, we discuss the possibility\nof extending some of them into the regression setting. A general concept of\nregression depth function is also provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 04:24:27 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Liu", "Xiaohui", ""], ["Li", "Yuanyuan", ""]]}, {"id": "1710.03953", "submitter": "Bilal Khan", "authors": "Bilal Khan, Hsuan-Wei Lee, Ian Fellows, Kirk Dombrowski", "title": "One-step Estimation of Networked Population Size: Respondent-Driven\n  Capture-Recapture with Anonymity", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0195959", "report-no": null, "categories": "cs.SI math.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimates for hidden and hard-to-reach populations are\nparticularly important when members are known to suffer from disproportion\nhealth issues or to pose health risks to the larger ambient population in which\nthey are embedded. Efforts to derive size estimates are often frustrated by a\nrange of factors that preclude conventional survey strategies, including social\nstigma associated with group membership or members' involvement in illegal\nactivities.\n  This paper extends prior research on the problem of network population size\nestimation, building on established survey/sampling methodologies commonly used\nwith hard-to-reach groups. Three novel one-step, network-based population size\nestimators are presented, to be used in the context of uniform random sampling,\nrespondent-driven sampling, and when networks exhibit significant clustering\neffects. Provably sufficient conditions for the consistency of these estimators\n(in large configuration networks) are given. Simulation experiments across a\nwide range of synthetic network topologies validate the performance of the\nestimators, which are seen to perform well on a real-world location-based\nsocial networking data set with significant clustering. Finally, the proposed\nschemes are extended to allow them to be used in settings where participant\nanonymity is required. Systematic experiments show favorable tradeoffs between\nanonymity guarantees and estimator performance.\n  Taken together, we demonstrate that reasonable population estimates can be\nderived from anonymous respondent driven samples of 250-750 individuals, within\nambient populations of 5,000-40,000. The method thus represents a novel and\ncost-effective means for health planners and those agencies concerned with\nhealth and disease surveillance to estimate the size of hidden populations.\nLimitations and future work are discussed in the concluding section.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 08:21:00 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Khan", "Bilal", ""], ["Lee", "Hsuan-Wei", ""], ["Fellows", "Ian", ""], ["Dombrowski", "Kirk", ""]]}, {"id": "1710.04030", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Zhiyong Zhou, Anders Garpebring, Jun Yu", "title": "Sparsity estimation in compressive sensing with application to MR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of compressive sensing (CS) asserts that an unknown signal\n$\\mathbf{x} \\in \\mathbb{C}^N$ can be accurately recovered from $m$ measurements\nwith $m\\ll N$ provided that $\\mathbf{x}$ is sparse. Most of the recovery\nalgorithms need the sparsity $s=\\lVert\\mathbf{x}\\rVert_0$ as an input. However,\ngenerally $s$ is unknown, and directly estimating the sparsity has been an open\nproblem. In this study, an estimator of sparsity is proposed by using Bayesian\nhierarchical model. Its statistical properties such as unbiasedness and\nasymptotic normality are proved. In the simulation study and real data study,\nmagnetic resonance image data is used as input signal, which becomes sparse\nafter sparsified transformation. The results from the simulation study confirm\nthe theoretical properties of the estimator. In practice, the estimate from a\nreal MR image can be used for recovering future MR images under the framework\nof CS if they are believed to have the same sparsity level after\nsparsification.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:13:42 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Wang", "Jianfeng", ""], ["Zhou", "Zhiyong", ""], ["Garpebring", "Anders", ""], ["Yu", "Jun", ""]]}, {"id": "1710.04093", "submitter": "Florian Maire", "authors": "Aidan Boland, Nial Friel, Florian Maire", "title": "Efficient MCMC for Gibbs Random Fields using pre-computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference of Gibbs random fields (GRFs) is often referred to as a\ndoubly intractable problem, since the likelihood function is intractable. The\nexploration of the posterior distribution of such models is typically carried\nout with a sophisticated Markov chain Monte Carlo (MCMC) method, the exchange\nalgorithm (Murray et al., 2006), which requires simulations from the likelihood\nfunction at each iteration. The purpose of this paper is to consider an\napproach to dramatically reduce this computational overhead. To this end we\nintroduce a novel class of algorithms which use realizations of the GRF model,\nsimulated offline, at locations specified by a grid that spans the parameter\nspace. This strategy speeds up dramatically the posterior inference, as\nillustrated on several examples. However, using the pre-computed graphs\nintroduces a noise in the MCMC algorithm, which is no longer exact. We study\nthe theoretical behaviour of the resulting approximate MCMC algorithm and\nderive convergence bounds using a recent theoretical development on approximate\nMCMC methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:37:05 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 09:49:40 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Boland", "Aidan", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1710.04105", "submitter": "Yetkin Tua\\c{c}", "authors": "Yetkin Tua\\c{c} and Olcay Arslan", "title": "Variable Selection in Restricted Linear Regression Models", "comments": "12 pages, 4 figures 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of prior information in the linear regression is well known to\nprovide more efficient estimators of regression coefficients. The methods of\nnon-stochastic restricted regression estimation proposed by Theil and\nGoldberger (1961) are preferred when prior information is available. In this\nstudy, we will consider parameter estimation and the variable selection in\nnon-stochastic restricted linear regression model, using least absolute\nshrinkage and selection operator (LASSO) method introduced by Tibshirani\n(1996). A small simulation study and real data example are provided to\nillustrate the performance of the proposed method for dealing with the variable\nselection and the parameter estimation in restricted linear regression models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:10:06 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Tua\u00e7", "Yetkin", ""], ["Arslan", "Olcay", ""]]}, {"id": "1710.04238", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Regression-aware decompositions", "comments": "19 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear least-squares regression with a \"design\" matrix A approximates a given\nmatrix B via minimization of the spectral- or Frobenius-norm discrepancy\n||AX-B|| over every conformingly sized matrix X. Another popular approximation\nis low-rank approximation via principal component analysis (PCA) -- which is\nessentially singular value decomposition (SVD) -- or interpolative\ndecomposition (ID). Classically, PCA/SVD and ID operate solely with the matrix\nB being approximated, not supervised by any auxiliary matrix A. However, linear\nleast-squares regression models can inform the ID, yielding regression-aware\nID. As a bonus, this provides an interpretation as regression-aware PCA for a\nkind of canonical correlation analysis between A and B. The regression-aware\ndecompositions effectively enable supervision to inform classical\ndimensionality reduction, which classically has been totally unsupervised. The\nregression-aware decompositions reveal the structure inherent in B that is\nrelevant to regression against A.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 18:06:25 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 22:11:18 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "1710.04379", "submitter": "Nilanjan Chatterjee", "authors": "Parichoy Pal Choudhury, Anil K. Chaturvedi, Nilanjan Chatterjee", "title": "Evaluating discriminatory accuracy of models using partial risk-scores\n  in two-phase studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior to clinical applications, it is critical that risk prediction models\nare evaluated in independent studies that did not contribute to model\ndevelopment. While prospective cohort studies provide a natural setting for\nmodel validation, they often ascertain information on some risk factors (e.g.,\nan expensive biomarker) in a nested sub-study of the original cohort, typically\nselected based on case-control status, and possibly some additional covariates.\nIn this article, we propose an efficient approach for evaluating discriminatory\nability of models using data from all individuals in a cohort study\nirrespective of whether they were sampled in the nested sub-study for measuring\nthe complete set of risk factors. For evaluation of the Area Under the Curve\n(AUC) statistics, we estimate probabilities of risk-scores for cases being\nlarger than those in controls conditional on partial risk-scores, the component\nof the risk-score that could be defined based on partial covariate information.\nThe use of partial risk-scores, as opposed to actual multivariate risk-factor\nprofiles, allows estimation of the underlying conditional expectations using\nsubjects with complete covariate information in a non-parametric fashion even\nwhen numerous covariates are involved. We propose an influence function based\napproach for estimation of the variance of the resulting AUC statistics. We\nevaluate finite sample performance of the proposed method and compare it to an\ninverse probability weighted (IPW) estimator through extensive simulation\nstudies. Finally, we illustrate an application of the proposed method for\nevaluating performance of a lung cancer risk prediction model using data from\nthe Prostate, Lung, Colorectal and Ovarian Cancer (PLCO) trial.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 06:21:37 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Choudhury", "Parichoy Pal", ""], ["Chaturvedi", "Anil K.", ""], ["Chatterjee", "Nilanjan", ""]]}, {"id": "1710.04382", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Dennis Prangle and Philip Maybank and Mark Bell", "title": "Marginal sequential Monte Carlo for doubly intractable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for models that have an intractable partition function is\nknown as a doubly intractable problem, where standard Monte Carlo methods are\nnot applicable. The past decade has seen the development of auxiliary variable\nMonte Carlo techniques (M{\\o}ller et al., 2006; Murray et al., 2006) for\ntackling this problem; these approaches being members of the more general class\nof pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and\nRoberts, 2009), which make use of unbiased estimates of intractable posteriors.\nEveritt et al. (2017) investigated the use of exact-approximate importance\nsampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems,\nbut focussed only on SMC algorithms that used data-point tempering. This paper\ndescribes SMC samplers that may use alternative sequences of distributions, and\ndescribes ways in which likelihood estimates may be improved adaptively as the\nalgorithm progresses, building on ideas from Moores et al. (2015). This\napproach is compared with a number of alternative algorithms for doubly\nintractable problems, including approximate Bayesian computation (ABC), which\nwe show is closely related to the method of M{\\o}ller et al. (2006).\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 06:36:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Everitt", "Richard G.", ""], ["Prangle", "Dennis", ""], ["Maybank", "Philip", ""], ["Bell", "Mark", ""]]}, {"id": "1710.04549", "submitter": "Diego Giuliani", "authors": "Yves Till\\'e, Maria Michela Dickson, Giuseppe Espa, Diego Giuliani", "title": "Measuring the spatial balance of a sample: A new measure based on the\n  Moran's I index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the degree of spatial spreading of a sample can be of great\ninterest when sampling from a spatial population. The commonly used spatial\nbalance index by Grafstr\\\"om et al. (2012) is particularly effective in\ncomparing the level of spatial spreading of different samples from the same\npopulation. However, its unbounded and uninterpretable scale of measurement\ndoes not allow to assess the level of spatial spreading in absolute terms and\nconfines its use to only raw comparisons. In this paper, we introduce a new\nabsolute measure of the spatial spreading of a sample using a normalized\nversion of the Moran's $I$ index. The properties and behaviour of the proposed\nmeasure are analysed through two simulation experiments, one based on\nartificial populations and the other on a population of real business units\nlocated in the province of Siena (Italy).\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 14:55:11 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:25:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Till\u00e9", "Yves", ""], ["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Giuliani", "Diego", ""]]}, {"id": "1710.04560", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Subhashis Ghosal, Jeffrey Prescott and Kingshuk Roy\n  Choudhury", "title": "Bayesian Modeling of the Structural Connectome for Studying Alzheimer\n  Disease", "comments": null, "journal-ref": "Annals of Applied Statistics 2019, Vol. 13, No. 3, 1791-1816", "doi": "10.1214/19-AOAS1257", "report-no": "AOAS1257", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study possible relations between the structure of the connectome, white\nmatter connecting different regions of brain, and Alzheimer disease. Regression\nmodels in covariates including age, gender and disease status for the extent of\nwhite matter connecting each pair of regions of brain are proposed. Subject We\nstudy possible relations between the Alzheimer's disease progression and the\nstructure of the connectome, white matter connecting different regions of\nbrain. Regression models in covariates including age, gender and disease status\nfor the extent of white matter connecting each pair of regions of brain are\nproposed. Subject inhomogeneity is also incorporated in the model through\nrandom effects with an unknown distribution. As there are large number of pairs\nof regions, we also adopt a dimension reduction technique through graphon\n(Lovasz and Szegedy (2006)) functions, which reduces functions of pairs of\nregions to functions of regions. The connecting graphon functions are\nconsidered unknown but assumed smoothness allows putting priors of low\ncomplexity on them. We pursue a nonparametric Bayesian approach by assigning a\nDirichlet process scale mixture of zero mean normal prior on the distributions\nof the random effects and finite random series of tensor products of B-splines\npriors on the underlying graphon functions. Markov chain Monte Carlo\ntechniques, for drawing samples for the posterior distributions are developed.\nThe proposed Bayesian method overwhelmingly outperforms similar ANCOVA models\nin the simulation setup. The proposed Bayesian approach is applied on a dataset\nof 100 subjects and 83 brain regions and key regions implicated in the changing\nconnectome are identified.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 15:16:02 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 00:44:44 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 20:43:05 GMT"}, {"version": "v4", "created": "Sun, 31 Mar 2019 20:18:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Roy", "Arkaprava", ""], ["Ghosal", "Subhashis", ""], ["Prescott", "Jeffrey", ""], ["Choudhury", "Kingshuk Roy", ""]]}, {"id": "1710.04721", "submitter": "Chiu-Hsieh Hsu", "authors": "Chiu-Hsieh Hsu and Mandi Yu", "title": "Cox regression analysis with missing covariates via multiple imputation", "comments": "22 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the situation of estimating Cox regression in which some\ncovariates are subject to missing, and there exists additional information\n(including observed event time, censoring indicator and fully observed\ncovariates) which may be predictive of the missing covariates. We propose to\nuse two working regression models: one for predicting the missing covariates\nand the other for predicting the missing probabilities. For each missing\ncovariate observation, these two working models are used to define a nearest\nneighbor imputing set. This set is then used to nonparametrically impute\ncovariate values for the missing observation. Upon the completion of\nimputation, Cox regression is performed on the multiply imputed datasets to\nestimate the regression coefficients. In a simulation study, we compare the\nnonparametric multiple imputation approach with the augmented inverse\nprobability weighted (AIPW) method, which directly incorporates the two working\nmodels into estimation of Cox regression. We show that all approaches can\nreduce bias due to non-ignorable missing mechanism. The proposed nonparametric\nimputation method is robust to mis-specification of either one of the two\nworking models and robust to mis-specification of the link function of the two\nworking models. In contrast, the AIPW method is not robust to mis-specification\nof the link function of the two working models and is sensitive to the\nselection probability. We apply the approaches to a breast cancer dataset from\nSurveillance, Epidemiology and End Results (SEER) Program .\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 21:05:22 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Hsu", "Chiu-Hsieh", ""], ["Yu", "Mandi", ""]]}, {"id": "1710.04795", "submitter": "Mohammad Arashi", "authors": "M. Arashi, Y. Asar and B. Yuzbasi", "title": "LLASSO: A linear unified LASSO for multicollinear situations", "comments": "20 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rescaled LASSO, by premultipying the LASSO with a matrix term,\nnamely linear unified LASSO (LLASSO) for multicollinear situations. Our\nnumerical study has shown that the LLASSO is comparable with other sparse\nmodeling techniques and often outperforms the LASSO and elastic net. Our\nfindings open new visions about using the LASSO still for sparse modeling and\nvariable selection. We conclude our study by pointing that the LLASSO can be\nsolved by the same efficient algorithm for solving the LASSO and suggest to\nfollow the same construction technique for other penalized estimators.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 03:54:55 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Arashi", "M.", ""], ["Asar", "Y.", ""], ["Yuzbasi", "B.", ""]]}, {"id": "1710.04801", "submitter": "Robert Garrard", "authors": "Robert Garrard", "title": "A Goodness-of-Fit Test for Sampled Subgraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of testing whether a graph's degree distribution\nbelongs to a particular family, such as poisson or scale-free, given that we\nonly observe a sampled subgraph. In particular, we focus on induced subgraph\nsampling, a sampling design which systematically distorts the degree\ndistribution of interest. We estimate the parameter indexing the hypothesized\nfamily by generalized method of moments and utilize the Kolmogorov-Smirnov test\nstatistic to assess goodness-of-fit. Since the distribution in the null\nhypothesis has been estimated, critical values for the test statistic must be\nsimulated. We propose a novel bootstrap in which we construct a graph whose\ndegree distribution conforms to the null hypothesis from which we may draw\npseudo-samples in the form of induced subgraphs. We investigate the properties\nof this procedure with a monte carlo study which confirms that the bootstrap is\nable to attain size close to the nominal level while exhibiting power under the\nalternative hypothesis. We present an application of this test to the protein\ninteraction network (PIN) of the yeast Saccharomyces cerevisiae. Accounting for\nthe high rates of false negatives present in PIN measurement, we are able to\nreject the hypothesis that the PIN of S. cerevisiae follows an Erdos-Renyi\nrandom graph family of degree distributions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 04:34:24 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Garrard", "Robert", ""]]}, {"id": "1710.04813", "submitter": "Konstantinos  Fokianos", "authors": "Konstantinos Fokianos, Anne Leucht, Michael H. Neumann", "title": "On Integrated $L^{1}$ Convergence Rate of an Isotonic Regression\n  Estimator for Multivariate Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general monotone regression estimation where we allow for\nindependent and dependent regressors. We propose a modification of the\nclassical isotonic least squares estimator and establish its rate of\nconvergence for the integrated $L_1$-loss function. The methodology captures\nthe shape of the data without assuming additivity or a parametric form for the\nregression function. Furthermore, the degree of smoothing is chosen\nautomatically and no auxiliary tuning is required for the theoretical analysis.\nSome simulations and two real data illustrations complement the study of the\nproposed estimator.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 05:52:26 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 19:21:54 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Fokianos", "Konstantinos", ""], ["Leucht", "Anne", ""], ["Neumann", "Michael H.", ""]]}, {"id": "1710.04824", "submitter": "Luyan Ji", "authors": "Xiurui Geng, Luyan Ji, Yongchao Zhao", "title": "The basic equation for target detection in remote sensing", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research has revealed a hidden relationship among several basic\ncomponents, which leads to the best target detection result. Further, we have\nproved that the matched filter (MF) is always superior to the constrained\nenergy minimization (CEM) operator, both of which were originally of parallel\nimportance in the field of target detection for remotely sensed image.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 06:49:40 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Geng", "Xiurui", ""], ["Ji", "Luyan", ""], ["Zhao", "Yongchao", ""]]}, {"id": "1710.04977", "submitter": "Theodore  Kypraios", "authors": "Muteb Alharthi, Theodore Kypraios and Philip D. O'Neill", "title": "Bayes factors for partially observed stochastic epidemic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model choice for stochastic epidemic models given\npartial observation of a disease outbreak through time. Our main focus is on\nthe use of Bayes factors. Although Bayes factors have appeared in the epidemic\nmodelling literature before, they can be hard to compute and little attention\nhas been given to fundamental questions concerning their utility. In this paper\nwe derive analytic expressions for Bayes factors given complete observation\nthrough time, which suggest practical guidelines for model choice problems. We\nextend the power posterior method for computing Bayes factors so as to account\nfor missing data and apply this approach to partially observed epidemics. For\ncomparison, we also explore the use of a deviance information criterion for\nmissing data scenarios. The methods are illustrated via examples involving both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 15:55:21 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Alharthi", "Muteb", ""], ["Kypraios", "Theodore", ""], ["O'Neill", "Philip D.", ""]]}, {"id": "1710.05013", "submitter": "Matthew Heaton", "authors": "Matthew J. Heaton, Abhirup Datta, Andrew Finley, Reinhard Furrer,\n  Rajarshi Guhaniyogi, Florian Gerber, Robert B. Gramacy, Dorit Hammerling,\n  Matthias Katzfuss, Finn Lindgren, Douglas W. Nychka, Furong Sun and Andrew\n  Zammit-Mangion", "title": "A Case Study Competition Among Methods for Analyzing Large Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process is an indispensable tool for spatial data analysts. The\nonset of the \"big data\" era, however, has lead to the traditional Gaussian\nprocess being computationally infeasible for modern spatial data. As such,\nvarious alternatives to the full Gaussian process that are more amenable to\nhandling big spatial data have been proposed. These modern methods often\nexploit low rank structures and/or multi-core and multi-threaded computing\nenvironments to facilitate computation. This study provides, first, an\nintroductory overview of several methods for analyzing large spatial data.\nSecond, this study describes the results of a predictive competition among the\ndescribed methods as implemented by different groups with strong expertise in\nthe methodology. Specifically, each research group was provided with two\ntraining datasets (one simulated and one observed) along with a set of\nprediction locations. Each group then wrote their own implementation of their\nmethod to produce predictions at the given location and each which was\nsubsequently run on a common computing environment. The methods were then\ncompared in terms of various predictive diagnostics. Supplementary materials\nregarding implementation details of the methods and code are available for this\narticle online.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:30:56 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 17:33:19 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Heaton", "Matthew J.", ""], ["Datta", "Abhirup", ""], ["Finley", "Andrew", ""], ["Furrer", "Reinhard", ""], ["Guhaniyogi", "Rajarshi", ""], ["Gerber", "Florian", ""], ["Gramacy", "Robert B.", ""], ["Hammerling", "Dorit", ""], ["Katzfuss", "Matthias", ""], ["Lindgren", "Finn", ""], ["Nychka", "Douglas W.", ""], ["Sun", "Furong", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "1710.05021", "submitter": "Zilin Li", "authors": "Zilin Li, Yaowu Liu and Xihong Lin", "title": "Simultaneous Detection of Signal Regions Using Quadratic Scan Statistics\n  With Applications in Whole Genome Association Studies", "comments": null, "journal-ref": "Journal of American Statistical Association (2020)", "doi": "10.1080/01621459.2020.1822849", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider in this paper detection of signal regions associated with disease\noutcomes in whole genome association studies. Gene- or region-based methods\nhave become increasingly popular in whole genome association analysis as a\ncomplementary approach to traditional individual variant analysis. However,\nthese methods test for the association between an outcome and the genetic\nvariants in a pre-specified region, e.g., a gene. In view of massive intergenic\nregions in whole genome sequencing (WGS) studies, we propose a computationally\nefficient quadratic scan (Q-SCAN) statistic based method to detect the\nexistence and the locations of signal regions by scanning the genome\ncontinuously. The proposed method accounts for the correlation (linkage\ndisequilibrium) among genetic variants, and allows for signal regions to have\nboth causal and neutral variants, and the effects of signal variants to be in\ndifferent directions. We study the asymptotic properties of the proposed Q-SCAN\nstatistics. We derive an empirical threshold that controls for the family-wise\nerror rate, and show that under regularity conditions the proposed method\nconsistently selects the true signal regions. We perform simulation studies to\nevaluate the finite sample performance of the proposed method. Our simulation\nresults show that the proposed procedure outperforms the existing methods,\nespecially when signal regions have causal variants whose effects are in\ndifferent directions, or are contaminated with neutral variants. We illustrate\nQ-SCAN by analyzing the WGS data from the Atherosclerosis Risk in Communities\n(ARIC) study.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:51:46 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 16:55:11 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 02:45:53 GMT"}, {"version": "v4", "created": "Thu, 25 Jul 2019 06:28:29 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Zilin", ""], ["Liu", "Yaowu", ""], ["Lin", "Xihong", ""]]}, {"id": "1710.05069", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio Mariano Bayer, D\\'ebora Missio Bayer, Guilherme Pumi", "title": "Kumaraswamy autoregressive moving average models for double bounded\n  environmental data", "comments": "25 pages, 4 tables, 4 figures", "journal-ref": "Journal of Hydrology, Volume 555, December 2017, Pages 385-396", "doi": "10.1016/j.jhydrol.2017.10.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the Kumaraswamy autoregressive moving average\nmodels (KARMA), which is a dynamic class of models for time series taking\nvalues in the double bounded interval $(a,b)$ following the Kumaraswamy\ndistribution. The Kumaraswamy family of distribution is widely applied in many\nareas, especially hydrology and related fields. Classical examples are time\nseries representing rates and proportions observed over time. In the proposed\nKARMA model, the median is modeled by a dynamic structure containing\nautoregressive and moving average terms, time-varying regressors, unknown\nparameters and a link function. We introduce the new class of models and\ndiscuss conditional maximum likelihood estimation, hypothesis testing\ninference, diagnostic analysis and forecasting. In particular, we provide\nclosed-form expressions for the conditional score vector and conditional Fisher\ninformation matrix. An application to environmental real data is presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 20:37:07 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 14:03:36 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bayer", "F\u00e1bio Mariano", ""], ["Bayer", "D\u00e9bora Missio", ""], ["Pumi", "Guilherme", ""]]}, {"id": "1710.05248", "submitter": "Daniel Cooley", "authors": "Daniel Cooley, Emeric Thibaud, Federico Castillo, Michael F. Wehner", "title": "A Nonparametric Method for Producing Isolines of Bivariate Exceedance\n  Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for drawing isolines indicating regions of equal joint\nexceedance probability for bivariate data. The method relies on bivariate\nregular variation, a dependence framework widely used for extremes. This\nframework enables drawing isolines corresponding to very low exceedance\nprobabilities and these lines may lie beyond the range of the data. The method\nwe utilize for characterizing dependence in the tail is largely nonparametric.\nFurthermore, we extend this method to the case of asymptotic independence and\npropose a procedure which smooths the transition from asymptotic independence\nin the interior to the first-order behavior on the axes. We propose a\ndiagnostic plot for assessing isoline estimate and choice of smoothing, and a\nbootstrap procedure to visually assess uncertainty.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 23:23:01 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cooley", "Daniel", ""], ["Thibaud", "Emeric", ""], ["Castillo", "Federico", ""], ["Wehner", "Michael F.", ""]]}, {"id": "1710.05263", "submitter": "Lixing Zhu", "authors": "Lingzhu Li and Lixing Zhu", "title": "Specification testing for regressions: an approach bridging between\n  local smoothing and global smoothing methods", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For regression models, most of existing specification tests can be\ncategorized into the class of local smoothing tests and of global smoothing\ntests. Compared with global smoothing tests, local smoothing tests can only\ndetect local alternatives distinct from the null hypothesis at a much slower\nrate when the dimension of predictor vector is high, but can be more sensitive\nto oscillating alternatives. In this paper, we suggest a projection-based test\nto bridge between the local and global smoothing-based methodologies such that\nthe test can benefit from the advantages of these two types of tests. The test\nconstruction is based on a kernel estimation-based method and the resulting\ntest becomes a distance-based test with a closed form. The asymptotic\nproperties are investigated. Simulations and a real data analysis are conducted\nto evaluate the performance of the test in finite sample cases.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 02:17:47 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 02:50:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Li", "Lingzhu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1710.05283", "submitter": "Lu Liu Dr", "authors": "Lu Liu, Lili Wang, Qingsong Xu", "title": "Uniform Consistency in Stochastic Block Model with Continuous Community\n  Label", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\cite{bickel2009nonparametric} developed a general framework to establish\nconsistency of community detection in stochastic block model (SBM). In most\napplications of this framework, the community label is discrete. For example,\nin \\citep{bickel2009nonparametric,zhao2012consistency} the degree corrected SBM\nis assumed to have a discrete degree parameter. In this paper, we generalize\nthe method of \\cite{bickel2009nonparametric} to give consistency analysis of\nmaximum likelihood estimator (MLE) in SBM with continuous community label. We\nshow that there is a standard procedure to transform the $||\\cdot||_2$ error\nbound to the uniform error bound. We demonstrate the application of our general\nresults by proving the uniform consistency (strong consistency) of the MLE in\nthe exponential network model with interaction effect. Unfortunately, in the\ncontinuous parameter case, the condition ensuring uniform consistency we\nobtained is much stronger than that in the discrete parameter case, namely\n$n\\mu_n^5/(\\log n)^{8}\\rightarrow\\infty$ versus $n\\mu_n/\\log\nn\\rightarrow\\infty$. Where $n\\mu_n$ represents the average degree of the\nnetwork. But continuous is the limit of discrete. So it is not surprising as we\nshow that by discretizing the community label space into sufficiently small\n(but not too small) pieces and applying the MLE on the discretized community\nlabel space, uniform consistency holds under almost the same condition as in\ndiscrete community label space. Such a phenomenon is surprising since the\ndiscretization does not depend on the data or the model. This reminds us of the\nthresholding method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 06:39:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Liu", "Lu", ""], ["Wang", "Lili", ""], ["Xu", "Qingsong", ""]]}, {"id": "1710.05287", "submitter": "Lu Liu Dr", "authors": "Lu Liu", "title": "On the Log Partition Function of Ising Model on Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sparse stochastic block model (SBM) with two communities is defined by the\ncommunity probability $\\pi_0,\\pi_1$, and the connection probability between\ncommunities $a,b\\in\\{0,1\\}$, namely $q_{ab} = \\frac{\\alpha_{ab}}{n}$. When\n$q_{ab}$ is constant in $a,b$, the random graph is simply the\nErd\\H{o}s-R\\'{e}ny random graph. We evaluate the log partition function of the\nIsing model on sparse SBM with two communities.\n  As an application, we give consistent parameter estimation of the sparse SBM\nwith two communities in a special case. More specifically, let $d_0,d_1$ be the\naverage degree of the two communities, i.e.,\n$d_0\\overset{def}{=}\\pi_0\\alpha_{00}+\\pi_1\\alpha_{01},d_1\\overset{def}{=}\\pi_0\\alpha_{10}+\\pi_1\\alpha_{11}$.\nWe focus on the regime $d_0=d_1$ (the regime $d_0\\ne d_1$ is trivial). In this\nregime, there exists $d,\\lambda$ and $r\\geq 0$ with $\\pi_0=\\frac{1}{1+r},\n\\pi_1=\\frac{r}{1+r}$, $\\alpha_{00}=d(1+r\\lambda), \\alpha_{01}=\\alpha_{10} =\nd(1-\\lambda), \\alpha_{11} = d(1+\\frac{\\lambda}{r})$. We give a consistent\nestimator of $r$ when $\\lambda<0$. The estimator of $\\lambda$ given by\n\\citep{mossel2015reconstruction} is valid in the general situation. We also\nprovide a random clustering algorithm which does not require knowledge of\nparameters and which is positively correlated with the true community label\nwhen $\\lambda<0$.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 06:46:37 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Liu", "Lu", ""]]}, {"id": "1710.05377", "submitter": "Ge Zhao", "authors": "Ge Zhao, Yanyuan Ma and Wenbin Lu", "title": "Efficient Estimation for Dimension Reduction with Censored Data", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general index model for survival data, which generalizes many\ncommonly used semiparametric survival models and belongs to the framework of\ndimension reduction. Using a combination of geometric approach in\nsemiparametrics and martingale treatment in survival data analysis, we devise\nestimation procedures that are feasible and do not require\ncovariate-independent censoring as assumed in many dimension reduction methods\nfor censored survival data. We establish the root-$n$ consistency and\nasymptotic normality of the proposed estimators and derive the most efficient\nestimator in this class for the general index model. Numerical experiments are\ncarried out to demonstrate the empirical performance of the proposed estimators\nand an application to an AIDS data further illustrates the usefulness of the\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 18:50:30 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhao", "Ge", ""], ["Ma", "Yanyuan", ""], ["Lu", "Wenbin", ""]]}, {"id": "1710.05406", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli", "title": "Variable selection for (realistic) stochastic blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic blockmodels provide a convenient representation of relations\nbetween communities of nodes in a network. However, they imply a notion of\nstochastic equivalence that is often unrealistic for real networks, and they\ncomprise large number of parameters that can make them hardly interpretable. We\ndiscuss two extensions of stochastic blockmodels, and a recently proposed\nvariable selection approach based on penalized inference, which allows to infer\na sparse reduced graph summarizing relations between communities. We compare\nthis approach with maximum likelihood estimation on two datasets on\nface-to-face interactions in a French primary school and on bill cosponsorships\nin the Italian Parliament.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 21:43:46 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Signorelli", "Mirko", ""]]}, {"id": "1710.05451", "submitter": "Nima Hejazi", "authors": "Nima S. Hejazi, Mark J. van der Laan, and Alan E. Hubbard", "title": "A generalization of moderated statistics to data adaptive semiparametric\n  estimation in high-dimensional biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread availability of high-dimensional biological data has made the\nsimultaneous screening of numerous biological characteristics a central\nstatistical problem in computational biology. While the dimensionality of such\ndatasets continues to increase, the problem of teasing out the effects of\nbiomarkers in studies measuring baseline confounders while avoiding model\nmisspecification remains only partially addressed. Efficient estimators\nconstructed from data adaptive estimates of the data-generating distribution\nprovide an avenue for avoiding model misspecification; however, in the context\nof high-dimensional problems requiring simultaneous estimation of numerous\nparameters, standard variance estimators have proven unstable, resulting in\nunreliable Type-I error control under standard multiple testing corrections. We\npresent the formulation of a general approach for applying empirical Bayes\nshrinkage approaches to asymptotically linear estimators of parameters defined\nin the nonparametric model. The proposal applies existing shrinkage estimators\nto the estimated variance of the influence function, allowing for increased\ninferential stability in high-dimensional settings. A methodology for\nnonparametric variable importance analysis for use with high-dimensional\nbiological datasets with modest sample sizes is introduced and the proposed\ntechnique is demonstrated to be robust in small samples even when relying on\ndata adaptive estimators that eschew parametric forms. Use of the proposed\nvariance moderation strategy in constructing stabilized variable importance\nmeasures of biomarkers is demonstrated by application to an observational study\nof occupational exposure. The result is a data adaptive approach for robustly\nuncovering stable associations in high-dimensional data with limited sample\nsizes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 01:13:42 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 03:08:17 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 07:33:24 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hejazi", "Nima S.", ""], ["van der Laan", "Mark J.", ""], ["Hubbard", "Alan E.", ""]]}, {"id": "1710.05559", "submitter": "Nicolas Brosse", "authors": "Nicolas Brosse, Alain Durmus, \\'Eric Moulines and Sotirios Sabanis", "title": "The Tamed Unadjusted Langevin Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of sampling from a probability\nmeasure $\\pi$ having a density on $\\mathbb{R}^d$ known up to a normalizing\nconstant, $x\\mapsto \\mathrm{e}^{-U(x)} / \\int_{\\mathbb{R}^d} \\mathrm{e}^{-U(y)}\n\\mathrm{d} y$. The Euler discretization of the Langevin stochastic differential\nequation (SDE) is known to be unstable in a precise sense, when the potential\n$U$ is superlinear, i.e. $\\liminf_{\\Vert x \\Vert\\to+\\infty} \\Vert \\nabla U(x)\n\\Vert / \\Vert x \\Vert = +\\infty$. Based on previous works on the taming of\nsuperlinear drift coefficients for SDEs, we introduce the Tamed Unadjusted\nLangevin Algorithm (TULA) and obtain non-asymptotic bounds in $V$-total\nvariation norm and Wasserstein distance of order $2$ between the iterates of\nTULA and $\\pi$, as well as weak error bounds. Numerical experiments are\npresented which support our findings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 08:32:28 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 07:53:43 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 07:38:08 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Brosse", "Nicolas", ""], ["Durmus", "Alain", ""], ["Moulines", "\u00c9ric", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "1710.05575", "submitter": "Maria Luz Gamiz", "authors": "Maria Luz Gamiz, Maria Dolores Martinez-Miranda and Jens Perch Nielsen", "title": "Multiplicative local linear hazard estimation and best one-sided\n  cross-validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops detailed mathematical statistical theory of a new class\nof cross-validation techniques of local linear kernel hazards and their\nmultiplicative bias corrections. The new class of cross-validation combines\nprinciples of local information and recent advances in indirect\ncross-validation. A few applications of cross-validating multiplicative kernel\nhazard estimation do exist in the literature. However, detailed mathematical\nstatistical theory and small sample performance are introduced via this paper\nand further upgraded to our new class of best one-sided cross-validation. Best\none-sided cross-validation turns out to have excellent performance in its\npractical illustrations, in its small sample performance and in its\nmathematical statistical theoretical performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 09:10:07 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gamiz", "Maria Luz", ""], ["Martinez-Miranda", "Maria Dolores", ""], ["Nielsen", "Jens Perch", ""]]}, {"id": "1710.05729", "submitter": "Marcela Alfaro C\\'ordoba", "authors": "Merve Yasemin Tekbudak, Marcela Alfaro C\\'ordoba, Arnab Maity, and\n  Ana-Maria Staicu", "title": "A Comparison of Testing Methods in Scalar-on-Function Regression", "comments": "38 pages, 14 pages of Supplementary Material, 40 images in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scalar-response functional model describes the association between a scalar\nresponse and a set of functional covariates. An important problem in the\nfunctional data literature is to test the nullity or linearity of the effect of\nthe functional covariate in the context of scalar-on-function regression. This\narticle provides an overview of the existing methods for testing both the null\nhypotheses that there is no relationship and that there is a linear\nrelationship between the functional covariate and scalar response, and a\ncomprehensive numerical comparison of their performance. The methods are\ncompared for a variety of realistic scenarios: when the functional covariate is\nobserved at dense or sparse grids and measurements include noise or not.\nFinally, the methods are illustrated on the Tecator data set.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 14:27:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Tekbudak", "Merve Yasemin", ""], ["C\u00f3rdoba", "Marcela Alfaro", ""], ["Maity", "Arnab", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1710.06021", "submitter": "Oscar Garc\\'ia", "authors": "Oscar Garc\\'ia", "title": "Estimating reducible stochastic differential equations by conversion to\n  a least-squares problem", "comments": null, "journal-ref": "Computational Statistics (online 10 Sep 2018)", "doi": "10.1007/s00180-018-0837-4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are increasingly used in\nlongitudinal data analysis, compartmental models, growth modelling, and other\napplications in a number of disciplines. Parameter estimation, however,\ncurrently requires specialized software packages that can be difficult to use\nand understand. This work develops and demonstrates an approach for estimating\nreducible SDEs using standard nonlinear least squares or mixed-effects\nsoftware. Reducible SDEs are obtained through a change of variables in linear\nSDEs, and are sufficiently flexible for modelling many situations. The approach\nis based on extending a known technique that converts maximum likelihood\nestimation for a Gaussian model with a nonlinear transformation of the\ndependent variable into an equivalent least-squares problem. A similar idea can\nbe used for Bayesian maximum a posteriori estimation. It is shown how to obtain\nparameter estimates for reducible SDEs containing both process and observation\nnoise, including hierarchical models with either fixed or random group\nparameters. Code and examples in R are given. Univariate SDEs are discussed in\ndetail, with extensions to the multivariate case outlined more briefly. The use\nof well tested and familiar standard software should make SDE modelling more\ntransparent and accessible. Keywords: stochastic processes; longitudinal data;\ngrowth curves; compartmental models; mixed-effects; R\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 22:46:23 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 19:17:02 GMT"}, {"version": "v3", "created": "Sat, 23 Jun 2018 00:42:55 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Garc\u00eda", "Oscar", ""]]}, {"id": "1710.06030", "submitter": "Martin Slawski", "authors": "Martin Slawski and Emanuel Ben-David", "title": "Linear Regression with Sparsely Permuted Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression analysis of multivariate data, it is tacitly assumed that\nresponse and predictor variables in each observed response-predictor pair\ncorrespond to the same entity or unit. In this paper, we consider the situation\nof \"permuted data\" in which this basic correspondence has been lost. Several\nrecent papers have considered this situation without further assumptions on the\nunderlying permutation. In applications, the latter is often to known to have\nadditional structure that can be leveraged. Specifically, we herein consider\nthe common scenario of \"sparsely permuted data\" in which only a small fraction\nof the data is affected by a mismatch between response and predictors. However,\nan adverse effect already observed for sparsely permuted data is that the least\nsquares estimator as well as other estimators not accounting for such partial\nmismatch are inconsistent. One approach studied in detail herein is to treat\npermuted data as outliers which motivates the use of robust regression\nformulations to estimate the regression parameter. The resulting estimate can\nsubsequently be used to recover the permutation. A notable benefit of the\nproposed approach is its computational simplicity given the general lack of\nprocedures for the above problem that are both statistically sound and\ncomputationally appealing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 23:24:23 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 22:37:38 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Slawski", "Martin", ""], ["Ben-David", "Emanuel", ""]]}, {"id": "1710.06031", "submitter": "Blakeley McShane", "authors": "Blakeley B. McShane and Jennifer L. Tackett and Ulf Bockenholt and\n  Andrew Gelman", "title": "Large Scale Replication Projects in Contemporary Psychological Research", "comments": null, "journal-ref": "The American Statistician 2019, Vol. 73:sup1, 99-105", "doi": "10.1080/00031305.2018.1505655", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication is complicated in psychological research because studies of a\ngiven psychological phenomenon can never be direct or exact replications of one\nanother, and thus effect sizes vary from one study of the phenomenon to the\nnext--an issue of clear importance for replication. Current large scale\nreplication projects represent an important step forward for assessing\nreplicability, but provide only limited information because they have thus far\nbeen designed in a manner such that heterogeneity either cannot be assessed or\nis intended to be eliminated. Consequently, the non-trivial degree of\nheterogeneity found in these projects represents a lower bound on\nheterogeneity. We recommend enriching large scale replication projects going\nforward by em- bracing heterogeneity. We argue this is key for assessing\nreplicability: if effect sizes are sufficiently heterogeneous--even if the sign\nof the effect is consistent--the phenomenon in question does not seem\nparticularly replicable and the theory underlying it seems poorly constructed\nand in need of enrichment. Uncovering why and revising theory in light of it\nwill lead to improved theory that explains heterogeneity and in- creases\nreplicability. Given this, large scale replication projects can play an\nimportant role not only in assessing replicability but also in advancing\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 23:27:15 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 23:49:02 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 18:15:20 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["McShane", "Blakeley B.", ""], ["Tackett", "Jennifer L.", ""], ["Bockenholt", "Ulf", ""], ["Gelman", "Andrew", ""]]}, {"id": "1710.06047", "submitter": "Justin Silverman", "authors": "Justin D. Silverman and Rachel K. Silverman", "title": "The Bayesian Sorting Hat: A Decision-Theoretic Approach to\n  Size-Constrained Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Size-constrained clustering (SCC) refers to the dual problem of using\nobservations to determine latent cluster structure while at the same time\nassigning observations to the unknown clusters subject to an analyst defined\nconstraint on cluster sizes. While several approaches have been proposed, SCC\nremains a difficult problem due to the combinatorial dependency between\nobservations introduced by the size-constraints. Here we reformulate SCC as a\ndecision problem and introduce a novel loss function to capture various types\nof size constraints. As opposed to prior work, our approach is uniquely suited\nto situations in which size constraints reflect and external limitation or\ndesire rather than an internal feature of the data generation process. To\ndemonstrate our approach, we develop a Bayesian mixture model for clustering\nrespondents using both simulated and real categorical survey data. Our\nmotivation for the development of this decision theoretic approach to SCC was\nto determine optimal team assignments for a Harry Potter themed scavenger hunt\nbased on categorical survey data from participants.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 01:32:28 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Silverman", "Justin D.", ""], ["Silverman", "Rachel K.", ""]]}, {"id": "1710.06056", "submitter": "Xiaoou Li", "authors": "Xi Chen, Yunxiao Chen, Xiaoou Li", "title": "Asymptotically Optimal Sequential Design for Rank Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequential design problem for rank aggregation is commonly encountered in\npsychology, politics, marketing, sports, etc. In this problem, a decision maker\nis responsible for ranking $K$ items by sequentially collecting pairwise noisy\ncomparison from judges. The decision maker needs to choose a pair of items for\ncomparison in each step, decide when to stop data collection, and make a final\ndecision after stopping, based on a sequential flow of information. Due to the\ncomplex ranking structure, existing sequential analysis methods are not\nsuitable.\n  In this paper, we formulate the problem under a Bayesian decision framework\nand propose sequential procedures that are asymptotically optimal. These\nprocedures achieve asymptotic optimality by seeking for a balance between\nexploration (i.e. finding the most indistinguishable pair of items) and\nexploitation (i.e. comparing the most indistinguishable pair based on the\ncurrent information). New analytical tools are developed for proving the\nasymptotic results, combining advanced change of measure techniques for\nhandling the level crossing of likelihood ratios and classic large deviation\nresults for martingales, which are of separate theoretical interest in solving\ncomplex sequential design problems. A mirror-descent algorithm is developed for\nthe computation of the proposed sequential procedures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:15:55 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Chen", "Xi", ""], ["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "1710.06075", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Ruey S. Tsay, Rong Chen", "title": "Constrained Factor Models for High-Dimensional Matrix-Variate Time\n  Series", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2019.1584899", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional matrix-variate time series data are becoming widely\navailable in many scientific fields, such as economics, biology, and\nmeteorology. To achieve significant dimension reduction while preserving the\nintrinsic matrix structure and temporal dynamics in such data, Wang et al.\n(2017) proposed a matrix factor model that is shown to provide effective\nanalysis. In this paper, we establish a general framework for incorporating\ndomain or prior knowledge in the matrix factor model through linear\nconstraints. The proposed framework is shown to be useful in achieving\nparsimonious parameterization, facilitating interpretation of the latent matrix\nfactor, and identifying specific factors of interest. Fully utilizing the\nprior-knowledge-induced constraints results in more efficient and accurate\nmodeling, inference, dimension reduction as well as a clear and better\ninterpretation of the results. In this paper, constrained, multi-term, and\npartially constrained factor models for matrix-variate time series are\ndeveloped, with efficient estimation procedures and their asymptotic\nproperties. We show that the convergence rates of the constrained factor\nloading matrices are much faster than those of the conventional matrix factor\nanalysis under many situations. Simulation studies are carried out to\ndemonstrate the finite-sample performance of the proposed method and its\nassociated asymptotic properties. We illustrate the proposed model with three\napplications, where the constrained matrix-factor models outperform their\nunconstrained counterparts in the power of variance explanation under the\nout-of-sample 10-fold cross-validation setting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 03:38:28 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 19:24:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Tsay", "Ruey S.", ""], ["Chen", "Rong", ""]]}, {"id": "1710.06078", "submitter": "Felix Xiaofeng Ye", "authors": "Felix X.-F. Ye, Yi-an Ma and Hong Qian", "title": "Estimate exponential memory decay in Hidden Markov Model and its\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in hidden Markov model has been challenging in terms of scalability\ndue to dependencies in the observation data. In this paper, we utilize the\ninherent memory decay in hidden Markov models, such that the forward and\nbackward probabilities can be carried out with subsequences, enabling efficient\ninference over long sequences of observations. We formulate this forward\nfiltering process in the setting of the random dynamical system and there exist\nLyapunov exponents in the i.i.d random matrices production. And the rate of the\nmemory decay is known as $\\lambda_2-\\lambda_1$, the gap of the top two Lyapunov\nexponents almost surely. An efficient and accurate algorithm is proposed to\nnumerically estimate the gap after the soft-max parametrization. The length of\nsubsequences $B$ given the controlled error $\\epsilon$ is\n$B=\\log(\\epsilon)/(\\lambda_2-\\lambda_1)$. We theoretically prove the validity\nof the algorithm and demonstrate the effectiveness with numerical examples. The\nmethod developed here can be applied to widely used algorithms, such as\nmini-batch stochastic gradient method. Moreover, the continuity of Lyapunov\nspectrum ensures the estimated $B$ could be reused for the nearby parameter\nduring the inference.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 03:54:11 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ye", "Felix X. -F.", ""], ["Ma", "Yi-an", ""], ["Qian", "Hong", ""]]}, {"id": "1710.06083", "submitter": "Ra\\'ul Alejandro Mor\\'an-V\\'asquez", "authors": "Ra\\'ul Alejandro Mor\\'an-V\\'asquez, Silvia L. P. Ferrari", "title": "Box-Cox elliptical distributions with application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study the class of Box-Cox elliptical distributions. It\nprovides alternative distributions for modeling multivariate positive,\nmarginally skewed and possibly heavy-tailed data. This new class of\ndistributions has as a special case the class of log-elliptical distributions,\nand reduces to the Box-Cox symmetric class of distributions in the univariate\nsetting. The parameters are interpretable in terms of quantiles and relative\ndispersions of the marginal distributions and of associations between pairs of\nvariables. The relation between the scale parameters and quantiles makes the\nBox-Cox elliptical distributions attractive for regression modeling purposes.\nApplications to data on vitamin intake are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:16:05 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mor\u00e1n-V\u00e1squez", "Ra\u00fal Alejandro", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1710.06191", "submitter": "Yichong Zhang", "authors": "Liangjun Su, Wuyi Wang, and Yichong Zhang", "title": "Strong Consistency of Spectral Clustering for Stochastic Block Models", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove the strong consistency of several methods based on the\nspectral clustering techniques that are widely used to study the community\ndetection problem in stochastic block models (SBMs). We show that under some\nweak conditions on the minimal degree, the number of communities, and the\neigenvalues of the probability block matrix, the K-means algorithm applied to\nthe eigenvectors of the graph Laplacian associated with its first few largest\neigenvalues can classify all individuals into the true community uniformly\ncorrectly almost surely. Extensions to both regularized spectral clustering and\ndegree-corrected SBMs are also considered. We illustrate the performance of\ndifferent methods on simulated networks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 10:05:16 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 03:15:22 GMT"}, {"version": "v3", "created": "Wed, 15 May 2019 07:06:50 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Su", "Liangjun", ""], ["Wang", "Wuyi", ""], ["Zhang", "Yichong", ""]]}, {"id": "1710.06229", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "Iterative Supervised Principal Components", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics, PMLR 84:106-114, 2018.\n  http://proceedings.mlr.press/v84/piironen18a.html", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional prediction problems, where the number of features may\ngreatly exceed the number of training instances, fully Bayesian approach with a\nsparsifying prior is known to produce good results but is computationally\nchallenging. To alleviate this computational burden, we propose to use a\npreprocessing step where we first apply a dimension reduction to the original\ndata to reduce the number of features to something that is computationally\nconveniently handled by Bayesian methods. To do this, we propose a new\ndimension reduction technique, called iterative supervised principal components\n(ISPC), which combines variable screening and dimension reduction and can be\nconsidered as an extension to the existing technique of supervised principal\ncomponents (SPCs). Our empirical evaluations confirm that, although not\nfoolproof, the proposed approach provides very good results on several\nmicroarray benchmark datasets with very affordable computation time, and can\nalso be very useful for visualizing high-dimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:00:45 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1710.06325", "submitter": "Elynn Y Chen", "authors": "Elynn Yi Chen, Rong Chen", "title": "Factor Models for High-Dimensional Dynamic Networks: with Application to\n  International Trade Flow Time Series 1981-2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic network analysis has found an increasing interest in the literature\nbecause of the importance of different kinds of dynamic social networks,\nbiological networks, and economic networks. Most available probability and\nstatistical models for dynamic network data are deduced from random graph\ntheory where the networks are characterized on the node and edge level. They\nare often very restrictive for applications and unscalable to high-dimensional\ndynamic network data which is very common nowadays. In this paper, we take a\ndifferent perspective: The evolving sequence of networks are treated as a time\nseries of network matrices. We adopt a matrix factor model where the observed\nsurface dynamic network is assumed to be driven by a latent dynamic network\nwith lower dimensions. The linear relationship between the surface network and\nthe latent network is characterized by unknown but deterministic loading\nmatrices. The latent network and the corresponding loadings are estimated via\nan eigenanalysis of a positive definite matrix constructed from the\nauto-cross-covariances of the network times series, thus capturing the dynamics\npresenting in the network. The proposed method is able to unveil the latent\ndynamic structure and achieve the objective of dimension reduction. Different\nfrom other dynamic network analytical methods that build on latent variables,\nour approach imposes neither any distributional assumptions on the underlying\nnetwork nor any parametric forms on its covariance function. The latent network\nis learned directly from the data with little subjective input. We applied the\nproposed method to the monthly international trade flow data from 1981 to 2015.\nThe results unveil an interesting evolution of the latent trading network and\nthe relations between the latent entities and the countries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:54:45 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Chen", "Elynn Yi", ""], ["Chen", "Rong", ""]]}, {"id": "1710.06351", "submitter": "Elynn Chen", "authors": "Elynn Yi Chen, Qiwei Yao, Rong Chen", "title": "Multivariate Spatial-temporal Prediction on Latent Low-dimensional\n  Functional Structure with Non-stationarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatio-temporal data arise more and more frequently in a wide\nrange of applications; however, there are relatively few general statistical\nmethods that can readily use that incorporate spatial, temporal and variable\ndependencies simultaneously. In this paper, we propose a new approach to\nrepresent non-parametrically the linear dependence structure of a multivariate\nspatio-temporal process in terms of latent common factors. The matrix structure\nof observations from the multivariate spatio-temporal process is well reserved\nthrough the matrix factor model configuration. The spatial loading functions\nare estimated non-parametrically by sieve approximation and the variable\nloading matrix is estimated via an eigen-analysis of a symmetric non-negative\ndefinite matrix. Though factor decomposition along the space mode is similar to\nthe low-rank approximation methods in spatial statistics, the fundamental\ndifference is that the low-dimensional structure is completely unknown in our\nsetting. Additionally, our method accommodates non-stationarity over space. The\nestimated loading functions facilitate spatial prediction. For temporal\nforecasting, we preserve the matrix structure of observations at each time\npoint by utilizing the matrix autoregressive model of order one MAR(1).\nAsymptotic properties of the proposed methods are established. Performance of\nthe proposed method is investigated on both synthetic and real datasets\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 15:55:13 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 21:22:23 GMT"}, {"version": "v3", "created": "Sat, 11 Nov 2017 16:24:48 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Chen", "Elynn Yi", ""], ["Yao", "Qiwei", ""], ["Chen", "Rong", ""]]}, {"id": "1710.06611", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli and Luisa Cutillo", "title": "On community structure validation in real networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community structure is a commonly observed feature of real networks. The term\nrefers to the presence in a network of groups of nodes (communities) that\nfeature high internal connectivity, but are poorly connected between each\nother. Whereas the issue of community detection has been addressed in several\nworks, the problem of validating a partition of nodes as a good community\nstructure for a real network has received considerably less attention and\nremains an open issue. We propose a set of indices for community structure\nvalidation of network partitions, which rely on concepts from network\nenrichment analysis. The proposed indices allow to compare the adequacy of\ndifferent partitions of nodes as community structures. Moreover, they can be\nemployed to assess whether two networks share the same or similar community\nstructures, and to evaluate the performance of different network clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:11:37 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 11:16:48 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 02:02:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Signorelli", "Mirko", ""], ["Cutillo", "Luisa", ""]]}, {"id": "1710.06642", "submitter": "Frederik Beaujean", "authors": "Frederik Beaujean, Allen Caldwell and Olaf Reimann", "title": "Is the bump significant? An axion-search example", "comments": "18 pages, 8 figures. v2 fixes arxiv's parsing of the URL in the\n  abstract", "journal-ref": null, "doi": "10.1140/epjc/s10052-018-6217-y", "report-no": null, "categories": "hep-ex stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments in physics involve searching for a localized excess over\nbackground expectations in an observed spectrum. If the background is known and\nthere is Gaussian noise, the amount of excess of successive observations can be\nquantified by the runs statistic taking care of the look-elsewhere effect. The\ndistribution of the runs statistic under the background model is known\nanalytically but the computation becomes too expensive for more than about a\nhundred observations. This work demonstrates a principled high-precision\nextrapolation from a few dozen up to millions of data points. It is most\nprecise in the interesting regime when an excess is present. The method is\nverified for benchmark cases and successfully applied to real data from an\naxion search. The code that implements our method is available at\nhttps://github.com/fredRos/runs .\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:31:36 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 12:56:00 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Beaujean", "Frederik", ""], ["Caldwell", "Allen", ""], ["Reimann", "Olaf", ""]]}, {"id": "1710.06660", "submitter": "Beatriz Bueno-Larraz", "authors": "Beatriz Bueno-Larraz, Johannes Klepsch", "title": "Variable selection for the prediction of C[0,1]-valued AR processes\n  using RKHS", "comments": "39 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model for the prediction of functional time series is introduced, where\nobservations are assumed to be continuous random functions. We model the\ndependence of the data with a nonstandard autoregressive structure, motivated\nin terms of the Reproducing Kernel Hilbert Space (RKHS) generated by the\nauto-covariance function of the data. The new approach helps to find relevant\npoints of the curves in terms of prediction accuracy. This dimension reduction\ntechnique is particularly useful for applications, since the results are\nusually directly interpretable in terms of the original curves. An empirical\nstudy involving real and simulated data is included, which generates\ncompetitive results. Supplementary material includes R-Code, tables and\nmathematical comments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 10:13:51 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 06:57:23 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 17:20:45 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 14:06:24 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Bueno-Larraz", "Beatriz", ""], ["Klepsch", "Johannes", ""]]}, {"id": "1710.06671", "submitter": "Filippo Monari Dr", "authors": "Filippo Monari", "title": "An approach for identifying sources of inadequacy and upgrades in models\n  with high-dimensional outputs and boundary conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of computer models (mathematical models implemented in\ncomputer codes), with respect to observed phenomena, is usually undertaken by\nbuilding different variants depending on modeller sensibility, and choosing the\none yielding the best fit of the field data, according to Root Mean Squared\nError (RMSE) based measures. Usually a particular model is chosen because of\nits marginally lower RMSE, and not because of its actual higher adequacy,\nrisking that its capability of extrapolating predictions is poor. This work\naims at improving the current practice in the creation of computer models by\nproposing an approach similar to those employed in statistical modelling,\nwherein starting from the simplest hypothesis, effective model upgrades are\nidentified by analysing discrepancies between observations and predictions, and\ndifferent model variants are compared according to robust likelihood based\ncriteria. The method, focused on models with high dimensional outputs and\nboundary conditions and centred on Bayesian calibration, is demonstrated on\nnumerical experiments considering a series of building energy models. The\nobject of the modelling is a test facility used for round robin tests in the\ncontext of the International Energy Agency (IEA), Energy Building and\nCommunities (EBC), Annex 58.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 11:00:44 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 16:03:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Monari", "Filippo", ""]]}, {"id": "1710.06727", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Todd McNutt, Ilya Shpitser", "title": "Semiparametric Causal Sufficient Dimension Reduction Of High Dimensional\n  Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cause-effect relationships are typically evaluated by comparing the outcome\nresponses to binary treatment values, representing two arms of a hypothetical\nrandomized controlled trial. However, in certain applications, treatments of\ninterest are continuous and high dimensional. For example, understanding the\ncausal relationship between severity of radiation therapy, represented by a\nhigh dimensional vector of radiation exposure values and post-treatment side\neffects is a problem of clinical interest in radiation oncology. An appropriate\nstrategy for making interpretable causal conclusions is to reduce the dimension\nof treatment. If individual elements of a high dimensional treatment vector\nweakly affect the outcome, but the overall relationship between the treatment\nvariable and the outcome is strong, careless approaches to dimension reduction\nmay not preserve this relationship. Moreover, methods developed for regression\nproblems do not transfer in a straightforward way to causal inference due to\nconfounding complications between the treatment and outcome. In this paper, we\nuse semiparametric inference theory for structural models to give a general\napproach to causal sufficient dimension reduction of a high dimensional\ntreatment such that the cause-effect relationship between the treatment and\noutcome is preserved. We illustrate the utility of our proposal through\nsimulations and a real data application in radiation oncology.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 13:38:42 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 21:47:52 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 17:25:54 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Nabi", "Razieh", ""], ["McNutt", "Todd", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1710.06735", "submitter": "Frederic Pascal", "authors": "Eug\\'enie Terreaux, Jean-Philippe Ovarlez and Fr\\'ed\\'eric Pascal", "title": "Robust Model Order Selection in Large Dimensional Elliptically Symmetric\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with model order selection in context of correlated noise.\nMore precisely, one considers sources embedded in an additive Complex\nElliptically Symmetric (CES) noise, with unknown parameters. The main\ndifficultly for estimating the model order lies into the noise correlation,\nnamely the scatter matrix of the corresponding CES distribution. In this work,\nto tackle that problem, one adopts a two-step approach: first, we develop two\ndifferent methods based on a Toeplitz-structured model for estimating this\nunknown scatter matrix and for whitening the correlated noise. Then, we apply\nMaronna's $M$-estimators on the whitened signal to estimate the covariance\nmatrix of the \"decorrelated\" signal in order to estimate the model order. The\nproposed methodology is based both on robust estimation theory as well as large\nRandom Matrix Theory, and original results are derived, proving the efficiency\nof this methodology. Indeed, the main theoretical contribution is to derive\nconsistent robust estimators for the covariance matrix of the\nsignal-plus-correlated noise in a large dimensional regime and to propose\nefficient methodology to estimate the rank of signal subspace. Finally, as\nshown in the analysis, these results show a great improvement compared to the\nstate-of-the-art, on both simulated and real hyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 13:56:54 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Terreaux", "Eug\u00e9nie", ""], ["Ovarlez", "Jean-Philippe", ""], ["Pascal", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1710.06826", "submitter": "Thomas Opitz", "authors": "Thomas Opitz", "title": "Spatial random field models based on L\\'evy indicator convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process convolutions yield random fields with flexible marginal distributions\nand dependence beyond Gaussianity, but statistical inference is often hampered\nby a lack of closed-form marginal distributions, and simulation-based inference\nmay be prohibitively computer-intensive. We here remedy such issues through a\nclass of process convolutions based on smoothing a (d+1)-dimensional L\\'evy\nbasis with an indicator function kernel to construct a d-dimensional\nconvolution process. Indicator kernels ensure univariate distributions in the\nL\\'evy basis family, which provides a sound basis for interpretation,\nparametric modeling and statistical estimation. We propose a class of isotropic\nstationary convolution processes constructed through hypograph indicator sets\ndefined as the space between the curve (s,H(s)) of a spherical probability\ndensity function H and the plane (s,0). If H is radially decreasing, the\ncovariance is expressed through the univariate distribution function of H. The\nbivariate joint tail behavior in such convolution processes is studied in\ndetail. Simulation and modeling extensions beyond isotropic stationary spatial\nmodels are discussed, including latent process models. For statistical\ninference of parametric models, we develop pairwise likelihood techniques and\nillustrate these on spatially indexed weed counts in the Bjertop data set, and\non daily wind speed maxima observed over 30 stations in the Netherlands.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 16:56:45 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Opitz", "Thomas", ""]]}, {"id": "1710.06891", "submitter": "Iavor Bojinov", "authors": "Iavor Bojinov, Natesh Pillai and Donald Rubin", "title": "Diagnosing missing always at random in multivariate data", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asz061", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for analyzing multivariate data sets with missing values require\nstrong, often unassessable, assumptions. The most common of these is that the\nmechanism that created the missing data is ignorable - a twofold assumption\ndependent on the mode of inference. The first part, which is the focus here,\nunder the Bayesian and direct-likelihood paradigms, requires that the missing\ndata are missing at random; in contrast, the frequentist-likelihood paradigm\ndemands that the missing data mechanism always produces missing at random data,\na condition known as missing always at random. Under certain regularity\nconditions, assuming missing always at random leads to an assumption that can\nbe tested using the observed data alone namely, the missing data indicators\nonly depend on fully observed variables. Here, we propose three different\ndiagnostic tests that not only indicate when this assumption is incorrect but\nalso suggest which variables are the most likely culprits. Although missing\nalways at random is not a necessary condition to ensure validity under the\nBayesian and direct-likelihood paradigms, it is sufficient, and evidence for\nits violation should encourage the careful statistician to conduct targeted\nsensitivity analyses.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 18:40:24 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:45:10 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 12:54:28 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Bojinov", "Iavor", ""], ["Pillai", "Natesh", ""], ["Rubin", "Donald", ""]]}, {"id": "1710.06900", "submitter": "Feras Saad", "authors": "Feras A. Saad, Vikash K. Mansinghka", "title": "Temporally-Reweighted Chinese Restaurant Process Mixtures for\n  Clustering, Imputing, and Forecasting Multivariate Time Series", "comments": "19 pages, 10 figures, 2 tables. Appearing in AISTATS 2018", "journal-ref": "Proceedings of the 21st International Conference on Artificial\n  Intelligence and Statistics, PMLR 84:755-764, 2018", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a Bayesian nonparametric method for forecasting,\nimputation, and clustering in sparsely observed, multivariate time series data.\nThe method is appropriate for jointly modeling hundreds of time series with\nwidely varying, non-stationary dynamics. Given a collection of $N$ time series,\nthe Bayesian model first partitions them into independent clusters using a\nChinese restaurant process prior. Within a cluster, all time series are modeled\njointly using a novel \"temporally-reweighted\" extension of the Chinese\nrestaurant process mixture. Markov chain Monte Carlo techniques are used to\nobtain samples from the posterior distribution, which are then used to form\npredictive inferences. We apply the technique to challenging forecasting and\nimputation tasks using seasonal flu data from the US Center for Disease Control\nand Prevention, demonstrating superior forecasting accuracy and competitive\nimputation accuracy as compared to multiple widely used baselines. We further\nshow that the model discovers interpretable clusters in datasets with hundreds\nof time series, using macroeconomic data from the Gapminder Foundation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 19:17:43 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 21:13:18 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Saad", "Feras A.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1710.06930", "submitter": "Abby Flynt", "authors": "Abby Flynt and Nema Dean", "title": "Growth Mixture Modeling with Measurement Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growth mixture models are an important tool for detecting group structure in\nrepeated measures data. Unlike traditional clustering methods, they explicitly\nmodel the repeat measurements on observations, and the statistical framework\nthey are based on allows for model selection methods to be used to select the\nnumber of clusters. However, the basic growth mixture model makes the\nassumption that all of the measurements in the data have grouping\ninformation/separate the clusters. In other clustering contexts, it has been\nshown that including non-clustering variables in clustering procedures can lead\nto poor estimation of the group structure both in terms of the number of\nclusters and cluster membership/parameters. In this paper, we present an\nextension of the growth mixture model that allows for incorporation of stepwise\nvariable selection based on the work done by Maugis et al. (2009) and Raftery\nand Dean (2006). Results presented on a simulation study suggest that the\nmethod performs well in correctly selecting the clustering variables and\nimproves on recovery of the cluster structure compared with the basic growth\nmixture model. The paper also presents an application of the model to a\nclinical study dataset and concludes with a discussion and suggestions for\ndirections of future work in this area.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 20:36:53 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Flynt", "Abby", ""], ["Dean", "Nema", ""]]}, {"id": "1710.06933", "submitter": "Joshua Snoke", "authors": "Joshua Snoke and Timothy R. Brick and Aleksandra Slavkovic and Michael\n  D. Hunter", "title": "Providing Accurate Models across Private Partitioned Data: Secure\n  Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the privacy paradigm of providing access to researchers\nto remotely carry out analyses on sensitive data stored behind firewalls. We\naddress the situation where the analysis demands data from multiple physically\nseparate databases which cannot be combined. Motivating this problem are\nanalyses using multiple data sources that currently are only possible through\nextension work creating a trusted user network. We develop and demonstrate a\nmethod for accurate calculation of the multivariate normal likelihood equation,\nfor a set of parameters given the partitioned data, which can then be maximized\nto obtain estimates. These estimates are achieved without sharing any data or\nany true intermediate statistics of the data across firewalls. We show that\nunder a certain set of assumptions our method for estimation across these\npartitions achieves identical results as estimation with the full data. Privacy\nis maintained by adding noise at each partition. This ensures each party\nreceives noisy statistics, such that the noise cannot be removed until the last\nstep to obtain a single value, the true total log-likelihood. Potential\napplications include all methods utilizing parameter estimation through\nmaximizing the multivariate normal likelihood equation. We give detailed\nalgorithms, along with available software, and both a real data example and\nsimulations estimating structural equation models (SEMs) with partitioned data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 20:58:30 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Snoke", "Joshua", ""], ["Brick", "Timothy R.", ""], ["Slavkovic", "Aleksandra", ""], ["Hunter", "Michael D.", ""]]}, {"id": "1710.07004", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Modal Regression using Kernel Density Estimation: a Review", "comments": "29 pages, 2 figures; a short invited review paper; new section on\n  softwares for modal regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We review recent advances in modal regression studies using kernel density\nestimation. Modal regression is an alternative approach for investigating\nrelationship between a response variable and its covariates. Specifically,\nmodal regression summarizes the interactions between the response variable and\ncovariates using the conditional mode or local modes. We first describe the\nunderlying model of modal regression and its estimators based on kernel density\nestimation. We then review the asymptotic properties of the estimators and\nstrategies for choosing the smoothing bandwidth. We also discuss useful\nalgorithms and similar alternative approaches for modal regression, and propose\nfuture direction in this field.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 05:25:07 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 04:10:05 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1710.07039", "submitter": "Monia Lupparelli", "authors": "Monia Lupparelli and Alessandra Mattei", "title": "Causal inference for binary non-independent outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference on multiple non-independent outcomes raises serious\nchallenges, because multivariate techniques that properly account for the\noutcome's dependence structure need to be considered. We focus on the case of\nbinary outcomes framing our discussion in the potential outcome approach to\ncausal inference. We define causal effects of treatment on joint outcomes\nintroducing the notion of product outcomes. We also discuss a decomposition of\nthe causal effect on product outcomes into intrinsic and extrinsic causal\neffects, which respectively provide information on treatment effect on the\nintrinsic (product) structure of the product outcomes and on the outcomes'\ndependence structure. We propose a log-mean linear regression approach for\nmodeling the distribution of the potential outcomes, which is particularly\nappealing because all the causal estimands of interest and the decomposition\ninto intrinsic and extrinsic causal effects can be easily derived by model\nparameters. The method is illustrated in two randomized experiments concerning\n(i) the effect of the administration of oral pre-surgery morphine on pain\nintensity after surgery; and (ii) the effect of honey on nocturnal cough and\nsleep difficulty associated with childhood upper respiratory tract infections.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:40:24 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 08:35:00 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Lupparelli", "Monia", ""], ["Mattei", "Alessandra", ""]]}, {"id": "1710.07154", "submitter": "Ivan Grechikhin", "authors": "Ivan Grechikhin", "title": "Comparison of statistical procedures for Gaussian graphical model\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are used in a variety of problems to uncover hidden\nstructures. There is a huge number of different identification procedures,\nconstructed for different purposes. However, it is important to research\ndifferent properties of such procedures and compare them in order to find out\nthe best procedure or the best use case for some specific procedure. In this\npaper, some statistical identification procedures are compared using different\nmeasures, such as Type I and Type II errors, ROC AUC.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 14:21:11 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Grechikhin", "Ivan", ""]]}, {"id": "1710.07156", "submitter": "Andreas F. Haselsteiner", "authors": "Andreas F. Haselsteiner, Jan-Hendrik Ohlendorf, Klaus-Dieter Thoben", "title": "Environmental contours based on kernel density estimation", "comments": "version that the authors' prepared for the proceedings of DEWEK 2017,\n  4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An offshore wind turbine needs to withstand the environmental loads, which\ncan be expected during its life time. Consequently, designers must define loads\nbased on extreme environmental conditions to verify structural integrity. The\nenvironmental contour method is an approach to systematically derive these\nextreme environmental design conditions. The method needs a probability density\nfunction as its input. Here we propose the use of constant bandwidth kernel\ndensity estimation to derive the joint probability density function of\nsignificant wave height and wind speed. We compare kernel density estimation\nwith the currently recommended conditional modeling approach. In comparison,\nkernel density estimation seems better suited to describe the statistics of\nenvironmental conditions of simultaneously high significant wave height and\nwind speed. Consequently, an environmental contour based on kernel density\nestimation does include these environmental conditions while an environmental\ncontour based on the conditional modeling approach does not. Since these\nenvironmental conditions often lead to the highest structural responses, it is\nespecially important that the used method outputs these conditions as design\nrequirements.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 14:22:57 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Haselsteiner", "Andreas F.", ""], ["Ohlendorf", "Jan-Hendrik", ""], ["Thoben", "Klaus-Dieter", ""]]}, {"id": "1710.07201", "submitter": "Jingsi Ming", "authors": "Jingsi Ming, Mingwei Dai, Mingxuan Cai, Xiang Wan, Jin Liu and Can\n  Yang", "title": "LSMM: A statistical approach to integrating functional annotations with\n  genome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thousands of risk variants underlying complex phenotypes (quantitative traits\nand diseases) have been identified in genome-wide association studies (GWAS).\nHowever, there are still two major challenges towards deepening our\nunderstanding of the genetic architectures of complex phenotypes. First, the\nmajority of GWAS hits are in the non-coding region and their biological\ninterpretation is still unclear. Second, accumulating evidence from GWAS\nsuggests the polygenicity of complex traits, i.e., a complex trait is often\naffected by many variants with small or moderate effects, whereas a large\nproportion of risk variants with small effects remains unknown. The\navailability of functional annotation data enables us to address the above\nchallenges. In this study, we propose a latent sparse mixed model (LSMM) to\nintegrate functional annotations with GWAS data. Not only does it increase\nstatistical power of the identification of risk variants, but also offers more\nbiological insights by detecting relevant functional annotations. To allow LSMM\nscalable to millions of variants and hundreds of functional annotations, we\ndeveloped an efficient variational expectation-maximization (EM) algorithm for\nmodel parameter estimation and statistical inference. We first conducted\ncomprehensive simulation studies to evaluate the performance of LSMM. Then we\napplied it to analyze 30 GWAS of complex phenotypes integrated with 9 genic\ncategory annotations and 127 tissue-specific functional annotations from the\nRoadmap project. The results demonstrate that our method possesses more\nstatistical power over conventional methods, and can help researchers achieve\ndeeper understanding of genetic architecture of these complex phenotypes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:49:27 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Ming", "Jingsi", ""], ["Dai", "Mingwei", ""], ["Cai", "Mingxuan", ""], ["Wan", "Xiang", ""], ["Liu", "Jin", ""], ["Yang", "Can", ""]]}, {"id": "1710.07420", "submitter": "Zhiyuan Lu", "authors": "Zhiyuan Lu, Moulinath Banerjee, George Michailidis", "title": "Intelligent sampling for multiple change-points in exceedingly long time\n  series with rate guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point estimation in its offline version is traditionally performed by\noptimizing over the data set of interest, by considering each data point as the\ntrue location parameter and computing a data fit criterion. Subsequently, the\ndata point that minimizes the criterion is declared as the change point\nestimate. For estimating multiple change points, the procedures are analogous\nin spirit, but significantly more involved in execution. Since change-points\nare local discontinuities, only data points close to the actual change point\nprovide useful information for estimation, while data points far away are\nsuperfluous, to the point where using only a few points close to the true\nparameter is just as precise as using the full data set. Leveraging this\n\"locality principle\", we introduce a two-stage procedure for the problem at\nhand, which in the 1st stage uses a sparse subsample to obtain pilot estimates\nof the underlying change points, and in the 2nd stage refines these estimates\nby sampling densely in appropriately defined neighborhoods around them. We\nestablish that this method achieves the same rate of convergence and even\nvirtually the same asymptotic distribution as the analysis of the full data,\nwhile reducing computational complexity to O(N^0.5) time (N being the length of\ndata set), as opposed to at least O(N) time for all current procedures, making\nit promising for the analysis on exceedingly long data sets with adequately\nspaced out change points. The main results are established under a signal plus\nnoise model with independent and identically distributed error terms, but\nextensions to dependent data settings, as well as multiple stage (>2)\nprocedures are also provided. The performance of our procedure -- which is\ncoined \"intelligent sampling\" -- is illustrated on both synthetic and real\nInternet data streams.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 05:53:24 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:42:43 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 10:55:16 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 04:29:28 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lu", "Zhiyuan", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1710.07422", "submitter": "P{\\aa}l Christie Ryalen", "authors": "P{\\aa}l Christie Ryalen, Mats Julius Stensrud, Kjetil R{\\o}ysland", "title": "Transforming cumulative hazard estimates", "comments": "22 pages, 4 figures. Added Lemma 1 stating sufficient conditions for\n  P-UT for our considerations, and Proposition 1 showing the conditions are\n  satisfied for estimated additive hazard coefficients and their martingale\n  residuals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time to event outcomes are often evaluated on the hazard scale, but\ninterpreting hazards may be difficult. Recently, there has been concern in the\ncausal inference literature that hazards actually have a built in\nselection-effect that prevents simple causal interpretations. This is even a\nproblem in randomized controlled trials, where hazard ratios have become a\nstandard measure of treatment effects. Modeling on the hazard scale is\nnevertheless convenient, e.g. to adjust for covariates. Using hazards for\nintermediate calculations may therefore be desirable. Here, we provide a\ngeneric method for transforming hazard estimates consistently to other scales\nat which these built in selection effects are avoided. The method is based on\ndifferential equations, and generalize a well known relation between the\nNelson-Aalen and Kaplan-Meier estimators. Using the martingale central limit\ntheorem we also find that covariances can be estimated consistently for a large\nclass of estimators, thus allowing for rapid calculations of confidence\nintervals. Hence, given cumulative hazard estimates based on e.g. Aalen's\nadditive hazard model, we can obtain many other parameters without much more\neffort. We present several examples and associated estimators. Coverage and\nconvergence speed is explored using simulations, suggesting that reliable\nestimates can be obtained in real-life scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 06:19:34 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 13:13:24 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 15:28:13 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 17:15:50 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Ryalen", "P\u00e5l Christie", ""], ["Stensrud", "Mats Julius", ""], ["R\u00f8ysland", "Kjetil", ""]]}, {"id": "1710.07468", "submitter": "Valeriy Baskakov Prof", "authors": "Valery Baskakov and Anna Bartunova", "title": "Nonparametric estimation of multivariate distribution function for\n  truncated and censored lifetime data", "comments": "32 pages, 11 figures", "journal-ref": null, "doi": "10.1007/s13385-019-00194-1", "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of models for generating statistical data in various fields of\ninsurance, including life insurance, pensions, and general insurance have been\nconsidered. It is shown that the insurance statistics data, as a rule, are\ntruncated and censored, and often multivariate. We propose a non-parametric\nestimation of the distribution function for multivariate truncated-censored\ndata in the form of a quasi-empirical distribution and a simple iterative\nalgorithm for its construction. To check the accuracy of the proposed\nevaluation of the distribution function for truncated-censored data, simulation\nstudies have been conducted, which showed its high efficiency. The proposed\nestimates have been tested for many years by the IAAC Group of Companies in the\nactuarial valuation of corporate social liabilities according to IAS 19\nEmployee Benefits. Apart from insurance, some results of the work can be used,\nfor example in medicine, biology, demography, mathematical theory of\nreliability, etc.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 10:00:37 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 18:19:39 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Baskakov", "Valery", ""], ["Bartunova", "Anna", ""]]}, {"id": "1710.07575", "submitter": "Yuya Sasaki", "authors": "Arie Beresteanu, Yuya Sasaki", "title": "Quantile Regression with Interval Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the identification of quantiles and quantile\nregression parameters when observations are set valued. We define the\nidentification set of quantiles of random sets in a way that extends the\ndefinition of quantiles for regular random variables. We then give sharp\ncharacterization of this set by extending concepts from random set theory. For\nquantile regression parameters, we show that the identification set is\ncharacterized by a system of conditional moment inequalities. This\ncharacterization extends that of parametric quantile regression for regular\nrandom variables. Estimation and inference theories are developed for\ncontinuous cases, discrete cases, nonparametric conditional quantiles, and\nparametric quantile regressions. A fast computational algorithm of set linear\nprogramming is proposed. Monte Carlo experiments support our theoretical\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 15:29:12 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:37:57 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Beresteanu", "Arie", ""], ["Sasaki", "Yuya", ""]]}, {"id": "1710.07747", "submitter": "Xin Tong Thomson", "authors": "Matthias Morzfeld, Xin T. Tong, and Youssef M. Marzouk", "title": "Localization for MCMC: sampling high-dimensional posterior distributions\n  with local structure", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how ideas from covariance localization in numerical weather\nprediction can be used in Markov chain Monte Carlo (MCMC) sampling of\nhigh-dimensional posterior distributions arising in Bayesian inverse problems.\nTo localize an inverse problem is to enforce an anticipated \"local\" structure\nby (i) neglecting small off-diagonal elements of the prior precision and\ncovariance matrices; and (ii) restricting the influence of observations to\ntheir neighborhood. For linear problems we can specify the conditions under\nwhich posterior moments of the localized problem are close to those of the\noriginal problem. We explain physical interpretations of our assumptions about\nlocal structure and discuss the notion of high dimensionality in local\nproblems, which is different from the usual notion of high dimensionality in\nfunction space MCMC. The Gibbs sampler is a natural choice of MCMC algorithm\nfor localized inverse problems and we demonstrate that its convergence rate is\nindependent of dimension for localized linear problems. Nonlinear problems can\nalso be tackled efficiently by localization and, as a simple illustration of\nthese ideas, we present a localized Metropolis-within-Gibbs sampler. Several\nlinear and nonlinear numerical examples illustrate localization in the context\nof MCMC samplers for inverse problems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 03:26:53 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 03:57:00 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 02:28:48 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 01:35:25 GMT"}, {"version": "v5", "created": "Tue, 10 Jul 2018 03:29:17 GMT"}, {"version": "v6", "created": "Sun, 11 Nov 2018 02:58:30 GMT"}, {"version": "v7", "created": "Tue, 8 Jan 2019 12:34:58 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Morzfeld", "Matthias", ""], ["Tong", "Xin T.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1710.07781", "submitter": "Kevin Kokot", "authors": "Holger Dette, Kevin Kokot and Alexander Aue", "title": "Functional data analysis in the Banach space of continuous functions", "comments": "- updated footnote and acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is typically conducted within the $L^2$-Hilbert\nspace framework. There is by now a fully developed statistical toolbox allowing\nfor the principled application of the functional data machinery to real-world\nproblems, often based on dimension reduction techniques such as functional\nprincipal component analysis. At the same time, there have recently been a\nnumber of publications that sidestep dimension reduction steps and focus on a\nfully functional $L^2$-methodology. This paper goes one step further and\ndevelops data analysis methodology for functional time series in the space of\nall continuous functions. The work is motivated by the fact that objects with\nrather different shapes may still have a small $L^2$-distance and are therefore\nidentified as similar when using an $L^2$-metric. However, in applications it\nis often desirable to use metrics reflecting the visualization of the curves in\nthe statistical analysis. The methodological contributions are focused on\ndeveloping two-sample and change-point tests as well as confidence bands, as\nthese procedures appear do be conducive to the proposed setting. Particular\ninterest is put on relevant differences; that is, on not trying to test for\nexact equality, but rather for pre-specified deviations under the null\nhypothesis.\n  The procedures are justified through large-sample theory. To ensure\npracticability, non-standard bootstrap procedures are developed and\ninvestigated addressing particular features that arise in the problem of\ntesting relevant hypotheses. The finite sample properties are explored through\na simulation study and an application to annual temperature profiles.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 10:25:41 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 10:15:54 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dette", "Holger", ""], ["Kokot", "Kevin", ""], ["Aue", "Alexander", ""]]}, {"id": "1710.07849", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Yanli Wang, Gurong Wu", "title": "Heat Kernel Smoothing in Irregular Image Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discrete version of heat kernel smoothing on graph data\nstructure. The method is used to smooth data in an irregularly shaped domains\nin 3D images.\n  New statistical properties are derived. As an application, we show how to\nfilter out data in the lung blood vessel trees obtained from computed\ntomography. The method can be further used in representing the complex vessel\ntrees parametrically and extracting the skeleton representation of the trees.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 19:53:36 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chung", "Moo K.", ""], ["Wang", "Yanli", ""], ["Wu", "Gurong", ""]]}, {"id": "1710.08054", "submitter": "William Neill", "authors": "William H. Neill, Ray H. Kamps, Scott J. Walker, Hsin-i Wu, T. Scott\n  Brandes, Delbert M. Gatlin III, Tiffany L. Hopper, and Robert R. Vega", "title": "Consilience: A Holistic Measure of Goodness-of-Fit", "comments": "This 3rd update of the ms. (permanent arXiv identifier 1710.08054,\n  23OCT2017) differs from the 2nd only in that the 3rd provides an additional,\n  alternative pathway for retrieving files cited via people.tamu.edu-hyperlinks\n  in the ms. <end>", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an apparently new measure of multivariate goodness-of-fit between\nsets of quantitative results from a model (simulation, analytical, or multiple\nregression), paired with those observed under corresponding conditions from the\nsystem being modeled. Our approach returns a single, integrative measure, even\nthough it can accommodate complex systems that produce responses of M types.\nFor each response-type, the goodness-of-fit measure, which we label\n\"Consilience\" (C), is maximally 1, for perfect fit; near 0 for the large-sample\ncase (number of pairs, N, more than about 25) in which the modeled series is a\nrandom sample from a quasi-normal distribution with the same mean and variance\nas that of the observed series (null model); and, less than 0, toward\nminus-infinity, for progressively worse fit. In addition, lack-of-fit for each\nresponse-type can be apportioned between systematic and non-systematic\n(unexplained) components of error. Finally, for statistical assessment of\nmodels relative to the equivalent null model, we offer provisional estimates of\ncritical C vs. N, and of critical joint-C vs. N and M, at various levels of\nPr(type-I error). Application of our proposed methodology requires only MS\nExcel (2003 or later); we provide Excel XLS and XLSX templates that afford\nsemi-automatic computation for systems involving up to M = 5 response types,\neach represented by up to N = 1000 observed-and-modeled result pairs. N need\nnot be equal, nor response pairs in complete overlap, over M.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 01:21:40 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 15:56:59 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 02:36:28 GMT"}, {"version": "v4", "created": "Sat, 20 Oct 2018 21:42:52 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Neill", "William H.", ""], ["Kamps", "Ray H.", ""], ["Walker", "Scott J.", ""], ["Wu", "Hsin-i", ""], ["Brandes", "T. Scott", ""], ["Gatlin", "Delbert M.", "III"], ["Hopper", "Tiffany L.", ""], ["Vega", "Robert R.", ""]]}, {"id": "1710.08073", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu, Yuanyuan Li, Qing Liu", "title": "A new class of $L_q$-norm zonoid depths", "comments": "16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zonoid depth, as a well-known ordering tool, has been widely used in\nmultivariate analysis. However, since its depth value vanishes outside the\nconvex hull of the data cloud, it suffers from the so-called `outside problem',\nwhich consequently hinders its many practical applications, e.g.,\nclassification. In this note, we propose a new class of \\emph{$L_q$-norm zonoid\ndepths}, which has no such problem. Examples are also provided to illustrate\ntheir contours.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 02:52:10 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Liu", "Xiaohui", ""], ["Li", "Yuanyuan", ""], ["Liu", "Qing", ""]]}, {"id": "1710.08074", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Regularized calibrated estimation of propensity scores with model\n  misspecification and high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score methods are widely used for estimating treatment effects\nfrom observational studies. A popular approach is to estimate propensity scores\nby maximum likelihood based on logistic regression, and then apply inverse\nprobability weighted estimators or extensions to estimate treatment effects.\nHowever, a challenging issue is that such inverse probability weighting methods\nincluding doubly robust methods can perform poorly even when the logistic model\nappears adequate as examined by conventional techniques. In addition, there is\nincreasing difficulty to appropriately estimate propensity scores when dealing\nwith a large number of covariates. To address these issues, we study calibrated\nestimation as an alternative to maximum likelihood estimation for fitting\nlogistic propensity score models. We show that, with possible model\nmisspecification, minimizing the expected calibration loss underlying the\ncalibrated estimators involves reducing both the expected likelihood loss and a\nmeasure of relative errors which controls the mean squared errors of inverse\nprobability weighted estimators. Furthermore, we propose a regularized\ncalibrated estimator by minimizing the calibration loss with a Lasso penalty.\nWe develop a novel Fisher scoring descent algorithm for computing the proposed\nestimator, and provide a high-dimensional analysis of the resulting inverse\nprobability weighted estimators of population means, leveraging the control of\nrelative errors for calibrated estimation. We present a simulation study and an\nempirical application to demonstrate the advantages of the proposed methods\ncompared with maximum likelihood and regularization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 02:56:40 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "1710.08083", "submitter": "Ziwei Zhu", "authors": "Jianqing Fan, Wenyan Gong, Ziwei Zhu", "title": "Generalized High-Dimensional Trace Regression via Nuclear Norm\n  Regularization", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalized trace regression with a near low-rank regression\ncoefficient matrix, which extends notion of sparsity for regression coefficient\nvectors. Specifically, given a matrix covariate $X$, the probability density\nfunction $f(Y\\vert X)=c(Y)\\exp{(\\phi^{-1}\\left[-Y\\eta^* + b(\\eta^*)\\right])}$,\nwhere $\\eta^*=tr({\\Theta^*}^TX)$. This model accommodates various types of\nresponses and embraces many important problem setups such as reduced-rank\nregression, matrix regression that accommodates a panel of regressors, matrix\ncompletion, among others. We estimate $\\Theta^*$ through minimizing empirical\nnegative log-likelihood plus nuclear norm penalty. We first establish a general\ntheory and then for each specific problem, we derive explicitly the statistical\nrate of the proposed estimator. They all match the minimax rates in the linear\ntrace regression up to logarithmic factors. Numerical studies confirm the rates\nwe established and demonstrate the advantage of generalized trace regression\nover linear trace regression when the response is dichotomous. We also show the\nbenefit of incorporating nuclear norm regularization in dynamic stock return\nprediction and in image classification.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 04:22:14 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Fan", "Jianqing", ""], ["Gong", "Wenyan", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1710.08388", "submitter": "Pramita Bagchi", "authors": "Pramita Bagchi and Holger Dette", "title": "A Test for Separability in Covariance Operators of Random Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of separability is a simplifying and very popular assumption\nin the analysis of spatio-temporal or hypersurface data structures. It is often\nmade in situations where the covariance structure cannot be easily estimated,\nfor example because of a small sample size or because of computational storage\nproblems. In this paper we propose a new and very simple test to validate this\nassumption. Our approach is based on a measure of separability which is zero in\nthe case of separability and positive otherwise. The measure can be estimated\nwithout calculating the full non-separable covariance operator. We prove\nasymptotic normality of the corresponding statistic with a limiting variance,\nwhich can easily be estimated from the available data. As a consequence\nquantiles of the standard normal distribution can be used to obtain critical\nvalues and the new test of separability is very easy to implement. In\nparticular, our approach does neither require projections on subspaces\ngenerated by the eigenfunctions of the covariance operator, nor resampling\nprocedures to obtain critical values nor distributional assumptions as used by\nother available methods of constructing tests for separability. We investigate\nthe finite sample performance by means of a simulation study and also provide a\ncomparison with the currently available methodology. Finally, the new procedure\nis illustrated analyzing wind speed and temperature data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 16:59:26 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 13:59:10 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 09:04:48 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Bagchi", "Pramita", ""], ["Dette", "Holger", ""]]}, {"id": "1710.08508", "submitter": "Meng Li", "authors": "Meng Li and Armin Schwartzman", "title": "Standardization of multivariate Gaussian mixture models and background\n  adjustment of PET images in brain oncology", "comments": null, "journal-ref": null, "doi": "10.1214/18-AOAS1149", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brain oncology, it is routine to evaluate the progress or remission of the\ndisease based on the differences between a pre-treatment and a post-treatment\nPositron Emission Tomography (PET) scan. Background adjustment is necessary to\nreduce confounding by tissue-dependent changes not related to the disease. When\nmodeling the voxel intensities for the two scans as a bivariate Gaussian\nmixture, background adjustment translates into standardizing the mixture at\neach voxel, while tumor lesions present themselves as outliers to be detected.\nIn this paper, we address the question of how to standardize the mixture to a\nstandard multivariate normal distribution, so that the outliers (i.e., tumor\nlesions) can be detected using a statistical test. We show theoretically and\nnumerically that the tail distribution of the standardized scores is favorably\nclose to standard normal in a wide range of scenarios while being conservative\nat the tails, validating voxelwise hypothesis testing based on standardized\nscores. To address standardization in spatially heterogeneous image data, we\npropose a spatial and robust multivariate expectation-maximization (EM)\nalgorithm, where prior class membership probabilities are provided by\ntransformation of spatial probability template maps and the estimation of the\nclass mean and covariances are robust to outliers. Simulations in both\nunivariate and bivariate cases suggest that standardized scores with soft\nassignment have tail probabilities that are either very close to or more\nconservative than standard normal. The proposed methods are applied to a real\ndata set from a PET phantom experiment, yet they are generic and can be used in\nother contexts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:13:50 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 19:40:27 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Meng", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1710.08511", "submitter": "Denisa Roberts", "authors": "Lucas Roberts and Denisa Roberts", "title": "An Expectation Maximization Framework for Yule-Simon Preferential\n  Attachment Models", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop an Expectation Maximization(EM) algorithm to\nestimate the parameter of a Yule-Simon distribution. The Yule-Simon\ndistribution exhibits the \"rich get richer\" effect whereby an 80-20 type of\nrule tends to dominate. These distributions are ubiquitous in industrial\nsettings. The EM algorithm presented provides both frequentist and Bayesian\nestimates of the $\\lambda$ parameter. By placing the estimation method within\nthe EM framework we are able to derive Standard errors of the resulting\nestimate. Additionally, we prove convergence of the Yule-Simon EM algorithm and\nstudy the rate of convergence. An explicit, closed form solution for the rate\nof convergence of the algorithm is given. Applications including graph node\ndegree distribution estimation are listed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:33:31 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 23:11:55 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 14:10:43 GMT"}, {"version": "v4", "created": "Sat, 14 Nov 2020 20:09:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Roberts", "Lucas", ""], ["Roberts", "Denisa", ""]]}, {"id": "1710.08749", "submitter": "Corwin Zigler", "authors": "Corwin M Zigler and Matthew Cefalu", "title": "Posterior Predictive Treatment Assignment for Estimating Causal Effects\n  with Limited Overlap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects with propensity scores relies upon the availability\nof treated and untreated units observed at each value of the estimated\npropensity score. In settings with strong confounding, limited so-called\n\"overlap\" in propensity score distributions can undermine the empirical basis\nfor estimating causal effects and yield erratic finite-sample performance of\nexisting estimators. We propose a Bayesian procedure designed to estimate\ncausal effects in settings where there is limited overlap in propensity score\ndistributions. Our method relies on the posterior predictive treatment\nassignment (PPTA), a quantity that is derived from the propensity score but\nserves different role in estimation of causal effects. We use the PPTA to\nestimate causal effects by marginalizing over the uncertainty in whether each\nobservation is a member of an unknown subset for which treatment assignment can\nbe assumed unconfounded. The resulting posterior distribution depends on the\nempirical basis for estimating a causal effect for each observation and has\ncommonalities with recently-proposed \"overlap weights\" of Li et al. (2016). We\nshow that the PPTA approach can be construed as a stochastic version of\nexisting ad-hoc approaches such as pruning based on the propensity score or\ntruncation of inverse probability of treatment weights, and highlight several\npractical advantages including uncertainty quantification and improved\nfinite-sample performance. We illustrate the method in an evaluation of the\neffectiveness of technologies for reducing harmful pollution emissions from\npower plants in the United States.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 13:11:48 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Zigler", "Corwin M", ""], ["Cefalu", "Matthew", ""]]}, {"id": "1710.08775", "submitter": "Patrick Forr\\'e", "authors": "Patrick Forr\\'e and Joris M. Mooij", "title": "Markov Properties for Graphical Models with Cycles and Latent Variables", "comments": "131 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate probabilistic graphical models that allow for both cycles and\nlatent variables. For this we introduce directed graphs with hyperedges\n(HEDGes), generalizing and combining both marginalized directed acyclic graphs\n(mDAGs) that can model latent (dependent) variables, and directed mixed graphs\n(DMGs) that can model cycles. We define and analyse several different Markov\nproperties that relate the graphical structure of a HEDG with a probability\ndistribution on a corresponding product space over the set of nodes, for\nexample factorization properties, structural equations properties,\nordered/local/global Markov properties, and marginal versions of these. The\nvarious Markov properties for HEDGes are in general not equivalent to each\nother when cycles or hyperedges are present, in contrast with the simpler case\nof directed acyclic graphical (DAG) models (also known as Bayesian networks).\nWe show how the Markov properties for HEDGes - and thus the corresponding\ngraphical Markov models - are logically related to each other.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 13:52:34 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Forr\u00e9", "Patrick", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1710.08976", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Wenlong Gong", "title": "A class of multi-resolution approximations for large spatial datasets", "comments": null, "journal-ref": "Statistica Sinica, 30(4), 2203-2226 (2020)", "doi": "10.1007/s13253-020-00401-7", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are popular and flexible models for spatial, temporal, and\nfunctional data, but they are computationally infeasible for large datasets. We\ndiscuss Gaussian-process approximations that use basis functions at multiple\nresolutions to achieve fast inference and that can (approximately) represent\nany spatial covariance structure. We consider two special cases of this\nmulti-resolution-approximation framework, a taper version and a\ndomain-partitioning (block) version. We describe theoretical properties and\ninference procedures, and study the computational complexity of the methods.\nNumerical comparisons and an application to satellite data are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 20:17:45 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 17:45:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Gong", "Wenlong", ""]]}, {"id": "1710.08978", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Spectral Density Estimation for Random Fields via Periodic Embeddings", "comments": null, "journal-ref": "Biometrika, 106(2), 267-286 (2019)", "doi": "10.1093/biomet/asz004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce methods for estimating the spectral density of a random field on\na $d$-dimensional lattice from incomplete gridded data. Data are iteratively\nimputed onto an expanded lattice according to a model with a periodic\ncovariance function. The imputations are convenient computationally, in that\ncirculant embedding and preconditioned conjugate gradient methods can produce\nimputations in $O(n\\log n)$ time and $O(n)$ memory. However, these so-called\nperiodic imputations are motivated mainly by their ability to produce accurace\nspectral density estimates. In addition, we introduce a parametric filtering\nmethod that is designed to reduce periodogram smoothing bias. The paper\ncontains theoretical results studying properties of the imputed data\nperiodogram and numerical and simulation studies comparing the performance of\nthe proposed methods to existing approaches in a number of scenarios. We\npresent an application to a gridded satellite surface temperature dataset with\nmissing values.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 20:24:33 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 15:20:32 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1710.08994", "submitter": "Yuchen Zheng", "authors": "Yuchen Zheng, Ilbin Lee, Nicoleta Serban", "title": "Variable Partitioning for Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about how to partition decision variables while decomposing a\nlarge-scale optimization problem for the best performance of distributed\nsolution methods. Solving a large-scale optimization problem sequen- tially can\nbe computationally challenging. One classic approach is to decompose the\nproblem into smaller sub-problems and solve them in a distributed fashion.\nHowever, there is little discussion in the literature on which variables should\nbe grouped together to form the sub-problems, especially when the optimization\nformulation involves complex constraints. We focus on one of the most popular\ndistributed approaches, dual decomposition and distributed sub-gradient\nmethods. Based on a theoretical guarantee on its convergence rate, we explain\nthat a partition of variables can critically affect the speed of convergence\nand highlight the importance of the number of dualized constraints. Then, we\nintroduce a novel approach to find a partition that reduces the number of\ndualized constraints by utilizing a community detection algorithm from physics\nliterature. Roughly speaking, the proposed method groups decision variables\nthat appear together in con- straints and solves the resulting sub-problems\nwith blocks of variables in parallel. Empirical experiments on a real\napplication show that the proposed method significantly accelerates the\nconvergence of the distributed sub-gradient method. The advantage of our\napproach becomes more significant as the size of the problem increases and each\nconstraint involves more variables.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 21:15:44 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Zheng", "Yuchen", ""], ["Lee", "Ilbin", ""], ["Serban", "Nicoleta", ""]]}, {"id": "1710.09009", "submitter": "Fabian Dunker", "authors": "Fabian Dunker, Stephan Klasen, Tatyana Krivobokova", "title": "Asymptotic Distribution and Simultaneous Confidence Bands for Ratios of\n  Quantile Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ratio of medians or other suitable quantiles of two distributions is widely\nused in medical research to compare treatment and control groups or in\neconomics to compare various economic variables when repeated cross-sectional\ndata are available. Inspired by the so-called growth incidence curves\nintroduced in poverty research, we argue that the ratio of quantile functions\nis a more appropriate and informative tool to compare two distributions. We\npresent an estimator for the ratio of quantile functions and develop\ncorresponding simultaneous confidence bands, which allow to assess significance\nof certain features of the quantile functions ratio. Derived simultaneous\nconfidence bands rely on the asymptotic distribution of the quantile functions\nratio and do not require re-sampling techniques. The performance of the\nsimultaneous confidence bands is demonstrated in simulations. Analysis of the\nexpenditure data from Uganda in years 1999, 2002 and 2005 illustrates the\nrelevance of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:28:58 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Dunker", "Fabian", ""], ["Klasen", "Stephan", ""], ["Krivobokova", "Tatyana", ""]]}, {"id": "1710.09020", "submitter": "Ziwei Zhu", "authors": "Ziwei Zhu, Wenjing Zhou", "title": "Taming heavy-tailed features by shrinkage", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on a variant of the generalized linear model (GLM)\ncalled corrupted GLM (CGLM) with heavy-tailed features and responses. To\nrobustify the statistical inference on this model, we propose to apply\n$\\ell_4$-norm shrinkage to the feature vectors in the low-dimensional regime\nand apply elementwise shrinkage to them in the high-dimensional regime. Under\nbounded fourth moment assumptions, we show that the maximum likelihood\nestimator (MLE) based on the shrunk data enjoys nearly the minimax optimal rate\nwith an exponential deviation bound. Our simulations demonstrate that the\nproposed feature shrinkage significantly enhances the statistical performance\nin linear regression and logistic regression on heavy-tailed data. Finally, we\napply our shrinkage principle to guard against mislabeling and image noise in\nthe human-written digit recognition problem. We add an $\\ell_4$-norm shrinkage\nlayer to the original neural net and reduce the testing misclassification rate\nby more than $30\\%$ relatively in the presence of mislabeling and image noise.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 23:47:07 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 23:18:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhu", "Ziwei", ""], ["Zhou", "Wenjing", ""]]}, {"id": "1710.09069", "submitter": "Ryan Cumings", "authors": "Ryan Cumings-Menon", "title": "Shape-Constrained Density Estimation via Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraining the maximum likelihood density estimator to satisfy a\nsufficiently strong constraint, $\\log-$concavity being a common example, has\nthe effect of restoring consistency without requiring additional parameters.\nSince many results in economics require densities to satisfy a regularity\ncondition, these estimators are also attractive for the structural estimation\nof economic models. In all of the examples of regularity conditions provided by\nBagnoli and Bergstrom (2005) and Ewerhart (2013), $\\log-$concavity is\nsufficient to ensure that the density satisfies the required conditions.\nHowever, in many cases $\\log-$concavity is far from necessary, and it has the\nunfortunate side effect of ruling out sub-exponential tail behavior.\n  In this paper, we use optimal transport to formulate a shape constrained\ndensity estimator. We initially describe the estimator using a $\\rho-$concavity\nconstraint. In this setting we provide results on consistency, asymptotic\ndistribution, convexity of the optimization problem defining the estimator, and\nformulate a test for the null hypothesis that the population density satisfies\na shape constraint. Afterward, we provide sufficient conditions for these\nresults to hold using an arbitrary shape constraint. This generalization is\nused to explore whether the California Department of Transportation's decision\nto award construction contracts with the use of a first price auction is cost\nminimizing. We estimate the marginal costs of construction firms subject to\nMyerson's (1981) regularity condition, which is a requirement for the first\nprice reverse auction to be cost minimizing. The proposed test fails to reject\nthat the regularity condition is satisfied.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 04:08:29 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 01:26:23 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Cumings-Menon", "Ryan", ""]]}, {"id": "1710.09095", "submitter": "Sophie Achard", "authors": "Sophie Achard (1), Ir\\`ene Gannaz (2), Marianne Clausel, Fran\\c{c}ois\n  Roueff (3) ((1) GIPSA-CICS, (2) ICJ, (3) LTCI)", "title": "New results on approximate Hilbert pairs of wavelet filters with common\n  factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the design of wavelet filters based on the Thiran\ncommon-factor approach proposed in Selesnick [2001]. This approach aims at\nbuilding finite impulseresponse filters of a Hilbert-pair of wavelets serving\nas real and imaginary part of a complexwavelet. Unfortunately it is not\npossible to construct wavelets which are both finitelysupported and analytic.\nThe wavelet filters constructed using the common-factor approachare then\napproximately analytic. Thus, it is of interest to control their analyticity.\nThepurpose of this paper is to first provide precise and explicit expressions\nas well as easilyexploitable bounds for quantifying the analytic approximation\nof this complex wavelet.Then, we prove the existence of such filters enjoying\nthe classical perfect reconstructionconditions, with arbitrarily many vanishing\nmoments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 07:19:59 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Achard", "Sophie", "", "GIPSA-CICS"], ["Gannaz", "Ir\u00e8ne", "", "ICJ"], ["Clausel", "Marianne", "", "LTCI"], ["Roueff", "Fran\u00e7ois", "", "LTCI"]]}, {"id": "1710.09116", "submitter": "Roberto Benedetti", "authors": "Roberto Benedetti and Federica Piersimoni", "title": "Fast Selection of Spatially Balanced Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from very large spatial populations is challenging. The solutions\nsuggested in recent literature on this subject often require that the randomly\nselected units are well distributed across the study region by using complex\nalgorithms that have the feature, essential in a design-based framework, to\nrespect the fixed first-order inclusion probabilities for every unit of the\npopulation. The size of the frame, $N$, often causes some problems to these\nalgorithms since, being based on the distance matrix between the units of the\npopulation, have at least a computational cost of order $N^2$. In this paper we\npropose a draw-by-draw algorithm that randomly selects a sample of size $n$ in\nexactly $n$ steps, updating at each step the selection probability of\nnot-selected units depending on their distance from the units already selected\nin the previous steps. The performance of this solution is compared with those\nof other methods derived from the {\\it spatially balanced sampling} literature\nin terms of their root mean squared error (RMSE) using the simple random\nsampling (SRS) without replacement as benchmark. The fundamental interest is\nnot only to evaluate the efficiency of a such different procedure, but also to\nunderstand if similar results can be obtained even with a notable reduction in\nthe computational burden needed to obtain more efficient sampling designs.\nRepeated sample selections on real and simulated populations support this\nperspective. An application to the Land Use and Land Cover Survey (LUCAS) 2012\ndata-set in an Italian region is presented as a concrete and practical\nillustration of the capabilities of the proposed sample selection method.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 08:29:15 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Benedetti", "Roberto", ""], ["Piersimoni", "Federica", ""]]}, {"id": "1710.09326", "submitter": "Jaron Arbet", "authors": "Jaron Arbet, Matt McGue, Saonli Basu", "title": "A Robust and Unified Framework for Estimating Heritability in Twin\n  Studies using Generalized Estimating Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a complex disease is an intricate interplay of genetic and\nenvironmental factors. \"Heritability\" is defined as the proportion of total\ntrait variance due to genetic factors within a given population. Studies with\nmonozygotic (MZ) and dizygotic (DZ) twins allow us to estimate heritability by\nfitting an \"ACE\" model which estimates the proportion of trait variance\nexplained by additive genetic (A), common shared environment (C), and unique\nnon-shared environmental (E) latent effects, thus helping us better understand\ndisease risk and etiology. In this paper, we develop a flexible generalized\nestimating equations framework (\"GEE2\") for fitting twin ACE models that\nrequires minimal distributional assumptions, rather only the first two moments\nneed to be correctly specified. We prove that two commonly used methods for\nestimating heritability, the normal ACE model (\"NACE\") and Falconer's method,\ncan both be fit within this unified GEE2 framework, which additionally provides\nrobust standard errors. Although the traditional Falconer's method cannot\ndirectly adjust for covariates, we show that the corresponding GEE2 version\n(\"GEE2-Falconer\") can incorporate covariate effects for both mean and\nvariance-level parameters (e.g. let heritability vary by sex or age). Given\nnon-normal data, we show that the GEE2 models attain significantly better\ncoverage of the true heritability compared to the traditional NACE and\nFalconer's methods. Finally, we demonstrate an important scenario where the\nNACE model produces biased estimates of heritability while Falconer's method\nremains unbiased. Overall, we recommend using the robust and flexible\nGEE2-Falconer model for estimating heritability in twin studies.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 16:32:00 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 20:37:10 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 22:58:10 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Arbet", "Jaron", ""], ["McGue", "Matt", ""], ["Basu", "Saonli", ""]]}, {"id": "1710.09461", "submitter": "Itay Kavaler", "authors": "Itay Kavaler and Rann Smorodinsky", "title": "On Comparison Of Experts", "comments": "Journal title: Games and Economic Behavior. Article accepted for\n  publication: 19-AUG-2019", "journal-ref": "Games and Economic Behavior., Volume 118, November 2019, Pages\n  94-109", "doi": "10.1016/j.geb.2019.08.005", "report-no": null, "categories": "stat.ME cs.GT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A policy maker faces a sequence of unknown outcomes. At each stage two\n(self-proclaimed) experts provide probabilistic forecasts on the outcome in the\nnext stage. A comparison test is a protocol for the policy maker to\n(eventually) decide which of the two experts is better informed. The protocol\ntakes as input the sequence of pairs of forecasts and actual realizations and\n(weakly) ranks the two experts. We propose two natural properties that such a\ncomparison test must adhere to and show that these essentially uniquely\ndetermine the comparison test. This test is a function of the derivative of the\ninduced pair of measures at the realization.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 17:22:38 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 21:51:00 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 10:32:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kavaler", "Itay", ""], ["Smorodinsky", "Rann", ""]]}, {"id": "1710.09551", "submitter": "Yi Yang", "authors": "Yi Yang, Mingwei Dai, Jian Huang, Xinyi Lin, Can Yang, Jin Liu, and\n  Min Chen", "title": "LPG: a four-groups probabilistic approach to leveraging pleiotropy in\n  genome-wide association studies", "comments": "81 page (include supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, genome-wide association studies (GWAS) have successfully identified\ntens of thousands of genetic variants among a variety of traits/diseases,\nshedding a light on the genetic architecture of complex diseases. Polygenicity\nof complex diseases, which refers to the phenomenon that a vast number of risk\nvariants collectively contribute to the heritability of complex diseases with\nmodest individual effects, have been widely accepted. This imposes a major\nchallenge towards fully characterizing the genetic bases of complex diseases.\nAn immediate implication of polygenicity is that a much larger sample size is\nrequired to detect risk variants with weak/moderate effects. Meanwhile,\naccumulating evidence suggests that different complex diseases can share\ngenetic risk variants, a phenomenon known as pleiotropy. In this study, we\npropose a statistical framework for Leveraging Pleiotropic effects in\nlarge-scale GWAS data (LPG). LPG utilizes a variational Bayesian\nexpectation-maximization (VBEM) algorithm, making it computationally efficient\nand scalable for genome-wide scale analysis. To demon- strate the advantage of\nLPG over existing methods that do not leverage pleiotropy, we conducted\nextensive simulation studies and also applied LPG to analyze three au- toimmune\ndisorders (Crohn's disease, rheumatoid arthritis and Type 1 diabetes). The\nresults indicate that LPG can improve the power of prioritization of risk\nvariants and accuracy of risk prediction by leveraging pleiotropy. The software\nis available at http- s://github.com/Shufeyangyi2015310117/LPG.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 06:02:43 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Yang", "Yi", ""], ["Dai", "Mingwei", ""], ["Huang", "Jian", ""], ["Lin", "Xinyi", ""], ["Yang", "Can", ""], ["Liu", "Jin", ""], ["Chen", "Min", ""]]}, {"id": "1710.09588", "submitter": "Caleb Miles", "authors": "Caleb H. Miles, Maya Petersen, Mark J. van der Laan", "title": "Causal Inference When Counterfactuals Depend on the Proportion of All\n  Subjects Exposed", "comments": "23 pages main article, 23 pages supplementary materials + references,\n  4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption that no subject's exposure affects another subject's outcome,\nknown as the no-interference assumption, has long held a foundational position\nin the study of causal inference. However, this assumption may be violated in\nmany settings, and in recent years has been relaxed considerably. Often this\nhas been achieved with either the aid of a known underlying network, or the\nassumption that the population can be partitioned into separate groups, between\nwhich there is no interference, and within which each subject's outcome may be\naffected by all the other subjects in the group via the proportion exposed (the\nstratified interference assumption). In this paper, we instead consider a\ncomplete interference setting, in which each subject affects every other\nsubject's outcome. In particular, we make the stratified interference\nassumption for a single group consisting of the entire sample. This can occur\nwhen the exposure is a shared resource whose efficacy is modified by the number\nof subjects among whom it is shared. We show that a targeted maximum likelihood\nestimator for the i.i.d.~setting can be used to estimate a class of causal\nparameters that includes direct effects and overall effects under certain\ninterventions. This estimator remains doubly-robust, semiparametric efficient,\nand continues to allow for incorporation of machine learning under our model.\nWe conduct a simulation study, and present results from a data application\nwhere we study the effect of a nurse-based triage system on the outcomes of\npatients receiving HIV care in Kenyan health clinics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 08:41:46 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 23:28:42 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Miles", "Caleb H.", ""], ["Petersen", "Maya", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1710.09877", "submitter": "Minggang Wang", "authors": "Minggang Wang, Andre L.M.Vilela, Ruijin Du, Longfeng Zhao, Gaogao\n  Dong, Lixin Tian and H. Eugene Stanley", "title": "Exact results of the limited penetrable horizontal visibility graph\n  associated to random time series and its application", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited penetrable horizontal visibility algorithm is a new time analysis\ntool and is a further development of the horizontal visibility algorithm. We\npresent some exact results on the topological properties of the limited\npenetrable horizontal visibility graph associated with random series. We show\nthat the random series maps on a limited penetrable horizontal visibility graph\nwith exponential degree distribution $P(k)\\sim exp[-\\lambda (k-2\\rho-2)],\n\\lambda = ln[(2\\rho+3)/(2\\rho+2)],\\rho=0,1,2,...,k=2\\rho+2,2\\rho+3,...$,\nindependent of the probability distribution from which the series was\ngenerated. We deduce the exact expressions of the mean degree and the\nclustering coefficient and demonstrate the long distance visibility property.\nNumerical simulations confirm the accuracy of our theoretical results. We then\nexamine several deterministic chaotic series (a logistic map, the\nH$\\acute{e}$non map, the Lorentz system, and an energy price chaotic system)\nand a real crude oil price series to test our results. The empirical results\nshow that the limited penetrable horizontal visibility algorithm is direct, has\na low computational cost when discriminating chaos from uncorrelated\nrandomness, and is able to measure the global evolution characteristics of the\nreal time series.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:14:01 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Wang", "Minggang", ""], ["Vilela", "Andre L. M.", ""], ["Du", "Ruijin", ""], ["Zhao", "Longfeng", ""], ["Dong", "Gaogao", ""], ["Tian", "Lixin", ""], ["Stanley", "H. Eugene", ""]]}, {"id": "1710.09890", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou", "title": "Bayesian Nonparametric Models for Biomedical Data Analysis", "comments": "PhD thesis, UT Austin, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we develop nonparametric Bayesian models for biomedical\ndata analysis. In particular, we focus on inference for tumor heterogeneity and\ninference for missing data. First, we present a Bayesian feature allocation\nmodel for tumor subclone reconstruction using mutation pairs. The key\ninnovation lies in the use of short reads mapped to pairs of proximal single\nnucleotide variants (SNVs). In contrast, most existing methods use only\nmarginal reads for unpaired SNVs. In the same context of using mutation pairs,\nin order to recover the phylogenetic relationship of subclones, we then develop\na Bayesian treed feature allocation model. In contrast to commonly used feature\nallocation models, we allow the latent features to be dependent, using a tree\nstructure to introduce dependence. Finally, we propose a nonparametric Bayesian\napproach to monotone missing data in longitudinal studies with non-ignorable\nmissingness. In contrast to most existing methods, our method allows for\nincorporating information from auxiliary covariates and is able to capture\ncomplex structures among the response, missingness and auxiliary covariates.\nOur models are validated through simulation studies and are applied to\nreal-world biomedical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:39:36 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Zhou", "Tianjian", ""]]}, {"id": "1710.09923", "submitter": "Yingying Zhuang", "authors": "Yingying Zhuang, Ying Huang, Peter B. Gilbert", "title": "Evaluation of Treatment Effect Modification by Biomarkers Measured Pre-\n  and Post-randomization in the Presence of Non-monotone Missingness", "comments": "Manuscript is planned to submit November, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In vaccine studies, investigators are often interested in studying effect\nmodifiers of clinical treatment efficacy by biomarker-based principal strata,\nwhich is useful for selecting biomarker study endpoints for evaluating\ntreatments in new trials, exploring biological mechanisms of clinical treatment\nefficacy, and studying mediators of clinical treatment efficacy. However, in\ntrials where participants may enter the study with prior exposure therefore\nwith variable baseline biomarker values, clinical treatment efficacy may depend\njointly on a biomarker measured at baseline and measured at a fixed time after\nvaccination. Therefore, it is of interest to conduct a bivariate effect\nmodification analysis by biomarker-based principal strata and baseline\nbiomarker values. Previous methods allow this assessment if participants who\nhave the biomarker measured at the the fixed time point post randomization\nwould also have the biomarker measured at baseline. However, additional\ncomplications in study design could happen in practice. For example, in the\nDengue correlates study, baseline biomarker values were only available from a\nfraction of participants who have biomarkers measured post-randomization. How\nto conduct the bivariate effect modification analysis in these studies remains\nan open research question. In this article, we propose an estimated likelihood\nmethod to utilize the sub-sampled baseline biomarker in the effect modification\nanalysis and illustrate our method with datasets from two dengue phase 3\nvaccine efficacy trials.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 21:42:32 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Zhuang", "Yingying", ""], ["Huang", "Ying", ""], ["Gilbert", "Peter B.", ""]]}, {"id": "1710.09945", "submitter": "Frederic Pascal", "authors": "Gordana Draskovic and Frederic Pascal", "title": "New insights into the statistical properties of $M$-estimators", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 66, pp. 4253-4263,\n  August 2018", "doi": "10.1109/TSP.2018.2841892", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an original approach to better understanding the behavior\nof robust scatter matrix $M$-estimators. Scatter matrices are of particular\ninterest for many signal processing applications since the resulting\nperformance strongly relies on the quality of the matrix estimation. In this\ncontext, $M$-estimators appear as very interesting candidates, mainly due to\ntheir flexibility to the statistical model and their robustness to outliers\nand/or missing data. However, the behavior of such estimators still remains\nunclear and not well understood since they are described by fixed-point\nequations that make their statistical analysis very difficult. To fill this\ngap, the main contribution of this work is to prove that these estimators\ndistribution is more accurately described by a Wishart distribution than by the\nclassical asymptotical Gaussian approximation. To that end, we propose a new\n`Gaussian-core' representation for Complex Elliptically Symmetric (CES)\ndistributions and we analyze the proximity between $M$-estimators and a\nGaussian-based Sample Covariance Matrix (SCM), unobservable in practice and\nplaying only a theoretical role. To confirm our claims we also provide results\nfor a widely used function of $M$-estimators, the Mahalanobis distance.\nFinally, Monte Carlo simulations for various scenarios are presented to\nvalidate theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 23:22:50 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 22:23:10 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Draskovic", "Gordana", ""], ["Pascal", "Frederic", ""]]}, {"id": "1710.09975", "submitter": "Renato J Cintra", "authors": "A. Edirisuriya, A. Madanayake, R. J. Cintra, V. S. Dimitrov", "title": "A Single-Channel Architecture for Algebraic Integer Based 8$\\times$8 2-D\n  DCT Computation", "comments": "8 pages, 6 figures, 5 tables", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  volume 23, number 12, pages 2083-2089, Dec. 2013", "doi": "10.1109/TCSVT.2013.2270397", "report-no": null, "categories": "cs.AR cs.MM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An area efficient row-parallel architecture is proposed for the real-time\nimplementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine\ntransform (DCT) for image and video processing. The proposed architecture\ncomputes 8$\\times$8 2-D DCT transform based on the Arai DCT algorithm. An\nimproved fast algorithm for AI based 1-D DCT computation is proposed along with\na single channel 2-D DCT architecture. The design improves on the 4-channel AI\nDCT architecture that was published recently by reducing the number of integer\nchannels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The\narchitecture offers exact computation of 8$\\times$8 blocks of the 2-D DCT\ncoefficients up to the FRS, which converts the coefficients from the AI\nrepresentation to fixed-point format using the method of expansion factors.\nPrototype circuits corresponding to FRS blocks based on two expansion factors\nare realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6\nXC6VLX240T device. Post place-and-route results show a 20% reduction in terms\nof area compared to the 2-D DCT architecture requiring five 1-D AI cores. The\narea-time and area-time${}^2$ complexity metrics are also reduced by 23% and\n22% respectively for designs with 8-bit input word length. The digital\nrealizations are simulated up to place and route for ASICs using 45 nm CMOS\nstandard cells. The maximum estimated clock rate is 951 MHz for the CMOS\nrealizations indicating 7.608$\\cdot$10$^9$ pixels/seconds and a 8$\\times$8\nblock rate of 118.875 MHz.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 03:32:48 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.09982", "submitter": "Tiejun Tong", "authors": "Zongliang Hu, Tiejun Tong and Marc G. Genton", "title": "Diagonal Likelihood Ratio Test for Equality of Mean Vectors in\n  High-Dimensional Data", "comments": "38 pages, 5 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a likelihood ratio test framework for testing normal mean vectors\nin high-dimensional data under two common scenarios: the one-sample test and\nthe two-sample test with equal covariance matrices. We derive the test\nstatistics under the assumption that the covariance matrices follow a diagonal\nmatrix structure. In comparison with the diagonal Hotelling's tests, our\nproposed test statistics display some interesting characteristics. In\nparticular, they are a summation of the log-transformed squared $t$-statistics\nrather than a direct summation of those components. More importantly, to derive\nthe asymptotic normality of our test statistics under the null and local\nalternative hypotheses, we do not need the requirement that the covariance\nmatrices follow a diagonal matrix structure. As a consequence, our proposed\ntest methods are very flexible and readily applicable in practice. Simulation\nstudies and a real data analysis are also carried out to demonstrate the\nadvantages of our likelihood ratio test methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 04:03:30 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 10:20:03 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hu", "Zongliang", ""], ["Tong", "Tiejun", ""], ["Genton", "Marc G.", ""]]}, {"id": "1710.10102", "submitter": "Matthew Williams", "authors": "Matthew R. Williams and Terrance D. Savitsky", "title": "Bayesian Pairwise Estimation Under Dependent Informative Sampling", "comments": "35 pages, 9 figures", "journal-ref": "Electron. J. Statist. Volume 12, Number 1 (2018), 1631-1661", "doi": "10.1214/18-EJS1435", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An informative sampling design leads to the selection of units whose\ninclusion probabilities are correlated with the response variable of interest.\nModel inference performed on the resulting observed sample will be biased for\nthe population generative model. One approach that produces asymptotically\nunbiased inference employs marginal inclusion probabilities to form sampling\nweights used to exponentiate each likelihood contribution of a pseudo\nlikelihood used to form a pseudo posterior distribution. Conditions for\nposterior consistency restrict applicable sampling designs to those under which\npairwise inclusion dependencies asymptotically limit to 0. There are many\nsampling designs excluded by this restriction; for example, a multi-stage\ndesign that samples individuals within households. Viewing each household as a\npopulation, the dependence among individuals does not attenuate. We propose a\nmore targeted approach in this paper for inference focused on pairs of\nindividuals or sampled units; for example, the substance use of one spouse in a\nshared household, conditioned on the substance use of the other spouse. We\nformulate the pseudo likelihood with weights based on pairwise or second order\nprobabilities and demonstrate consistency, removing the requirement for\nasymptotic independence and replacing it with restrictions on higher order\nselection probabilities. Our approach provides a nearly automated estimation\nprocedure applicable to any model specified by the data analyst. We demonstrate\nour method on the National Survey on Drug Use and Health.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:17:04 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Williams", "Matthew R.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1710.10261", "submitter": "Marko Obradovi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c and Marko Obradovi\\'c", "title": "Comparison of Efficiencies of Symmetry Tests around Unknown Center", "comments": "The final version will appear in Statistics", "journal-ref": null, "doi": "10.1080/02331888.2018.1526938", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, some recent and classical tests of symmetry are modified for\nthe case of an unknown center.\n  The unknown center is estimated with its $\\alpha$-trimmed mean estimator.\n  The asymptotic behavior of the new tests is explored.\n  The local approximate Bahadur efficiency is used to compare the tests to each\nother as well as to some other tests.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:02:28 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 10:19:08 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1710.10279", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, John Choi, Bijan Pesaran, Demba Ba, and Vahid Tarokh", "title": "Wavelet Shrinkage and Thresholding based Robust Classification for Brain\n  Computer Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A macaque monkey is trained to perform two different kinds of tasks, memory\naided and visually aided. In each task, the monkey saccades to eight possible\ntarget locations. A classifier is proposed for direction decoding and task\ndecoding based on local field potentials (LFP) collected from the prefrontal\ncortex. The LFP time-series data is modeled in a nonparametric regression\nframework, as a function corrupted by Gaussian noise. It is shown that if the\nfunction belongs to Besov bodies, then using the proposed wavelet shrinkage and\nthresholding based classifier is robust and consistent. The classifier is then\napplied to the LFP data to achieve high decoding performance. The proposed\nclassifier is also quite general and can be applied for the classification of\nother types of time-series data as well, not necessarily brain data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 18:04:51 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 18:16:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Banerjee", "Taposh", ""], ["Choi", "John", ""], ["Pesaran", "Bijan", ""], ["Ba", "Demba", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1710.10342", "submitter": "Nicole Pashley", "authors": "Nicole E. Pashley and Luke W. Miratrix", "title": "Insights on Variance Estimation for Blocked and Matched Pairs Designs", "comments": null, "journal-ref": "Journal of Educational and Behavioral Statistics, 46(3) (2021) p.\n  271-296", "doi": "10.3102/1076998620946272", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating blocked randomized experiments from a potential outcomes\nperspective has two primary branches of work. The first focuses on larger\nblocks, with multiple treatment and control units in each block. The second\nfocuses on matched pairs, with a single treatment and control unit in each\nblock. These literatures not only provide different estimators for the standard\nerrors of the estimated average impact, but they are also built on different\nsets of assumptions. Neither literature handles cases with blocks of varying\nsize that contain singleton treatment or control units, a case which can occur\nin a variety of contexts, such as with different forms of matching or\npost-stratification. In this paper, we reconcile the literatures by carefully\nexamining the performance of variance estimators under several different\nframeworks. We then use these insights to derive novel variance estimators for\nexperiments containing blocks of different sizes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 21:45:29 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 21:57:16 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 17:58:43 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 15:13:07 GMT"}, {"version": "v5", "created": "Wed, 3 Jul 2019 00:06:14 GMT"}, {"version": "v6", "created": "Mon, 29 Jun 2020 16:45:48 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Pashley", "Nicole E.", ""], ["Miratrix", "Luke W.", ""]]}, {"id": "1710.10351", "submitter": "Andrew Brown", "authors": "D. Andrew Brown, Christopher S. McMahan, Russell T. Shinohara, Kristin\n  A. Linn", "title": "Bayesian Spatial Binary Regression for Label Fusion in Structural\n  Neuroimaging", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many analyses of neuroimaging data involve studying one or more regions of\ninterest (ROIs) in a brain image. In order to do so, each ROI must first be\nidentified. Since every brain is unique, the location, size, and shape of each\nROI varies across subjects. Thus, each ROI in a brain image must either be\nmanually identified or (semi-) automatically delineated, a task referred to as\nsegmentation. Automatic segmentation often involves mapping a previously\nmanually segmented image to a new brain image and propagating the labels to\nobtain an estimate of where each ROI is located in the new image. A more recent\napproach to this problem is to propagate labels from multiple manually\nsegmented atlases and combine the results using a process known as label\nfusion. To date, most label fusion algorithms either employ voting procedures\nor impose prior structure and subsequently find the maximum a posteriori\nestimator (i.e., the posterior mode) through optimization. We propose using a\nfully Bayesian spatial regression model for label fusion that facilitates\ndirect incorporation of covariate information while making accessible the\nentire posterior distribution. We discuss the implementation of our model via\nMarkov chain Monte Carlo and illustrate the procedure through both simulation\nand application to segmentation of the hippocampus, an anatomical structure\nknown to be associated with Alzheimer's disease.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 22:24:24 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 14:14:14 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Brown", "D. Andrew", ""], ["McMahan", "Christopher S.", ""], ["Shinohara", "Russell T.", ""], ["Linn", "Kristin A.", ""]]}, {"id": "1710.10382", "submitter": "HaiYing Wang", "authors": "HaiYing Wang, Min Yang, and John Stufken", "title": "Information-Based Optimal Subdata Selection for Big Data Linear\n  Regression", "comments": "56 pages, 18 figures", "journal-ref": null, "doi": "10.1080/01621459.2017.1408468", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraordinary amounts of data are being produced in many branches of science.\nProven statistical methods are no longer applicable with extraordinary large\ndata sets due to computational limitations. A critical step in big data\nanalysis is data reduction. Existing investigations in the context of linear\nregression focus on subsampling-based methods. However, not only is this\napproach prone to sampling errors, it also leads to a covariance matrix of the\nestimators that is typically bounded from below by a term that is of the order\nof the inverse of the subdata size. We propose a novel approach, termed\ninformation-based optimal subdata selection (IBOSS). Compared to leading\nexisting subdata methods, the IBOSS approach has the following advantages: (i)\nit is significantly faster; (ii) it is suitable for distributed parallel\ncomputing; (iii) the variances of the slope parameter estimators converge to 0\nas the full data size increases even if the subdata size is fixed, i.e., the\nconvergence rate depends on the full data size; (iv) data analysis for IBOSS\nsubdata is straightforward and the sampling distribution of an IBOSS estimator\nis easy to assess. Theoretical results and extensive simulations demonstrate\nthat the IBOSS approach is superior to subsampling-based methods, sometimes by\norders of magnitude. The advantages of the new approach are also illustrated\nthrough analysis of real data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 03:25:11 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""], ["Yang", "Min", ""], ["Stufken", "John", ""]]}, {"id": "1710.10489", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "ILAPF: Incremental Learning Assisted Particle Filtering", "comments": "5 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with dynamic system state estimation based on a\nseries of noisy measurement with the presence of outliers. An incremental\nlearning assisted particle filtering (ILAPF) method is presented, which can\nlearn the value range of outliers incrementally during the process of particle\nfiltering. The learned range of outliers is then used to improve subsequent\nfiltering of the future state. Convergence of the outlier range estimation\nprocedure is indicated by extensive empirical simulations using a set of\ndiffering outlier distribution models. The validity of the ILAPF algorithm is\nevaluated by illustrative simulations, and the result shows that ILAPF is more\naccurate and faster than a recently published state-ofthe-art robust particle\nfilter. It also shows that the incremental learning property of the ILAPF\nalgorithm provides an efficient way to implement transfer learning among\nrelated state filtering tasks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 16:11:45 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 09:53:58 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1710.10526", "submitter": "Holger Dette", "authors": "Holger Dette, Maria Konstantinou, Kirsten Schorning, Josua G\\\"osmann", "title": "Optimal designs for regression with spherical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper optimal designs for regression problems with spherical\npredictors of arbitrary dimension are considered. Our work is motivated by\napplications in material sciences, where crystallographic textures such as the\nmissorientation distribution or the grain boundary distribution (depending on a\nfour dimensional spherical predictor) are represented by series of\nhyperspherical harmonics, which are estimated from experimental or simulated\ndata. For this type of estimation problems we explicitly determine optimal\ndesigns with respect to Kiefers $\\Phi_p$-criteria and a class of orthogonally\ninvariant information criteria recently introduced in the literature. In\nparticular, we show that the uniform distribution on the $m$-dimensional sphere\nis optimal and construct discrete and implementable designs with the same\ninformation matrices as the continuous optimal designs. Finally, we illustrate\nthe advantages of the new designs for series estimation by hyperspherical\nharmonics, which are symmetric with respect to the first and second\ncrystallographic point group.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 20:13:20 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Dette", "Holger", ""], ["Konstantinou", "Maria", ""], ["Schorning", "Kirsten", ""], ["G\u00f6smann", "Josua", ""]]}, {"id": "1710.10558", "submitter": "Brendan McVeigh", "authors": "Brendan S. McVeigh, Jared S. Murray", "title": "Practical Bayesian Inference for Record Linkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic record linkage (PRL) is the process of determining which\nrecords in two databases correspond to the same underlying entity in the\nabsence of a unique identifier. Bayesian solutions to this problem provide a\npowerful mechanism for propagating uncertainty due to uncertain links between\nrecords (via the posterior distribution). However, computational considerations\nseverely limit the practical applicability of existing Bayesian approaches. We\npropose a new computational approach, providing both a fast algorithm for\nderiving point estimates of the linkage structure that properly account for\none-to-one matching and a restricted MCMC algorithm that samples from an\napproximate posterior distribution. Our advances make it possible to perform\nBayesian PRL for larger problems, and to assess the sensitivity of results to\nvarying prior specifications. We demonstrate the methods on simulated data and\nan application to a post-enumeration survey for coverage estimation in the\nItalian census.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 03:41:35 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 22:27:13 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["McVeigh", "Brendan S.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1710.10626", "submitter": "Eric Lock", "authors": "Eric F Lock, Nidhi Kohli, and Maitreyee Bose", "title": "Detecting Multiple Random Changepoints in Bayesian Piecewise Growth\n  Mixture Models", "comments": "32 pages, 10 tables, 6 figures", "journal-ref": "Psychometrika 83(3), 733-750, 2018", "doi": "10.1007/s11336-017-9594-5", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise growth mixture models (PGMM) are a flexible and useful class of\nmethods for analyzing segmented trends in individual growth trajectory over\ntime, where the individuals come from a mixture of two or more latent classes.\nThese models allow each segment of the overall developmental process within\neach class to have a different functional form; examples include two linear\nphases of growth, or a quadratic phase followed by a linear phase. The\nchangepoint (knot) is the time of transition from one developmental phase\n(segment) to another. Inferring the location of the changepoint(s) is often of\npractical interest, along with inference for other model parameters. A random\nchangepoint allows for individual differences in the transition time within\neach class. The primary objectives of our study are: (1) to develop a PGMM\nusing a Bayesian inference approach that allows the estimation of multiple\nrandom changepoints within each class; (2) to develop a procedure to\nempirically detect the number of random changepoints within each class; and (3)\nto empirically investigate the bias and precision of the estimation of the\nmodel parameters, including the random changepoints, via a simulation study. We\nhave developed the user-friendly package BayesianPGMM for R to facilitate the\nadoption of this methodology in practice, which is available at\nhttps://github.com/lockEF/BayesianPGMM . We describe an application to\nmouse-tracking data for a visual recognition task.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 15:11:00 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Lock", "Eric F", ""], ["Kohli", "Nidhi", ""], ["Bose", "Maitreyee", ""]]}, {"id": "1710.10709", "submitter": "Debraj Das", "authors": "Debraj Das and S. N. Lahiri", "title": "Distributional Consistency of Lasso by Perturbation Bootstrap", "comments": "17 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least Absolute Shrinkage and Selection Operator or the Lasso, introduced by\nTibshirani (1996), is a popular estimation procedure in multiple linear\nregression when underlying design has a sparse structure, because of its\nproperty that it sets some regression coefficients exactly equal to 0. In this\narticle, we develop a perturbation bootstrap method and establish its validity\nin approximating the distribution of the Lasso in heteroscedastic linear\nregression. We allow the underlying covariates to be either random or\nnon-random. We show that the proposed bootstrap method works irrespective of\nthe nature of the covariates, unlike the resample-based bootstrap of Freedman\n(1981) which must be tailored based on the nature (random vs non-random) of the\ncovariates. Simulation study also justifies our method in finite samples.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 22:36:48 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Das", "Debraj", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1710.10713", "submitter": "Chiyu Gu", "authors": "Chiyu Gu, Veerabhadran Baladandayuthapani, Subharup Guha", "title": "Nonparametric Bayes Differential Analysis for Dependent Multigroup Data\n  with Application to DNA Methylation Analyses in Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cancer genomics datasets involve widely varying sizes and scales,\nmeasurement variables, and correlation structures. A fundamental analytical\ngoal in these high-throughput studies is the development of general statistical\ntechniques that can cleanly sift the signal from noise in identifying\ndisease-specific genomic signatures across a set of experimental or biological\nconditions. We propose BayesDiff, a nonparametric Bayesian approach based on a\nnovel class of first order mixture models, called the Sticky Poisson-Dirichlet\nprocess or multicuisine restaurant franchise. The BayesDiff methodology\nflexibly utilizes information from all the measurements and adaptively\naccommodates any serial dependence in the data, accounting for the inter-probe\ndistances, to perform simultaneous inferences on the variables. The technique\nis applied to analyze a DNA methylation gastrointestinal (GI) cancer dataset,\nwhich displays both serial correlations and complex interaction patterns. Our\nanalyses and results both support and complement known aspects of DNA\nmethylation and gene association in upper GI cancers. In simulation studies, we\ndemonstrate the effectiveness of the BayesDiff procedure relative to existing\ntechniques for differential DNA methylation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 23:02:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 03:33:34 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 16:47:05 GMT"}, {"version": "v4", "created": "Tue, 10 Mar 2020 01:24:32 GMT"}, {"version": "v5", "created": "Fri, 17 Apr 2020 02:58:30 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gu", "Chiyu", ""], ["Baladandayuthapani", "Veerabhadran", ""], ["Guha", "Subharup", ""]]}, {"id": "1710.10742", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei", "title": "Implicit Causal Models for Genome-wide Association Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in probabilistic generative models has accelerated, developing\nricher models with neural architectures, implicit densities, and with scalable\nalgorithms for their Bayesian inference. However, there has been limited\nprogress in models that capture causal relationships, for example, how\nindividual genetic factors cause major human diseases. In this work, we focus\non two challenges in particular: How do we build richer causal models, which\ncan capture highly nonlinear relationships and interactions between multiple\ncauses? How do we adjust for latent confounders, which are variables\ninfluencing both cause and effect and which prevent learning of causal\nrelationships? To address these challenges, we synthesize ideas from causality\nand modern probabilistic modeling. For the first, we describe implicit causal\nmodels, a class of causal models that leverages neural architectures with an\nimplicit density. For the second, we describe an implicit causal model that\nadjusts for confounders by sharing strength across examples. In experiments, we\nscale Bayesian inference on up to a billion genetic measurements. We achieve\nstate of the art accuracy for identifying causal factors: we significantly\noutperform existing genetics methods by an absolute difference of 15-45.3%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:05:10 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1710.10870", "submitter": "Mathias Trabs", "authors": "Denis Belomestny, Mathias Trabs and Alexandre B. Tsybakov", "title": "Sparse covariance matrix estimation in high-dimensional deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of the covariance matrix $\\Sigma$ of a\n$p$-dimensional normal random vector based on $n$ independent observations\ncorrupted by additive noise. Only a general nonparametric assumption is imposed\non the distribution of the noise without any sparsity constraint on its\ncovariance matrix. In this high-dimensional semiparametric deconvolution\nproblem, we propose spectral thresholding estimators that are adaptive to the\nsparsity of $\\Sigma$. We establish an oracle inequality for these estimators\nunder model miss-specification and derive non-asymptotic minimax convergence\nrates that are shown to be logarithmic in $n/\\log p$. We also discuss the\nestimation of low-rank matrices based on indirect observations as well as the\ngeneralization to elliptical distributions. The finite sample performance of\nthe threshold estimators is illustrated in a numerical example.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 11:14:25 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 08:39:49 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Belomestny", "Denis", ""], ["Trabs", "Mathias", ""], ["Tsybakov", "Alexandre B.", ""]]}, {"id": "1710.10921", "submitter": "Marianna Pensky", "authors": "Felix Abramovich, Daniela De Canditiis and Marianna Pensky", "title": "Solution of linear ill-posed problems by model selection and aggregation", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general statistical linear inverse problem, where the solution\nis represented via a known (possibly overcomplete) dictionary that allows its\nsparse representation. We propose two different approaches. A model selection\nestimator selects a single model by minimizing the penalized empirical risk\nover all possible models. By contrast with direct problems, the penalty depends\non the model itself rather than on its size only as for complexity penalties. A\nQ-aggregate estimator averages over the entire collection of estimators with\nproperly chosen weights. Under mild conditions on the dictionary, we establish\noracle inequalities both with high probability and in expectation for the two\nestimators. Moreover, for the latter estimator these inequalities are sharp.\nThe proposed procedures are implemented numerically and their performance is\nassessed by a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 13:08:28 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Abramovich", "Felix", ""], ["De Canditiis", "Daniela", ""], ["Pensky", "Marianna", ""]]}, {"id": "1710.10936", "submitter": "Minh Tang", "authors": "Minh Tang and Joshua Cape and Carey E. Priebe", "title": "Asymptotically efficient estimators for stochastic blockmodels: the\n  naive MLE, the rank-constrained MLE, and the spectral", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish asymptotic normality results for estimation of the block\nprobability matrix $\\mathbf{B}$ in stochastic blockmodel graphs using spectral\nembedding when the average degrees grows at the rate of $\\omega(\\sqrt{n})$ in\n$n$, the number of vertices. As a corollary, we show that when $\\mathbf{B}$ is\nof full-rank, estimates of $\\mathbf{B}$ obtained from spectral embedding are\nasymptotically efficient. When $\\mathbf{B}$ is singular the estimates obtained\nfrom spectral embedding can have smaller mean square error than those obtained\nfrom maximizing the log-likelihood under no rank assumption, and furthermore,\ncan be almost as efficient as the true MLE that assume known\n$\\mathrm{rk}(\\mathbf{B})$. Our results indicate, in the context of stochastic\nblockmodel graphs, that spectral embedding is not just computationally\ntractable, but that the resulting estimates are also admissible, even when\ncompared to the purportedly optimal but computationally intractable maximum\nlikelihood estimation under no rank assumption.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 13:44:06 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tang", "Minh", ""], ["Cape", "Joshua", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1710.10980", "submitter": "Giulio Cimini", "authors": "Matteo Serafino, Andrea Gabrielli, Guido Caldarelli, Giulio Cimini", "title": "Statistical validation of financial time series via visibility graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.SI physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical physics of complex systems exploits network theory not only to\nmodel, but also to effectively extract information from many dynamical\nreal-world systems. A pivotal case of study is given by financial systems:\nmarket prediction represents an unsolved scientific challenge yet with crucial\nimplications for society, as financial crises have devastating effects on real\neconomies. Thus, nowadays the quest for a robust estimator of market efficiency\nis both a scientific and institutional priority. In this work we study the\nvisibility graphs built from the time series of several trade market indices.\nWe propose a validation procedure for each link of these graphs against a null\nhypothesis derived from ARCH-type modeling of such series. Building on this\nframework, we devise a market indicator that turns out to be highly correlated\nand even predictive of financial instability periods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:38:04 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Serafino", "Matteo", ""], ["Gabrielli", "Andrea", ""], ["Caldarelli", "Guido", ""], ["Cimini", "Giulio", ""]]}, {"id": "1710.11028", "submitter": "Ghislain Durif", "authors": "Ghislain Durif, Laurent Modolo, Jeff E. Mold, Sophie Lambert-Lacroix\n  and Franck Picard", "title": "Probabilistic Count Matrix Factorization for Single Cell Expression Data\n  Analysis", "comments": "22 pages, 5 figures, 1 table, Appendix 19 pages, 13 figures", "journal-ref": "Bioinformatics 35, 4011-4019 (2019)", "doi": "10.1093/bioinformatics/btz177", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of high throughput single-cell sequencing technologies now\nallows the investigation of the population level diversity of cellular\ntranscriptomes. This diversity has shown two faces. First, the expression\ndynamics (gene to gene variability) can be quantified more accurately, thanks\nto the measurement of lowly-expressed genes. Second, the cell-to-cell\nvariability is high, with a low proportion of cells expressing the same gene at\nthe same time/level. Those emerging patterns appear to be very challenging from\nthe statistical point of view, especially to represent and to provide a\nsummarized view of single-cell expression data. PCA is one of the most powerful\nframework to provide a suitable representation of high dimensional datasets, by\nsearching for latent directions catching the most variability in the data.\nUnfortunately, classical PCA is based on Euclidean distances and projections\nthat work poorly in presence of over-dispersed counts that show drop-out events\n(zero-inflation) like single-cell expression data We propose a probabilistic\nCount Matrix Factorization (pCMF) approach for single-cell expression data\nanalysis, that relies on a sparse Gamma-Poisson factor model. This hierarchical\nmodel is inferred using a variational EM algorithm. It is able to jointly build\na low dimensional representation of cells and genes. We show how this\nprobabilistic framework induces a geometry that is suitable for single-cell\ndata visualization, and produces a compression of the data that is very\npowerful for clustering purposes. Our method is competed against other standard\nrepresentation methods like t-SNE, and we illustrate its performance for the\nrepresentation of single-cell data. We especially focus on publicly available\nsingle-cell RNA-seq datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 15:58:02 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 14:38:20 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 12:40:08 GMT"}, {"version": "v4", "created": "Thu, 7 Feb 2019 10:39:50 GMT"}, {"version": "v5", "created": "Tue, 12 Mar 2019 09:58:45 GMT"}], "update_date": "2021-04-10", "authors_parsed": [["Durif", "Ghislain", ""], ["Modolo", "Laurent", ""], ["Mold", "Jeff E.", ""], ["Lambert-Lacroix", "Sophie", ""], ["Picard", "Franck", ""]]}, {"id": "1710.11172", "submitter": "Juliana Scudilio", "authors": "Juliana Scudilio and Gustavo H. A. Pereira", "title": "Adjusted quantile residual for generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models are widely used in many areas of knowledge. As in\nother classes of regression models, it is desirable to perform diagnostic\nanalysis in generalized linear models using residuals that are approximately\nstandard normally distributed. Diagnostic analysis in this class of models are\nusually performed using the standardized Pearson residual or the standardized\ndeviance residual. The former has skewed distribution and the latter has\nnegative mean, specially when the variance of the response variable is high. In\nthis work, we introduce the adjusted quantile residual for generalized linear\nmodels. Using Monte Carlo simulation techniques and two applications, we\ncompare this residual with the standardized Pearson residual, the standardized\ndeviance residual and two other residuals. Overall, the results suggest that\nthe adjusted quantile residual is a better tool for diagnostic analysis in\ngeneralized linear models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:29:46 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Scudilio", "Juliana", ""], ["Pereira", "Gustavo H. A.", ""]]}, {"id": "1710.11200", "submitter": "Renato J Cintra", "authors": "N. Rajapaksha, A. Madanayake, R. J. Cintra, J. Adikari, V. S. Dimitrov", "title": "VLSI Computational Architectures for the Arithmetic Cosine Transform", "comments": "8 pages, 2 figures, 6 tables", "journal-ref": "IEEE Transactions on Computers, vol. 64, no. 9, Sep 2015", "doi": "10.1109/TC.2014.2366732", "report-no": null, "categories": "cs.AR cs.DS cs.MM math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is a widely-used and important signal\nprocessing tool employed in a plethora of applications. Typical fast algorithms\nfor nearly-exact computation of DCT require floating point arithmetic, are\nmultiplier intensive, and accumulate round-off errors. Recently proposed fast\nalgorithm arithmetic cosine transform (ACT) calculates the DCT exactly using\nonly additions and integer constant multiplications, with very low area\ncomplexity, for null mean input sequences. The ACT can also be computed\nnon-exactly for any input sequence, with low area complexity and low power\nconsumption, utilizing the novel architecture described. However, as a\ntrade-off, the ACT algorithm requires 10 non-uniformly sampled data points to\ncalculate the 8-point DCT. This requirement can easily be satisfied for\napplications dealing with spatial signals such as image sensors and biomedical\nsensor arrays, by placing sensor elements in a non-uniform grid. In this work,\na hardware architecture for the computation of the null mean ACT is proposed,\nfollowed by a novel architectures that extend the ACT for non-null mean\nsignals. All circuits are physically implemented and tested using the Xilinx\nXC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for\nperformance assessment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:06:19 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rajapaksha", "N.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Adikari", "J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.11217", "submitter": "Claudia Di Caterina", "authors": "C. Di Caterina and I. Kosmidis", "title": "Location-adjusted Wald statistics for scalar parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference about a scalar parameter of interest is a core statistical task\nthat has attracted immense research in statistics. The Wald statistic is a\nprime candidate for the task, on the grounds of the asymptotic validity of the\nstandard normal approximation to its finite-sample distribution, simplicity and\nlow computational cost. It is well known, though, that this normal\napproximation can be inadequate, especially when the sample size is small or\nmoderate relative to the number of parameters. A novel, algebraic adjustment to\nthe Wald statistic is proposed, delivering significant improvements in\ninferential performance with only small implementation and computational\noverhead, predominantly due to additional matrix multiplications. The Wald\nstatistic is viewed as an estimate of a transformation of the model parameters\nand is appropriately adjusted, using either maximum likelihood or reduced-bias\nestimators, bringing its expectation asymptotically closer to zero. The\nlocation adjustment depends on the expected information, an approximation to\nthe bias of the estimator, and the derivatives of the transformation, which are\nall either readily available or easily obtainable in standard software for a\nwealth of models. An algorithm for the implementation of the location-adjusted\nWald statistics in general models is provided, as well as a bootstrap scheme\nfor the further scale correction of the location-adjusted statistic. Ample\nanalytical and numerical evidence is presented for the adoption of the\nlocation-adjusted statistic in prominent modelling settings, including\ninference about log-odds and binomial proportions, logistic regression in the\npresence of nuisance parameters, beta regression, and gamma regression. The\nlocation-adjusted Wald statistics are used for the construction of significance\nmaps for the analysis of multiple sclerosis lesions from MRI data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:59:57 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 15:07:19 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 13:36:26 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2019 19:07:52 GMT"}, {"version": "v5", "created": "Mon, 11 Mar 2019 17:09:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Di Caterina", "C.", ""], ["Kosmidis", "I.", ""]]}, {"id": "1710.11292", "submitter": "Dalia Chakrabarty Dr.", "authors": "Kangrui Wang, and Dalia Chakrabarty", "title": "Correlation between Multivariate Datasets, from Inter-Graph Distance\n  computed using Graphical Models Learnt With Uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for simultaneous Bayesian learning of the correlation\nmatrix and graphical model of a multivariate dataset, along with uncertainties\nin each, to subsequently compute distance between the learnt graphical models\nof a pair of datasets, using a new metric that approximates an\nuncertainty-normalised Hellinger distance between the posterior probabilities\nof the graphical models given the respective dataset; correlation between the\npair of datasets is then computed as a corresponding affinity measure. We\nachieve a closed-form likelihood of the between-columns correlation matrix by\nmarginalising over the between-row matrices. This between-columns correlation\nis updated first, given the data, and the graph is then updated, given the\npartial correlation matrix that is computed given the updated correlation,\nallowing for learning of the 95$\\%$ Highest Probability Density credible\nregions of the correlation matrix and graphical model of the data. Difference\nmade to the learnt graphical model, by acknowledgement of measurement noise, is\ndemonstrated on a small simulated dataset, while the large human\ndisease-symptom network--with $>8,000$ nodes--is learnt using real data. Data\non vino-chemical attributes of Portuguese red and white wine samples are\nemployed to learn with-uncertainty graphical model of each dataset, and\nsubsequently, the distance between these learnt graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 01:48:23 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 23:13:41 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 02:52:35 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1710.11298", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan", "title": "Effective Tensor Sketching via Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate effective sketching schemes via sparsification\nfor high dimensional multilinear arrays or tensors. More specifically, we\npropose a novel tensor sparsification algorithm that retains a subset of the\nentries of a tensor in a judicious way, and prove that it can attain a given\nlevel of approximation accuracy in terms of tensor spectral norm with a much\nsmaller sample complexity when compared with existing approaches. In\nparticular, we show that for a $k$th order $d\\times\\cdots\\times d$ cubic tensor\nof {\\it stable rank} $r_s$, the sample size requirement for achieving a\nrelative error $\\varepsilon$ is, up to a logarithmic factor, of the order\n$r_s^{1/2} d^{k/2} /\\varepsilon$ when $\\varepsilon$ is relatively large, and\n$r_s d /\\varepsilon^2$ and essentially optimal when $\\varepsilon$ is\nsufficiently small. It is especially noteworthy that the sample size\nrequirement for achieving a high accuracy is of an order independent of $k$. To\nfurther demonstrate the utility of our techniques, we also study how higher\norder singular value decomposition (HOSVD) of large tensors can be efficiently\napproximated via sparsification.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 02:04:39 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 15:31:16 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 14:20:51 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""]]}, {"id": "1710.11459", "submitter": "Patrick Breheny", "authors": "Ryan Miller and Patrick Breheny", "title": "Marginal false discovery rate control for likelihood-based penalized\n  regression models", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of penalized regression in high-dimensional data analysis has\nled to a demand for new inferential tools for these models. False discovery\nrate control is widely used in high-dimensional hypothesis testing, but has\nonly recently been considered in the context of penalized regression. Almost\nall of this work, however, has focused on lasso-penalized linear regression. In\nthis paper, we derive a general method for controlling the marginal false\ndiscovery rate that can be applied to any penalized likelihood-based model,\nsuch as logistic regression and Cox regression. Our approach is fast, flexible\nand can be used with a variety of penalty functions including lasso, elastic\nnet, MCP, and MNet. We derive theoretical results under which the proposed\nmethod is valid, and use simulation studies to demonstrate that the approach is\nreasonably robust, albeit slightly conservative, when these assumptions are\nviolated. Despite being conservative, we show that our method often offers more\npower to select causally important features than existing approaches. Finally,\nthe practical utility of the method is demonstrated on gene expression data\nsets with binary and time-to-event outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 13:32:00 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 14:52:55 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 20:53:19 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2019 21:19:48 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Miller", "Ryan", ""], ["Breheny", "Patrick", ""]]}, {"id": "1710.11529", "submitter": "Daniel Paulin", "authors": "Daniel Paulin, Ajay Jasra, Alexandros Beskos and Dan Crisan", "title": "A 4D-Var Method with Flow-Dependent Background Covariances for the\n  Shallow-Water Equations", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ao-ph physics.flu-dyn physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 4D-Var method for filtering partially observed nonlinear chaotic\ndynamical systems consists of finding the maximum a-posteriori (MAP) estimator\nof the initial condition of the system given observations over a time window,\nand propagating it forward to the current time via the model dynamics. This\nmethod forms the basis of most currently operational weather forecasting\nsystems. In practice the optimization becomes infeasible if the time window is\ntoo long due to the non-convexity of the cost function, the effect of model\nerrors, and the limited precision of the ODE solvers. Hence the window has to\nbe kept sufficiently short, and the observations in the previous windows can be\ntaken into account via a Gaussian background (prior) distribution. The choice\nof the background covariance matrix is an important question that has received\nmuch attention in the literature. In this paper, we define the background\ncovariances in a principled manner, based on observations in the previous $b$\nassimilation windows, for a parameter $b\\ge 1$. The method is at most $b$ times\nmore computationally expensive than using fixed background covariances,\nrequires little tuning, and greatly improves the accuracy of 4D-Var. As a\nconcrete example, we focus on the shallow-water equations. The proposed method\nis compared against state-of-the-art approaches in data assimilation and is\nshown to perform favourably on simulated data. We also illustrate our approach\non data from the recent tsunami of 2011 in Fukushima, Japan.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 15:21:49 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 09:28:24 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 08:39:17 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 20:57:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Paulin", "Daniel", ""], ["Jasra", "Ajay", ""], ["Beskos", "Alexandros", ""], ["Crisan", "Dan", ""]]}, {"id": "1710.11566", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Sivaraman Balakrishnan", "title": "Discussion of \"Data-driven confounder selection via Markov and Bayesian\n  networks\" by Jenny H\\\"aggstr\\\"om", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this discussion we consider why it is important to estimate causal effect\nparameters well even they are not identified, propose a partially identified\napproach for causal inference in the presence of colliders, point out an\nunder-appreciated advantage of double robustness, discuss the relative\ndifficulty of independence testing versus regression, and finally commend\nH\\\"aggstr\\\"om for her exploration of causal inference with high-dimensional\nconfounding, while making a call for further research in this same vein.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:27:51 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Balakrishnan", "Sivaraman", ""]]}, {"id": "1710.11595", "submitter": "Casey Kneale", "authors": "Casey Kneale, Steven D. Brown", "title": "Small Moving Window Calibration Models for Soft Sensing Processes with\n  Limited History", "comments": "Fixed many errors and improved clarity", "journal-ref": null, "doi": "10.1016/j.chemolab.2018.10.007", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Five simple soft sensor methodologies with two update conditions were\ncompared on two experimentally-obtained datasets and one simulated dataset. The\nsoft sensors investigated were moving window partial least squares regression\n(and a recursive variant), moving window random forest regression, the mean\nmoving window of $y$, and a novel random forest partial least squares\nregression ensemble (RF-PLS), all of which can be used with small sample sizes\nso that they can be rapidly placed online. It was found that, on two of the\ndatasets studied, small window sizes led to the lowest prediction errors for\nall of the moving window methods studied. On the majority of datasets studied,\nthe RF-PLS calibration method offered the lowest one-step-ahead prediction\nerrors compared to those of the other methods, and it demonstrated greater\npredictive stability at larger time delays than moving window PLS alone. It was\nfound that both the random forest and RF-PLS methods most adequately modeled\nthe datasets that did not feature purely monotonic increases in property\nvalues, but that both methods performed more poorly than moving window PLS\nmodels on one dataset with purely monotonic property values. Other data\ndependent findings are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:15:37 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 22:45:10 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 16:52:15 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kneale", "Casey", ""], ["Brown", "Steven D.", ""]]}]