[{"id": "1903.00037", "submitter": "Chuanping Yu", "authors": "Chuanping Yu, Xiaoming Huo", "title": "Distance-Based Independence Screening for Canonical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method named Distance-based Independence\nScreening for Canonical Analysis (DISCA) to reduce dimensions of two random\nvectors with arbitrary dimensions. The objective of our method is to identify\nthe low dimensional linear projections of two random vectors, such that any\ndimension reduction based on linear projection with lower dimensions will\nsurely affect some dependent structure -- the removed components are not\nindependent. The essence of DISCA is to use the distance correlation to\neliminate the \"redundant\" dimensions until infeasible. Unlike the existing\ncanonical analysis methods, DISCA does not require the dimensions of the\nreduced subspaces of the two random vectors to be equal, nor does it require\ncertain distributional assumption on the random vectors. We show that under\nmild conditions, our approach does undercover the lowest possible linear\ndependency structures between two random vectors, and our conditions are weaker\nthan some sufficient linear subspace-based methods. Numerically, DISCA is to\nsolve a non-convex optimization problem. We formulate it as a\ndifference-of-convex (DC) optimization problem, and then further adopt the\nalternating direction method of multipliers (ADMM) on the convex step of the DC\nalgorithms to parallelize/accelerate the computation. Some sufficient linear\nsubspace-based methods use potentially numerically-intensive bootstrap method\nto determine the dimensions of the reduced subspaces in advance; our method\navoids this complexity. In simulations, we present cases that DISCA can solve\neffectively, while other methods cannot. In both the simulation studies and\nreal data cases, when the other state-of-the-art dimension reduction methods\nare applicable, we observe that DISCA performs either comparably or better than\nmost of them. Codes and an R package can be found in GitHub\nhttps://github.com/ChuanpingYu/DISCA.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 19:26:15 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yu", "Chuanping", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1903.00177", "submitter": "Subhradev Sen", "authors": "Hazem Al-Mofleh and Subhradev Sen", "title": "The wrapped xgamma distribution for modeling circular data appearing in\n  geological context", "comments": "11 Pages, Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of wrapping of a univariate probability distribution is very\neffective in getting a circular form of the underlying density. In this\narticle, we introduce the circular (wrapped) version of xgamma distribution and\nstudy its different distributional properties. To estimate the unknown\nparameter, maximum likelihood method is proposed. A Monte-Carlo simulation\nstudy is performed to understand the behaviour of the estimates for varying\nsample size. To illustrate the application of the proposed distribution, a real\ndata set on the long axis orientation of feldspar laths in basalt rock is\nanalyzed and compared with other circular distributions.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 06:52:46 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Al-Mofleh", "Hazem", ""], ["Sen", "Subhradev", ""]]}, {"id": "1903.00390", "submitter": "Kevin Josey", "authors": "Kevin P. Josey, Elizabeth Juarez-Colunga, Fan Yang and Debashis Ghosh", "title": "A Framework for Covariate Balance using Bregman Distances", "comments": null, "journal-ref": null, "doi": "10.1111/sjos.12457", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common goal in observational research is to estimate marginal causal\neffects in the presence of confounding variables. One solution to this problem\nis to use the covariate distribution to weight the outcomes such that the data\nappear randomized. The propensity score is a natural quantity that arises in\nthis setting. Propensity score weights have desirable asymptotic properties,\nbut they often fail to adequately balance covariate data in finite samples.\nEmpirical covariate balancing methods pose as an appealing alternative by\nexactly balancing the sample moments of the covariate distribution. With this\nobjective in mind, we propose a framework for estimating balancing weights by\nsolving a constrained convex program where the criterion function to be\noptimized is a Bregman distance. We then show that the different distances in\nthis class render identical weights to those of other covariate balancing\nmethods. A series of numerical studies are presented to demonstrate these\nsimilarities.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 16:21:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 18:48:36 GMT"}, {"version": "v3", "created": "Sun, 23 Feb 2020 21:26:25 GMT"}, {"version": "v4", "created": "Sat, 11 Apr 2020 19:35:38 GMT"}, {"version": "v5", "created": "Sat, 15 Aug 2020 00:45:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Josey", "Kevin P.", ""], ["Juarez-Colunga", "Elizabeth", ""], ["Yang", "Fan", ""], ["Ghosh", "Debashis", ""]]}, {"id": "1903.00434", "submitter": "Stephen Bates", "authors": "Stephen Bates, Emmanuel Cand\\`es, Lucas Janson, and Wenshuo Wang", "title": "Metropolized Knockoff Sampling", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1729163", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-X knockoffs is a wrapper that transforms essentially any feature\nimportance measure into a variable selection algorithm, which discovers true\neffects while rigorously controlling the expected fraction of false positives.\nA frequently discussed challenge to apply this method is to construct knockoff\nvariables, which are synthetic variables obeying a crucial exchangeability\nproperty with the explanatory variables under study. This paper introduces\ntechniques for knockoff generation in great generality: we provide a sequential\ncharacterization of all possible knockoff distributions, which leads to a\nMetropolis-Hastingsformulation of an exact knockoff sampler. We further show\nhow to use conditional independence structure to speed up computations.\nCombining these two threads, we introduce an explicit set of sequential\nalgorithms and empirically demonstrate their effectiveness. Our theoretical\nanalysis proves that our algorithms achieve near-optimal computational\ncomplexity in certain cases. The techniques we develop are sufficiently rich to\nenable knockoff sampling in challenging models including cases where the\ncovariates are continuous and heavy-tailed, and follow a graphical model such\nas the Ising model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:51:52 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Janson", "Lucas", ""], ["Wang", "Wenshuo", ""]]}, {"id": "1903.00593", "submitter": "Yuan-chin Ivan Chang", "authors": "Zimu Chen and Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Sequential estimation for GEE with adaptive variables and subject\n  selection", "comments": "0", "journal-ref": null, "doi": null, "report-no": "19", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling correlated or highly stratified multiple-response data becomes a\ncommon data analysis task due to modern data monitoring facilities and methods.\nGeneralized estimating equations (GEE) is one of the popular statistical\nmethods for analyzing this kind of data. In this paper, we present a sequential\nestimation procedure for obtaining GEE-based estimates. In addition to the\nconventional random sampling, the proposed method features adaptive subject\nrecruiting and variable selection. Moreover, we equip our method with an\nadaptive shrinkage property so that it can decide the effective variables\nduring the estimation procedure and build a confidence set with a pre-specified\nprecision for the corresponding parameters. In addition to the statistical\nproperties of the proposed procedure, we assess our method using both simulated\ndata and real data sets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 01:14:21 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Chen", "Zimu", ""], ["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1903.00708", "submitter": "Phyllis Wan", "authors": "Phyllis Wan, Richard A. Davis", "title": "Goodness-of-Fit Testing for Time Series Models via Distance Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical modeling frameworks, goodness-of-fit tests are typically\nadministered to the estimated residuals. In the time series setting, whiteness\nof the residuals is assessed using the sample autocorrelation function. For\nmany time series models, especially those used for financial time series, the\nkey assumption on the residuals is that they are in fact independent and not\njust uncorrelated. In this paper, we apply the auto-distance covariance\nfunction (ADCV) to evaluate the serial dependence of the estimated residuals.\nDistance covariance can discriminate between dependence and independence of two\nrandom vectors. The limit behavior of the test statistic based on the ADCV is\nderived for a general class of time series models. One of the key aspects in\nthis theory is adjusting for the dependence that arises due to parameter\nestimation. This adjustment has essentially the same form regardless of the\nmodel specification. We illustrate the results in simulated examples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 14:25:46 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wan", "Phyllis", ""], ["Davis", "Richard A.", ""]]}, {"id": "1903.00776", "submitter": "Lilun Du", "authors": "Lilun Du and Inchi Hu", "title": "An Empirical Bayes Method for Chi-Squared Data", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a thought-provoking paper, Efron (2011) investigated the merit and\nlimitation of an empirical Bayes method to correct selection bias based on\nTweedie's formula first reported by \\cite{Robbins:1956}. The exceptional virtue\nof Tweedie's formula for the normal distribution lies in its representation of\nselection bias as a simple function of the derivative of log marginal\nlikelihood. Since the marginal likelihood and its derivative can be estimated\nfrom the data directly without invoking prior information, bias correction can\nbe carried out conveniently. We propose a Bayesian hierarchical model for\nchi-squared data such that the resulting Tweedie's formula has the same virtue\nas that of the normal distribution. Because the family of noncentral\nchi-squared distributions, the common alternative distributions for chi-squared\ntests, does not constitute an exponential family, our results cannot be\nobtained by extending existing results. Furthermore, the corresponding\nTweedie's formula manifests new phenomena quite different from those of the\nnormal distribution and suggests new ways of analyzing chi-squared data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:15:24 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:13:59 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Du", "Lilun", ""], ["Hu", "Inchi", ""]]}, {"id": "1903.00870", "submitter": "Tiangang Cui", "authors": "Johnathan Bardsley and Tiangang Cui and Youssef Marzouk and Zheng Wang", "title": "Scalable optimization-based sampling on function space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization-based samplers such as randomize-then-optimize (RTO) [2] provide\nan efficient and parallellizable approach to solving large-scale Bayesian\ninverse problems. These methods solve randomly perturbed optimization problems\nto draw samples from an approximate posterior distribution. \"Correcting\" these\nsamples, either by Metropolization or importance sampling, enables\ncharacterization of the original posterior distribution. This paper focuses on\nthe scalability of RTO to problems with high- or infinite-dimensional\nparameters. We introduce a new subspace acceleration strategy that makes the\ncomputational complexity of RTO scale linearly with the parameter dimension.\nThis subspace perspective suggests a natural extension of RTO to a function\nspace setting. We thus formalize a function space version of RTO and establish\nsufficient conditions for it to produce a valid Metropolis-Hastings proposal,\nyielding dimension-independent sampling performance. Numerical examples\ncorroborate the dimension-independence of RTO and demonstrate sampling\nperformance that is also robust to small observational noise.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 09:35:36 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 04:15:34 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Bardsley", "Johnathan", ""], ["Cui", "Tiangang", ""], ["Marzouk", "Youssef", ""], ["Wang", "Zheng", ""]]}, {"id": "1903.00928", "submitter": "Zikun Yang", "authors": "Andrew Womack, Zikun Yang", "title": "Heavy Tailed Horseshoe Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally adaptive shrinkage in the Bayesian framework is achieved through the\nuse of local-global prior distributions that model both the global level of\nsparsity as well as individual shrinkage parameters for mean structure\nparameters. The most popular of these models is the Horseshoe prior and its\nvariants due to their spike and slab behavior involving an asymptote at the\norigin and heavy tails. In this article, we present an alternative Horseshoe\nprior that exhibits both a sharper asymptote at the origin as well as heavier\ntails, which we call the Heavy-tailed Horseshoe prior. We prove that mixing on\nthe shape parameters provides improved spike and slab behavior as well as\nbetter reconstruction properties than other Horseshoe variants. A simulation\nstudy is provided to show the advantage of the heavy-tailed Horseshoe in terms\nof absolute error to both the truth mean structure as well as the oracle.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 15:51:55 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Womack", "Andrew", ""], ["Yang", "Zikun", ""]]}, {"id": "1903.00961", "submitter": "Ryan Martin", "authors": "Ryan Martin and Yiqi Tang", "title": "Empirical priors for prediction in sparse high-dimensional linear\n  regression", "comments": "29 pages, 1 figure, 7 tables. Comments welcome at\n  https://www.researchers.one/article/2019-03-1", "journal-ref": "Journal of Machine Learning Research, 2020, volume 21, pages 1--30", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we adopt the familiar sparse, high-dimensional linear\nregression model and focus on the important but often overlooked task of\nprediction. In particular, we consider a new empirical Bayes framework that\nincorporates data in the prior in two ways: one is to center the prior for the\nnon-zero regression coefficients and the other is to provide some additional\nregularization. We show that, in certain settings, the asymptotic concentration\nof the proposed empirical Bayes posterior predictive distribution is very fast,\nand we establish a Bernstein--von Mises theorem which ensures that the derived\nempirical Bayes prediction intervals achieve the targeted frequentist coverage\nprobability. The empirical prior has a convenient conjugate form, so posterior\ncomputations are relatively simple and fast. Finally, our numerical results\ndemonstrate the proposed method's strong finite-sample performance in terms of\nprediction accuracy, uncertainty quantification, and computation time compared\nto existing Bayesian methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 18:54:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 14:32:53 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Martin", "Ryan", ""], ["Tang", "Yiqi", ""]]}, {"id": "1903.01035", "submitter": "Thomas Metzger", "authors": "Thomas A. Metzger and Christopher T. Franck", "title": "Detection of latent heteroscedasticity and group-based regression\n  effects in linear models via Bayesian model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard linear modeling approaches make potentially simplistic assumptions\nregarding the structure of categorical effects that may obfuscate more complex\nrelationships governing data. For example, recent work focused on the two-way\nunreplicated layout has shown that hidden groupings among the levels of one\ncategorical predictor frequently interact with the ungrouped factor. We extend\nthe notion of a \"latent grouping factor\" to linear models in general. The\nproposed work allows researchers to determine whether an apparent grouping of\nthe levels of a categorical predictor reveals a plausible hidden structure\ngiven the observed data. Specifically, we offer Bayesian model selection-based\napproaches to reveal latent group-based heteroscedasticity, regression effects,\nand/or interactions. Failure to account for such structures can produce\nmisleading conclusions. Since the presence of latent group structures is\nfrequently unknown a priori to the researcher, we use fractional Bayes factor\nmethods and mixture $g$-priors to overcome lack of prior information.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:29:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Metzger", "Thomas A.", ""], ["Franck", "Christopher T.", ""]]}, {"id": "1903.01059", "submitter": "Denis Kojevnikov", "authors": "Denis Kojevnikov, Vadim Marmer, Kyungchul Song", "title": "Limit Theorems for Network Dependent Random Variables", "comments": null, "journal-ref": null, "doi": "10.1016/j.jeconom.2020.05.019", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with cross-sectional dependence arising because\nobservations are interconnected through an observed network. Following Doukhan\nand Louhichi (1999), we measure the strength of dependence by covariances of\nnonlinearly transformed variables. We provide a law of large numbers and\ncentral limit theorem for network dependent variables. We also provide a method\nof calculating standard errors robust to general forms of network dependence.\nFor that purpose, we rely on a network heteroskedasticity and autocorrelation\nconsistent (HAC) variance estimator, and show its consistency. The results rely\non conditions characterized by tradeoffs between the rate of decay of\ndependence across a network and network's denseness. Our approach can\naccommodate data generated by network formation models, random fields on\ngraphs, conditional dependency graphs, and large functional-causal systems of\nequations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 03:39:56 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 19:52:13 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 00:16:37 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 21:59:24 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2020 12:02:20 GMT"}, {"version": "v6", "created": "Fri, 26 Feb 2021 18:14:00 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kojevnikov", "Denis", ""], ["Marmer", "Vadim", ""], ["Song", "Kyungchul", ""]]}, {"id": "1903.01130", "submitter": "Michael Genin", "authors": "Michael Genin and Mohamed-Salem Ahmed", "title": "A functional-model-adjusted spatial scan statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new spatial scan statistic designed to adjust cluster\ndetection for longitudinal confounding factors indexed in space. The\nfunctional-model-adjusted statistic was developed using generalized functional\nlinear models in which longitudinal confounding factors were considered to be\nfunctional covariates. A general framework was developed for application to\nvarious probability models. Application to a Poisson model showed that the new\nmethod is equivalent to a conventional spatial scan statistic that adjusts the\nunderlying population for covariates. In a simulation study with univariate and\nmultivariate models, we found that our new method adjusts the cluster detection\nprocedure more accurately than other methods. Use of the new spatial scan\nstatistic was illustrated by analysing data on premature mortality in France\nover the period from 1998 to 2013, with the quarterly unemployment rate as a\nlongitudinal confounding factor.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 08:42:30 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Genin", "Michael", ""], ["Ahmed", "Mohamed-Salem", ""]]}, {"id": "1903.01138", "submitter": "Irene Tubikanec", "authors": "Evelyn Buckwar, Massimiliano Tamborrino, Irene Tubikanec", "title": "Spectral Density-Based and Measure-Preserving ABC for partially observed\n  diffusion processes. An illustration on Hamiltonian SDEs", "comments": "35 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) has become one of the major tools of\nlikelihood-free statistical inference in complex mathematical models.\nSimultaneously, stochastic differential equations (SDEs) have developed to an\nestablished tool for modelling time dependent, real world phenomena with\nunderlying random effects. When applying ABC to stochastic models, two major\ndifficulties arise. First, the derivation of effective summary statistics and\nproper distances is particularly challenging, since simulations from the\nstochastic process under the same parameter configuration result in different\ntrajectories. Second, exact simulation schemes to generate trajectories from\nthe stochastic model are rarely available, requiring the derivation of suitable\nnumerical methods for the synthetic data generation. To obtain summaries that\nare less sensitive to the intrinsic stochasticity of the model, we propose to\nbuild up the statistical method (e.g., the choice of the summary statistics) on\nthe underlying structural properties of the model. Here, we focus on the\nexistence of an invariant measure and we map the data to their estimated\ninvariant density and invariant spectral density. Then, to ensure that these\nmodel properties are kept in the synthetic data generation, we adopt\nmeasure-preserving numerical splitting schemes. The derived property-based and\nmeasure-preserving ABC method is illustrated on the broad class of partially\nobserved Hamiltonian type SDEs, both with simulated data and with real\nelectroencephalography (EEG) data. The proposed ingredients can be incorporated\ninto any type of ABC algorithm and directly applied to all SDEs that are\ncharacterised by an invariant distribution and for which a measure-preserving\nnumerical method can be derived.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 08:59:02 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 16:47:20 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Buckwar", "Evelyn", ""], ["Tamborrino", "Massimiliano", ""], ["Tubikanec", "Irene", ""]]}, {"id": "1903.01301", "submitter": "Bingxin Zhao", "authors": "Bingxin Zhao and Hongtu Zhu", "title": "On genetic correlation estimation with summary statistics from\n  genome-wide association studies", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) have been widely used to examine the\nassociation between single nucleotide polymorphisms (SNPs) and complex traits,\nwhere both the sample size n and the number of SNPs p can be very large.\nRecently, cross-trait polygenic risk score (PRS) method has gained extremely\npopular for assessing genetic correlation of complex traits based on GWAS\nsummary statistics (e.g., SNP effect size). However, empirical evidence has\nshown a common bias phenomenon that even highly significant cross-trait PRS can\nonly account for a very small amount of genetic variance (R^2 often <1%). The\naim of this paper is to develop a novel and powerful method to address the bias\nphenomenon of cross-trait PRS. We theoretically show that the estimated genetic\ncorrelation is asymptotically biased towards zero when complex traits are\nhighly polygenic/omnigenic. When all p SNPs are used to construct PRS, we show\nthat the asymptotic bias of PRS estimator is independent of the unknown number\nof causal SNPs m. We propose a consistent PRS estimator to correct such\nasymptotic bias. We also develop a novel estimator of genetic correlation which\nis solely based on two sets of GWAS summary statistics. In addition, we\ninvestigate whether or not SNP screening by GWAS p-values can lead to improved\nestimation and show the effect of overlapping samples among GWAS. Our results\nmay help demystify and tackle the puzzling \"missing genetic overlap\" phenomenon\nof cross-trait PRS for dissecting the genetic similarity of closely related\nheritable traits. We illustrate the finite sample performance of our\nbias-corrected PRS estimator by using both numerical experiments and the UK\nBiobank data, in which we assess the genetic correlation between brain white\nmatter tracts and neuropsychiatric disorders.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 15:11:29 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhao", "Bingxin", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1903.01362", "submitter": "Elena Kulinskaya", "authors": "Ilyas Bakbergenuly, David C. Hoaglin and Elena Kulinskaya", "title": "Simulation study of estimating between-study variance and overall effect\n  in meta-analysis of standardized mean difference", "comments": "20 pages and full simulation results, comprising 130 figures, each\n  presenting 12 combinations of sample sizes and numbers of studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for random-effects meta-analysis require an estimate of the\nbetween-study variance, $\\tau^2$. The performance of estimators of $\\tau^2$\n(measured by bias and coverage) affects their usefulness in assessing\nheterogeneity of study-level effects, and also the performance of related\nestimators of the overall effect. For the effect measure standardized mean\ndifference (SMD), we provide the results from extensive simulations on five\npoint estimators of $\\tau^2$ (the popular methods of DerSimonian-Laird,\nrestricted maximum likelihood, and Mandel and Paule (MP); the less-familiar\nmethod of Jackson; the new method (KDB) based on the improved approximation to\nthe distribution of the Q statistic by Kulinskaya, Dollinger and\nBj{\\o}rkest{\\o}l (2011) ), five interval estimators for $\\tau^2$ (profile\nlikelihood, Q-profile, Biggerstaff and Jackson, Jackson, and the new KDB\nmethod), six point estimators of the overall effect (the five related to the\npoint estimators of $\\tau^2$ and an estimator whose weights use only\nstudy-level sample sizes), and eight interval estimators for the overall effect\n(five based on the point estimators for $\\tau^2$; the\nHartung-Knapp-Sidik-Jonkman (HKSJ) interval; a modification of HKSJ; and an\ninterval based on the sample-size-weighted estimator).\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 16:53:03 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Bakbergenuly", "Ilyas", ""], ["Hoaglin", "David C.", ""], ["Kulinskaya", "Elena", ""]]}, {"id": "1903.01459", "submitter": "Michael Vogt", "authors": "Michael Vogt, Oliver Linton", "title": "Multiscale clustering of nonparametric regression curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of modern applications, we observe a large number of time\nseries rather than only a single one. It is often natural to suppose that there\nis some group structure in the observed time series. When each time series is\nmodelled by a nonparametric regression equation, one may in particular assume\nthat the observed time series can be partitioned into a small number of groups\nwhose members share the same nonparametric regression function. We develop a\nbandwidth-free clustering method to estimate the unknown group structure from\nthe data. More precisely speaking, we construct multiscale estimators of the\nunknown groups and their unknown number which are free of classical bandwidth\nor smoothing parameters. In the theoretical part of the paper, we analyze the\nstatistical properties of our estimators. Our theoretical results are derived\nunder general conditions which allow the data to be dependent both in time\nseries direction and across different time series. The technical analysis of\nthe paper is complemented by a simulation study and a real-data application.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:34:53 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Vogt", "Michael", ""], ["Linton", "Oliver", ""]]}, {"id": "1903.01485", "submitter": "Nina Golyandina", "authors": "Nina Golyandina", "title": "Statistical approach to detection of signals by Monte Carlo singular\n  spectrum analysis: Multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical approach to detection of a signal in noisy series is\nconsidered in the framework of Monte Carlo singular spectrum analysis. This\napproach contains a technique to control both type I and type II errors and\nalso compare criteria. For simultaneous testing of multiple frequencies, a\nmultiple version of MC-SSA is suggested to control the family-wise error rate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:02:09 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Golyandina", "Nina", ""]]}, {"id": "1903.01598", "submitter": "Hao Chen", "authors": "Hao Chen", "title": "Change-point detection for multivariate and non-Euclidean data with\n  local dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sequence of multivariate observations or non-Euclidean data objects,\nsuch as networks, local dependence is common and could lead to false\nchange-point discoveries. We propose a new way of permutation -- circular block\npermutation with a random starting point -- to address this problem. This\npermutation scheme is studied on a non-parametric change-point detection\nframework based on a similarity graph constructed on the observations, leading\nto a general framework for change-point detection for data with local\ndependency. Simulation studies show that this new framework retains the same\nlevel of power when there is no local dependency, while it controls type I\nerror correctly for sequences with and without local dependency. We also derive\nan analytic p-value approximation under this new framework. The approximation\nworks well for sequences with length in hundreds and above, making this\napproach fast-applicable for long data sequences.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:12:29 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chen", "Hao", ""]]}, {"id": "1903.01603", "submitter": "Gane Samb Lo", "authors": "Tchilabalo Abozou Kpanzou, Diam Ba, Cherif Moctar Mamadou Traor\\'e,\n  Gane Samb Lo", "title": "A multinomial Asymptotic Representation of Zenga's Discrete Index, its\n  Influence Function and Data-driven Applications", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Zenga index, one of the most recent inequality\nindex. We keep the finite-valued original form and address the asymptotic\ntheory. The asymptotic normality is established through a multinomial\nrepresentation. The Influence function is also given. Th results are simulated\nand applied to Senegalese data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:22:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Kpanzou", "Tchilabalo Abozou", ""], ["Ba", "Diam", ""], ["Traor\u00e9", "Cherif Moctar Mamadou", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1903.01680", "submitter": "Daniel Andrade", "authors": "Daniel Andrade, Kenji Fukumizu, Yuzuru Okajima", "title": "Convex Covariate Clustering for Classification", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering, like covariate selection for classification, is an important step\nto compress and interpret the data. However, clustering of covariates is often\nperformed independently of the classification step, which can lead to\nundesirable clustering results that harm interpretability and compression rate.\nTherefore, we propose a method that can cluster covariates while taking into\naccount class label information of samples. We formulate the problem as a\nconvex optimization problem which uses both, a-priori similarity information\nbetween covariates, and information from class-labeled samples. Like ordinary\nconvex clustering [Chi and Lange, 2015], the proposed method offers a unique\nglobal minima making it insensitive to initialization. In order to solve the\nconvex problem, we propose a specialized alternating direction method of\nmultipliers (ADMM), which scales up to several thousands of variables.\nFurthermore, in order to circumvent computationally expensive cross-validation,\nwe propose a model selection criterion based on approximating the marginal\nlikelihood. Experiments on synthetic and real data confirm the usefulness of\nthe proposed clustering method and the selection criterion.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 05:36:43 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:21:23 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Andrade", "Daniel", ""], ["Fukumizu", "Kenji", ""], ["Okajima", "Yuzuru", ""]]}, {"id": "1903.01706", "submitter": "Jonathan Levy", "authors": "Jonathan Levy", "title": "Tutorial: Deriving The Efficient Influence Curve for Large Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to provide a tutorial for upper level undergraduate and\ngraduate students in statistics, biostatistics and epidemiology on deriving\ninfluence functions for non-parametric and semi-parametric models. The author\nwill build on previously known efficiency theory and provide a useful identity\nand formulaic technique only relying on the basics of integration which, are\nself-contained in this tutorial and can be used in most any setting one might\nencounter in practice. The paper provides many examples of such derivations for\nwell-known influence functions as well as for new parameters of interest. The\ninfluence function remains a central object for constructing efficient\nestimators for large models, such as the one-step estimator and the targeted\nmaximum likelihood estimator. We will not touch upon these estimators at all\nbut readers familiar with these estimators might find this tutorial of\nparticular use.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:38:17 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:52:51 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 07:20:46 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Levy", "Jonathan", ""]]}, {"id": "1903.01979", "submitter": "Ray Bai", "authors": "Ray Bai, Gemma E. Moran, Joseph Antonelli, Yong Chen, Mary R. Boland", "title": "Spike-and-Slab Group Lassos for Grouped Regression and Sparse\n  Generalized Additive Models", "comments": "67 pages. 9 figures, 5 tables. Additional details for the data\n  analysis were added to Section 8 and Appendix C", "journal-ref": null, "doi": "10.1080/01621459.2020.1765784", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the spike-and-slab group lasso (SSGL) for Bayesian estimation\nand variable selection in linear regression with grouped variables. We further\nextend the SSGL to sparse generalized additive models (GAMs), thereby\nintroducing the first nonparametric variant of the spike-and-slab lasso\nmethodology. Our model simultaneously performs group selection and estimation,\nwhile our fully Bayes treatment of the mixture proportion allows for model\ncomplexity control and automatic self-adaptivity to different levels of\nsparsity. We develop theory to uniquely characterize the global posterior mode\nunder the SSGL and introduce a highly efficient block coordinate ascent\nalgorithm for maximum a posteriori (MAP) estimation. We further employ\nde-biasing methods to provide uncertainty quantification of our estimates.\nThus, implementation of our model avoids the computational intensiveness of\nMarkov chain Monte Carlo (MCMC) in high dimensions. We derive posterior\nconcentration rates for both grouped linear regression and sparse GAMs when the\nnumber of covariates grows at nearly exponential rate with sample size.\nFinally, we illustrate our methodology through extensive simulations and data\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 18:48:16 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 17:06:18 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 15:16:44 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 23:34:03 GMT"}, {"version": "v5", "created": "Fri, 8 Nov 2019 22:02:41 GMT"}, {"version": "v6", "created": "Tue, 3 Mar 2020 13:52:08 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bai", "Ray", ""], ["Moran", "Gemma E.", ""], ["Antonelli", "Joseph", ""], ["Chen", "Yong", ""], ["Boland", "Mary R.", ""]]}, {"id": "1903.02071", "submitter": "Hossein Mohammadi", "authors": "Hossein Mohammadi, Peter Challenor, Marc Goodfellow, Daniel Williamson", "title": "Emulating computer models with step-discontinuous outputs using Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications we are interested in approximating costly\nfunctions that are analytically unknown, e.g. complex computer codes. An\nemulator provides a fast approximation of such functions relying on a limited\nnumber of evaluations. Gaussian processes (GPs) are commonplace emulators due\nto their statistical properties such as the ability to estimate their own\nuncertainty. GPs are essentially developed to fit smooth, continuous functions.\nHowever, the assumptions of continuity and smoothness is unwarranted in many\nsituations. For example, in computer models where bifurcations or tipping\npoints occur, the outputs can be discontinuous. This work examines the capacity\nof GPs for emulating step-discontinuous functions. Several approaches are\nproposed for this purpose. Two special covariance functions/kernels are adapted\nwith the ability to model discontinuities. They are the neural network and\nGibbs kernels whose properties are demonstrated using several examples. Another\napproach, which is called warping, is to transform the input space into a new\nspace where a GP with a standard kernel, such as the Matern family, is able to\npredict the function well. The transformation is perform by a parametric map\nwhose parameters are estimated by maximum likelihood. The results show that the\nproposed approaches have superior performance to GPs with standard kernels in\ncapturing sharp jumps in the true function.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 21:41:56 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 13:45:15 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 15:39:12 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 20:32:16 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Mohammadi", "Hossein", ""], ["Challenor", "Peter", ""], ["Goodfellow", "Marc", ""], ["Williamson", "Daniel", ""]]}, {"id": "1903.02124", "submitter": "Stefan Wager", "authors": "Stefan Wager and Kuang Xu", "title": "Experimenting in Equilibrium", "comments": "Forthcoming in Management Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches to experimental design assume that intervening on one\nunit does not affect other units. There are many important settings, however,\nwhere this non-interference assumption does not hold, as when running\nexperiments on supply-side incentives on a ride-sharing platform or subsidies\nin an energy marketplace. In this paper, we introduce a new approach to\nexperimental design in large-scale stochastic systems with considerable\ncross-unit interference, under an assumption that the interference is\nstructured enough that it can be captured via mean-field modeling. Our approach\nenables us to accurately estimate the effect of small changes to system\nparameters by combining unobstrusive randomization with lightweight modeling,\nall while remaining in equilibrium. We can then use these estimates to optimize\nthe system by gradient descent. Concretely, we focus on the problem of a\nplatform that seeks to optimize supply-side payments p in a centralized\nmarketplace where different suppliers interact via their effects on the overall\nsupply-demand equilibrium, and show that our approach enables the platform to\noptimize p in large systems using vanishingly small perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 00:16:43 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 22:24:36 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 04:20:42 GMT"}, {"version": "v4", "created": "Sun, 23 Feb 2020 22:25:55 GMT"}, {"version": "v5", "created": "Tue, 30 Jun 2020 05:19:17 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wager", "Stefan", ""], ["Xu", "Kuang", ""]]}, {"id": "1903.02129", "submitter": "Yura Kim", "authors": "Yura Kim, Elizaveta Levina", "title": "Graph-aware Modeling of Brain Connectivity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connections in the brain are frequently represented by weighted\nnetworks, with nodes representing locations in the brain, and edges\nrepresenting the strength of connectivity between these locations. One\nchallenge in analyzing such data is that inference at the individual edge level\nis not particularly biologically meaningful; interpretation is more useful at\nthe level of so-called functional regions, or groups of nodes and connections\nbetween them; this is often called \"graph-aware\" inference in the neuroimaging\nliterature. However, pooling over functional regions leads to significant loss\nof information and lower accuracy. Another challenge is correlation among edge\nweights within a subject, which makes inference based on independence\nassumptions unreliable. We address both these challenges with a linear mixed\neffects model, which accounts for functional regions and for edge dependence,\nwhile still modeling individual edge weights to avoid loss of information. The\nmodel allows for comparing two populations, such as patients and healthy\ncontrols, both at the functional regions level and at individual edge level,\nleading to biologically meaningful interpretations. We fit this model to a\nresting state fMRI data on schizophrenics and healthy controls, obtaining\ninterpretable results consistent with the schizophrenia literature.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 00:59:28 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 19:02:22 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Kim", "Yura", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1903.02136", "submitter": "Koji Miyawaki", "authors": "Steven N. MacEachern and Koji Miyawaki", "title": "Economic variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression plays a key role in many research areas and its variable selection\nis a classic and major problem. This study emphasizes cost of predictors to be\npurchased for future use, when we select a subset of them. Its economic aspect\nis naturally formalized by the decision-theoretic approach. In addition, two\nBayesian approaches are proposed to address uncertainty about model parameters\nand models: the restricted and extended approaches, which lead us to rethink\nabout model averaging. From objective, rule-based, or robust Bayes point of\nview, the former is preferred. Proposed method is applied to three popular\ndatasets for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 01:55:17 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 00:02:22 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 02:05:50 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 04:00:48 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["MacEachern", "Steven N.", ""], ["Miyawaki", "Koji", ""]]}, {"id": "1903.02517", "submitter": "Laura Fee Schneider", "authors": "Laura Fee Schneider, Andrea Krajina, Tatyana Krivobokova", "title": "Threshold Selection in Univariate Extreme Value Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold selection plays a key role for various aspects of statistical\ninference of rare events. Most classical approaches tackling this problem for\nheavy-tailed distributions crucially depend on tuning parameters or critical\nvalues to be chosen by the practitioner. To simplify the use of automated,\ndata-driven threshold selection methods, we introduce two new procedures not\nrequiring the manual choice of any parameters. The first method measures the\ndeviation of the log-spacings from the exponential distribution and achieves\ngood performance in simulations for estimating high quantiles. The second\napproach smoothly estimates the asymptotic mean square error of the Hill\nestimator and performs consistently well over a wide range of distributions.\nThe methods are compared to existing procedures in an extensive simulation\nstudy and applied to a dataset of financial losses, where the underlying\nextreme value index is assumed to vary over time. This application strongly\nemphasizes the importance of solid automated threshold selection.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 17:52:00 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Schneider", "Laura Fee", ""], ["Krajina", "Andrea", ""], ["Krivobokova", "Tatyana", ""]]}, {"id": "1903.02774", "submitter": "Katarzyna Reluga", "authors": "Katarzyna Reluga, Mar\\'ia Jos\\'e Lombard\\'ia, Stefan Andreas Sperlich", "title": "Simultaneous inference for mixed and small area parameters", "comments": "28 pages, 5 figures, a new version includes some changes regarding\n  the notation as well as methodological developments of the construction of\n  simultaneous prediction intervals and multiple tests", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address simultaneous inference for mixed parameters which are the key\ningredients in small area estimation. We assume linear mixed model framework.\nFirstly, we analyse statistical properties of a max-type statistic and use it\nto construct simultaneous prediction intervals as well as to implement multiple\ntesting procedure. Secondly, we derive bands based on the volume-of-tube\nformula. In addition, we adapt some of the simultaneous inference methods from\nregression and nonparametric curve estimation and compare them with our\napproaches. Simultaneous intervals are necessary to compare clusters since the\npresently available intervals are not statistically valid for such analysis.\nThe proposed testing procedures can be used to validate certain statements\nabout the set of mixed parameters or to test pairwise differences. Our proposal\nis accompanied by simulation experiments and a data example on small area\nhousehold incomes. Both of them demonstrate an excellent performance and\nutility of our techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:53:13 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 08:18:16 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Reluga", "Katarzyna", ""], ["Lombard\u00eda", "Mar\u00eda Jos\u00e9", ""], ["Sperlich", "Stefan Andreas", ""]]}, {"id": "1903.02806", "submitter": "Dongming Huang", "authors": "Dongming Huang and Lucas Janson", "title": "Relaxing the Assumptions of Knockoffs by Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent paper Cand\\`es et al. (2018) introduced model-X knockoffs, a\nmethod for variable selection that provably and non-asymptotically controls the\nfalse discovery rate with no restrictions or assumptions on the dimensionality\nof the data or the conditional distribution of the response given the\ncovariates. The one requirement for the procedure is that the covariate samples\nare drawn independently and identically from a precisely-known (but arbitrary)\ndistribution. The present paper shows that the exact same guarantees can be\nmade without knowing the covariate distribution fully, but instead knowing it\nonly up to a parametric model with as many as $\\Omega(n^{*}p)$ parameters,\nwhere $p$ is the dimension and $n^{*}$ is the number of covariate samples\n(which may exceed the usual sample size $n$ of labeled samples when unlabeled\nsamples are also available). The key is to treat the covariates as if they are\ndrawn conditionally on their observed value for a sufficient statistic of the\nmodel. Although this idea is simple, even in Gaussian models conditioning on a\nsufficient statistic leads to a distribution supported on a set of zero\nLebesgue measure, requiring techniques from topological measure theory to\nestablish valid algorithms. We demonstrate how to do this for three models of\ninterest, with simulations showing the new approach remains powerful under the\nweaker assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 10:13:33 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 21:00:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Huang", "Dongming", ""], ["Janson", "Lucas", ""]]}, {"id": "1903.02973", "submitter": "Wojciech Zieli\\'nski", "authors": "Wojciech Zieli\\'nski", "title": "A comment on \"New non-parametric inferences for low-income proportions\"\n  by Shan Luo and Gengsheng Qin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shan Luo and Gengsheng Qin published the article \"New non-parametric\ninferences for low-income proportions\" Ann Inst Stat Math, 69, 599-626. In the\nnote their approach is compared to Zieli\\'nski 2009 approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:40:50 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.03153", "submitter": "Arthur Berg", "authors": "J. G. Liao, Vishal Midya, and Arthur Berg", "title": "Connecting Bayes factor and the Region of Practical Equivalence (ROPE)\n  Procedure for testing interval null hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been strong recent interest in testing interval null hypothesis for\nimproved scientific inference. For example, Lakens et al (2018) and Lakens and\nHarms (2017) use this approach to study if there is a pre-specified meaningful\ntreatment effect in gerontology and clinical trials, which is different from\nthe more traditional point null hypothesis that tests for any treatment effect.\nTwo popular Bayesian approaches are available for interval null hypothesis\ntesting. One is the standard Bayes factor and the other is the Region of\nPractical Equivalence (ROPE) procedure championed by Kruschke and others over\nmany years. This paper establishes a formal connection between these two\napproaches with two benefits. First, it helps to better understand and improve\nthe ROPE procedure. Second, it leads to a simple and effective algorithm for\ncomputing Bayes factor in a wide range of problems using draws from posterior\ndistributions generated by standard Bayesian programs such as BUGS, JAGS and\nStan. The tedious and error-prone task of coding custom-made software specific\nfor Bayes factor is then avoided.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:57:45 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 20:31:45 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Liao", "J. G.", ""], ["Midya", "Vishal", ""], ["Berg", "Arthur", ""]]}, {"id": "1903.03327", "submitter": "Wojciech Zieli\\'nski", "authors": "Wojciech Zieli\\'nski", "title": "A New Exact Confidence Interval for the Difference of Two Binomial\n  Proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interval estimation of the difference between two binomial\nproportions. Several methods of constructing such an interval are known.\nUnfortunately those confidence intervals have poor coverage probability: it is\nsignificantly smaller than the nominal confidence level. In this paper a new\nconfidence interval is proposed. The construction needs only information on\nsample sizes and sample difference between proportions. The coverage\nprobability of the proposed confidence interval is at least the nominal\nconfidence level. The new confidence interval is illustrated by a medical\nexample.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 08:55:47 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.03387", "submitter": "Kaniav Kamary", "authors": "Kaniav Kamary, Merlin Keller, Pierre Barbillon, C\\'edric G{\\oe}ury,\n  \\'Eric Parent", "title": "Computer code validation via mixture model estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When computer codes are used for modeling complex physical systems, their\nunknown parameters are tuned by calibration techniques. A discrepancy function\nmay be added to the computer code in order to capture its discrepancy with the\nreal physical process. By considering the validation question of a computer\ncode as a Bayesian selection model problem, Damblin et al. (2016) have\nhighlighted a possible confounding effect in certain configurations between the\ncode discrepancy and a linear computer code by using a Bayesian testing\nprocedure based on the intrinsic Bayes factor. In this paper, we investigate\nthe issue of code error identifiability by applying another Bayesian model\nselection technique which has been recently developed by Kamary et al. (2014).\nBy embedding the competing models within an encompassing mixture model, Kamary\net al. (2014)'s method allows each observation to belong to a different mixing\ncomponent, providing a more flexible inference, while remaining competitive in\nterms of computational cost with the intrinsic Bayesian approach. By using the\ntechnique of sharing parameters mentioned in Kamary et al. (2014), an improper\nnon-informative prior can be used for some computer code parameters and we\ndemonstrate that the resulting posterior distribution is proper. We then check\nthe sensitivity of our posterior estimates to the choice of the parameter prior\ndistributions. We illustrate that the value of the correlation length of the\ndiscrepancy Gaussian process prior impacts the Bayesian inference of the\nmixture model parameters and that the model discrepancy can be identified by\napplying the Kamary et al. (2014) method when the correlation length is not too\nsmall. Eventually, the proposed method is applied on a hydraulic code in an\nindustrial context.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 12:08:16 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Kamary", "Kaniav", ""], ["Keller", "Merlin", ""], ["Barbillon", "Pierre", ""], ["G\u0153ury", "C\u00e9dric", ""], ["Parent", "\u00c9ric", ""]]}, {"id": "1903.03531", "submitter": "Xuan Cao", "authors": "Xuan Cao, Kshitij Khare, Malay Ghosh", "title": "Consistent Bayesian Sparsity Selection for High-dimensional Gaussian DAG\n  Models with Multiplicative and Beta-mixture Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix for high-dimensional multivariate\ndatasets is a challenging and important problem in modern statistics. In this\npaper, we focus on high-dimensional Gaussian DAG models where sparsity is\ninduced on the Cholesky factor L of the inverse covariance matrix. In recent\nwork, ([Cao, Khare, and Ghosh, 2019]), we established high-dimensional sparsity\nselection consistency for a hierarchical Bayesian DAG model, where an\nErdos-Renyi prior is placed on the sparsity pattern in the Cholesky factor L,\nand a DAG-Wishart prior is placed on the resulting non-zero Cholesky entries.\nIn this paper we significantly improve and extend this work, by (a) considering\nmore diverse and effective priors on the sparsity pattern in L, namely the\nbeta-mixture prior and the multiplicative prior, and (b) establishing sparsity\nselection consistency under significantly relaxed conditions on p, and the\nsparsity pattern of the true model. We demonstrate the validity of our\ntheoretical results via numerical simulations, and also use further simulations\nto demonstrate that our sparsity selection approach is competitive with\nexisting state-of-the-art methods including both frequentist and Bayesian\napproaches in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:20:59 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Cao", "Xuan", ""], ["Khare", "Kshitij", ""], ["Ghosh", "Malay", ""]]}, {"id": "1903.03630", "submitter": "Masatoshi Uehara", "authors": "Masatoshi Uehara, Takeru Matsuda, Jae Kwang Kim", "title": "Imputation estimators for unnormalized models with missing data", "comments": "To appear (AISTATS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistical models are given in the form of unnormalized densities,\nand calculation of the normalization constant is intractable. We propose\nestimation methods for such unnormalized models with missing data. The key\nconcept is to combine imputation techniques with estimators for unnormalized\nmodels including noise contrastive estimation and score matching. In addition,\nwe derive asymptotic distributions of the proposed estimators and construct\nconfidence intervals. Simulation results with truncated Gaussian graphical\nmodels and the application to real data of wind direction reveal that the\nproposed methods effectively enable statistical inference with unnormalized\nmodels from missing data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 19:01:45 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 21:51:57 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Uehara", "Masatoshi", ""], ["Matsuda", "Takeru", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1903.03662", "submitter": "Daniel Malinsky", "authors": "Daniel Malinsky and Ilya Shpitser and Thomas Richardson", "title": "A Potential Outcomes Calculus for Identifying Conditional Path-Specific\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The do-calculus is a well-known deductive system for deriving connections\nbetween interventional and observed distributions, and has been proven complete\nfor a number of important identifiability problems in causal inference.\nNevertheless, as it is currently defined, the do-calculus is inapplicable to\ncausal problems that involve complex nested counterfactuals which cannot be\nexpressed in terms of the \"do\" operator. Such problems include analyses of\npath-specific effects and dynamic treatment regimes. In this paper we present\nthe potential outcome calculus (po-calculus), a natural generalization of\ndo-calculus for arbitrary potential outcomes. We thereby provide a bridge\nbetween identification approaches which have their origins in artificial\nintelligence and statistics, respectively. We use po-calculus to give a\ncomplete identification algorithm for conditional path-specific effects with\napplications to problems in mediation analysis and algorithmic fairness.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 20:37:00 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""], ["Richardson", "Thomas", ""]]}, {"id": "1903.03690", "submitter": "Kara Rudolph", "authors": "Kara E Rudolph and Jonathan Levy and Mark J van der Laan", "title": "Transporting stochastic direct and indirect effects to new populations", "comments": null, "journal-ref": "Biometrics. 2020", "doi": "10.1111/biom.13274", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transported mediation effects may contribute to understanding how and why\ninterventions may work differently when applied to new populations. However, we\nare not aware of any estimators for such effects. Thus, we propose several\ndifferent estimators of transported stochastic direct and indirect effects: an\ninverse-probability of treatment stabilized weighted estimator, a doubly robust\nestimator that solves the estimating equation, and a doubly robust substitution\nestimator in the targeted minimum loss-based framework. We demonstrate their\nfinite sample properties in a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 22:42:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rudolph", "Kara E", ""], ["Levy", "Jonathan", ""], ["van der Laan", "Mark J", ""]]}, {"id": "1903.03742", "submitter": "Lixing Zhu", "authors": "Lingzhu Li, Xuehu Zhu, Lixing Zhu", "title": "Adaptive-to-model hybrid of tests for regressions", "comments": "35pages, 6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model checking for regressions, nonparametric estimation-based tests\nusually have tractable limiting null distributions and are sensitive to\noscillating alternative models, but suffer from the curse of dimensionality. In\ncontrast, empirical process-based tests can, at the fastest possible rate,\ndetect local alternatives distinct from the null model, but is less sensitive\nto oscillating alternative models and with intractable limiting null\ndistributions. It has long been an issue on how to construct a test that can\nfully inherit the merits of these two types of tests and avoid the\nshortcomings. We in this paper propose a generic adaptive-to-model hybrid of\nmoment and conditional moment-based test to achieve this goal. Further, a\nsignificant feature of the method is to make nonparametric estimation-based\ntests, under the alternatives, also share the merits of existing empirical\nprocess-based tests. This methodology can be readily applied to other kinds of\ndata and constructing other hybrids. As a by-product in sufficient dimension\nreduction field, the estimation of residual-related central subspace is used to\nindicate the underlying models for model adaptation. A systematic study is\ndevoted to showing when alternative models can be indicated and when cannot.\nThis estimation is of its own interest and can be applied to the problems with\nother kinds of data. Numerical studies are conducted to verify the powerfulness\nof the proposed test.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 06:06:08 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Lingzhu", ""], ["Zhu", "Xuehu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1903.03810", "submitter": "Xingxiang Li", "authors": "Xingxiang Li, Runze Li, Zhiming Xia, Chen Xu", "title": "Distributed Feature Screening via Componentwise Debiasing", "comments": "28 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature screening is a powerful tool in the analysis of high dimensional\ndata. When the sample size $N$ and the number of features $p$ are both large,\nthe implementation of classic screening methods can be numerically challenging.\nIn this paper, we propose a distributed screening framework for big data setup.\nIn the spirit of \"divide-and-conquer\", the proposed framework expresses a\ncorrelation measure as a function of several component parameters, each of\nwhich can be distributively estimated using a natural U-statistic from data\nsegments. With the component estimates aggregated, we obtain a final\ncorrelation estimate that can be readily used for screening features. This\nframework enables distributed storage and parallel computing and thus is\ncomputationally attractive. Due to the unbiased distributive estimation of the\ncomponent parameters, the final aggregated estimate achieves a high accuracy\nthat is insensitive to the number of data segments $m$ specified by the problem\nitself or to be chosen by users. Under mild conditions, we show that the\naggregated correlation estimator is as efficient as the classic centralized\nestimator in terms of the probability convergence bound; the corresponding\nscreening procedure enjoys sure screening property for a wide range of\ncorrelation measures. The promising performances of the new method are\nsupported by extensive numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 14:32:21 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Xingxiang", ""], ["Li", "Runze", ""], ["Xia", "Zhiming", ""], ["Xu", "Chen", ""]]}, {"id": "1903.03883", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "Two seemingly paradoxical results in linear models: the variance\n  inflation factor and the analysis of covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A result from a standard linear model course is that the variance of the\nordinary least squares (OLS) coefficient of a variable will never decrease when\nincluding additional covariates. The variance inflation factor (VIF) measures\nthe increase of the variance. Another result from a standard linear model or\nexperimental design course is that including additional covariates in a linear\nmodel of the outcome on the treatment indicator will never increase the\nvariance of the OLS coefficient of the treatment at least asymptotically. This\ntechnique is called the analysis of covariance (ANCOVA), which is often used to\nimprove the efficiency of treatment effect estimation. So we have two\nparadoxical results: adding covariates never decreases the variance in the\nfirst result but never increases the variance in the second result. In fact,\nthese two results are derived under different assumptions. More precisely, the\nVIF result conditions on the treatment indicators but the ANCOVA result\naverages over them. Comparing the estimators with and without adjusting for\nadditional covariates in a completely randomized experiment, I show that the\nformer has smaller variance averaging over the treatment indicators, and the\nlatter has smaller variance at the cost of a larger bias conditioning on the\ntreatment indicators. Therefore, there is no real paradox.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 22:38:07 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 22:37:00 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 05:47:22 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "1903.03935", "submitter": "Ellis Patrick", "authors": "Ellis Patrick and Samuel Mueller", "title": "Lasso tuning through the flexible-weighted bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized regression approaches such as the Lasso have been widely adopted\nfor constructing sparse linear models in high-dimensional datasets. A\ncomplexity in fitting these models is the tuning of the parameters which\ncontrol the level of introduced sparsity through penalization. The most common\napproach to select the penalty parameter is through $k$-fold cross-validation.\nWhile cross-validation is used to minimise the empirical prediction error,\napproaches such as the $m$-out-of-$n$ paired bootstrap which use smaller\ntraining datasets provide consistency in selecting the non-zero coefficients in\nthe oracle model, performing well in an asymptotic setting but having\nlimitations when $n$ is small. In fact, for models such as the Lasso there is a\nmonotonic relationship between the size of training sets and the penalty\nparameter. We propose a generalization of these methods for selecting the\nregularization parameter based on a flexible-weighted bootstrap procedure that\nmimics the $m$-out-of-$n$ bootstrap and overcomes its challenges for all sample\nsizes. Through simulation studies we demonstrate that when selecting a penalty\nparameter, the choice of weights in the bootstrap procedure can be used to\ndictate the size of the penalty parameter and hence the sparsity of the fitted\nmodel. We empirically illustrate our weighted bootstrap procedure by applying\nthe Lasso to integrate clinical and microRNA data in the modeling of\nAlzheimer's disease. In both the real and simulated data we find a narrow part\nof the parameter space to perform well, emulating an $m$-out-of-$n$ bootstrap,\nand that our procedure can be used to improve interpretation of other\noptimization heuristics.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 06:24:36 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Patrick", "Ellis", ""], ["Mueller", "Samuel", ""]]}, {"id": "1903.04043", "submitter": "Matt Wand Professor", "authors": "M. Menictas, T.H. Nolan, D.G. Simpson and M.P. Wand", "title": "Streamlined Variational Inference for Higher Level Group-Specific Curve\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-level group-specific curve model is such that the mean response of each\nmember of a group is a separate smooth function of a predictor of interest. The\nthree-level extension is such that one grouping variable is nested within\nanother one, and higher level extensions are analogous. Streamlined variational\ninference for higher level group-specific curve models is a challenging\nproblem. We confront it by systematically working through two-level and then\nthree-level cases and making use of the higher level sparse matrix\ninfrastructure laid down in Nolan and Wand (2018). A motivation is analysis of\ndata from ultrasound technology for which three-level group-specific curve\nmodels are appropriate. Whilst extension to the number of levels exceeding\nthree is not covered explicitly, the pattern established by our systematic\napproach sheds light on what is required for even higher level group-specific\ncurve models.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 19:02:42 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Menictas", "M.", ""], ["Nolan", "T. H.", ""], ["Simpson", "D. G.", ""], ["Wand", "M. P.", ""]]}, {"id": "1903.04059", "submitter": "Ioannis Papastathopoulos", "authors": "Ioannis Papastathopoulos and Jonathan A. Tawn", "title": "Hidden tail chains and recurrence equations for dependence parameters\n  associated with extremes of higher-order Markov chains", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive some key extremal features for kth order Markov chains, which can\nbe used to understand how the process moves between an extreme state and the\nbody of the process. The chains are studied given that there is an exceedance\nof a threshold, as the threshold tends to the upper endpoint of the\ndistribution. Unlike previous studies with k>1 we consider processes where\nstandard limit theory describes each extreme event as a single observation\nwithout any information about the transition to and from the body of the\ndistribution. The extremal properties of the Markov chain at lags up to k are\ndetermined by the kernel of the chain, through a joint initialisation\ndistribution, with the subsequent values determined by the conditional\nindependence structure through a transition behaviour. We study the extremal\nproperties of each of these elements under weak assumptions for broad classes\nof extremal dependence structures. For chains with k>1, these transitions\ninvolve novel functions of the k previous states, in comparison to just the\nsingle value, when k=1. This leads to an increase in the complexity of\ndetermining the form of this class of functions, their properties and the\nmethod of their derivation in applications. We find that it is possible to find\nan affine normalization, dependent on the threshold excess, such that\nnon-degenerate limiting behaviour of the process is assured for all lags. These\nnormalization functions have an attractive structure that has parallels to the\nYule-Walker equations. Furthermore, the limiting process is always linear in\nthe innovations. We illustrate the results with the study of kth order\nstationary Markov chains based on widely studied families of copula dependence\nstructures.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 21:08:55 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:54:37 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 08:42:33 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Papastathopoulos", "Ioannis", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1903.04168", "submitter": "James McGree", "authors": "Mahasen Dehideniya, Antony M. Overstall, Chris C. Drovandi and James\n  M. McGree", "title": "A synthetic likelihood-based Laplace approximation for efficient design\n  of biological processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex models used to describe biological processes in epidemiology and\necology often have computationally intractable or expensive likelihoods. This\nposes significant challenges in terms of Bayesian inference but more\nsignificantly in the design of experiments. Bayesian designs are found by\nmaximising the expectation of a utility function over a design space, and\ntypically this requires sampling from or approximating a large number of\nposterior distributions. This renders approaches adopted in inference\ncomputationally infeasible to implement in design. Consequently, optimal design\nin such fields has been limited to a small number of dimensions or a restricted\nrange of utility functions. To overcome such limitations, we propose a\nsynthetic likelihood-based Laplace approximation for approximating utility\nfunctions for models with intractable likelihoods. As will be seen, the\nproposed approximation is flexible in that a wide range of utility functions\ncan be considered, and remains computationally efficient in high dimensions. To\nexplore the validity of this approximation, an illustrative example from\nepidemiology is considered. Then, our approach is used to design experiments\nwith a relatively large number of observations in two motivating applications\nfrom epidemiology and ecology.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 08:21:35 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Dehideniya", "Mahasen", ""], ["Overstall", "Antony M.", ""], ["Drovandi", "Chris C.", ""], ["McGree", "James M.", ""]]}, {"id": "1903.04223", "submitter": "Wojciech Zieli\\'nski", "authors": "Alina J\\k{e}drzejczak, Dorota Pekasiewicz, Wojciech Zieli\\'nski", "title": "Confidence Interval for Quantile Ratio of the Dagum Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In economic research inequality measures based on ratios of quantiles are\nfrequently applied to the analysis of income distributions. In the paper, we\nconstruct a confidence interval for such measures under the Dagum distribution\nwhich has been widely assumed as a~model for income distributions in empirical\nanalyses Its properties are investigated on the basis of computer simulations.\nThe constructed confidence interval is applied to the analysis of inequality\nincome in Poland in 2015.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:36:53 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["J\u0119drzejczak", "Alina", ""], ["Pekasiewicz", "Dorota", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.04226", "submitter": "Wojciech Zieli\\'nski", "authors": "Alina J\\c{e}drzejczak, Dorota Pekasiewicz, Wojciech Zieli\\'nski", "title": "The Shortest Confidence Interval for the Ratio of Quantiles of the Dagum\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  J\\k{e}drzejczak et al. (2018) constructed a confidence interval for a ratio\nof quantiles coming from the Dagum distribution, which is frequently applied as\na theoretical model in numerous income distribution analyses. The proposed\ninterval is symmetric with respect to the ratio of sample quantiles, which\nresult may be unsatisfactory in many practical applications. The search for a\nconfidence interval with a smaller length led to the derivation of the shortest\ninterval with the ends being asymmetric relative to the ratio of sample\nquantiles. In the paper, the existence of the shortest confidence interval is\nshown and the method of obtaining such an interval is presented. The results of\nthe calculation show a reduction in the length of the confidence intervals by\nseveral percent in relation to the symmetric confidence interval.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 11:42:31 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["J\u0229drzejczak", "Alina", ""], ["Pekasiewicz", "Dorota", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.04367", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Jong-Shi Pang, Yufeng Liu", "title": "Estimating Individualized Decision Rules with Tail Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of precision medicine, estimating optimal individualized\ndecision rules (IDRs) has attracted tremendous attention in many scientific\nareas. Most existing literature has focused on finding optimal IDRs that can\nmaximize the expected outcome for each individual. Motivated by complex\nindividualized decision making procedures and popular conditional value at risk\n(CVaR) measures, we propose a new robust criterion to estimate optimal IDRs in\norder to control the average lower tail of the subjects' outcomes. In addition\nto improving the individualized expected outcome, our proposed criterion takes\nrisks into consideration, and thus the resulting IDRs can prevent adverse\nevents. The optimal IDR under our criterion can be interpreted as the decision\nrule that maximizes the ``worst-case\" scenario of the individualized outcome\nwhen the underlying distribution is perturbed within a constrained set. An\nefficient non-convex optimization algorithm is proposed with convergence\nguarantees. We investigate theoretical properties for our estimated optimal\nIDRs under the proposed criterion such as consistency and finite sample error\nbounds. Simulation studies and a real data application are used to further\ndemonstrate the robust performance of our method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 15:21:05 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 01:11:30 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Qi", "Zhengling", ""], ["Pang", "Jong-Shi", ""], ["Liu", "Yufeng", ""]]}, {"id": "1903.04408", "submitter": "Zhe Fei", "authors": "Zhe Fei and Yi Li", "title": "Estimation and Inference for High Dimensional Generalized Linear Models:\n  A Splitting and Smoothing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of modern biomedical studies has gradually shifted to explanation\nand estimation of joint effects of high dimensional predictors on disease\nrisks. Quantifying uncertainty in these estimates may provide valuable insight\ninto prevention strategies or treatment decisions for both patients and\nphysicians. High dimensional inference, including confidence intervals and\nhypothesis testing, has sparked much interest. While much work has been done in\nthe linear regression setting, there is lack of literature on inference for\nhigh dimensional generalized linear models. We propose a novel and\ncomputationally feasible method, which accommodates a variety of outcome types,\nincluding normal, binomial, and Poisson data. We use a \"splitting and\nsmoothing\" approach, which splits samples into two parts, performs variable\nselection using one part and conducts partial regression with the other part.\nAveraging the estimates over multiple random splits, we obtain the smoothed\nestimates, which are numerically stable. We show that the estimates are\nconsistent, asymptotically normal, and construct confidence intervals with\nproper coverage probabilities for all predictors. We examine the finite sample\nperformance of our method by comparing it with the existing methods and\napplying it to analyze a lung cancer cohort study.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:19:04 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 17:25:53 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 22:44:27 GMT"}, {"version": "v4", "created": "Sat, 6 Mar 2021 01:11:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fei", "Zhe", ""], ["Li", "Yi", ""]]}, {"id": "1903.04416", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Yun Yang", "title": "Diffusion $K$-means clustering on manifolds: provable exact recovery via\n  semidefinite relaxations", "comments": "accepted to Applied and Computational Harmonic Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2020.03.002", "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the {\\it diffusion $K$-means} clustering method on Riemannian\nsubmanifolds, which maximizes the within-cluster connectedness based on the\ndiffusion distance. The diffusion $K$-means constructs a random walk on the\nsimilarity graph with vertices as data points randomly sampled on the manifolds\nand edges as similarities given by a kernel that captures the local geometry of\nmanifolds. The diffusion $K$-means is a multi-scale clustering tool that is\nsuitable for data with non-linear and non-Euclidean geometric features in mixed\ndimensions. Given the number of clusters, we propose a polynomial-time convex\nrelaxation algorithm via the semidefinite programming (SDP) to solve the\ndiffusion $K$-means. In addition, we also propose a nuclear norm regularized\nSDP that is adaptive to the number of clusters. In both cases, we show that\nexact recovery of the SDPs for diffusion $K$-means can be achieved under\nsuitable between-cluster separability and within-cluster connectedness of the\nsubmanifolds, which together quantify the hardness of the manifold clustering\nproblem. We further propose the {\\it localized diffusion $K$-means} by using\nthe local adaptive bandwidth estimated from the nearest neighbors. We show that\nexact recovery of the localized diffusion $K$-means is fully adaptive to the\nlocal probability density and geometric structures of the underlying\nsubmanifolds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:29:27 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 01:01:56 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 03:27:37 GMT"}, {"version": "v4", "created": "Mon, 16 Mar 2020 16:49:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Xiaohui", ""], ["Yang", "Yun", ""]]}, {"id": "1903.04478", "submitter": "Ali Taylan Cemgil", "authors": "Ali Taylan Cemgil, Mehmet Burak Kurutmaz, Sinan Yildirim, Melih\n  Barsbey, Umut Simsekli", "title": "Bayesian Allocation Model: Inference by Sequential Monte Carlo for\n  Nonnegative Tensor Factorizations and Topic Models using Polya Urns", "comments": "70 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic generative model, Bayesian allocation model (BAM),\nwhich establishes explicit connections between nonnegative tensor factorization\n(NTF), graphical models of discrete probability distributions and their\nBayesian extensions, and the topic models such as the latent Dirichlet\nallocation. BAM is based on a Poisson process, whose events are marked by using\na Bayesian network, where the conditional probability tables of this network\nare then integrated out analytically. We show that the resulting marginal\nprocess turns out to be a Polya urn, an integer valued self-reinforcing\nprocess. This urn processes, which we name a Polya-Bayes process, obey certain\nconditional independence properties that provide further insight about the\nnature of NTF. These insights also let us develop space efficient simulation\nalgorithms that respect the potential sparsity of data: we propose a class of\nsequential importance sampling algorithms for computing NTF and approximating\ntheir marginal likelihood, which would be useful for model selection. The\nresulting methods can also be viewed as a model scoring method for topic models\nand discrete Bayesian networks with hidden variables. The new algorithms have\nfavourable properties in the sparse data regime when contrasted with\nvariational algorithms that become more accurate when the total sum of the\nelements of the observed tensor goes to infinity. We illustrate the performance\non several examples and numerically study the behaviour of the algorithms for\nvarious data regimes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:54:59 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Cemgil", "Ali Taylan", ""], ["Kurutmaz", "Mehmet Burak", ""], ["Yildirim", "Sinan", ""], ["Barsbey", "Melih", ""], ["Simsekli", "Umut", ""]]}, {"id": "1903.04641", "submitter": "Asad Haris", "authors": "Asad Haris, Noah Simon, Ali Shojaie", "title": "Generalized Sparse Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for estimation and analysis of generalized\nadditive models in high dimensions. The framework defines a large class of\npenalized regression estimators, encompassing many existing methods. An\nefficient computational algorithm for this class is presented that easily\nscales to thousands of observations and features. We prove minimax optimal\nconvergence bounds for this class under a weak compatibility condition. In\naddition, we characterize the rate of convergence when this compatibility\ncondition is not met. Finally, we also show that the optimal penalty parameters\nfor structure and sparsity penalties in our framework are linked, allowing\ncross-validation to be conducted over only a single tuning parameter. We\ncomplement our theoretical results with empirical studies comparing some\nexisting methods within this framework.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 22:50:29 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Haris", "Asad", ""], ["Simon", "Noah", ""], ["Shojaie", "Ali", ""]]}, {"id": "1903.04697", "submitter": "Judith Lok", "authors": "Judith J. Lok, Ronald J. Bosch", "title": "Causal organic indirect and direct effects: closer to Baron and Kenny,\n  with a product method for binary mediators", "comments": "20 pages, plus 13 pages web-appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis, which started with Baron and Kenny (1986), is used\nextensively by applied researchers. Indirect and direct effects are the part of\na treatment effect that is mediated by a covariate and the part that is not.\nSubsequent work on natural indirect and direct effects provides a formal causal\ninterpretation, based on cross-worlds counterfactuals: outcomes under treatment\nwith the mediator set to its value without treatment. Organic indirect and\ndirect effects (Lok 2016) avoid cross-worlds counterfactuals, using so-called\norganic interventions on the mediator while keeping the initial treatment fixed\nat treatment. Organic indirect and direct effects apply also to settings where\nthe mediator cannot be set. In linear models where the outcome model does not\nhave treatment-mediator interaction, both organic and natural indirect and\ndirect effects lead to the same estimators as in Baron and Kenny (1986). Here,\nwe generalize organic interventions on the mediator to include interventions\ncombined with the initial treatment fixed at no treatment. We show that the\nproduct method holds in linear models for organic indirect and direct effects\nrelative to no treatment even if there is treatment-mediator interaction.\nMoreover, we find a product method for binary mediators. Furthermore, we argue\nthat the organic indirect effect relative to no treatment is very relevant for\ndrug development. We illustrate the benefits of our approach by estimating the\norganic indirect effect of curative HIV-treatments mediated by two\nHIV-persistence measures, using ART-interruption data without curative\nHIV-treatments combined with an estimated/hypothesized effect of the curative\nHIV-treatments on these mediators.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 01:54:53 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 15:21:32 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 18:32:03 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 15:21:25 GMT"}, {"version": "v5", "created": "Wed, 16 Dec 2020 18:35:17 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Lok", "Judith J.", ""], ["Bosch", "Ronald J.", ""]]}, {"id": "1903.04703", "submitter": "Jian Wu", "authors": "Jian Wu, Saul Toscano-Palmerin, Peter I. Frazier and Andrew Gordon\n  Wilson", "title": "Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is popular for optimizing time-consuming black-box\nobjectives. Nonetheless, for hyperparameter tuning in deep neural networks, the\ntime required to evaluate the validation error for even a few hyperparameter\nsettings remains a bottleneck. Multi-fidelity optimization promises relief\nusing cheaper proxies to such objectives --- for example, validation error for\na network trained using a subset of the training points or fewer iterations\nthan required for convergence. We propose a highly flexible and practical\napproach to multi-fidelity Bayesian optimization, focused on efficiently\noptimizing hyperparameters for iteratively trained supervised learning models.\nWe introduce a new acquisition function, the trace-aware knowledge-gradient,\nwhich efficiently leverages both multiple continuous fidelity controls and\ntrace observations --- values of the objective at a sequence of fidelities,\navailable when varying fidelity using training iterations. We provide a\nprovably convergent method for optimizing our acquisition function and show it\noutperforms state-of-the-art alternatives for hyperparameter tuning of deep\nneural networks and large-scale kernel learning.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 02:14:04 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wu", "Jian", ""], ["Toscano-Palmerin", "Saul", ""], ["Frazier", "Peter I.", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1903.04919", "submitter": "Rolf Larsson", "authors": "Rolf Larsson", "title": "Discrete factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for factor analysis of discrete data. This\nis accomplished by fitting a dependent Poisson model with a factor structure.\nTo be able to analyze ordinal data, we also consider a truncated Poisson\ndistribution. We try to find the model with the lowest AIC by employing a\nforward selection procedure. The probability to find the correct model is\ninvestigated in a simulation study. Moreover, we heuristically derive the\ncorresponding asymptotic probabilities. An empirical study is also included.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 13:55:04 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Larsson", "Rolf", ""]]}, {"id": "1903.04927", "submitter": "Cristina Zucca", "authors": "Alessia Civallero and Cristina Zucca", "title": "The Inverse first passage time method for a two dimensional Ornstein\n  Uhlenbeck process with neuronal application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Inverse First Passage time problem seeks to determine the boundary\ncorresponding to a given stochastic process and a fixed first passage time\ndistribution. Here, we determine the numerical solution of this problem in the\ncase of a two dimensional Gauss-Markov diffusion process. We investigate the\nboundary shape corresponding to Inverse Gaussian or Gamma first passage time\ndistributions for different choices of the parameters, including heavy and\nlight tails instances. Applications in neuroscience framework are illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 13:59:48 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 09:32:35 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Civallero", "Alessia", ""], ["Zucca", "Cristina", ""]]}, {"id": "1903.05036", "submitter": "John Tipton", "authors": "John R. Tipton, Mevin B. Hooten, Connor Nolan, Robert K. Booth, and\n  Jason McLachlan", "title": "Predicting paleoclimate from compositional data using multivariate\n  Gaussian process inverse prediction", "comments": "20 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate compositional count data arise in many applications including\necology, microbiology, genetics, and paleoclimate. A frequent question in the\nanalysis of multivariate compositional count data is what values of a\ncovariate(s) give rise to the observed composition. Learning the relationship\nbetween covariates and the compositional count allows for inverse prediction of\nunobserved covariates given compositional count observations. Gaussian\nprocesses provide a flexible framework for modeling functional responses with\nrespect to a covariate without assuming a functional form. Many scientific\ndisciplines use Gaussian process approximations to improve prediction and make\ninference on latent processes and parameters. When prediction is desired on\nunobserved covariates given realizations of the response variable, this is\ncalled inverse prediction. Because inverse prediction is mathematically and\ncomputationally challenging, predicting unobserved covariates often requires\nfitting models that are different from the hypothesized generative model. We\npresent a novel computational framework that allows for efficient inverse\nprediction using a Gaussian process approximation to generative models. Our\nframework enables scientific learning about how the latent processes co-vary\nwith respect to covariates while simultaneously providing predictions of\nmissing covariates. The proposed framework is capable of efficiently exploring\nthe high dimensional, multi-modal latent spaces that arise in the inverse\nproblem. To demonstrate flexibility, we apply our method in a generalized\nlinear model framework to predict latent climate states given multivariate\ncount data. Based on cross-validation, our model has predictive skill\ncompetitive with current methods while simultaneously providing formal,\nstatistical inference on the underlying community dynamics of the biological\nsystem previously not available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:37:36 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Tipton", "John R.", ""], ["Hooten", "Mevin B.", ""], ["Nolan", "Connor", ""], ["Booth", "Robert K.", ""], ["McLachlan", "Jason", ""]]}, {"id": "1903.05054", "submitter": "Yang Tang", "authors": "Michael P. B. Gallaugher and Yang Tang and Paul D. McNicholas", "title": "Flexible Clustering with a Sparse Mixture of Generalized Hyperbolic\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust clustering of high-dimensional data is an important topic because, in\nmany practical situations, real data sets are heavy-tailed and/or asymmetric.\nMoreover, traditional model-based clustering often fails for high dimensional\ndata due to the number of free covariance parameters. A parametrization of the\ncomponent scale matrices for the mixture of generalized hyperbolic\ndistributions is proposed by including a penalty term in the likelihood\nconstraining the parameters resulting in a flexible model for high dimensional\ndata and a meaningful interpretation. An analytically feasible EM algorithm is\ndeveloped by placing a gamma-Lasso penalty constraining the concentration\nmatrix. The proposed methodology is investigated through simulation studies and\ntwo real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 17:02:40 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1903.05212", "submitter": "Shu Yang", "authors": "Shu Yang, Jae Kwang Kim, and Rui Song", "title": "Doubly Robust Inference when Combining Probability and Non-probability\n  Samples with High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-probability samples become increasingly popular in survey statistics but\nmay suffer from selection biases that limit the generalizability of results to\nthe target population. We consider integrating a non-probability sample with a\nprobability sample which provides high-dimensional representative covariate\ninformation of the target population. We propose a two-step approach for\nvariable selection and finite population inference. In the first step, we use\npenalized estimating equations with folded-concave penalties to select\nimportant variables for the sampling score of selection into the\nnon-probability sample and the outcome model. We show that the penalized\nestimating equation approach enjoys the selection consistency property for\ngeneral probability samples. The major technical hurdle is due to the possible\ndependence of the sample under the finite population framework. To overcome\nthis challenge, we construct martingales which enable us to apply Bernstein\nconcentration inequality for martingales. In the second step, we focus on a\ndoubly robust estimator of the finite population mean and re-estimate the\nnuisance model parameters by minimizing the asymptotic squared bias of the\ndoubly robust estimator. This estimating strategy mitigates the possible\nfirst-step selection error and renders the doubly robust estimator root-n\nconsistent if either the sampling probability or the outcome model is correctly\nspecified.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 20:57:11 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 12:12:45 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""], ["Song", "Rui", ""]]}, {"id": "1903.05262", "submitter": "Jingyi Jessica Li", "authors": "Jingyi Jessica Li, Yiling Chen, and Xin Tong", "title": "A flexible model-free prediction-based framework for feature ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the availability of numerous statistical and machine learning tools\nfor joint feature modeling, many scientists investigate features marginally,\ni.e., one feature at a time. This is partly due to training and convention but\nalso roots in scientists' strong interests in simple visualization and\ninterpretability. As such, marginal feature ranking for some predictive tasks,\ne.g., prediction of cancer driver genes, is widely practiced in the process of\nscientific discoveries. In this work, we focus on marginal ranking for binary\nprediction, the arguably most common predictive tasks. We argue that the most\nwidely used marginal ranking criteria, including the Pearson correlation, the\ntwo-sample t test, and two-sample Wilcoxon rank-sum test, do not fully take\nfeature distributions and prediction objectives into account. To address this\ngap in practice, we propose two ranking criteria corresponding to two\nprediction objectives: the classical criterion (CC) and the Neyman-Pearson\ncriterion (NPC), both of which use model-free nonparametric implementations to\naccommodate diverse feature distributions. Theoretically, we show that under\nregularity conditions both criteria achieve sample-level ranking consistent\nwith their population-level counterpart with high probability. Moreover, NPC is\nrobust to sampling bias when the two class proportions in a sample deviate from\nthose in the population. This property endows NPC good potential in biomedical\nresearch where sampling bias is common. We demonstrate the use and relative\nadvantages of CC and NPC in simulation and real data studies. Our model-free\nobjective-based ranking idea is extendable to ranking feature subsets and\ngeneralizable to other prediction tasks and learning objectives.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 23:41:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 23:11:21 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 23:27:05 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Jingyi Jessica", ""], ["Chen", "Yiling", ""], ["Tong", "Xin", ""]]}, {"id": "1903.05367", "submitter": "Konstantin Posch", "authors": "Konstantin Posch, Maximilian Arbeiter, J\\\"urgen Pilz", "title": "A novel Bayesian approach for variable selection in linear regression\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian approach to the problem of variable selection in\nmultiple linear regression models. In particular, we present a hierarchical\nsetting which allows for direct specification of a-priori beliefs about the\nnumber of nonzero regression coefficients as well as a specification of beliefs\nthat given coefficients are nonzero. To guarantee numerical stability, we adopt\na $g$-prior with an additional ridge parameter for the unknown regression\ncoefficients. In order to simulate from the joint posterior distribution an\nintelligent random walk Metropolis-Hastings algorithm which is able to switch\nbetween different models is proposed. Testing our algorithm on real and\nsimulated data illustrates that it performs at least on par and often even\nbetter than other well-established methods. Finally, we prove that under some\nnominal assumptions, the presented approach is consistent in terms of model\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 09:01:43 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Posch", "Konstantin", ""], ["Arbeiter", "Maximilian", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "1903.05480", "submitter": "Adam Foster", "authors": "Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye\n  Teh, Tom Rainforth, Noah Goodman", "title": "Variational Bayesian Optimal Experimental Design", "comments": "Published as a conference paper at the Thirty-third Conference on\n  Neural Information Processing Systems, Vancouver 2019.\n  https://papers.nips.cc/paper/9553-variational-bayesian-optimal-experimental-design.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal experimental design (BOED) is a principled framework for\nmaking efficient use of limited experimental resources. Unfortunately, its\napplicability is hampered by the difficulty of obtaining accurate estimates of\nthe expected information gain (EIG) of an experiment. To address this, we\nintroduce several classes of fast EIG estimators by building on ideas from\namortized variational inference. We show theoretically and empirically that\nthese estimators can provide significant gains in speed and accuracy over\nprevious approaches. We further demonstrate the practicality of our approach on\na number of end-to-end experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 13:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 14:39:15 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 14:49:02 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Foster", "Adam", ""], ["Jankowiak", "Martin", ""], ["Bingham", "Eli", ""], ["Horsfall", "Paul", ""], ["Teh", "Yee Whye", ""], ["Rainforth", "Tom", ""], ["Goodman", "Noah", ""]]}, {"id": "1903.05522", "submitter": "Lily Wang", "authors": "Jiangyan Wang, Guanqun Cao, Li Wang, Lijian Yang", "title": "Simultaneous Confidence Band for Stationary Covariance Function of Dense\n  Functional Data", "comments": "45 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference via simultaneous confidence band is studied for stationary\ncovariance function of dense functional data. A two-stage estimation procedure\nis proposed based on spline approximation, the first stage involving estimation\nof all the individual trajectories and the second stage involving estimation of\nthe covariance function through smoothing the empirical covariance function.\nThe proposed covariance estimator is smooth and as efficient as the oracle\nestimator when all individual trajectories are known. An asymptotic\nsimultaneous confidence band (SCB) is developed for the true covariance\nfunction, and the coverage probabilities are shown to be asymptotically\ncorrect. Simulation experiments are conducted on the numerical performance of\nthe proposed estimator and SCB. The proposed method is also illustrated by two\nreal data examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 14:52:03 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 01:22:14 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 17:31:23 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Wang", "Jiangyan", ""], ["Cao", "Guanqun", ""], ["Wang", "Li", ""], ["Yang", "Lijian", ""]]}, {"id": "1903.05701", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Rejoinder: \"Gene Hunting with Hidden Markov Model Knockoffs\"", "comments": "12 pages, 4 figures", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 35-45", "doi": "10.1093/biomet/asy075", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deepen and enlarge the reflection on the possible advantages\nof a knockoff approach to genome wide association studies (Sesia et al., 2018),\nstarting from the discussions in Bottolo & Richardson (2019); Jewell & Witten\n(2019); Rosenblatt et al. (2019) and Marchini (2019). The discussants bring up\na number of important points, either related to the knockoffs methodology in\ngeneral, or to its specific application to genetic studies. In the following we\noffer some clarifications, mention relevant recent developments and highlight\nsome of the still open problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:58:31 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1903.05726", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "A Multi-armed Bandit MCMC, with applications in sampling from doubly\n  intractable posterior", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are widely used to sample from\ncomplicated distributions, especially to sample from the posterior distribution\nin Bayesian inference. However, MCMC is not directly applicable when facing the\ndoubly intractable problem. In this paper, we discussed and compared two\nexisting solutions -- Pseudo-marginal Monte Carlo and Exchange Algorithm. This\npaper also proposes a novel algorithm: Multi-armed Bandit MCMC (MABMC), which\nchooses between two (or more) randomized acceptance ratios in each step. MABMC\ncould be applied directly to incorporate Pseudo-marginal Monte Carlo and\nExchange algorithm, with higher average acceptance probability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:38:48 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:39:01 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "1903.05828", "submitter": "Xiaowei Zhang", "authors": "Weiwei Fan and L. Jeff Hong and Xiaowei Zhang", "title": "Distributionally Robust Selection of the Best", "comments": "45 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specifying a proper input distribution is often a challenging task in\nsimulation modeling. In practice, there may be multiple plausible distributions\nthat can fit the input data reasonably well, especially when the data volume is\nnot large. In this paper, we consider the problem of selecting the best from a\nfinite set of simulated alternatives, in the presence of such input\nuncertainty. We model such uncertainty by an ambiguity set consisting of a\nfinite number of plausible input distributions, and aim to select the\nalternative with the best worst-case mean performance over the ambiguity set.\nWe refer to this problem as robust selection of the best (RSB). To solve the\nRSB problem, we develop a two-stage selection procedure and a sequential\nselection procedure; we then prove that both procedures can achieve at least a\nuser-specified probability of correct selection under mild conditions.\nExtensive numerical experiments are conducted to investigate the computational\nefficiency of the two procedures. Finally, we apply the RSB approach to study a\nqueueing system's staffing problem using synthetic data and an\nappointment-scheduling problem using real data from a large hospital in China.\nWe find that the RSB approach can generate decisions significantly better than\nother widely used approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 06:24:09 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Fan", "Weiwei", ""], ["Hong", "L. Jeff", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1903.05842", "submitter": "Ziyu Jia", "authors": "Ziyu Jia, Youfang Lin, Zehui Jiao, Yan Ma, Jing Wang", "title": "Detecting causality in multivariate time series via non-uniform\n  embedding", "comments": null, "journal-ref": null, "doi": "10.3390/e21121233", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal analysis based on non-uniform embedding schemes is an important way to\ndetect the underlying interactions between dynamic systems. However, there are\nstill some obstacles to estimate high-dimensional conditional mutual\ninformation and form optimal mixed embedding vector in traditional non-uniform\nembedding schemes. In this study, we present a new non-uniform embedding method\nframed in information theory to detect causality for multivariate time series,\nnamed LM-PMIME, which integrates the low-dimensional approximation of\nconditional mutual information and the mixed search strategy for the\nconstruction of the mixed embedding vector. We apply the proposed method to\nsimulations of linear stochastic, nonlinear stochastic, and chaotic systems,\ndemonstrating its superiority over partial conditional mutual information from\nmixed embedding (PMIME) method. Moreover, the proposed method works well for\nmultivariate time series with weak coupling strengths, especially for chaotic\nsystems. In the actual application, we show its applicability to epilepsy\nmultichannel electrocorticographic recordings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 07:42:19 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 13:10:14 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 12:46:49 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jia", "Ziyu", ""], ["Lin", "Youfang", ""], ["Jiao", "Zehui", ""], ["Ma", "Yan", ""], ["Wang", "Jing", ""]]}, {"id": "1903.06023", "submitter": "Rui Li", "authors": "Rui Li, Howard D. Bondell, Brian J. Reich", "title": "Deep Distribution Regression", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their flexibility and predictive performance, machine-learning based\nregression methods have become an important tool for predictive modeling and\nforecasting. However, most methods focus on estimating the conditional mean or\nspecific quantiles of the target quantity and do not provide the full\nconditional distribution, which contains uncertainty information that might be\ncrucial for decision making. In this article, we provide a general solution by\ntransforming a conditional distribution estimation problem into a constrained\nmulti-class classification problem, in which tools such as deep neural\nnetworks. We propose a novel joint binary cross-entropy loss function to\naccomplish this goal. We demonstrate its performance in various simulation\nstudies comparing to state-of-the-art competing methods. Additionally, our\nmethod shows improved accuracy in a probabilistic solar energy forecasting\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:19:39 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Li", "Rui", ""], ["Bondell", "Howard D.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1903.06092", "submitter": "Richard Samworth", "authors": "Min Xu and Richard J. Samworth", "title": "High-dimensional nonparametric density estimation via symmetry and shape\n  constraints", "comments": "93 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of high-dimensional nonparametric density estimation by\ntaking the class of log-concave densities on $\\mathbb{R}^p$ and incorporating\nwithin it symmetry assumptions, which facilitate scalable estimation algorithms\nand can mitigate the curse of dimensionality. Our main symmetry assumption is\nthat the super-level sets of the density are $K$-homothetic (i.e. scalar\nmultiples of a convex body $K \\subseteq \\mathbb{R}^p$). When $K$ is known, we\nprove that the $K$-homothetic log-concave maximum likelihood estimator based on\n$n$ independent observations from such a density has a worst-case risk bound\nwith respect to, e.g., squared Hellinger loss, of $O(n^{-4/5})$, independent of\n$p$. Moreover, we show that the estimator is adaptive in the sense that if the\ndata generating density admits a special form, then a nearly parametric rate\nmay be attained. We also provide worst-case and adaptive risk bounds in cases\nwhere $K$ is only known up to a positive definite transformation, and where it\nis completely unknown and must be estimated nonparametrically. Our estimation\nalgorithms are fast even when $n$ and $p$ are on the order of hundreds of\nthousands, and we illustrate the strong finite-sample performance of our\nmethods on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 15:54:28 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Xu", "Min", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1903.06286", "submitter": "Peng Ding", "authors": "Peng Ding, Fan Li", "title": "A bracketing relationship between difference-in-differences and\n  lagged-dependent-variable adjustment", "comments": "To appear in Political Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences is a widely-used evaluation strategy that draws\ncausal inference from observational panel data. Its causal identification\nrelies on the assumption of parallel trends, which is scale dependent and may\nbe questionable in some applications. A common alternative is a regression\nmodel that adjusts for the lagged dependent variable, which rests on the\nassumption of ignorability conditional on past outcomes. In the context of\nlinear models, \\citet{APbook} show that the difference-in-differences and\nlagged-dependent-variable regression estimates have a bracketing relationship.\nNamely, for a true positive effect, if ignorability is correct, then mistakenly\nassuming parallel trends will overestimate the effect; in contrast, if the\nparallel trends assumption is correct, then mistakenly assuming ignorability\nwill underestimate the effect. We show that the same bracketing relationship\nholds in general nonparametric (model-free) settings. We also extend the result\nto semiparametric estimation based on inverse probability weighting. We provide\nthree examples to illustrate the theoretical results with replication files in\n\\citet{ding2019bracketingData}.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:35:36 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 04:18:30 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 00:11:00 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ding", "Peng", ""], ["Li", "Fan", ""]]}, {"id": "1903.06287", "submitter": "Jeffrey N\\\"af", "authors": "Simon Hediger and Loris Michel and Jeffrey N\\\"af", "title": "On the Use of Random Forest for Two-Sample Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the line of classification-based two-sample testing, tests based on\nthe Random Forest classifier are proposed. The developed tests are easy to use,\nrequire almost no tuning, and are applicable for any distribution on\n$\\mathbb{R}^d$. Furthermore, the built-in variable importance measure of the\nRandom Forest gives potential insights into which variables make out the\ndifference in distribution. An asymptotic power analysis for the proposed tests\nis developed. Finally, two real-world applications illustrate the usefulness of\nthe introduced methodology. To simplify the use of the method, the R-package\n\"hypoRF\" is provided.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:42:08 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 13:31:31 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 20:02:28 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 05:05:25 GMT"}, {"version": "v5", "created": "Sun, 27 Dec 2020 10:12:41 GMT"}, {"version": "v6", "created": "Thu, 6 May 2021 12:53:33 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hediger", "Simon", ""], ["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""]]}, {"id": "1903.06488", "submitter": "Zachary Shahn", "authors": "Ellen C Caniglia, Eleanor J Murray, Miguel A Hernan and Zach Shahn", "title": "A Note on Estimating Optimal Dynamic Treatment Strategies Under Resource\n  Constraints Using Dynamic Marginal Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing strategies for determining the optimal treatment or monitoring\nstrategy typically assume unlimited access to resources. However, when a health\nsystem has resource constraints, such as limited funds, access to medication,\nor monitoring capabilities, medical decisions must balance impacts on both\nindividual and population health outcomes. That is, decisions should account\nfor competition between individuals in resource usage. One simple solution is\nto estimate the (counterfactual) resource usage under the possible\ninterventions and choose the optimal strategy for which resource usage is\nwithin acceptable limits. We propose a method to identify the optimal dynamic\nintervention strategy that leads to the best expected health outcome accounting\nfor a health system's resource constraints. We then apply this method to\ndetermine the optimal dynamic monitoring strategy for people living with HIV\nwhen resource limits on monitoring exist using observational data from the\nHIV-CAUSAL Collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:55:01 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Caniglia", "Ellen C", ""], ["Murray", "Eleanor J", ""], ["Hernan", "Miguel A", ""], ["Shahn", "Zach", ""]]}, {"id": "1903.06540", "submitter": "Atte Aalto", "authors": "Atte Aalto and Jorge Goncalves", "title": "Linear system identification from ensemble snapshot observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in transcriptomics techniques have caused a large demand in\ntailored computational methods for modelling gene expression dynamics from\nexperimental data. Recently, so-called single-cell experiments have\nrevolutionised genetic studies. These experiments yield gene expression data in\nsingle cell resolution for a large number of cells at a time. However, the\ncells are destroyed in the measurement process, and so the data consist of\nsnapshots of an ensemble evolving over time, instead of time series. The\nproblem studied in this article is how such data can be used in modelling gene\nregulatory dynamics. Two different paradigms are studied for linear system\nidentification. The first is based on tracking the evolution of the\ndistribution of cells over time. The second is based on the so-called\npseudotime concept, identifying a common trajectory through the state space,\nalong which cells propagate with different rates. Therefore, at any given time,\nthe population contains cells in different stages of the trajectory. Resulting\nmethods are compared in numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 13:24:12 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Aalto", "Atte", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1903.06552", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Christeen Wijethunga", "title": "On confidence intervals centered on bootstrap smoothed estimators", "comments": "The abstract and introduction have been improved. Also, an outline of\n  the proof of Theorem 1 has been taken from the Supplementary Material and\n  added to the appendix. arXiv admin note: text overlap with arXiv:1610.09802", "journal-ref": "Stat, 8, e233 (2019)", "doi": "10.1002/sta4.233", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap smoothed (bagged) estimators have been proposed as an improvement\non estimators found after preliminary data-based model selection. Efron, 2014,\nderived a widely applicable formula for a delta method approximation to the\nstandard deviation of the bootstrap smoothed estimator. He also considered a\nconfidence interval centered on the bootstrap smoothed estimator, with width\nproportional to the estimate of this standard deviation. Kabaila and\nWijethunga, 2019, assessed the performance of this confidence interval in the\nscenario of two nested linear regression models, the full model and a simpler\nmodel, for the case of known error variance and preliminary model selection\nusing a hypothesis test. They found that the performance of this confidence\ninterval was not substantially better than the usual confidence interval based\non the full model, with the same minimum coverage. We extend this assessment to\nthe case of unknown error variance by deriving a computationally convenient\nexact formula for the ideal (i.e. in the limit as the number of bootstrap\nreplications diverges to infinity) delta method approximation to the standard\ndeviation of the bootstrap smoothed estimator. Our results show that, unlike\nthe known error variance case, there are circumstances in which this confidence\ninterval has attractive properties.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 02:15:09 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 04:13:27 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Kabaila", "Paul", ""], ["Wijethunga", "Christeen", ""]]}, {"id": "1903.06568", "submitter": "Lukas Koch", "authors": "Lukas Koch", "title": "A response-matrix-centred approach to presenting cross-section\n  measurements", "comments": "26 pages, added reference to Phystat-nu", "journal-ref": null, "doi": "10.1088/1748-0221/14/09/P09013", "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current canonical approach to publishing cross-section data is to unfold\nthe reconstructed distributions. Detector effects like efficiency and smearing\nare undone mathematically, yielding distributions in true event properties.\nThis is an ill-posed problem, as even small statistical variations in the\nreconstructed data can lead to large changes in the unfolded spectra.\n  This work presents an alternative or complementary approach: the\nresponse-matrix-centred forward-folding approach. It offers a convenient way to\nforward-fold model expectations in truth space to reconstructed quantities.\nThese can then be compared to the data directly, similar to what is usually\ndone with full detector simulations within the experimental collaborations. For\nthis, the detector response (efficiency and smearing) is parametrised as a\nmatrix. The effects of the detector on the measurement of a given model is\nsimulated by simply multiplying the binned truth expectation values by this\nresponse matrix.\n  Systematic uncertainties in the detector response are handled by providing a\nset of matrices according to the prior distribution of the detector properties\nand marginalising over them. Background events can be included in the\nlikelihood calculation by giving background events their own bins in truth\nspace.\n  To facilitate a straight-forward use of response matrices, a new software\nframework has been developed: the Response Matrix Utilities (ReMU). ReMU is a\nPython package distributed via the Python Package Index. It only uses widely\navailable, standard scientific Python libraries and does not depend on any\ncustom experiment-specific software. It offers all methods needed to build\nresponse matrices from Monte Carlo data sets, use the response matrix to\nforward-fold truth-level model predictions, and compare the predictions to real\ndata using Bayesian or frequentist statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:19:32 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 13:05:54 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Koch", "Lukas", ""]]}, {"id": "1903.06616", "submitter": "Matt Wand", "authors": "Tui H. Nolan, Marianne Menictas and Matt P. Wand", "title": "Streamlined Computing for Variational Inference with Higher Level Random\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and present explicit algorithms to facilitate streamlined computing\nfor variational inference for models containing higher level random effects.\nExisting literature, such as Lee and Wand (2016), is such that streamlined\nvariational inference is restricted to mean field variational Bayes algorithms\nfor two-level random effects models. Here we provide the following extensions:\n(1) explicit Gaussian response mean field variational Bayes algorithms for\nthree-level models, (2) explicit algorithms for the alternative variational\nmessage passing approach in the case of two-level and three-level models, and\n(3) an explanation of how arbitrarily high levels of nesting can be handled\nbased on the recently published matrix algebraic results of the authors. A\npay-off from (2) is simple extension to non-Gaussian response models. In\nsummary, we remove barriers for streamlining variational inference algorithms\nbased on either the mean field variational Bayes approach or the variational\nmessage passing approach when higher level random effects are present.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:56:40 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 22:43:22 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 03:16:26 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 04:50:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Nolan", "Tui H.", ""], ["Menictas", "Marianne", ""], ["Wand", "Matt P.", ""]]}, {"id": "1903.06626", "submitter": "Paul Smith", "authors": "Paul A. Smith and Wesley Yung", "title": "A review and evaluation of the use of longitudinal approaches in\n  business surveys", "comments": null, "journal-ref": "Longitudinal and Life Course Studies 2019 vol 10 pp 491-511", "doi": "10.1332/175795919X15694142999134", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business surveys are not generally considered to be longitudinal by design.\nHowever, the largest businesses are almost always included in each wave of\nrecurrent surveys because they are essential for producing good estimates; and\nshort-period business surveys frequently make use of rotating panel designs to\nimprove the estimates of change by inducing sample overlaps between different\nperiods. These design features mean that business surveys share some\nmethodological challenges with longitudinal surveys. We review the longitudinal\nmethods and approaches which can be used to improve the design and operation of\nbusiness surveys, giving examples of their use. We also look in the other\ndirection, considering the aspects of longitudinal analysis which have the\npotential to improve the accuracy, relevance and interpretation of business\nsurvey outputs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 20:21:46 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 12:34:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Smith", "Paul A.", ""], ["Yung", "Wesley", ""]]}, {"id": "1903.06675", "submitter": "Bal\\'azs Dobi", "authors": "Bal\\'azs Dobi and Andr\\'as Zempl\\'eni", "title": "Markov Chain-based Cost-Optimal Control Charts for Healthcare Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control charts have traditionally been used in industrial statistics, but are\nconstantly seeing new areas of application, especially in the age of Industry\n4.0. This paper introduces a new method, which is suitable for applications in\nthe healthcare sector, especially for monitoring a health-characteristic of a\npatient. We adapt a Markov chain-based approach and develop a method in which\nnot only the shift size (i.e. the degradation of the patient's health) can be\nrandom, but the effect of the repair (i.e. treatment) and time between\nsamplings (i.e. visits) too. This means that we do not use many often-present\nassumptions which are usually not applicable for medical treatments. The\naverage cost of the protocol, which is determined by the time between samplings\nand the control limit, can be estimated using the stationary distribution of\nthe Markov chain.\n  Furthermore, we incorporate the standard deviation of the cost into the\noptimisation procedure, which is often very important from a process control\nviewpoint. The sensitivity of the optimal parameters and the resulting average\ncost and cost standard deviation on different parameter values is investigated.\nWe demonstrate the usefulness of the approach for real-life data of patients\ntreated in Hungary: namely the monitoring of cholesterol level of patients with\ncardiovascular event risk. The results showed that the optimal parameters from\nour approach can be somewhat different from the original medical parameters.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:01:06 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dobi", "Bal\u00e1zs", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1903.06768", "submitter": "Anindya Bhadra", "authors": "Yunfan Li, Jyotishka Datta, Bruce A. Craig, Anindya Bhadra", "title": "Joint Mean-Covariance Estimation via the Horseshoe with an Application\n  in Genomic Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seemingly unrelated regression is a natural framework for regressing multiple\ncorrelated responses on multiple predictors. The model is very flexible, with\nmultiple linear regression and covariance selection models being special cases.\nHowever, its practical deployment in genomic data analysis under a Bayesian\nframework is limited due to both statistical and computational challenges. The\nstatistical challenge is that one needs to infer both the mean vector and the\ninverse covariance matrix, a problem inherently more complex than separately\nestimating each. The computational challenge is due to the dimensionality of\nthe parameter space that routinely exceeds the sample size. We propose the use\nof horseshoe priors on both the mean vector and the inverse covariance matrix.\nThis prior has demonstrated excellent performance when estimating a mean vector\nor inverse covariance matrix separately. The current work shows these\nadvantages are also present when addressing both simultaneously. A full\nBayesian treatment is proposed, with a sampling algorithm that is linear in the\nnumber of predictors. MATLAB code implementing the algorithm is freely\navailable from github at https://github.com/liyf1988/HS_GHS. Extensive\nperformance comparisons are provided with both frequentist and Bayesian\nalternatives, and both estimation and prediction performances are verified on a\ngenomic data set.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:16:53 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 17:19:45 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Li", "Yunfan", ""], ["Datta", "Jyotishka", ""], ["Craig", "Bruce A.", ""], ["Bhadra", "Anindya", ""]]}, {"id": "1903.06850", "submitter": "Yunxiao Li", "authors": "Yunxiao Li, Yi-Juan Hu and Glen A. Satten", "title": "A Bottom-up Approach to Testing Hypotheses That Have a Branching Tree\n  Dependence Structure, with False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical analyses often involve testing large numbers of\nhypotheses. In many situations, these hypotheses may have an underlying tree\nstructure that not only helps determine the order that tests should be\nconducted but also imposes a dependency between tests that must be accounted\nfor. Our motivating example comes from testing the association between a trait\nof interest and groups of microbes that have been organized into operational\ntaxonomic units (OTUs) or amplicon sequence variants (ASVs). Given p-values\nfrom association tests for each individual OTU or ASV, we would like to know if\nwe can declare that a certain species, genus, or higher taxonomic grouping can\nbe considered to be associated with the trait. For this problem, a bottom-up\ntesting algorithm that starts at the lowest level of the tree (OTUs or ASVs)\nand proceeds upward through successively higher taxonomic groupings (species,\ngenus, family etc.) is required. We develop such a bottom-up testing algorithm\nthat controls the error rate of decisions made at higher levels in the tree,\nconditional on findings at lower levels in the tree. We further show this\nalgorithm controls the false discovery rate based on the global null hypothesis\nthat no taxa are associated with the trait. By simulation, we also show that\nour approach is better at finding driver taxa, the highest level taxa below\nwhich there are dense association signals. We illustrate our approach using\ndata from a study of the microbiome among patients with ulcerative colitis and\nhealthy controls.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 00:41:44 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Yunxiao", ""], ["Hu", "Yi-Juan", ""], ["Satten", "Glen A.", ""]]}, {"id": "1903.06936", "submitter": "Benjamin Sischka", "authors": "G\\\"oran Kauermann and Benjamin Sischka", "title": "EM based smooth Graphon Estimation using Bayesian and Spline based\n  Approaches", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes the estimation of a smooth graphon function for network\ndata analysis using principles of the EM algorithm. The approach considers\nboth, variability with respect to ordering the nodes of a network and smooth\nestimation of the graphon function by nonparametric regression. To do so,\n(linear) B-splines are used, which allow for smooth estimation of the graphon,\nconditional on the ordering of the nodes. This provides the M-step. The true\nordering of the nodes resulting from the graphon model remains unobserved and\nBayesian ideas are employed to obtain posterior samples, given the network\ndata. This yields the E-step. Combining both steps gives an EM based approach\nfor smooth graphon estimation. The proposed graphon estimate allows to explore\nboth the degree distribution and the ordering of the nodes with respect to\ntheir connectivity behavior. Variability and uncertainty is taken into account\nusing MCMC techniques. Examples and a simulation study support the\napplicability of the approach.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 14:56:11 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 10:04:19 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 16:11:34 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Kauermann", "G\u00f6ran", ""], ["Sischka", "Benjamin", ""]]}, {"id": "1903.06964", "submitter": "Rui Jin", "authors": "Rui Jin, Aixin Tan", "title": "Fast Markov chain Monte Carlo for high dimensional Bayesian regression\n  models with shrinkage priors", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, many Bayesian shrinkage models have been developed for\nlinear regression problems where the number of covariates, $p$, is large.\nComputing the intractable posterior are often done with three-block Gibbs\nsamplers (3BG), based on representing the shrinkage priors as scale mixtures of\nNormal distributions. An alternative computing tool is a state of the art\nHamiltonian Monte Carlo (HMC) method, which can be easily implemented in the\nStan software. However, we found both existing methods to be inefficient and\noften impractical for large $p$ problems. Following the general idea of\nRajaratnam et al. (2018), we propose two-block Gibbs samplers (2BG) for three\ncommonly used shrinkage models, namely, the Bayesian group lasso, the Bayesian\nsparse group lasso and the Bayesian fused lasso models. We demonstrate with\nsimulated and real data examples that the Markov chains underlying 2BG's\nconverge much faster than that of 3BG's, and no worse than that of HMC. At the\nsame time, the computing costs of 2BG's per iteration are as low as that of\n3BG's, and can be several orders of magnitude lower than that of HMC. As a\nresult, the newly proposed 2BG is the only practical computing solution to do\nBayesian shrinkage analysis for datasets with large $p$. Further, we provide\ntheoretical justifications for the superior performance of 2BG's. We establish\ngeometric ergodicity (GE) of Markov chains associated with the 2BG for each of\nthe three Bayesian shrinkage models. We also prove, for most cases of the\nBayesian group lasso and the Bayesian sparse group lasso model, the Markov\noperators for the 2BG chains are trace-class. Whereas for all cases of all\nthree Bayesian shrinkage models, the Markov operator for the 3BG chains are not\neven Hilbert-Schmidt.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 17:49:30 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 21:26:40 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Jin", "Rui", ""], ["Tan", "Aixin", ""]]}, {"id": "1903.06980", "submitter": "Simone Manganelli", "authors": "Simone Manganelli", "title": "Deciding with Judgment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decision maker starts from a judgmental decision and moves to the closest\nboundary of the confidence interval. This statistical decision rule is\nadmissible and does not perform worse than the judgmental decision with a\nprobability equal to the confidence level, which is interpreted as a\ncoefficient of statistical risk aversion. The confidence level is related to\nthe decision maker's aversion to uncertainty and can be elicited with\nlaboratory experiments using urns a la Ellsberg. The decision rule is applied\nto a problem of asset allocation for an investor whose judgmental decision is\nto keep all her wealth in cash.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 20:05:50 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Manganelli", "Simone", ""]]}, {"id": "1903.07006", "submitter": "Jun Li", "authors": "Jun Li, Minya Xu, Ping-Shou Zhong, Lingjun Li", "title": "Change Point Detection in the Mean of High-Dimensional Time Series Data\n  under Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time series are characterized by a large number of\nmeasurements and complex dependence, and often involve abrupt change points. We\npropose a new procedure to detect change points in the mean of high-dimensional\ntime series data. The proposed procedure incorporates spatial and temporal\ndependence of data and is able to test and estimate the change point occurred\non the boundary of time series. We study its asymptotic properties under mild\nconditions. Simulation studies demonstrate its robust performance through the\ncomparison with other existing methods. Our procedure is applied to an fMRI\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 00:06:18 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Li", "Jun", ""], ["Xu", "Minya", ""], ["Zhong", "Ping-Shou", ""], ["Li", "Lingjun", ""]]}, {"id": "1903.07208", "submitter": "Xi Chen", "authors": "Xi Chen and Wen-Xin Zhou", "title": "Robust Inference via Multiplier Bootstrap", "comments": "81 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the theoretical underpinnings of two fundamental\nstatistical inference problems, the construction of confidence sets and\nlarge-scale simultaneous hypothesis testing, in the presence of heavy-tailed\ndata. With heavy-tailed observation noise, finite sample properties of the\nleast squares-based methods, typified by the sample mean, are suboptimal both\ntheoretically and empirically. In this paper, we demonstrate that the adaptive\nHuber regression, integrated with the multiplier bootstrap procedure, provides\na useful robust alternative to the method of least squares. Our theoretical and\nempirical results reveal the effectiveness of the proposed method, and\nhighlight the importance of having inference methods that are robust to heavy\ntailedness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 00:11:30 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Xi", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1903.07239", "submitter": "Yuki Kawakubo", "authors": "Yuki Kawakubo and Genya Kobayashi", "title": "Small area estimation of general finite-population parameters based on\n  grouped data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new model-based approach to small area estimation of\ngeneral finite-population parameters based on grouped data or frequency data,\nwhich is often available from sample surveys. Grouped data contains information\non frequencies of some pre-specified groups in each area, for example the\nnumbers of households in the income classes, and thus provides more detailed\ninsight about small areas than area-level aggregated data. A direct application\nof the widely used small area methods, such as the Fay-Herriot model for\narea-level data and nested error regression model for unit-level data, is not\nappropriate since they are not designed for grouped data. The newly proposed\nmethod adopts the multinomial likelihood function for the grouped data. In\norder to connect the group probabilities of the multinomial likelihood and the\nauxiliary variables within the framework of small area estimation, we introduce\nthe unobserved unit-level quantities of interest which follows the linear mixed\nmodel with the random intercepts and dispersions after some transformation.\nThen the probabilities that a unit belongs to the groups can be derived and are\nused to construct the likelihood function for the grouped data given the random\neffects. The unknown model parameters (hyperparameters) are estimated by a\nnewly developed Monte Carlo EM algorithm using an efficient importance\nsampling. The empirical best predicts (empirical Bayes estimates) of small area\nparameters can be calculated by a simple Gibbs sampling algorithm. The\nnumerical performance of the proposed method is illustrated based on the\nmodel-based and design-based simulations. In the application to the city level\ngrouped income data of Japan, we complete the patchy maps of the Gini\ncoefficient as well as mean income across the country.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 02:53:45 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 07:00:30 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kawakubo", "Yuki", ""], ["Kobayashi", "Genya", ""]]}, {"id": "1903.07509", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J. Reich and Dipankar Bandyopadhyay", "title": "A Spatial Bayesian Semiparametric Mixture Model for Positive Definite\n  Matrices with Applications to Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": "10.1002/cjs.11601", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging (DTI) is a popular magnetic resonance imaging\ntechnique used to characterize microstructural changes in the brain. DTI\nstudies quantify the diffusion of water molecules in a voxel using an estimated\n3x3 symmetric positive definite diffusion tensor matrix. Statistical analysis\nof DTI data is challenging because the data are positive definite matrices.\nMatrix-variate information is often summarized by a univariate quantity, such\nas the fractional anisotropy (FA), leading to a loss of information.\nFurthermore, DTI analyses often ignore the spatial association of neighboring\nvoxels, which can lead to imprecise estimates. Although the spatial modeling\nliterature is abundant, modeling spatially dependent positive definite matrices\nis challenging. To mitigate these issues, we propose a matrix-variate Bayesian\nsemiparametric mixture model, where the positive definite matrices are\ndistributed as a mixture of inverse Wishart distributions with the spatial\ndependence captured by a Markov model for the mixture component labels.\nConjugacy and the double Metropolis-Hastings algorithm result in fast and\nelegant Bayesian computing. Our simulation study shows that the proposed method\nis more powerful than non-spatial methods. We also apply the proposed method to\ninvestigate the effect of cocaine use on brain structure. The contribution of\nour work is to provide a novel statistical inference tool for DTI analysis by\nextending spatial statistics to matrix-variate data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:32:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J.", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "1903.07594", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik", "title": "Combining Model and Parameter Uncertainty in Bayesian Neural Networks", "comments": "16 pages, 8 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural networks (BNNs) have recently regained a significant amount\nof attention in the deep learning community due to the development of scalable\napproximate Bayesian inference techniques. There are several advantages of\nusing Bayesian approach: Parameter and prediction uncertainty become easily\navailable, facilitating rigid statistical analysis. Furthermore, prior\nknowledge can be incorporated. However so far there have been no scalable\ntechniques capable of combining both model (structural) and parameter\nuncertainty. In this paper we introduce the concept of model uncertainty in\nBNNs and hence make inference in the joint space of models and parameters.\nMoreover, we suggest an adaptation of a scalable variational inference approach\nwith reparametrization of marginal inclusion probabilities to incorporate the\nmodel space constraints. Finally, we show that incorporating model uncertainty\nvia Bayesian model averaging and Bayesian model selection allows to drastically\nsparsify the structure of BNNs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:41:33 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 17:49:40 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 14:07:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""]]}, {"id": "1903.07677", "submitter": "Matthew Dixon", "authors": "Matthew F. Dixon and Nicholas G. Polson", "title": "Deep Fundamental Factor Models", "comments": null, "journal-ref": "Forthcoming in SIAM J. Financial Mathematics, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep fundamental factor models are developed to automatically capture\nnon-linearity and interaction effects in factor modeling. Uncertainty\nquantification provides interpretability with interval estimation, ranking of\nfactor importances and estimation of interaction effects. With no hidden layers\nwe recover a linear factor model and for one or more hidden layers, uncertainty\nbands for the sensitivity to each input naturally arise from the network\nweights. Using 3290 assets in the Russell 1000 index over a period of December\n1989 to January 2018, we assess a 49 factor model and generate information\nratios that are approximately 1.5x greater than the OLS factor model.\nFurthermore, we compare our deep fundamental factor model with a quadratic\nLASSO model and demonstrate the superior performance and robustness to\noutliers. The Python source code and the data used for this study are provided.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 19:10:09 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 19:57:38 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 17:22:04 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dixon", "Matthew F.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1903.07780", "submitter": "Kanchana Nadarajah", "authors": "Kanchana Nadarajah, Gael M Martin and Donald S Poskitt", "title": "Optimal Bias Correction of the Log-periodogram Estimator of the\n  Fractional Parameter: A Jackknife Approach", "comments": "57 pages", "journal-ref": null, "doi": "10.1016/j.jspi.2020.04.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the jackknife to bias correct the log-periodogram regression(LPR)\nestimator of the fractional parameter in a stationary fractionally integrated\nmodel. The weights for the jackknife estimator are chosen in such a way that\nbias reduction is achieved without the usual increase in asymptotic variance,\nwith the estimator viewed as `optimal' in this sense. The theoretical results\nare valid under both the non-overlapping and moving-block sub-sampling schemes\nthat can be used in the jackknife technique, and do not require the assumption\nof Gaussianity for the data generating process. A Monte Carlo study explores\nthe finite sample performance of different versions of the jackknife estimator,\nunder a variety of scenarios. The simulation experiments reveal that when the\nweights are constructed using the parameter values of the true data generating\nprocess, a version of the optimal jackknife estimator almost always\nout-performs alternative semi-parametric bias-corrected estimators. A feasible\nversion of the jackknife estimator, in which the weights are constructed using\nestimates of the unknown parameters, whilst not dominant overall, is still the\nleast biased estimator in some cases. Even when misspecified short run dynamics\nare assumed in the construction of the weights, the feasible jackknife still\nshows significant reduction in bias under certain designs. As is not\nsurprising, parametric maximum likelihood estimation out-performs all\nsemi-parametric methods when the true values of the short memory parameters are\nknown, but is dominated by the semi-parametric methods (in terms of bias) when\nthe short memory parameters need to be estimated, and in particular when the\nmodel is misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 00:52:26 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 11:00:26 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 14:37:21 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nadarajah", "Kanchana", ""], ["Martin", "Gael M", ""], ["Poskitt", "Donald S", ""]]}, {"id": "1903.07782", "submitter": "Yingrui Yang", "authors": "Yingrui Yang, Molin Wang", "title": "Semiparametric Methods for Exposure Misclassification in Propensity\n  Score-Based Time-to-Event Data Analysis", "comments": "Withdrawn due to grant related requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiology, identifying the effect of exposure variables in relation to\na time-to-event outcome is a classical research area of practical importance.\nIncorporating propensity score in the Cox regression model, as a measure to\ncontrol for confounding, has certain advantages when outcome is rare. However,\nin situations involving exposure measured with moderate to substantial error,\nidentifying the exposure effect using propensity score in Cox models remains a\nchallenging yet unresolved problem. In this paper, we propose an estimating\nequation method to correct for the exposure misclassification-caused bias in\nthe estimation of exposure-outcome associations. We also discuss the asymptotic\nproperties and derive the asymptotic variances of the proposed estimators. We\nconduct a simulation study to evaluate the performance of the proposed\nestimators in various settings. As an illustration, we apply our method to\ncorrect for the misclassification-caused bias in estimating the association of\nPM2.5 level with lung cancer mortality using a nationwide prospective cohort,\nthe Nurses' Health Study (NHS). The proposed methodology can be applied using\nour user-friendly R function published online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 00:57:16 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 07:50:43 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Yang", "Yingrui", ""], ["Wang", "Molin", ""]]}, {"id": "1903.07942", "submitter": "Martin Bladt", "authors": "Martin Bladt, Hansjoerg Albrecher, Jan Beirlant", "title": "Threshold selection and trimming in extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider removing lower order statistics from the classical Hill estimator\nin extreme value statistics, and compensating for it by rescaling the remaining\nterms. Trajectories of these trimmed statistics as a function of the extent of\ntrimming turn out to be quite flat near the optimal threshold value. For the\nregularly varying case, the classical threshold selection problem in tail\nestimation is then revisited, both visually via trimmed Hill plots and, for the\nHall class, also mathematically via minimizing the expected empirical variance.\nThis leads to a simple threshold selection procedure for the classical Hill\nestimator which circumvents the estimation of some of the tail characteristics,\na problem which is usually the bottleneck in threshold selection. As a\nby-product, we derive an alternative estimator of the tail index, which assigns\nmore weight to large observations, and works particularly well for relatively\nlighter tails. A simple ratio statistic routine is suggested to evaluate the\ngoodness of the implied selection of the threshold. We illustrate the\nfavourable performance and the potential of the proposed method with simulation\nstudies and real insurance data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 11:20:52 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 06:12:58 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 07:05:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bladt", "Martin", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "1903.08008", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n  Paul-Christian B\\\"urkner", "title": "Rank-normalization, folding, and localization: An improved $\\widehat{R}$\n  for assessing convergence of MCMC", "comments": "Two small fixes. Published in Bayesian analysis\n  https://doi.org/10.1214/20-BA1221", "journal-ref": null, "doi": "10.1214/20-BA1221", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo is a key computational tool in Bayesian statistics,\nbut it can be challenging to monitor the convergence of an iterative stochastic\nalgorithm. In this paper we show that the convergence diagnostic $\\widehat{R}$\nof Gelman and Rubin (1992) has serious flaws. Traditional $\\widehat{R}$ will\nfail to correctly diagnose convergence failures when the chain has a heavy tail\nor when the variance varies across the chains. In this paper we propose an\nalternative rank-based diagnostic that fixes these problems. We also introduce\na collection of quantile-based local efficiency measures, along with a\npractical approach for computing Monte Carlo error estimates for quantiles. We\nsuggest that common trace plots should be replaced with rank plots from\nmultiple chains. Finally, we give recommendations for how these methods should\nbe used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:12:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 18:39:02 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 14:16:29 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 07:38:26 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 07:58:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Simpson", "Daniel", ""], ["Carpenter", "Bob", ""], ["B\u00fcrkner", "Paul-Christian", ""]]}, {"id": "1903.08125", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Alessandro Rinaldo, Larry Wasserman", "title": "Predictive clustering", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to convert any clustering into a prediction set. This has the\neffect of converting the clustering into a (possibly overlapping) union of\nspheres or ellipsoids. The tuning parameters can be chosen to minimize the size\nof the prediction set. When applied to k-means clustering, this method solves\nseveral problems: the method tells us how to choose k, how to merge clusters\nand how to replace the Voronoi partition with more natural shapes. We show that\nthe same reasoning can be applied to other clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 17:33:36 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 02:11:49 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1903.08509", "submitter": "Yanxun Xu", "authors": "Yanxun Xu, Daniel Scharfstein, Peter M\\\"uller, Michael Daniels", "title": "A Bayesian Nonparametric Approach for Evaluating the Causal Effect of\n  Treatment in Randomized Trials with Semi-Competing Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric (BNP) approach to evaluate the causal\neffect of treatment in a randomized trial where a nonterminal event may be\ncensored by a terminal event, but not vice versa (i.e., semi-competing risks).\nBased on the idea of principal stratification, we define a novel estimand for\nthe causal effect of treatment on the nonterminal event. We introduce\nidentification assumptions, indexed by a sensitivity parameter, and show how to\ndraw inference using our BNP approach. We conduct simulation studies and\nillustrate our methodology using data from a brain cancer trial.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 14:00:27 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 09:37:49 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Xu", "Yanxun", ""], ["Scharfstein", "Daniel", ""], ["M\u00fcller", "Peter", ""], ["Daniels", "Michael", ""]]}, {"id": "1903.08605", "submitter": "Rui Gao", "authors": "Rui Gao and Filip Tronarp and Simo S\\\"arkk\\\"a", "title": "Iterated Extended Kalman Smoother-based Variable Splitting for\n  $L_1$-Regularized State Estimation", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TSP.2019.2935868", "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework for solving state estimation\nproblems with an additional sparsity-promoting $L_1$-regularizer term. We first\nformulate such problems as minimization of the sum of linear or nonlinear\nquadratic error terms and an extra regularizer, and then present novel\nalgorithms which solve the linear and nonlinear cases. The methods are based on\na combination of the iterated extended Kalman smoother and variable splitting\ntechniques such as alternating direction method of multipliers (ADMM). We\npresent a general algorithmic framework for variable splitting methods, where\nthe iterative steps involving minimization of the nonlinear quadratic terms can\nbe computed efficiently by iterated smoothing. Due to the use of state\nestimation algorithms, the proposed framework has a low per-iteration time\ncomplexity, which makes it suitable for solving a large-scale or\nhigh-dimensional state estimation problem. We also provide convergence results\nfor the proposed algorithms. The experiments show the promising performance and\nspeed-ups provided by the methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:38:22 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 11:12:17 GMT"}, {"version": "v3", "created": "Fri, 2 Aug 2019 16:50:05 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Gao", "Rui", ""], ["Tronarp", "Filip", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1903.08648", "submitter": "Johan Elkink", "authors": "Johan A. Elkink and Thomas U. Grund", "title": "Modelling Diffusion through Statistical Network Analysis: A Simulation\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of international relations by definition deals with\ninterdependencies among countries. One form of interdependence between\ncountries is the diffusion of country-level features, such as policies,\npolitical regimes, or conflict. In these studies, the outcome variable tends to\nbe categorical, and the primary concern is the clustering of the outcome\nvariable among connected countries. Statistically, such clustering is studied\nwith spatial econometric models. This paper instead proposes the use of a\nstatistical network approach to model diffusion with a binary outcome variable.\nUsing statistical network instead of spatial econometric models allows for a\nmore natural specification of the diffusion process, assuming autocorrelation\nin the outcomes rather than the corresponding latent variable, and it\nsimplifies the inclusion of temporal dynamics, higher level interdependencies\nand interactions between network ties and country-level features. In our\nsimulations, the performance of the Stochastic Actor-Oriented Model (SAOM)\nestimator is evaluated. Our simulation results show that spatial parameters and\ncoefficients on additional covariates in a static binary spatial autoregressive\nmodel are accurately recovered when using SAOM, albeit on a different scale. To\ndemonstrate the use of this model, the paper applies the model to the\ninternational diffusion of same-sex marriage.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 02:36:36 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Elkink", "Johan A.", ""], ["Grund", "Thomas U.", ""]]}, {"id": "1903.08656", "submitter": "Michael Trosset", "authors": "Michael W. Trosset, Carey E. Priebe", "title": "Approximate Information Tests on Statistical Submanifolds", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric inference posits a statistical model that is a specified family of\nprobability distributions. Restricted inference, e.g., restricted likelihood\nratio testing, attempts to exploit the structure of a statistical submodel that\nis a subset of the specified family. We consider the problem of testing a\nsimple hypothesis against alternatives from such a submodel. In the case of an\nunknown submodel, it is not clear how to realize the benefits of restricted\ninference. To do so, we first construct information tests that are locally\nasymptotically equivalent to likelihood ratio tests. Information tests are\nconceptually appealing but (in general) computationally intractable. However,\nunlike restricted likelihood ratio tests, restricted information tests can be\napproximated even when the statistical submodel is unknown. We construct\napproximate information tests using manifold learning procedures to extract\ninformation from samples of an unknown (or intractable) submodel, thereby\nproviding a roadmap for computational solutions to a class of previously\nimpenetrable problems in statistical inference. Examples illustrate the\nefficacy of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:01:43 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Trosset", "Michael W.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1903.08687", "submitter": "Carlos Matr\\'an", "authors": "Eustasio del Barrio, Hristo Inouzhe and Carlos Matr\\'an", "title": "On approximate validation of models: A Kolmogorov-Smirnov based approach", "comments": "14 figures, 32 pages", "journal-ref": null, "doi": "10.1007/s11749-019-00691-1", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical tests of fit typically reject a model for large enough real data\nsamples. In contrast, often in statistical practice a model offers a good\ndescription of the data even though it is not the \"true\" random generator. We\nconsider a more flexible approach based on contamination neighbourhoods around\na model. Using trimming methods and the Kolmogorov metric we introduce a\nfunctional statistic measuring departures from a contaminated model and the\nassociated estimator corresponding to its sample version. We show how this\nestimator allows testing of fit for the (slightly) contaminated model vs\nsensible deviations from it, with uniformly exponentially small type I and type\nII error probabilities. We also address the asymptotic behavior of the\nestimator showing that, under suitable regularity conditions, it asymptotically\nbehaves as the supremum of a Gaussian process. As an application we explore\nmethods of comparison between descriptive models based on the paradigm of model\nfalseness. We also include some connections of our approach with the\nFalse-Discovery-Rate setting, showing competitive behavior when estimating the\ncontamination level, although applicable in a wider framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 18:30:59 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Inouzhe", "Hristo", ""], ["Matr\u00e1n", "Carlos", ""]]}, {"id": "1903.08747", "submitter": "Kenneth Hung", "authors": "Kenneth Hung, William Fithian", "title": "Statistical Methods for Replicability Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale replication studies like the Reproducibility Project: Psychology\n(RP:P) provide invaluable systematic data on scientific replicability, but most\nanalyses and interpretations of the data fail to agree on the definition of\n\"replicability\" and disentangle the inexorable consequences of known selection\nbias from competing explanations. We discuss three concrete definitions of\nreplicability based on (1) whether published findings about the signs of\neffects are mostly correct, (2) how effective replication studies are in\nreproducing whatever true effect size was present in the original experiment,\nand (3) whether true effect sizes tend to diminish in replication. We apply\ntechniques from multiple testing and post-selection inference to develop new\nmethods that answer these questions while explicitly accounting for selection\nbias. Our analyses suggest that the RP:P dataset is largely consistent with\npublication bias due to selection of significant effects. The methods in this\npaper make no distributional assumptions about the true effect sizes.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:15:12 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 00:04:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Hung", "Kenneth", ""], ["Fithian", "William", ""]]}, {"id": "1903.08920", "submitter": "Adrien Ehrhardt", "authors": "Adrien Ehrhardt, Christophe Biernacki, Vincent Vandewalle, Philippe\n  Heinrich", "title": "Feature quantization for parsimonious and interpretable predictive\n  models", "comments": "9 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For regulatory and interpretability reasons, logistic regression is still\nwidely used. To improve prediction accuracy and interpretability, a\npreprocessing step quantizing both continuous and categorical data is usually\nperformed: continuous features are discretized and, if numerous, levels of\ncategorical features are grouped. An even better predictive accuracy can be\nreached by embedding this quantization estimation step directly into the\npredictive estimation step itself. But doing so, the predictive loss has to be\noptimized on a huge set. To overcome this difficulty, we introduce a specific\ntwo-step optimization strategy: first, the optimization problem is relaxed by\napproximating discontinuous quantization functions by smooth functions; second,\nthe resulting relaxed optimization problem is solved via a particular neural\nnetwork. The good performances of this approach, which we call glmdisc, are\nillustrated on simulated and real data from the UCI library and Cr\\'edit\nAgricole Consumer Finance (a major European historic player in the consumer\ncredit market).\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 10:54:16 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Ehrhardt", "Adrien", ""], ["Biernacki", "Christophe", ""], ["Vandewalle", "Vincent", ""], ["Heinrich", "Philippe", ""]]}, {"id": "1903.09247", "submitter": "Anna Kutschireiter", "authors": "Anna Kutschireiter, Simone Carlo Surace, Jean-Pascal Pfister", "title": "The Hitchhiker's Guide to Nonlinear Filtering", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear filtering is the problem of online estimation of a dynamic hidden\nvariable from incoming data and has vast applications in different fields,\nranging from engineering, machine learning, economic science and natural\nsciences. We start our review of the theory on nonlinear filtering from the\nsimplest `filtering' task we can think of, namely static Bayesian inference.\nFrom there we continue our journey through discrete-time models, which is\nusually encountered in machine learning, and generalize to and further\nemphasize continuous-time filtering theory. The idea of changing the\nprobability measure connects and elucidates several aspects of the theory, such\nas the parallels between the discrete- and continuous-time problems and between\ndifferent observation models. Furthermore, it gives insight into the\nconstruction of particle filtering algorithms. This tutorial is targeted at\nscientists and engineers and should serve as an introduction to the main ideas\nof nonlinear filtering, and as a segway to more advanced and specialized\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:46:02 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 21:29:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kutschireiter", "Anna", ""], ["Surace", "Simone Carlo", ""], ["Pfister", "Jean-Pascal", ""]]}, {"id": "1903.09253", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Doubly stochastic models for replicated spatio-temporal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a log-linear model for the latent intensity functions of\na replicated spatio-temporal point process. By simultaneously fitting\ncorrelated spatial and temporal Karhunen-Lo\\`eve expansions, the model produces\nspatial and temporal components that are usually easy to interpret and capture\nthe most important modes of variation and spatio-temporal correlation of the\nprocess. The asymptotic distribution of the estimators is derived. The finite\nsample properties are studied by simulations. As an example of application, we\nanalyze bike usage patterns on the Divvy bike sharing system of the city of\nChicago.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 22:03:12 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1903.09364", "submitter": "Andrew Bray", "authors": "Simon Couch, Zeki Kazan, Kaiyan Shi, Andrew Bray, and Adam Groce", "title": "Differentially Private Nonparametric Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis tests are a crucial statistical tool for data mining and are the\nworkhorse of scientific research in many fields. Here we study differentially\nprivate tests of independence between a categorical and a continuous variable.\nWe take as our starting point traditional nonparametric tests, which require no\ndistributional assumption (e.g., normality) about the data distribution. We\npresent private analogues of the Kruskal-Wallis, Mann-Whitney, and Wilcoxon\nsigned-rank tests, as well as the parametric one-sample t-test. These tests use\nnovel test statistics developed specifically for the private setting. We\ncompare our tests to prior work, both on parametric and nonparametric tests. We\nfind that in all cases our new nonparametric tests achieve large improvements\nin statistical power, even when the assumptions of parametric tests are met.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:41:11 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Couch", "Simon", ""], ["Kazan", "Zeki", ""], ["Shi", "Kaiyan", ""], ["Bray", "Andrew", ""], ["Groce", "Adam", ""]]}, {"id": "1903.09445", "submitter": "Ian Dryden", "authors": "Ian L. Dryden, Kwang-Rae Kim, Charles A. Laughton and Huiling Le", "title": "Principal nested shape space analysis of molecular dynamics data", "comments": "24 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics simulations produce huge datasets of temporal sequences of\nmolecules. It is of interest to summarize the shape evolution of the molecules\nin a succinct, low-dimensional representation. However, Euclidean techniques\nsuch as principal components analysis (PCA) can be problematic as the data may\nlie far from in a flat manifold. Principal nested spheres gives a fundamentally\ndifferent decomposition of data from the usual Euclidean sub-space based PCA\n(Jung, Dryden and Marron, 2012, Biometrika). Sub-spaces of successively lower\ndimension are fitted to the data in a backwards manner, with the aim of\nretaining signal and dispensing with noise at each stage. We adapt the\nmethodology to 3D sub-shape spaces and provide some practical fitting\nalgorithms. The methodology is applied to cluster analysis of peptides, where\ndifferent states of the molecules can be identified. Also, the temporal\ntransitions between cluster states are explored.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 11:14:49 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Dryden", "Ian L.", ""], ["Kim", "Kwang-Rae", ""], ["Laughton", "Charles A.", ""], ["Le", "Huiling", ""]]}, {"id": "1903.09478", "submitter": "Cristina Fernandes", "authors": "Luis Roque, Cristina A. C. Fernandes and Tony Silva", "title": "Optimal Combination Forecasts on Retail Multi-Dimensional Sales Data", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data in the retail world are particularly rich in terms of\ndimensionality, and these dimensions can be aggregated in groups or\nhierarchies. Valuable information is nested in these complex structures, which\nhelps to predict the aggregated time series data. From a portfolio of brands\nunder HUUB's monitoring, we selected two to explore their sales behaviour,\nleveraging the grouping properties of their product structure. Using\nstatistical models, namely SARIMA, to forecast each level of the hierarchy, an\noptimal combination approach was used to generate more consistent forecasts in\nthe higher levels. Our results show that the proposed methods can indeed\ncapture nested information in the more granular series, helping to improve the\nforecast accuracy of the aggregated series. The Weighted Least Squares (WLS)\nmethod surpasses all other methods proposed in the study, including the Minimum\nTrace (MinT) reconciliation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:53:23 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Roque", "Luis", ""], ["Fernandes", "Cristina A. C.", ""], ["Silva", "Tony", ""]]}, {"id": "1903.09668", "submitter": "Yuexi Wang", "authors": "Yuexi Wang, Nicholas G. Polson, Vadim O. Sokolov", "title": "Scalable Data Augmentation for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable Data Augmentation (SDA) provides a framework for training deep\nlearning models using auxiliary hidden layers. Scalable MCMC is available for\nnetwork training and inference. SDA provides a number of computational\nadvantages over traditional algorithms, such as avoiding backtracking, local\nmodes and can perform optimization with stochastic gradient descent (SGD) in\nTensorFlow. Standard deep neural networks with logit, ReLU and SVM activation\nfunctions are straightforward to implement. To illustrate our architectures and\nmethodology, we use P\\'{o}lya-Gamma logit data augmentation for a number of\nstandard datasets. Finally, we conclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 18:28:20 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wang", "Yuexi", ""], ["Polson", "Nicholas G.", ""], ["Sokolov", "Vadim O.", ""]]}, {"id": "1903.09741", "submitter": "Qiang Sun", "authors": "Jianqing Fan, Bai Jiang, Qiang Sun", "title": "Bayesian Factor-adjusted Sparse Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the high-dimensional linear regression with highly\ncorrelated covariates. In this setup, the traditional sparsity assumption on\nthe regression coefficients often fails to hold, and consequently many model\nselection procedures do not work. To address this challenge, we model the\nvariations of covariates by a factor structure. Specifically, strong\ncorrelations among covariates are explained by common factors and the remaining\nvariations are interpreted as idiosyncratic components of each covariate. This\nleads to a factor-adjusted regression model with both common factors and\nidiosyncratic components as covariates. We generalize the traditional sparsity\nassumption accordingly and assume that all common factors but only a small\nnumber of idiosyncratic components contribute to the response. A Bayesian\nprocedure with a spike-and-slab prior is then proposed for parameter estimation\nand model selection. Simulation studies show that our Bayesian method\noutperforms its lasso analogue, manifests insensitivity to the overestimates of\nthe number of common factors, pays a negligible price in the no correlation\ncase, and scales up well with increasing sample size, dimensionality and\nsparsity. Numerical results on a real dataset of U.S. bond risk premia and\nmacroeconomic indicators lend strong support to our methodology.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 01:01:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Fan", "Jianqing", ""], ["Jiang", "Bai", ""], ["Sun", "Qiang", ""]]}, {"id": "1903.09873", "submitter": "Emil Aas Stoltenberg", "authors": "Emil A. Stoltenberg, Per A. Mykland and Lan Zhang", "title": "A CLT for second difference estimators with an application to volatility\n  and intensity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a general method for estimating the quadratic\ncovariation of one or more spot parameters processes associated with continuous\ntime semimartingales. This estimator is applicable to a wide range of spot\nparameter processes, and may also be used to estimate the leverage effect of\nstochastic volatility models. The estimator we introduce is based on sums of\nsquared increments of second differences of the observed process, and the\nintervals over which the differences are computed are rolling and overlapping.\nThis latter feature lets us take full advantage of the data, and, by\nsufficiency considerations, ought to outperform estimators that are only based\non one partition of the observational window. The main result of the paper is a\ncentral limit theorem for such triangular array rolling quadratic variations.\nWe highlight the wide applicability of this theorem by showcasing how it might\nbe applied to a novel leverage effect estimator. The principal motivation for\nthe present study, however, is that the discrete times at which a continuous\ntime semimartingale is observed might depend on features of the observable\nprocess other than its level, such as its (non-observable) spot-volatility\nprocess. As the main application of our estimator, we therefore show how it may\nbe used to estimate the quadratic covariation between the spot-volatility\nprocess and the intensity process of the observation times, when both of these\nare taken to be semimartingales. The finite sample properties of this estimator\nare studied by way of a simulation experiment, and we also apply this estimator\nin an empirical analysis of the Apple stock. Our analysis of the Apple stock\nindicates a rather strong correlation between the spot volatility process of\nthe log-prices process and the times at which this stock is traded (hence\nobserved).\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2019 20:00:13 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 21:44:35 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 15:51:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Stoltenberg", "Emil A.", ""], ["Mykland", "Per A.", ""], ["Zhang", "Lan", ""]]}, {"id": "1903.09935", "submitter": "Dominik Sieradzki", "authors": "Dominik Sieradzki, Wojciech Zieli\\'nski", "title": "Cost Issue in Estimation of Proportion in a Finite Population Divided\n  Among Two Strata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimation of the proportion of units with a given attribute\nin a~finite population is considered. From the population a sample is drawn due\nto the simple random sampling without replacement. There are limited funds for\nconducting survey sample. Suppose that the population is divided into two\nstrata. The question now arises: how should sample sizes be chosen from each\nstrata to obtain the best estimation of proportion without exceeding the budget\nplanned. In the paper it is shown, that with the appropriate sample allocation\nthe variance of the stratified estimator may be reduced up to 30% off of the\nstandard, unstratified estimator.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 06:51:52 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Sieradzki", "Dominik", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.09981", "submitter": "Evandro Konzen", "authors": "Evandro Konzen, Jian Qing Shi and Zhanfeng Wang", "title": "Modeling Function-Valued Processes with Nonseparable and/or\n  Nonstationary Covariance Structure", "comments": "Added subsection 2.2.1: Local Interpretation of the Varying\n  Anisotropy Matrix; Replaced simulation studies; Replaced application by two\n  new ones; Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a general Bayesian framework on modeling multidimensional\nfunction-valued processes by using a Gaussian process or a heavy-tailed process\nas a prior, enabling us to handle nonseparable and/or nonstationary covariance\nstructure. The nonstationarity is introduced by a convolution-based approach\nthrough a varying anisotropy matrix, whose parameters vary along the input\nspace and are estimated via a local empirical Bayesian method. For the varying\nmatrix, we propose to use a spherical parametrization, leading to unconstrained\nand interpretable parameters. The unconstrained nature allows the parameters to\nbe modeled as a nonparametric function of time, spatial location or other\ncovariates. The interpretation of the parameters is based on closed-form\nexpressions, providing valuable insights into nonseparable covariance\nstructures. Furthermore, to extract important information in data with complex\ncovariance structure, the Bayesian framework can decompose the function-valued\nprocesses using the eigenvalues and eigensurfaces calculated from the estimated\ncovariance structure. The results are demonstrated by simulation studies and by\nan application to wind intensity data. Supplementary materials for this article\nare available online.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 12:19:41 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 15:22:52 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Konzen", "Evandro", ""], ["Shi", "Jian Qing", ""], ["Wang", "Zhanfeng", ""]]}, {"id": "1903.10058", "submitter": "Jonathan Moyer", "authors": "Jonathan Moyer and Ken Kleinman", "title": "Correct power for cluster-randomized difference-in-difference trials\n  with loss to follow-up", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster randomized trials with measurements at baseline can improve power\nover post-test only designs by using difference in difference designs. However,\nsubjects may be lost to follow-up between the baseline and follow-up periods.\nWhile equations for sample size and variance have been developed assuming no\nloss to follow-up (\"cohort\") and completely different subjects at baseline and\nfollow-up (\"cross-sectional\") difference in difference designs, equations have\nyet to be developed when some subjects are observed in both periods (\"mixture\"\ndesigns). We present a general equation for calculating the variance in\ndifference in difference designs and derive special cases assuming loss to\nfollow-up with replacement of lost subjects and assuming loss to follow-up with\nno replacement but retaining the baseline measurements of all subjects.\nRelative efficiency plots, plots of variance against subject autocorrelation,\nand plots of variance by follow-up rate and subject autocorrelation are used to\ncompare cohort, cross-sectional, and mixture approaches. Results indicate that\nwhen loss to follow-up to uncommon, mixture designs are almost as efficient as\ncohort designs with a given initial sample size. When loss to follow-up is\ncommon, mixture designs with full replacement maintain efficiency relative to\ncohort designs. Finally, our results provide guidance on whether to replace\nlost subjects during trial design and analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 21:01:57 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Moyer", "Jonathan", ""], ["Kleinman", "Ken", ""]]}, {"id": "1903.10184", "submitter": "Murray Pollock", "authors": "Paul A. Jenkins, Murray Pollock, Gareth O. Roberts and Michael\n  S{\\o}rensen", "title": "Simulating bridges using confluent diffusions", "comments": "Significant revision of prior submission, with an improved\n  methodology which is far broader in its applicability. Updated author\n  listing. 19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusions are a fundamental class of models in many fields, including\nfinance, engineering, and biology. Simulating diffusions is challenging as\ntheir sample paths are infinite-dimensional and their transition functions are\ntypically intractable. In statistical settings such as parameter inference for\ndiscretely observed diffusions, we require simulation techniques for diffusions\nconditioned on hitting a given endpoint, which introduces further complication.\nIn this paper we introduce a Markov chain Monte Carlo algorithm for simulating\nbridges of ergodic diffusions which (i) is exact in the sense that there is no\ndiscretisation error, (ii) has computational cost that is linear in the\nduration of the bridges, and (iii) provides bounds on local maxima and minima\nof the simulated trajectory. Our approach works directly on diffusion path\nspace, by constructing a proposal (which we term a confluence) that is then\ncorrected with an accept/reject step in a pseudo-marginal algorithm. Our method\nrequires only the simulation of unconditioned diffusion sample paths. We apply\nour approach to the simulation of Langevin diffusion bridges, a practical\nproblem arising naturally in many situations, such as statistical inference in\ndistributed settings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:57:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:21:04 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1903.10199", "submitter": "Oliver Dukes", "authors": "Oliver Dukes, Stijn Vansteelandt", "title": "Uniformly valid confidence intervals for conditional treatment effects\n  in misspecified high-dimensional models", "comments": "40 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliminating the effect of confounding in observational studies typically\ninvolves fitting a model for an outcome adjusted for covariates. When, as\noften, these covariates are high-dimensional, this necessitates the use of\nsparse estimators such as the Lasso, or other regularisation approaches. Naiive\nuse of such estimators yields confidence intervals for the conditional\ntreatment effect parameter that are not uniformly valid. Moreover, as the\nnumber of covariates grows with sample size, correctly specifying a model for\nthe outcome is non-trivial. In this work, we deal with both of these concerns\nsimultaneously, delivering confidence intervals for conditional treatment\neffects that are uniformly valid, regardless of whether the outcome model is\ncorrect. This is done by incorporating an additional model for the\ntreatment-selection mechanism. When both models are correctly specified, we can\nweaken the standard conditions on model sparsity. Our procedure extends to\nmultivariate treatment effect parameters and complex longitudinal settings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 09:32:50 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dukes", "Oliver", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1903.10221", "submitter": "Christopher Pooley Dr", "authors": "C. M. Pooley, S. C. Bishop, A. Doeschl-Wilson and G. Marion", "title": "Posterior-based proposals for speeding up Markov chain Monte Carlo", "comments": "54 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is widely used for Bayesian inference in\nmodels of complex systems. Performance, however, is often unsatisfactory in\nmodels with many latent variables due to so-called poor mixing, necessitating\ndevelopment of application specific implementations. This paper introduces\n\"posterior-based proposals\" (PBPs), a new type of MCMC update applicable to a\nhuge class of statistical models (whose conditional dependence structures are\nrepresented by directed acyclic graphs). PBPs generates large joint updates in\nparameter and latent variable space, whilst retaining good acceptance rates\n(typically 33%). Evaluation against other approaches (from standard Gibbs /\nrandom walk updates to state-of-the-art Hamiltonian and particle MCMC methods)\nwas carried out for widely varying model types: an individual-based model for\ndisease diagnostic test data, a financial stochastic volatility model, a mixed\nmodel used in statistical genetics and a population model used in ecology.\nWhilst different methods worked better or worse in different scenarios, PBPs\nwere found to be either near to the fastest or significantly faster than the\nnext best approach (by up to a factor of 10). PBPs therefore represent an\nadditional general purpose technique that can be usefully applied in a wide\nvariety of contexts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:16:08 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:07:20 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pooley", "C. M.", ""], ["Bishop", "S. C.", ""], ["Doeschl-Wilson", "A.", ""], ["Marion", "G.", ""]]}, {"id": "1903.10315", "submitter": "Maja Von Cube", "authors": "Maja von Cube and Martin Schumacher and Martin Wolkewitz", "title": "Causal inference with multi-state models - estimands and estimators of\n  the population-attributable fraction", "comments": "A revised version of this manuscript has been submitted to a journal\n  on March 8 2019", "journal-ref": "J R Stat Soc A Stat, 2019", "doi": "10.1111/rssa.12486", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population-attributable fraction (PAF) is a popular epidemiological\nmeasure for the burden of a harmful exposure within a population. It is often\ninterpreted causally as proportion of preventable cases after an elimination of\nexposure. Originally, the PAF has been defined for cohort studies of fixed\nlength with a baseline exposure or cross-sectional studies.\n  An extension of the definition to complex time-to-event data is not\nstraightforward. We revise the proposed approaches in literature and provide a\nclear concept of the PAF for these data situations. The conceptualization is\nachieved by a proper differentiation between estimands and estimators as well\nas causal effect measures and measures of association.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 13:41:37 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["von Cube", "Maja", ""], ["Schumacher", "Martin", ""], ["Wolkewitz", "Martin", ""]]}, {"id": "1903.10353", "submitter": "Thitiya Theparod", "authors": "Peter Neal and Thitiya Theparod", "title": "The basic reproduction number, $R_0$, in structured populations", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a straightforward approach to defining and deriving\nthe key epidemiological quantity, the basic reproduction number, $R_0$, for\nMarkovian epidemics in structured populations. The methodology derived is\napplicable to, and demonstrated on, both $SIR$ and $SIS$ epidemics and allows\nfor population as well as epidemic dynamics. The approach taken is to consider\nthe epidemic process as a multitype process by identifying and classifying the\ndifferent types of infectious units along with the infections from, and the\ntransitions between, infectious units. For the household model, we show that\nour expression for $R_0$ agrees with earlier work despite the alternative\nnature of the construction of the mean reproductive matrix, and hence, the\nbasic reproduction number.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:23:00 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Neal", "Peter", ""], ["Theparod", "Thitiya", ""]]}, {"id": "1903.10462", "submitter": "Dhaker Hamza", "authors": "Hamza Dhakera, El Hadji Demeb and Youssou Cissb", "title": "$\\beta$-Divergence loss for the kernel density estimation with bias\n  reduced", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allthough nonparametric kernel density estimation with bias reduce is\nnowadays a standard technique in explorative data-analysis, there is still a\nbig dispute on how to assess the quality of the estimate and which choice of\nbandwidth is optimal. This article examines the most important bandwidth\nselection methods for kernel density estimation with bias reduce, in\nparticular, normal reference, least squares cross-validation, biased\ncrossvalidation and $\\beta$-Divergence loss. Methods are described and\nexpressions are presented. We will compare these various bandwidth selector on\nsimulated data. As an example of real data, we will use econometric data sets\nCO2 per capita in example 1 and the second\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:53:49 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dhakera", "Hamza", ""], ["Demeb", "El Hadji", ""], ["Cissb", "Youssou", ""]]}, {"id": "1903.10464", "submitter": "Martin Jullum PhD", "authors": "Kjersti Aas, Martin Jullum, Anders L{\\o}land", "title": "Explaining individual predictions when features are dependent: More\n  accurate approximations to Shapley values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining complex or seemingly simple machine learning models is an\nimportant practical problem. We want to explain individual predictions from a\ncomplex machine learning model by learning simple, interpretable explanations.\nShapley values is a game theoretic concept that can be used for this purpose.\nThe Shapley value framework has a series of desirable theoretical properties,\nand can in principle handle any predictive model. Kernel SHAP is a\ncomputationally efficient approximation to Shapley values in higher dimensions.\nLike several other existing methods, this approach assumes that the features\nare independent, which may give very wrong explanations. This is the case even\nif a simple linear model is used for predictions. In this paper, we extend the\nKernel SHAP method to handle dependent features. We provide several examples of\nlinear and non-linear models with various degrees of feature dependence, where\nour method gives more accurate approximations to the true Shapley values. We\nalso propose a method for aggregating individual Shapley values, such that the\nprediction can be explained by groups of dependent variables.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:57:11 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 08:07:18 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 13:31:18 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Aas", "Kjersti", ""], ["Jullum", "Martin", ""], ["L\u00f8land", "Anders", ""]]}, {"id": "1903.10498", "submitter": "Sean McGrath", "authors": "Sean McGrath, XiaoFei Zhao, Russell Steele, Brett D. Thombs, Andrea\n  Benedetti and the DEPRESsion Screening Data (DEPRESSD) Collaboration", "title": "Estimating the sample mean and standard deviation from commonly reported\n  quantiles in meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers increasingly use meta-analysis to synthesize the results of\nseveral studies in order to estimate a common effect. When the outcome variable\nis continuous, standard meta-analytic approaches assume that the primary\nstudies report the sample mean and standard deviation of the outcome. However,\nwhen the outcome is skewed, authors sometimes summarize the data by reporting\nthe sample median and one or both of (i) the minimum and maximum values and\n(ii) the first and third quartiles, but do not report the mean or standard\ndeviation. To include these studies in meta-analysis, several methods have been\ndeveloped to estimate the sample mean and standard deviation from the reported\nsummary data. A major limitation of these widely used methods is that they\nassume that the outcome distribution is normal, which is unlikely to be tenable\nfor studies reporting medians. We propose two novel approaches to estimate the\nsample mean and standard deviation when data are suspected to be non-normal.\nOur simulation results and empirical assessments show that the proposed methods\noften perform better than the existing methods when applied to non-normal data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 17:55:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["McGrath", "Sean", "", "DEPRESSD"], ["Zhao", "XiaoFei", "", "DEPRESSD"], ["Steele", "Russell", "", "DEPRESSD"], ["Thombs", "Brett D.", "", "DEPRESSD"], ["Benedetti", "Andrea", "", "DEPRESSD"], ["Data", "the DEPRESsion Screening", "", "DEPRESSD"], ["Collaboration", "", ""]]}, {"id": "1903.10766", "submitter": "Jaromil Frossard", "authors": "Jaromil Frossard, Olivier Renaud", "title": "Choosing the correlation structure of mixed effect models for\n  experiments with stimuli", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of experiments in psychology can often be summarized to\nparticipants reacting to stimuli. For such an experiment, the mixed effects\nmodel with crossed random effects is usually the appropriate tool to analyse\nthe data because it considers the sampling of both participants and stimuli.\nHowever, these models let to users several choices when analysing data and this\npractice may be disruptive for researchers trained to a set of standardized\nanalysis such as ANOVA. In the present article, we are focusing on the choice\nof the correlation structure of the data, because it is both subtle and\ninfluential on the results of the analysis. We provide an overview of several\ncorrelation structures used in the literature and we propose a new one that is\nthe natural extension of the repeated measures ANOVA. A large simulation study\nshows that correlation structures that are either too simple or too complex\nfail to deliver credible results, even for designs with only three variables.\nWe also show how the design of the experiment influences the correlation\nstructure of the data. Moreover, we provide R code to estimate all the\ncorrelation structures presented in this article, as well as functions\nimplemented in an R package to compute our new proposal.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 10:02:04 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 03:50:51 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 09:56:40 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Frossard", "Jaromil", ""], ["Renaud", "Olivier", ""]]}, {"id": "1903.10816", "submitter": "Thomas Pitschel", "authors": "Thomas Pitschel", "title": "Deterministic bootstrapping for a class of bootstrap methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is described that enables efficient deterministic approximate\ncomputation of the bootstrap distribution for any linear bootstrap method\n$T_n^*$, alleviating the need for repeated resampling from observations (resp.\ninput-derived data). In essence, the algorithm computes the distribution\nfunction from a linear mixture of independent random variables each having a\nfinite discrete distribution. The algorithm is applicable to elementary\nbootstrap scenarios (targetting the mean as parameter of interest), for block\nbootstrap, as well as for certain residual bootstrap scenarios. Moreover, the\nalgorithm promises a much broader applicability, in non-bootstrapped hypothesis\ntesting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 11:53:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:14:00 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Pitschel", "Thomas", ""]]}, {"id": "1903.10894", "submitter": "Viktor Ra\\v{c}inskij", "authors": "Viktor Ra\\v{c}inskij, Paul A. Smith, Peter G. M. van der Heijden", "title": "Linkage Free Dual System Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper it is shown that under certain conditions there is a\nrelationship between the parameter estimation of the Fellegi--Sunter\nprobabilistic linkage model and dual system estimation. This relationship can\nbe used as the basis of an approach to population size estimation. In this case\nit is sufficient to estimate the parameters of the linkage model in order to\nobtain the population size estimate. Neither classification of the record pairs\ninto links and non-links, nor forcing the records into a series of 1-1 matches,\nnor clerical review of the potential links is required. The accuracy of the\nproposed estimator appears to be bounded by the accuracy of the dual system\nestimator with perfect linkage and it diminishes as the discriminatory power of\nthe linkage variables decreases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 13:46:25 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ra\u010dinskij", "Viktor", ""], ["Smith", "Paul A.", ""], ["van der Heijden", "Peter G. M.", ""]]}, {"id": "1903.10988", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak", "title": "Non-asymptotic error controlled sparse high dimensional precision matrix\n  estimation", "comments": "17 pages, 7 figures, 1 table", "journal-ref": "JMVA 181 (2021)", "doi": "10.1016/j.jmva.2020.104690", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of a high dimensional precision matrix is a critical problem to\nmany areas of statistics including Gaussian graphical models and inference on\nhigh dimensional data. Working under the structural assumption of sparsity, we\npropose a novel methodology for estimating such matrices while controlling the\nfalse positive rate, percentage of matrix entries incorrectly chosen to be\nnon-zero. We specifically focus on false positive rates tending towards zero\nwith finite sample guarantees. This methodology is distribution free, but is\nparticularly applicable to the problem of Gaussian network recovery. We also\nconsider applications to constructing gene networks in genomics data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:16:58 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""]]}, {"id": "1903.11005", "submitter": "Jouchi Nakajima", "authors": "Jouchi Nakajima", "title": "Skew selection for factor stochastic volatility models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes factor stochastic volatility models with skew error\ndistributions. The generalized hyperbolic skew t-distribution is employed for\ncommon-factor processes and idiosyncratic shocks. Using a Bayesian sparsity\nmodeling strategy for the skewness parameter provides a parsimonious skew\nstructure for possibly high-dimensional stochastic volatility models. Analyses\nof daily stock returns are provided. Empirical results show that the skewness\nis important for common-factor processes but less for idiosyncratic shocks. The\nsparse skew structure improves prediction and portfolio performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 16:45:31 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Nakajima", "Jouchi", ""]]}, {"id": "1903.11187", "submitter": "Chi Feng", "authors": "Chi Feng and Youssef M. Marzouk", "title": "A layered multiple importance sampling scheme for focused optimal\n  Bayesian experimental design", "comments": "33 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new computational approach for \"focused\" optimal Bayesian\nexperimental design with nonlinear models, with the goal of maximizing expected\ninformation gain in targeted subsets of model parameters. Our approach\nconsiders uncertainty in the full set of model parameters, but employs a design\nobjective that can exploit learning trade-offs among different parameter\nsubsets. We introduce a new layered multiple importance sampling scheme that\nprovides consistent estimates of expected information gain in this focused\nsetting. This sampling scheme yields significant reductions in estimator bias\nand variance for a given computational effort, making optimal design more\ntractable for a wide range of computationally intensive problems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 23:00:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Feng", "Chi", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1903.11200", "submitter": "Yangmei Zhou", "authors": "Yangmei Zhou, Weixin Yao", "title": "Maximum Likelihood Estimation of a Semiparametric Two-component Mixture\n  Model using Log-concave Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by studies in biological sciences to detect differentially\nexpressed genes, a semiparametric two-component mixture model with one known\ncomponent is being studied in this paper. Assuming the density of the unknown\ncomponent to be log-concave, which contains a very broad family of densities,\nwe develop a semiparametric maximum likelihood estimator and propose an EM\nalgorithm to compute it. Our new estimation method finds the mixing proportions\nand the distribution of the unknown component simultaneously. We establish the\nidentifiability of the proposed semiparametric mixture model and prove the\nexistence and consistency of the proposed estimators. We further compare our\nestimator with several existing estimators through simulation studies and apply\nour method to two real data sets from biological sciences and astronomy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:32:57 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhou", "Yangmei", ""], ["Yao", "Weixin", ""]]}, {"id": "1903.11232", "submitter": "Tiffany Tang", "authors": "Yulia Baker, Tiffany M. Tang, Genevera I. Allen", "title": "Feature Selection for Data Integration with Mixed Multi-view Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration methods that analyze multiple sources of data simultaneously\ncan often provide more holistic insights than can separate inquiries of each\ndata source. Motivated by the advantages of data integration in the era of \"big\ndata\", we investigate feature selection for high-dimensional multi-view data\nwith mixed data types (e.g. continuous, binary, count-valued). This\nheterogeneity of multi-view data poses numerous challenges for existing feature\nselection methods. However, after critically examining these issues through\nempirical and theoretically-guided lenses, we develop a practical solution, the\nBlock Randomized Adaptive Iterative Lasso (B-RAIL), which combines the\nstrengths of the randomized Lasso, adaptive weighting schemes, and stability\nselection. B-RAIL serves as a versatile data integration method for sparse\nregression and graph selection, and we demonstrate the effectiveness of B-RAIL\nthrough extensive simulations and a case study to infer the ovarian cancer gene\nregulatory network. In this case study, B-RAIL successfully identifies\nwell-known biomarkers associated with ovarian cancer and hints at novel\ncandidates for future ovarian cancer research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 02:56:26 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 16:40:47 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Baker", "Yulia", ""], ["Tang", "Tiffany M.", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1903.11309", "submitter": "Marko Laine", "authors": "Marko Laine", "title": "Introduction to Dynamic Linear Models for Time Series Analysis", "comments": "A chapter submitted to a book with a proposed title: Geodetic Time\n  Series Analysis and Applications, editors. J.-P. Montillet and M. Bos", "journal-ref": null, "doi": "10.1007/978-3-030-21718-1_4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic linear models (DLM) offer a very generic framework to analyse time\nseries data. Many classical time series models can be formulated as DLMs,\nincluding ARMA models and standard multiple linear regression models. The\nmodels can be seen as general regression models where the coefficients can vary\nin time. In addition, they allow for a state space representation and a\nformulation as hierarchical statistical models, which in turn is the key for\nefficient estimation by Kalman formulas and by Markov chain Monte Carlo (MCMC)\nmethods. A dynamic linear model can handle non-stationary processes, missing\nvalues and non-uniform sampling as well as observations with varying\naccuracies. This chapter gives an introduction to DLM and shows how to build\nvarious useful models for analysing trends and other sources of variability in\ngeodetic time series.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 09:39:38 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 13:57:45 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Laine", "Marko", ""]]}, {"id": "1903.11371", "submitter": "Thorsten Dickhaus", "authors": "Andr\\'e Neumann, Thorsten Dickhaus", "title": "Non-parametric Archimedean generator estimation with implications for\n  multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple testing, the family-wise error rate can be bounded under some\nconditions by the copula of the test statistics. Assuming that this copula is\nArchimedean, we consider two non-parametric Archimedean generator estimators.\nMore specifically, we use the non-parametric estimator from Genest et al.\n(2011) and a slight modification thereof. In simulations, we compare the\nresulting multiple tests with the Bonferroni test and the multiple test derived\nfrom the true generator as baselines.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:17:23 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Neumann", "Andr\u00e9", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "1903.11372", "submitter": "Neo Christopher Chung", "authors": "Neo Christopher Chung, B{\\l}a\\.zej Miasojedow, Micha{\\l} Startek, Anna\n  Gambin", "title": "Jaccard/Tanimoto similarity test and estimation methods", "comments": null, "journal-ref": "BMC Bioinformatics (2019) 20(Suppl 15): 644", "doi": "10.1186/s12859-019-3118-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary data are used in a broad area of biological sciences. Using binary\npresence-absence data, we can evaluate species co-occurrences that help\nelucidate relationships among organisms and environments. To summarize\nsimilarity between occurrences of species, we routinely use the\nJaccard/Tanimoto coefficient, which is the ratio of their intersection to their\nunion. It is natural, then, to identify statistically significant\nJaccard/Tanimoto coefficients, which suggest non-random co-occurrences of\nspecies. However, statistical hypothesis testing using this similarity\ncoefficient has been seldom used or studied.\n  We introduce a hypothesis test for similarity for biological presence-absence\ndata, using the Jaccard/Tanimoto coefficient. Several key improvements are\npresented including unbiased estimation of expectation and centered\nJaccard/Tanimoto coefficients, that account for occurrence probabilities. We\nderived the exact and asymptotic solutions and developed the bootstrap and\nmeasurement concentration algorithms to compute statistical significance of\nbinary similarity. Comprehensive simulation studies demonstrate that our\nproposed methods produce accurate p-values and false discovery rates. The\nproposed estimation methods are orders of magnitude faster than the exact\nsolution. The proposed methods are implemented in an open source R package\ncalled jaccard (https://cran.r-project.org/package=jaccard).\n  We introduce a suite of statistical methods for the Jaccard/Tanimoto\nsimilarity coefficient, that enable straightforward incorporation of\nprobabilistic measures in analysis for species co-occurrences. Due to their\ngenerality, the proposed methods and implementations are applicable to a wide\nrange of binary data arising from genomics, biochemistry, and other areas of\nscience.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:22:31 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chung", "Neo Christopher", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Startek", "Micha\u0142", ""], ["Gambin", "Anna", ""]]}, {"id": "1903.11455", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and Lucia C. Petito and Sarah E. Robertson and Miguel\n  A. Hern\\'an and Jon A. Steingrimsson", "title": "Towards causally interpretable meta-analysis: transporting inferences\n  from multiple studies to a target population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take steps towards causally interpretable meta-analysis by describing\nmethods for transporting causal inferences from a collection of randomized\ntrials to a new target population, one-trial-at-a-time and pooling all trials.\nWe discuss identifiability conditions for average treatment effects in the\ntarget population and provide identification results. We show that assuming\ninferences are transportable from all trials in the collection to the same\ntarget population has implications for the law underlying the observed data. We\npropose average treatment effect estimators that rely on different working\nmodels and provide code for their implementation in statistical software. We\ndiscuss how to use the data to examine whether transported inferences are\nhomogeneous across the collection of trials, sketch approaches for sensitivity\nanalysis to violations of the identifiability conditions, and describe\nextensions to address non-adherence in the trials. Last, we illustrate the\nproposed methods using data from the HALT-C multi-center trial.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:44:44 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 14:09:04 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 20:49:11 GMT"}, {"version": "v4", "created": "Sat, 8 Feb 2020 21:32:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Petito", "Lucia C.", ""], ["Robertson", "Sarah E.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Steingrimsson", "Jon A.", ""]]}, {"id": "1903.11555", "submitter": "Wojciech Zieli\\'nski", "authors": "Stanis{\\l}aw Jaworski, Wojciech Zieli\\'nski", "title": "The shortest confidence interval for the weighted sum of two Binomial\n  proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval estimation of the probability of success in a Binomial model is\nconsidered. Zieli\\'nski (2018) showed that the confidence interval which uses\ninformation about non-homogeneity of the sample is better than the classical\none. In the following paper the shortest confidence interval for non-homogenous\nsample is constructed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:16:39 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Jaworski", "Stanis\u0142aw", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.11647", "submitter": "Virgilio Gomez-Rubio", "authors": "Francisco Palmi-Perales, Virgilio Gomez-Rubio, Gonzalo Lopez-Abente,\n  Rebeca Ramis-Prieto, Jose Miguel Sanz-Anquela, Pablo Fernandez-Navarro", "title": "Approximate Bayesian inference for multivariate point pattern analysis\n  in disease mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for the analysis of multivariate case-control\ngeoreferenced data using Bayesian inference in the context of disease mapping,\nwhere the spatial distribution of different types of cancers is analyzed.\nExtending other methodology in point pattern analysis, we propose a\nlog-Gaussian Cox process for point pattern of cases and the controls, which\naccounts for risk factors, such as exposure to pollution sources, and includes\na term to measure spatial residual variation.\n  For each disease, its intensity is modeled on a baseline spatial effect\n(estimated from both controls and cases), a disease-specific spatial term and\nthe effects on covariates that account for risk factors. By fitting these\nmodels the effect of the covariates on the set of cases can be assessed, and\nthe residual spatial terms can be easily compared to detect areas of high risk\nnot explained by the covariates.\n  Three different types of effects to model exposure to pollution sources are\nconsidered. First of all, a fixed effect on the distance to the source. Next,\nsmooth terms on the distance are used to model non-linear effects by means of a\ndiscrete random walk of order one and a Gaussian process in one dimension with\na Mat\\'ern covariance.\n  Models are fit using the integrated nested Laplace approximation (INLA) so\nthat the spatial terms are approximated using an approach based on solving\nStochastic Partial Differential Equations (SPDE). Finally, this new framework\nis applied to a dataset of three different types of cancer and a set of\ncontrols from Alcal\\'a de Henares (Madrid, Spain). Covariates available include\nthe distance to several polluting industries and socioeconomic indicators. Our\nfindings point to a possible risk increase due to the proximity to some of\nthese industries.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 18:52:38 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Palmi-Perales", "Francisco", ""], ["Gomez-Rubio", "Virgilio", ""], ["Lopez-Abente", "Gonzalo", ""], ["Ramis-Prieto", "Rebeca", ""], ["Sanz-Anquela", "Jose Miguel", ""], ["Fernandez-Navarro", "Pablo", ""]]}, {"id": "1903.11695", "submitter": "Justin Silverman", "authors": "Justin D. Silverman, Kimberly Roche, Zachary C. Holmes, Lawrence A.\n  David, Sayan Mukherjee", "title": "Bayesian Multinomial Logistic Normal Models through Marginally Latent\n  Matrix-T Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian multinomial logistic-normal (MLN) models are popular for the\nanalysis of sequence count data (e.g., microbiome or gene expression data) due\nto their ability to model multivariate count data with complex covariance\nstructure. However, existing implementations of MLN models are limited to\nhandling small data sets due to the non-conjugacy of the multinomial and\nlogistic-normal distributions. We introduce MLN models which can be written as\nmarginally latent matrix-t process (LTP) models. Marginally LTP models describe\na flexible class of generalized linear regression, non-linear regression, and\ntime series models. We develop inference schemes for Marginally LTP models and,\nthrough application to MLN models, demonstrate that our inference schemes are\nboth highly accurate and often 4-5 orders of magnitude faster than MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:45:12 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 17:56:45 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 16:28:42 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Silverman", "Justin D.", ""], ["Roche", "Kimberly", ""], ["Holmes", "Zachary C.", ""], ["David", "Lawrence A.", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "1903.11696", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Caroline \\\"Ubelh\\\"or, Steven W. Mes, Roland\n  Martens, Thomas Koopman, Pim de Graaf, Floris H.P. van Velden, Ronald\n  Boellaard, Jonas A. Castelijns, Dennis E. te Beest, Martijn W. Heymans, Mark\n  A. van de Wiel", "title": "Stable prediction with radiomics data", "comments": "52 pages: 14 pages Main Text and 38 pages of Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Radiomics refers to the high-throughput mining of quantitative\nfeatures from radiographic images. It is a promising field in that it may\nprovide a non-invasive solution for screening and classification. Standard\nmachine learning classification and feature selection techniques, however, tend\nto display inferior performance in terms of (the stability of) predictive\nperformance. This is due to the heavy multicollinearity present in radiomic\ndata. We set out to provide an easy-to-use approach that deals with this\nproblem.\n  Results: We developed a four-step approach that projects the original\nhigh-dimensional feature space onto a lower-dimensional latent-feature space,\nwhile retaining most of the covariation in the data. It consists of (i)\npenalized maximum likelihood estimation of a redundancy filtered correlation\nmatrix. The resulting matrix (ii) is the input for a maximum likelihood factor\nanalysis procedure. This two-stage maximum-likelihood approach can be used to\n(iii) produce a compact set of stable features that (iv) can be directly used\nin any (regression-based) classifier or predictor. It outperforms other\nclassification (and feature selection) techniques in both external and internal\nvalidation settings regarding survival in squamous cell cancers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:45:58 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["\u00dcbelh\u00f6r", "Caroline", ""], ["Mes", "Steven W.", ""], ["Martens", "Roland", ""], ["Koopman", "Thomas", ""], ["de Graaf", "Pim", ""], ["van Velden", "Floris H. P.", ""], ["Boellaard", "Ronald", ""], ["Castelijns", "Jonas A.", ""], ["Beest", "Dennis E. te", ""], ["Heymans", "Martijn W.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1903.11697", "submitter": "Nicol\\'as Kuschinski", "authors": "Nicol\\'as E. Kuschinski, J. Andr\\'es Christen, Adriana Monroy,\n  Silvestre Alavez", "title": "Bayesian Experimental Design for Oral Glucose Tolerance Tests (OGTT)", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OGTT is a common test, frequently used to diagnose insulin resistance or\ndiabetes, in which a patient's blood sugar is measured at various times over\nthe course of a few hours. Recent developments in the study of OGTT results\nhave framed it as an inverse problem which has been the subject of Bayesian\ninference. This is a powerful new tool for analyzing the results of an OGTT\ntest,and the question arises as to whether the test itself can be improved. It\nis of particular interest to discover whether the times at which a patient's\nglucose is measured can be changed to improve the effectiveness of the test.\nThe purpose of this paper is to explore the possibility of finding a better\nexperimental design, that is, a set of times to perform the test. We review the\ntheory of Bayesian experimental design and propose an estimator for the\nexpected utility of a design. We then study the properties of this estimator\nand propose a new method for quantifying the uncertainty in comparisons between\ndesigns. We implement this method to find a new design and the proposed design\nis compared favorably to the usual testing scheme.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:54:16 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kuschinski", "Nicol\u00e1s E.", ""], ["Christen", "J. Andr\u00e9s", ""], ["Monroy", "Adriana", ""], ["Alavez", "Silvestre", ""]]}, {"id": "1903.11797", "submitter": "Vladimir Minin", "authors": "Michael D. Karcher, Marc A. Suchard, Gytis Dudas, Vladimir N. Minin", "title": "Estimating effective population size changes from preferentially sampled\n  genetic sequences", "comments": "47 pages", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007774", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalescent theory combined with statistical modeling allows us to estimate\neffective population size fluctuations from molecular sequences of individuals\nsampled from a population of interest. When sequences are sampled serially\nthrough time and the distribution of the sampling times depends on the\neffective population size, explicit statistical modeling of sampling times\nimproves population size estimation. Previous work assumed that the genealogy\nrelating sampled sequences is known and modeled sampling times as an\ninhomogeneous Poisson process with log-intensity equal to a linear function of\nthe log-transformed effective population size. We improve this approach in two\nways. First, we extend the method to allow for joint Bayesian estimation of the\ngenealogy, effective population size trajectory, and other model parameters.\nNext, we improve the sampling time model by incorporating additional sources of\ninformation in the form of time-varying covariates. We validate our new\nmodeling framework using a simulation study and apply our new methodology to\nanalyses of population dynamics of seasonal influenza and to the recent Ebola\nvirus outbreak in West Africa.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 06:20:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Karcher", "Michael D.", ""], ["Suchard", "Marc A.", ""], ["Dudas", "Gytis", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1903.11886", "submitter": "Michael Lebacher", "authors": "Michael Lebacher and G\\\"oran Kauermann", "title": "Regression-based Network Reconstruction with Nodal and Dyadic Covariates\n  and Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network (or matrix) reconstruction is a general problem which occurs if the\nmargins of a matrix are given and the matrix entries need to be predicted. In\nthis paper we show that the predictions obtained from the iterative\nproportional fitting procedure (IPFP) or equivalently maximum entropy (ME) can\nbe obtained by restricted maximum likelihood estimation relying on augmented\nLagrangian optimization. Based on the equivalence we extend the framework of\nnetwork reconstruction towards regression by allowing for exogenous covariates\nand random heterogeneity effects. The proposed estimation approach is compared\nwith different competing methods for network reconstruction and matrix\nestimation. Exemplary, we apply the approach to interbank lending data,\nprovided by the Bank for International Settlement (BIS). This dataset provides\nfull knowledge of the real network and is therefore suitable to evaluate the\npredictions of our approach. It is shown that the inclusion of exogenous\ninformation allows for superior predictions in terms of $L_1$ and $L_2$ errors.\nAdditionally, the approach allows to obtain prediction intervals via bootstrap\nthat can be used to quantify the uncertainty attached to the predictions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 10:38:36 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 11:56:32 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 15:20:54 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 10:38:25 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Lebacher", "Michael", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1903.12044", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Joaqu\\'in M\\'iguez", "title": "Convergence rates for optimised adaptive importance samplers", "comments": "Revised version, new results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate\nexpectations with respect to some target distribution which \\textit{adapt}\nthemselves to obtain better estimators over a sequence of iterations. Although\nit is straightforward to show that they have the same $\\mathcal{O}(1/\\sqrt{N})$\nconvergence rate as standard importance samplers, where $N$ is the number of\nMonte Carlo samples, the behaviour of adaptive importance samplers over the\nnumber of iterations has been left relatively unexplored. In this work, we\ninvestigate an adaptation strategy based on convex optimisation which leads to\na class of adaptive importance samplers termed \\textit{optimised adaptive\nimportance samplers} (OAIS). These samplers rely on the iterative minimisation\nof the $\\chi^2$-divergence between an exponential-family proposal and the\ntarget. The analysed algorithms are closely related to the class of adaptive\nimportance samplers which minimise the variance of the weight function. We\nfirst prove non-asymptotic error bounds for the mean squared errors (MSEs) of\nthese algorithms, which explicitly depend on the number of iterations and the\nnumber of samples together. The non-asymptotic bounds derived in this paper\nimply that when the target belongs to the exponential family, the $L_2$ errors\nof the optimised samplers converge to the optimal rate of\n$\\mathcal{O}(1/\\sqrt{N})$ and the rate of convergence in the number of\niterations are explicitly provided. When the target does not belong to the\nexponential family, the rate of convergence is the same but the asymptotic\n$L_2$ error increases by a factor $\\sqrt{\\rho^\\star} > 1$, where $\\rho^\\star -\n1$ is the minimum $\\chi^2$-divergence between the target and an\nexponential-family proposal.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:21:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 09:17:45 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 19:56:08 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 01:08:45 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1903.12077", "submitter": "Ke Zhu", "authors": "Jiayuan Zhou, Feiyu Jiang, Ke Zhu, Wai Keung Li", "title": "Time series models for realized covariance matrices based on the\n  matrix-F distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Conditional BEKK matrix-F (CBF) model for the time-varying\nrealized covariance (RCOV) matrices. This CBF model is capable of capturing\nheavy-tailed RCOV, which is an important stylized fact but could not be handled\nadequately by the Wishart-based models. To further mimic the long memory\nfeature of the RCOV, a special CBF model with the conditional heterogeneous\nautoregressive (HAR) structure is introduced. Moreover, we give a systematical\nstudy on the probabilistic properties and statistical inferences of the CBF\nmodel, including exploring its stationarity, establishing the asymptotics of\nits maximum likelihood estimator, and giving some new inner-product-based tests\nfor its model checking. In order to handle a large dimensional RCOV matrix, we\nconstruct two reduced CBF models -- the variance-target CBF model (for moderate\nbut fixed dimensional RCOV matrix) and the factor CBF model (for high\ndimensional RCOV matrix). For both reduced models, the asymptotic theory of the\nestimated parameters is derived. The importance of our entire methodology is\nillustrated by simulation results and two real examples.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 05:13:05 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 09:02:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhou", "Jiayuan", ""], ["Jiang", "Feiyu", ""], ["Zhu", "Ke", ""], ["Li", "Wai Keung", ""]]}, {"id": "1903.12342", "submitter": "Daniel Ahfock", "authors": "Daniel Ahfock, Saumyadipta Pyne, Geoffrey J. McLachlan", "title": "Statistical matching of non-Gaussian data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical matching problem is a data integration problem with\nstructured missing data. The general form involves the analysis of multiple\ndatasets that only have a strict subset of variables jointly observed across\nall datasets. The simplest version involves two datasets, labelled A and B,\nwith three variables of interest $X, Y$ and $Z$. Variables $X$ and $Y$ are\nobserved in dataset A and variables $X$ and $Z$ are observed in dataset $B$.\nStatistical inference is complicated by the absence of joint $(Y, Z)$\nobservations. Parametric modelling can be challenging due to identifiability\nissues and the difficulty of parameter estimation. We develop computationally\nfeasible procedures for the statistical matching of non-Gaussian data using\nsuitable data augmentation schemes and identifiability constraints.\nNearest-neighbour imputation is a common alternative technique due to its ease\nof use and generality. Nearest-neighbour matching is based on a conditional\nindependence assumption that may be inappropriate for non-Gaussian data. The\nviolation of the conditional independence assumption can lead to improper\nimputations. We compare model based approaches to nearest-neighbour imputation\non a number of flow cytometry datasets and find that the model based approach\ncan address some of the weaknesses of the nonparametric nearest-neighbour\ntechnique.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 03:41:39 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Ahfock", "Daniel", ""], ["Pyne", "Saumyadipta", ""], ["McLachlan", "Geoffrey J.", ""]]}]