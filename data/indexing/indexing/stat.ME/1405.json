[{"id": "1405.0294", "submitter": "Julio C\\'esar Hern\\'andez S\\'anchez", "authors": "Jos\\'e Luis Vicente-Villard\\'on, Julio C\\'esar Hern\\'andez S\\'anchez", "title": "Logistic Biplots for Ordinal Data with an Application to Job\n  Satisfaction of Doctorate Degree Holders in Spain", "comments": "26 pages, 11 figures,2 tables. arXiv admin note: text overlap with\n  arXiv:1309.5486", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biplot Methods allow for the simultaneous representation of individuals and\nvariables of a data matrix. For Binary or Nominal data, Logistic biplots have\nbeen recently developed to extend the classical linear representations for\ncontinuous data. When data are ordinal, linear, binary or nominal logistic\nbiplots are not adequate and techniques as Categorical Principal Component\nAnalysis (CATPCA) or Item Response Theory (IRT) for ordinal items should be\nused instead.\n  In this paper we extend the Biplot to ordinal data. The resulting method is\ntermed Ordinal Logistic Biplot (OLB). Row scores are computed to have ordinal\nlogistic responses along the dimensions and column parameters produce logistic\nresponse surfaces that, projected onto the space spanned by the row scores,\ndefine a linear biplot. A proportional odds model is used, obtaining a\nmultidimensional model known as graded response model in the Item Response\nTheory literature. We study the geometry of such a representation and construct\ncomputational algorithms for the estimation of parameters and the calculation\nof prediction directions. Ordinal Logistic Biplots extend both CATPCA and IRT\nin the sense that gives a graphical representation for IRT similar to the\nbiplot for CATPCA.\n  The main theoretical results are applied to the study of job satisfaction of\ndoctorate (PhD) holders in Spain. Holders of doctorate degrees or other\nresearch qualifications are crucial to the creation, commercialization and\ndissemination of knowledge and to innovation. The proposed methods are used to\nextract useful information from the Spanish data from the international 'Survey\non the careers of doctorate holders (CDH)', jointly carried out Eurostat, the\nOrganisation for Economic Co-operation and Development (OECD) and UNESCO's\nInstitute for Statistics (UIS).\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 20:06:17 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Vicente-Villard\u00f3n", "Jos\u00e9 Luis", ""], ["S\u00e1nchez", "Julio C\u00e9sar Hern\u00e1ndez", ""]]}, {"id": "1405.0362", "submitter": "Yves Rozenholc", "authors": "Nelo Magalh\\~aes and Yves Rozenholc", "title": "An efficient algorithm for T-estimation", "comments": "18 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an efficient and exact algorithm, together with a faster but\napproximate version, which implements with a sub-quadratic complexity the\nhold-out derived from T-estimation. We study empirically the performance of\nthis hold-out in the context of density estimation considering well-known\ncompetitors (hold-out derived from least-squares or Kullback-Leibler\ndivergence, model selection procedures, etc.) and classical problems including\nhistogram or bandwidth selection. Our algorithms are integrated in a companion\nR-package called {\\it Density.T.HoldOut} available on the CRAN:\n{\\url{http://cran.r-project.org/web/packages/Density.T.HoldOut/index.html}}.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 09:00:01 GMT"}, {"version": "v2", "created": "Mon, 22 Sep 2014 21:12:12 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Magalh\u00e3es", "Nelo", ""], ["Rozenholc", "Yves", ""]]}, {"id": "1405.0377", "submitter": "Antonio Punzo", "authors": "Antonio Punzo, Ryan P. Browne, Paul D. McNicholas", "title": "Hypothesis Testing for Parsimonious Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models with eigen-decomposed covariance structures make up\nthe most popular family of mixture models for clustering and classification,\ni.e., the Gaussian parsimonious clustering models (GPCM). Although the GPCM\nfamily has been used for almost 20 years, selecting the best member of the\nfamily in a given situation remains a troublesome problem. Likelihood ratio\ntests are developed to tackle this problems. These likelihood ratio tests use\nthe heteroscedastic model under the alternative hypothesis but provide much\nmore flexibility and real-world applicability than previous approaches that\ncompare the homoscedastic Gaussian mixture versus the heteroscedastic one.\nAlong the way, a novel maximum likelihood estimation procedure is developed for\ntwo members of the GPCM family. Simulations show that the $\\chi^2$ reference\ndistribution gives reasonable approximation for the LR statistics only when the\nsample size is considerable and when the mixture components are well separated;\naccordingly, following Lo (2008), a parametric bootstrap is adopted.\nFurthermore, by generalizing the idea of Greselin and Punzo (2013) to the\nclustering context, a closed testing procedure, having the defined likelihood\nratio tests as local tests, is introduced to assess a unique model in the\ngeneral family. The advantages of this likelihood ratio testing procedure are\nillustrated via an application to the well-known Iris data set.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 10:33:49 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Punzo", "Antonio", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1405.0379", "submitter": "Robert Dahl Jacobsen", "authors": "Robert Dahl Jacobsen and Jesper M{\\o}ller", "title": "Frequentist and Bayesian inference for Gaussian-log-Gaussian wavelet\n  trees, and statistical signal processing applications", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.156", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new estimation methods for a sub-class of the Gaussian scale\nmixture models for wavelet trees by Wainwright, Simoncelli & Willsky that rely\non modern results for composite likelihoods and approximate Bayesian inference.\nOur methodology is illustrated for denoising and edge detection problems in\ntwo-dimensional images.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 11:20:36 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 09:32:38 GMT"}, {"version": "v3", "created": "Sun, 29 Jan 2017 18:59:14 GMT"}, {"version": "v4", "created": "Thu, 10 Aug 2017 20:00:37 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Jacobsen", "Robert Dahl", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1405.0384", "submitter": "Yohann De Castro", "authors": "Flavia Barsotti, Yohann De Castro (LM-Orsay), Thibault Espinasse\n  (ICJ), Paul Rochet (LMJL)", "title": "Estimating the transition matrix of a Markov chain observed at random\n  times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a statistical estimation technique to recover the\ntransition kernel $P$ of a Markov chain $X=(X_m)_{m \\in \\mathbb N}$ in presence\nof censored data. We consider the situation where only a sub-sequence of $X$ is\navailable and the time gaps between the observations are iid random variables.\nUnder the assumption that neither the time gaps nor their distribution are\nknown, we provide an estimation method which applies when some transitions in\nthe initial Markov chain $X$ are known to be unfeasible. A consistent estimator\nof $P$ is derived in closed form as a solution of a minimization problem. The\nasymptotic performance of the estimator is then discussed in theory and through\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 May 2014 11:39:27 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Barsotti", "Flavia", "", "LM-Orsay"], ["De Castro", "Yohann", "", "LM-Orsay"], ["Espinasse", "Thibault", "", "ICJ"], ["Rochet", "Paul", "", "LMJL"]]}, {"id": "1405.0602", "submitter": "Ian Fellows", "authors": "Ian E Fellows", "title": "Why (and When and How) Contrastive Divergence Works", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Contrastive divergence (CD) is a promising method of inference in high\ndimensional distributions with intractable normalizing constants, however, the\ntheoretical foundations justifying its use are somewhat shaky. This document\nproposes a framework for understanding CD inference, how/when it works, and\nprovides multiple justifications for the CD moment conditions, including\nframing them as a variational approximation. Algorithms for performing\ninference are discussed and are applied to social network data using an\nexponential-family random graph models (ERGM). The framework also provides\nguidance about how to construct MCMC kernels providing good CD inference, which\nturn out to be quite different from those used typically to provide fast global\nmixing.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 15:52:00 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Fellows", "Ian E", ""]]}, {"id": "1405.0629", "submitter": "Yue Hu", "authors": "Yue Hu and Genevera I. Allen", "title": "Local-Aggregate Modeling for Big-Data via Distributed Optimization:\n  Applications to Neuroimaging", "comments": "41 pages, 5 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances have led to a proliferation of structured big data\nthat have matrix-valued covariates. We are specifically motivated to build\npredictive models for multi-subject neuroimaging data based on each subject's\nbrain imaging scans. This is an ultra-high-dimensional problem that consists of\na matrix of covariates (brain locations by time points) for each subject; few\nmethods currently exist to fit supervised models directly to this tensor data.\nWe propose a novel modeling and algorithmic strategy to apply generalized\nlinear models (GLMs) to this massive tensor data in which one set of variables\nis associated with locations. Our method begins by fitting GLMs to each\nlocation separately, and then builds an ensemble by blending information across\nlocations through regularization with what we term an aggregating penalty. Our\nso called, Local-Aggregate Model, can be fit in a completely distributed manner\nover the locations using an Alternating Direction Method of Multipliers (ADMM)\nstrategy, and thus greatly reduces the computational burden. Furthermore, we\npropose to select the appropriate model through a novel sequence of faster\nalgorithmic solutions that is similar to regularization paths. We will\ndemonstrate both the computational and predictive modeling advantages of our\nmethods via simulations and an EEG classification problem.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 23:04:47 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 19:31:34 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Hu", "Yue", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1405.0673", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "A Note on the Identifiability of Generalized Linear Mixed Models", "comments": "9 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present here a simple proof that, under general regularity conditions, the\nstandard parametrization of generalized linear mixed model is identifiable. The\nproof is based on the assumptions of generalized linear mixed models on the\nfirst and second order moments and some general mild regularity conditions,\nand, therefore, is extensible to quasi-likelihood based generalized linear\nmodels. In particular, binomial and Poisson mixed models with dispersion\nparameter are identifiable when equipped with the standard parametrization.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 09:36:03 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "1405.0685", "submitter": "Geoffrey McLachlan", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "Finite Mixtures of Canonical Fundamental Skew t-Distributions", "comments": "This is an extended version of the paper Lee and McLachlan (2014b)\n  with simulations and applications added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an extended version of the paper Lee and McLachlan (2014b) with\nsimulations and applications added. This paper introduces a finite mixture of\ncanonical fundamental skew t (CFUST) distributions for a model-based approach\nto clustering where the clusters are asymmetric and possibly long-tailed (Lee\nand McLachlan, 2014b). The family of CFUST distributions includes the\nrestricted multivariate skew t (rMST) and unrestricted multivariate skew t\n(uMST) distributions as special cases. In recent years, a few versions of the\nmultivariate skew t (MST) model have been put forward, together with various\nEM-type algorithms for parameter estimation. These formulations adopted either\na restricted or unrestricted characterization for their MST densities. In this\npaper, we examine a natural generalization of these developments, employing the\nCFUST distribution as the parametric family for the component distributions,\nand point out that the restricted and unrestricted characterizations can be\nunified under this general formulation. We show that an exact implementation of\nthe EM algorithm can be achieved for the CFUST distribution and mixtures of\nthis distribution, and present some new analytical results for a conditional\nexpectation involved in the E-step.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 11:51:28 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1405.0716", "submitter": "Mumtaz Ahmed", "authors": "Mumtaz Ahmed, Asad Zaman", "title": "A Minimax Bias Estimator for OLS Variances under Heteroskedasticity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic evaluation of heteroskedasticity consistent covariance matrix\nestimates (HCCME) is difficult because of the complexity of the formulae\ncurrently available. We obtain new analytic formulae for the bias of a class of\nestimators of the covariance matrix of OLS in a standard linear regression\nmodel. These formulae provide substantial insight into the properties and\nperformance characteristics of these estimators. In particular, we find a new\nestimator which minimizes the maximum possible bias and improves substantially\non the standard Eicker-White estimate.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 16:51:51 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Ahmed", "Mumtaz", ""], ["Zaman", "Asad", ""]]}, {"id": "1405.0769", "submitter": "Xumeng Cao", "authors": "Xumeng Cao", "title": "Non-Bernoulli Perturbation Distributions for Small Samples in\n  Simultaneous Perturbation Stochastic Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous perturbation stochastic approximation (SPSA) has proven to be\nefficient for recursive optimization. SPSA uses a centered difference\napproximation to the gradient based on two function evaluations regardless of\nthe dimension of the problem. Typically, the Bernoulli +-1 distribution is used\nfor perturbation vectors and theory has been established to prove the\nasymptotic optimality of this distribution. However, optimality of the\nBernoulli distribution may not hold for small-sample stochastic approximation\n(SA) runs. In this paper, we investigate the performance of the segmented\nuniform as a perturbation distribution for small-sample SPSA. In particular, we\nconduct a theoretical analysis for one iteration of SA, which is a reasonable\nstarting point and can be used as a basis for generalization to other\nsmall-sample SPSA settings with more than one iteration. In this work, we show\nthat the Bernoulli distribution may not be the best choice for perturbation\nvectors under certain choices of parameters in small-sample SPSA\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 02:33:32 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Cao", "Xumeng", ""]]}, {"id": "1405.0808", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh", "title": "Divergence based Robust Estimation of the Tail Index through An\n  Exponential Regression Model", "comments": "Pre-Print, 35 pages, To appear in \"Statistical Methods and\n  Applications\"", "journal-ref": "Statistical Methods & Applications (2017), Volume 26, Issue 2, pp\n  181--213", "doi": "10.1007/s10260-016-0364-9", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme value theory is very popular in applied sciences including\nFinance, economics, hydrology and many other disciplines. In univariate extreme\nvalue theory, we model the data by a suitable distribution from the general\nmax-domain of attraction (MAD) characterized by its tail index; there are three\nbroad classes of tails -- the Pareto type, the Weibull type and the Gumbel\ntype. The simplest and most common estimator of the tail index is the Hill\nestimator that works only for Pareto type tails and has a high bias; it is also\nhighly non-robust in presence of outliers with respect to the assumed model.\nThere have been some recent attempts to produce asymptotically unbiased or\nrobust alternative to the Hill estimator; however all the robust alternatives\nwork for any one type of tail. This paper proposes a new general estimator of\nthe tail index that is both robust and has smaller bias under all the three\ntail types compared to the existing robust estimators. This essentially\nproduces a robust generalization of the estimator proposed by Matthys and\nBeirlant (2003) under the same model approximation through a suitable\nexponential regression framework using the density power divergence. The\nrobustness properties of the estimator are derived in the paper along with an\nextensive simulation study. A method for bias correction is also proposed with\napplication to some real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 07:44:57 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 19:44:58 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "1405.0922", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Brian Caffo, Brian Schwartz and Vadim Zipunnikov", "title": "Fast, Exact Bootstrap Principal Component Analysis for p>1 million", "comments": "25 pages, including 9 figures and link to R package. 2014-05-14\n  update: final formatting edits for journal submission, condensed figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many have suggested a bootstrap procedure for estimating the sampling\nvariability of principal component analysis (PCA) results. However, when the\nnumber of measurements per subject ($p$) is much larger than the number of\nsubjects ($n$), the challenge of calculating and storing the leading principal\ncomponents from each bootstrap sample can be computationally infeasible. To\naddress this, we outline methods for fast, exact calculation of bootstrap\nprincipal components, eigenvalues, and scores. Our methods leverage the fact\nthat all bootstrap samples occupy the same $n$-dimensional subspace as the\noriginal sample. As a result, all bootstrap principal components are limited to\nthe same $n$-dimensional subspace and can be efficiently represented by their\nlow dimensional coordinates in that subspace. Several uncertainty metrics can\nbe computed solely based on the bootstrap distribution of these low dimensional\ncoordinates, without calculating or storing the $p$-dimensional bootstrap\ncomponents. Fast bootstrap PCA is applied to a dataset of sleep\nelectroencephalogram (EEG) recordings ($p=900$, $n=392$), and to a dataset of\nbrain magnetic resonance images (MRIs) ($p\\approx$ 3 million, $n=352$). For the\nbrain MRI dataset, our method allows for standard errors for the first 3\nprincipal components based on 1000 bootstrap samples to be calculated on a\nstandard laptop in 47 minutes, as opposed to approximately 4 days with standard\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 15:19:42 GMT"}, {"version": "v2", "created": "Tue, 6 May 2014 22:04:00 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 14:12:12 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Fisher", "Aaron", ""], ["Caffo", "Brian", ""], ["Schwartz", "Brian", ""], ["Zipunnikov", "Vadim", ""]]}, {"id": "1405.1275", "submitter": "Joseph Koopmeiners", "authors": "Joseph S. Koopmeiners and Andrew Wey", "title": "The Randomized CRM: An Approach to Overcoming the Long-Memory Property\n  of the CRM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary object of a phase I clinical trial is to determine the maximum\ntolerated dose (MTD). Typically, the MTD is identified using a dose-escalation\nstudy, where initial subjects are treated at the lowest dose level and\nsubsequent subjects are treated at progressively higher dose levels until the\nMTD is identified. The continual reassessment method (CRM) is a popular\nmodel-based dose-escalation design, which utilizes a formal model for the\nrelationship between dose and toxicity to guide dose-finding. Recently, it was\nshown that the CRM has a tendency to get \"stuck\" on a dose-level, with little\nescalation or de-escalation in the late stages of the trial, due to the\nlong-memory property of the CRM. We propose the randomized CRM (rCRM), which\nintroduces random escalation and de-escalation into the standard CRM\ndose-finding algorithm, as an approach to overcoming the long-memory property\nof the CRM. We discuss two approaches to random escalation and de-escalation\nand compare the operating characteristics of the rCRM to the standard CRM by\nsimulation. Our simulation results show that the rCRM identifies the true MTD\nat a similar rate and results in a similar number of DLTs compared to the\nstandard CRM, while reducing the trial-to-trial variability in the number of\ncohorts treated at the true MTD.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 14:04:22 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Koopmeiners", "Joseph S.", ""], ["Wey", "Andrew", ""]]}, {"id": "1405.1299", "submitter": "Matthieu Marbac", "authors": "Matthieu Marbac, Christophe Biernacki, and Vincent Vandewalle", "title": "Model-based clustering of Gaussian copulas for mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering task of mixed data is a challenging problem. In a probabilistic\nframework, the main difficulty is due to a shortage of conventional\ndistributions for such data. In this paper, we propose to achieve the mixed\ndata clustering with a Gaussian copula mixture model, since copulas, and in\nparticular the Gaussian ones, are powerful tools for easily modelling the\ndistribution of multivariate variables. Indeed, considering a mixing of\ncontinuous, integer and ordinal variables (thus all having a cumulative\ndistribution function), this copula mixture model defines intra-component\ndependencies similar to a Gaussian mixture, so with classical correlation\nmeaning. Simultaneously, it preserves standard margins associated to\ncontinuous, integer and ordered features, namely the Gaussian, the Poisson and\nthe ordered multinomial distributions. As an interesting by-product, the\nproposed mixture model generalizes many well-known ones and also provides tools\nof visualization based on the parameters. At a practical level, the Bayesian\ninference is retained and it is achieved with a Metropolis-within-Gibbs\nsampler. Experiments on simulated and real data sets finally illustrate the\nexpected advantages of the proposed model for mixed data: flexible and\nmeaningful parametrization combined with visualization features.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 15:10:21 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2014 17:22:11 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 20:59:06 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Marbac", "Matthieu", ""], ["Biernacki", "Christophe", ""], ["Vandewalle", "Vincent", ""]]}, {"id": "1405.1326", "submitter": "Jianxi Su", "authors": "Edward Furman, Jianxi Su, and Ri\\v{c}ardas Zitikis", "title": "Paths and indices of maximal tail dependence", "comments": "ASTIN Bulletin: The Journal of the International Actuarial\n  Association, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.RM q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate both analytically and numerically that the existing methods\nfor measuring tail dependence in copulas may sometimes underestimate the extent\nof extreme co-movements of dependent risks and, therefore, may not always\ncomply with the new paradigm of prudent risk management. This phenomenon holds\nin the context of both symmetric and asymmetric copulas with and without\nsingularities. As a remedy, we introduce a notion of paths of maximal (tail)\ndependence and utilize it to propose several new indices of tail dependence.\nThe suggested new indices are conservative, conform with the basic concepts of\nmodern quantitative risk management, and are able to distinguish between\ndistinct risky positions in situations when the existing indices fail to do so.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 15:42:31 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 20:38:44 GMT"}, {"version": "v3", "created": "Sat, 16 Jul 2016 13:03:47 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Furman", "Edward", ""], ["Su", "Jianxi", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1405.1400", "submitter": "Armin Schwartzman", "authors": "Dan Cheng and Armin Schwartzman", "title": "Multiple Testing of Local Maxima for Detection of Peaks in Random Fields", "comments": "30 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1203.3063", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A topological multiple testing scheme is presented for detecting peaks in\nimages under stationary ergodic Gaussian noise, where tests are performed at\nlocal maxima of the smoothed observed signals. The procedure generalizes the\none-dimensional scheme of Schwartzman et al. (2011) to Euclidean domains of\narbitrary dimension. Two methods are developed according to two different ways\nof computing p-values: (i) using the exact distribution of the height of local\nmaxima (Cheng and Schwartzman, 2014), available explicitly when the noise field\nis isotropic; (ii) using an approximation to the overshoot distribution of\nlocal maxima above a pre-threshold (Cheng and Schwartzman, 2014), applicable\nwhen the exact distribution is unknown, such as when the stationary noise field\nis non-isotropic. The algorithms, combined with the Benjamini-Hochberg\nprocedure for thresholding p-values, provide asymptotic strong control of the\nFalse Discovery Rate (FDR) and power consistency, with specific rates, as the\nsearch space and signal strength get large. The optimal smoothing bandwidth and\noptimal pre-threshold are obtained to achieve maximum power. Simulations show\nthat FDR levels are maintained in non-asymptotic conditions. The methods are\nillustrated in a nanoscopy image analysis problem of detecting fluorescent\nmolecules against the image background.\n", "versions": [{"version": "v1", "created": "Tue, 6 May 2014 19:06:53 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 03:12:21 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Cheng", "Dan", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1405.1478", "submitter": "Ery Arias-Castro", "authors": "Nicolas Verzelen and Ery Arias-Castro", "title": "Detection and Feature Selection in Sparse Mixture Models", "comments": "70 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Gaussian mixture models in high dimensions and concentrate on the\ntwin tasks of detection and feature selection. Under sparsity assumptions on\nthe difference in means, we derive information bounds and establish the\nperformance of various procedures, including the top sparse eigenvalue of the\nsample covariance matrix and other projection tests based on moments, such as\nthe skewness and kurtosis tests of Malkovich and Afifi (1973), and other\nvariants which we were better able to control under the null.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 00:36:44 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 16:10:50 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 17:27:22 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Verzelen", "Nicolas", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1405.1603", "submitter": "Wei Sun", "authors": "Min Jin Ha, Wei Sun, Jichun Xie", "title": "PenPC: A Two-step Approach to Estimate the Skeletons of High Dimensional\n  Directed Acyclic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the skeleton of a directed acyclic graph (DAG) is of great\nimportance for understanding the underlying DAG and causaleffects can be\nassessed from the skeleton when the DAG is notidentifiable. We propose a novel\nmethod named PenPC toestimate the skeleton of a high-dimensional DAG by a\ntwo-stepapproach. We first estimate the non-zero entries of a\nconcentrationmatrix using penalized regression, and then fix the\ndifferencebetween the concentration matrix and the skeleton by evaluating aset\nof conditional independence hypotheses. For high dimensionalproblems where the\nnumber of vertices $p$ is in polynomial orexponential scale of sample size $n$,\nwe study the asymptoticproperty of PenPC on two types of graphs:\ntraditionalrandom graphs where all the vertices have the same expected numberof\nneighbors, and scale-free graphs where a few vertices may have alarge number of\nneighbors. As illustrated by extensive simulationsand applications on gene\nexpression data of cancer patients, PenPChas higher sensitivity and specificity\nthan the standard-of-the-artmethod, the PC-stable algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 13:37:00 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Ha", "Min Jin", ""], ["Sun", "Wei", ""], ["Xie", "Jichun", ""]]}, {"id": "1405.1792", "submitter": "Ping Li", "authors": "Radhendushka Srivastava, Ping Li, David Ruppert", "title": "RAPTT: An Exact Two-Sample Test in High Dimensions Using Random\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensions, the classical Hotelling's $T^2$ test tends to have low\npower or becomes undefined due to singularity of the sample covariance matrix.\nIn this paper, this problem is overcome by projecting the data matrix onto\nlower dimensional subspaces through multiplication by random matrices. We\npropose RAPTT (RAndom Projection T-Test), an exact test for equality of means\nof two normal populations based on projected lower dimensional data. RAPTT does\nnot require any constraints on the dimension of the data or the sample size. A\nsimulation study indicates that in high dimensions the power of this test is\noften greater than that of competing tests. The advantage of RAPTT is\nillustrated on high-dimensional gene expression data involving the\ndiscrimination of tumor and normal colon tissues.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 02:09:51 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Srivastava", "Radhendushka", ""], ["Li", "Ping", ""], ["Ruppert", "David", ""]]}, {"id": "1405.1796", "submitter": "Shifeng Xiong Doc", "authors": "Ke Zhang, Fan Yin, Shifeng Xiong", "title": "Comparisons of penalized least squares methods by simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized least squares methods are commonly used for simultaneous estimation\nand variable selection in high-dimensional linear models. In this paper we\ncompare several prevailing methods including the lasso, nonnegative garrote,\nand SCAD in this area through Monte Carlo simulations. Criterion for evaluating\nthese methods in terms of variable selection and estimation are presented. This\npaper focuses on the traditional n > p cases. For larger p, our results are\nstill helpful to practitioners after the dimensionality is reduced by a\nscreening method. K\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 03:18:27 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Zhang", "Ke", ""], ["Yin", "Fan", ""], ["Xiong", "Shifeng", ""]]}, {"id": "1405.1868", "submitter": "Jan Ernest", "authors": "Jan Ernest and Peter B\\\"uhlmann", "title": "Marginal integration for nonparametric causal inference", "comments": "40 pages, 14 figures", "journal-ref": "Electronic Journal of Statistics 2015, Vol. 9, 3155-3194", "doi": "10.1214/15-EJS1075", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inferring the total causal effect of a single\nvariable intervention on a (response) variable of interest. We propose a\ncertain marginal integration regression technique for a very general class of\npotentially nonlinear structural equation models (SEMs) with known structure,\nor at least known superset of adjustment variables: we call the procedure\nS-mint regression. We easily derive that it achieves the convergence rate as\nfor nonparametric regression: for example, single variable intervention effects\ncan be estimated with convergence rate $n^{-2/5}$ assuming smoothness with\ntwice differentiable functions. Our result can also be seen as a major\nrobustness property with respect to model misspecification which goes much\nbeyond the notion of double robustness. Furthermore, when the structure of the\nSEM is not known, we can estimate (the equivalence class of) the directed\nacyclic graph corresponding to the SEM, and then proceed by using S-mint based\non these estimates. We empirically compare the S-mint regression method with\nmore classical approaches and argue that the former is indeed more robust, more\nreliable and substantially simpler.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 10:29:54 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 21:42:55 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2015 13:18:52 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Ernest", "Jan", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1405.2094", "submitter": "Roger Levy", "authors": "Roger Levy", "title": "Using R formulae to test for main effects in the presence of\n  higher-order interactions", "comments": "Minor update from version 1 to correct a small error in Section 3;\n  thanks due to Kevin Tang for pointing out the error", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional analysis of variance (ANOVA) software allows researchers to test\nfor the significance of main effects in the presence of interactions without\nexposure to the details of how the software encodes main effects and\ninteractions to make these tests possible. Now that increasing numbers of\nresearchers are using more general regression software, including mixed-effects\nmodels, to supplant the traditional uses of ANOVA software, conducting such\ntests generally requires greater knowledge of how to parameterize one's\nstatistical models appropriately. Here I present information on how to conduct\nsuch tests using R, including relevant background information and worked\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 20:45:09 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 09:00:42 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Levy", "Roger", ""]]}, {"id": "1405.2105", "submitter": "Johan Segers", "authors": "Johan Segers", "title": "Hybrid Copula Estimators", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extension of the empirical copula is considered by combining an estimator\nof a multivariate cumulative distribution function with estimators of the\nmarginal cumulative distribution functions for marginal estimators that are not\nnecessarily equal to the margins of the joint estimator. Such a hybrid\nestimator may be reasonable when there is additional information available for\nsome margins in the form of additional data or stronger modelling assumptions.\nA functional central limit theorem is established and some examples are\ndeveloped.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 21:51:22 GMT"}, {"version": "v2", "created": "Fri, 28 Nov 2014 11:50:53 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Segers", "Johan", ""]]}, {"id": "1405.2106", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo", "title": "Information Theoretical Estimators Toolbox", "comments": "5 pages; ITE toolbox: https://bitbucket.org/szzoli/ite/", "journal-ref": "Journal of Machine Learning Research 15:283-287, 2014", "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ITE (information theoretical estimators) a free and open source,\nmulti-platform, Matlab/Octave toolbox that is capable of estimating many\ndifferent variants of entropy, mutual information, divergence, association\nmeasures, cross quantities, and kernels on distributions. Thanks to its highly\nmodular design, ITE supports additionally (i) the combinations of the\nestimation techniques, (ii) the easy construction and embedding of novel\ninformation theoretical estimators, and (iii) their immediate application in\ninformation theoretical optimization problems. ITE also includes a prototype\napplication in a central problem class of signal processing, independent\nsubspace analysis and its extensions.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 21:54:34 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Szabo", "Zoltan", ""]]}, {"id": "1405.2134", "submitter": "Lixing Zhu", "authors": "Xu Guo and Lixing Zhu", "title": "Model checking for generalized linear models: a dimension-reduction\n  model-adaptive approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local smoothing testing that is based on multivariate nonparametric\nregression estimation is one of the main model checking methodologies in the\nliterature. However, relevant tests suffer from the typical curse of\ndimensionality resulting in slow convergence rates to their limits under the\nnull hypotheses and less deviation from the null under alternatives. This\nproblem leads tests to not well maintain the significance level and to be less\nsensitive to alternatives. In this paper, a dimension-reduction model-adaptive\ntest is proposed for generalized linear models. The test behaves like a local\nsmoothing test as if the model were univariate, and can be consistent against\nany global alternatives and can detect local alternatives distinct from the\nnull at a fast rate that existing local smoothing tests can achieve only when\nthe model is univariate. Simulations are carried out to examine the performance\nof our methodology. A real data analysis is conducted for illustration. The\nmethod can readily be extended to global smoothing methodology and other\ntesting problems.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 03:29:55 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Guo", "Xu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1405.2200", "submitter": "Teresa Ledwina", "authors": "Teresa Ledwina", "title": "Dependence function for bivariate cdf's", "comments": "15 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring a strength of dependence of random variables is an important\nproblem in statistical practice. In this paper, we propose a new function\nvalued measure of dependence of two random variables. It allows one to study\nand visualize explicit dependence structure, both in some theoretical models\nand empirically, without prior model structure. This provides a comprehensive\nview of association structure and makes possible much detailed inference than\nbased on standard numeric measures of association. We present theoretical\nproperties of the new measure of dependence and discuss in detail estimation\nand application of copula-based variant of it. Some artificial and real data\nexamples illustrate the behavior and practical utility of the measure and its\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 10:29:39 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Ledwina", "Teresa", ""]]}, {"id": "1405.2292", "submitter": "Philip Dawid", "authors": "A. Philip Dawid", "title": "Statistical Causality from a Decision-Theoretic Perspective", "comments": "58 pages, 12 figures", "journal-ref": "Annual Review of Statistics and its Application 2 (2015), 273-303", "doi": "10.1146/annurev-statistics-010814-020105", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of the decision-theoretic framework of statistical\ncausality, which is well-suited for formulating and solving problems of\ndetermining the effects of applied causes. The approach is described in detail,\nand is related to and contrasted with other current formulations, such as\nstructural equation models and potential responses. Topics and applications\ncovered include confounding, the effect of treatment on the treated,\ninstrumental variables, and dynamic treatment strategies.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 17:12:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. Philip", ""]]}, {"id": "1405.2350", "submitter": "Yi-Hui Zhou", "authors": "Yi-Hui Zhou, Fred Wright", "title": "Hypothesis testing at the extremes: fast and robust association for\n  high-throughput data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of biomedical problems require performing many hypothesis tests,\nwith an attendant need to apply stringent thresholds. Often the data take the\nform of a series of predictor vectors, each of which must be compared with a\nsingle response vector, perhaps with nuisance covariates. Parametric tests of\nassociation are often used, but can result in inaccurate type I error at the\nextreme thresholds, even for large sample sizes. Furthermore, standard\ntwo-sided testing can reduce power compared to the doubled $p$-value, due to\nasymmetry in the null distribution. Exact (permutation) testing is attractive,\nbut can be computationally intensive and cumbersome. We present an\napproximation to exact association tests of trend that is accurate and fast\nenough for standard use in high-throughput settings, and can easily provide\nstandard two-sided or doubled $p$-values. The approach is shown to be\nequivalent under permutation to likelihood ratio tests for the most commonly\nused generalized linear models. For linear regression, covariates are handled\nby working with covariate-residualized responses and predictors. For\ngeneralized linear models, stratified covariates can be handled in a manner\nsimilar to exact conditional testing. Simulations and examples illustrate the\nwide applicability of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 20:30:23 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Zhou", "Yi-Hui", ""], ["Wright", "Fred", ""]]}, {"id": "1405.2370", "submitter": "Masashi Hyodo Dr", "authors": "Masashi Hyodo and Takahiro Nishiyama", "title": "A one-sample location test based on weighted averaging of two test\n  statistics in high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We discuss a one-sample location test that can be used in the case of\nhigh-dimensional data. For high-dimensional data, the power of Hotelling's test\ndecrises when the dimension is close to the sample size. To address this loss\nof power, some non-exact approaches were proposed, e.g., Dempster (1958, 1960),\nBai and Saranadasa (1996) and Srivastava and Du (2006). In this paper, we focus\non Hotelling's test and Dempster's test. The comparative merits and demerits of\nthese two tests vary according to the local parameters. In particular, we\nconsider the situation where it is difficult to determine which test should be\nused, that is, where the two tests are asymptotically equivalent in terms of\nlocal power. We propose a new statistic based on the weighted averaging of\nHotelling's $T^2$ statistic and Dempster's statistic that can be applied in\nsuch a situation. Our weight is determined on the basis of the maximum local\nasymptotic power on a restricted parameter space that induces local asymptotic\nequivalence between Hotelling's test and Dempster's test. In addition, some\ngood asymptotic properties with respect to the local power are shown. Numerical\nresults show that our test is more stable than Hotelling's $T^2$ statistic and\nDempster's statistic in most parameter settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 23:42:38 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Hyodo", "Masashi", ""], ["Nishiyama", "Takahiro", ""]]}, {"id": "1405.2377", "submitter": "James Brofos", "authors": "James Brofos", "title": "A Hybrid Monte Carlo Architecture for Parameter Optimization", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much recent research has been conducted in the area of Bayesian learning,\nparticularly with regard to the optimization of hyper-parameters via Gaussian\nprocess regression. The methodologies rely chiefly on the method of maximizing\nthe expected improvement of a score function with respect to adjustments in the\nhyper-parameters. In this work, we present a novel algorithm that exploits\nnotions of confidence intervals and uncertainties to enable the discovery of\nthe best optimal within a targeted region of the parameter space. We\ndemonstrate the efficacy of our algorithm with respect to machine learning\nproblems and show cases where our algorithm is competitive with the method of\nmaximizing expected improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 02:03:22 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Brofos", "James", ""]]}, {"id": "1405.2528", "submitter": "Esa Ollila", "authors": "Esa Ollila and David E. Tyler", "title": "Regularized $M$-estimators of scatter matrix", "comments": "Submitted to IEEE Transactions on Signal Processing (contains a\n  corrected proof of convergence of the proposed iterative algorithm)", "journal-ref": null, "doi": "10.1109/TSP.2014.2360826", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a general class of regularized $M$-estimators of scatter\nmatrix are proposed which are suitable also for low or insufficient sample\nsupport (small $n$ and large $p$) problems. The considered class constitutes a\nnatural generalization of $M$-estimators of scatter matrix (Maronna, 1976) and\nare defined as a solution to a penalized $M$-estimation cost function that\ndepend on a pair $(\\alpha,\\beta)$ of regularization parameters. We derive\ngeneral conditions for uniqueness of the solution using concept of geodesic\nconvexity. Since these conditions do not include Tyler's $M$-estimator,\nnecessary and sufficient conditions for uniqueness of the penalized Tyler's\ncost function are established separately. For the regularized Tyler's\n$M$-estimator, we also derive a simple, closed form and data dependent solution\nfor choosing the regularization parameter based on shape matrix matching in the\nmean squared sense. An iterative algorithm that converges to the solution of\nthe regularized $M$-estimating equation is also provided. Finally, some\nsimulations studies illustrate the improved accuracy of the proposed\nregularized $M$-estimators of scatter compared to their non-regularized\ncounterparts in low sample support problems. An example of radar detection\nusing normalized matched filter (NMF) illustrate that an adaptive NMF detector\nbased on regularized $M$-estimators are able to maintain accurately the preset\nCFAR level and at at the same time provide similar probability of detection as\nthe (theoretical) NMF detector.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 12:34:43 GMT"}, {"version": "v2", "created": "Thu, 15 May 2014 08:59:21 GMT"}, {"version": "v3", "created": "Mon, 9 Jun 2014 08:31:00 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Ollila", "Esa", ""], ["Tyler", "David E.", ""]]}, {"id": "1405.2601", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay, and Emanuel Parzen", "title": "LP Approach to Statistical Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to statistical data modeling and exploratory data\nanalysis called `LP Statistical Data Science.' It aims to generalize and unify\ntraditional and novel statistical measures, methods, and exploratory tools.\nThis article outlines fundamental concepts along with real-data examples to\nillustrate how the `LP Statistical Algorithm' can systematically tackle\ndifferent varieties of data types, data patterns, and data structures under a\ncoherent theoretical framework. A fundamental role is played by specially\ndesigned orthonormal basis of a random variable X for linear (Hilbert space\ntheory) representation of a general function of X, such as $\\mbox{E}[Y \\mid\nX]$.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 23:16:37 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1405.2709", "submitter": "David Golan", "authors": "David Golan and Saharon Rosset", "title": "Effective Genetic Risk Prediction Using Mixed Models", "comments": "main text: 14 pages, 3 figures. Supplementary text: 16 pages, 21\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, efforts to produce high-quality polygenic risk scores from\ngenome-wide studies of common disease have focused on estimating and\naggregating the effects of multiple SNPs. Here we propose a novel statistical\napproach for genetic risk prediction, based on random and mixed effects models.\nOur approach (termed GeRSI) circumvents the need to estimate the effect sizes\nof numerous SNPs by treating these effects as random, producing predictions\nwhich are consistently superior to current state of the art, as we demonstrate\nin extensive simulation. When applying GeRSI to seven phenotypes from the WTCCC\nstudy, we confirm that the use of random effects is most beneficial for\ndiseases that are known to be highly polygenic: hypertension (HT) and bipolar\ndisorder (BD). For HT, there are no significant associations in the WTCCC data.\nThe best existing model yields an AUC of 54%, while GeRSI improves it to 59%.\nFor BD, using GeRSI improves the AUC from 55% to 62%. For individuals ranked at\nthe top 10% of BD risk predictions, using GeRSI substantially increases the BD\nrelative risk from 1.4 to 2.5.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 11:27:28 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Golan", "David", ""], ["Rosset", "Saharon", ""]]}, {"id": "1405.2722", "submitter": "Pierre Latouche", "authors": "P. Latouche and E. Birmel\\'e and C. Ambroise", "title": "Model Selection in Overlapping Stochastic Block Models", "comments": "article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a commonly used mathematical model to describe the rich set of\ninteractions between objects of interest. Many clustering methods have been\ndeveloped in order to partition such structures, among which several rely on\nunderlying probabilistic models, typically mixture models. The relevant hidden\nstructure may however show overlapping groups in several applications. The\nOverlapping Stochastic Block Model (2011) has been developed to take this\nphenomenon into account. Nevertheless, the problem of the choice of the number\nof classes in the inference step is still open. To tackle this issue, we\nconsider the proposed model in a Bayesian framework and develop a new criterion\nbased on a non asymptotic approximation of the marginal log-likelihood. We\ndescribe how the criterion can be computed through a variational Bayes EM\nalgorithm, and demonstrate its efficiency by running it on both simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 12:10:08 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Latouche", "P.", ""], ["Birmel\u00e9", "E.", ""], ["Ambroise", "C.", ""]]}, {"id": "1405.2818", "submitter": "Guido Consonni", "authors": "Guido Consonni and Laura Deldossi", "title": "Objective Bayesian Model Discrimination in Follow-up Experimental\n  Designs", "comments": "20 pages; 2 figures; plus Supplementary Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An initial screening experiment may lead to ambiguous conclusions regarding\nthe factors which are active in explaining the variation of an outcome\nvariable: thus adding follow-up runs becomes necessary. We propose a fully\nBayes objective approach to follow-up designs, using prior distributions\nsuitably tailored to model selection. We adopt a model criterion based on a\nweighted average of Kullback-Leibler divergences between predictive\ndistributions for all possible pairs of models. When applied to real data, our\nmethod produces results which compare favorably to previous analyses based on\nsubjective weakly informative priors.\n", "versions": [{"version": "v1", "created": "Mon, 12 May 2014 16:08:35 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Consonni", "Guido", ""], ["Deldossi", "Laura", ""]]}, {"id": "1405.3292", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki, Rafael Bassi Stern", "title": "Learning with many experts: model selection and sparsity", "comments": "This is the pre-peer reviewed version", "journal-ref": "Izbicki, R., Stern, R. B. \"Learning with many experts: Model\n  selection and sparsity.\" Statistical Analysis and Data Mining 6.6 (2013):\n  565-577", "doi": "10.1002/sam.11206", "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experts classifying data are often imprecise. Recently, several models have\nbeen proposed to train classifiers using the noisy labels generated by these\nexperts. How to choose between these models? In such situations, the true\nlabels are unavailable. Thus, one cannot perform model selection using the\nstandard versions of methods such as empirical risk minimization and cross\nvalidation. In order to allow model selection, we present a surrogate loss and\nprovide theoretical guarantees that assure its consistency. Next, we discuss\nhow this loss can be used to tune a penalization which introduces sparsity in\nthe parameters of a traditional class of models. Sparsity provides more\nparsimonious models and can avoid overfitting. Nevertheless, it has seldom been\ndiscussed in the context of noisy labels due to the difficulty in model\nselection and, therefore, in choosing tuning parameters. We apply these\ntechniques to several sets of simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 20:03:14 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Izbicki", "Rafael", ""], ["Stern", "Rafael Bassi", ""]]}, {"id": "1405.3319", "submitter": "Longhai Li", "authors": "Longhai Li and Weixin Yao", "title": "Fully Bayesian Logistic Regression with Hyper-Lasso Priors for\n  High-dimensional Feature Selection", "comments": "33 pages. arXiv admin note: substantial text overlap with\n  arXiv:1308.4690", "journal-ref": "Journal of Statistical Computation and Simulation, 2018, 88:14,\n  2827-2851", "doi": "10.1080/00949655.2018.1490418", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional feature selection arises in many areas of modern science.\nFor example, in genomic research we want to find the genes that can be used to\nseparate tissues of different classes (e.g. cancer and normal) from tens of\nthousands of genes that are active (expressed) in certain tissue cells. To this\nend, we wish to fit regression and classification models with a large number of\nfeatures (also called variables, predictors). In the past decade, penalized\nlikelihood methods for fitting regression models based on hyper-LASSO\npenalization have received increasing attention in the literature. However,\nfully Bayesian methods that use Markov chain Monte Carlo (MCMC) are still in\nlack of development in the literature. In this paper we introduce an MCMC\n(fully Bayesian) method for learning severely multi-modal posteriors of\nlogistic regression models based on hyper-LASSO priors (non-convex penalties).\nOur MCMC algorithm uses Hamiltonian Monte Carlo in a restricted Gibbs sampling\nframework; we call our method Bayesian logistic regression with hyper-LASSO\n(BLRHL) priors. We have used simulation studies and real data analysis to\ndemonstrate the superior performance of hyper-LASSO priors, and to investigate\nthe issues of choosing heaviness and scale of hyper-LASSO priors.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 22:31:09 GMT"}, {"version": "v2", "created": "Mon, 19 May 2014 19:03:36 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 16:50:44 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 03:10:10 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Li", "Longhai", ""], ["Yao", "Weixin", ""]]}, {"id": "1405.3340", "submitter": "Stephen Reid", "authors": "Stephen Reid and Jonathan Taylor and Robert Tibshirani", "title": "Post-selection point and interval estimation of signal sizes in Gaussian\n  samples", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of the estimation of a vector of means from a single\nvector-valued observation $y$. Whereas previous work reduces the size of the\nestimates for the largest (absolute) sample elements via shrinkage (like\nJames-Stein) or biases estimated via empirical Bayes methodology, we take a\nnovel approach. We adapt recent developments by Lee et al (2013) in post\nselection inference for the Lasso to the orthogonal setting, where sample\nelements have different underlying signal sizes. This is exactly the setup\nencountered when estimating many means. It is shown that other selection\nprocedures, like selecting the $K$ largest (absolute) sample elements and the\nBenjamini-Hochberg procedure, can be cast into their framework, allowing us to\nleverage their results. Point and interval estimates for signal sizes are\nproposed. These seem to perform quite well against competitors, both recent and\nmore tenured.\n  Furthermore, we prove an upper bound to the worst case risk of our estimator,\nwhen combined with the Benjamini-Hochberg procedure, and show that it is within\na constant multiple of the minimax risk over a rich set of parameter spaces\nmeant to evoke sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 01:48:36 GMT"}, {"version": "v2", "created": "Sun, 18 May 2014 16:28:11 GMT"}, {"version": "v3", "created": "Fri, 23 May 2014 02:51:58 GMT"}, {"version": "v4", "created": "Wed, 18 Mar 2015 17:02:21 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Reid", "Stephen", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1405.3344", "submitter": "Stephen Reid", "authors": "Stephen Reid and Robert Tibshirani", "title": "Regularisation Paths for Conditional Logistic Regression: the clogitL1\n  package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the cyclic coordinate descent algorithm of Friedman, Hastie and\nTibshirani (2010) to the fitting of a conditional logistic regression model\nwith lasso ($\\ell_1$) and elastic net penalties. The sequential strong rules of\nTibshirani et al (2012) are also used in the algorithm and it is shown that\nthese offer a considerable speed up over the standard coordinate descent\nalgorithm with warm starts.\n  Once implemented, the algorithm is used in simulation studies to compare the\nvariable selection and prediction performance of the conditional logistic\nregression model against that of its unconditional (standard) counterpart. We\nfind that the conditional model performs admirably on datasets drawn from a\nsuitable conditional distribution, outperforming its unconditional counterpart\nat variable selection. The conditional model is also fit to a small real world\ndataset, demonstrating how we obtain regularisation paths for the parameters of\nthe model and how we apply cross validation for this method where natural\nunconditional prediction rules are hard to come by.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 02:05:32 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Reid", "Stephen", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1405.3380", "submitter": "Kosuke Morikawa", "authors": "Kosuke Morikawa and Yutaka Kano", "title": "Identification Problem for The Analysis of Binary Data with\n  Non-ignorable Missing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a missing-data mechanism is NMAR or non-ignorable, missingness is itself\nvital information and it must be taken into the likelihood, which, however,\nneeds to introduce additional parameters to be estimated. The incompleteness of\nthe data and introduction of more parameters can cause the identification\nproblem. When a response variable is binary, it becomes a more serious problem\nbecause of less information of bi- nary data, however, there are no methods to\nbriefly verify whether a mode is identified or not. Therefore, we provide a new\nnecessary and sufficient condition to easily check model identifiability when\nanalyzing binary data with non-ignorable missing by condi- tional models. This\ncondition can give us what condition is needed for a model to have\nidentifiability as well as make easily check the identifiability of a model.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 06:50:54 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Morikawa", "Kosuke", ""], ["Kano", "Yutaka", ""]]}, {"id": "1405.3489", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt", "title": "Adiabatic Monte Carlo", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy for inference in complex models is the relaxation of a\nsimple model into the more complex target model, for example the prior into the\nposterior in Bayesian inference. Existing approaches that attempt to generate\nsuch transformations, however, are sensitive to the pathologies of complex\ndistributions and can be difficult to implement in practice. Leveraging the\ngeometry of thermodynamic processes I introduce a principled and robust\napproach to deforming measures that presents a powerful new tool for inference.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 13:28:57 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 17:41:48 GMT"}, {"version": "v3", "created": "Fri, 12 Sep 2014 13:42:27 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 16:18:11 GMT"}, {"version": "v5", "created": "Thu, 6 Aug 2015 18:11:41 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Betancourt", "M. J.", ""]]}, {"id": "1405.3559", "submitter": "Andrea Mignatti", "authors": "Giorgio Corani and Andrea Mignatti", "title": "Credal Model Averaging for classification: representing prior ignorance\n  and expert opinions", "comments": "15 pages 6 figures Preprint submitted to the International Journal of\n  Approximate Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI q-bio.PE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging (BMA) is the state of the art approach for\novercoming model uncertainty. Yet, especially on small data sets, the results\nyielded by BMA might be sensitive to the prior over the models. Credal Model\nAveraging (CMA) addresses this problem by substituting the single prior over\nthe models by a set of priors (credal set). Such approach solves the problem of\nhow to choose the prior over the models and automates sensitivity analysis. We\ndiscuss various CMA algorithms for building an ensemble of logistic regressors\ncharacterized by different sets of covariates. We show how CMA can be\nappropriately tuned to the case in which one is prior-ignorant and to the case\nin which instead domain knowledge is available. CMA detects prior-dependent\ninstances, namely instances in which a different class is more probable\ndepending on the prior over the models. On such instances CMA suspends the\njudgment, returning multiple classes. We thoroughly compare different BMA and\nCMA variants on a real case study, predicting presence of Alpine marmot burrows\nin an Alpine valley. We find that BMA is almost a random guesser on the\ninstances recognized as prior-dependent by CMA.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 16:06:39 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Corani", "Giorgio", ""], ["Mignatti", "Andrea", ""]]}, {"id": "1405.3689", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Nearest Neighbor Methods for Testing Reflexivity and\n  Species-Correspondence", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-14-1", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (NN) methods are employed for drawing inferences about\nspatial patterns of points from two or more classes. We consider Pielou's test\nof niche specificity which is defined using a contingency table based on the NN\nrelationships between the data points. We demonstrate that Pielou's contingency\ntable for niche specificity is actually more appropriate for testing\nreflexivity in NN structure, hence we call this table as NN reflexivity\ncontingency table (NN-RCT) henceforth. We also derive an asymptotic\napproximation for the distribution of the entries of the NN-RCT and consider\nvariants of Fisher's exact test on it. Moreover, we introduce a new test of\nclass- or species-correspondence inspired by spatial niche/habitat specificity\nand the associated contingency table called species-correspondence contingency\ntable (SCCT). We also determine the appropriate null hypotheses and the\nunderlying conditions appropriate for these tests. We investigate the finite\nsample performance of the tests in terms of empirical size and power by\nextensive Monte Carlo simulations and the methods are illustrated on a\nreal-life ecological data set.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 21:09:28 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "1405.3718", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer and Francisco Cribari-Neto", "title": "Model selection criteria in beta regression with varying dispersion", "comments": "22 pages, 3 figures, 7 tables", "journal-ref": "Communications in Statistics - Simulation and Computation, 2017,\n  Vol 46, Issue 1", "doi": "10.1016/j.cnsns.2016.10.016", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of model selection in beta regressions with varying\ndispersion. The model consists of two submodels, namely: for the mean and for\nthe dispersion. Our focus is on the selection of the covariates for each\nsubmodel. Our Monte Carlo evidence reveals that the joint selection of\ncovariates for the two submodels is not accurate in finite samples. We\nintroduce two new model selection criteria that explicitly account for varying\ndispersion and propose a fast two step model selection scheme which is\nconsiderably more accurate and is computationally less costly than usual joint\nmodel selection. Monte Carlo evidence is presented and discussed. We also\npresent the results of an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 00:08:14 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 19:59:30 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1405.3880", "submitter": "Scott Holan", "authors": "Aaron T. Porter and Scott H. Holan and Christopher K. Wikle", "title": "Bayesian Semiparametric Hierarchical Empirical Likelihood Spatial Models", "comments": "29 pages, 3 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general hierarchical Bayesian framework that incorporates a\nflexible nonparametric data model specification through the use of empirical\nlikelihood methodology, which we term semiparametric hierarchical empirical\nlikelihood (SHEL) models. Although general dependence structures can be readily\naccommodated, we focus on spatial modeling, a relatively underdeveloped area in\nthe empirical likelihood literature. Importantly, the models we develop\nnaturally accommodate spatial association on irregular lattices and irregularly\nspaced point-referenced data. We illustrate our proposed framework by means of\na simulation study and through three real data examples. First, we develop a\nspatial Fay-Herriot model in the SHEL framework and apply it to the problem of\nsmall area estimation in the American Community Survey. Next, we illustrate the\nSHEL model in the context of areal data (on an irregular lattice) through the\nNorth Carolina sudden infant death syndrome (SIDS) dataset. Finally, we analyze\na point-referenced dataset from the North American Breeding Bird survey that\nconsiders dove counts for the state of Missouri. In all cases, we demonstrate\nsuperior performance of our model, in terms of mean squared prediction error,\nover standard parametric analyses.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 15:25:52 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Porter", "Aaron T.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1405.3904", "submitter": "Benjamin A. Shaby", "authors": "Benjamin A. Shaby, Brian J. Reich, Daniel Cooley, Cari G. Kaufman", "title": "A Markov-switching model for heat waves", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS873 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2016, Vol. 10, No. 1, 74-93", "doi": "10.1214/15-AOAS873", "report-no": "IMS-AOAS-AOAS873", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heat waves merit careful study because they inflict severe economic and\nsocietal damage. We use an intuitive, informal working definition of a heat\nwave-a persistent event in the tail of the temperature distribution-to motivate\nan interpretable latent state extreme value model. A latent variable with\ndependence in time indicates membership in the heat wave state. The strength of\nthe temporal dependence of the latent variable controls the frequency and\npersistence of heat waves. Within each heat wave, temperatures are modeled\nusing extreme value distributions, with extremal dependence across time\naccomplished through an extreme value Markov model. One important virtue of\ninterpretability is that model parameters directly translate into quantities of\ninterest for risk management, so that questions like whether heat waves are\nbecoming longer, more severe or more frequent are easily answered by querying\nan appropriate fitted model. We demonstrate the latent state model on two\nrecent, calamitous, examples: the European heat wave of 2003 and the Russian\nheat wave of 2010.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 16:38:10 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 16:12:28 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 08:51:16 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Shaby", "Benjamin A.", ""], ["Reich", "Brian J.", ""], ["Cooley", "Daniel", ""], ["Kaufman", "Cari G.", ""]]}, {"id": "1405.3920", "submitter": "Joshua Loftus", "authors": "Joshua R. Loftus, Jonathan E. Taylor", "title": "A significance test for forward stepwise model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the methods developed by Lockhart et al. (2013) and Taylor et al.\n(2013) on significance tests for penalized regression to forward stepwise model\nselection. A general framework for selection procedures described by quadratic\ninequalities includes a variant of forward stepwise with grouped variables,\nallowing us to handle categorical variables and factor models. We provide an\nalgorithm to compute a new statistic with an exact null distribution\nconditional on the outcome of the model selection procedure. This new\nstatistic, which we denote $T\\chi$, has a truncated $\\chi$ distribution under\nthe global null. We apply this test in forward stepwise iteratively on the\nresidual after each step. The resulting method has the computational strengths\nof stepwise selection and addresses the problem of invalid test statistics due\nto model selection. We illustrate the flexibility of this method by applying it\nto several specialized applications of forward stepwise including a\nhierarchical interactions model and a recently described additive model that\nadaptively chooses between linear and nonlinear effects for each variable.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 17:40:20 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Loftus", "Joshua R.", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1405.4047", "submitter": "Berk Ustun", "authors": "Berk Ustun and Cynthia Rudin", "title": "Methods and Models for Interpretable Linear Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integer programming framework to build accurate and\ninterpretable discrete linear classification models. Unlike existing\napproaches, our framework is designed to provide practitioners with the control\nand flexibility they need to tailor accurate and interpretable models for a\ndomain of choice. To this end, our framework can produce models that are fully\noptimized for accuracy, by minimizing the 0--1 classification loss, and that\naddress multiple aspects of interpretability, by incorporating a range of\ndiscrete constraints and penalty functions. We use our framework to produce\nmodels that are difficult to create with existing methods, such as scoring\nsystems and M-of-N rule tables. In addition, we propose specially designed\noptimization methods to improve the scalability of our framework through\ndecomposition and data reduction. We show that discrete linear classifiers can\nattain the training accuracy of any other linear classifier, and provide an\nOccam's Razor type argument as to why the use of small discrete coefficients\ncan provide better generalization. We demonstrate the performance and\nflexibility of our framework through numerical experiments and a case study in\nwhich we construct a highly tailored clinical tool for sleep apnea diagnosis.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 01:30:11 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 23:33:31 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Ustun", "Berk", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1405.4141", "submitter": "Alexander Matthews BA MSci MA (Cantab)", "authors": "Alexander G. de. G Matthews and Zoubin Ghahramani", "title": "Classification using log Gaussian Cox processes", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  McCullagh and Yang (2006) suggest a family of classification algorithms based\non Cox processes. We further investigate the log Gaussian variant which has a\nnumber of appealing properties. Conditioned on the covariates, the distribution\nover labels is given by a type of conditional Markov random field. In the\nsupervised case, computation of the predictive probability of a single test\npoint scales linearly with the number of training points and the multiclass\ngeneralization is straightforward. We show new links between the supervised\nmethod and classical nonparametric methods. We give a detailed analysis of the\npairwise graph representable Markov random field, which we use to extend the\nmodel to semi-supervised learning problems, and propose an inference method\nbased on graph min-cuts. We give the first experimental analysis on supervised\nand semi-supervised datasets and show good empirical performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 12:10:12 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 13:02:49 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Matthews", "Alexander G. de. G", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1405.4225", "submitter": "Tom Michoel", "authors": "Tom Michoel", "title": "Natural coordinate descent algorithm for L1-penalised regression in\n  generalised linear models", "comments": "15 pages, 3 figures; revised version with additional numerical\n  experiments", "journal-ref": "Computational Statistics and Data Analysis (2016), pp. 60-70", "doi": "10.1016/j.csda.2015.11.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding the maximum likelihood estimates for the regression\ncoefficients in generalised linear models with an L1 sparsity penalty is shown\nto be equivalent to minimising the unpenalised maximum log-likelihood function\nover a box with boundary defined by the L1-penalty parameter. In one-parameter\nmodels or when a single coefficient is estimated at a time, this result implies\na generic soft-thresholding mechanism which leads to a novel coordinate descent\nalgorithm for generalised linear models that is entirely described in terms of\nthe natural formulation of the model and is guaranteed to converge to the true\noptimum. A prototype implementation for logistic regression tested on two\nlarge-scale cancer gene expression datasets shows that this algorithm is\nefficient, particularly so when a solution is computed at set values of the\nL1-penalty parameter as opposed to along a regularisation path. Source code and\ntest data are available from http://tmichoel.github.io/glmnat/.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 16:07:37 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 13:58:22 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 09:17:53 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Michoel", "Tom", ""]]}, {"id": "1405.4251", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Noah Simon, and Daniela Witten", "title": "Selection Bias Correction and Effect Size Estimation under Dependence", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale studies in which it is of interest to test a very\nlarge number of hypotheses, and then to estimate the effect sizes corresponding\nto the rejected hypotheses. For instance, this setting arises in the analysis\nof gene expression or DNA sequencing data. However, naive estimates of the\neffect sizes suffer from selection bias, i.e., some of the largest naive\nestimates are large due to chance alone. Many authors have proposed methods to\nreduce the effects of selection bias under the assumption that the naive\nestimates of the effect sizes are independent. Unfortunately, when the effect\nsize estimates are dependent, these existing techniques can have very poor\nperformance, and in practice there will often be dependence. We propose an\nestimator that adjusts for selection bias under a recently-proposed frequentist\nframework, without the independence assumption. We study some properties of the\nproposed estimator, and illustrate that it outperforms past proposals in a\nsimulation study and on two gene expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 17:32:34 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2015 19:31:52 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Tan", "Kean Ming", ""], ["Simon", "Noah", ""], ["Witten", "Daniela", ""]]}, {"id": "1405.4275", "submitter": "Anil Damle", "authors": "Anil Damle, Yuekai Sun", "title": "A geometric approach to archetypal analysis and non-negative matrix\n  factorization", "comments": "36 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal analysis and non-negative matrix factorization (NMF) are staples\nin a statisticians toolbox for dimension reduction and exploratory data\nanalysis. We describe a geometric approach to both NMF and archetypal analysis\nby interpreting both problems as finding extreme points of the data cloud. We\nalso develop and analyze an efficient approach to finding extreme points in\nhigh dimensions. For modern massive datasets that are too large to fit on a\nsingle machine and must be stored in a distributed setting, our approach makes\nonly a small number of passes over the data. In fact, it is possible to obtain\nthe NMF or perform archetypal analysis with just two passes over the data.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 19:22:19 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 21:16:10 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Damle", "Anil", ""], ["Sun", "Yuekai", ""]]}, {"id": "1405.4316", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Harouna Sangar\\'e, Cheikhna Hamallah Ndiaye", "title": "A Review on asymptotic normality of sums of associated random variables", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we make a round up of the theory of asymptotic normality of\nsums of associated random variables, in a coherent approach in view of further\ncontributions for new researchers in the field. (Version 01)\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 21:42:16 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 18:50:11 GMT"}, {"version": "v3", "created": "Sat, 17 Nov 2018 10:49:09 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Lo", "Gane Samb", ""], ["Sangar\u00e9", "Harouna", ""], ["Ndiaye", "Cheikhna Hamallah", ""]]}, {"id": "1405.4525", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer and Francisco Cribari-Neto", "title": "Bootstrap-based model selection criteria for beta regressions", "comments": "27 pages, 1 figure, 7 tables", "journal-ref": "TEST, Volume 24, Issue 4, pp 776-795 (2015)", "doi": "10.1007/s11749-015-0434-6", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Akaike information criterion (AIC) is a model selection criterion widely\nused in practical applications. The AIC is an estimator of the log-likelihood\nexpected value, and measures the discrepancy between the true model and the\nestimated model. In small samples the AIC is biased and tends to select\noverparameterized models. To circumvent that problem, we propose two new\nselection criteria, namely: the bootstrapped likelihood quasi-CV (BQCV) and its\n632QCV variant. We use Monte Carlo simulation to compare the finite sample\nperformances of the two proposed criteria to those of the AIC and its\nvariations that use the bootstrapped log-likelihood in the class of varying\ndispersion beta regressions. The numerical evidence shows that the proposed\nmodel selection criteria perform well in small samples. We also present and\ndiscuss and empirical application.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 16:47:49 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1405.4529", "submitter": "Ali Akbar Jafari", "authors": "Abbas Pak and Nayereh Bagheri Khoolenjani and Ali Akbar Jafari", "title": "Inference on P(Y<X) in Bivariate Rayleigh Distribution", "comments": "Accepted for publication. Communications in Statistics- Theory and\n  Methods, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of reliability $R=P(Y<X)$ when $X$ is a\nrandom strength of a component subjected to a random stress $Y$ and $(X,Y)$\nfollows a bivariate Rayleigh distribution. The maximum likelihood estimator of\n$R$ and its asymptotic distribution are obtained. An asymptotic confidence\ninterval of $R$ is constructed using the asymptotic distribution. Also, two\nconfidence intervals are proposed based on Bootstrap method and a computational\napproach. Testing of the reliability based on asymptotic distribution of $R$ is\ndiscussed. Simulation study to investigate performance of the confidence\nintervals and tests has been carried out. Also, a numerical example is given to\nillustrate the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 17:27:49 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Pak", "Abbas", ""], ["Khoolenjani", "Nayereh Bagheri", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1405.4532", "submitter": "Ali Akbar Jafari", "authors": "Kamel Abdollahnezhad and M. Babanezhad and Ali Akbar Jafari", "title": "Inference on Difference of Means of two Log-Normal Distributions; A\n  Generalized Approach", "comments": "http://www.scienpress.com/journal_focus.asp?main_id=68&Sub_id=IV&Issue=300", "journal-ref": "Journal of Statistical and Econometric Methods, vol.1, no.2, 2012,\n  125-131", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, various methods for comparing the means of two\nlog-normal have been proposed. Some of them are differing in terms of how the\nstatistic test adjust to accept or to reject the null hypothesis. In this\nstudy, a new method of test for comparing the means of two log-normal\npopulations is given through the generalized measure of evidence to have\nagainst the null hypothesis. However calculations of this method are simple, we\nfind analytically that the considered method is doing well through comparing\nthe size and power statistic test. In addition to the simulations, an example\nwith real data is illustrated.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 18:19:09 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Abdollahnezhad", "Kamel", ""], ["Babanezhad", "M.", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1405.4538", "submitter": "Hui Jiang", "authors": "Hui Jiang and Tianyu Zhan", "title": "Unit-free and robust detection of differential expression from RNA-Seq\n  data", "comments": "27 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra high-throughput sequencing of transcriptomes (RNA-Seq) is a widely used\nmethod for quantifying gene expression levels due to its low cost, high\naccuracy and wide dynamic range for detection. However, the nature of RNA-Seq\nmakes it nearly impossible to provide absolute measurements of transcript\nabundances. Several units or data summarization methods for transcript\nquantification have been proposed in the past to account for differences in\ntranscript lengths and sequencing depths across different genes and different\nsamples. Nevertheless, further between-sample normalization is still needed for\nreliable detection of differentially expressed genes. In this paper we propose\na unified statistical model for joint detection of differential gene expression\nand between-sample normalization. Our method is independent of the unit in\nwhich gene expression levels are summarized. We also introduce an efficient\nalgorithm for model fitting. Due to the L0-penalized likelihood used in our\nmodel, it is able to reliably normalize the data and detect differential gene\nexpression in some cases when more than $50\\%$ of the genes are differentially\nexpressed in an asymmetric manner. We compare our method with existing methods\nusing simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 19:09:10 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2016 22:02:28 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 20:26:52 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Jiang", "Hui", ""], ["Zhan", "Tianyu", ""]]}, {"id": "1405.4540", "submitter": "Chris A. J. Klaassen", "authors": "Chris A.J. Klaassen", "title": "Evidential Value in ANOVA-Regression Results in Scientific Integrity\n  Studies", "comments": "10 pages, 2 tables. arXiv admin note: substantial text overlap with\n  arXiv:1304.7198", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some scientific publications are under suspicion of fabrication of data.\nSince humans are bad random number generators, there might be some evidential\nvalue in favor of fabrication in the statistical results as presented in such\npapers. In case of ANOVA-Regression studies we present the evidential value of\nthe results of such a study in favor of the hypothesis of a dependence\nstructure in the underlying data, which indicates fabrication, versus the\nhypothesis of independence, which is the ANOVA model assumption. Applications\nof this approach are also presented.\n", "versions": [{"version": "v1", "created": "Sun, 18 May 2014 19:19:44 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 13:56:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Klaassen", "Chris A. J.", ""]]}, {"id": "1405.4574", "submitter": "Kristjan Greenewald", "authors": "Kristjan H. Greenewald and Alfred O. Hero III", "title": "Kronecker PCA Based Spatio-Temporal Modeling of Video for Dismount\n  Classification", "comments": "8 pages. To appear in Proceeding of SPIE DSS. arXiv admin note: text\n  overlap with arXiv:1402.5568", "journal-ref": null, "doi": "10.1117/12.2050184", "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of KronPCA spatio-temporal modeling techniques\n[Greenewald et al 2013, Tsiligkaridis et al 2013] to the extraction of\nspatiotemporal features for video dismount classification. KronPCA performs a\nlow-rank type of dimensionality reduction that is adapted to spatio-temporal\ndata and is characterized by the T frame multiframe mean and covariance of p\nspatial features. For further regularization and improved inverse estimation,\nwe also use the diagonally corrected KronPCA shrinkage methods we presented in\n[Greenewald et al 2013]. We apply this very general method to the modeling of\nthe multivariate temporal behavior of HOG features extracted from pedestrian\nbounding boxes in video, with gender classification in a challenging dataset\nchosen as a specific application. The learned covariances for each class are\nused to extract spatiotemporal features which are then classified, achieving\ncompetitive classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 01:22:34 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Greenewald", "Kristjan H.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1405.4578", "submitter": "Daniel Vasiliu", "authors": "D. Vasiliu, T. Dey and I. L. Dryden", "title": "Penalized Euclidean Distance Regression", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed for variable screening, variable selection and\nprediction in linear regression problems where the number of predictors can be\nmuch larger than the number of observations. The method involves minimizing a\npenalized Euclidean distance, where the penalty is the geometric mean of the\n$\\ell_1$ and $\\ell_2$ norms of the regression coefficients. This particular\nformulation exhibits a grouping effect, which is useful for screening out\npredictors in higher or ultra-high dimensional problems. Also, an important\nresult is a signal recovery theorem, which does not require an estimate of the\nnoise standard deviation. Practical performances of variable selection and\nprediction are evaluated through simulation studies and the analysis of a\ndataset of mass spectrometry scans from melanoma patients, where excellent\npredictive performance is obtained.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 01:39:58 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 01:30:02 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 22:09:14 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Vasiliu", "D.", ""], ["Dey", "T.", ""], ["Dryden", "I. L.", ""]]}, {"id": "1405.4637", "submitter": "Cedric Taverne", "authors": "Cedric Taverne and Philippe Lambert", "title": "Inflated Discrete Beta Regression Models for Likert and Discrete Rating\n  Scale Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete ordinal responses such as Likert scales are regularly proposed in\nquestionnaires and used as dependent variable in modeling. The response\ndistribution for such scales is always discrete, with bounded support and often\nskewed. In addition, one particular level of the scale is frequently inflated\nas it cumulates respondents who invariably choose that particular level\n(typically the middle or one extreme of the scale) without hesitation with\nthose who chose that alternative but might have selected a neighboring one. The\ninflated discrete beta regression (IDBR) model addresses those four critical\ncharacteristics that have never been taken into account simultaneously by\nexisting models. The mean and the dispersion of rates are jointly regressed on\ncovariates using an underlying beta distribution. The probability that choosers\nof the inflated level invariably make that choice is also regressed on\ncovariates. Simulation studies used to evaluate the statistical properties of\nthe IDBR model suggest that it produces more precise predictions than competing\nmodels. The ability to jointly model the location and dispersion of (the\ndistribution of) an ordinal response, as well as to characterize the profile of\nsubject selecting an \"inflated\" alternative are the most relevant features of\nthe IDBR model. It is illustrated with the analysis of the political\npositioning on a \"left-right\" scale of the Belgian respondents in the 2012\nEuropean Social Survey.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 08:26:26 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Taverne", "Cedric", ""], ["Lambert", "Philippe", ""]]}, {"id": "1405.4667", "submitter": "Raymond J. Carroll", "authors": "Raymond J. Carroll", "title": "Estimating the Distribution of Dietary Consumption Patterns", "comments": "Published in at http://dx.doi.org/10.1214/12-STS413 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: substantial text\n  overlap with arXiv:1107.4868", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 2-8", "doi": "10.1214/12-STS413", "report-no": "IMS-STS-STS413", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the United States the preferred method of obtaining dietary intake data is\nthe 24-hour dietary recall, yet the measure of most interest is usual or\nlong-term average daily intake, which is impossible to measure. Thus, usual\ndietary intake is assessed with considerable measurement error. We were\ninterested in estimating the population distribution of the Healthy Eating\nIndex-2005 (HEI-2005), a multi-component dietary quality index involving ratios\nof interrelated dietary components to energy, among children aged 2-8 in the\nUnited States, using a national survey and incorporating survey weights. We\ndeveloped a highly nonlinear, multivariate zero-inflated data model with\nmeasurement error to address this question. Standard nonlinear mixed model\nsoftware such as SAS NLMIXED cannot handle this problem. We found that taking a\nBayesian approach, and using MCMC, resolved the computational issues and doing\nso enabled us to provide a realistic distribution estimate for the HEI-2005\ntotal score. While our computation and thinking in solving this problem was\nBayesian, we relied on the well-known close relationship between Bayesian\nposterior means and maximum likelihood, the latter not computationally\nfeasible, and thus were able to develop standard errors using balanced repeated\nreplication, a survey-sampling approach.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 10:39:10 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Carroll", "Raymond J.", ""]]}, {"id": "1405.4679", "submitter": "Daniela De Angelis", "authors": "Daniela De Angelis, Anne M. Presanis, Stefano Conti, A. E. Ades", "title": "Estimation of HIV Burden through Bayesian Evidence Synthesis", "comments": "Published in at http://dx.doi.org/10.1214/13-STS428 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 9-17", "doi": "10.1214/13-STS428", "report-no": "IMS-STS-STS428", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning, implementation and evaluation of public health policies to control\nthe human immunodeficiency virus (HIV) epidemic require regular monitoring of\ndisease burden. This includes the proportion living with HIV, whether diagnosed\nor not, and the rate of new infections in the general population and in\nspecific risk groups and regions. Estimation of these quantities is not\nstraightforward: data informing them directly are not typically available, but\na wealth of indirect information from surveillance systems and ad hoc studies\ncan inform functions of these quantities. In this paper we show how the\nestimation problem can be successfully solved through a Bayesian evidence\nsynthesis approach, relaxing the focus on \"best available\" data to which\nclassical methods are typically restricted. This more comprehensive and\nflexible use of evidence has led to the adoption of our proposed approach as\nthe official method to estimate HIV prevalence in the United Kingdom since\n2005.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 11:15:39 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["De Angelis", "Daniela", ""], ["Presanis", "Anne M.", ""], ["Conti", "Stefano", ""], ["Ades", "A. E.", ""]]}, {"id": "1405.4682", "submitter": "Mariel M. Finucane", "authors": "Mariel M. Finucane, Christopher J. Paciorek, Goodarz Danaei, Majid\n  Ezzati", "title": "Bayesian Estimation of Population-Level Trends in Measures of Health\n  Status", "comments": "Published in at http://dx.doi.org/10.1214/13-STS427 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 18-25", "doi": "10.1214/13-STS427", "report-no": "IMS-STS-STS427", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving health worldwide will require rigorous quantification of\npopulation-level trends in health status. However, global-level surveys are not\navailable, forcing researchers to rely on fragmentary country-specific data of\nvarying quality. We present a Bayesian model that systematically combines\ndisparate data to make country-, region- and global-level estimates of time\ntrends in important health indicators. The model allows for time and age\nnonlinearity, and it borrows strength in time, age, covariates, and within and\nacross regional country clusters to make estimates where data are sparse. The\nBayesian approach allows us to account for uncertainty from the various aspects\nof missingness as well as sampling and parameter uncertainty. MCMC sampling\nallows for inference in a high-dimensional, constrained parameter space, while\nproviding posterior draws that allow straightforward inference on the wide\nvariety of functionals of interest. Here we use blood pressure as an example\nhealth metric. High blood pressure is the leading risk factor for\ncardiovascular disease, the leading cause of death worldwide. The results\nhighlight a risk transition, with decreasing blood pressure in high-income\nregions and increasing levels in many lower-income regions.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 11:32:46 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Finucane", "Mariel M.", ""], ["Paciorek", "Christopher J.", ""], ["Danaei", "Goodarz", ""], ["Ezzati", "Majid", ""]]}, {"id": "1405.4687", "submitter": "Andrew Gelman", "authors": "Andrew Gelman", "title": "How Bayesian Analysis Cracked the Red-State, Blue-State Problem", "comments": "Published in at http://dx.doi.org/10.1214/13-STS458 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 26-35", "doi": "10.1214/13-STS458", "report-no": "IMS-STS-STS458", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the United States as in other countries, political and economic divisions\ncut along geographic and demographic lines. Richer people are more likely to\nvote for Republican candidates while poorer voters lean Democratic; this is\nconsistent with the positions of the two parties on economic issues. At the\nsame time, richer states on the coasts are bastions of the Democrats, while\nmost of the generally lower-income areas in the middle of the country strongly\nsupport Republicans. During a research project lasting several years, we\nreconciled these patterns by fitting a series of multilevel models to perform\ninference on geographic and demographic subsets of the population. We were\nusing national survey data with relatively small samples in some states, ethnic\ngroups and income categories; this motivated the use of Bayesian inference to\npartially pool between fitted models and local data. Previous, non-Bayesian\nanalyses of income and voting had failed to connect individual and state-level\npatterns. Now that our analysis has been done, we believe it could be\nreplicated using non-Bayesian methods, but Bayesian inference helped us crack\nthe problem by directly handling the uncertainty that is inherent in working\nwith sparse data.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 11:56:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Gelman", "Andrew", ""]]}, {"id": "1405.4692", "submitter": "Sandra Johnson", "authors": "Sandra Johnson, Eva Abal, Kathleen Ahern, Grant Hamilton", "title": "From Science to Management: Using Bayesian Networks to Learn about\n  Lyngbya", "comments": "Published in at http://dx.doi.org/10.1214/13-STS424 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 36-41", "doi": "10.1214/13-STS424", "report-no": "IMS-STS-STS424", "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxic blooms of Lyngbya majuscula occur in coastal areas worldwide and have\nmajor ecological, health and economic consequences. The exact causes and\ncombinations of factors which lead to these blooms are not clearly understood.\nLyngbya experts and stakeholders are a particularly diverse group, including\necologists, scientists, state and local government representatives, community\norganisations, catchment industry groups and local fishermen. An integrated\nBayesian network approach was developed to better understand and model this\ncomplex environmental problem, identify knowledge gaps, prioritise future\nresearch and evaluate management options.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 12:15:18 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Johnson", "Sandra", ""], ["Abal", "Eva", ""], ["Ahern", "Kathleen", ""], ["Hamilton", "Grant", ""]]}, {"id": "1405.4696", "submitter": "Sakari Kuikka", "authors": "Sakari Kuikka, Jarno Vanhatalo, Henni Pulkkinen, Samu M\\\"antyniemi,\n  Jukka Corander", "title": "Experiences in Bayesian Inference in Baltic Salmon Management", "comments": "Published in at http://dx.doi.org/10.1214/13-STS431 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 42-49", "doi": "10.1214/13-STS431", "report-no": "IMS-STS-STS431", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a success story regarding Bayesian inference in fisheries\nmanagement in the Baltic Sea. The management of salmon fisheries is currently\nbased on the results of a complex Bayesian population dynamic model, and\nmanagers and stakeholders use the probabilities in their discussions. We also\ndiscuss the technical and human challenges in using Bayesian modeling to give\npractical advice to the public and to government officials and suggest future\nareas in which it can be applied. In particular, large databases in fisheries\nscience offer flexible ways to use hierarchical models to learn the population\ndynamics parameters for those by-catch species that do not have similar large\nstock-specific data sets like those that exist for many target species. This\ninformation is required if we are to understand the future ecosystem risks of\nfisheries.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 12:36:46 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Kuikka", "Sakari", ""], ["Vanhatalo", "Jarno", ""], ["Pulkkinen", "Henni", ""], ["M\u00e4ntyniemi", "Samu", ""], ["Corander", "Jukka", ""]]}, {"id": "1405.4701", "submitter": "Daniel Mortlock", "authors": "Daniel Mortlock", "title": "Finding the Most Distant Quasars Using Bayesian Selection Methods", "comments": "Published in at http://dx.doi.org/10.1214/13-STS432 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 50-57", "doi": "10.1214/13-STS432", "report-no": "IMS-STS-STS432", "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasars, the brightly glowing disks of material that can form around the\nsuper-massive black holes at the centres of large galaxies, are amongst the\nmost luminous astronomical objects known and so can be seen at great distances.\nThe most distant known quasars are seen as they were when the Universe was less\nthan a billion years old (i.e., $\\sim\\!7%$ of its current age). Such distant\nquasars are, however, very rare, and so are difficult to distinguish from the\nbillions of other comparably-bright sources in the night sky. In searching for\nthe most distant quasars in a recent astronomical sky survey (the UKIRT\nInfrared Deep Sky Survey, UKIDSS), there were $\\sim\\!10^3$ apparently plausible\ncandidates for each expected quasar, far too many to reobserve with other\ntelescopes. The solution to this problem was to apply Bayesian model\ncomparison, making models of the quasar population and the dominant\ncontaminating population (Galactic stars) to utilise the information content in\nthe survey measurements. The result was an extremely efficient selection\nprocedure that was used to quickly identify the most promising UKIDSS\ncandidates, one of which was subsequently confirmed as the most distant quasar\nknown to date.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 12:54:03 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mortlock", "Daniel", ""]]}, {"id": "1405.4708", "submitter": "Adrian E. Raftery", "authors": "Adrian E. Raftery, Leontine Alkema, Patrick Gerland", "title": "Bayesian Population Projections for the United Nations", "comments": "Published in at http://dx.doi.org/10.1214/13-STS419 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 58-68", "doi": "10.1214/13-STS419", "report-no": "IMS-STS-STS419", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United Nations regularly publishes projections of the populations of all\nthe world's countries broken down by age and sex. These projections are the de\nfacto standard and are widely used by international organizations, governments\nand researchers. Like almost all other population projections, they are\nproduced using the standard deterministic cohort-component projection method\nand do not yield statements of uncertainty. We describe a Bayesian method for\nproducing probabilistic population projections for most countries which are\nprojections that the United Nations could use. It has at its core Bayesian\nhierarchical models for the total fertility rate and life expectancy at birth.\nWe illustrate the method and show how it can be extended to address concerns\nabout the UN's current assumptions about the long-term distribution of\nfertility. The method is implemented in the R packages bayesTFR, bayesLife,\nbayesPop and bayesDem.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 13:08:26 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Raftery", "Adrian E.", ""], ["Alkema", "Leontine", ""], ["Gerland", "Patrick", ""]]}, {"id": "1405.4713", "submitter": "Huiyue  Yi", "authors": "Huiyue Yi", "title": "RMT Estimator with Adaptive Decision Criteria for Estimating the Number\n  of Signals Based on Random Matrix Theory", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of signals embedded in noise is a fundamental problem\nin signal processing. As a classic estimator based on random matrix theory\n(RMT), the RMT estimator estimates the number of signals via sequentially\ntesting the likelihood of an eigenvalue as arising from a signal or noise for a\ngiven over-detection probability. However, it tends to under-estimate the\nnumber of signals as weak signal eigenvalues may be immersed in the\nnon-negligible bias term among eigenvalues for finite sample size. In order to\nsolve this problem, we propose an RMT estimator with adaptive decision\ncriterion (termed as RMT-ADC estimator) by adaptively incorporating the bias\nterm into the decision criterion of the RMT estimator. Firstly, we analyze the\neffect of this bias term among eigenvalues on the estimation performance of the\nRMT estimator. Then, we derive both the decreased over-estimation probability\nand the increased under-estimation probability of the RMT estimator incurred by\nthe bias term when assuming the eigenvalue being tested is arising from a\nsignal, and also derive the increased under-estimation probability of the RMT\nestimator incurred by the bias term when assuming the eigenvalue being tested\nis arising from noise. Based on these results, the RMT-ADC estimator can\nadaptively determine whether the noise variance should be estimated under the\nassumption that the eigenvalue being tested is arising from a signal or from\nnoise, and thus can adaptively select its decision criterion. Moreover, the\nRMT-ADC estimator can adaptively determine whether the bias term among\neigenvalues should be incorporated into the selected decision criterion or not.\nTherefore, the RMT-ADC estimator can avoid the higher under-estimation\nprobability of the RMT estimator. Finally, simulation results are presented to\nshow that the proposed RMT-ADC estimator significantly outperforms the existing\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 13:18:47 GMT"}, {"version": "v10", "created": "Mon, 30 Mar 2015 14:02:26 GMT"}, {"version": "v11", "created": "Wed, 22 Apr 2015 11:38:17 GMT"}, {"version": "v12", "created": "Mon, 31 Aug 2015 12:24:00 GMT"}, {"version": "v13", "created": "Thu, 10 Sep 2015 14:41:24 GMT"}, {"version": "v14", "created": "Mon, 26 Oct 2015 11:51:49 GMT"}, {"version": "v15", "created": "Tue, 17 Nov 2015 13:25:27 GMT"}, {"version": "v16", "created": "Mon, 1 Feb 2016 14:07:46 GMT"}, {"version": "v17", "created": "Thu, 16 Jun 2016 15:10:52 GMT"}, {"version": "v18", "created": "Thu, 6 Oct 2016 15:05:27 GMT"}, {"version": "v19", "created": "Thu, 2 Nov 2017 14:31:06 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 11:18:28 GMT"}, {"version": "v20", "created": "Thu, 28 Jun 2018 12:56:41 GMT"}, {"version": "v21", "created": "Sat, 13 Apr 2019 16:07:50 GMT"}, {"version": "v22", "created": "Tue, 26 Nov 2019 15:02:25 GMT"}, {"version": "v3", "created": "Wed, 13 Aug 2014 13:19:06 GMT"}, {"version": "v4", "created": "Wed, 20 Aug 2014 12:29:16 GMT"}, {"version": "v5", "created": "Mon, 6 Oct 2014 08:34:21 GMT"}, {"version": "v6", "created": "Thu, 6 Nov 2014 14:42:54 GMT"}, {"version": "v7", "created": "Wed, 7 Jan 2015 13:20:51 GMT"}, {"version": "v8", "created": "Wed, 4 Mar 2015 13:22:46 GMT"}, {"version": "v9", "created": "Thu, 19 Mar 2015 14:16:23 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Yi", "Huiyue", ""]]}, {"id": "1405.4720", "submitter": "Lawrence D. Stone", "authors": "Lawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, Johan P.\n  Strumpfer", "title": "Search for the Wreckage of Air France Flight AF 447", "comments": "Published in at http://dx.doi.org/10.1214/13-STS420 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 69-80", "doi": "10.1214/13-STS420", "report-no": "IMS-STS-STS420", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the early morning hours of June 1, 2009, during a flight from Rio de\nJaneiro to Paris, Air France Flight AF 447 disappeared during stormy weather\nover a remote part of the Atlantic carrying 228 passengers and crew to their\ndeaths. After two years of unsuccessful search, the authors were asked by the\nFrench Bureau d'Enqu\\^{e}tes et d'Analyses pour la s\\'{e}curit\\'{e} de\nl'aviation to develop a probability distribution for the location of the\nwreckage that accounted for all information about the crash location as well as\nfor previous search efforts. We used a Bayesian procedure developed for search\nplanning to produce the posterior target location distribution. This\ndistribution was used to guide the search in the third year, and the wreckage\nwas found with one week of undersea search. In this paper we discuss why\nBayesian analysis is ideally suited to solving this problem, review previous\nnon-Bayesian efforts, and describe the methodology used to produce the\nposterior probability distribution for the location of the wreck.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 13:44:53 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Stone", "Lawrence D.", ""], ["Keller", "Colleen M.", ""], ["Kratzke", "Thomas M.", ""], ["Strumpfer", "Johan P.", ""]]}, {"id": "1405.4801", "submitter": "Guido Consonni", "authors": "Guido Consonni and Roberta Paroli", "title": "Objective Bayesian Comparison of Constrained Analysis of Variance Models", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the social sciences we are often interested in comparing models specified\nby parametric equality or inequality constraints. For instance, when examining\nthree group means $\\{ \\mu_1, \\mu_2, \\mu_3\\}$ through an analysis of variance\n(ANOVA), a model may specify that $\\mu_1<\\mu_2<\\mu_3$, while another one may\nstate that $\\{ \\mu_1=\\mu_3\\} <\\mu_2$, and finally a third model may instead\nsuggest that all means are unrestricted. This is a challenging problem, because\nit involves a combination of non-nested models, as well as nested models having\nthe same dimension. We adopt an objective Bayesian approach, and derive the\nposterior probability of each model under consideration. Our method is based on\nthe intrinsic prior methodology, with suitably modifications to accommodate\nequality and inequality constraints. Focussing on normal ANOVA models, a\ncomparative assessment is carried out through simulation studies, showing that\ncorrect model identification is possible even in situations where frequentist\npower is low. We also present an application to real data collected in a\npsychological experiment.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 16:48:07 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Consonni", "Guido", ""], ["Paroli", "Roberta", ""]]}, {"id": "1405.4895", "submitter": "Colin Stoneking", "authors": "Colin J. Stoneking", "title": "Bayesian inference of Gaussian mixture models with noninformative priors", "comments": "26 pages 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with Bayesian inference of a mixture of Gaussian\ndistributions. A novel formulation of the mixture model is introduced, which\nincludes the prior constraint that each Gaussian component is always assigned a\nminimal number of data points. This enables noninformative improper priors such\nas the Jeffreys prior to be placed on the component parameters. We demonstrate\ndifficulties involved in specifying a prior for the standard Gaussian mixture\nmodel, and show how the new model can be used to overcome these. MCMC methods\nare given for efficient sampling from the posterior of this model.\n", "versions": [{"version": "v1", "created": "Mon, 19 May 2014 21:03:15 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Stoneking", "Colin J.", ""]]}, {"id": "1405.4955", "submitter": "Sourabh Bhattacharya", "authors": "Moumita Das and Sourabh Bhattacharya", "title": "Nonstationary, Nonparametric, Nonseparable Bayesian Spatio-Temporal\n  Modeling Using Kernel Convolution of Order Based Dependent Dirichlet Process", "comments": "A significantly updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, using kernel convolution of order based dependent Dirichlet\nprocess (Griffin and Steel (2006)) we construct a nonstationary, nonseparable,\nnonparametric space-time process, which, as we show, satisfies desirable\nproperties, and includes the stationary, separable, parametric processes as\nspecial cases. We also investigate the smoothness properties of our proposed\nmodel. Since our model entails an infinite random series, for Bayesian model\nfitting purpose we must either truncate the series or more appropriately\nconsider a random number of summands, which renders the model dimension a\nrandom variable. We attack the variable dimensionality problem using\nTransdimensional Transformation based Markov Chain Monte Carlo introduced by\nDas and Bhattacharya (2019b), which can update all the variables and also\nchange dimensions in a single block using essentially a single random variable\ndrawn from some arbitrary density defined on a relevant support. For the sake\nof completeness we also address the problem of truncating the infinite series\nby providing a uniform bound on the error incurred by truncating the infinite\nseries.\n  We illustrate the effectiveness of our model and methodologies on a simulated\ndata set and demonstrate that our approach significantly outperforms that of\nFuentes and Reich (2013) which is based on principles somewhat similar to ours.\nWe also fit two real, spatial and spatio-temporal datasets with our approach\nand obtain quite encouraging results in both the cases.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 05:21:12 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 11:00:32 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Das", "Moumita", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1405.4976", "submitter": "Ian Vernon", "authors": "Ian Vernon, Michael Goldstein, Richard Bower", "title": "Galaxy Formation: Bayesian History Matching for the Observable Universe", "comments": "Published in at http://dx.doi.org/10.1214/12-STS412 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 81-90", "doi": "10.1214/12-STS412", "report-no": "IMS-STS-STS412", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosmologists at the Institute of Computational Cosmology, Durham University,\nhave developed a state of the art model of galaxy formation known as Galform,\nintended to contribute to our understanding of the formation, growth and\nsubsequent evolution of galaxies in the presence of dark matter. Galform\nrequires the specification of many input parameters and takes a significant\ntime to complete one simulation, making comparison between the model's output\nand real observations of the Universe extremely challenging. This paper\nconcerns the analysis of this problem using Bayesian emulation within an\niterative history matching strategy, and represents the most detailed\nuncertainty analysis of a galaxy formation simulation yet performed.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:26:11 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Vernon", "Ian", ""], ["Goldstein", "Michael", ""], ["Bower", "Richard", ""]]}, {"id": "1405.4977", "submitter": "Peter B\\\"{u}hlmann", "authors": "Peter B\\\"uhlmann", "title": "Discussion of Big Bayes Stories and BayesBag", "comments": "Published in at http://dx.doi.org/10.1214/13-STS460 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 91-94", "doi": "10.1214/13-STS460", "report-no": "IMS-STS-STS460", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I congratulate all the authors for their insightful papers with wide-ranging\ncontributions. The articles demonstrate the power and elegance of the Bayesian\ninference paradigm. In particular, it allows to incorporate prior knowledge as\nwell as hierarchical model building in a convincing way. Regarding the latter,\nthe contribution by Raftery, Alkema and German is a very fascinating piece, as\nit addresses a set of problems of great public interest and presents\npredictions for the world populations and other interesting quantities with\nuncertainty regions. Their approach is based on a hierarchical model, taking\nvarious characteristics into account (e.g., fertility projections). It would\nhave been very difficult to come up with a \"better\" solution which would be as\nclear in terms of interpretation (in contrast to a \"black-box machine\") and\nwhich would provide (model-based) uncertainties for the predictions into the\nfuture.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 07:36:19 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["B\u00fchlmann", "Peter", ""]]}, {"id": "1405.4982", "submitter": "Mark A. Girolami", "authors": "Mark A. Girolami", "title": "Contribution by M. A. Girolami", "comments": "Published in at http://dx.doi.org/10.1214/13-STS459 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 97-97", "doi": "10.1214/13-STS459", "report-no": "IMS-STS-STS459", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This collection of Big Bayes Stories could be partitioned into two groups,\none relating to the sciences, cosmology in particular, and the other relating\nto public policy, that is, health, fisheries management and demographics.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 08:04:28 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Girolami", "Mark A.", ""]]}, {"id": "1405.4986", "submitter": "David J. Hand", "authors": "David J. Hand", "title": "Wonderful Examples, but Let's not Close Our Eyes", "comments": "Published in at http://dx.doi.org/10.1214/13-STS446 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 98-100", "doi": "10.1214/13-STS446", "report-no": "IMS-STS-STS446", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The papers in this collection are superb illustrations of the power of modern\nBayesian methods. They give examples of problems which are well suited to being\ntackled using such methods, but one must not lose sight of the merits of having\nmultiple different strategies and tools in one's inferential armoury.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 08:15:31 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Hand", "David J.", ""]]}, {"id": "1405.4991", "submitter": "A. H. Welsh", "authors": "A. H. Welsh", "title": "Discussion of \"Search for the Wreckage of Air France Flight AF 447\"", "comments": "Published in at http://dx.doi.org/10.1214/13-STS447 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 101-102", "doi": "10.1214/13-STS447", "report-no": "IMS-STS-STS447", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Search for the Wreckage of Air France Flight AF 447\" by\nLawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, Johan P. Strumpfer\n[arXiv:1405.4720].\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 08:29:40 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Welsh", "A. H.", ""]]}, {"id": "1405.4995", "submitter": "Kenneth F. Wallis", "authors": "Kenneth F. Wallis", "title": "The Two-Piece Normal, Binormal, or Double Gaussian Distribution: Its\n  Origin and Rediscoveries", "comments": "Published in at http://dx.doi.org/10.1214/13-STS417 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 106-112", "doi": "10.1214/13-STS417", "report-no": "IMS-STS-STS417", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper traces the history of the two-piece normal distribution from its\norigin in the posthumous Kollektivmasslehre (1897) of Gustav Theodor Fechner to\nits rediscoveries and generalisations. The denial of Fechner's originality by\nKarl Pearson, reiterated a century later by Oscar Sheynin, is shown to be\nwithout foundation.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 09:04:01 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Wallis", "Kenneth F.", ""]]}, {"id": "1405.5020", "submitter": "Liang Peng", "authors": "Liang Peng, Yongcheng Qi, Fang Wang", "title": "Test for a Mean Vector with Fixed or Divergent Dimension", "comments": "Published in at http://dx.doi.org/10.1214/13-STS425 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 113-127", "doi": "10.1214/13-STS425", "report-no": "IMS-STS-STS425", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a long history in testing whether a mean vector with a fixed\ndimension has a specified value. Some well-known tests include the Hotelling\n$T^2$-test and the empirical likelihood ratio test proposed by Owen [Biometrika\n75 (1988) 237-249; Ann. Statist. 18 (1990) 90-120]. Recently, Hotelling\n$T^2$-test has been modified to work for a high-dimensional mean, and the\nempirical likelihood method for a mean has been shown to be valid when the\ndimension of the mean vector goes to infinity. However, the asymptotic\ndistributions of these tests depend on whether the dimension of the mean vector\nis fixed or goes to infinity. In this paper, we propose to split the sample\ninto two parts and then to apply the empirical likelihood method to two\nequations instead of d equations, where d is the dimension of the underlying\nrandom vector. The asymptotic distribution of the new test is independent of\nthe dimension of the mean vector. A simulation study shows that the new test\nhas a very stable size with respect to the dimension of the mean vector, and is\nmuch more powerful than the modified Hotelling $T^2$-test.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 10:17:41 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Peng", "Liang", ""], ["Qi", "Yongcheng", ""], ["Wang", "Fang", ""]]}, {"id": "1405.5040", "submitter": "Marco Riani", "authors": "Marco Riani, Anthony C. Atkinson, Domenico Perrotta", "title": "A Parametric Framework for the Comparison of Methods of Very Robust\n  Regression", "comments": "Published in at http://dx.doi.org/10.1214/13-STS437 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 128-143", "doi": "10.1214/13-STS437", "report-no": "IMS-STS-STS437", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several methods for obtaining very robust estimates of regression\nparameters that asymptotically resist 50% of outliers in the data. Differences\nin the behaviour of these algorithms depend on the distance between the\nregression data and the outliers. We introduce a parameter $\\lambda$ that\ndefines a parametric path in the space of models and enables us to study, in a\nsystematic way, the properties of estimators as the groups of data move from\nbeing far apart to close together. We examine, as a function of $\\lambda$, the\nvariance and squared bias of five estimators and we also consider their power\nwhen used in the detection of outliers. This systematic approach provides tools\nfor gaining knowledge and better understanding of the properties of robust\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:39:14 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Riani", "Marco", ""], ["Atkinson", "Anthony C.", ""], ["Perrotta", "Domenico", ""]]}, {"id": "1405.5051", "submitter": "Anthony C. Atkinson", "authors": "Anthony C. Atkinson", "title": "Selecting a Biased-Coin Design", "comments": "Published in at http://dx.doi.org/10.1214/13-STS449 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 144-163", "doi": "10.1214/13-STS449", "report-no": "IMS-STS-STS449", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biased-coin designs are used in clinical trials to allocate treatments with\nsome randomness while maintaining approximately equal allocation. More recent\nrules are compared with Efron's [Biometrika 58 (1971) 403-417] biased-coin rule\nand extended to allow balance over covariates. The main properties are loss of\ninformation, due to imbalance, and selection bias. Theoretical results, mostly\nlarge sample, are assembled and assessed by small-sample simulations. The\nproperties of the rules fall into three clear categories. A Bayesian rule is\nshown to have appealing properties; at the cost of slight imbalance, bias is\nvirtually eliminated for large samples.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 12:11:27 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Atkinson", "Anthony C.", ""]]}, {"id": "1405.5055", "submitter": "Raymond J. Carroll", "authors": "Raymond J. Carroll", "title": "Reply to the Discussion of \"Estimating the Distribution of Dietary\n  Consumption Patterns\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS466 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 103-103", "doi": "10.1214/14-STS466", "report-no": "IMS-STS-STS466", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reply to the \"Discussion of \"Estimating the Distribution of Dietary\nConsumption Patterns\" by Raymond J. Carroll [arXiv:1405.4667]\" by Stephen E.\nFienberg and Rebecca C. Steorts [arXiv:1403.0566].\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 12:25:54 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Carroll", "Raymond J.", ""]]}, {"id": "1405.5228", "submitter": "Simone Padoan PhD", "authors": "G. Marcon, S. A. Padoan, P. Naveau, P. Muliere and J. Segers", "title": "Multivariate Nonparametric Estimation of the Pickands Dependence\n  Function using Bernstein Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in risk analysis, especially in environmental sciences,\nrequire the estimation of the dependence among multivariate maxima. A way to do\nthis is by inferring the Pickands dependence function of the underlying\nextreme-value copula. A nonparametric estimator is constructed as the sample\nequivalent of a multivariate extension of the madogram. Shape constraints on\nthe family of Pickands dependence functions are taken into account by means of\na representation in terms of a specific type of Bernstein polynomials. The\nlarge-sample theory of the estimator is developed and its finite-sample\nperformance is evaluated with a simulation study. The approach is illustrated\nby analyzing clusters consisting of seven weather stations that have recorded\nweekly maxima of hourly rainfall in France from 1993 to 2011.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 20:11:17 GMT"}, {"version": "v2", "created": "Tue, 26 Aug 2014 11:49:41 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2016 14:17:54 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Marcon", "G.", ""], ["Padoan", "S. A.", ""], ["Naveau", "P.", ""], ["Muliere", "P.", ""], ["Segers", "J.", ""]]}, {"id": "1405.5239", "submitter": "Rui Song", "authors": "Ailin Fan, Wenbin Lu and Rui Song", "title": "Sequential Advantage Selection for Optimal Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for optimal treatment regime in a clinical trial or an\nobservational study is getting more attention. Most existing variable selection\ntechniques focused on selecting variables that are important for prediction,\ntherefore some variables that are poor in prediction but are critical for\ndecision-making may be ignored. A qualitative interaction of a variable with\ntreatment arises when treatment effect changes direction as the value of this\nvariable varies. The qualitative interaction indicates the importance of this\nvariable for decision-making. Gunter et al. (2011) proposed S-score which\ncharacterizes the magnitude of qualitative interaction of each variable with\ntreatment individually. In this article, we developed a sequential advantage\nselection method based on the modified S-score. Our method selects\nqualitatively interacted variables sequentially, and hence excludes marginally\nimportant but jointly unimportant variables {or vice versa}. The optimal\ntreatment regime based on variables selected via joint model is more\ncomprehensive and reliable. With the proposed stopping criteria, our method can\nhandle a large amount of covariates even if sample size is small. Simulation\nresults show our method performs well in practical settings. We further applied\nour method to data from a clinical trial for depression.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 20:36:40 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Fan", "Ailin", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "1405.5297", "submitter": "Curtis Storlie", "authors": "Curtis B. Storlie, William A. Lane, Emily M. Ryan, James R. Gattiker,\n  David M. Higdon", "title": "Calibration of Computational Models with Categorical Parameters and\n  Correlated Outputs via Bayesian Smoothing Spline ANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become commonplace to use complex computer models to predict outcomes\nin regions where data does not exist. Typically these models need to be\ncalibrated and validated using some experimental data, which often consists of\nmultiple correlated outcomes. In addition, some of the model parameters may be\ncategorical in nature, such as a pointer variable to alternate models (or\nsubmodels) for some of the physics of the system. Here we present a general\napproach for calibration in such situations where an emulator of the\ncomputationally demanding models and a discrepancy term from the model to\nreality are represented within a Bayesian Smoothing Spline (BSS) ANOVA\nframework. The BSS-ANOVA framework has several advantages over the traditional\nGaussian Process, including ease of handling categorical inputs and correlated\noutputs, and improved computational efficiency. Finally this framework is then\napplied to the problem that motivated its design; a calibration of a\ncomputational fluid dynamics model of a bubbling fluidized which is used as an\nabsorber in a CO2 capture system.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 04:43:58 GMT"}, {"version": "v2", "created": "Tue, 17 Jun 2014 21:36:20 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Storlie", "Curtis B.", ""], ["Lane", "William A.", ""], ["Ryan", "Emily M.", ""], ["Gattiker", "James R.", ""], ["Higdon", "David M.", ""]]}, {"id": "1405.5311", "submitter": "Atanu Ghosh KUMAR", "authors": "Atanu Kumar Ghosh, Arnab Chakraborty", "title": "Compressive Sampling Using EM Algorithm", "comments": "9 pages, 4 figures. This paper has been published as a technical\n  report in Applied Statistics Unit in Indian Statistical Institute, Kolkata", "journal-ref": null, "doi": null, "report-no": "Technical Report No: ASU/2014/4", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches of sampling signals follow the celebrated theorem of\nNyquist and Shannon. Compressive sampling, introduced by Donoho, Romberg and\nTao, is a new paradigm that goes against the conventional methods in data\nacquisition and provides a way of recovering signals using fewer samples than\nthe traditional methods use. Here we suggest an alternative way of\nreconstructing the original signals in compressive sampling using EM algorithm.\nWe first propose a naive approach which has certain computational difficulties\nand subsequently modify it to a new approach which performs better than the\nconventional methods of compressive sampling. The comparison of the different\napproaches and the performance of the new approach has been studied using\nsimulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 06:53:16 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Ghosh", "Atanu Kumar", ""], ["Chakraborty", "Arnab", ""]]}, {"id": "1405.5575", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Oumar Thiam and Mohamed Cheikh Haidara", "title": "High moments Jarque-Bera tests for arbitrary distribution functions", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jarque-Bera's fitting test for normality is a celebrated and powerful\none. In this paper, we consider general Jarque-Bera tests for any distribution\nfunction df having at least 4k finite moments for k greater than 2. The tests\nuse as many moments as possible whereas the JB classical test is supposed to\ntest only skewness and kurtosis for normal variates. But our results unveil the\nrelations between the coeffients in the JB classical test and the moments,\nshowing that it really depends on the first eight moments. This is a new\nexplanation for the powerfulness of such tests. General Chi-square tests for an\narbitraty model, not only normal, are also derived. We make use of the modern\nfunctional empirical processes approach that makes it easier to handle\nstatistics based on the high moments and allows the generalization of the JB\ntest both in the number of involved moments and in the underlying distribution.\nSimulation studies are provided and comparison cases with the\nKolmogorov-Smirnov's tests and the classical JB test are given.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 23:50:11 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 17:18:41 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Lo", "Gane Samb", ""], ["Thiam", "Oumar", ""], ["Haidara", "Mohamed Cheikh", ""]]}, {"id": "1405.5577", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "A simple note on some empirical stochastic process as a tool in uniform\n  L-statistics weak laws", "comments": "11 page", "journal-ref": "Afrika Statistika 2010, 5, pp. 245-251", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the stochastic process \\begin{equation}\n\\beta_{n}(q_{t},t)=\\beta_{n}(t)=\\frac{1}{\\sqrt{n}}\\sum_{j=1}^{n}\\left\\{G_{t,n}(Y(t))-G_{t}(Y_{j}(t))\\right\\}\nq_{t}(Y_{j}(t)), \\tag{A} \\end{equation} where for $n\\geq1$ and $T>0$, the\nsequences $\\{Y_{1}(t),Y_{2}(t),...,Y_{n}(t),t\\in [0,T]\\}$ are independant\nobservations of some real stochastic process ${Y(t),t\\in [0,T]}$, for each $t\n\\in [0,T]$, $G_{t}$ is the distribution function of $% Y(t)$ and $G_{t,n}$ is\nthe empirical distribution function based on $%\nY_{1}(t),Y_{2}(t),...,Y_{n}(t)$, and finally $q_{t}$ is a bounded real fonction\ndefined on $\\mathbb{R}$. This process appears when investigating some\ntime-dependent L-Statistics which are expressed as a function of some\nfunctional empirical process and the process (A). Since the functional\nempirical process is widely investigated in the literature, the process reveals\nitself as an important key for L-Statistics laws. In this paper, we state an\nextended study of this process, give complete calculations of the first\nmoments, the covariance function and find conditions for asymptotic tightness.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 00:04:40 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1405.5578", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo and Serigne Touba Sall", "title": "Asymptotic Representation Theorems for Poverty Indices", "comments": "12 pages", "journal-ref": "Afrika statistika 2010, 5, pp. 238-244", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We set general conditions under which the general poverty index, which\nsummarizes all the available indices, is asymptotically represented with some\nempirical processes. This representation theorem offers a general key, in most\ndirections, for the asymptotics of the bulk of poverty indices and issues in\npoverty analysis. Our representation results uniformly hold on a large\ncollection of poverty indices. They enable the continuous measure of poverty\nwith longitudinal data.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 00:10:09 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Lo", "Gane Samb", ""], ["Sall", "Serigne Touba", ""]]}, {"id": "1405.5786", "submitter": "Nirian Mart\\'in", "authors": "Narayanaswamy Balakrishnan, Nirian Mart\\'in and Leandro Pardo", "title": "Empirical phi-divergence test statistics for testing simple and\n  composite null hypotheses", "comments": "Long version of the forthcoming version to appear in Statistics: A\n  Journal of Theoretical and Applied Statistics, which is estimated to be\n  published in 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to introduce first a new family of\nempirical test statistics for testing a simple null hypothesis when the vector\nof parameters of interest are defined through a specific set of unbiased\nestimating functions. This family of test statistics is based on a distance\nbetween two probability vectors, with the first probability vector obtained by\nmaximizing the empirical likelihood on the vector of parameters, and the second\nvector defined from the fixed vector of parameters under the simple null\nhypothesis. The distance considered for this purpose is the phi-divergence\nmeasure. The asymptotic distribution is then derived for this family of test\nstatistics. The proposed methodology is illustrated through the well-known data\nof Newcomb's measurements on the passage time for light. A simulation study is\ncarried out to compare its performance with that of the empirical likelihood\nratio test when confidence intervals are constructed based on the respective\nstatistics for small sample sizes. The results suggest that the \"empirical\nmodified likelihood ratio test statistic\" provides a competitive alternative to\nthe empirical likelihood ratio test statistic, and is also more robust than the\nempirical likelihood ratio test statistic in the presence of contamination in\nthe data. Finally, we propose empirical phi-divergence test statistics for\ntesting a composite null hypothesis and present some asymptotic as well as\nsimulation results for evaluating the performance of these test procedures.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 15:08:18 GMT"}, {"version": "v2", "created": "Sun, 17 Aug 2014 11:14:32 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Balakrishnan", "Narayanaswamy", ""], ["Mart\u00edn", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1405.5802", "submitter": "Jean Peyhardi", "authors": "Jean Peyhardi, Catherine Trottier and Yann Gu\\'edon", "title": "Partitioned conditional generalized linear models for categorical data", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In categorical data analysis, several regression models have been proposed\nfor hierarchically-structured response variables, e.g. the nested logit model.\nBut they have been formally defined for only two or three levels in the\nhierarchy. Here, we introduce the class of partitioned conditional generalized\nlinear models (PCGLMs) defined for any numbers of levels. The hierarchical\nstructure of these models is fully specified by a partition tree of categories.\nUsing the genericity of the (r,F,Z) specification, the PCGLM can handle\nnominal, ordinal but also partially-ordered response variables.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 15:45:33 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Peyhardi", "Jean", ""], ["Trottier", "Catherine", ""], ["Gu\u00e9don", "Yann", ""]]}, {"id": "1405.5978", "submitter": "Ale\\v{s} \\v{Z}iberna", "authors": "Ale\\v{s} \\v{Z}iberna", "title": "Blockmodeling of multilevel networks", "comments": "33 pages, 10 figures, 6 tables", "journal-ref": "Social networks, vol. 39, no. 1, pages 46-61, October 2014", "doi": "10.1016/j.socnet.2014.04.002", "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents several approaches to the blockmodeling of multilevel\nnetwork data. Multilevel network data consist of networks that are measured on\nat least two levels (e.g. between organizations and people) and information on\nties between those levels (e.g. information on which people are members of\nwhich organizations). Several approaches will be considered: a separate\nanalysis of the levels; transforming all networks to one level and\nblockmodeling on this level using information from all levels; and a truly\nmultilevel approach where all levels and ties among them are modeled at the\nsame time. Advantages and disadvantages of these approaches will be discussed.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 07:38:50 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["\u017diberna", "Ale\u0161", ""]]}, {"id": "1405.5985", "submitter": "Lawrence D. Stone", "authors": "Lawrence D. Stone", "title": "Response to Discussion by A. H. Welsh on the AF 447 Paper", "comments": "Published in at http://dx.doi.org/10.1214/13-STS463 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 1, 104-105", "doi": "10.1214/13-STS463", "report-no": "IMS-STS-STS463", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response to \"Discussion of \"Search for the Wreckage of Air France Flight AF\n447\" by by Lawrence D. Stone, Colleen M. Keller, Thomas M. Kratzke, Johan P.\nStrumpfer [arXiv:1405.4720]\" by A. H. Welsh [arXiv:1405.4991].\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 08:12:58 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Stone", "Lawrence D.", ""]]}, {"id": "1405.6057", "submitter": "Eliane  Pinheiro", "authors": "Silvia L. P. Ferrari and Eliane C. Pinheiro", "title": "Small-sample one-sided testing in extreme value regression models", "comments": "20 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive adjusted signed likelihood ratio statistics for a general class of\nextreme value regression models. The adjustments reduce the error in the\nstandard normal approximation to the distribution of the signed likelihood\nratio statistic. We use Monte Carlo simulations to compare the finite-sample\nperformance of the different tests. Our simulations suggest that the signed\nlikelihood ratio test tends to be liberal when the sample size is not large,\nand that the adjustments are effective in shrinking the size distortion. Two\nreal data applications are presented and discussed.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 13:14:00 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Ferrari", "Silvia L. P.", ""], ["Pinheiro", "Eliane C.", ""]]}, {"id": "1405.6070", "submitter": "Carey Priebe", "authors": "Shakira Suwan, Dominic S. Lee, Runze Tang, Daniel L. Sussman, Minh\n  Tang, Carey E. Priebe", "title": "Empirical Bayes Estimation for the Stochastic Blockmodel", "comments": "to appear at Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for the stochastic blockmodel is currently of burgeoning interest\nin the statistical community, as well as in various application domains as\ndiverse as social networks, citation networks, brain connectivity networks\n(connectomics), etc. Recent theoretical developments have shown that spectral\nembedding of graphs yields tractable distributional results; in particular, a\nrandom dot product latent position graph formulation of the stochastic\nblockmodel informs a mixture of normal distributions for the adjacency spectral\nembedding. We employ this new theory to provide an empirical Bayes methodology\nfor estimation of block memberships of vertices in a random graph drawn from\nthe stochastic blockmodel, and demonstrate its practical utility. The posterior\ninference is conducted using a Metropolis-within-Gibbs algorithm. The theory\nand methods are illustrated through Monte Carlo simulation studies, both within\nthe stochastic blockmodel and beyond, and experimental results on a Wikipedia\ndata set are presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 14:08:08 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 13:52:04 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 10:06:00 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Suwan", "Shakira", ""], ["Lee", "Dominic S.", ""], ["Tang", "Runze", ""], ["Sussman", "Daniel L.", ""], ["Tang", "Minh", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1405.6176", "submitter": "Sandipan Roy", "authors": "Sandipan Roy, Yves Atchade and George Michailidis", "title": "Change-Point Estimation in High-Dimensional Markov Random Field Models", "comments": "41 pages, 7 figures", "journal-ref": "Journal of the Royal Statistical Society : Series B, Volume 79,\n  Issue 4, Pages 1187--1206, 2017", "doi": "10.1111/rssb.12205", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a change-point estimation problem in the context of\nhigh-dimensional Markov Random Field models. Change-points represent a key\nfeature in many dynamically evolving network structures. The change-point\nestimate is obtained by maximizing a profile penalized pseudo-likelihood\nfunction under a sparsity assumption. We also derive a tight bound for the\nestimate, up to a logarithmic factor, even in settings where the number of\npossible edges in the network far exceeds the sample size. The performance of\nthe proposed estimator is evaluated on synthetic data sets and is also used to\nexplore voting patterns in the US Senate in the 1979-2012 period.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 18:49:32 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 05:01:59 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Roy", "Sandipan", ""], ["Atchade", "Yves", ""], ["Michailidis", "George", ""]]}, {"id": "1405.6210", "submitter": "Jacob Bien", "authors": "Jacob Bien, Florentina Bunea, Luo Xiao", "title": "Convex Banding of the Covariance Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new sparse estimator of the covariance matrix for\nhigh-dimensional models in which the variables have a known ordering. Our\nestimator, which is the solution to a convex optimization problem, is\nequivalently expressed as an estimator which tapers the sample covariance\nmatrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result of\nthis adaptivity, the convex banding estimator enjoys theoretical optimality\nproperties not attained by previous banding or tapered estimators. In\nparticular, our convex banding estimator is minimax rate adaptive in Frobenius\nand operator norms, up to log factors, over commonly-studied classes of\ncovariance matrices, and over more general classes. Furthermore, it correctly\nrecovers the bandwidth when the true covariance is exactly banded. Our convex\nformulation admits a simple and efficient algorithm. Empirical studies\ndemonstrate its practical effectiveness and illustrate that our exactly-banded\nestimator works well even when the true covariance matrix is only close to a\nbanded matrix, confirming our theoretical results. Our method compares\nfavorably with all existing methods, in terms of accuracy and speed. We\nillustrate the practical merits of the convex banding estimator by showing that\nit can be used to improve the performance of discriminant analysis for\nclassifying sound recordings.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 20:00:52 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Bien", "Jacob", ""], ["Bunea", "Florentina", ""], ["Xiao", "Luo", ""]]}, {"id": "1405.6231", "submitter": "Noureddine El Karoui", "authors": "Noureddine El Karoui and Hau-tieng Wu", "title": "Connection graph Laplacian methods can be made robust to noise", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.SP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several data analytic techniques based on connection graph\nlaplacian (CGL) ideas have appeared in the literature. At this point, the\nproperties of these methods are starting to be understood in the setting where\nthe data is observed without noise. We study the impact of additive noise on\nthese methods, and show that they are remarkably robust. As a by-product of our\nanalysis, we propose modifications of the standard algorithms that increase\ntheir robustness to noise. We illustrate our results in numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 May 2014 21:04:30 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Karoui", "Noureddine El", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1405.6416", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts and M. Delores Ugarte", "title": "Discussion of \"Single and Two-Stage Cross-Sectional and Time Series\n  Benchmarking Procedures for SAE\"", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We congratulate the authors for a stimulating and valuable manuscript,\nproviding a careful review of the state-of the-art in cross-sectional and\ntime-series benchmarking procedures for small area estimation. They develop a\nnovel two-stage benchmarking method for hierarchical time series models, where\nthey evaluate their procedure by estimating monthly total unemployment using\ndata from the U.S. Census Bureau. We discuss three topics: linearity and model\nmisspecification, computational complexity and model comparisons, and, some\naspects on small area estimation in practice. More specifically, we pose the\nfollowing questions to the authors, that they may wish to answer: How robust is\ntheir model to misspecification? Is it time to perhaps move away from linear\nmodels of the type considered by (Battese et al. 1988; Fay and Herriot 1979)?\nWhat is the asymptotic computational complexity and what comparisons can be\nmade to other models? Should the benchmarking constraints be inherently fixed\nor should they be random?\n", "versions": [{"version": "v1", "created": "Sun, 25 May 2014 18:36:29 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Ugarte", "M. Delores", ""]]}, {"id": "1405.6531", "submitter": "Suman Guha", "authors": "Suman Guha and Sourabh Bhattacharya", "title": "Gaussian Random Functional Dynamic Spatio-Temporal Modeling of Discrete\n  Time Spatial Time Series Data", "comments": "A significantly rewritten version of the previous draft. The thorough\n  discussion on identifiability issue of the GRFDSTM and the section on the\n  application of GRFDSTM to moderate sized spatio-temporal data is added to the\n  current version. The Article length is 50 pages excluding the reference.\n  Total 43 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete time spatial time series data arise routinely in meteorological and\nenvironmental studies. Inference and prediction associated with them are mostly\ncarried out using any of the several variants of the linear state space model\nthat are collectively called linear dynamic spatio-temporal models (LDSTMs).\nHowever, real world environmental processes are highly complex and are seldom\nrepresentable by models with such simple linear structure. Hence, nonlinear\ndynamic spatio-temporal models (NLDSTMs) based on the idea of nonlinear\nobservational and evolutionary equation have been proposed as an alternative.\nHowever, in that case, the caveat lies in selecting the specific form of\nnonlinearity from a large class of potentially appropriate nonlinear functions.\nMoreover, modeling by NLDSTMs requires precise knowledge about the dynamics\nunderlying the data. In this article, we address this problem by introducing\nthe Gaussian random functional dynamic spatio-temporal model (GRFDSTM). Unlike\nthe LDSTMs or NLDSTMs, in GRFDSTM both the functions governing the\nobservational and evolutionary equations are composed of Gaussian random\nfunctions. We exhibit many interesting theoretical properties of the GRFDSTM\nand demonstrate how model fitting and prediction can be carried out coherently\nin a Bayesian framework. We also conduct an extensive simulation study and\napply our model to a real, SO2 pollution data over Europe. The results are\nhighly encouraging.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 10:27:54 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 03:27:49 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 13:49:38 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Guha", "Suman", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1405.6628", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Antonio Lijoi, Bernardo Nipoti", "title": "Full Bayesian inference with hazard mixture models", "comments": "27 pages, 5 figures", "journal-ref": "Computational Statistics & Data Analysis, 93 (2016) 359-372", "doi": "10.1016/j.csda.2014.12.003", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric inferential procedures based on Markov chain Monte\nCarlo marginal methods typically yield point estimates in the form of posterior\nexpectations. Though very useful and easy to implement in a variety of\nstatistical problems, these methods may suffer from some limitations if used to\nestimate non-linear functionals of the posterior distribution. The main goal of\nthe present paper is to develop a novel methodology that extends a\nwell-established marginal procedure designed for hazard mixture models, in\norder to draw approximate inference on survival functions that is not limited\nto the posterior mean but includes, as remarkable examples, credible intervals\nand median survival time. Our approach relies on a characterization of the\nposterior moments that, in turn, is used to approximate the posterior\ndistribution by means of a technique based on Jacobi polynomials. The\ninferential performance of our methodology is analyzed by means of an extensive\nstudy of simulated data and real data consisting of leukemia remission times.\nAlthough tailored to the survival analysis context, the procedure we introduce\ncan be adapted to a range of other models for which moments of the posterior\ndistribution can be estimated.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 15:58:44 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 11:19:47 GMT"}, {"version": "v3", "created": "Tue, 7 Oct 2014 19:59:24 GMT"}, {"version": "v4", "created": "Thu, 9 Oct 2014 05:27:39 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Arbel", "Julyan", ""], ["Lijoi", "Antonio", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1405.6751", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "The weak limiting behavior of the de Haan-Resnick estimator of the\n  exponent of a stable distribution", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating the exponent of a stable law received a\nconsiderable attention in the recent literature. Here, we deal with an estimate\nof such a exponent introduced by De Haan and Resnick when the corresponding\ndistribution function belongs to the Gumbel's domain of attraction. This study\npermits to construct new statistical tests. Examples and simulations are given.\nThe limiting law are shown to be the Gumbel's law and particular cases are\ngiven with norming constants expressed with iterated logarithms and\nexponentials.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 22:08:58 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1405.6798", "submitter": "Jinchi Lv", "authors": "Jinchi Lv, Zemin Zheng", "title": "Discussion: \"A significance test for the lasso\"", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1175D the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 493-500", "doi": "10.1214/13-AOS1175D", "report-no": "IMS-AOS-AOS1175D", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A significance test for the lasso\" by Richard Lockhart,\nJonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:01:00 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Lv", "Jinchi", ""], ["Zheng", "Zemin", ""]]}, {"id": "1405.6800", "submitter": "Larry Wasserman", "authors": "Larry Wasserman", "title": "Discussion: \"A significance test for the lasso\"", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1175E the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 501-508", "doi": "10.1214/13-AOS1175E", "report-no": "IMS-AOS-AOS1175E", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A significance test for the lasso\" by Richard Lockhart,\nJonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:09:27 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Wasserman", "Larry", ""]]}, {"id": "1405.6803", "submitter": "A. Buja", "authors": "A. Buja, L. Brown", "title": "Discussion: \"A significance test for the lasso\"", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1175F the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 509-517", "doi": "10.1214/14-AOS1175F", "report-no": "IMS-AOS-AOS1175F", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"A significance test for the lasso\" by Richard Lockhart,\nJonathan Taylor, Ryan J. Tibshirani, Robert Tibshirani [arXiv:1301.7161].\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 06:20:58 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Buja", "A.", ""], ["Brown", "L.", ""]]}, {"id": "1405.6884", "submitter": "Nickos Papadatos D", "authors": "Nickos Papadatos", "title": "Maximizing the expected range from dependent observations under\n  mean-variance information", "comments": "34 pages", "journal-ref": "Statistics (2016), vol. 50(3), pp. 596-629", "doi": "10.1080/02331888.2015.1074234", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we derive the best possible upper bound for\n$E[\\max{X_i}-\\min_i{X_i}]$ under given means and variances on $n$ random\nvariables $X_i$. The random vector $(X_1,...,X_n)$ is allowed to have any\ndependence structure, provided $E X_i=\\mu_i$ and $Var X_i=\\sigma_i^2$,\n$0<\\sigma_i<\\infty$. We provide an explicit characterization of the $n$-variate\ndistributions that attain the equality (extremal random vectors), and the tight\nbound is compared to other existing results.\n  Key words and phrases: Range; Dependent Observations; Tight Expectation\nBounds; Extremal Random Vectors; Probability Matrices; Characterizations.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 12:27:17 GMT"}, {"version": "v2", "created": "Wed, 11 Jun 2014 18:29:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Papadatos", "Nickos", ""]]}, {"id": "1405.6900", "submitter": "C\\'ecile Chauvel", "authors": "C\\'ecile Chauvel and John O'Quigley", "title": "Survival model construction guided by fit and predictive strength", "comments": "30 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We describe a unified framework within which we can build survival models.\nThe motivation for this work comes from a study on the prediction of relapse\namong breast cancer patients treated at the Curie Institute in Paris, France.\nOur focus is on how to best code, or characterize, the effects of the\nvariables, either alone or in combination with others. We consider simple\ngraphical techniques that not only provide an immediate indication as to the\ngoodness of fit but, in cases of departure from model assumptions, point in the\ndirection of a more involved alternative model. These techniques help support\nour intuition. This intuition is backed up by formal theorems that underlie the\nprocess of building richer models from simpler ones. Goodness-of-fit techniques\nare used alongside measures of predictive strength and, again, formal theorems\nshow that these measures can be used to help identify models closest to the\nunknown non-proportional hazards mechanism that we can suppose generates the\nobservations. We consider many examples and show how these tools can be of help\nin guiding the practical problem of efficient model construction for survival\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 13:16:30 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Chauvel", "C\u00e9cile", ""], ["O'Quigley", "John", ""]]}, {"id": "1405.6947", "submitter": "\\'Oli Geirsson", "authors": "\\'Oli P\\'all Geirsson, Birgir Hrafnkelsson and Daniel Simpson", "title": "Computationally efficient spatial modeling of annual maximum 24 hour\n  precipitation. An application to data from Iceland", "comments": "32 pages, 16 figures, submitted to Environmetrics", "journal-ref": null, "doi": "10.1002/env.2343", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient statistical method to obtain\ndistributional properties of annual maximum 24 hour precipitation on a 1 km by\n1 km regular grid over Iceland. A latent Gaussian model is built which takes\ninto account observations, spatial variations and outputs from a local\nmeteorological model. A covariate based on the meteorological model is\nconstructed at each observational site and each grid point in order to\nassimilate available scientific knowledge about precipitation into the\nstatistical model. The model is applied to two data sets on extreme\nprecipitation, one uncorrected data set and one data set that is corrected for\nphase and wind. The observations are assumed to follow the generalized extreme\nvalue distribution. At the latent level, we implement SPDE spatial models for\nboth the location and scale parameters of the likelihood. An efficient MCMC\nsampler which exploits the model structure is constructed, which yields fast\ncontinuous spatial predictions for spatially varying model parameters and\nquantiles.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 15:18:01 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Geirsson", "\u00d3li P\u00e1ll", ""], ["Hrafnkelsson", "Birgir", ""], ["Simpson", "Daniel", ""]]}, {"id": "1405.7091", "submitter": "Jonathan Heydari", "authors": "Jonathan Heydari", "title": "Bayesian hierarchical modelling for inferring genetic interactions in\n  yeast", "comments": "All images rasterized. Contact author for non-rasterized and\n  searchable Figures. 158 pages, PhD thesis, Newcastle University (2014),\n  Institute for Cell & Molecular Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying genetic interactions for a given microorganism such as yeast is\ndifficult. Quantitative Fitness Analysis (QFA) is a high-throughput\nexperimental and computational methodology for quantifying the fitness of\nmicrobial cultures. QFA can be used to compare between fitness observations for\ndifferent genotypes and thereby infer genetic interaction strengths. Current\n\"naive\" frequentist statistical approaches used in QFA do not model\nbetween-genotype variation or difference in genotype variation under different\nconditions. In this thesis, a Bayesian approach is introduced to evaluate\nhierarchical models that better reflect the structure or design of QFA\nexperiments. First, a two-stage approach is presented: a hierarchical logistic\nmodel is fitted to microbial culture growth curves and then a hierarchical\ninteraction model is fitted to fitness summaries inferred for each genotype.\nNext, a one-stage Bayesian approach is presented: a joint hierarchical model\nwhich does not require a univariate summary of fitness, used to pass\ninformation between models. The new hierarchical approaches are then compared\nusing a dataset examining the effect of telomere defects on yeast. By better\ndescribing the experimental structure, new evidence is found for genes and\ncomplexes which interact with the telomere cap. Various extensions of these\nmodels, including models for data transformation, batch effects, and\nintrinsically stochastic growth models are also considered.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 23:49:48 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Heydari", "Jonathan", ""]]}, {"id": "1405.7107", "submitter": "Marianna Pensky", "authors": "Fabienne Comte, Charles-A. Cuenod, Marianna Pensky and Yves Rozenholc", "title": "Laplace deconvolution on the basis of time domain data and its\n  application to Dynamic Contrast Enhanced imaging", "comments": "36 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1207.2231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider the problem of Laplace deconvolution with\nnoisy discrete non-equally spaced observations on a finite time interval. We\npropose a new method for Laplace deconvolution which is based on expansions of\nthe convolution kernel, the unknown function and the observed signal over\nLaguerre functions basis (which acts as a surrogate eigenfunction basis of the\nLaplace convolution operator) using regression setting. The expansion results\nin a small system of linear equations with the matrix of the system being\ntriangular and Toeplitz. Due to this triangular structure, there is a common\nnumber $m$ of terms in the function expansions to control, which is realized\nvia complexity penalty. The advantage of this methodology is that it leads to\nvery fast computations, produces no boundary effects due to extension at zero\nand cut-off at $T$ and provides an estimator with the risk within a logarithmic\nfactor of the oracle risk. We emphasize that, in the present paper, we consider\nthe true observational model with possibly nonequispaced observations which are\navailable on a finite interval of length $T$ which appears in many different\ncontexts, and account for the bias associated with this model (which is not\npresent when $T\\rightarrow\\infty$). The study is motivated by perfusion imaging\nusing a short injection of contrast agent, a procedure which is applied for\nmedical assessment of micro-circulation within tissues such as cancerous\ntumors. Presence of a tuning parameter $a$ allows to choose the most\nadvantageous time units, so that both the kernel and the unknown right hand\nside of the equation are well represented for the deconvolution. The\nmethodology is illustrated by an extensive simulation study and a real data\nexample which confirms that the proposed technique is fast, efficient,\naccurate, usable from a practical point of view and very competitive.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 03:00:06 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 02:03:14 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Comte", "Fabienne", ""], ["Cuenod", "Charles-A.", ""], ["Pensky", "Marianna", ""], ["Rozenholc", "Yves", ""]]}, {"id": "1405.7206", "submitter": "Arnab Hazra", "authors": "Arnab Hazra, Sourabh Bhattacharya, Sabyasachi Bhattacharya, Pabitra\n  Banik", "title": "A Note on the Misuse of the Variance Test in Meteorological Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The erroneous assumption \"for all distributions for which the theoretical\nvariance can be computed independently from parameters estimated by any method\ndifferent from the method of moments\" has been used in the case of fitting the\ngamma distribution to a rainfall data by Mooley (1973) which was followed by\nseveral researchers. We show that the asymptotic distribution of the test\nstatistic is generally not even comparable to any central chi-square\ndistribution. We also describe a method for checking the validity of the\nasymptotic distribution for a class of distributions.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 11:46:50 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Hazra", "Arnab", ""], ["Bhattacharya", "Sourabh", ""], ["Bhattacharya", "Sabyasachi", ""], ["Banik", "Pabitra", ""]]}, {"id": "1405.7395", "submitter": "Erik van Zwet", "authors": "Erik van Zwet", "title": "Partial exchangeability of the prior via shuffling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In inference problems involving a multi-dimensional parameter $\\theta$, it is\noften natural to consider decision rules that have a risk which is invariant\nunder some group $G$ of permutations of $\\theta$. We show that this implies\nthat the Bayes risk of the rule is {\\em as if} the prior distribution of the\nparameter is partially exchangeable with respect to $G$. We provide a\nsymmetrization technique for incorporating partial exchangeability of $\\theta$\ninto a statistical model, without assuming any other prior information. We\nrefer to this technique as {\\em shuffling}. Shuffling can be viewed as an\ninstance of empirical Bayes, where we estimate the (unordered) multiset of\nparameter values $\\{\\theta_1,\\theta_2,\\dots,\\theta_p\\}$ while using a uniform\nprior on $G$ for their ordering. Estimation of the multiset is a missing data\nproblem which can be tackled with a stochastic EM algorithm. We show that in\nthe special case of estimating the mean-value parameter in a regular\nexponential family model, shuffling leads to an estimator that is a weighted\naverage of permuted versions of the usual maximum likelihood estimator. This is\na novel form of shrinkage.\n", "versions": [{"version": "v1", "created": "Wed, 28 May 2014 21:01:52 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 22:11:59 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["van Zwet", "Erik", ""]]}, {"id": "1405.7447", "submitter": "Michel d. S. Mesquita", "authors": "Michel d. S. Mesquita, Bj{\\o}rn {\\AA}dlandsvik, Cindy Bruy\\`ere, Anne\n  D. Sandvik", "title": "Horizontal resolution in a nested-domain WRF simulation: a Bayesian\n  analysis approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast-paced development of state-of-the-art limited area models and faster\ncomputational resources have made it possible to create simulations at\nincreasing horizontal resolution. This has led to a ubiquitous demand for even\nhigher resolutions from users of various disciplines. This study revisits one\nof the simulations used in marine ecosystem projects at the Bjerknes Centre. We\npresent a fresh perspective on the assessment of these data, related more\nspecifically to: a) the value added by increased horizontal resolution; and b)\na new method for comparing sensitivity studies. The assessment is made using a\nBayesian framework for the distribution of mean surface temperature in the\nHardanger fjord region in Norway. Population estimates are calculated based on\nsamples from the joint posterior distribution generated using a Monte Carlo\nprocedure. The Bayesian statistical model is applied to output data from the\nWeather Research and Forecasting (WRF) model at three horizontal resolutions\n(9, 3 and 1 km) and the ERA Interim Reanalysis. The period considered in this\nstudy is from 2007 to 2009, for the months of April, May and June.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 03:04:31 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Mesquita", "Michel d. S.", ""], ["\u00c5dlandsvik", "Bj\u00f8rn", ""], ["Bruy\u00e8re", "Cindy", ""], ["Sandvik", "Anne D.", ""]]}, {"id": "1405.7721", "submitter": "Micha{\\l} Warcho{\\l}", "authors": "Holger Drees, Johan Segers, Micha{\\l} Warcho{\\l}", "title": "Statistics for Tail Processes of Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At high levels, the asymptotic distribution of a stationary, regularly\nvarying Markov chain is conveniently given by its tail process. The latter\ntakes the form of a geometric random walk, the increment distribution depending\non the sign of the process at the current state and on the flow of time, either\nforward or backward. Estimation of the tail process provides a nonparametric\napproach to analyze extreme values. A duality between the distributions of the\nforward and backward increments provides additional information that can be\nexploited in the construction of more efficient estimators. The large-sample\ndistribution of such estimators is derived via empirical process theory for\ncluster functionals. Their finite-sample performance is evaluated via Monte\nCarlo simulations involving copula-based Markov models and solutions to\nstochastic recurrence equations. The estimators are applied to stock price data\nto study the absence or presence of symmetries in the succession of large gains\nand losses.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 20:47:35 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 10:57:22 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Drees", "Holger", ""], ["Segers", "Johan", ""], ["Warcho\u0142", "Micha\u0142", ""]]}, {"id": "1405.7728", "submitter": "Michael S{\\o}rensen", "authors": "Mogens Bladt and Samuel Finch and Michael S{\\o}rensen", "title": "Simulation of multivariate diffusion bridge", "comments": "arXiv admin note: text overlap with arXiv:1403.1762", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose simple methods for multivariate diffusion bridge simulation, which\nplays a fundamental role in simulation-based likelihood and Bayesian inference\nfor stochastic differential equations. By a novel application of classical\ncoupling methods, the new approach generalizes a previously proposed simulation\nmethod for one-dimensional bridges to the multi-variate setting. First a method\nof simulating approximate, but often very accurate, diffusion bridges is\nproposed. These approximate bridges are used as proposal for easily\nimplementable MCMC algorithms that produce exact diffusion bridges. The new\nmethod is much more generally applicable than previous methods. Another\nadvantage is that the new method works well for diffusion bridges in long\nintervals because the computational complexity of the method is linear in the\nlength of the interval. In a simulation study the new method performs well, and\nits usefulness is illustrated by an application to Bayesian estimation for the\nmultivariate hyperbolic diffusion model.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 21:06:02 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Bladt", "Mogens", ""], ["Finch", "Samuel", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1405.7746", "submitter": "Alice Morais", "authors": "Alice L. Morais and Silvia L. P. Ferrari", "title": "A class of regression models for parallel and series systems with a\n  random number of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the Weibull power series (WPS) class of distributions\nand named this new class as extended Weibull power series (EWPS) class of\ndistributions. The EWPS distributions are related to series and parallel\nsystems with a random num- ber of components, whereas the WPS distributions\n(Morais and Barreto-Souza, 2011) are related to series systems only. Unlike the\nWPS distributions, for which the Weibull is a limiting special case, the\nWeibull law is a particular case of the EWPS distributions. We prove that the\ndistributions in this class are identifiable under a simple assumption. We also\nprove stochastic and hazard rate order results and highlight that the shapes of\nthe EWPS distributions are markedly more flexible than the shapes of the WPS\ndistributions. We define a regression model for the EWPS response random\nvariable to model a scale parameter and its quantiles. We present the maximum\nlikelihood estimator and prove its consistency and normal asymptotic\ndistribution. Although the construction of this class was motivated by series\nand parallel systems, the EWPS distributions are suitable for modeling a wide\nrange of positive data sets. To illustrate potential uses of this model, we\napply it to a real data set on the tensile strength of coconut fibers and\npresent a simple device for diagnostic purposes.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 23:26:40 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 00:03:49 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Morais", "Alice L.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1405.7945", "submitter": "Valeria Vitelli", "authors": "Valeria Vitelli, {\\O}ystein S{\\o}rensen, Marta Crispino, Arnoldo\n  Frigessi, Elja Arjas", "title": "Probabilistic preference learning with the Mallows rank model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking and comparing items is crucial for collecting information about\npreferences in many areas, from marketing to politics. The Mallows rank model\nis among the most successful approaches to analyse rank data, but its\ncomputational complexity has limited its use to a particular form based on\nKendall distance. We develop new computationally tractable methods for Bayesian\ninference in Mallows models that work with any right-invariant distance. Our\nmethod performs inference on the consensus ranking of the items, also when\nbased on partial rankings, such as top-k items or pairwise comparisons. We\nprove that items that none of the assessors has ranked do not influence the\nmaximum a posteriori consensus ranking, and can therefore be ignored. When\nassessors are many or heterogeneous, we propose a mixture model for clustering\nthem in homogeneous subgroups, with cluster-specific consensus rankings. We\ndevelop approximate stochastic algorithms that allow a fully probabilistic\nanalysis, leading to coherent quantifications of uncertainties. We make\nprobabilistic predictions on the class membership of assessors based on their\nranking of just some items, and predict missing individual preferences, as\nneeded in recommendation systems. We test our approach using several\nexperimental and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 18:25:46 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 10:13:32 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2015 11:55:29 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 13:52:43 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Vitelli", "Valeria", ""], ["S\u00f8rensen", "\u00d8ystein", ""], ["Crispino", "Marta", ""], ["Frigessi", "Arnoldo", ""], ["Arjas", "Elja", ""]]}]