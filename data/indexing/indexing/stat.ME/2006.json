[{"id": "2006.00077", "submitter": "Daniel Lawson", "authors": "Daniel J. Lawson, Vinesh Solanki, Igor Yanovich, Johannes Dellert,\n  Damian Ruck and Phillip Endicott", "title": "CLARITY -- Comparing heterogeneous data using dissimiLARITY", "comments": "R package available from https://github.com/danjlawson/CLARITY . 23\n  pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating datasets from different disciplines is hard because the data are\noften qualitatively different in meaning, scale, and reliability. When two\ndatasets describe the same entities, many scientific questions can be phrased\naround whether the similarities between entities are conserved. Our method,\nCLARITY, quantifies consistency across datasets, identifies where\ninconsistencies arise, and aids in their interpretation. We explore three\ndiverse comparisons: Gene Methylation vs Gene Expression, evolution of language\nsounds vs word use, and country-level economic metrics vs cultural beliefs. The\nnon-parametric approach is robust to noise and differences in scaling, and\nmakes only weak assumptions about how the data were generated. It operates by\ndecomposing similarities into two components: the `structural' component\nanalogous to a clustering, and an underlying `relationship' between those\nstructures. This allows a `structural comparison' between two similarity\nmatrices using their predictability from `structure'. The software, CLARITY, is\navailable as an R package from https://github.com/danjlawson/CLARITY.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:56:32 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lawson", "Daniel J.", ""], ["Solanki", "Vinesh", ""], ["Yanovich", "Igor", ""], ["Dellert", "Johannes", ""], ["Ruck", "Damian", ""], ["Endicott", "Phillip", ""]]}, {"id": "2006.00089", "submitter": "Ramin Nikzad-Langerodi Dr.", "authors": "Ramin Nikzad-Langerodi and Florian Sobieczky", "title": "Graph-based calibration transfer", "comments": "Preprint submitted to Journal of Chemometrics", "journal-ref": null, "doi": "10.1002/cem.3319", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of transferring calibrations from a primary to a secondary\ninstrument, i.e. calibration transfer (CT), has been a matter of considerable\nresearch in chemometrics over the past decades. Current state-of-the-art (SoA)\nmethods like (piecewise) direct standardization perform well when suitable\ntransfer standards are available. However, stable calibration standards that\nshare similar (spectral) features with the calibration samples are not always\navailable. Towards enabling CT with arbitrary calibration standards, we propose\na novel CT technique that employs manifold regularization of the partial least\nsquares (PLS) objective. In particular, our method enforces that calibration\nstandards, measured on primary and secondary instruments, have (nearly)\ninvariant projections in the latent variable space of the primary calibration\nmodel. Thereby, our approach implicitly removes inter-device variation in the\npredictive directions of X which is in contrast to most state-of-the-art\ntechniques that employ explicit pre-processing of the input data. We test our\napproach on the well-known corn benchmark data set employing the NBS glass\nstandard spectra for instrument standardization and compare the results with\ncurrent SoA methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:30:37 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Nikzad-Langerodi", "Ramin", ""], ["Sobieczky", "Florian", ""]]}, {"id": "2006.00105", "submitter": "Guanyu Hu", "authors": "Guanyu Hu, Zhihua Ma, Insu Paek", "title": "A Nonparametric Bayesian Item Response Modeling Approach for Clustering\n  Items and Individuals Simultaneously", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item response theory (IRT) is a popular modeling paradigm for measuring\nsubject latent traits and item properties according to discrete responses in\ntests or questionnaires. There are very limited discussions on heterogeneity\npattern detection for both items and individuals. In this paper, we introduce a\nnonparametric Bayesian approach for clustering items and individuals\nsimultaneously under the Rasch model. Specifically, our proposed method is\nbased on the mixture of finite mixtures (MFM) model. MFM obtains the number of\nclusters and the clustering configurations for both items and individuals\nsimultaneously. The performance of parameters estimation and parameters\nclustering under the MFM Rasch model is evaluated by simulation studies, and a\nreal date set is applied to illustrate the MFM Rasch modeling.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:34:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Guanyu", ""], ["Ma", "Zhihua", ""], ["Paek", "Insu", ""]]}, {"id": "2006.00111", "submitter": "Thiago Oliveira", "authors": "Thiago de Paula Oliveira, Rafael de Andrade Moral", "title": "Global Short-Term Forecasting of Covid-19 Cases", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-021-87230-x", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuously growing number of COVID-19 cases pressures healthcare\nservices worldwide. Accurate short-term forecasting is thus vital to support\ncountry-level policy making. The strategies adopted by countries to combat the\npandemic vary, generating different uncertainty levels about the actual number\nof cases. Accounting for the hierarchical structure of the data and\naccommodating extra-variability is therefore fundamental. We introduce a new\nmodelling framework to describe the course of the pandemic with great accuracy,\nand provide short-term daily forecasts for every country in the world. We show\nthat our model generates highly accurate forecasts up to six days ahead, and\nuse estimated model components to cluster countries based on recent events. We\nintroduce statistical novelty in terms of modelling the autoregressive\nparameter as a function of time, increasing predictive power and flexibility to\nadapt to each country. Our model can also be used to forecast the number of\ndeaths, study the effects of covariates (such as lockdown policies), and\ngenerate forecasts for smaller regions within countries. Consequently, it has\nstrong implications for global planning and decision making. We constantly\nupdate forecasts and make all results freely available to any country in the\nworld through an online Shiny dashboard.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:53:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Oliveira", "Thiago de Paula", ""], ["Moral", "Rafael de Andrade", ""]]}, {"id": "2006.00150", "submitter": "Travis Hee Wai", "authors": "Travis Hee Wai, Michael T. Young, Adam A. Szpiro", "title": "Random Spatial Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce random spatial forests, a method of bagging regression trees\nallowing for spatial correlation. Our main contribution is the development of a\ncomputationally efficient tree building algorithm which selects each split of\nthe tree adjusting for spatial correlation. We evaluate two different\napproaches for estimation of random spatial forests, a pseudo-likelihood\napproach combining random forests with kriging and a non-parametric version for\na general class of spatial smoothers. We show improved prediction accuracy of\nour method compared to existing two-step approaches combining random forests\nand kriging across a range of numerical simulations and demonstrate its\nperformance on elemental carbon, organic carbon, silicon, and sulfur\nmeasurements across the continental United States from 2009-2010.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 02:22:15 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:49:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wai", "Travis Hee", ""], ["Young", "Michael T.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "2006.00160", "submitter": "Ivan Fernandez-Val", "authors": "Paolo Frumento, Matteo Bottai and Iv\\'an Fern\\'andez-Val", "title": "Parametric Modeling of Quantile Regression Coefficient Functions with\n  Longitudinal Data", "comments": "71 pages, 2 figures, includes appendix, R companion package available\n  at https://cran.r-project.org/web/packages/qrcm/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ordinary quantile regression, quantiles of different order are estimated\none at a time. An alternative approach, which is referred to as quantile\nregression coefficients modeling (QRCM), is to model quantile regression\ncoefficients as parametric functions of the order of the quantile. In this\npaper, we describe how the QRCM paradigm can be applied to longitudinal data.\nWe introduce a two-level quantile function, in which two different quantile\nregression models are used to describe the (conditional) distribution of the\nwithin-subject response and that of the individual effects. We propose a novel\ntype of penalized fixed-effects estimator, and discuss its advantages over\nstandard methods based on $\\ell_1$ and $\\ell_2$ penalization. We provide model\nidentifiability conditions, derive asymptotic properties, describe\ngoodness-of-fit measures and model selection criteria, present simulation\nresults, and discuss an application. The proposed method has been implemented\nin the R package qrcm.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:29:10 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Frumento", "Paolo", ""], ["Bottai", "Matteo", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "2006.00261", "submitter": "Hyung Park", "authors": "Hyung Park, Eva Petkova, Thaddeus Tarpey and R. Todd Ogden", "title": "Sufficient Dimension Reduction for Interactions", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction lies at the heart of many statistical methods. In\nregression, dimension reduction has been linked to the notion of sufficiency\nwhereby the relation of the response to a set of predictors is explained by a\nlower dimensional subspace in the predictor space. In this paper, we consider\nthe notion of a dimension reduction in regression on subspaces that are\nsufficient to explain interaction effects between predictors and another\nvariable of interest. The motivation for this work is from precision medicine\nwhere the performance of an individualized treatment rule, given a set of\npretreatment predictors, is determined by interaction effects.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:21:47 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Park", "Hyung", ""], ["Petkova", "Eva", ""], ["Tarpey", "Thaddeus", ""], ["Ogden", "R. Todd", ""]]}, {"id": "2006.00265", "submitter": "Hyung Park", "authors": "Hyung Park, Eva Petkova, Thaddeus Tarpey and R. Todd Ogden", "title": "A constrained sparse additive model for treatment effect-modifier\n  selection", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse additive modeling is a class of effective methods for performing\nhigh-dimensional nonparametric regression. This paper develops a sparse\nadditive model focused on estimation of treatment effect-modification with\nsimultaneous treatment effect-modifier selection. We propose a version of the\nsparse additive model uniquely constrained to estimate the interaction effects\nbetween treatment and pretreatment covariates, while leaving the main effects\nof the pretreatment covariates unspecified. The proposed regression model can\neffectively identify treatment effect-modifiers that exhibit possibly nonlinear\ninteractions with the treatment variable, that are relevant for making optimal\ntreatment decisions. A set of simulation experiments and an application to a\ndataset from a randomized clinical trial are presented to demonstrate the\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:36:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Park", "Hyung", ""], ["Petkova", "Eva", ""], ["Tarpey", "Thaddeus", ""], ["Ogden", "R. Todd", ""]]}, {"id": "2006.00266", "submitter": "Hyung Park", "authors": "Hyung Park, Eva Petkova, Thaddeus Tarpey and R. Todd Ogden", "title": "Functional additive models for optimizing individualized treatment rules", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel functional additive model is proposed which is uniquely modified and\nconstrained to model nonlinear interactions between a treatment indicator and a\npotentially large number of functional and/or scalar pretreatment covariates.\nThe primary motivation for this approach is to optimize individualized\ntreatment rules based on data from a randomized clinical trial. We generalize\nfunctional additive regression models by incorporating treatment-specific\ncomponents into additive effect components. A structural constraint is imposed\non the treatment-specific components in order to provide a class of additive\nmodels with main effects and interaction effects that are orthogonal to each\nother. If primary interest is in the interaction between treatment and the\ncovariates, as is generally the case when optimizing individualized treatment\nrules, we can thereby circumvent the need to estimate the main effects of the\ncovariates, obviating the need to specify their form and thus avoiding the\nissue of model misspecification. The methods are illustrated with data from a\ndepression clinical trial with electroencephalogram functional data as\npatients' pretreatment covariates.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:36:21 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 09:46:01 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Park", "Hyung", ""], ["Petkova", "Eva", ""], ["Tarpey", "Thaddeus", ""], ["Ogden", "R. Todd", ""]]}, {"id": "2006.00267", "submitter": "Hyung Park", "authors": "Hyung Park, Eva Petkova, Thaddeus Tarpey and R. Todd Ogden", "title": "A single-index model with a surface-link for optimizing individualized\n  dose rules", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of modeling and estimating interaction\neffects between covariates and a continuous treatment variable on an outcome,\nusing a single-index regression approach. The primary motivation is to estimate\nan optimal individualized dose rule in an observational study. To model\npossibly nonlinear interaction effects between patients' covariates and a\ncontinuous treatment variable, we employ a two-dimensional penalized spline\nregression on an index-treatment domain, where the index is defined as a linear\nprojection of the covariates. The method is illustrated using two applications\nas well as simulation experiments. A unique contribution of this work is in the\nparsimonious (single-index) parametrization specifically defined for the\ninteraction effect term.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:36:35 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 01:25:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Park", "Hyung", ""], ["Petkova", "Eva", ""], ["Tarpey", "Thaddeus", ""], ["Ogden", "R. Todd", ""]]}, {"id": "2006.00294", "submitter": "Johannes Lederer", "authors": "Mahsa Taheri and Fang Xie and Johannes Lederer", "title": "Statistical Guarantees for Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become standard tools in the analysis of data, but they\nlack comprehensive mathematical theories. For example, there are very few\nstatistical guarantees for learning neural networks from data, especially for\nclasses of estimators that are used in practice or at least similar to such. In\nthis paper, we develop a general statistical guarantee for estimators that\nconsist of a least-squares term and a regularizer. We then exemplify this\nguarantee with $\\ell_1$-regularization, showing that the corresponding\nprediction error increases at most sub-linearly in the number of layers and at\nmost logarithmically in the total number of parameters. Our results establish a\nmathematical basis for regularized estimation of neural networks, and they\ndeepen our mathematical understanding of neural networks and deep learning more\ngenerally.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 15:28:47 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 09:18:34 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Taheri", "Mahsa", ""], ["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "2006.00326", "submitter": "Ander Wilson", "authors": "Ander Wilson, Jessica Tryner, Christian L'Orange, John Volckens", "title": "Bayesian Nonparametric Monotone Regression", "comments": null, "journal-ref": "Environmetrics 2020", "doi": "10.1002/env.2642", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications there is interest in estimating the relation between a\npredictor and an outcome when the relation is known to be monotone or otherwise\nconstrained due to the physical processes involved. We consider one such\napplication--inferring time-resolved aerosol concentration from a low-cost\ndifferential pressure sensor. The objective is to estimate a monotone function\nand make inference on the scaled first derivative of the function. We proposed\nBayesian nonparametric monotone regression which uses a Bernstein polynomial\nbasis to construct the regression function and puts a Dirichlet process prior\non the regression coefficients. The base measure of the Dirichlet process is a\nfinite mixture of a mass point at zero and a truncated normal. This\nconstruction imposes monotonicity while clustering the basis functions.\nClustering the basis functions reduces the parameter space and allows the\nestimated regression function to be linear. With the proposed approach we can\nmake closed-formed inference on the derivative of the estimated function\nincluding full quantification of uncertainty. In a simulation study the\nproposed method performs similar to other monotone regression approaches when\nthe true function is wavy but performs better when the true function is linear.\nWe apply the method to estimate time-resolved aerosol concentration with a\nnewly-developed portable aerosol monitor. The R package bnmr is made available\nto implement the method.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 17:43:10 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wilson", "Ander", ""], ["Tryner", "Jessica", ""], ["L'Orange", "Christian", ""], ["Volckens", "John", ""]]}, {"id": "2006.00343", "submitter": "Charles Manski", "authors": "Charles F. Manski and Aleksey Tetenov", "title": "Statistical Decision Properties of Imprecise Trials Assessing COVID-19\n  Drugs", "comments": "28 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 pandemic progresses, researchers are reporting findings of\nrandomized trials comparing standard care with care augmented by experimental\ndrugs. The trials have small sample sizes, so estimates of treatment effects\nare imprecise. Seeing imprecision, clinicians reading research articles may\nfind it difficult to decide when to treat patients with experimental drugs.\nWhatever decision criterion one uses, there is always some probability that\nrandom variation in trial outcomes will lead to prescribing sub-optimal\ntreatments. A conventional practice when comparing standard care and an\ninnovation is to choose the innovation only if the estimated treatment effect\nis positive and statistically significant. This practice defers to standard\ncare as the status quo. To evaluate decision criteria, we use the concept of\nnear-optimality, which jointly considers the probability and magnitude of\ndecision errors. An appealing decision criterion from this perspective is the\nempirical success rule, which chooses the treatment with the highest observed\naverage patient outcome in the trial. Considering the design of recent and\nongoing COVID-19 trials, we show that the empirical success rule yields\ntreatment results that are much closer to optimal than those generated by\nprevailing decision criteria based on hypothesis tests.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 19:45:27 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Manski", "Charles F.", ""], ["Tetenov", "Aleksey", ""]]}, {"id": "2006.00371", "submitter": "Trevor Hastie", "authors": "Trevor Hastie", "title": "Ridge Regularizaton: an Essential Concept in Data Science", "comments": "17 pages, 5 figures. This paper was invited by Technometrics to\n  appear in a special section to celebrate the 50th anniversary of the 1970\n  original ridge paper by Hoerl and Kennard", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge or more formally $\\ell_2$ regularization shows up in many areas of\nstatistics and machine learning. It is one of those essential devices that any\ngood data scientist needs to master for their craft. In this brief ridge fest I\nhave collected together some of the magic and beauty of ridge that my\ncolleagues and I have encountered over the past 40 years in applied statistics.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 21:36:06 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hastie", "Trevor", ""]]}, {"id": "2006.00383", "submitter": "Victor Freguglia", "authors": "Victor Freguglia and Nancy Lopes Garcia", "title": "Inference tools for Markov Random Fields on lattices: The R package\n  mrf2d", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields on two-dimensional lattices are behind many image\nanalysis methodologies. mrf2d provides tools for statistical inference on a\nclass of discrete stationary Markov random field models with pairwise\ninteraction, which includes many of the popular models such as the Potts model\nand texture image models. The package introduces representations of dependence\nstructures and parameters, visualization functions and efficient (C++ based)\nimplementations of sampling algorithms, common estimation methods and other key\nfeatures of the model, providing a useful framework to implement algorithms and\nworking with the model in general. This paper presents a description and\ndetails of the package, as well as some reproducible examples of usage.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 22:43:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:09:35 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 17:22:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Freguglia", "Victor", ""], ["Garcia", "Nancy Lopes", ""]]}, {"id": "2006.00398", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "Prevalence Threshold and the Geometry of Screening Curves", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0240215", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between a screening tests' positive predictive value,\n$\\rho$, and its target prevalence, $\\phi$, is proportional - though not linear\nin all but a special case. In consequence, there is a point of local extrema of\ncurvature defined only as a function of the sensitivity $a$ and specificity $b$\nbeyond which the rate of change of a test's $\\rho$ drops precipitously relative\nto $\\phi$. Herein, we show the mathematical model exploring this phenomenon and\ndefine the $prevalence$ $threshold$ ($\\phi_e$) point where this change occurs\nas:\n  $\\phi_e=\\frac{\\sqrt{a\\left(-b+1\\right)}+b-1}{(\\varepsilon-1)}$ where\n$\\varepsilon$ = $a$+$b$.\n  From the prevalence threshold we deduce a more generalized relationship\nbetween prevalence and positive predictive value as a function of\n$\\varepsilon$, which represents a fundamental theorem of screening, herein\ndefined as:\n  $\\displaystyle\\lim_{\\varepsilon \\to 2}{\\displaystyle\n\\int_{0}^{1}}{\\rho(\\phi)d\\phi} = 1$\n  Understanding the concepts described in this work can help contextualize the\nvalidity of screening tests in real time, and help guide the interpretation of\ndifferent clinical scenarios in which screening is undertaken.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 01:12:42 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 22:06:27 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 11:56:34 GMT"}, {"version": "v4", "created": "Sun, 18 Oct 2020 23:44:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2006.00426", "submitter": "Lingzhou Xue", "authors": "Xiufan Yu, Danning Li, and Lingzhou Xue", "title": "Fisher's combined probability test for high-dimensional covariance\n  matrices", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing large covariance matrices is of fundamental importance in statistical\nanalysis with high-dimensional data. In the past decade, three types of test\nstatistics have been studied in the literature: quadratic form statistics,\nmaximum form statistics, and their weighted combination. It is known that\nquadratic form statistics would suffer from low power against sparse\nalternatives and maximum form statistics would suffer from low power against\ndense alternatives. The weighted combination methods were introduced to enhance\nthe power of quadratic form statistics or maximum form statistics when the\nweights are appropriately chosen. In this paper, we provide a new perspective\nto exploit the full potential of quadratic form statistics and maximum form\nstatistics for testing high-dimensional covariance matrices. We propose a\nscale-invariant power enhancement test based on Fisher's method to combine the\np-values of quadratic form statistics and maximum form statistics. After\ncarefully studying the asymptotic joint distribution of quadratic form\nstatistics and maximum form statistics, we prove that the proposed combination\nmethod retains the correct asymptotic size and boosts the power against more\ngeneral alternatives. Moreover, we demonstrate the finite-sample performance in\nsimulation studies and a real application.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:32:26 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yu", "Xiufan", ""], ["Li", "Danning", ""], ["Xue", "Lingzhou", ""]]}, {"id": "2006.00436", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke, Yucong Ma, Xihong Lin", "title": "Estimation of the number of spiked eigenvalues in a covariance matrix by\n  bulk eigenvalue matching analysis", "comments": "48 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spiked covariance model has gained increasing popularity in\nhigh-dimensional data analysis. A fundamental problem is determination of the\nnumber of spiked eigenvalues, $K$. For estimation of $K$, most attention has\nfocused on the use of $top$ eigenvalues of sample covariance matrix, and there\nis little investigation into proper ways of utilizing $bulk$ eigenvalues to\nestimate $K$. We propose a principled approach to incorporating bulk\neigenvalues in the estimation of $K$. Our method imposes a working model on the\nresidual covariance matrix, which is assumed to be a diagonal matrix whose\nentries are drawn from a gamma distribution. Under this model, the bulk\neigenvalues are asymptotically close to the quantiles of a fixed parametric\ndistribution. This motivates us to propose a two-step method: the first step\nuses bulk eigenvalues to estimate parameters of this distribution, and the\nsecond step leverages these parameters to assist the estimation of $K$. The\nresulting estimator $\\hat{K}$ aggregates information in a large number of bulk\neigenvalues. We show the consistency of $\\hat{K}$ under a standard spiked\ncovariance model. We also propose a confidence interval estimate for $K$. Our\nextensive simulation studies show that the proposed method is robust and\noutperforms the existing methods in a range of scenarios. We apply the proposed\nmethod to analysis of a lung cancer microarray data set and the 1000 Genomes\ndata set.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 04:36:07 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 03:35:40 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Ma", "Yucong", ""], ["Lin", "Xihong", ""]]}, {"id": "2006.00447", "submitter": "\\'Alvaro Eduardo Gajardo Cataldo", "authors": "\\'Alvaro Gajardo and Hans-Georg M\\\"uller", "title": "Point Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point processes in time have a wide range of applications that include the\nclaims arrival process in insurance or the analysis of queues in operations\nresearch. Due to advances in technology, such samples of point processes are\nincreasingly encountered. A key object of interest is the local intensity\nfunction. It has a straightforward interpretation that allows to understand and\nexplore point process data. We consider functional approaches for point\nprocesses, where one has a sample of repeated realizations of the point\nprocess. This situation is inherently connected with Cox processes, where the\nintensity functions of the replications are modeled as random functions. Here\nwe study a situation where one records covariates for each replication of the\nprocess, such as the daily temperature for bike rentals. For modeling point\nprocesses as responses with vector covariates as predictors we propose a novel\nregression approach for the intensity function that is intrinsically\nnonparametric. While the intensity function of a point process that is only\nobserved once on a fixed domain cannot be identified, we show how covariates\nand repeated observations of the process can be utilized to make consistent\nestimation possible, and we also derive asymptotic rates of convergence without\ninvoking parametric assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 06:32:59 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gajardo", "\u00c1lvaro", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2006.00487", "submitter": "Kun Chen", "authors": "Xiaokang Liu, Xiaomei Cong, Gen Li, Kendra Maas, Kun Chen", "title": "Multivariate Log-Contrast Regression with Sub-Compositional Predictors:\n  Testing the Association Between Preterm Infants' Gut Microbiome and\n  Neurobehavioral Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so-called gut-brain axis has stimulated extensive research on\nmicrobiomes. One focus is to assess the association between certain clinical\noutcomes and the relative abundances of gut microbes, which can be presented as\nsub-compositional data in conformity with the taxonomic hierarchy of bacteria.\nMotivated by a study for identifying the microbes in the gut microbiome of\npreterm infants that impact their later neurobehavioral outcomes, we formulate\na constrained integrative multi-view regression, where the neurobehavioral\nscores form multivariate response, the sub-compositional microbiome data form\nmulti-view feature matrices, and a set of linear constraints on their\ncorresponding sub-coefficient matrices ensures the conformity to the simplex\ngeometry. To enable joint selection and inference of sub-compositions/views, we\nassume all the sub-coefficient matrices are possibly of low-rank, i.e., the\noutcomes are associated with the microbiome through different sets of latent\nsub-compositional factors from different taxa. We propose a scaled composite\nnuclear norm penalization approach for model estimation and develop a\nhypothesis testing procedure through de-biasing to assess the significance of\ndifferent views. Simulation studies confirm the effectiveness of the proposed\nprocedure. In the preterm infant study, the identified microbes are mostly\nconsistent with existing studies and biological understandings. Our approach\nsupports that stressful early life experiences imprint gut microbiome through\nthe regulation of the gut-brain axis.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 10:24:44 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Liu", "Xiaokang", ""], ["Cong", "Xiaomei", ""], ["Li", "Gen", ""], ["Maas", "Kendra", ""], ["Chen", "Kun", ""]]}, {"id": "2006.00567", "submitter": "Weichi Yao", "authors": "Weichi Yao and Halina Frydman and Denis Larocque and Jeffrey S.\n  Simonoff", "title": "Ensemble Methods for Survival Data with Time-Varying Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival data with time-varying covariates are common in practice. If\nrelevant, such covariates can improve on the estimation of a survival function.\nHowever, the traditional survival forests - conditional inference forest,\nrelative risk forest and random survival forest - have accommodated only\ntime-invariant covariates.\n  We generalize the conditional inference and relative risk forests to allow\ntime-varying covariates. We compare their performance with that of the extended\nCox model, a commonly used method, and the transformation forest method,\ndesigned to detect non-proportional hazards deviations and adapted here to\naccommodate time-varying covariates, through a comprehensive simulation study\nin which the Kaplan-Meier estimate serves as a benchmark and the integrated L2\ndifference between the true and estimated survival functions is used for\nevaluation.\n  In general, the performance of the two proposed forests substantially\nimproves over the Kaplan-Meier estimate. Under the proportional-hazard setting,\nthe best method is always one of the two proposed forests, while under the\nnon-proportional hazards setting, it is the adapted transformation forest. We\nuse K-fold cross-validation to choose between the methods, which is shown to be\nan effective tool to provide guidance in practice. The performance of the\nproposed forest methods for time-invariant covariate data is broadly similar to\nthat found for time-varying covariate data.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 17:25:04 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:51:29 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 17:33:17 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yao", "Weichi", ""], ["Frydman", "Halina", ""], ["Larocque", "Denis", ""], ["Simonoff", "Jeffrey S.", ""]]}, {"id": "2006.00595", "submitter": "Lu Zhang", "authors": "Lu Zhang, Sudipto Banerjee", "title": "Spatial Factor Modeling: A Bayesian Matrix-Normal Approach for\n  Misaligned Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatially-oriented data sets are prevalent in the environmental\nand physical sciences. Scientists seek to jointly model multiple variables,\neach indexed by a spatial location, to capture any underlying spatial\nassociation for each variable and associations among the different dependent\nvariables. Multivariate latent spatial process models have proved effective in\ndriving statistical inference and rendering better predictive inference at\narbitrary locations for the spatial process. High-dimensional multivariate\nspatial data, which is the theme of this article, refers to data sets where the\nnumber of spatial locations and the number of spatially dependent variables is\nvery large. The field has witnessed substantial developments in scalable models\nfor univariate spatial processes, but such methods for multivariate spatial\nprocesses, especially when the number of outcomes are moderately large, are\nlimited in comparison. Here, we extend scalable modeling strategies for a\nsingle process to multivariate processes. We pursue Bayesian inference which is\nattractive for full uncertainty quantification of the latent spatial process.\nOur approach exploits distribution theory for the Matrix-Normal distribution,\nwhich we use to construct scalable versions of a hierarchical linear model of\ncoregionalization (LMC) and spatial factor models that deliver inference over a\nhigh-dimensional parameter space including the latent spatial process. We\nillustrate the computational and inferential benefits of our algorithms over\ncompeting methods using simulation studies and an analysis of a massive\nvegetation index data set.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 19:43:42 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 02:07:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Lu", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2006.00702", "submitter": "Dimitra Maoutsa", "authors": "Dimitra Maoutsa, Sebastian Reich, Manfred Opper", "title": "Interacting particle solutions of Fokker-Planck equations through\n  gradient-log-density estimation", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": "10.3390/e22080802", "report-no": null, "categories": "cond-mat.stat-mech math.DS math.PR physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fokker-Planck equations are extensively employed in various scientific fields\nas they characterise the behaviour of stochastic systems at the level of\nprobability density functions. Although broadly used, they allow for analytical\ntreatment only in limited settings, and often is inevitable to resort to\nnumerical solutions. Here, we develop a computational approach for simulating\nthe time evolution of Fokker-Planck solutions in terms of a mean field limit of\nan interacting particle system. The interactions between particles are\ndetermined by the gradient of the logarithm of the particle density,\napproximated here by a novel statistical estimator. The performance of our\nmethod shows promising results, with more accurate and less fluctuating\nstatistics compared to direct stochastic simulations of comparable particle\nnumber. Taken together, our framework allows for effortless and reliable\nparticle-based simulations of Fokker-Planck equations in low and moderate\ndimensions. The proposed gradient-log-density estimator is also of independent\ninterest, for example, in the context of optimal control.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 04:06:38 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Maoutsa", "Dimitra", ""], ["Reich", "Sebastian", ""], ["Opper", "Manfred", ""]]}, {"id": "2006.00767", "submitter": "Minsuk Shin", "authors": "Minsuk Shin, Lu Wang, and Jun S Liu", "title": "Scalable Uncertainty Quantification via GenerativeBootstrap Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been believed that the virtue of using statistical procedures is on\nuncertainty quantification in statistical decisions, and the bootstrap method\nhas been commonly used for this purpose. However, nowadays as the size of data\nmassively increases and statistical models become more complicated, the\nimplementation of bootstrapping turns out to be practically challenging due to\nits repetitive nature in computation. To overcome this issue, we propose a\nnovel computational procedure called {\\it Generative Bootstrap Sampler} (GBS),\nwhich constructs a generator function of bootstrap evaluations, and this\nfunction transforms the weights on the observed data points to the bootstrap\ndistribution. The GBS is implemented by one single optimization, without\nrepeatedly evaluating the optimizer of bootstrapped loss function as in\nstandard bootstrapping procedures. As a result, the GBS is capable of reducing\ncomputational time of bootstrapping by hundreds of folds when the data size is\nmassive. We show that the bootstrapped distribution evaluated by the GBS is\nasymptotically equivalent to the conventional counterpart and empirically they\nare indistinguishable. We examine the proposed idea to bootstrap various models\nsuch as linear regression, logistic regression, Cox proportional hazard model,\nand Gaussian process regression model, quantile regression, etc. The results\nshow that the GBS procedure is not only accelerating the computational speed,\nbut it also attains a high level of accuracy to the target bootstrap\ndistribution. Additionally, we apply this idea to accelerate the computation of\nother repetitive procedures such as bootstrapped cross-validation, tuning\nparameter selection, and permutation test.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 07:55:16 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Shin", "Minsuk", ""], ["Wang", "Lu", ""], ["Liu", "Jun S", ""]]}, {"id": "2006.00783", "submitter": "Cheng Li", "authors": "Rajarshi Guhaniyogi, Cheng Li, Terrance D. Savitsky, Sanvesh\n  Srivastava", "title": "Distributed Bayesian Varying Coefficient Modeling Using a Gaussian\n  Process Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient models (VCMs) are widely used for estimating nonlinear\nregression functions in functional data models. Their Bayesian variants using\nGaussian process (GP) priors on the functional coefficients, however, have\nreceived limited attention in massive data applications. This is primarily due\nto the prohibitively slow posterior computations using Markov chain Monte Carlo\n(MCMC) algorithms. We address this problem using a divide-and-conquer Bayesian\napproach that operates in three steps. The first step creates a large number of\ndata subsets with much smaller sample sizes by sampling without replacement\nfrom the full data. The second step formulates VCM as a linear mixed-effects\nmodel and develops a data augmentation (DA)-type algorithm for obtaining MCMC\ndraws of the parameters and predictions on all the subsets in parallel. The\nDA-type algorithm appropriately modifies the likelihood such that every subset\nposterior distribution is an accurate approximation of the corresponding true\nposterior distribution. The third step develops a combination algorithm for\naggregating MCMC-based estimates of the subset posterior distributions into a\nsingle posterior distribution called the Aggregated Monte Carlo (AMC)\nposterior. Theoretically, we derive minimax optimal posterior convergence rates\nfor the AMC posterior distributions of both the varying coefficients and the\nmean regression function. We provide quantification on the orders of subset\nsample sizes and the number of subsets according to the smoothness properties\nof the multivariate GP. The empirical results show that the combination schemes\nthat satisfy our theoretical assumptions, including the one in the AMC\nalgorithm, have better nominal coverage, shorter credible intervals, smaller\nmean square errors, and higher effective sample size than their main\ncompetitors across diverse simulations and in a real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:16:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Li", "Cheng", ""], ["Savitsky", "Terrance D.", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "2006.00789", "submitter": "Xuejun Ma X.J. Ma", "authors": "Xuejun Ma and Ping Zhang", "title": "Quantile regression for compositional covariates", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a very important tool to explore the relationship\nbetween the response variable and its covariates. Motivated by mean regression\nwith LASSO for compositional covariates proposed by Lin et al. (2014), we\nconsider quantile regression with no-penalty and penalty function. We develop\nthe computational algorithms based on linear programming. Numerical studies\nindicate that our methods provides the better alternative than mean regression\nunder many settings, particularly for heavy-tailed or skewed distribution of\nthe error term. Finally, we study the fat data using the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:23:09 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ma", "Xuejun", ""], ["Zhang", "Ping", ""]]}, {"id": "2006.00952", "submitter": "Tao Zhang", "authors": "Tao Zhang, Kengo Kato and David Ruppert", "title": "Bootstrap inference for quantile-based modal regression", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop uniform inference methods for the conditional mode\nbased on quantile regression. Specifically, we propose to estimate the\nconditional mode by minimizing the derivative of the estimated conditional\nquantile function defined by smoothing the linear quantile regression\nestimator, and develop two bootstrap methods, a novel pivotal bootstrap and the\nnonparametric bootstrap, for our conditional mode estimator. Building on\nhigh-dimensional Gaussian approximation techniques, we establish the validity\nof simultaneous confidence rectangles constructed from the two bootstrap\nmethods for the conditional mode. We also extend the preceding analysis to the\ncase where the dimension of the covariate vector is increasing with the sample\nsize. Finally, we conduct simulation experiments and a real data analysis using\nU.S. wage data to demonstrate the finite sample performance of our inference\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:59:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 18:20:59 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:59:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Tao", ""], ["Kato", "Kengo", ""], ["Ruppert", "David", ""]]}, {"id": "2006.01002", "submitter": "Kei Hirose", "authors": "Kei Hirose", "title": "Interpretable modeling for short- and medium-term electricity load\n  forecasting", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of short- and medium-term electricity load\nforecasting by using past loads and daily weather forecast information.\nConventionally, many researchers have directly applied regression analysis.\nHowever, interpreting the effect of weather on these loads is difficult with\nthe existing methods. In this study, we build a statistical model that resolves\nthis interpretation issue. A varying coefficient model with basis expansion is\nused to capture the nonlinear structure of the weather effect. This approach\nresults in an interpretable model when the regression coefficients are\nnonnegative. To estimate the nonnegative regression coefficients, we employ\nnonnegative least squares. Three real data analyses show the practicality of\nour proposed statistical modeling. Two of them demonstrate good forecast\naccuracy and interpretability of our proposed method. In the third example, we\ninvestigate the effect of COVID-19 on electricity loads. The interpretation\nwould help make strategies for energy-saving interventions and demand response.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:09:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 08:07:06 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Hirose", "Kei", ""]]}, {"id": "2006.01055", "submitter": "Yucong Ma", "authors": "Yucong Ma, Jun S. Liu", "title": "On Posterior Consistency of Bayesian Factor Models in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a principled dimension reduction technique, factor models have been widely\nadopted in social science, economics, bioinformatics, and many other fields.\nHowever, in high-dimensional settings, conducting a 'correct' Bayesianfactor\nanalysis can be subtle since it requires both a careful prescription of the\nprior distribution and a suitable computational strategy. In particular, we\nanalyze the issues related to the attempt of being \"noninformative\" for\nelements of the factor loading matrix, especially for sparse Bayesian factor\nmodels in high dimensions, and propose solutions to them. We show here why\nadopting the orthogonal factor assumption is appropriate and can result in a\nconsistent posterior inference of the loading matrix conditional on the true\nidiosyncratic variance and the allocation of nonzero elements in the true\nloading matrix. We also provide an efficient Gibbs sampler to conduct the full\nposterior inference based on the prior setup from Rockova and George (2016)and\na uniform orthogonal factor assumption on the factor matrix.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:30:11 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 20:49:01 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 18:33:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ma", "Yucong", ""], ["Liu", "Jun S.", ""]]}, {"id": "2006.01230", "submitter": "Jingchen Hu", "authors": "Terrance D. Savitsky, Jingchen Hu, Matthew R. Williams", "title": "Re-weighting of Vector-weighted Mechanisms for Utility Maximization\n  under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present practical aspects of implementing a pseudo posterior synthesizer\nfor microdata dissemination under a new re-weighting strategy for utility\nmaximization. Our re-weighting strategy applies to any vector-weighting\napproach under which a vector of observation-indexed weight are used to\ndownweight likelihood contributions for high disclosure risk records. We\ndemonstrate our method on two different vector-weighted schemes that target\nhigh-risk records by exponentiating each of their likelihood contributions with\na record-indexed weight, $\\alpha_i \\in [0,1]$ for record $i \\in (1,\\ldots,n)$.\nWe compute the overall Lipschitz bound, $\\Delta_{\\bm{\\alpha},\\mathbf{x}}$, for\nthe database $\\mathbf{x}$, under each vector-weighted scheme where a local\n$\\epsilon_{x} = 2\\Delta_{\\bm{\\alpha},\\mathbf{x}}$. Our new method for\nconstructing record-indexed downeighting maximizes the data utility under any\nprivacy budget for the vector-weighted synthesizers by adjusting the by-record\nweights, $(\\alpha_{i})_{i = 1}^{n}$, such that their individual Lipschitz\nbounds, $\\Delta_{\\bm{\\alpha},x_{i}}$, approach the bound for the entire\ndatabase, $\\Delta_{\\bm{\\alpha},\\mathbf{x}}$. We illustrate our methods using\nsimulated count data with and without over-dispersion-induced skewness and\ncompare the results to a scalar-weighted synthesizer under the Exponential\nMechanism (EM). We demonstrate our pDP result in a simulation study and our\nmethods on a sample of the Survey of Doctorate Recipients.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 20:03:37 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 14:36:33 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 15:56:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Hu", "Jingchen", ""], ["Williams", "Matthew R.", ""]]}, {"id": "2006.01298", "submitter": "Ryan Hornby", "authors": "Ryan Hornby, Jingchen Hu", "title": "Identification Risks Evaluation of Partially Synthetic Data with the\n  $\\texttt{IdentificationRiskCalculation}$ R Package", "comments": "16 pages with 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a general approach to evaluating identification risk of synthesized\nvariables in partially synthetic data. For multiple continuous synthesized\nvariables, we introduce the use of a radius $r$ in the construction of\nidentification risk probability of each target record, and illustrate with\nworking examples. We create the $\\texttt{IdentificationRiskCalculation}$ R\npackage to aid researchers and data disseminators in performing these\nidentification risks evaluation calculations. We demonstrate our methods\nthrough the R package with applications to a data sample from the Consumer\nExpenditure Surveys, and discuss the impacts on risk and data utility of 1) the\nchoice of radius $r$, 2) the choice of synthesized variables, and 3) the choice\nof number of synthetic datasets. We give recommendations for statistical\nagencies for synthesizing and evaluating identification risk of continuous\nvariables.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:35:36 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 00:03:00 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 06:47:11 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 02:05:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hornby", "Ryan", ""], ["Hu", "Jingchen", ""]]}, {"id": "2006.01328", "submitter": "Karl Schurter", "authors": "Joris Pinkse, Karl Schurter", "title": "Estimates of derivatives of (log) densities and related objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the density and its derivatives using a local polynomial\napproximation to the logarithm of an unknown density $f$. The estimator is\nguaranteed to be nonnegative and achieves the same optimal rate of convergence\nin the interior as well as the boundary of the support of $f$. The estimator is\ntherefore well-suited to applications in which nonnegative density estimates\nare required, such as in semiparametric maximum likelihood estimation. In\naddition, we show that our estimator compares favorably with other kernel-based\nmethods, both in terms of asymptotic performance and computational ease.\nSimulation results confirm that our method can perform similarly in finite\nsamples to these alternative methods when they are used with optimal inputs,\ni.e. an Epanechnikov kernel and optimally chosen bandwidth sequence. Further\nsimulation evidence demonstrates that, if the researcher modifies the inputs\nand chooses a larger bandwidth, our approach can even improve upon these\noptimized alternatives, asymptotically. We provide code in several languages.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 00:59:37 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Pinkse", "Joris", ""], ["Schurter", "Karl", ""]]}, {"id": "2006.01340", "submitter": "Maoran Xu", "authors": "Maoran Xu, Leo L. Duan", "title": "Bayesian Inference with the l1-ball Prior: Solving Combinatorial\n  Problems with Exact Zeros", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The l1-regularization is very popular in high dimensional statistics -- it\nchanges a combinatorial problem of choosing which subset of the parameter are\nzero, into a simple continuous optimization. Using a continuous prior\nconcentrated near zero, the Bayesian counterparts are successful in quantifying\nthe uncertainty in the variable selection problems; nevertheless, the lack of\nexact zeros makes it difficult for broader problems such as the change-point\ndetection and rank selection. Inspired by the duality of the l1-regularization\nas a constraint onto an l1-ball, we propose a new prior by projecting a\ncontinuous distribution onto the l1-ball. This creates a positive probability\non the ball boundary, which contains both continuous elements and exact zeros.\nUnlike the spike-and-slab prior, this l1-ball projection is continuous and\ndifferentiable almost surely, making the posterior estimation amenable to the\nHamiltonian Monte Carlo algorithm. We examine the properties, such as the\nvolume change due to the projection, the connection to the combinatorial prior,\nthe minimax concentration rate in the linear problem. We demonstrate the\nusefulness of exact zeros that simplify the combinatorial problems, such as the\nchange-point detection in time series, the dimension selection of mixture model\nand the low-rank-plus-sparse change detection in the medical images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 01:48:07 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 02:39:40 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 20:16:47 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Xu", "Maoran", ""], ["Duan", "Leo L.", ""]]}, {"id": "2006.01366", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz and Nicholas Williams and Katherine L. Hoffman and\n  Edward J. Schenck", "title": "Non-parametric causal effects based on longitudinal modified treatment\n  policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most causal inference methods consider counterfactual variables under\ninterventions that set the treatment deterministically. With continuous or\nmulti-valued treatments or exposures, such counterfactuals may be of little\npractical interest because no feasible intervention can be implemented that\nwould bring them about. Furthermore, violations to the positivity assumption,\nnecessary for identification, are exacerbated with continuous and multi-valued\ntreatments and deterministic interventions. In this paper we propose\nlongitudinal modified treatment policies (LMTPs) as a non-parametric\nalternative. LMTPs can be designed to guarantee positivity, and yield effects\nof immediate practical relevance with an interpretation that is familiar to\nregular users of linear regression adjustment. We study the identification of\nthe LMTP parameter, study properties of the statistical estimand such as the\nefficient influence function, and propose four different estimators. Two of our\nestimators are efficient, and one is sequentially doubly robust in the sense\nthat it is consistent if, for each time point, either an outcome regression or\na treatment mechanism is consistently estimated. We perform a simulation study\nto illustrate the properties of the estimators, and present the results of our\nmotivating study on hypoxemia and mortality in Intensive Care Unit (ICU)\npatients. Software implementing our methods is provided in the form of the open\nsource \\texttt{R} package \\texttt{lmtp} freely available on GitHub\n(\\url{https://github.com/nt-williams/lmtp}).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 03:15:13 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 01:13:28 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 00:33:16 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 16:13:37 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Williams", "Nicholas", ""], ["Hoffman", "Katherine L.", ""], ["Schenck", "Edward J.", ""]]}, {"id": "2006.01393", "submitter": "Hyunseung Kang", "authors": "Hyunseung Kang, Youjin Lee, T. Tony Cai, Dylan S. Small", "title": "Two Robust Tools for Inference about Causal Effects with Invalid\n  Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables have been widely used to estimate the causal effect of\na treatment on an outcome. Existing confidence intervals for causal effects\nbased on instrumental variables assume that all of the putative instrumental\nvariables are valid; a valid instrumental variable is a variable that affects\nthe outcome only by affecting the treatment and is not related to unmeasured\nconfounders. However, in practice, some of the putative instrumental variables\nare likely to be invalid. This paper presents two tools to conduct valid\ninference and tests in the presence of invalid instruments. First, we propose a\nsimple and general approach to construct confidence intervals based on taking\nunions of well-known confidence intervals. Second, we propose a novel test for\nthe null causal effect based on a collider bias. Our two proposals, especially\nwhen fused together, outperform traditional instrumental variable confidence\nintervals when invalid instruments are present, and can also be used as a\nsensitivity analysis when there is concern that instrumental variables\nassumptions are violated. The new approach is applied to a Mendelian\nrandomization study on the causal effect of low-density lipoprotein on the\nincidence of cardiovascular diseases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 05:15:02 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kang", "Hyunseung", ""], ["Lee", "Youjin", ""], ["Cai", "T. Tony", ""], ["Small", "Dylan S.", ""]]}, {"id": "2006.01395", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Nima Aghaeepour, Trevor Hastie, Robert Tibshirani", "title": "Feature-weighted elastic net: using \"features of features\" for better\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some supervised learning settings, the practitioner might have additional\ninformation on the features used for prediction. We propose a new method which\nleverages this additional information for better prediction. The method, which\nwe call the feature-weighted elastic net (\"fwelnet\"), uses these \"features of\nfeatures\" to adapt the relative penalties on the feature coefficients in the\nelastic net penalty. In our simulations, fwelnet outperforms the lasso in terms\nof test mean squared error and usually gives an improvement in true positive\nrate or false positive rate for feature selection. We also apply this method to\nearly prediction of preeclampsia, where fwelnet outperforms the lasso in terms\nof 10-fold cross-validated area under the curve (0.86 vs. 0.80). We also\nprovide a connection between fwelnet and the group lasso and suggest how\nfwelnet might be used for multi-task learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 05:18:15 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Aghaeepour", "Nima", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2006.01474", "submitter": "Danijel Kivaranovic", "authors": "Danijel Kivaranovic, Robin Ristl, Martin Posch, Hannes Leeb", "title": "Conformal prediction intervals for the individual treatment effect", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several prediction intervals procedures for the individual\ntreatment effect with either finite-sample or asymptotic coverage guarantee in\na non-parametric regression setting, where non-linear regression functions,\nheteroskedasticity and non-Gaussianity are allowed. The construct the\nprediction intervals we use the conformal method of Vovk et al. (2005). In\nextensive simulations, we compare the coverage probability and interval length\nof our prediction interval procedures. We demonstrate that complex learning\nalgorithms, such as neural networks, can lead to narrower prediction intervals\nthan simple algorithms, such as linear regression, if the sample size is large\nenough.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:17:48 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kivaranovic", "Danijel", ""], ["Ristl", "Robin", ""], ["Posch", "Martin", ""], ["Leeb", "Hannes", ""]]}, {"id": "2006.01507", "submitter": "Jing Zhang", "authors": "Yanyan Liu, Yuanshan Wu, Jing Zhang and Haibo Zhou", "title": "Cox regression analysis for distorted covariates with an unknown\n  distortion function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study inference for censored survival data where some covariates are\ndistorted by some unknown functions of an observable confounding variable in a\nmultiplicative form. Example of this kind of data in medical studies is the\ncommon practice to normalizing some important observed exposure variables by\npatients' body mass index (BMI), weight or age. Such phenomenon also appears\nfrequently in environmental studies where ambient measure is used for\nnormalization, and in genomic studies where library size needs to be normalized\nfor next generation sequencing data. We propose a new covariate-adjusted Cox\nproportional hazards regression model and utilize the kernel smoothing method\nto estimate the distorting function, then employ an estimated maximum\nlikelihood method to derive estimator for the regression parameters. We\nestablish the large sample properties of the proposed estimator. Extensive\nsimulation studies demonstrate that the proposed estimator performs well in\ncorrecting the bias arising from distortion. A real data set from the National\nWilms' Tumor Study (NWTS) is used to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 10:17:24 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Liu", "Yanyan", ""], ["Wu", "Yuanshan", ""], ["Zhang", "Jing", ""], ["Zhou", "Haibo", ""]]}, {"id": "2006.01569", "submitter": "Rapha\\\"el Huser", "authors": "Peng Zhong, Rapha\\\"el Huser, and Thomas Opitz", "title": "Modeling Non-Stationary Temperature Maxima Based on Extremal Dependence\n  Changing with Event Magnitude", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of spatio-temporal trends in temperature extremes can help\nbetter understand the structure and frequency of heatwaves in a changing\nclimate. Here, we study annual temperature maxima over Southern Europe using a\ncentury-spanning dataset observed at 44 monitoring stations. Extending the\nspectral representation of max-stable processes, our modeling framework relies\non a novel construction of max-infinitely divisible processes, which include\ncovariates to capture spatio-temporal non-stationarities. Our new model keeps a\npopular max-stable process on the boundary of the parameter space, while\nflexibly capturing weakening extremal dependence at increasing quantile levels\nand asymptotic independence. This is achieved by linking the overall magnitude\nof a spatial event to its spatial correlation range, in such a way that more\nextreme events become less spatially dependent, thus more localized. Our model\nreveals salient features of the spatio-temporal variability of European\ntemperature extremes, and it clearly outperforms natural alternative models.\nResults show that the spatial extent of heatwaves is smaller for more severe\nevents at higher altitudes, and that recent heatwaves are moderately wider. Our\nprobabilistic assessment of the 2019 annual maxima confirms the severity of the\n2019 heatwaves both spatially and at individual sites, especially when compared\nto climatic conditions prevailing in 1950-1975.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 12:40:44 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:16:26 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhong", "Peng", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""]]}, {"id": "2006.01584", "submitter": "Yang Liu", "authors": "Yang Liu, Robert J.B. Goudie", "title": "Stochastic Approximation Cut Algorithm for Inference in Modularized\n  Bayesian Models", "comments": "31 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian modelling enables us to accommodate complex forms of data and make a\ncomprehensive inference, but the effect of partial misspecification of the\nmodel is a concern. One approach in this setting is to modularize the model,\nand prevent feedback from suspect modules, using a cut model. After observing\ndata, this leads to the cut distribution which normally does not have a\nclosed-form. Previous studies have proposed algorithms to sample from this\ndistribution, but these algorithms have unclear theoretical convergence\nproperties. To address this, we propose a new algorithm called the Stochastic\nApproximation Cut algorithm (SACut) as an alternative. The algorithm is divided\ninto two parallel chains. The main chain targets an approximation to the cut\ndistribution; the auxiliary chain is used to form an adaptive proposal\ndistribution for the main chain. We prove convergence of the samples drawn by\nthe proposed algorithm and present the exact limit. Although SACut is biased,\nsince the main chain does not target the exact cut distribution, we prove this\nbias can be reduced geometrically by increasing a user-chosen tuning parameter.\nIn addition, parallel computing can be easily adopted for SACut, which greatly\nreduces computation time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 13:19:15 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 03:19:21 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 12:20:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liu", "Yang", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2006.01586", "submitter": "Hisashi Kobayashi", "authors": "Hisashi Kobayashi", "title": "Stochastic Modeling of an Infectious Disease, Part I: Understand the\n  Negative Binomial Distribution and Predict an Epidemic More Reliably", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why are the epidemic patterns of COVID-19 so different among different cities\nor countries which are similar in their populations, medical infrastructures,\nand people's behavior? Why are forecasts or predictions made by so-called\nexperts often grossly wrong, concerning the numbers of people who get infected\nor die? The purpose of this study is to better understand the stochastic nature\nof an epidemic disease, and answer the above questions. Much of the work on\ninfectious diseases has been based on \"SIR deterministic models,\" (Kermack and\nMcKendrick:1927.) We will explore stochastic models that can capture the\nessence of the seemingly erratic behavior of an infectious disease. A\nstochastic model, in its formulation, takes into account the random nature of\nan infectious disease.\n  The stochastic model we study here is based on the \"birth-and-death process\nwith immigration\" (BDI for short), which was proposed in the study of\npopulation growth or extinction of some biological species. The BDI process\nmodel ,however, has not been investigated by the epidemiology community. The\nBDI process is one of a few birth-and-death processes, which we can solve\nanalytically. Its time-dependent probability distribution function is a\n\"negative binomial distribution\" with its parameter $r$ less than $1$. The\n\"coefficient of variation\" of the process is larger than $\\sqrt{1/r} > 1$.\nFurthermore, it has a long tail like the zeta distribution. These properties\nexplain why infection patterns exhibit enormously large variations. The number\nof infected predicted by a deterministic model is much greater than the median\nof the distribution. This explains why any forecast based on a deterministic\nmodel will fail more often than not.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 13:25:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kobayashi", "Hisashi", ""]]}, {"id": "2006.01617", "submitter": "Sven Serneels", "authors": "Peter Filzmoser and Sven Serneels and Ricardo Maronna and Christophe\n  Croux", "title": "Robust multivariate methods in Chemometrics", "comments": "This article is an update of: P. Filzmoser, S. Serneels, R. Maronna,\n  P.J. Van Espen, 3.24 - Robust Multivariate Methods in Chemometrics, in\n  Comprehensive Chemometrics, 1st Edition, edited by Steven D. Brown, Rom\\'a\n  Tauler, Beata Walczak, Elsevier, 2009,\n  https://doi.org/10.1016/B978-044452701-1.00113-7", "journal-ref": "in: Comprehensive Chemometrics, 2nd Edition, Steven Brown, Roma\n  Tauler and Beata Walczak (Eds.), Elsevier, 26 May 2020, ISBN: 9780444641656,\n  Volume 3, Section 3.19, pages 393-430", "doi": "10.1016/B978-0-12-409547-2.14642-6", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This chapter presents an introduction to robust statistics with applications\nof a chemometric nature. Following a description of the basic ideas and\nconcepts behind robust statistics, including how robust estimators can be\nconceived, the chapter builds up to the construction (and use) of robust\nalternatives for some methods for multivariate analysis frequently used in\nchemometrics, such as principal component analysis and partial least squares.\nThe chapter then provides an insight into how these robust methods can be used\nor extended to classification. To conclude, the issue of validation of the\nresults is being addressed: it is shown how uncertainty statements associated\nwith robust estimates, can be obtained.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:02:46 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 15:20:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Filzmoser", "Peter", ""], ["Serneels", "Sven", ""], ["Maronna", "Ricardo", ""], ["Croux", "Christophe", ""]]}, {"id": "2006.01662", "submitter": "Sheng Xu", "authors": "Sheng Xu, Zhou Fan, Sahand Negahban", "title": "Tree-Projected Gradient Descent for Estimating Gradient-Sparse\n  Parameters on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation of a gradient-sparse parameter vector\n$\\boldsymbol{\\theta}^* \\in \\mathbb{R}^p$, having strong gradient-sparsity\n$s^*:=\\|\\nabla_G \\boldsymbol{\\theta}^*\\|_0$ on an underlying graph $G$. Given\nobservations $Z_1,\\ldots,Z_n$ and a smooth, convex loss function $\\mathcal{L}$\nfor which $\\boldsymbol{\\theta}^*$ minimizes the population risk\n$\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta};Z_1,\\ldots,Z_n)]$, we propose to\nestimate $\\boldsymbol{\\theta}^*$ by a projected gradient descent algorithm that\niteratively and approximately projects gradient steps onto spaces of vectors\nhaving small gradient-sparsity over low-degree spanning trees of $G$. We show\nthat, under suitable restricted strong convexity and smoothness assumptions for\nthe loss, the resulting estimator achieves the squared-error risk\n$\\frac{s^*}{n} \\log (1+\\frac{p}{s^*})$ up to a multiplicative constant that is\nindependent of $G$. In contrast, previous polynomial-time algorithms have only\nbeen shown to achieve this guarantee in more specialized settings, or under\nadditional assumptions for $G$ and/or the sparsity pattern of $\\nabla_G\n\\boldsymbol{\\theta}^*$. As applications of our general framework, we apply our\nresults to the examples of linear models and generalized linear models with\nrandom design.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 20:08:13 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Xu", "Sheng", ""], ["Fan", "Zhou", ""], ["Negahban", "Sahand", ""]]}, {"id": "2006.01786", "submitter": "Yingying Ma", "authors": "Yingying Ma and Hansheng Wang", "title": "Hyperparameter Selection for Subsampling Bootstraps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive data analysis becomes increasingly prevalent, subsampling methods\nlike BLB (Bag of Little Bootstraps) serves as powerful tools for assessing the\nquality of estimators for massive data. However, the performance of the\nsubsampling methods are highly influenced by the selection of tuning parameters\n( e.g., the subset size, number of resamples per subset ). In this article we\ndevelop a hyperparameter selection methodology, which can be used to select\ntuning parameters for subsampling methods. Specifically, by a careful\ntheoretical analysis, we find an analytically simple and elegant relationship\nbetween the asymptotic efficiency of various subsampling estimators and their\nhyperparameters. This leads to an optimal choice of the hyperparameters. More\nspecifically, for an arbitrarily specified hyperparameter set, we can improve\nit to be a new set of hyperparameters with no extra CPU time cost, but the\nresulting estimator's statistical efficiency can be much improved. Both\nsimulation studies and real data analysis demonstrate the superior advantage of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:10:45 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ma", "Yingying", ""], ["Wang", "Hansheng", ""]]}, {"id": "2006.01799", "submitter": "Olli Saarela", "authors": "Olli Saarela, David A. Stephens, Erica E. M. Moodie", "title": "The role of exchangeability in causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of exchangeability has been recognized in the causal inference\nliterature in various guises, but only rarely in the original Bayesian meaning\nas a symmetry property between individual units in statistical inference. Since\nthe latter is a standard ingredient in Bayesian inference, we argue that in\nBayesian causal inference it is natural to link the causal model, including the\nnotion of confounding and definition of causal contrasts of interest, to the\nconcept of exchangeability. Here we relate the Bayesian notion of\nexchangeability to alternative conditions for unconfounded inferences, commonly\nstated using potential outcomes, and define causal contrasts in the presence of\nexchangeability in terms of limits of posterior predictive expectations for\nfurther exchangeable units. While our main focus is in a point treatment\nsetting, we also investigate how this reasoning carries over to longitudinal\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:33:33 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 02:03:15 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Saarela", "Olli", ""], ["Stephens", "David A.", ""], ["Moodie", "Erica E. M.", ""]]}, {"id": "2006.01864", "submitter": "Chiara Bocci", "authors": "Paul A. Smith, Chiara Bocci, Nikos Tzavidis, Sabine Krieg, Marc J.E.\n  Smeets", "title": "Robust estimation for small domains in business surveys", "comments": null, "journal-ref": "J R Stat Soc Series C, 70: 312-334 (2021)", "doi": "10.1111/rssc.12460", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area (or small domain) estimation is still rarely applied in business\nstatistics, because of challenges arising from the skewness and variability of\nvariables such as turnover. We examine a range of small area estimation methods\nas the basis for estimating the activity of industries within the retail sector\nin the Netherlands. We use tax register data and a sampling procedure which\nreplicates the sampling for the retail sector of Statistics Netherlands'\nStructural Business Survey as a basis for investigating the properties of small\narea estimators. In particular, we consider the use of the EBLUP under a random\neffects model and variations of the EBLUP derived under (a) a random effects\nmodel that includes a complex specification for the level 1 variance and (b) a\nrandom effects model that is fitted by using the survey weights. Although\naccounting for the survey weights in estimation is important, the impact of\ninfluential data points remains the main challenge in this case. The paper\nfurther explores the use of outlier robust estimators in business surveys, in\nparticular a robust version of the EBLUP, M-regression based synthetic\nestimators, and M-quantile small area estimators. The latter family of small\narea estimators includes robust projective (without and with survey weights)\nand robust predictive versions. M-quantile methods have the lowest empirical\nmean squared error and are substantially better than direct estimators, though\nthere is an open question about how to choose the tuning constant for bias\nadjustment in practice. The paper makes a further contribution by exploring a\ndoubly robust approach comprising the use of survey weights in conjunction with\noutlier robust methods in small area estimation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:24:35 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Smith", "Paul A.", ""], ["Bocci", "Chiara", ""], ["Tzavidis", "Nikos", ""], ["Krieg", "Sabine", ""], ["Smeets", "Marc J. E.", ""]]}, {"id": "2006.01882", "submitter": "Marta Cousido-Rocha", "authors": "Marta Cousido-Rocha, Jacobo de U\\~na-\\'Alvarez, Sebastian D\\\"ohler", "title": "Improved $q$-values for discrete uniform and homogeneous tests: a\n  comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale discrete uniform and homogeneous $P$-values often arise in\napplications with multiple testing. For example, this occurs in genome wide\nassociation studies whenever a nonparametric one-sample (or two-sample) test is\napplied throughout the gene loci. In this paper we consider $q$-values for such\nscenarios based on several existing estimators for the proportion of true null\nhypothesis, $\\pi_0$, which take the discreteness of the $P$-values into\naccount. The theoretical guarantees of the several approaches with respect to\nthe estimation of $\\pi_0$ and the false discovery rate control are reviewed.\nThe performance of the discrete $q$-values is investigated through intensive\nMonte Carlo simulations, including location, scale and omnibus nonparametric\ntests, and possibly dependent $P$-values. The methods are applied to genetic\nand financial data for illustration purposes too. Since the particular\nestimator of $\\pi_0$ used to compute the $q$-values may influence the power,\nrelative advantages and disadvantages of the reviewed procedures are discussed.\nPractical recommendations are given.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:01:27 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 11:15:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cousido-Rocha", "Marta", ""], ["de U\u00f1a-\u00c1lvarez", "Jacobo", ""], ["D\u00f6hler", "Sebastian", ""]]}, {"id": "2006.01924", "submitter": "H Frost", "authors": "H. Robert Frost", "title": "Eigenvectors from Eigenvalues Sparse Principal Component Analysis\n  (EESPCA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for sparse principal component analysis. This\nmethod, named Eigenvectors from Eigenvalues Sparse Principal Component Analysis\n(EESPCA), is based on the recently detailed formula for computing normed,\nsquared eigenvector loadings of a Hermitian matrix from the eigenvalues of the\nfull matrix and associated sub-matrices. Relative to the state-of-the-art\nLASSO-based sparse PCA method of Witten, Tibshirani and Hastie, the EESPCA\ntechnique offers a two-orders-of-magnitude improvement in computational speed,\ndoes not require estimation of tuning parameters, and can more accurately\nidentify true zero principal component loadings across a range of data matrix\nsizes and covariance structures. Importantly, EESPCA achieves these performance\nbenefits while maintaining a reconstruction error close to that generated by\nthe Witten et al. approach. EESPCA is a practical and effective technique for\nsparse PCA with particular relevance to computationally demanding problems such\nas the analysis of large data matrices or statistical techniques like\nresampling that involve the repeated application of sparse PCA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:14:55 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 15:42:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Frost", "H. Robert", ""]]}, {"id": "2006.01936", "submitter": "Carsten Botts", "authors": "Carsten Botts", "title": "An Alternative Metric for Detecting Anomalous Ship Behavior Using a\n  Variation of the DBSCAN Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need to quickly and accurately identify anomalous behavior\nin ships. This paper applies a variation of the Density Based Spatial\nClustering Among Noise (DBSCAN) algorithm to identify such anomalous behavior\ngiven a ship's Automatic Identification System (AIS) data. This variation of\nthe DBSCAN algorithm has been previously introduced in the literature, and in\nthis study, we elucidate and explore the mathematical details of this algorithm\nand introduce an alternative anomaly metric which is more statistically\ninformative than the one previously suggested.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:39:02 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 22:05:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Botts", "Carsten", ""]]}, {"id": "2006.02041", "submitter": "Sandipan Pramanik", "authors": "Sandipan Pramanik and Xianyang Zhang", "title": "Structure Adaptive Lasso", "comments": "42 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lasso is of fundamental importance in high-dimensional statistics and has\nbeen routinely used to regress a response on a high-dimensional set of\npredictors. In many scientific applications, there exists external information\nthat encodes the predictive power and sparsity structure of the predictors. In\nthis article, we develop a new method, called the Structure Adaptive Lasso\n(SA-Lasso), to incorporate these potentially useful side information into a\npenalized regression. The basic idea is to translate the external information\ninto different penalization strengths for the regression coefficients. We study\nthe risk properties of the resulting estimator. In particular, we generalize\nthe state evolution framework recently introduced for the analysis of the\napproximate message-passing algorithm to the SA-Lasso setting. We show that the\nfinite sample risk of the SA-Lasso estimator is consistent with the theoretical\nrisk predicted by the state evolution equation. Our theory suggests that the\nSA-Lasso with an informative group or covariate structure can significantly\noutperform the Lasso, Adaptive Lasso, and Sparse Group Lasso. This evidence is\nfurther confirmed in our numerical studies. We also demonstrate the usefulness\nand the superiority of our method in a real data application.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 04:42:55 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 20:23:00 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Pramanik", "Sandipan", ""], ["Zhang", "Xianyang", ""]]}, {"id": "2006.02070", "submitter": "Peng Tian", "authors": "Peng Tian, Jianfeng Yao", "title": "Ratio-consistent estimation for long range dependent Toeplitz covariance\n  with application to matrix data whitening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a data matrix $X:=R_M^{1/2}ZC_N^{1/2}$ where $R_M$ is a $M\\times\nM$ Toeplitz matrix, $Z$ is a $M\\times N$ random matrix of uncorrelated\nstandardized white noise, and $C_N$ a $N\\times N$ positive semi-definite\nmatrix. The model $X$ can be interpreted as a multivariate stationary time\nseries with a separable covariance function. When this series is short range\ndependent, two estimators $\\hat{R}_M$ and $\\hat{R}_M^b$ of $R_M$, constructed\nby toeplitzifying the sample covariance matrix $S=N^{-1}XX^*$, are commonly\nused to whiten the correlation $R_M$ in $X$. Both are proved to be consistent\nin spectral norm in previous articles under mild conditions.\n  In this paper, we establish that when the time series is long range\ndependent, the above spectral norm consistency does not always hold, but a\nweaker {\\it ratio consistency} for the unbiased estimator $\\hat{R}_M$ still\nholds. It is shown that this ratio consistency is sufficient for the whitening\nprocedure. For the biased estimator $\\hat{R}_M^b$, such ratio consistency does\nnot hold either, but a weaker {\\it ratio LSD consistency} does. Numeric\nsimulations are also provided to illustrate these new phenomena and their\nimpact on applications such as the whitening procedure.\n  Finally we apply our results to signal detection and high-dimensional PCA.\nLet $X=[YR_M^{1/2}]^*$ with $Y=A\\mathbf{M}+\\sigma^2\\mathbf{N}$ a complex\nGaussian signal plus noise model. Using the whitened sample covariance matrix\n$\\underline{S}_w=M^{-1}X^*\\hat{R}_M^{-1}X$, we estimate the number of signals\nand their strengths contained in $A$. Then we proceed PCA on $X$ to obtain a\ncompressed data matrix formed with its principal components.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 06:55:51 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 11:37:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tian", "Peng", ""], ["Yao", "Jianfeng", ""]]}, {"id": "2006.02290", "submitter": "Abhinav K. Jha", "authors": "Jinxin Liu, Ziping Liu, Joyce Mhlanga, Barry A. Siegel, Abhinav K. Jha", "title": "A no-gold-standard technique to objectively evaluate quantitative\n  imaging methods using patient data: Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective evaluation of quantitative imaging (QI) methods using measurements\ndirectly obtained from patient images is highly desirable but hindered by the\nnon-availability of gold standards. To address this issue, statistical\ntechniques have been proposed to objectively evaluate QI methods without a gold\nstandard. These techniques assume that the measured and true values are\nlinearly related by a slope, bias, and normally distributed noise term, where\nit is assumed that the noise term between the different methods is independent.\nHowever, the noise could be correlated since it arises in the process of\nmeasuring the same true value. To address this issue, we propose a new\nno-gold-standard evaluation (NGSE) technique that models this noise as a\nmultivariate normally distributed term, characterized by a covariance matrix.\nIn this manuscript, we derive a maximum-likelihood-based technique that,\nwithout any knowledge of the true QI values, estimates the slope, bias, and\ncovariance matrix terms. These are then used to rank the methods on the basis\nof precision of the measured QI values. Overall, the derivation demonstrates\nthe mathematical premise behind the proposed NGSE technique.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 14:20:32 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liu", "Jinxin", ""], ["Liu", "Ziping", ""], ["Mhlanga", "Joyce", ""], ["Siegel", "Barry A.", ""], ["Jha", "Abhinav K.", ""]]}, {"id": "2006.02302", "submitter": "Idir Arab", "authors": "Tommaso Lando, Idir Arab, and Paulo Eduardo Oliveira", "title": "Second-order stochastic comparisons of order statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of comparing ageing patterns of the lifetime of\nk-out-of-n systems. Mathematically, this reduces to being able to decide about\na stochastic ordering relationship between different order statistics. We\ndiscuss such relationships with respect to second-order stochastic dominance,\nobtaining characterizations through the verification of relative convexity with\nrespect to a suitably chosen reference distribution function. We introduce a\nhierarchy of such reference functions leading to classes, each expressing\ndifferent and increasing knowledge precision about the distribution of the\ncomponent lifetimes. Such classes are wide enough to include popular families\nof distributions, such as, for example, the increasing failure rate\ndistributions. We derive sufficient dominance conditions depending on the\nidentification of the class which includes the component lifetimes. Concerning\nthe conditions, as expected, relying on a larger class of distributions,\nmeaning that we have less precise information about the components behavior,\nleads to the need of stronger assumptions. We discuss the applicability of this\nmethod and characterize a test for the relative convexity, as this notion plays\na central role in the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 14:41:54 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Lando", "Tommaso", ""], ["Arab", "Idir", ""], ["Oliveira", "Paulo Eduardo", ""]]}, {"id": "2006.02423", "submitter": "Ting Ye", "authors": "Ting Ye, Luke Keele, Raiden Hasegawa, Dylan S. Small", "title": "A Negative Correlation Strategy for Bracketing in\n  Difference-in-Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of difference-in-differences (DID) is widely used to study the\ncausal effect of policy interventions in observational studies. DID employs a\nbefore and after comparison of the treated and control units to remove bias due\nto time-invariant unmeasured confounders under the parallel trends assumption.\nEstimates from DID, however, will be biased if the outcomes for the treated and\ncontrol units evolve differently in the absence of treatment, namely if the\nparallel trends assumption is violated. We propose a general identification\nstrategy that leverages two groups of control units whose outcomes relative to\nthe treated units exhibit a negative correlation, and achieves partial\nidentification of the average treatment effect for the treated. The identified\nset is of a union bounds form that involves the minimum and maximum operators,\nwhich makes the canonical bootstrap generally inconsistent and naive methods\noverly conservative. By utilizing the directional inconsistency of the\nbootstrap distribution, we develop a novel bootstrap method to construct\nuniformly valid confidence intervals for the identified set and parameter of\ninterest when the identified set is of a union bounds form, and we establish\nthe method's theoretical properties. We develop a simple falsification test and\nsensitivity analysis. We apply the proposed strategy for bracketing to study\nwhether minimum wage laws affect employment levels.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:53:30 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 03:17:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ye", "Ting", ""], ["Keele", "Luke", ""], ["Hasegawa", "Raiden", ""], ["Small", "Dylan S.", ""]]}, {"id": "2006.02504", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Plots of the cumulative differences between observed and expected values\n  of ordered Bernoulli variates", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many predictions are probabilistic in nature; for example, a prediction could\nbe for precipitation tomorrow, but with only a 30 percent chance. Given both\nthe predictions and the actual outcomes, \"reliability diagrams\" (also known as\n\"calibration plots\") help detect and diagnose statistically significant\ndiscrepancies between the predictions and the outcomes. The canonical\nreliability diagrams are based on histogramming the observed and expected\nvalues of the predictions; several variants of the standard reliability\ndiagrams propose to replace the hard histogram binning with soft kernel density\nestimation using smooth convolutional kernels of widths similar to the widths\nof the bins. In all cases, an important question naturally arises: which widths\nare best (or are multiple plots with different widths better)? Rather than\nanswering this question, plots of the cumulative differences between the\nobserved and expected values largely avoid the question, by displaying\nmiscalibration directly as the slopes of secant lines for the graphs. Slope is\neasy to perceive with quantitative precision even when the constant offsets of\nthe secant lines are irrelevant. There is no need to bin or perform kernel\ndensity estimation with a somewhat arbitrary kernel.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 20:15:43 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 14:54:54 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 01:49:14 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "2006.02544", "submitter": "Matteo Sesia", "authors": "Yaniv Romano, Matteo Sesia, Emmanuel J. Cand\\`es", "title": "Classification with Valid and Adaptive Coverage", "comments": "10 pages, 3 figures; 13 supplementary pages, 4 supplementary figures,\n  4 supplementary tables", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal inference, cross-validation+, and the jackknife+ are hold-out\nmethods that can be combined with virtually any machine learning algorithm to\nconstruct prediction sets with guaranteed marginal coverage. In this paper, we\ndevelop specialized versions of these techniques for categorical and unordered\nresponse labels that, in addition to providing marginal coverage, are also\nfully adaptive to complex data distributions, in the sense that they perform\nfavorably in terms of approximate conditional coverage compared to alternative\nmethods. The heart of our contribution is a novel conformity score, which we\nexplicitly demonstrate to be powerful and intuitive for classification\nproblems, but whose underlying principle is potentially far more general.\nExperiments on synthetic and real data demonstrate the practical value of our\ntheoretical guarantees, as well as the statistical advantages of the proposed\nmethods over the existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 21:42:04 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2006.02611", "submitter": "Yuefeng Han", "authors": "Yuefeng Han, Rong Chen, Dan Yang and Cun-Hui Zhang", "title": "Tensor Factor Model Estimation by Iterative Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor time series, which is a time series consisting of tensorial\nobservations, has become ubiquitous. It typically exhibits high dimensionality.\nOne approach for dimension reduction is to use a factor model structure, in a\nform similar to Tucker tensor decomposition, except that the time dimension is\ntreated as a dynamic process with a time dependent structure. In this paper we\nintroduce two approaches to estimate such a tensor factor model by using\niterative orthogonal projections of the original tensor time series. The\napproaches extend the existing estimation procedures and our theoretical\ninvestigation shows that they improve the estimation accuracy and convergence\nrate significantly. The developed approaches are similar to higher order\northogonal projection methods for tensor decomposition, but with significant\ndifferences and theoretical properties. Simulation study is conducted to\nfurther illustrate the statistical properties of these estimators.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 02:11:37 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Han", "Yuefeng", ""], ["Chen", "Rong", ""], ["Yang", "Dan", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2006.02649", "submitter": "Alex Rodrigo dos Santos Sousa", "authors": "Alex Rodrigo dos S. Sousa, Magno T.F. Severino and Florencia G.\n  Leonardi", "title": "Model selection criteria for regression models with splines and the\n  automatic localization of knots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model selection approach to fit a regression model\nusing splines with a variable number of knots. We introduce a penalized\ncriterion to estimate the number and the position of the knots where to anchor\nthe splines basis. The method is evaluated on simulated data and applied to\ncovid-19 daily reported cases for short-term prediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 05:47:57 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 19:21:24 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Sousa", "Alex Rodrigo dos S.", ""], ["Severino", "Magno T. F.", ""], ["Leonardi", "Florencia G.", ""]]}, {"id": "2006.02700", "submitter": "Noirrit Kiran Chandra", "authors": "Noirrit Kiran Chandra, Antonio Canale and David B. Dunson", "title": "Escaping the curse of dimensionality in Bayesian model based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, there is interest in clustering very high-dimensional\ndata. A common strategy is first stage dimensionality reduction followed by a\nstandard clustering algorithm, such as k-means. This approach does not target\ndimension reduction to the clustering objective, and fails to quantify\nuncertainty. Model-based Bayesian approaches provide an appealing alternative,\nbut often have poor performance in high-dimensions, producing too many or too\nfew clusters. This article provides an explanation for this behavior through\nstudying the clustering posterior in a non-standard setting with fixed sample\nsize and increasing dimensionality. We show that the finite sample posterior\ntends to either assign every observation to a different cluster or all\nobservations to the same cluster as dimension grows, depending on the kernels\nand prior specification but not on the true data-generating model. To find\nmodels avoiding this pitfall, we define a Bayesian oracle for clustering, with\nthe oracle clustering posterior based on the true values of low-dimensional\nlatent variables. We define a class of LAtent Mixtures for Bayesian (Lamb)\nclustering that have equivalent behavior to this oracle as dimension grows.\nLamb is shown to have good performance in simulation studies and an application\nto inferring cell types based on scRNAseq.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:40:42 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 01:52:51 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 15:55:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Canale", "Antonio", ""], ["Dunson", "David B.", ""]]}, {"id": "2006.02941", "submitter": "Ian Grooms", "authors": "Ian Grooms", "title": "A note on the formulation of the Ensemble Adjustment Kalman Filter", "comments": "3 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.comp-ph stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ensemble adjustment Kalman filter (EAKF; Anderson, 2001) is one of the\nearliest ensemble square root filters. This note clarifies the correct\nformulation of the EAKF, which depends on a careful treatment of an\neigen-decomposition of one of the matrices involved in the formulation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:25:35 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Grooms", "Ian", ""]]}, {"id": "2006.02954", "submitter": "Alessio Serafini", "authors": "Alessio Serafini, Thomas Brendan Murphy, Luca Scrucca", "title": "Handling missing data in model-based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Mixture models (GMMs) are a powerful tool for clustering,\nclassification and density estimation when clustering structures are embedded\nin the data. The presence of missing values can largely impact the GMMs\nestimation process, thus handling missing data turns out to be a crucial point\nin clustering, classification and density estimation. Several techniques have\nbeen developed to impute the missing values before model estimation. Among\nthese, multiple imputation is a simple and useful general approach to handle\nmissing data. In this paper we propose two different methods to fit Gaussian\nmixtures in the presence of missing data. Both methods use a variant of the\nMonte Carlo Expectation-Maximisation (MCEM) algorithm for data augmentation.\nThus, multiple imputations are performed during the E-step, followed by the\nstandard M-step for a given eigen-decomposed component-covariance matrix. We\nshow that the proposed methods outperform the multiple imputation approach,\nboth in terms of clusters identification and density estimation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:36:31 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Serafini", "Alessio", ""], ["Murphy", "Thomas Brendan", ""], ["Scrucca", "Luca", ""]]}, {"id": "2006.02995", "submitter": "Silvia D'Angelo", "authors": "Silvia D'Angelo, Lorraine Brennan and Isobel Claire Gormley", "title": "Inferring food intake from multiple biomarkers using a latent variable\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metabolomic based approaches have gained much attention in recent years due\nto their promising potential to deliver objective tools for assessment of food\nintake. In particular, multiple biomarkers have emerged for single foods.\nHowever, there is a lack of statistical tools available for combining multiple\nbiomarkers to infer food intake. Furthermore, there is a paucity of approaches\nfor estimating the uncertainty around biomarker based prediction of intake.\n  Here, to facilitate inference on the relationship between multiple\nmetabolomic biomarkers and food intake in an intervention study conducted under\nthe A-DIET research programme, a latent variable model, multiMarker, is\nproposed. The proposed model draws on factor analytic and mixture of experts\nmodels, describing intake as a continuous latent variable whose value gives\nraise to the observed biomarker values. We employ a mixture of Gaussian\ndistributions to flexibly model the latent variable. A Bayesian hierarchical\nmodelling framework provides flexibility to adapt to different biomarker\ndistributions and facilitates prediction of the latent intake along with its\nassociated uncertainty.\n  Simulation studies are conducted to assess the performance of the proposed\nmultiMarker framework, prior to its application to the motivating application\nof quantifying apple intake.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 16:29:16 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 14:03:49 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 10:27:43 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["D'Angelo", "Silvia", ""], ["Brennan", "Lorraine", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "2006.03025", "submitter": "Ben Boukai", "authors": "Melissa C. Key and Ben Boukai", "title": "A statistical Testing Procedure for Validating Class Labels", "comments": "A total of 20 pages with 6 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an open problem of validating protein identities in label-free\nshotgun proteomics work-flows, we present a testing procedure to validate\nclass/protein labels using available measurements across instances/peptides.\nMore generally, we present a solution to the problem of identifying instances\nthat are deemed, based on some distance (or quasi-distance) measure, as\noutliers relative to the subset of instances assigned to the same class. The\nproposed procedure is non-parametric and requires no specific distributional\nassumption on the measured distances. The only assumption underlying the\ntesting procedure is that measured distances between instances within the same\nclass are stochastically smaller than measured distances between instances from\ndifferent classes. The test is shown to simultaneously control the Type I and\nType II error probabilities whilst also controlling the overall error\nprobability of the repeated testing invoked in the validation procedure of\ninitial class labeling. The theoretical results are supplemented with results\nfrom an extensive numerical study, simulating a typical setup for labeling\nvalidation in proteomics work-flow applications. These results illustrate the\napplicability and viability of our method. Even with up to 25% of instances\nmislabeled, our testing procedure maintains a high specificity and greatly\nreduces the proportion of mislabeled instances.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:19:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Key", "Melissa C.", ""], ["Boukai", "Ben", ""]]}, {"id": "2006.03105", "submitter": "Yongming Qu", "authors": "Yongming Qu, Linda Shurzinske, Shanthi Sethuraman", "title": "Defining Estimands Using a Mix of Strategies to Handle Intercurrent\n  Events in Clinical Trials", "comments": "21 page, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials (RCT) are the gold standard for evaluation of\nthe efficacy and safety of investigational interventions. If every patient in\nan RCT were to adhere to the randomized treatment, one could simply analyze the\ncomplete data to infer the treatment effect. However, intercurrent events\n(ICEs) including the use of concomitant medication for unsatisfactory efficacy,\ntreatment discontinuation due to adverse events, or lack of efficacy, may lead\nto interventions that deviate from the original treatment assignment.\nTherefore, defining the appropriate estimand (the appropriate parameter to be\nestimated) based on the primary objective of the study is critical prior to\ndetermining the statistical analysis method and analyzing the data. The\nInternational Council for Harmonisation (ICH) E9 (R1), published on November\n20, 2019, provided 5 strategies to define the estimand: treatment policy,\nhypothetical, composite variable, while on treatment and principal stratum. In\nthis article, we propose an estimand using a mix of strategies in handling\nICEs. This estimand is an average of the null treatment difference for those\nwith ICEs potentially related to safety and the treatment difference for the\nother patients if they would complete the assigned treatments. Two examples\nfrom clinical trials evaluating anti-diabetes treatments are provided to\nillustrate the estimation of this proposed estimand and to compare it with the\nestimates for estimands using hypothetical and treatment policy strategies in\nhandling ICEs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:25:08 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Qu", "Yongming", ""], ["Shurzinske", "Linda", ""], ["Sethuraman", "Shanthi", ""]]}, {"id": "2006.03140", "submitter": "Mireille Schnitzer", "authors": "Mireille E. Schnitzer, Daphna Harel, Vikki Ho, Anita Koushik and\n  Joanna Merckx", "title": "Identifiability and estimation under the test-negative design with\n  population controls with the goal of identifying risk and preventive factors\n  for SARS-CoV-2 infection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapidly evolving COVID-19 pandemic caused by the SARS-CoV-2 virus,\nquick public health investigations of the relationships between behaviours and\ninfection risk are essential. Recently the test-negative design was proposed to\nrecruit and survey participants who are symptomatic and being tested for\nSARS-CoV-2 infection with the goal of evaluating associations between the\nsurvey responses (including behaviours and environment) and testing positive on\nthe test. It was also proposed to recruit additional controls who are part of\nthe general population as a baseline comparison group in order to evaluate risk\nfactors specific to SARS-CoV-2 infection. In this study, we consider an\nalternative design where we recruit among all individuals, symptomatic and\nasymptomatic, being tested for the virus in addition to population controls. We\ndefine a regression parameter related to a prospective risk factor analysis and\ninvestigate its identifiability under the two study designs. We review the\ndifference between the prospective risk factor parameter and the parameter\ntargeted in the typical test-negative design where only symptomatic and tested\npeople are recruited.\n  Using missing data directed acyclic graphs we provide conditions and required\ndata collection under which identifiability of the prospective risk factor\nparameter is possible and compare the benefits and limitations of the\nalternative study designs and target parameters. We propose a novel inverse\nprobability weighting estimator and demonstrate the performance of this\nestimator through simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 21:41:51 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 15:18:22 GMT"}, {"version": "v3", "created": "Fri, 5 Feb 2021 22:36:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Schnitzer", "Mireille E.", ""], ["Harel", "Daphna", ""], ["Ho", "Vikki", ""], ["Koushik", "Anita", ""], ["Merckx", "Joanna", ""]]}, {"id": "2006.03151", "submitter": "Theodore Papamarkou", "authors": "Matt Baucum, Anahita Khojandi, Theodore Papamarkou", "title": "Hidden Markov models as recurrent neural networks: An application to\n  Alzheimer's disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are commonly used for disease progression\nmodeling when the true patient health state is not fully known. Since HMMs may\nhave multiple local optima, performance can be improved by incorporating\nadditional patient covariates to inform estimation. To allow for this, we\ndevelop hidden Markov recurrent neural networks (HMRNNs), a special case of\nrecurrent neural networks with the same likelihood function as a corresponding\ndiscrete-observation HMM. The HMRNN can be combined with any other predictive\nneural networks that take patient information as input, with all parameters\nestimated simultaneously via gradient descent. Using a dataset of Alzheimer's\ndisease patients, we demonstrate how combining the HMRNN with other predictive\nneural networks improves disease forecasting performance and offers a novel\nclinical interpretation compared with a standard HMM trained via\nexpectation-maximization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 22:06:06 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 09:12:40 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Baucum", "Matt", ""], ["Khojandi", "Anahita", ""], ["Papamarkou", "Theodore", ""]]}, {"id": "2006.03211", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Jane-Ling Wang", "title": "Mean and Covariance Estimation for Functional Snippets", "comments": "54 pages, 4 figures; to appear in Journal of the American Statistical\n  Association", "journal-ref": null, "doi": "10.1080/01621459.2020.1777138", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of mean and covariance functions of functional\nsnippets, which are short segments of functions possibly observed irregularly\non an individual specific subinterval that is much shorter than the entire\nstudy interval. Estimation of the covariance function for functional snippets\nis challenging since information for the far off-diagonal regions of the\ncovariance structure is completely missing. We address this difficulty by\ndecomposing the covariance function into a variance function component and a\ncorrelation function component. The variance function can be effectively\nestimated nonparametrically, while the correlation part is modeled\nparametrically, possibly with an increasing number of parameters, to handle the\nmissing information in the far off-diagonal regions. Both theoretical analysis\nand numerical simulations suggest that this hybrid strategy %\ndivide-and-conquer strategy is effective. In addition, we propose a new\nestimator for the variance of measurement errors and analyze its asymptotic\nproperties. This estimator is required for the estimation of the variance\nfunction from noisy measurements.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:52:15 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Lin", "Zhenhua", ""], ["Wang", "Jane-Ling", ""]]}, {"id": "2006.03238", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu, Allan Timmermann", "title": "Can Two Forecasts Have the Same Conditional Expected Accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method for testing equal predictive accuracy for pairs of forecasting\nmodels proposed by Giacomini and White (2006) has found widespread use in\nempirical work. The procedure assumes that the parameters of the underlying\nforecasting models are estimated using a rolling window of fixed width and\nincorporates the effect of parameter estimation in the null hypothesis that two\nforecasts have identical conditionally expected loss. We show that this null\nhypothesis cannot be valid under a rolling window estimation scheme and even\nfails in the absence of parameter estimation for many types of stochastic\nprocesses in common use. This means that the approach does not guarantee\nappropriate comparisons of predictive accuracy of forecasting models. We also\nshow that the Giacomini-White approach can lead to substantial size distortions\nin tests of equal unconditional predictive accuracy and propose an alternative\nprocedure with better properties.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 05:24:05 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhu", "Yinchu", ""], ["Timmermann", "Allan", ""]]}, {"id": "2006.03241", "submitter": "Sho Ichigozaki", "authors": "Sho Ichigozaki, Takahiro Kawashima and Hayaru Shouno", "title": "Bayesian Sparse Covariance Structure Analysis for Correlated Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Bayesian Graphical LASSO for correlated countable\ndata and apply it to spatial crime data. In the proposed model, we assume a\nGaussian Graphical Model for the latent variables which dominate the potential\nrisks of crimes. To evaluate the proposed model, we determine optimal\nhyperparameters which represent samples better. We apply the proposed model for\nestimation of the sparse inverse covariance of the latent variable and evaluate\nthe partial correlation coefficients. Finally, we illustrate the results on\ncrime spots data and consider the estimated latent variables and the partial\ncorrelation coefficients of the sparse inverse covariance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 05:34:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ichigozaki", "Sho", ""], ["Kawashima", "Takahiro", ""], ["Shouno", "Hayaru", ""]]}, {"id": "2006.03246", "submitter": "Tingyu Zhu", "authors": "Weijuan Liang, Shuangge Ma, Qingzhao Zhang, Tingyu Zhu", "title": "Integrative Sparse Partial Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial least squares, as a dimension reduction method, has become\nincreasingly important for its ability to deal with problems with a large\nnumber of variables. Since noisy variables may weaken the performance of the\nmodel, the sparse partial least squares (SPLS) technique has been proposed to\nidentify important variables and generate more interpretable results. However,\nthe small sample size of a single dataset limits the performance of\nconventional methods. An effective solution comes from gathering information\nfrom multiple comparable studies. The integrative analysis holds an important\nstatus among multi-datasets analyses. The main idea is to improve estimation\nresults by assembling raw datasets and analyzing them jointly. In this paper,\nwe develop an integrative SPLS (iSPLS) method using penalization based on the\nSPLS technique. The proposed approach consists of two penalties. The first\npenalty conducts variable selection under the context of integrative analysis;\nThe second penalty, a contrasted one, is imposed to encourage the similarity of\nestimates across datasets and generate more reasonable and accurate results.\nComputational algorithms are provided. Simulation experiments are conducted to\ncompare iSPLS with alternative approaches. The practical utility of iSPLS is\nshown in the analysis of two TCGA gene expression data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 06:04:01 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Liang", "Weijuan", ""], ["Ma", "Shuangge", ""], ["Zhang", "Qingzhao", ""], ["Zhu", "Tingyu", ""]]}, {"id": "2006.03284", "submitter": "Yi Yu", "authors": "Yaofang Hu and Wanjie Wang and Yi Yu", "title": "Graph matching beyond perfectly-overlapping Erd\\H{o}s--R\\'enyi random\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching is a fruitful area in terms of both algorithms and theories.\nIn this paper, we exploit the degree information, which was previously used\nonly in noiseless graphs and perfectly-overlapping Erd\\H{o}s--R\\'enyi random\ngraphs matching. We are concerned with graph matching of partially-overlapping\ngraphs and stochastic block models, which are more useful in tackling real-life\nproblems. We propose the edge exploited degree profile graph matching method\nand two refined varations. We conduct a thorough analysis of our proposed\nmethods' performances in a range of challenging scenarios, including a\nzebrafish neuron activity data set and a coauthorship data set. Our methods are\nproved to be numerically superior than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:07:43 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hu", "Yaofang", ""], ["Wang", "Wanjie", ""], ["Yu", "Yi", ""]]}, {"id": "2006.03332", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "fbst: An R package for the Full Bayesian Significance Test for testing a\n  sharp null hypothesis against its alternative via the e-value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is a central statistical method in psychology and the\ncognitive sciences. However, the problems of null hypothesis significance\ntesting (NHST) and p-values have been debated widely, but few attractive\nalternatives exist. This article introduces the fbst R package, which\nimplements the Full Bayesian Significance Test (FBST) to test a sharp null\nhypothesis against its alternative via the e-value. The statistical theory of\nthe FBST has been introduced by Pereira et al. (1999) more than two decades ago\nand since then, the FBST has shown to be a Bayesian alternative to NHST and\np-values with both theoretical and practical highly appealing properties. The\nalgorithm provided in the fbst package is applicable to any Bayesian model as\nlong as the posterior distribution can be obtained at least numerically. The\ncore function of the package provides the Bayesian evidence against the null\nhypothesis, the e-value. Additionally, p-values based on asymptotic arguments\ncan be computed and rich visualisations for communication and interpretation of\nthe results can be produced. Three examples of frequently used statistical\nprocedures in the cognitive sciences are given in this paper which demonstrate\nhow to apply the FBST in practice using the fbst package. Based on the success\nof the FBST in statistical science, the fbst package should be of interest to a\nbroad range of researchers in psychology and the cognitive sciences and\nhopefully will encourage researchers to consider the FBST as a possible\nalternative when conducting hypothesis tests of a sharp null hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:32:27 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "2006.03334", "submitter": "Riko Kelter", "authors": "Riko Kelter, Julio Michael Stern", "title": "The Full Bayesian Significance Test and the e-value -- Foundations,\n  theory and application in the cognitive sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is a central statistical method in psychological research\nand the cognitive sciences. While the problems of null hypothesis significance\ntesting (NHST) have been debated widely, few attractive alternatives exist. In\nthis paper, we provide a tutorial on the Full Bayesian Significance Test (FBST)\nand the e-value, which is a fully Bayesian alternative to traditional\nsignificance tests which rely on p-values. The FBST is an advanced\nmethodological procedure which can be applied to several areas. In this\ntutorial, we showcase with two examples of widely used statistical methods in\npsychological research how the FBST can be used in practice, provide\nresearchers with explicit guidelines on how to conduct it and make available\nR-code to reproduce all results. The FBST is an innovative method which has\nclearly demonstrated to perform better than frequentist significance testing.\nHowever, to our best knowledge, it has not been used so far in the\npsychological sciences and should be of wide interest to a broad range of\nresearchers in psychology and the cognitive sciences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:34:38 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kelter", "Riko", ""], ["Stern", "Julio Michael", ""]]}, {"id": "2006.03459", "submitter": "Rouven Schmidt", "authors": "Rouven Schmidt, Thomas Kneib", "title": "Analytic expressions for the Cumulative Distribution Function of the\n  Composed Error Term in Stochastic Frontier Analysis with Truncated Normal and\n  Exponential Inefficiencies", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic frontier model, the composed error term consists of the\nmeasurement error and the inefficiency term. A general assumption is that the\ninefficiency term follows a truncated normal or exponential distribution. In a\nwide variety of models evaluating the cumulative distribution function of the\ncomposed error term is required. This work introduces and proves four\nrepresentation theorems for these distributions - two for each distributional\nassumptions. These representations can be utilized for a fast and accurate\nevaluation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:03:58 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Schmidt", "Rouven", ""], ["Kneib", "Thomas", ""]]}, {"id": "2006.03572", "submitter": "Yi Yu", "authors": "Daren Wang and Yi Yu and Rebecca Willett", "title": "Detecting Abrupt Changes in High-Dimensional Self-Exciting Poisson\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional self-exciting point processes have been widely used in many\napplication areas to model discrete event data in which past and current events\naffect the likelihood of future events. In this paper, we are concerned with\ndetecting abrupt changes of the coefficient matrices in discrete-time\nhigh-dimensional self-exciting Poisson processes, which have yet to be studied\nin the existing literature due to both theoretical and computational challenges\nrooted in the non-stationary and high-dimensional nature of the underlying\nprocess. We propose a penalized dynamic programming approach which is supported\nby a theoretical rate analysis and numerical evidence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 17:36:37 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Willett", "Rebecca", ""]]}, {"id": "2006.03914", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz, Moritz Berger", "title": "Non Proportional Odds Models are Widely Dispensable -- Sparser Modeling\n  based on Parametric and Additive Location-Shift Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of location-shift models to find adequate models between the\nproportional odds model and the non-proportional odds model is investigated. It\nis demonstrated that these models are very useful in ordinal modeling. While\nproportional odds models are often too simple, non proportional odds models are\ntypically unnecessary complicated and seem widely dispensable. The class of\nlocation-shift models is also extended to allow for smooth effects. The\nadditive location-shift model contains two functions for each explanatory\nvariable, one for the location and one for dispersion. It is much sparser than\nhard-to-handle additive models with category-specific covariate functions but\nmore flexible than common vector generalized additive models.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 16:44:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Tutz", "Gerhard", ""], ["Berger", "Moritz", ""]]}, {"id": "2006.03933", "submitter": "Jordan Trinka", "authors": "Jordan Trinka, Hossein Haghbin, and Mehdi Maadooliat", "title": "Multivariate Functional Singular Spectrum Analysis Over Different\n  Dimensional Domains", "comments": "28 pages for the paper, 15 pages for the supplementary material, 8\n  figures total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop multivariate functional singular spectrum analysis\n(MFSSA) over different dimensional domains which is the functional extension of\nmultivariate singular spectrum analysis (MSSA). In the following, we provide\nall of the necessary theoretical details supporting the work as well as the\nimplementation strategy that contains the recipes needed for the algorithm. We\nprovide a simulation study showcasing the better performance in reconstruction\naccuracy of a multivariate functional time series (MFTS) signal found using\nMFSSA as compared to other approaches and we give a real data study showing how\nMFSSA enriches analysis using intraday temperature curves and remote sensing\nimages of vegetation. MFSSA is available for use through the Rfssa R package.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:33:29 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Trinka", "Jordan", ""], ["Haghbin", "Hossein", ""], ["Maadooliat", "Mehdi", ""]]}, {"id": "2006.03936", "submitter": "Ranjan Maitra", "authors": "Karin S. Dorman and Ranjan Maitra", "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets", "comments": "16 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:41:36 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 05:32:31 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 20:18:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Dorman", "Karin S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2006.03980", "submitter": "Molei Liu", "authors": "Molei Liu, Eugene Katsevich, Lucas Janson and Aaditya Ramdas", "title": "Fast and Powerful Conditional Randomization Testing via Distillation", "comments": "This paper has been merged with a parallel work arXiv:2006.08482 by\n  Eugene Katsevich and Aaditya Ramdas", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of conditional independence testing: given a response\nY and covariates (X,Z), we test the null hypothesis that Y is independent of X\ngiven Z. The conditional randomization test (CRT) was recently proposed as a\nway to use distributional information about X|Z to exactly (non-asymptotically)\ncontrol Type-I error using any test statistic in any dimensionality without\nassuming anything about Y|(X,Z). This flexibility in principle allows one to\nderive powerful test statistics from complex prediction algorithms while\nmaintaining statistical validity. Yet the direct use of such advanced test\nstatistics in the CRT is prohibitively computationally expensive, especially\nwith multiple testing, due to the CRT's requirement to recompute the test\nstatistic many times on resampled data. We propose the distilled CRT, a novel\napproach to using state-of-the-art machine learning algorithms in the CRT while\ndrastically reducing the number of times those algorithms need to be run,\nthereby taking advantage of their power and the CRT's statistical guarantees\nwithout suffering the usual computational expense. In addition to distillation,\nwe propose a number of other tricks like screening and recycling computations\nto further speed up the CRT without sacrificing its high power and exact\nvalidity. Indeed, we show in simulations that all our proposals combined lead\nto a test that has similar power to the most powerful existing CRT\nimplementations but requires orders of magnitude less computation, making it a\npractical tool even for large data sets. We demonstrate these benefits on a\nbreast cancer dataset by identifying biomarkers related to cancer stage.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 21:20:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 03:46:58 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 06:17:54 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 05:25:29 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Molei", ""], ["Katsevich", "Eugene", ""], ["Janson", "Lucas", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.04038", "submitter": "Roland Matsouaka", "authors": "Yunji Zhou, Roland A. Matsouaka, Laine Thomas", "title": "Propensity score weighting under limited overlap and model\n  misspecification", "comments": "46 pages, 12 figures", "journal-ref": "Statistical Methods in Medical Research. 2020;29(12):3721-3756", "doi": "10.1177/0962280220940334", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score (PS) weighting methods are often used in non-randomized\nstudies to adjust for confounding and assess treatment effects. The most\npopular among them, the inverse probability weighting (IPW), assigns weights\nthat are proportional to the inverse of the conditional probability of a\nspecific treatment assignment, given observed covariates. A key requirement for\nIPW estimation is the positivity assumption, i.e., the PS must be bounded away\nfrom 0 and 1. In practice, violations of the positivity assumption often\nmanifest by the presence of limited overlap in the PS distributions between\ntreatment groups. When these practical violations occur, a small number of\nhighly influential IPW weights may lead to unstable IPW estimators, with biased\nestimates and large variances. To mitigate these issues, a number of\nalternative methods have been proposed, including IPW trimming, overlap weights\n(OW), matching weights (MW), and entropy weights (EW). Because OW, MW, and EW\ntarget the population for whom there is equipoise (and with adequate overlap)\nand their estimands depend on the true PS, a common criticism is that these\nestimators may be more sensitive to misspecifications of the PS model. In this\npaper, we conduct extensive simulation studies to compare the performances of\nIPW and IPW trimming against those of OW, MW, and EW under limited overlap and\nmisspecified propensity score models. Across the wide range of scenarios we\nconsidered, OW, MW, and EW consistently outperform IPW in terms of bias, root\nmean squared error, and coverage probability.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 04:08:20 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 03:46:31 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 23:16:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhou", "Yunji", ""], ["Matsouaka", "Roland A.", ""], ["Thomas", "Laine", ""]]}, {"id": "2006.04141", "submitter": "Alberto Sorrentino", "authors": "Alessandro Viani, Gianvittorio Luria, Harald Bornfleth and Alberto\n  Sorrentino", "title": "Where Bayes tweaks Gauss: Conditionally Gaussian priors for stable\n  multi-dipole estimation", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a very simple yet powerful generalization of a previously\ndescribed model and algorithm for estimation of multiple dipoles from\nmagneto/electro-encephalographic data. Specifically, the generalization\nconsists in the introduction of a log-uniform hyperprior on the standard\ndeviation of a set of conditionally linear/Gaussian variables. We use numerical\nsimulations and an experimental dataset to show that the approximation to the\nposterior distribution remains extremely stable under a wide range of values of\nthe hyperparameter, virtually removing the dependence on the hyperparameter.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:05:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Viani", "Alessandro", ""], ["Luria", "Gianvittorio", ""], ["Bornfleth", "Harald", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "2006.04220", "submitter": "Tongrong Wang", "authors": "Tongrong Wang and Huiman X. Barnhart", "title": "Overall Agreement for Multiple Raters with Replicated Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple raters are often needed to be used interchangeably in practice for\nmeasurement or evaluation. Assessing agreement among these multiple raters via\nagreement indices are necessary before their participation. While the\nintuitively appealing agreement indices such as coverage probability and total\ndeviation index, and relative area under coverage probability curve, have been\nextended for assessing overall agreement among multiple raters, these\nextensions have limitations. The existing overall agreement indices either\nrequire normality and homogeneity assumptions or did not preserve the intuitive\ninterpretation of the indices originally defined for two raters. In this paper,\nwe propose a new set of overall agreement indices based on maximum pairwise\ndifferences among all raters. The proposed new overall coverage probability,\noverall total deviation index and relative area under overall coverage\nprobability curve retain the original intuitive interpretation from the\npairwise version. Without making any distributional assumption, we also propose\na new unified nonparametric estimation and inference approach for the overall\nindices based on generalized estimating equations that can accommodate\nreplications made by the same rater. Under mild assumptions, the proposed\nvariance estimator is shown to achieve efficiency bound under independent\nworking correlation matrix. Simulation studies under different scenarios are\nconducted to assess the performance of the proposed estimation and inference\napproach with and without replications. We illustrate the methodology by using\na blood pressure data with three raters who made three replications on each\nsubjects.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 18:23:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Tongrong", ""], ["Barnhart", "Huiman X.", ""]]}, {"id": "2006.04269", "submitter": "Yan Liu", "authors": "Campbell R. Harvey and Yan Liu", "title": "False (and Missed) Discoveries in Financial Economics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing plagues many important questions in finance such as fund and\nfactor selection. We propose a new way to calibrate both Type I and Type II\nerrors. Next, using a double-bootstrap method, we establish a t-statistic\nhurdle that is associated with a specific false discovery rate (e.g., 5%). We\nalso establish a hurdle that is associated with a certain acceptable ratio of\nmisses to false discoveries (Type II error scaled by Type I error), which\neffectively allows for differential costs of the two types of mistakes.\nEvaluating current methods, we find that they lack power to detect\noutperforming managers.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 21:08:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Harvey", "Campbell R.", ""], ["Liu", "Yan", ""]]}, {"id": "2006.04292", "submitter": "Yaniv Romano", "authors": "Yaniv Romano and Stephen Bates and Emmanuel J. Cand\\`es", "title": "Achieving Equalized Odds by Resampling Sensitive Attributes", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a flexible framework for learning predictive models that\napproximately satisfy the equalized odds notion of fairness. This is achieved\nby introducing a general discrepancy functional that rigorously quantifies\nviolations of this criterion. This differentiable functional is used as a\npenalty driving the model parameters towards equalized odds. To rigorously\nevaluate fitted models, we develop a formal hypothesis test to detect whether a\nprediction rule violates this property, the first such test in the literature.\nBoth the model fitting and hypothesis testing leverage a resampled version of\nthe sensitive attribute obeying equalized odds, by construction. We demonstrate\nthe applicability and validity of the proposed framework both in regression and\nmulti-class classification problems, reporting improved performance over\nstate-of-the-art methods. Lastly, we show how to incorporate techniques for\nequitable uncertainty quantification---unbiased for each group under study---to\ncommunicate the results of the data analysis in exact terms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 00:18:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Romano", "Yaniv", ""], ["Bates", "Stephen", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2006.04347", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith, Aaditya Ramdas", "title": "Confidence sequences for sampling without replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical tasks involve sampling sequentially without replacement (WoR)\nfrom a finite population of size $N$, in an attempt to estimate some parameter\n$\\theta^\\star$. Accurately quantifying uncertainty throughout this process is a\nnontrivial task, but is necessary because it often determines when we stop\ncollecting samples and confidently report a result. We present a suite of tools\nfor designing confidence sequences (CS) for $\\theta^\\star$. A CS is a sequence\nof confidence sets $(C_n)_{n=1}^N$, that shrink in size, and all contain\n$\\theta^\\star$ simultaneously with high probability. We present a generic\napproach to constructing a frequentist CS using Bayesian tools, based on the\nfact that the ratio of a prior to the posterior at the ground truth is a\nmartingale. We then present Hoeffding- and empirical-Bernstein-type\ntime-uniform CSs and fixed-time confidence intervals for sampling WoR, which\nimprove on previous bounds in the literature and explicitly quantify the\nbenefit of WoR sampling.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 04:30:25 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:31:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 02:42:04 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 15:45:00 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.04381", "submitter": "Aijun Zhang", "authors": "Kun Kuang, Hengtao Zhang, Fei Wu, Yueting Zhuang and Aijun Zhang", "title": "Balance-Subsampled Stable Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, it is commonly assumed that training and test data share\nthe same population distribution. However, this assumption is often violated in\npractice because the sample selection bias may induce the distribution shift\nfrom training data to test data. Such a model-agnostic distribution shift\nusually leads to prediction instability across unknown test data. In this\npaper, we propose a novel balance-subsampled stable prediction (BSSP) algorithm\nbased on the theory of fractional factorial design. It isolates the clear\neffect of each predictor from the confounding variables. A design-theoretic\nanalysis shows that the proposed method can reduce the confounding effects\namong predictors induced by the distribution shift, hence improve both the\naccuracy of parameter estimation and prediction stability. Numerical\nexperiments on both synthetic and real-world data sets demonstrate that our\nBSSP algorithm significantly outperforms the baseline methods for stable\nprediction across unknown test data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 07:01:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kuang", "Kun", ""], ["Zhang", "Hengtao", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""], ["Zhang", "Aijun", ""]]}, {"id": "2006.04536", "submitter": "Rui Yao", "authors": "Rui Yao, Shlomo Bekhor", "title": "Experiments on route choice set generation using a large GPS trajectory\n  set", "comments": "hEART 2020 : 9th Symposium of the European Association for Research\n  in Transportation", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several route choice models developed in the literature were based on a\nrelatively small number of observations. With the extensive use of tracking\ndevices in recent surveys, there is a possibility to obtain insights with\nrespect to the traveler's choice behavior. In this paper, different path\ngeneration algorithms are evaluated using a large GPS trajectory dataset. The\ndataset contains 6,000 observations from Tel-Aviv metropolitan area. An initial\nanalysis is performed by generating a single route based on the shortest path.\nAlmost 60% percent of the 6,000 observations can be covered (assuming a\nthreshold of 80% overlap) using a single path. This result significantly\ncontrasts previous literature findings. Link penalty, link elimination,\nsimulation and via-node methods are applied to generate route sets, and the\nconsistency of the algorithms are compared. A modified link penalty method,\nwhich accounts for preference of using higher hierarchical roads, provides a\nroute set with 97% coverage (80% overlap threshold). The via-node method\nproduces route set with satisfying coverage, and generates routes that are more\nheterogeneous (in terms number of links and routes ratio).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:44:49 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yao", "Rui", ""], ["Bekhor", "Shlomo", ""]]}, {"id": "2006.04613", "submitter": "Claude Renaux", "authors": "Christoph Schultheiss, Claude Renaux, Peter B\\\"uhlmann", "title": "Multicarving for high-dimensional post-selection inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider post-selection inference for high-dimensional (generalized)\nlinear models. Data carving (Fithian et al., 2014) is a promising technique to\nperform this task. However, it suffers from the instability of the model\nselector and hence, may lead to poor replicability, especially in\nhigh-dimensional settings. We propose the multicarve method inspired by\nmultisplitting to improve upon stability and replicability. Furthermore, we\nextend existing concepts to group inference and illustrate the applicability of\nthe methodology also for generalized linear models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:10:37 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 11:35:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Schultheiss", "Christoph", ""], ["Renaux", "Claude", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2006.04632", "submitter": "Seongoh Park Dr", "authors": "Seongoh Park, Xinlei Wang, Johan Lim", "title": "Estimating High-dimensional Covariance and Precision Matrices under\n  General Missing Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sample covariance matrix $\\boldsymbol{S}$ of completely observed data is\nthe key statistic in a large variety of multivariate statistical procedures,\nsuch as structured covariance/precision matrix estimation, principal component\nanalysis, and testing of equality of mean vectors. However, when the data are\npartially observed, the sample covariance matrix from the available data is\nbiased and does not provide valid multivariate procedures. To correct the bias,\na simple adjustment method called inverse probability weighting (IPW) has been\nused in previous research, yielding the IPW estimator. The estimator plays the\nrole of $\\boldsymbol{S}$ in the missing data context so that it can be plugged\ninto off-the-shelf multivariate procedures. However, theoretical properties\n(e.g. concentration) of the IPW estimator have been only established under very\nsimple missing structures; every variable of each sample is independently\nsubject to missing with equal probability.\n  We investigate the deviation of the IPW estimator when observations are\npartially observed under general missing dependency. We prove the optimal\nconvergence rate $O_p(\\sqrt{\\log p / n})$ of the IPW estimator based on the\nelement-wise maximum norm. We also derive similar deviation results even when\nimplicit assumptions (known mean and/or missing probability) are relaxed. The\noptimal rate is especially crucial in estimating a precision matrix, because of\nthe \"meta-theorem\" that claims the rate of the IPW estimator governs that of\nthe resulting precision matrix estimator. In the simulation study, we discuss\nnon-positive semi-definiteness of the IPW estimator and compare the estimator\nwith imputation methods, which are practically important.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:30:33 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 13:50:09 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 12:56:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Park", "Seongoh", ""], ["Wang", "Xinlei", ""], ["Lim", "Johan", ""]]}, {"id": "2006.04656", "submitter": "Frank R\\\"ottger", "authors": "Fritjof Freise, Ulrike Gra{\\ss}hoff, Frank R\\\"ottger, Rainer Schwabe", "title": "$ D $-optimal designs for Poisson regression with synergetic interaction\n  effect", "comments": "21 pages, 6 figures", "journal-ref": "TEST (2021)", "doi": "10.1007/s11749-020-00752-w", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize $D$-optimal designs in the two-dimensional Poisson regression\nmodel with synergetic interaction and provide an explicit proof. The proof is\nbased on the idea of reparameterization of the design region in terms of\ncontours of constant intensity. This approach leads to a substantial reduction\nof complexity as properties of the sensitivity can be treated along and across\nthe contours separately. Furthermore, some extensions of this result to higher\ndimensions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:01:41 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Freise", "Fritjof", ""], ["Gra\u00dfhoff", "Ulrike", ""], ["R\u00f6ttger", "Frank", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2006.04699", "submitter": "Ian Grooms", "authors": "Gregor Robinson and Ian Grooms", "title": "A hybrid particle-ensemble Kalman filter for problems with medium\n  nonlinearity", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0248266", "report-no": null, "categories": "stat.ME physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid particle ensemble Kalman filter is developed for problems with\nmedium non-Gaussianity, i.e. problems where the prior is very non-Gaussian but\nthe posterior is approximately Gaussian. Such situations arise, e.g., when\nnonlinear dynamics produce a non-Gaussian forecast but a tight Gaussian\nlikelihood leads to a nearly-Gaussian posterior. The hybrid filter starts by\nfactoring the likelihood. First the particle filter assimilates the\nobservations with one factor of the likelihood to produce an intermediate prior\nthat is close to Gaussian, and then the ensemble Kalman filter completes the\nassimilation with the remaining factor. How the likelihood gets split between\nthe two stages is determined in such a way to ensure that the particle filter\navoids collapse, and particle degeneracy is broken by a mean-preserving random\northogonal transformation. The hybrid is tested in a simple two-dimensional\n(2D) problem and a multiscale system of ODEs motivated by the Lorenz-`96 model.\nIn the 2D problem it outperforms both a pure particle filter and a pure\nensemble Kalman filter, and in the multiscale Lorenz-`96 model it is shown to\noutperform a pure ensemble Kalman filter, provided that the ensemble size is\nlarge enough.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:56:08 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 23:21:15 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Robinson", "Gregor", ""], ["Grooms", "Ian", ""]]}, {"id": "2006.04709", "submitter": "Qiming Du", "authors": "Qiming Du, G\\'erard Biau, Fran\\c{c}ois Petit and Rapha\\\"el Porcher", "title": "Wasserstein Random Forests and Applications in Heterogeneous Treatment\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new insights into causal inference in the context of Heterogeneous\nTreatment Effects by proposing natural variants of Random Forests to estimate\nthe key conditional distributions. To achieve this, we recast Breiman's\noriginal splitting criterion in terms of Wasserstein distances between\nempirical measures. This reformulation indicates that Random Forests are well\nadapted to estimate conditional distributions and provides a natural extension\nof the algorithm to multivariate outputs. Following the philosophy of Breiman's\nconstruction, we propose some variants of the splitting rule that are\nwell-suited to the conditional distribution estimation problem. Some\npreliminary theoretical connections are established along with various\nnumerical experiments, which show how our approach may help to conduct more\ntransparent causal inference in complex situations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:08:10 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:46:07 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 09:10:38 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Du", "Qiming", ""], ["Biau", "G\u00e9rard", ""], ["Petit", "Fran\u00e7ois", ""], ["Porcher", "Rapha\u00ebl", ""]]}, {"id": "2006.04840", "submitter": "Simon Tavar\\'e", "authors": "Poly H. da Silva, Arash Jamshidpey, Simon Tavar\\'e", "title": "Random derangements and the Ewens Sampling Formula", "comments": "16 pages, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study derangements of $\\{1,2,\\ldots,n\\}$ under the Ewens distribution with\nparameter $\\theta$. We give the moments and marginal distributions of the cycle\ncounts, the number of cycles, and asymptotic distributions for large $n$. We\ndevelop a $\\{0,1\\}$-valued non-homogeneous Markov chain with the property that\nthe counts of lengths of spacings between the 1s have the derangement\ndistribution. This chain, an analog of the so-called Feller Coupling, provides\na simple way to simulate derangements in time independent of $\\theta$ for a\ngiven $n$ and linear in the size of the derangement.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:04:16 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["da Silva", "Poly H.", ""], ["Jamshidpey", "Arash", ""], ["Tavar\u00e9", "Simon", ""]]}, {"id": "2006.04877", "submitter": "Vahid Partovi Nia", "authors": "Vahid Partovi Nia, Xinlin Li, Masoud Asgharian, Shoubo Hu, Zhitang\n  Chen, Yanhui Geng", "title": "Clustering Causal Additive Noise Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive noise models are commonly used to infer the causal direction for a\ngiven set of observed data. Most causal models assume a single homogeneous\npopulation. However, observations may be collected under different conditions\nin practice. Such data often require models that can accommodate possible\nheterogeneity caused by different conditions under which data have been\ncollected. We propose a clustering algorithm inspired by the $k$-means\nalgorithm, but with unknown $k$. Using the proposed algorithm, both the labels\nand the number of components are estimated from the collected data. The\nestimated labels are used to adjust the causal direction test statistic. The\nadjustment significantly improves the performance of the test statistic in\nidentifying the correct causal direction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:59:14 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Nia", "Vahid Partovi", ""], ["Li", "Xinlin", ""], ["Asgharian", "Masoud", ""], ["Hu", "Shoubo", ""], ["Chen", "Zhitang", ""], ["Geng", "Yanhui", ""]]}, {"id": "2006.04917", "submitter": "Haakon Bakka", "authors": "Haakon Bakka, Elias Krainski, David Bolin, H{\\aa}vard Rue, Finn\n  Lindgren", "title": "The diffusion-based extension of the Mat\\'ern field to space-time", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mat\\' ern field is the most well known family of covariance functions\nused for Gaussian processes in spatial models. We build upon the original\nresearch of Whittle (1953, 1964) and develop the diffusion-based extension of\nthe Mat\\' ern field to space-time (DEMF). We argue that this diffusion-based\nextension is the natural extension of these processes, due to the strong\nphysical interpretation. The corresponding non-separable spatio-temporal\nGaussian process is a spatio-temporal analogue of the Mat\\' ern field, with\nrange parameters in space and time, smoothness parameters in space and time,\nand a separability parameter. We provide a sparse representation based on\nfinite element methods that is well suited for statistical inference.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 20:21:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bakka", "Haakon", ""], ["Krainski", "Elias", ""], ["Bolin", "David", ""], ["Rue", "H\u00e5vard", ""], ["Lindgren", "Finn", ""]]}, {"id": "2006.05002", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang and Qihua Wang", "title": "Determination and estimation of optimal quarantine duration for\n  infectious diseases with application to data analysis of COVID-19", "comments": null, "journal-ref": "biometrics,2021", "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quarantine measure is a commonly used non-pharmaceutical intervention during\nthe outbreak of infectious diseases. A key problem for implementing quarantine\nmeasure is to determine the duration of quarantine. In this paper, a policy\nwith optimal quarantine duration is developed. The policy suggests different\nquarantine durations for every individual with different characteristic. The\npolicy is optimal in the sense that it minimizes the average quarantine\nduration of uninfected people with the constraint that the probability of\nsymptom presentation for infected people attains the given value closing to 1.\nThe optimal solution for the quarantine duration is obtained and estimated by\nsome statistic methods with application to analyzing COVID-19 data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 01:11:51 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 10:03:38 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 07:41:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Wang", "Ruoyu", ""], ["Wang", "Qihua", ""]]}, {"id": "2006.05026", "submitter": "Cong Shen", "authors": "Cong Shen, Zhiyang Wang, Sofia S. Villar, and Mihaela van der Schaar", "title": "Learning for Dose Allocation in Adaptive Clinical Trials with Safety\n  Constraints", "comments": "Accepted to the 37th International Conference on Machine Learning\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I dose-finding trials are increasingly challenging as the relationship\nbetween efficacy and toxicity of new compounds (or combination of them) becomes\nmore complex. Despite this, most commonly used methods in practice focus on\nidentifying a Maximum Tolerated Dose (MTD) by learning only from toxicity\nevents. We present a novel adaptive clinical trial methodology, called Safe\nEfficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the\ncumulative efficacies while satisfying the toxicity safety constraint with high\nprobability. We evaluate performance objectives that have operational meanings\nin practical clinical trials, including cumulative efficacy,\nrecommendation/allocation success probabilities, toxicity violation\nprobability, and sample efficiency. An extended SEEDA-Plateau algorithm that is\ntailored for the increase-then-plateau efficacy behavior of molecularly\ntargeted agents (MTA) is also presented. Through numerical experiments using\nboth synthetic and real-world datasets, we show that SEEDA outperforms\nstate-of-the-art clinical trial designs by finding the optimal dose with higher\nsuccess rate and fewer patients.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:06:45 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:41:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Shen", "Cong", ""], ["Wang", "Zhiyang", ""], ["Villar", "Sofia S.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2006.05371", "submitter": "Harrison Zhu", "authors": "Harrison Zhu, Xing Liu, Ruya Kang, Zhichao Shen, Seth Flaxman and\n  Fran\\c{c}ois-Xavier Briol", "title": "Bayesian Probabilistic Numerical Integration with Tree-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian quadrature (BQ) is a method for solving numerical integration\nproblems in a Bayesian manner, which allows users to quantify their uncertainty\nabout the solution. The standard approach to BQ is based on a Gaussian process\n(GP) approximation of the integrand. As a result, BQ is inherently limited to\ncases where GP approximations can be done in an efficient manner, thus often\nprohibiting very high-dimensional or non-smooth target functions. This paper\nproposes to tackle this issue with a new Bayesian numerical integration\nalgorithm based on Bayesian Additive Regression Trees (BART) priors, which we\ncall BART-Int. BART priors are easy to tune and well-suited for discontinuous\nfunctions. We demonstrate that they also lend themselves naturally to a\nsequential design setting and that explicit convergence rates can be obtained\nin a variety of settings. The advantages and disadvantages of this new\nmethodology are highlighted on a set of benchmark tests including the Genz\nfunctions, and on a Bayesian survey design problem.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:04:00 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 11:37:27 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhu", "Harrison", ""], ["Liu", "Xing", ""], ["Kang", "Ruya", ""], ["Shen", "Zhichao", ""], ["Flaxman", "Seth", ""], ["Briol", "Fran\u00e7ois-Xavier", ""]]}, {"id": "2006.05381", "submitter": "Yifan Cui", "authors": "Yifan Cui, Jan Hannig", "title": "A fiducial approach to nonparametric deconvolution problem: discrete\n  case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial inference, as generalized by Hannig et al. (2016), is applied to\nnonparametric g-modeling (Efron, 2016) in the discrete case. We propose a\ncomputationally efficient algorithm to sample from the fiducial distribution,\nand use generated samples to construct point estimates and confidence\nintervals. We study the theoretical properties of the fiducial distribution and\nperform extensive simulations in various scenarios. The proposed approach gives\nrise to surprisingly good statistical performance in terms of the mean squared\nerror of point estimators and coverage of confidence intervals. Furthermore, we\napply the proposed fiducial method to estimate the probability of each\nsatellite site being malignant using gastric adenocarcinoma data with 844\npatients (Efron, 2016).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:29:24 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 23:53:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 00:33:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cui", "Yifan", ""], ["Hannig", "Jan", ""]]}, {"id": "2006.05451", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon, Amy H. Herring, David B. Dunson", "title": "A generalized Bayes framework for probabilistic clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss-based clustering methods, such as k-means and its variants, are standard\ntools for finding groups in data. However, the lack of quantification of\nuncertainty in the estimated clusters is a disadvantage. Model-based clustering\nbased on mixture models provides an alternative, but such methods face\ncomputational problems and large sensitivity to the choice of kernel. This\narticle proposes a generalized Bayes framework that bridges between these two\nparadigms through the use of Gibbs posteriors. In conducting Bayesian updating,\nthe log likelihood is replaced by a loss function for clustering, leading to a\nrich family of clustering methods. The Gibbs posterior represents a coherent\nupdating of Bayesian beliefs without needing to specify a likelihood for the\ndata, and can be used for characterizing uncertainty in clustering. We consider\nlosses based on Bregman divergence and pairwise similarities, and develop\nefficient deterministic algorithms for point estimation along with sampling\nalgorithms for uncertainty quantification. Several existing clustering\nalgorithms, including k-means, can be interpreted as generalized Bayes\nestimators under our framework, and hence we provide a method of uncertainty\nquantification for these approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 18:49:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Rigon", "Tommaso", ""], ["Herring", "Amy H.", ""], ["Dunson", "David B.", ""]]}, {"id": "2006.05455", "submitter": "Jie Ren", "authors": "Jie Ren, Fei Zhou, Xiaoxi Li, Shuangge Ma, Yu Jiang and Cen Wu", "title": "Robust Bayesian variable selection for gene-environment interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene-environment (G$\\times$E) interactions have important implications to\nelucidate the etiology of complex diseases beyond the main genetic and\nenvironmental effects. Outliers and data contamination in disease phenotypes of\nG$\\times$E studies have been commonly encountered, leading to the development\nof a broad spectrum of robust regularization methods. Nevertheless, within the\nBayesian framework, the issue has not been taken care of in existing studies.\nWe develop a fully Bayesian robust variable selection method for G$\\times$E\ninteraction studies. The proposed Bayesian method can effectively accommodate\nheavy-tailed errors and outliers in the response variable while conducting\nvariable selection by accounting for structural sparsity. In particular, for\nthe robust sparse group selection, the spike-and-slab priors have been imposed\non both individual and group levels to identify important main and interaction\neffects robustly. An efficient Gibbs sampler has been developed to facilitate\nfast computation. Extensive simulation studies and analysis of both the\ndiabetes data with SNP measurements from the Nurses' Health Study and TCGA\nmelanoma data with gene expression measurements demonstrate the superior\nperformance of the proposed method over multiple competing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 18:52:47 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ren", "Jie", ""], ["Zhou", "Fei", ""], ["Li", "Xiaoxi", ""], ["Ma", "Shuangge", ""], ["Jiang", "Yu", ""], ["Wu", "Cen", ""]]}, {"id": "2006.05466", "submitter": "Chul Moon", "authors": "Chul Moon, Nicole A. Lazar", "title": "Hypothesis Testing for Shapes using Vectorized Persistence Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Topological data analysis involves the statistical characterization of the\nshape of data. Persistent homology is a primary tool of topological data\nanalysis, which can be used to analyze those topological features and perform\nstatistical inference. In this paper, we present a two-stage hypothesis test\nfor vectorized persistence diagrams. The first stage filters elements in the\nvectorized persistence diagrams to reduce false positives. The second stage\nconsists of multiple hypothesis tests, with false positives controlled by false\ndiscovery rates. We demonstrate applications of the proposed procedure on\nsimulated point clouds and three-dimensional rock image data. Our results show\nthat the proposed hypothesis tests can provide flexible and informative\ninferences on the shape of data with lower computational cost compared to the\npermutation test.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:20:54 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 18:07:03 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Moon", "Chul", ""], ["Lazar", "Nicole A.", ""]]}, {"id": "2006.05553", "submitter": "Yao-Hung Tsai", "authors": "Yao-Hung Hubert Tsai, Han Zhao, Makoto Yamada, Louis-Philippe Morency,\n  Ruslan Salakhutdinov", "title": "Neural Methods for Point-wise Dependency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, the neural estimation of mutual information (MI) has\ndemonstrated the empirical success of modeling expected dependency between\nhigh-dimensional random variables. However, MI is an aggregate statistic and\ncannot be used to measure point-wise dependency between different events. In\nthis work, instead of estimating the expected dependency, we focus on\nestimating point-wise dependency (PD), which quantitatively measures how likely\ntwo outcomes co-occur. We show that we can naturally obtain PD when we are\noptimizing MI neural variational bounds. However, optimizing these bounds is\nchallenging due to its large variance in practice. To address this issue, we\ndevelop two methods (free of optimizing MI variational bounds): Probabilistic\nClassifier and Density-Ratio Fitting. We demonstrate the effectiveness of our\napproaches in 1) MI estimation, 2) self-supervised representation learning, and\n3) cross-modal retrieval task.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:26:15 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 01:55:04 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 07:57:18 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 03:55:10 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tsai", "Yao-Hung Hubert", ""], ["Zhao", "Han", ""], ["Yamada", "Makoto", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2006.05581", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou and Yuan Ji", "title": "Semiparametric Bayesian Inference for the Transmission Dynamics of\n  COVID-19 with a State-Space Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of Coronavirus Disease 2019 (COVID-19) is an ongoing pandemic\naffecting over 200 countries and regions. Inference about the transmission\ndynamics of COVID-19 can provide important insights into the speed of disease\nspread and the effects of mitigation policies. We develop a novel Bayesian\napproach to such inference based on a probabilistic compartmental model using\ndata of daily confirmed COVID-19 cases. In particular, we consider a\nprobabilistic extension of the classical susceptible-infectious-recovered\nmodel, which takes into account undocumented infections and allows the\nepidemiological parameters to vary over time. We estimate the disease\ntransmission rate via a Gaussian process prior, which captures nonlinear\nchanges over time without the need of specific parametric assumptions. We\nutilize a parallel-tempering Markov chain Monte Carlo algorithm to efficiently\nsample from the highly correlated posterior space. Predictions for future\nobservations are done by sampling from their posterior predictive\ndistributions. Performance of the proposed approach is assessed using simulated\ndatasets. Finally, our approach is applied to COVID-19 data from four states of\nthe United States: Washington, New York, California, and Illinois. An R package\nBaySIR is made available at https://github.com/tianjianzhou/BaySIR for the\npublic to conduct independent analysis or reproduce the results in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:46:41 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:00:15 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.05591", "submitter": "Ramesh Johari", "authors": "Peter Glynn, Ramesh Johari, Mohammad Rasouli", "title": "Adaptive Experimental Design with Temporal Interference: A Maximum\n  Likelihood Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose an online platform wants to compare a treatment and control policy,\ne.g., two different matching algorithms in a ridesharing system, or two\ndifferent inventory management algorithms in an online retail site. Standard\nrandomized controlled trials are typically not feasible, since the goal is to\nestimate policy performance on the entire system. Instead, the typical current\npractice involves dynamically alternating between the two policies for fixed\nlengths of time, and comparing the average performance of each over the\nintervals in which they were run as an estimate of the treatment effect.\nHowever, this approach suffers from *temporal interference*: one algorithm\nalters the state of the system as seen by the second algorithm, biasing\nestimates of the treatment effect. Further, the simple non-adaptive nature of\nsuch designs implies they are not sample efficient.\n  We develop a benchmark theoretical model in which to study optimal\nexperimental design for this setting. We view testing the two policies as the\nproblem of estimating the steady state difference in reward between two unknown\nMarkov chains (i.e., policies). We assume estimation of the steady state reward\nfor each chain proceeds via nonparametric maximum likelihood, and search for\nconsistent (i.e., asymptotically unbiased) experimental designs that are\nefficient (i.e., asymptotically minimum variance). Characterizing such designs\nis equivalent to a Markov decision problem with a minimum variance objective;\nsuch problems generally do not admit tractable solutions. Remarkably, in our\nsetting, using a novel application of classical martingale analysis of Markov\nchains via Poisson's equation, we characterize efficient designs via a succinct\nconvex optimization problem. We use this characterization to propose a\nconsistent, efficient online experimental design that adaptively samples the\ntwo Markov chains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:07:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Glynn", "Peter", ""], ["Johari", "Ramesh", ""], ["Rasouli", "Mohammad", ""]]}, {"id": "2006.05690", "submitter": "Juan L Gamella", "authors": "Juan L Gamella and Christina Heinze-Deml", "title": "Active Invariant Causal Prediction: Experiment Selection through\n  Stability", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental difficulty of causal learning is that causal models can\ngenerally not be fully identified based on observational data only.\nInterventional data, that is, data originating from different experimental\nenvironments, improves identifiability. However, the improvement depends\ncritically on the target and nature of the interventions carried out in each\nexperiment. Since in real applications experiments tend to be costly, there is\na need to perform the right interventions such that as few as possible are\nrequired. In this work we propose a new active learning (i.e. experiment\nselection) framework (A-ICP) based on Invariant Causal Prediction (ICP) (Peters\net al., 2016). For general structural causal models, we characterize the effect\nof interventions on so-called stable sets, a notion introduced by (Pfister et\nal., 2019). We leverage these results to propose several intervention selection\npolicies for A-ICP which quickly reveal the direct causes of a response\nvariable in the causal graph while maintaining the error control inherent in\nICP. Empirically, we analyze the performance of the proposed policies in both\npopulation and finite-regime experiments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:07:27 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:23:02 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gamella", "Juan L", ""], ["Heinze-Deml", "Christina", ""]]}, {"id": "2006.05748", "submitter": "Lizanne Raubenheimer", "authors": "Andr\\'ehette Verster (1) and Lizanne Raubenheimer (2) ((1) Department\n  of Mathematical Statistics and Actuarial Science, University of the Free\n  State, Bloemfontein, South Africa, (2) School of Mathematical and Statistical\n  Sciences, North-West University, Potchefstroom, South Africa)", "title": "A different approach for choosing a threshold in peaks over threshold", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract In Extreme Value methodology the choice of threshold plays an\nimportant role in efficient modelling of observations exceeding the threshold.\nThe threshold must be chosen high enough to ensure an unbiased extreme value\nindex but choosing the threshold too high results in uncontrolled variances.\nThis paper investigates a generalized model that can assist in the choice of\noptimal threshold values in the \\gamma positive domain. A Bayesian approach is\nconsidered by deriving a posterior distribution for the unknown generalized\nparameter. Using the properties of the posterior distribution allows for a\nmethod to choose an optimal threshold without visual inspection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:34:56 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Verster", "Andr\u00e9hette", ""], ["Raubenheimer", "Lizanne", ""]]}, {"id": "2006.05750", "submitter": "Christoph Berninger", "authors": "Christoph Berninger, Almond St\\\"ocker, David R\\\"ugamer", "title": "A Bayesian Time-Varying Autoregressive Model for Improved Short- and\n  Long-Term Prediction", "comments": "Revised Introduction, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the application to German interest rates, we propose a\ntimevarying autoregressive model for short and long term prediction of time\nseries that exhibit a temporary non-stationary behavior but are assumed to mean\nrevert in the long run. We use a Bayesian formulation to incorporate prior\nassumptions on the mean reverting process in the model and thereby regularize\npredictions in the far future. We use MCMC-based inference by deriving relevant\nfull conditional distributions and employ a Metropolis-Hastings within Gibbs\nSampler approach to sample from the posterior (predictive) distribution. In\ncombining data-driven short term predictions with long term distribution\nassumptions our model is competitive to the existing methods in the short\nhorizon while yielding reasonable predictions in the long run. We apply our\nmodel to interest rate data and contrast the forecasting performance to the one\nof a 2-Additive-Factor Gaussian model as well as to the predictions of a\ndynamic Nelson-Siegel model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:39:07 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 18:24:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Berninger", "Christoph", ""], ["St\u00f6cker", "Almond", ""], ["R\u00fcgamer", "David", ""]]}, {"id": "2006.05788", "submitter": "Chiara Bocci", "authors": "Chiara Bocci, Laura Grassini, Emilia Rocco", "title": "A multiple inflated negative binomial hurdle regression model: analysis\n  of the Italians' tourism behaviour during the Great Recession", "comments": null, "journal-ref": null, "doi": "10.1007/s10260-020-00542-6", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse tourism behaviour of Italian residents in the period covering the\n2008 Great Recession. Using the Trips of Italian Residents in Italy and Abroad\nquarterly survey, carried out by the Italian National Institute of Statistics,\nwe investigate whether and how the economic recession has affected the total\nnumber of overnight stays. The response variable is the result of a two-stage\ndecision process: first we choose to take a holiday, then for how long.\nMoreover, since the number of overnight stays is typically concentrated on\nspecific lengths (week-end, week, fortnight) we observe multiple peculiar\nspikes in its distribution. To take into account these two distinctive\ncharacteristics, we generalise the usual hurdle regression model by specifying\na multiple inflated truncated negative binomial distribution for the positive\nresponses. Results show that the economic recession impacted negatively on both\ncomponents of the decision process and that, by controlling for the inflated\nnature of the response variable's distribution, the proposed formulation\nprovides a better representation of the Italians' tourism behaviour in\ncomparison with non-inflated hurdle models. Given this, we believe that our\nmodel can be a useful tool for policy makers who are trying to forecast the\neffects of new targeted policies to support tourism economy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 11:58:34 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bocci", "Chiara", ""], ["Grassini", "Laura", ""], ["Rocco", "Emilia", ""]]}, {"id": "2006.05795", "submitter": "Yu Du", "authors": "Yongming Qu, Yu Du, Ying Zhang, Lei Shen", "title": "Understanding and adjusting the selection bias from a proof-of-concept\n  study to a more confirmatory study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been noticed that the efficacy observed in small early phase\nstudies is generally better than that observed in later larger studies.\nHistorically, the inflation of the efficacy results from early proof-of-concept\nstudies is either ignored, or adjusted empirically using a frequentist or\nBayesian approach. In this article, we systematically explained the underlying\nreason for the inflation of efficacy results in small early phase studies from\nthe perspectives of measurement error models and selection bias. A systematic\nmethod was built to adjust the early phase study results from both frequentist\nand Bayesian perspectives. A hierarchical model was proposed to estimate the\ndistribution of the efficacy for a portfolio of compounds, which can serve as\nthe prior distribution for the Bayesian approach. We showed through theory that\nthe systematic adjustment provides an unbiased estimator for the true mean\nefficacy for a portfolio of compounds. The adjustment was applied to paired\ndata for the efficacy in early small and later larger studies for a set of\ncompounds in diabetes and immunology. After the adjustment, the bias in the\nearly phase small studies seems to be diminished.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:16:39 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Qu", "Yongming", ""], ["Du", "Yu", ""], ["Zhang", "Ying", ""], ["Shen", "Lei", ""]]}, {"id": "2006.06010", "submitter": "Ava Khamseh", "authors": "Sjoerd Viktor Beentjes, Ava Khamseh", "title": "Higher-order interactions in statistical physics and machine learning: A\n  model-independent solution to the inverse problem at equilibrium", "comments": "25 pages, 25 figures. Comments welcome", "journal-ref": null, "doi": "10.1103/PhysRevE.102.053314", "report-no": null, "categories": "stat.ME cond-mat.stat-mech hep-lat physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring pair-wise and higher-order interactions in complex\nsystems involving large numbers of interacting variables, from observational\ndata, is fundamental to many fields. Known to the statistical physics community\nas the inverse problem, it has become accessible in recent years due to real\nand simulated 'big' data being generated. Current approaches to the inverse\nproblem rely on parametric assumptions, physical approximations, e.g.\nmean-field theory, and ignoring higher-order interactions which may lead to\nbiased or incorrect estimates. We bypass these shortcomings using a\ncross-disciplinary approach and demonstrate that none of these assumptions and\napproximations are necessary: We introduce a universal, model-independent, and\nfundamentally unbiased estimator of all-order symmetric interactions, via the\nnon-parametric framework of Targeted Learning, a subfield of mathematical\nstatistics. Due to its universality, our definition is readily applicable to\nany system at equilibrium with binary and categorical variables, be it magnetic\nspins, nodes in a neural network, or protein networks in biology. Our approach\nis targeted, not requiring fitting unnecessary parameters. Instead, it expends\nall data on estimating interactions, hence substantially increasing accuracy.\nWe demonstrate the generality of our technique both analytically and\nnumerically on (i) the 2-dimensional Ising model, (ii) an Ising-like model with\n4-point interactions, (iii) the Restricted Boltzmann Machine, and (iv)\nsimulated individual-level human DNA variants and representative traits. The\nlatter demonstrates the applicability of this approach to discover epistatic\ninteractions causal of disease in population biomedicine.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:01:18 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 23:04:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Beentjes", "Sjoerd Viktor", ""], ["Khamseh", "Ava", ""]]}, {"id": "2006.06020", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Convergence of Pseudo-Bayes Factors in Forward and Inverse Regression\n  Problems", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian literature on model comparison, Bayes factors play the\nleading role. In the classical statistical literature, model selection criteria\nare often devised used cross-validation ideas. Amalgamating the ideas of Bayes\nfactor and cross-validation Geisser and Eddy (1979) created the pseudo-Bayes\nfactor. The usage of cross-validation inculcates several theoretical\nadvantages, computational simplicity and numerical stability in Bayes factors\nas the marginal density of the entire dataset is replaced with products of\ncross-validation densities of individual data points.\n  However, the popularity of pseudo-Bayes factors is still negligible in\ncomparison with Bayes factors, with respect to both theoretical investigations\nand practical applications. In this article, we establish almost sure\nexponential convergence of pseudo-Bayes factors for large samples under a\ngeneral setup consisting of dependent data and model misspecifications. We\nparticularly focus on general parametric and nonparametric regression setups in\nboth forward and inverse contexts.\n  We illustrate our theoretical results with various examples, providing\nexplicit calculations. We also supplement our asymptotic theory with simulation\nexperiments in small sample situations of Poisson log regression and geometric\nlogit and probit regression, additionally addressing the variable selection\nproblem. We consider both linear and nonparametric regression modeled by\nGaussian processes for our purposes. Our simulation results provide quite\ninteresting insights into the usage of pseudo-Bayes factors in forward and\ninverse setups.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:12:44 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2006.06138", "submitter": "Lihua Lei", "authors": "Lihua Lei and Emmanuel J. Cand\\`es", "title": "Conformal Inference of Counterfactuals and Individual Treatment Effects", "comments": "Accepted by Journal of the Royal Statistical Society: Series B\n  (JRSSB); 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating treatment effect heterogeneity widely informs treatment decision\nmaking. At the moment, much emphasis is placed on the estimation of the\nconditional average treatment effect via flexible machine learning algorithms.\nWhile these methods enjoy some theoretical appeal in terms of consistency and\nconvergence rates, they generally perform poorly in terms of uncertainty\nquantification. This is troubling since assessing risk is crucial for reliable\ndecision-making in sensitive and uncertain environments. In this work, we\npropose a conformal inference-based approach that can produce reliable interval\nestimates for counterfactuals and individual treatment effects under the\npotential outcome framework. For completely randomized or stratified randomized\nexperiments with perfect compliance, the intervals have guaranteed average\ncoverage in finite samples regardless of the unknown data generating mechanism.\nFor randomized experiments with ignorable compliance and general observational\nstudies obeying the strong ignorability assumption, the intervals satisfy a\ndoubly robust property which states the following: the average coverage is\napproximately controlled if either the propensity score or the conditional\nquantiles of potential outcomes can be estimated accurately. Numerical studies\non both synthetic and real datasets empirically demonstrate that existing\nmethods suffer from a significant coverage deficit even in simple models. In\ncontrast, our methods achieve the desired coverage with reasonably short\nintervals.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 01:03:32 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 00:54:35 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lei", "Lihua", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2006.06180", "submitter": "Shonosuke Sugasawa", "authors": "Tsubasa Ito and Shonosuke Sugasawa", "title": "Grouped GEE Analysis for Longitudinal Data", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized estimating equation (GEE) is widely adopted for regression\nmodeling for longitudinal data, taking account of potential correlations within\nthe same subjects. Although the standard GEE assumes common regression\ncoefficients among all the subjects, such assumption is not realistic when\nthere are potential heterogeneity in regression coefficients among subjects. In\nthis paper, we develop a flexible and interpretable approach, called grouped\nGEE analysis, to modeling longitudinal data with allowing heterogeneity in\nregression coefficients. The proposed method assumes that the subjects are\ndivided into a finite number of groups and subjects within the same group share\nthe same regression coefficient. We provide a simple algorithm for grouping\nsubjects and estimating the regression coefficients simultaneously, and show\nasymptotic properties of the proposed estimator. The number of groups can be\ndetermined by the cross validation with averaging method. We demonstrate the\nproposed method through simulation studies and an application to a real\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 04:08:55 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 01:20:49 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ito", "Tsubasa", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2006.06220", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka", "title": "Bayesian Singular Value Regularization via a Cumulative Shrinkage\n  Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a novel hierarchical prior for inferring possibly\nlow-rank matrices measured with noise. We consider three-component matrix\nfactorization, as in singular value decomposition, and its fully Bayesian\ninference. The proposed prior is specified by a scale mixture of exponential\ndistributions that has spike and slab components. The weights for the\nspike/slab parts are inferred using a special prior based on a cumulative\nshrinkage process. The proposed prior is designed to increasingly aggressively\npush less important, or essentially redundant, singular values toward zero,\nleading to more accurate estimates of low-rank matrices. To ensure the\nparameter identification, we simulate posterior draws from an approximated\nposterior, in which the constraints are slightly relaxed, using a No-U-Turn\nsampler. By means of a set of simulation studies, we show that our proposal is\ncompetitive with alternative prior specifications and that it does not incur\nsignificant additional computational burden. We apply the proposed approach to\nsectoral industrial production in the United States to analyze the structural\nchange during the Great Moderation period.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 06:17:30 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 07:38:33 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:07:52 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 06:32:00 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Tanaka", "Masahiro", ""]]}, {"id": "2006.06268", "submitter": "Paola Andreani", "authors": "Roberto Vio, Thomas W. Nagler, Paola Andreani", "title": "Modeling high-dimensional dependence among astronomical data", "comments": "A&A, in press", "journal-ref": "A&A 642, A156 (2020)", "doi": "10.1051/0004-6361/202038585", "report-no": null, "categories": "stat.ME astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixing the relationship of a set of experimental quantities is a fundamental\nissue in many scientific disciplines. In the 2D case, the classical approach is\nto compute the linear correlation coefficient from a scatterplot. This method,\nhowever, implicitly assumes a linear relationship between the variables. Such\nan assumption is not always correct. With the use of the partial correlation\ncoefficients, an extension to the multidimensional case is possible. However,\nthe problem of the assumed mutual linear relationship of the variables remains.\nA relatively recent approach that makes it possible to avoid this problem is\nthe modeling of the joint probability density function (PDF) of the data with\ncopulas. These are functions that contain all the information on the\nrelationship between two random variables. Although in principle this approach\nalso can work with multidimensional data, theoretical as well computational\ndifficulties often limit its use to the 2D case. In this paper, we consider an\napproach based on so-called vine copulas, which overcomes this limitation and\nat the same time is amenable to a theoretical treatment and feasible from the\ncomputational point of view. We applied this method to published data on the\nnear-IR and far-IR luminosities and atomic and molecular masses of the Herschel\nreference sample, a volume-limited sample in the nearby Universe. We determined\nthe relationship of the luminosities and gas masses and show that the far-IR\nluminosity can be considered as the key parameter relating the other three\nquantities. Once removed from the 4D relation, the residual relation among the\nlatter is negligible. This may be interpreted as the correlation between the\ngas masses and near-IR luminosity being driven by the far-IR luminosity, likely\nby the star formation activity of the galaxy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:18:08 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 15:03:36 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Vio", "Roberto", ""], ["Nagler", "Thomas W.", ""], ["Andreani", "Paola", ""]]}, {"id": "2006.06272", "submitter": "Sudhansu Sekhar Maiti", "authors": "Indrani Mukherjee, Sudhansu S. Maiti and Vijay Vir Singh", "title": "Study on estimators of the PDF and CDF of the one parameter polynomial\n  exponential distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we have considered one parameter polynomial exponential\n(OPPE) distribution. The exponential, Lindley, length-biased Lindley and\nSujatha distribution are particular cases. Two estimators viz, MLE and UMVUE of\nthe PDF and the CDF of the OPPE distribution have been discussed. The\nestimation issue of the length-biased Lindley and Sujatha distribution have\nbeen considered in detail. The estimators have been compared in MSE sense.\nMonte Carlo simulations and real data analysis are performed to compare the\nperformances of the proposed methods of estimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:26:11 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Mukherjee", "Indrani", ""], ["Maiti", "Sudhansu S.", ""], ["Singh", "Vijay Vir", ""]]}, {"id": "2006.06350", "submitter": "Matthieu Marbac", "authors": "Marie Du Roy de Chaumaray, Matthieu Marbac and Valentin Patilea", "title": "Wilks' theorem for semiparametric regressions with weakly dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The empirical likelihood inference is extended to a class of semiparametric\nmodels for stationary, weakly dependent series. A partially linear single-index\nregression is used for the conditional mean of the series given its past, and\nthe present and past values of a vector of covariates. A parametric model for\nthe conditional variance of the series is added to capture further nonlinear\neffects. We propose a fixed number of suitable moment equations which\ncharacterize the mean and variance model. We derive an empirical log-likelihood\nratio which includes nonparametric estimators of several functions, and we show\nthat this ratio has the same limit as in the case where these functions are\nknown.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 11:52:22 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 15:21:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["de Chaumaray", "Marie Du Roy", ""], ["Marbac", "Matthieu", ""], ["Patilea", "Valentin", ""]]}, {"id": "2006.06373", "submitter": "Jackie Baek", "authors": "Jackie Baek, Vivek F. Farias, Andreea Georgescu, Retsef Levi, Tianyi\n  Peng, Deeksha Sinha, Joshua Wilde, Andrew Zheng", "title": "The Limits to Learning a Diffusion Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides the first sample complexity lower bounds for the\nestimation of simple diffusion models, including the Bass model (used in\nmodeling consumer adoption) and the SIR model (used in modeling epidemics). We\nshow that one cannot hope to learn such models until quite late in the\ndiffusion. Specifically, we show that the time required to collect a number of\nobservations that exceeds our sample complexity lower bounds is large. For Bass\nmodels with low innovation rates, our results imply that one cannot hope to\npredict the eventual number of adopting customers until one is at least\ntwo-thirds of the way to the time at which the rate of new adopters is at its\npeak. In a similar vein, our results imply that in the case of an SIR model,\none cannot hope to predict the eventual number of infections until one is\napproximately two-thirds of the way to the time at which the infection rate has\npeaked. These limits are borne out in both product adoption data (Amazon), as\nwell as epidemic data (COVID-19).\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 12:47:16 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 03:26:28 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Baek", "Jackie", ""], ["Farias", "Vivek F.", ""], ["Georgescu", "Andreea", ""], ["Levi", "Retsef", ""], ["Peng", "Tianyi", ""], ["Sinha", "Deeksha", ""], ["Wilde", "Joshua", ""], ["Zheng", "Andrew", ""]]}, {"id": "2006.06448", "submitter": "Mingzhang Yin", "authors": "Mingzhang Yin, Nhat Ho, Bowei Yan, Xiaoning Qian, Mingyuan Zhou", "title": "Probabilistic Best Subset Selection via Gradient-Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional statistics, variable selection is an optimization problem\naiming to recover the latent sparse pattern from all possible covariate\ncombinations. In this paper, we propose a novel optimization method to solve\nthe exact $L_0$-regularized regression problem (a.k.a. best subset selection).\nWe reformulate the optimization problem from a discrete space to a continuous\none via probabilistic reparameterization. Within the framework of stochastic\ngradient descent, we propose a family of unbiased gradient estimators to\noptimize the $L_0$-regularized objective and a variational lower bound. Within\nthis family, we identify the estimator with a non-vanishing signal-to-noise\nratio and uniformly minimum variance. Theoretically, we study the general\nconditions under which the method is guaranteed to converge to the ground truth\nin expectation. In a wide variety of synthetic and semi-synthetic data sets,\nthe proposed method outperforms existing variable selection methods that are\nbased on penalized regression and mixed-integer optimization, in both sparse\npattern recovery and out-of-sample prediction. Our method can find the true\nregression model from thousands of covariates in a couple of seconds. a\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:57:29 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 18:28:46 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 04:23:34 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Yin", "Mingzhang", ""], ["Ho", "Nhat", ""], ["Yan", "Bowei", ""], ["Qian", "Xiaoning", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2006.06495", "submitter": "Yekun Wang", "authors": "Yekun Wang and Luis Pericchi", "title": "A Bridge between Cross-validation Bayes Factors and Geometric Intrinsic\n  Bayes Factors", "comments": "23 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model Selections in Bayesian Statistics are primarily made with statistics\nknown as Bayes Factors, which are directly related to Posterior Probabilities\nof models. Bayes Factors require a careful assessment of prior distributions as\nin the Intrinsic Priors of Berger and Pericchi (1996a) and integration over the\nparameter space, which may be highly dimensional. Recently researchers have\nbeen proposing alternatives to Bayes Factors that require neither integration\nnor specification of priors. These developments are still in a very early stage\nand are known as Prior-free Bayes Factors, Cross-Validation Bayes Factors\n(CVBF), and Bayesian \"Stacking.\" This kind of method and Intrinsic Bayes Factor\n(IBF) both avoid the specification of prior. However, this Prior-free Bayes\nfactor might need a careful choice of a training sample size. In this article,\na way of choosing training sample sizes for the Prior-free Bayes factor based\non Geometric Intrinsic Bayes Factors (GIBFs) is proposed and studied. We\npresent essential examples with a different number of parameters and study the\nstatistical behavior both numerically and theoretically to explain the ideas\nfor choosing a feasible training sample size for Prior-free Bayes Factors. We\nput forward the \"Bridge Rule\" as an assignment of a training sample size for\nCVBF's that makes them close to Geometric IBFs. We conclude that even though\ntractable Geometric IBFs are preferable, CVBF's, using the Bridge Rule, are\nuseful and economical approximations to Bayes Factors.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:07:52 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wang", "Yekun", ""], ["Pericchi", "Luis", ""]]}, {"id": "2006.06537", "submitter": "Kelly Moran", "authors": "Kelly R. Moran and Matthew W. Wheeler", "title": "Fast increased fidelity approximate Gibbs samplers for Bayesian Gaussian\n  process regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Gaussian processes (GPs) is supported by efficient sampling\nalgorithms, a rich methodological literature, and strong theoretical grounding.\nHowever, due to their prohibitive computation and storage demands, the use of\nexact GPs in Bayesian models is limited to problems containing at most several\nthousand observations. Sampling requires matrix operations that scale at\n$\\mathcal{O}(n^3),$ where $n$ is the number of unique inputs. Storage of\nindividual matrices scales at $\\mathcal{O}(n^2),$ and can quickly overwhelm the\nresources of most modern computers. To overcome these bottlenecks, we develop a\nsampling algorithm using $\\mathcal{H}$ matrix approximation of the matrices\ncomprising the GP posterior covariance. These matrices can approximate the true\nconditional covariance matrix within machine precision and allow for sampling\nalgorithms that scale at $\\mathcal{O}(n \\ \\mbox{log}^2 n)$ time and storage\ndemands scaling at $\\mathcal{O}(n \\ \\mbox{log} \\ n).$ We also describe how\nthese algorithms can be used as building blocks to model higher dimensional\nsurfaces at $\\mathcal{O}(d \\ n \\ \\mbox{log}^2 n)$, where $d$ is the dimension\nof the surface under consideration, using tensor products of one-dimensional\nGPs. Though various scalable processes have been proposed for approximating\nBayesian GP inference when $n$ is large, to our knowledge, none of these\nmethods show that the approximation's Kullback-Leibler divergence to the true\nposterior can be made arbitrarily small and may be no worse than the\napproximation provided by finite computer arithmetic. We describe\n$\\mathcal{H}-$matrices, give an efficient Gibbs sampler using these matrices\nfor one-dimensional GPs, offer a proposed extension to higher dimensional\nsurfaces, and investigate the performance of this fast increased fidelity\napproximate GP, FIFA-GP, using both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:51:11 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Moran", "Kelly R.", ""], ["Wheeler", "Matthew W.", ""]]}, {"id": "2006.06807", "submitter": "Michael Crowther", "authors": "Michael J. Crowther, Patrick Royston, Mark Clements", "title": "A flexible parametric accelerated failure time model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated failure time (AFT) models are used widely in medical research,\nthough to a much lesser extent than proportional hazards models. In an AFT\nmodel, the effect of covariates act to accelerate or decelerate the time to\nevent of interest, i.e. shorten or extend the time to event. Commonly used\nparametric AFT models are limited in the underlying shapes that they can\ncapture. In this article, we propose a general parametric AFT model, and in\nparticular concentrate on using restricted cubic splines to model the baseline\nto provide substantial flexibility. We then extend the model to accommodate\ntime-dependent acceleration factors. Delayed entry is also allowed, and hence,\ntime-dependent covariates. We evaluate the proposed model through simulation,\nshowing substantial improvements compared to standard parametric AFT models. We\nalso show analytically and through simulations that the AFT models are\ncollapsible, suggesting that this model class will be well suited to causal\ninference. We illustrate the methods with a dataset of patients with breast\ncancer. User friendly Stata and R software packages are provided.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 20:47:20 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Crowther", "Michael J.", ""], ["Royston", "Patrick", ""], ["Clements", "Mark", ""]]}, {"id": "2006.06843", "submitter": "Drew Lazar", "authors": "Lizhen Lin, Drew Lazar, Bayan Sarpabayeva, and David B. Dunson", "title": "Robust Optimization and Inference on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a robust and scalable procedure for general optimization and\ninference problems on manifolds leveraging the classical idea of\n`median-of-means' estimation. This is motivated by ubiquitous examples and\napplications in modern data science in which a statistical learning problem can\nbe cast as an optimization problem over manifolds. Being able to incorporate\nthe underlying geometry for inference while addressing the need for robustness\nand scalability presents great challenges. We address these challenges by first\nproving a key lemma that characterizes some crucial properties of geometric\nmedians on manifolds. In turn, this allows us to prove robustness and tighter\nconcentration of our proposed final estimator in a subsequent theorem. This\nestimator aggregates a collection of subset estimators by taking their\ngeometric median over the manifold. We illustrate bounds on this estimator via\ncalculations in explicit examples. The robustness and scalability of the\nprocedure is illustrated in numerical examples on both simulated and real data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:37:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lin", "Lizhen", ""], ["Lazar", "Drew", ""], ["Sarpabayeva", "Bayan", ""], ["Dunson", "David B.", ""]]}, {"id": "2006.06864", "submitter": "Ashton Wiens", "authors": "Ashton Wiens and William Kleiber and Douglas Nychka and Katherine R.\n  Barnhart", "title": "Nonrigid registration using Gaussian processes and local likelihood\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface registration, the task of aligning several multidimensional point\nsets, is a necessary task in many scientific fields. In this work, a novel\nstatistical approach is developed to solve the problem of nonrigid\nregistration. While the application of an affine transformation results in\nrigid registration, using a general nonlinear function to achieve nonrigid\nregistration is necessary when the point sets require deformations that change\nover space. The use of a local likelihood-based approach using windowed\nGaussian processes provides a flexible way to accurately estimate the nonrigid\ndeformation. This strategy also makes registration of massive data sets\nfeasible by splitting the data into many subsets. The estimation results yield\nspatially-varying local rigid registration parameters. Gaussian process surface\nmodels are then fit to the parameter fields, allowing prediction of the\ntransformation parameters at unestimated locations, specifically at observation\nlocations in the unregistered data set. Applying these transformations results\nin a global, nonrigid registration. A penalty on the transformation parameters\nis included in the likelihood objective function. Combined with smoothing of\nthe local estimates from the surface models, the nonrigid registration model\ncan prevent the problem of overfitting. The efficacy of the nonrigid\nregistration method is tested in two simulation studies, varying the number of\nwindows and number of points, as well as the type of deformation. The nonrigid\nmethod is applied to a pair of massive remote sensing elevation data sets\nexhibiting complex geological terrain, with improved accuracy and uncertainty\nquantification in a cross validation study versus two rigid registration\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 22:58:36 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wiens", "Ashton", ""], ["Kleiber", "William", ""], ["Nychka", "Douglas", ""], ["Barnhart", "Katherine R.", ""]]}, {"id": "2006.06978", "submitter": "Biswabrata Pradhan", "authors": "Siddhartha Chakraborty, Biswabrata Pradhan", "title": "Generalized Weighted Survival and Failure Entropies and their Dynamic\n  Versions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted forms of generalized survival and failure entropies of order\n($\\alpha,\\beta$) are proposed and some properties are obtained. We further\npropose the dynamic versions of weighted generalized survival and failures\nentropies and obtained some properties and bounds. Characterization for\nRayleigh and power distributions are done by dynamic weighted generalized\nentropies. We further consider the empirical versions of generalized weighted\nsurvival and failure entropies and using the difference between theoretical and\nempirical survival entropies a test for exponentiality is considered.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:33:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Chakraborty", "Siddhartha", ""], ["Pradhan", "Biswabrata", ""]]}, {"id": "2006.06982", "submitter": "Masahiro Kato", "authors": "Masahiro Kato", "title": "Confidence Interval for Off-Policy Evaluation from Dependent Samples via\n  Bandit Algorithm: Approach from Standardized Martingales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the problem of off-policy evaluation (OPE) from\ndependent samples obtained via the bandit algorithm. The goal of OPE is to\nevaluate a new policy using historical data obtained from behavior policies\ngenerated by the bandit algorithm. Because the bandit algorithm updates the\npolicy based on past observations, the samples are not independent and\nidentically distributed (i.i.d.). However, several existing methods for OPE do\nnot take this issue into account and are based on the assumption that samples\nare i.i.d. In this study, we address this problem by constructing an estimator\nfrom a standardized martingale difference sequence. To standardize the\nsequence, we consider using evaluation data or sample splitting with a two-step\nestimation. This technique produces an estimator with asymptotic normality\nwithout restricting a class of behavior policies. In an experiment, the\nproposed estimator performs better than existing methods, which assume that the\nbehavior policy converges to a time-invariant policy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:48:04 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Kato", "Masahiro", ""]]}, {"id": "2006.07040", "submitter": "Anpeng Wu", "authors": "Anpeng Wu, Kun Kuang, Junkun Yuan, Bo Li, Pan Zhou, Jianrong Tao,\n  Qiang Zhu, Yueting Zhuang, Fei Wu", "title": "Learning Decomposed Representation for Counterfactual Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental problem in the learning treatment effect from observational\ndata is confounder identification and balancing. Most of the previous methods\nrealized confounder balancing by treating all observed variables as\nconfounders, ignoring the identification of confounders and non-confounders. In\ngeneral, not all the observed variables are confounders which are the common\ncauses of both the treatment and the outcome, some variables only contribute to\nthe treatment and some contribute to the outcome. Balancing those\nnon-confounders would generate additional bias for treatment effect estimation.\nBy modeling the different relations among variables, treatment and outcome, we\npropose a synergistic learning framework to 1) identify and balance confounders\nby learning decomposed representation of confounders and non-confounders, and\nsimultaneously 2) estimate the treatment effect in observational studies via\ncounterfactual inference. Our empirical results demonstrate that the proposed\nmethod can precisely identify and balance confounders, while the estimation of\nthe treatment effect performs better than the state-of-the-art methods on both\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:50:42 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wu", "Anpeng", ""], ["Kuang", "Kun", ""], ["Yuan", "Junkun", ""], ["Li", "Bo", ""], ["Zhou", "Pan", ""], ["Tao", "Jianrong", ""], ["Zhu", "Qiang", ""], ["Zhuang", "Yueting", ""], ["Wu", "Fei", ""]]}, {"id": "2006.07052", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura and Tatsuya Kubokawa", "title": "Bayesian Predictive Density Estimation for a Chi-squared Model Using\n  Information from a Normal Observation with Unknown Mean and Variance", "comments": "18 pages, 1 figure, extensively rewritten", "journal-ref": null, "doi": "10.1016/j.jspi.2021.07.004", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the density function of\na Chi-squared variable on the basis of observations of another Chi-squared\nvariable and a normal variable under the Kullback-Leibler divergence. We assume\nthat these variables have a common unknown scale parameter and that the mean of\nthe normal variable is also unknown. We compare the risk functions of two\nBayesian predictive densities: one with respect to a hierarchical shrinkage\nprior and the other based on a noninformative prior. The hierarchical Bayesian\npredictive density depends on the normal variable while the Bayesian predictive\ndensity based on the noninformative prior does not. Sufficient conditions for\nthe former to dominate the latter are obtained. These predictive densities are\ncompared by simulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:11:52 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:07:43 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "2006.07074", "submitter": "Mohammad Arshad Rahman", "authors": "Georges Bresson and Anoop Chaturvedi and Mohammad Arshad Rahman and\n  Shalabh", "title": "Seemingly Unrelated Regression with Measurement Error: Estimation via\n  Markov chain Monte Carlo and Mean Field Variational Bayes Approximation", "comments": "29 Pages, 8 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression with measurement error in the covariates is a heavily\nstudied topic, however, the statistics/econometrics literature is almost silent\nto estimating a multi-equation model with measurement error. This paper\nconsiders a seemingly unrelated regression model with measurement error in the\ncovariates and introduces two novel estimation methods: a pure Bayesian\nalgorithm (based on Markov chain Monte Carlo techniques) and its mean field\nvariational Bayes (MFVB) approximation. The MFVB method has the added advantage\nof being computationally fast and can handle big data. An issue pertinent to\nmeasurement error models is parameter identification, and this is resolved by\nemploying a prior distribution on the measurement error variance. The methods\nare shown to perform well in multiple simulation studies, where we analyze the\nimpact on posterior estimates arising due to different values of reliability\nratio or variance of the true unobserved quantity used in the data generating\nprocess. The paper further implements the proposed algorithms in an application\ndrawn from the health literature and shows that modeling measurement error in\nthe data can improve model fitting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:58:10 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Bresson", "Georges", ""], ["Chaturvedi", "Anoop", ""], ["Rahman", "Mohammad Arshad", ""], ["Shalabh", "", ""]]}, {"id": "2006.07183", "submitter": "Roman Flury", "authors": "Roman Flury, Florian Gerber, Bernhard Schmid and Reinhard Furrer", "title": "Identification of Dominant Features in Spatial Data", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100483", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dominant features of spatial data are connected structures or patterns that\nemerge from location-based variation and manifest at specific scales or\nresolutions. To identify dominant features, we propose a sequential application\nof multiresolution decomposition and variogram function estimation.\nMultiresolution decomposition separates data into additive components, and in\nthis way enables the recognition of their dominant features. A dedicated\nmultiresolution decomposition method is developed for arbitrary gridded spatial\ndata, where the underlying model includes a precision and spatial-weight matrix\nto capture spatial correlation. The data are separated into their components by\nsmoothing on different scales, such that larger scales have longer spatial\ncorrelation ranges. Moreover, our model can handle missing values, which is\noften useful in applications. Variogram function estimation can be used to\ndescribe properties in spatial data. Such functions are therefore estimated for\neach component to determine its effective range, which assesses the\nwidth-extent of the dominant feature. Finally, Bayesian analysis enables the\ninference of identified dominant features and to judge whether these are\ncredibly different. The efficient implementation of the method relies mainly on\na sparse-matrix data structure and algorithms. By applying the method to\nsimulated data we demonstrate its applicability and theoretical soundness. In\ndisciplines that use spatial data, this method can lead to new insights, as we\nexemplify by identifying the dominant features in a forest dataset. In that\napplication, the width-extents of the dominant features have an ecological\ninterpretation, namely the species interaction range, and their estimates\nsupport the derivation of ecosystem properties such as biodiversity indices.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:38:39 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 13:18:10 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Flury", "Roman", ""], ["Gerber", "Florian", ""], ["Schmid", "Bernhard", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2006.07333", "submitter": "Alan Hubbard", "authors": "Jeremy R. Coyle, Nima S. Hejazi, Ivana Malenica, Rachael V. Phillips,\n  Benjamin F. Arnold, Andrew Mertens, Jade Benjamin-Chung, Weixin Cai, Sonali\n  Dayal, John M. Colford Jr., Alan E. Hubbard, Mark J. van der Laan", "title": "Targeting Learning: Robust Statistics for Reproducible Research", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted Learning is a subfield of statistics that unifies advances in causal\ninference, machine learning and statistical theory to help answer\nscientifically impactful questions with statistical confidence. Targeted\nLearning is driven by complex problems in data science and has been implemented\nin a diversity of real-world scenarios: observational studies with missing\ntreatments and outcomes, personalized interventions, longitudinal settings with\ntime-varying treatment regimes, survival analysis, adaptive randomized trials,\nmediation analysis, and networks of connected subjects. In contrast to the\n(mis)application of restrictive modeling strategies that dominate the current\npractice of statistics, Targeted Learning establishes a principled standard for\nstatistical estimation and inference (i.e., confidence intervals and p-values).\nThis multiply robust approach is accompanied by a guiding roadmap and a\nburgeoning software ecosystem, both of which provide guidance on the\nconstruction of estimators optimized to best answer the motivating question.\nThe roadmap of Targeted Learning emphasizes tailoring statistical procedures so\nas to minimize their assumptions, carefully grounding them only in the\nscientific knowledge available. The end result is a framework that honestly\nreflects the uncertainty in both the background knowledge and the available\ndata in order to draw reliable conclusions from statistical analyses -\nultimately enhancing the reproducibility and rigor of scientific findings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:17:01 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Coyle", "Jeremy R.", ""], ["Hejazi", "Nima S.", ""], ["Malenica", "Ivana", ""], ["Phillips", "Rachael V.", ""], ["Arnold", "Benjamin F.", ""], ["Mertens", "Andrew", ""], ["Benjamin-Chung", "Jade", ""], ["Cai", "Weixin", ""], ["Dayal", "Sonali", ""], ["Colford", "John M.", "Jr."], ["Hubbard", "Alan E.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2006.07368", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Aaditya Ramdas", "title": "Uncertainty quantification using martingales for misspecified Gaussian\n  processes", "comments": "Accepted as a conference paper at ALT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address uncertainty quantification for Gaussian processes (GPs) under\nmisspecified priors, with an eye towards Bayesian Optimization (BO). GPs are\nwidely used in BO because they easily enable exploration based on posterior\nuncertainty bands. However, this convenience comes at the cost of robustness: a\ntypical function encountered in practice is unlikely to have been drawn from\nthe data scientist's prior, in which case uncertainty estimates can be\nmisleading, and the resulting exploration can be suboptimal. We present a\nfrequentist approach to GP/BO uncertainty quantification. We utilize the GP\nframework as a working model, but do not assume correctness of the prior. We\ninstead construct a confidence sequence (CS) for the unknown function using\nmartingale techniques. There is a necessary cost to achieving robustness: if\nthe prior was correct, posterior GP bands are narrower than our CS.\nNevertheless, when the prior is wrong, our CS is statistically valid and\nempirically outperforms standard GP methods, in terms of both coverage and\nutility for BO. Additionally, we demonstrate that powered likelihoods provide\nrobustness against model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:58:59 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:35:13 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Neiswanger", "Willie", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.07433", "submitter": "Rune Christiansen", "authors": "Rune Christiansen, Niklas Pfister, Martin Emil Jakobsen, Nicola Gnecco\n  and Jonas Peters", "title": "A causal framework for distribution generalization", "comments": "49 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting a response from a set of covariates\nwhen the test distribution may differ from the training distribution. Motivated\nby the idea that such differences may have causal explanations, we consider a\nclass of test distributions that emerge from interventions in a structural\ncausal model, and focus on minimizing the worst-case risk over this class.\nCausal regression models, which regress the response variable on all of its\ndirect parents, remain valid under arbitrary interventions on any subset of\ncovariates, but they are not always optimal in the above sense. For example, in\nlinear models, for a set of interventions with bounded strength, alternative\nsolutions have been shown to be minimax prediction optimal. We introduce the\nformal framework of distribution generalization that allows us to analyze the\nabove problem in partially observed nonlinear models for both direct and\nindirect interventions on the covariates. It takes into account that, in\npractice, minimax solutions need to be identified from observational data. Our\nframework allows us to characterize under which class of interventions the\ncausal function is minimax optimal. We prove several sufficient conditions for\ndistribution generalization and present corresponding impossibility results. We\npropose a practical method, called NILE, that achieves distribution\ngeneralization in a nonlinear instrumental variables setting with linear\nextrapolation. We prove consistency and present empirical results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:24:02 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 20:00:08 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Christiansen", "Rune", ""], ["Pfister", "Niklas", ""], ["Jakobsen", "Martin Emil", ""], ["Gnecco", "Nicola", ""], ["Peters", "Jonas", ""]]}, {"id": "2006.07435", "submitter": "Zhanhao Peng", "authors": "Zhanhao Peng and Qing Zhou", "title": "An Empirical Bayes Approach to Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphon (W-graph), including the stochastic block model as a special\ncase, has been widely used in modeling and analyzing network data. This random\ngraph model is well-characterized by its graphon function, and estimation of\nthe graphon function has gained a lot of recent research interests. Most\nexisting works focus on community detection in the latent space of the model,\nwhile adopting simple maximum likelihood or Bayesian estimates for the graphon\nor connectivity parameters given the identified communities. In this work, we\npropose a hierarchical Binomial model and develop a novel empirical Bayes\nestimate of the connectivity matrix of a stochastic block model to approximate\nthe graphon function. Based on the likelihood of our hierarchical model, we\nfurther introduce a model selection criterion for choosing the number of\ncommunities. Numerical results on extensive simulations and two well-annotated\nsocial networks demonstrate the superiority of our approach in terms of\nestimation accuracy and model selection.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:29:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Peng", "Zhanhao", ""], ["Zhou", "Qing", ""]]}, {"id": "2006.07480", "submitter": "Eric Oh", "authors": "Eric J. Oh and Bryan E. Shepherd and Thomas Lumley and Pamela A. Shaw", "title": "Improved Generalized Raking Estimators to Address Dependent Covariate\n  and Failure-Time Outcome Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical studies that use electronic health records (EHR) data for\ninference are often subject to bias due to measurement error. The measurement\nerror present in EHR data is typically complex, consisting of errors of unknown\nfunctional form in covariates and the outcome, which can be dependent. To\naddress the bias resulting from such errors, generalized raking has recently\nbeen proposed as a robust method that yields consistent estimates without the\nneed to model the error structure. We provide rationale for why these\npreviously proposed raking estimators can be expected to be inefficient in\nfailure-time outcome settings involving misclassification of the event\nindicator. We propose raking estimators that utilize multiple imputation, to\nimpute either the target variables or auxiliary variables, to improve the\nefficiency. We also consider outcome-dependent sampling designs and investigate\ntheir impact on the efficiency of the raking estimators, either with or without\nmultiple imputation. We present an extensive numerical study to examine the\nperformance of the proposed estimators across various measurement error\nsettings. We then apply the proposed methods to our motivating setting, in\nwhich we seek to analyze HIV outcomes in an observational cohort with\nelectronic health records data from the Vanderbilt Comprehensive Care Clinic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:21:38 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Oh", "Eric J.", ""], ["Shepherd", "Bryan E.", ""], ["Lumley", "Thomas", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "2006.07546", "submitter": "Peter Marcy", "authors": "Peter W. Marcy and Curtis B. Storlie", "title": "Bayesian Calibration of Computer Models with Informative Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-18-21204", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many practical difficulties in the calibration of computer models\nto experimental data. One such complication is the fact that certain\ncombinations of the calibration inputs can cause the code to output data\nlacking fundamental properties, or even to produce no output at all. In many\ncases the researchers want or need to exclude the possibility of these\n\"failures\" within their analyses. We propose a Bayesian (meta-)model in which\nthe posterior distribution for the calibration parameters naturally excludes\nregions of the input space corresponding to failed runs. That is, we define a\nstatistical selection model to rigorously couple the disjoint problems of\nbinary classification and computer model calibration. We demonstrate our\nmethodology using data from a carbon capture experiment in which the numerics\nof the computational fluid dynamics are prone to instability.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:23:46 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Marcy", "Peter W.", ""], ["Storlie", "Curtis B.", ""]]}, {"id": "2006.07551", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Guanghui Cheng, Qiwei Yao", "title": "Testing for unit roots based on sample autocovariances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new unit-root test for a stationary null hypothesis $H_0$\nagainst a unit-root alternative $H_1$. Our approach is nonparametric as $H_0$\nonly assumes that the process concerned is $I(0)$ without specifying any\nparametric forms. The new test is based on the fact that the sample\nautocovariance function (ACVF) converges to the finite population ACVF for an\n$I(0)$ process while it diverges to infinity for a process with unit-roots.\nTherefore the new test rejects $H_0$ for the large values of the sample ACVF.\nTo address the technical challenge `how large is large', we split the sample\nand establish an appropriate normal approximation for the null-distribution of\nthe test statistic. The substantial discriminative power of the new test\nstatistic is rooted from the fact that it takes finite value under $H_0$ and\ndiverges to infinity under $H_1$. This allows us to truncate the critical\nvalues of the test to make it with the asymptotic power one. It also alleviates\nthe loss of power due to the sample-splitting. The test is implemented in a\nuser-friendly R-function.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:46:11 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 08:45:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Chang", "Jinyuan", ""], ["Cheng", "Guanghui", ""], ["Yao", "Qiwei", ""]]}, {"id": "2006.07561", "submitter": "Somak Dutta", "authors": "Dongjin Li, Somak Dutta and Vivekananda Roy", "title": "Model Based Screening Embedded Bayesian Variable Selection for\n  Ultra-high Dimensional Settings", "comments": "54 pages including supplementary,4 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian variable selection method, called SVEN, based on a\nhierarchical Gaussian linear model with priors placed on the regression\ncoefficients as well as on the model space. Sparsity is achieved by using\ndegenerate spike priors on inactive variables, whereas Gaussian slab priors are\nplaced on the coefficients for the important predictors making the posterior\nprobability of a model available in explicit form (up to a normalizing\nconstant). The strong model selection consistency is shown to be attained when\nthe number of predictors grows nearly exponentially with the sample size and\neven when the norm of mean effects solely due to the unimportant variables\ndiverge, which is a novel attractive feature. An appealing byproduct of SVEN is\nthe construction of novel model weight adjusted prediction intervals. Embedding\na unique model based screening and using fast Cholesky updates, SVEN produces a\nhighly scalable computational framework to explore gigantic model spaces,\nrapidly identify the regions of high posterior probabilities and make fast\ninference and prediction. A temperature schedule guided by our model selection\nconsistency derivations is used to further mitigate multimodal posterior\ndistributions. The performance of SVEN is demonstrated through a number of\nsimulation experiments and a real data example from a genome wide association\nstudy with over half a million markers.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 04:43:07 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 19:06:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Dongjin", ""], ["Dutta", "Somak", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2006.07571", "submitter": "Masahiro Fujisawa", "authors": "Masahiro Fujisawa, Takeshi Teshima, Issei Sato, Masashi Sugiyama", "title": "$\\gamma$-ABC: Outlier-Robust Approximate Bayesian Computation Based on a\n  Robust Divergence Estimator", "comments": "The 24th International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2021); 48 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a likelihood-free inference method\nthat has been employed in various applications. However, ABC can be sensitive\nto outliers if a data discrepancy measure is chosen inappropriately. In this\npaper, we propose to use a nearest-neighbor-based $\\gamma$-divergence estimator\nas a data discrepancy measure. We show that our estimator possesses a suitable\ntheoretical robustness property called the redescending property. In addition,\nour estimator enjoys various desirable properties such as high flexibility,\nasymptotic unbiasedness, almost sure convergence, and linear-time computational\ncomplexity. Through experiments, we demonstrate that our method achieves\nsignificantly higher robustness than existing discrepancy measures.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 06:09:27 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:06:41 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 05:16:43 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Fujisawa", "Masahiro", ""], ["Teshima", "Takeshi", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2006.07640", "submitter": "Shifeng Xiong Doc", "authors": "Chunya Li, Daijun Chen, and Shifeng Xiong", "title": "Linear screening for high-dimensional computer experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a linear variable screening method for computer\nexperiments when the number of input variables is larger than the number of\nruns. This method uses a linear model to model the nonlinear data, and screens\nthe important variables by existing screening methods for linear models. When\nthe underlying simulator is nearly sparse, we prove that the linear screening\nmethod is asymptotically valid under mild conditions. To improve the screening\naccuracy, we also provide a two-stage procedure that uses different basis\nfunctions in the linear model. The proposed methods are very simple and easy to\nimplement. Numerical results indicate that our methods outperform existing\nmodel-free screening methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 13:55:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Li", "Chunya", ""], ["Chen", "Daijun", ""], ["Xiong", "Shifeng", ""]]}, {"id": "2006.07663", "submitter": "Gyuhyeong Goh", "authors": "Gyuhyeong Goh, Jisang Yu", "title": "Bayesian causal inference with some invalid instrumental variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, instrumental variables estimation is greatly\nutilized to identify causal effects. One of the key conditions for the\ninstrumental variables estimator to be consistent is the exclusion restriction,\nwhich indicates that instruments affect the outcome of interest only via the\nexposure variable of interest. We propose a likelihood-free Bayesian approach\nto make consistent inferences about the causal effect when there are some\ninvalid instruments in a way that they violate the exclusion restriction\ncondition. Asymptotic properties of the proposed Bayes estimator, including\nconsistency and normality, are established. A simulation study demonstrates\nthat the proposed Bayesian method produces consistent point estimators and\nvalid credible intervals with correct coverage rates for Gaussian and\nnon-Gaussian data with some invalid instruments. We also demonstrate the\nproposed method through the real data application.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 15:35:06 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Goh", "Gyuhyeong", ""], ["Yu", "Jisang", ""]]}, {"id": "2006.07681", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli and Brenden Beck", "title": "Heterogeneous causal effects of neighborhood policing in New York City\n  with staggered adoption of the policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communities often self select into implementing a regulatory policy, and\nadopt the policy at different time points. In New York City, neighborhood\npolicing was adopted at the police precinct level over the years 2015-2018, and\nit is of interest to both (1) evaluate the impact of the policy, and (2)\nunderstand what types of communities are most impacted by the policy, raising\nquestions of heterogeneous treatment effects. We develop novel statistical\napproaches that are robust to unmeasured confounding bias to study the causal\neffect of policies implemented at the community level. Using techniques from\nhigh-dimensional Bayesian time-series modeling, we estimate treatment effects\nby predicting counterfactual values of what would have happened in the absence\nof neighborhood policing. We couple the posterior predictive distribution of\nthe treatment effect with flexible modeling to identify how the impact of the\npolicy varies across time and community characteristics. Using pre-treatment\ndata from New York City, we show our approach produces unbiased estimates of\ntreatment effects with valid measures of uncertainty. Lastly, we find that\nneighborhood policing decreases discretionary arrests, but has little effect on\ncrime or racial disparities in arrest rates.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 16:55:24 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 00:19:16 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Antonelli", "Joseph", ""], ["Beck", "Brenden", ""]]}, {"id": "2006.07687", "submitter": "Neil A. Spencer", "authors": "Neil A. Spencer, Brian Junker, Tracy M. Sweet", "title": "Faster MCMC for Gaussian Latent Position Network Models", "comments": "61 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent position network models are a versatile tool in network science;\napplications include clustering entities, controlling for causal confounders,\nand defining priors over unobserved graphs. Estimating each node's latent\nposition is typically framed as a Bayesian inference problem, with Metropolis\nwithin Gibbs being the most popular tool for approximating the posterior\ndistribution. However, it is well-known that Metropolis within Gibbs is\ninefficient for large networks; the acceptance ratios are expensive to compute,\nand the resultant posterior draws are highly correlated. In this article, we\npropose an alternative Markov chain Monte Carlo strategy---defined using a\ncombination of split Hamiltonian Monte Carlo and Firefly Monte Carlo---that\nleverages the posterior distribution's functional form for more efficient\nposterior computation. We demonstrate that these strategies outperform\nMetropolis within Gibbs and other algorithms on synthetic networks, as well as\non real information-sharing networks of teachers and staff in a school\ndistrict.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 17:38:34 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Spencer", "Neil A.", ""], ["Junker", "Brian", ""], ["Sweet", "Tracy M.", ""]]}, {"id": "2006.07708", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph and Ivan Diaz", "title": "Efficiently transporting causal (in)direct effects to new populations\n  under intermediate confounding and with multiple mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The same intervention can produce different effects in different sites.\nTransport mediation estimators can estimate the extent to which such\ndifferences can be explained by differences in compositional factors and the\nmechanisms by which mediating or intermediate variables are produced; however,\nthey are limited to consider a single, binary mediator. We propose novel\nnonparametric estimators of transported stochastic (in)direct effects that\nconsider multiple, high-dimensional mediators and intermediate variables. They\nare multiply robust, efficient, asymptotically normal, and can incorporate\ndata-adaptive estimation of nuisance parameters. They can be applied to\nunderstand differences in treatment effects across sites and/or to predict\ntreatment effects in a target site based on outcome data in source sites.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 19:57:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Diaz", "Ivan", ""]]}, {"id": "2006.07785", "submitter": "Yuan Shijie", "authors": "Jiaying Lyu, Tianjian Zhou, Shijie Yuan, Wentian Guo, Yuan Ji", "title": "MUCE: Bayesian Hierarchical Modeling for the Design and Analysis of\n  Phase 1b Multiple Expansion Cohort Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple cohort expansion (MUCE) approach as a design or\nanalysis method for phase 1b multiple expansion cohort trials, which are novel\nfirst-in-human studies conducted following phase 1a dose escalation. The MUCE\ndesign is based on a class of Bayesian hierarchical models that adaptively\nborrow information across arms. Statistical inference is directly based on the\nposterior probability of each arm being efficacious, facilitating the decision\nmaking that decides which arm to select for further testing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:39:18 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 07:19:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Lyu", "Jiaying", ""], ["Zhou", "Tianjian", ""], ["Yuan", "Shijie", ""], ["Guo", "Wentian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.07788", "submitter": "Wenbo Hu", "authors": "Zhiheng Zhang, Wenbo Hu, Tian Tian, Jun Zhu", "title": "Dynamic Window-level Granger Causality of Multi-channel Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality method analyzes the time series causalities without\nbuilding a complex causality graph. However, the traditional Granger causality\nmethod assumes that the causalities lie between time series channels and remain\nconstant, which cannot model the real-world time series data with dynamic\ncausalities along the time series channels. In this paper, we present the\ndynamic window-level Granger causality method (DWGC) for multi-channel time\nseries data. We build the causality model on the window-level by doing the\nF-test with the forecasting errors on the sliding windows. We propose the\ncausality indexing trick in our DWGC method to reweight the original time\nseries data. Essentially, the causality indexing is to decrease the\nauto-correlation and increase the cross-correlation causal effects, which\nimproves the DWGC method. Theoretical analysis and experimental results on two\nsynthetic and one real-world datasets show that the improved DWGC method with\ncausality indexing better detects the window-level causalities.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:53:42 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Zhiheng", ""], ["Hu", "Wenbo", ""], ["Tian", "Tian", ""], ["Zhu", "Jun", ""]]}, {"id": "2006.07902", "submitter": "Thomas Opitz", "authors": "Thomas Opitz, Haakon Bakka, Rapha\\\"el Huser, Luigi Lombardo", "title": "High-resolution Bayesian mapping of landslide hazard with unobserved\n  trigger event", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models for landslide hazard enable mapping of risk factors and\nlandslide occurrence intensity by using geomorphological covariates available\nat high spatial resolution. However, the spatial distribution of the triggering\nevent (e.g., precipitation or earthquakes) is often not directly observed. In\nthis paper, we develop Bayesian spatial hierarchical models for point patterns\nof landslide occurrences using different types of log-Gaussian Cox processes.\nStarting from a competitive baseline model that captures the unobserved\nprecipitation trigger through a spatial random effect at slope unit resolution,\nwe explore novel complex model structures that take clusters of events arising\nat small spatial scales into account, as well as nonlinear or spatially-varying\ncovariate effects. For a 2009 event of around 4000 precipitation-triggered\nlandslides in Sicily, Italy, we show how to fit our proposed models efficiently\nusing the integrated nested Laplace approximation (INLA), and rigorously\ncompare the performance of our models both from a statistical and applied\nperspective. In this context, we argue that model comparison should not be\nbased on a single criterion, and that different models of various complexity\nmay provide insights into complementary aspects of the same applied problem. In\nour application, our models are found to have mostly the same spatial\npredictive performance, implying that key to successful prediction is the\ninclusion of a slope-unit resolved random effect capturing the precipitation\ntrigger. Interestingly, a parsimonious formulation of space-varying slope\neffects reflects a physical interpretation of the precipitation trigger: in\nsubareas with weak trigger, the slope steepness is shown to be mostly\nirrelevant.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 13:42:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Opitz", "Thomas", ""], ["Bakka", "Haakon", ""], ["Huser", "Rapha\u00ebl", ""], ["Lombardo", "Luigi", ""]]}, {"id": "2006.07924", "submitter": "Chuang Wan", "authors": "Wei Zhong, Chuang Wan and Wenyang Zhang", "title": "Estimation and Inference for Multi-Kink Quantile Regression", "comments": "39pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multi-Kink Quantile Regression (MKQR) model is an important tool for\nanalyzing data with heterogeneous conditional distributions, especially when\nquantiles of response variable are of interest, due to its robustness to\noutliers and heavy-tailed errors in the response. It assumes different linear\nquantile regression forms in different regions of the domain of the threshold\ncovariate but are still continuous at kink points. In this paper, we\ninvestigate parameter estimation, kink point detection and statistical\ninference in MKQR models. We propose an iterative segmented quantile regression\nalgorithm for estimating both the regression coefficients and the locations of\nkink points. The proposed algorithm is much more computationally efficient than\nthe grid search algorithm and not sensitive to the selection of initial values.\nAsymptotic properties, such as selection consistency of the number of kink\npoints, asymptotic normality of the estimators of both regression coefficients\nand kink effects, are established to justify the proposed method theoretically.\nA score test, based on partial subgradients, is developed to verify whether the\nkink effects exist or not. Test-inversion confidence intervals for kink\nlocation parameters are also constructed. Intensive simulation studies\nconducted show the proposed methods work very well when sample size is finite.\nFinally, we apply the MKQR models together with the proposed methods to the\ndataset about secondary industrial structure of China and the dataset about\ntriceps skinfold thickness of Gambian females, which leads to some very\ninteresting findings. A new R package MultiKink is developed to implement the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 15:15:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhong", "Wei", ""], ["Wan", "Chuang", ""], ["Zhang", "Wenyang", ""]]}, {"id": "2006.08036", "submitter": "Marcos Prates O", "authors": "Victor H. Lachos Davila, Marcos O. Prates, Dipak K. Dey", "title": "Heckman selection-t model: parameter estimation via the EM-algorithm", "comments": "19 pages, 5 Tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heckman selection model is perhaps the most popular econometric model in the\nanalysis of data with sample selection. The analyses of this model are based on\nthe normality assumption for the error terms, however, in some applications,\nthe distribution of the error term departs significantly from normality, for\ninstance, in the presence of heavy tails and/or atypical observation. In this\npaper, we explore the Heckman selection-t model where the random errors follow\na bivariate Student's-t distribution. We develop an analytically tractable and\nefficient EM-type algorithm for iteratively computing maximum likelihood\nestimates of the parameters, with standard errors as a by-product. The\nalgorithm has closed-form expressions at the E-step, that rely on formulas for\nthe mean and variance of the truncated Student's-t distributions. Simulations\nstudies show the vulnerability of the Heckman selection-normal model, as well\nas the robustness aspects of the Heckman selection-t model. Two real examples\nare analyzed, illustrating the usefulness of the proposed methods. The proposed\nalgorithms and methods are implemented in the new R package HeckmanEM.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 21:59:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Davila", "Victor H. Lachos", ""], ["Prates", "Marcos O.", ""], ["Dey", "Dipak K.", ""]]}, {"id": "2006.08189", "submitter": "Anuran Makur", "authors": "Ali Jadbabaie and Anuran Makur and Devavrat Shah", "title": "Estimation of Skill Distributions", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning the skill distribution of a\npopulation of agents from observations of pairwise games in a tournament. These\ngames are played among randomly drawn agents from the population. The agents in\nour model can be individuals, sports teams, or Wall Street fund managers.\nFormally, we postulate that the likelihoods of game outcomes are governed by\nthe Bradley-Terry-Luce (or multinomial logit) model, where the probability of\nan agent beating another is the ratio between its skill level and the pairwise\nsum of skill levels, and the skill parameters are drawn from an unknown skill\ndensity of interest. The problem is, in essence, to learn a distribution from\nnoisy, quantized observations. We propose a simple and tractable algorithm that\nlearns the skill density with near-optimal minimax mean squared error scaling\nas $n^{-1+\\varepsilon}$, for any $\\varepsilon>0$, when the density is smooth.\nOur approach brings together prior work on learning skill parameters from\npairwise comparisons with kernel density estimation from non-parametric\nstatistics. Furthermore, we prove minimax lower bounds which establish minimax\noptimality of the skill parameter estimation technique used in our algorithm.\nThese bounds utilize a continuum version of Fano's method along with a covering\nargument. We apply our algorithm to various soccer leagues and world cups,\ncricket world cups, and mutual funds. We find that the entropy of a learnt\ndistribution provides a quantitative measure of skill, which provides rigorous\nexplanations for popular beliefs about perceived qualities of sporting events,\ne.g., soccer league rankings. Finally, we apply our method to assess the skill\ndistributions of mutual funds. Our results shed light on the abundance of low\nquality funds prior to the Great Recession of 2008, and the domination of the\nindustry by more skilled funds after the financial crisis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:35:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jadbabaie", "Ali", ""], ["Makur", "Anuran", ""], ["Shah", "Devavrat", ""]]}, {"id": "2006.08214", "submitter": "Yong He", "authors": "He Yong, Kong Xin-Bing, Yu Long, Zhang Xinsheng", "title": "Separating common and idiosyncratic components without moment condition\n  and on the weighted $L_1$ minimization path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-dimensional factor analysis, existing methods, such as principal\ncomponent analysis (PCA), assumed finite fourth moment of the idiosyncratic\ncomponents, in order to derive the convergence rates of the estimated factor\nloadings and scores. However, in many areas, such as finance and\nmacroeconomics, many variables are heavy-tailed. In this case, PCA-based\nestimators and their variations are not theoretically underpinned. In this\npaper, we investigate into the weighted $L_1$ minimization on the factor\nloadings and scores, which amounts to assuming a temporal and cross-sectional\nquantile structure for panel observations instead of the mean pattern in $L_2$\nminimization. Without any moment constraint on the idiosyncratic errors, we\ncorrectly identify the common and idiosyncratic components for each variable.\nWe obtained the convergence rates of a computationally feasible weighted $L_1$\nminimization estimators via iteratively alternating the quantile regression\ncross-sectionally and serially. Bahardur representations for the estimated\nfactor loadings and scores are provided under some mild conditions. In\naddition, a robust method is proposed to estimate the number of factors\nconsistently. Simulation experiments checked the validity of the theory. Our\nanalysis on a financial data set shows the superiority of the proposed method\nover other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:31:26 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 08:18:06 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 00:52:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Yong", "He", ""], ["Xin-Bing", "Kong", ""], ["Long", "Yu", ""], ["Xinsheng", "Zhang", ""]]}, {"id": "2006.08402", "submitter": "Stijn Vansteelandt", "authors": "Stijn Vansteelandt and Oliver Dukes", "title": "Assumption-lean inference for generalised linear model parameters", "comments": "37 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for the parameters indexing generalised linear models is routinely\nbased on the assumption that the model is correct and a priori specified. This\nis unsatisfactory because the chosen model is usually the result of a\ndata-adaptive model selection process, which may induce excess uncertainty that\nis not usually acknowledged. Moreover, the assumptions encoded in the chosen\nmodel rarely represent some a priori known, ground truth, making standard\ninferences prone to bias, but also failing to give a pure reflection of the\ninformation that is contained in the data. Inspired by developments on\nassumption-free inference for so-called projection parameters, we here propose\nnovel nonparametric definitions of main effect estimands and effect\nmodification estimands. These reduce to standard main effect and effect\nmodification parameters in generalised linear models when these models are\ncorrectly specified, but have the advantage that they continue to capture\nrespectively the primary (conditional) association between two variables, or\nthe degree to which two variables interact (in a statistical sense) in their\neffect on outcome, even when these models are misspecified. We achieve an\nassumption-lean inference for these estimands (and thus for the underlying\nregression parameters) by deriving their influence curve under the\nnonparametric model and invoking flexible data-adaptive (e.g., machine\nlearning) procedures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 13:49:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Vansteelandt", "Stijn", ""], ["Dukes", "Oliver", ""]]}, {"id": "2006.08442", "submitter": "Adeline Fermanian", "authors": "Adeline Fermanian", "title": "Functional linear regression with truncated signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We place ourselves in a functional regression setting and propose a novel\nmethodology for regressing a real output on vector-valued functional\ncovariates. This methodology is based on the notion of signature, which is a\nrepresentation of a function as an infinite series of its iterated integrals.\nThe signature depends crucially on a truncation parameter for which an\nestimator is provided, together with theoretical guarantees. An empirical study\non both simulated and real-world datasets shows that the resulting methodology\nis competitive with traditional functional linear models, in particular when\nthe functional covariates take their values in a high dimensional space.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:40:10 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 19:16:51 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 13:58:07 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Fermanian", "Adeline", ""]]}, {"id": "2006.08482", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich and Aaditya Ramdas", "title": "The leave-one-covariate-out conditional randomization test", "comments": "This paper has been withdrawn by the authors, because it has now been\n  merged with (and superseded by) a parallel work arXiv:2006.03980 by Molei Liu\n  and Lucas Janson", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence testing is an important problem, yet provably hard\nwithout assumptions. One of the assumptions that has become popular of late is\ncalled \"model-X\", where we assume we know the joint distribution of the\ncovariates, but assume nothing about the conditional distribution of the\noutcome given the covariates. Knockoffs is a popular methodology associated\nwith this framework, but it suffers from two main drawbacks: only one-bit\n$p$-values are available for inference on each variable, and the method is\nrandomized with significant variability across runs in practice. The\nconditional randomization test (CRT) is thought to be the \"right\" solution\nunder model-X, but usually viewed as computationally inefficient. This paper\nproposes a computationally efficient leave-one-covariate-out (LOCO) CRT that\naddresses both drawbacks of knockoffs. LOCO CRT produces valid $p$-values that\ncan be used to control the familywise error rate, and has nearly zero\nalgorithmic variability. For L1 regularized M-estimators, we develop an even\nfaster variant called L1ME CRT, which reuses computation by leveraging a novel\nobservation about the stability of the cross-validated lasso to removing\ninactive variables. Last, for multivariate Gaussian covariates, we present a\nclosed form expression for the LOCO CRT $p$-value, thus completely eliminating\nresampling in this important special case.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:38:24 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 14:34:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Katsevich", "Eugene", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.08522", "submitter": "Ruobin Gong", "authors": "Ruobin Gong", "title": "Transparent Privacy is Principled Privacy", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy revolutionizes the way we think about statistical\ndisclosure limitation. Among the benefits it brings to the table, one is\nparticularly profound and impactful. Under this formal approach to privacy, the\nmechanism with which data is privatized can be spelled out in full\ntransparency, without sacrificing the privacy guarantee. Curators of\nopen-source demographic and scientific data are at a position to offer privacy\nwithout obscurity. This paper supplies a technical treatment to the pitfalls of\nobscure privacy, and establishes transparent privacy as a prerequisite to\ndrawing correct statistical inference. It advocates conceiving transparent\nprivacy as a dynamic component that can improve data quality from the total\nsurvey error perspective, and discusses the limited statistical usability of\nmere procedural transparency which may arise when dealing with mandated\ninvariants. Transparent privacy is the only viable path towards principled\ninference from privatized data releases. Its arrival marks great progress\ntowards improved reproducibility, accountability and public trust.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:30:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Gong", "Ruobin", ""]]}, {"id": "2006.08570", "submitter": "Tommaso Di Fonzo", "authors": "Tommaso Di Fonzo, Daniele Girolimetto", "title": "Cross-temporal forecast reconciliation: Optimal combination method and\n  heuristic alternatives", "comments": "Main text: 49 pages, 10 figures, 2 tables. Appendix: 68 pages, 29\n  figures, 17 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecast reconciliation is a post-forecasting process aimed to improve the\nquality of the base forecasts for a system of hierarchical/grouped time series\n(Hyndman et al., 2011). Contemporaneous (cross-sectional) and temporal\nhierarchies have been considered in the literature, but - except for Kourentzes\nand Athanasopoulos (2019) - generally these two features have not been fully\nconsidered together. Adopting a notation able to simultaneously deal with both\nforecast reconciliation dimensions, the paper shows two new results: (i) an\niterative cross-temporal forecast reconciliation procedure which extends, and\novercomes some weaknesses of, the two-step procedure by Kourentzes and\nAthanasopoulos (2019), and (ii) the closed-form expression of the optimal (in\nleast squares sense) point forecasts which fulfill both contemporaneous and\ntemporal constraints. The feasibility of the proposed procedures, along with\nfirst evaluations of their performance as compared to the most performing\n`single dimension' (either cross-sectional or temporal) forecast reconciliation\nprocedures, is studied through a forecasting experiment on the 95 quarterly\ntime series of the Australian GDP from Income and Expenditure sides considered\nby Athanasopoulos et al. (2019).\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:34:05 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 14:15:59 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 19:18:51 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Di Fonzo", "Tommaso", ""], ["Girolimetto", "Daniele", ""]]}, {"id": "2006.08598", "submitter": "Lun Wang", "authors": "Lun Wang and Qi Pang and Dawn Song", "title": "Towards practical differentially private causal graph discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal graph discovery refers to the process of discovering causal relation\ngraphs from purely observational data. Like other statistical data, a causal\ngraph might leak sensitive information about participants in the dataset. In\nthis paper, we present a differentially private causal graph discovery\nalgorithm, Priv-PC, which improves both utility and running time compared to\nthe state-of-the-art. The design of Priv-PC follows a novel paradigm called\nsieve-and-examine which uses a small amount of privacy budget to filter out\n\"insignificant\" queries, and leverages the remaining budget to obtain highly\naccurate answers for the \"significant\" queries. We also conducted the first\nsensitivity analysis for conditional independence tests including conditional\nKendall's tau and conditional Spearman's rho. We evaluated Priv-PC on 4 public\ndatasets and compared with the state-of-the-art. The results show that Priv-PC\nachieves 10.61 to 32.85 times speedup and better utility.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:30:41 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Wang", "Lun", ""], ["Pang", "Qi", ""], ["Song", "Dawn", ""]]}, {"id": "2006.08790", "submitter": "Quentin Rebjock", "authors": "Armin Askari, Quentin Rebjock, Alexandre d'Aspremont and Laurent El\n  Ghaoui", "title": "FANOK: Knockoffs in Linear Time", "comments": "For code see https://github.com/qrebjock/fanok", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a series of algorithms that efficiently implement Gaussian\nmodel-X knockoffs to control the false discovery rate on large scale feature\nselection problems. Identifying the knockoff distribution requires solving a\nlarge scale semidefinite program for which we derive several efficient methods.\nOne handles generic covariance matrices, has a complexity scaling as $O(p^3)$\nwhere $p$ is the ambient dimension, while another assumes a rank $k$ factor\nmodel on the covariance matrix to reduce this complexity bound to $O(pk^2)$. We\nalso derive efficient procedures to both estimate factor models and sample\nknockoff covariates with complexity linear in the dimension. We test our\nmethods on problems with $p$ as large as $500,000$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 21:55:34 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Askari", "Armin", ""], ["Rebjock", "Quentin", ""], ["d'Aspremont", "Alexandre", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "2006.08807", "submitter": "Pingye Zhang", "authors": "Pingye Zhang, Junshui Ma, Xinqun Chen, Yue Shentu", "title": "A Nonparametric Method for Value Function Guided Subgroup Identification\n  via Gradient Tree Boosting for Censored Survival Data", "comments": "33 pages, 3 figures, 4 tables. Revisions Submitted to Statistics in\n  Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In randomized clinical trials with survival outcome, there has been an\nincreasing interest in subgroup identification based on baseline genomic,\nproteomic markers or clinical characteristics. Some of the existing methods\nidentify subgroups that benefit substantially from the experimental treatment\nby directly modeling outcomes or treatment effect. When the goal is to find an\noptimal treatment for a given patient rather than finding the right patient for\na given treatment, methods under the individualized treatment regime framework\nestimate an individualized treatment rule that would lead to the best expected\nclinical outcome as measured by a value function. Connecting the concept of\nvalue function to subgroup identification, we propose a nonparametric method\nthat searches for subgroup membership scores by maximizing a value function\nthat directly reflects the subgroup-treatment interaction effect based on\nrestricted mean survival time. A gradient tree boosting algorithm is proposed\nto search for the individual subgroup membership scores. We conduct simulation\nstudies to evaluate the performance of the proposed method and an application\nto an AIDS clinical trial is performed for illustration.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 22:26:58 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Pingye", ""], ["Ma", "Junshui", ""], ["Chen", "Xinqun", ""], ["Shentu", "Yue", ""]]}, {"id": "2006.08855", "submitter": "Ye Tian", "authors": "Ye Tian and Yang Feng", "title": "RaSE: Random Subspace Ensemble Classification", "comments": "93 pages, 13 figures", "journal-ref": "Journal of Machine Learning Research 22, no. 45 (2021): 1-93", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible ensemble classification framework, Random Subspace\nEnsemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate\nmany weak learners, where each weak learner is a base classifier trained in a\nsubspace optimally selected from a collection of random subspaces. To conduct\nsubspace selection, we propose a new criterion, ratio information criterion\n(RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis\nincludes the risk and Monte-Carlo variance of the RaSE classifier, establishing\nthe screening consistency and weak consistency of RIC, and providing an upper\nbound for the misclassification rate of the RaSE classifier. In addition, we\nshow that in a high-dimensional framework, the number of random subspaces needs\nto be very large to guarantee that a subspace covering signals is selected.\nTherefore, we propose an iterative version of the RaSE algorithm and prove that\nunder some specific conditions, a smaller number of generated random subspaces\nare needed to find a desirable subspace through iteration. An array of\nsimulations under various models and real-data applications demonstrate the\neffectiveness and robustness of the RaSE classifier and its iterative version\nin terms of low misclassification rate and accurate feature ranking. The RaSE\nalgorithm is implemented in the R package RaSEn on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:14:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 18:59:19 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 15:54:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2006.08864", "submitter": "Hangjin Jiang", "authors": "Hangjin Jiang", "title": "A Goodness-of-Fit Test for Statistical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling plays a fundamental role in understanding the underlying\nmechanism of massive data (statistical inference) and predicting the future\n(statistical prediction). Although all models are wrong, researchers try their\nbest to make some of them be useful. The question here is how can we measure\nthe usefulness of a statistical model for the data in hand? This is key to\nstatistical prediction. The important statistical problem of testing whether\nthe observations follow the proposed statistical model has only attracted\nrelatively few attentions. In this paper, we proposed a new framework for this\nproblem through building its connection with two-sample distribution\ncomparison. The proposed method can be applied to evaluate a wide range of\nmodels. Examples are given to show the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:58:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Jiang", "Hangjin", ""]]}, {"id": "2006.09009", "submitter": "Xiaomin Zhang", "authors": "Xiaomin Zhang, Xiaojin Zhu, Po-Ling Loh", "title": "Theory of Machine Learning Debugging via M-estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate problems in penalized $M$-estimation, inspired by applications\nin machine learning debugging. Data are collected from two pools, one\ncontaining data with possibly contaminated labels, and the other which is known\nto contain only cleanly labeled points. We first formulate a general\nstatistical algorithm for identifying buggy points and provide rigorous\ntheoretical guarantees under the assumption that the data follow a linear\nmodel. We then present two case studies to illustrate the results of our\ngeneral theory and the dependence of our estimator on clean versus buggy\npoints. We further propose an algorithm for tuning parameter selection of our\nLasso-based algorithm and provide corresponding theoretical guarantees.\nFinally, we consider a two-person \"game\" played between a bug generator and a\ndebugger, where the debugger can augment the contaminated data set with cleanly\nlabeled versions of points in the original data pool. We establish a\ntheoretical result showing a sufficient condition under which the bug generator\ncan always fool the debugger. Nonetheless, we provide empirical results showing\nthat such a situation may not occur in practice, making it possible for natural\naugmentation strategies combined with our Lasso debugging algorithm to succeed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:14:44 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Xiaomin", ""], ["Zhu", "Xiaojin", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2006.09061", "submitter": "Jack Jewson", "authors": "Beniamino Hadj-Amar, Jack Jewson and Mark Fiecas", "title": "Bayesian Approximations to Hidden Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian hidden Markov model for analyzing time series and\nsequential data where a special structure of the transition probability matrix\nis embedded to model explicit-duration semi-Markovian dynamics. Our formulation\nallows for the development of highly flexible and interpretable models that can\nintegrate available prior information on state durations while keeping a\nmoderate computational cost to perform efficient posterior inference. We show\nthe benefits of choosing a Bayesian approach over its frequentist counterpart,\nin terms of incorporation of prior information, quantification of uncertainty,\nmodel selection and out-of-sample forecasting. The use of our methodology is\nillustrated in an application relevant to e-Health, where we investigate\nrest-activity rhythms using telemetric activity data collected via a wearable\nsensing device.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:41:59 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Hadj-Amar", "Beniamino", ""], ["Jewson", "Jack", ""], ["Fiecas", "Mark", ""]]}, {"id": "2006.09157", "submitter": "Laura Wendelberger", "authors": "Laura J. Wendelberger, Brian J. Reich, Alyson G. Wilson", "title": "Multi-Model Penalized Regression", "comments": "39 Pages, 16 Figures. Submitted to Statistical Analysis and Data\n  Mining. Presented at Conference on Data Analysis (CoDA) 2020 (Feb 25-27)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model fitting often aims to fit a single model, assuming that the imposed\nform of the model is correct. However, there may be multiple possible\nunderlying explanatory patterns in a set of predictors that could explain a\nresponse. Model selection without regarding model uncertainty can fail to bring\nthese patterns to light. We present multi-model penalized regression (MMPR) to\nacknowledge model uncertainty in the context of penalized regression. In the\npenalty form explored here, we examine how different settings can promote\neither shrinkage or sparsity of coefficients in separate models. The method is\ntuned to explicitly limit model similarity. A choice of penalty form that\nenforces variable selection is applied to predict stacking fault energy (SFE)\nfrom steel alloy composition. The aim is to identify multiple models with\ndifferent subsets of covariates that explain a single type of response.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:06:55 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 03:56:26 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wendelberger", "Laura J.", ""], ["Reich", "Brian J.", ""], ["Wilson", "Alyson G.", ""]]}, {"id": "2006.09219", "submitter": "Alexander Henzi", "authors": "Alexander Henzi, Gian-Reto Kleger, Johanna F. Ziegel", "title": "Distributional (Single) Index Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Distributional (Single) Index Model (DIM) is a semi-parametric model for\ndistributional regression, that is, estimation of conditional distributions\ngiven covariates. The method is a combination of classical single index models\nfor the estimation of the conditional mean of a response given covariates, and\nisotonic distributional regression. The model for the index is parametric,\nwhereas the conditional distributions are estimated non-parametrically under a\nstochastic ordering constraint. We show consistency of our estimators and apply\nthem to a highly challenging data set on the length of stay (LoS) of patients\nin intensive care units. We use the model to provide skillful and calibrated\nprobabilistic predictions for the LoS of individual patients, that outperform\nthe available methods in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:50:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Henzi", "Alexander", ""], ["Kleger", "Gian-Reto", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "2006.09278", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "An one-factor copula mixed model for joint meta-analysis of multiple\n  diagnostic tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the meta-analysis of more than one diagnostic tests can impact clinical\ndecision making and patient health, there is an increasing body of research in\nmodels and methods for meta-analysis of studies comparing multiple diagnostic\ntests. The application of the existing models to compare the accuracy of three\nor more tests suffers from the curse of multi-dimensionality, i.e., either the\nnumber of model parameters increase rapidly or high dimensional integration is\nrequired. To overcome these issues in joint meta-analysis of studies comparing\n$T >2$ diagnostic tests in a multiple tests design with a gold standard, we\npropose a model that assumes the true positives and true negatives for each\ntest are conditionally independent and binomially distributed given the\n$2T$-variate latent vector of sensitivities and specificities. For the random\neffects distribution, we employ an one-factor copula that provides tail\ndependence or tail asymmetry. Maximum likelihood estimation of the model is\nstraightforward as the derivation of the likelihood requires bi-dimensional\ninstead of $2T$-dimensional integration. Our methodology is demonstrated with\nan extensive simulation study and an application example that determines which\nis the best test for the diagnosis of rheumatoid arthritis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:17:08 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 07:41:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "2006.09329", "submitter": "Philip White", "authors": "Philip White, Durban Keeler, Daniel Sheanshang, Summer Rupper", "title": "Improving Piecewise Linear Snow Density Models through Hierarchical\n  Spatial and Orthogonal Functional Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snow density estimates as a function of depth are used for understanding\nclimate processes, evaluating water accumulation trends in polar regions, and\nestimating glacier mass balances. The common and interpretable\nphysically-derived differential equation models for snow density are piecewise\nlinear as a function of depth (on a transformed scale); thus, they can fail to\ncapture important data features. Moreover, the differential equation parameters\nshow strong spatial autocorrelation. To address these issues, we allow the\nparameters of the physical model, including random change points over depth, to\nvary spatially. We also develop a framework for functionally smoothing the\nphysically-motivated model. To preserve inference on the interpretable physical\nmodel, we project the smoothing function into the physical model's spatially\nvarying null space. The proposed spatially and functionally smoothed snow\ndensity model better fits the data while preserving inference on physical\nparameters. Using this model, we find significant spatial variation in the\nparameters that govern snow densification.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:11:03 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 20:28:47 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 17:11:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["White", "Philip", ""], ["Keeler", "Durban", ""], ["Sheanshang", "Daniel", ""], ["Rupper", "Summer", ""]]}, {"id": "2006.09481", "submitter": "Brian Williamson", "authors": "Brian D. Williamson and Jean Feng", "title": "Efficient nonparametric statistical inference on population feature\n  importance using Shapley values", "comments": "15 pages, 3 figures. To be published in the Proceedings of the\n  Thirty-seventh International Conference on Machine Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The true population-level importance of a variable in a prediction task\nprovides useful knowledge about the underlying data-generating mechanism and\ncan help in deciding which measurements to collect in subsequent experiments.\nValid statistical inference on this importance is a key component in\nunderstanding the population of interest. We present a computationally\nefficient procedure for estimating and obtaining valid statistical inference on\nthe Shapley Population Variable Importance Measure (SPVIM). Although the\ncomputational complexity of the true SPVIM scales exponentially with the number\nof variables, we propose an estimator based on randomly sampling only\n$\\Theta(n)$ feature subsets given $n$ observations. We prove that our estimator\nconverges at an asymptotically optimal rate. Moreover, by deriving the\nasymptotic distribution of our estimator, we construct valid confidence\nintervals and hypothesis tests. Our procedure has good finite-sample\nperformance in simulations, and for an in-hospital mortality prediction task\nproduces similar variable importance estimates when different machine learning\nalgorithms are applied.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 19:47:11 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Williamson", "Brian D.", ""], ["Feng", "Jean", ""]]}, {"id": "2006.09525", "submitter": "Thi Tuyet Trang Chau", "authors": "Thi Tuyet Trang Chau, Pierre Ailliot, Val\\'erie Monbet", "title": "An algorithm for non-parametric estimation in state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models are ubiquitous in the statistical literature since they\nprovide a flexible and interpretable framework for analyzing many time series.\nIn most practical applications, the state-space model is specified through a\nparametric model. However, the specification of such a parametric model may\nrequire an important modeling effort or may lead to models which are not\nflexible enough to reproduce all the complexity of the phenomenon of interest.\nIn such situations, an appealing alternative consists in inferring the\nstate-space model directly from the data using a non-parametric framework. The\nrecent developments of powerful simulation techniques have permitted to improve\nthe statistical inference for parametric state-space models. It is proposed to\ncombine two of these techniques, namely the Stochastic Expectation-Maximization\n(SEM) algorithm and Sequential Monte Carlo (SMC) approaches, for non-parametric\nestimation in state-space models. The performance of the proposed algorithm is\nassessed through simulations on toy models and an application to environmental\ndata is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 21:28:52 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Chau", "Thi Tuyet Trang", ""], ["Ailliot", "Pierre", ""], ["Monbet", "Val\u00e9rie", ""]]}, {"id": "2006.09587", "submitter": "Christoph Breunig", "authors": "Christoph Breunig, Xiaohong Chen", "title": "Adaptive, Rate-Optimal Testing in Instrumental Variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes simple, data-driven, optimal rate-adaptive inferences on\na structural function in semi-nonparametric conditional moment restrictions. We\nconsider two types of hypothesis tests based on leave-one-out sieve estimators.\nA structure-space test (ST) uses a quadratic distance between the structural\nfunctions of endogenous variables; while an image-space test (IT) uses a\nquadratic distance of the conditional moment from zero. For both tests, we\nanalyze their respective classes of nonparametric alternative models that are\nseparated from the null hypothesis by the minimax rate of testing. That is, the\nsum of the type I and the type II errors of the test, uniformly over the class\nof nonparametric alternative models, cannot be improved by any other test. Our\nnew minimax rate of ST differs from the known minimax rate of estimation in\nnonparametric instrumental variables (NPIV) models. We propose computationally\nsimple and novel exponential scan data-driven choices of sieve regularization\nparameters and adjusted chi-squared critical values. The resulting tests attain\nthe minimax rate of testing, and hence optimally adapt to the unknown\nsmoothness of functions and are robust to the unknown degree of ill-posedness\n(endogeneity). Data-driven confidence sets are easily obtained by inverting the\nadaptive ST. Monte Carlo studies demonstrate that our adaptive ST has good size\nand power properties in finite samples for testing monotonicity or equality\nrestrictions in NPIV models. Empirical applications to nonparametric\nmulti-product demands with endogenous prices are presented.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 01:19:13 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Breunig", "Christoph", ""], ["Chen", "Xiaohong", ""]]}, {"id": "2006.09590", "submitter": "Barinder Thind", "authors": "Barinder Thind, Kevin Multani, Jiguo Cao", "title": "Deep Learning with Functional Inputs", "comments": "28 pages, 6 figures, submitted to JCGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a methodology for integrating functional data into deep densely\nconnected feed-forward neural networks. The model is defined for scalar\nresponses with multiple functional and scalar covariates. A by-product of the\nmethod is a set of dynamic functional weights that can be visualized during the\noptimization process. This visualization leads to greater interpretability of\nthe relationship between the covariates and the response relative to\nconventional neural networks. The model is shown to perform well in a number of\ncontexts including prediction of new data and recovery of the true underlying\nfunctional weights; these results were confirmed through real applications and\nsimulation studies. A forthcoming R package is developed on top of a popular\ndeep learning library (Keras) allowing for general use of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 01:23:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Thind", "Barinder", ""], ["Multani", "Kevin", ""], ["Cao", "Jiguo", ""]]}, {"id": "2006.09613", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Sivaraman Balakrishnan, Larry A. Wasserman", "title": "Discussion of \"On nearly assumption-free tests of nominal confidence\n  interval coverage for causal parameters estimated by machine learning\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We congratulate the authors on their exciting paper, which introduces a novel\nidea for assessing the estimation bias in causal estimates. Doubly robust\nestimators are now part of the standard set of tools in causal inference, but a\ntypical analysis stops with an estimate and a confidence interval. The authors\ngive an approach for a unique type of model-checking that allows the user to\ncheck whether the bias is sufficiently small with respect to the standard\nerror, which is generally required for confidence intervals to be reliable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 02:42:21 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry A.", ""]]}, {"id": "2006.09660", "submitter": "Yaqing Chen", "authors": "Yaqing Chen, Zhenhua Lin, and Hans-Georg M\\\"uller", "title": "Wasserstein Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of samples of random objects that do not lie in a vector space\nis gaining increasing attention in statistics. An important class of such\nobject data is univariate probability measures defined on the real line.\nAdopting the Wasserstein metric, we develop a class of regression models for\nsuch data, where random distributions serve as predictors and the responses are\neither also distributions or scalars. To define this regression model, we\nutilize the geometry of tangent bundles of the space of random measures endowed\nwith the Wasserstein metric for mapping distributions to tangent spaces. The\nproposed distribution-to-distribution regression model provides an extension of\nmultivariate linear regression for Euclidean data and function-to-function\nregression for Hilbert space valued data in functional data analysis. In\nsimulations, it performs better than an alternative transformation approach\nwhere one maps distributions to a Hilbert space through the log quantile\ndensity transformation and then applies traditional functional regression. We\nderive asymptotic rates of convergence for the estimator of the regression\noperator and for predicted distributions and also study an extension to\nautoregressive models for distribution-valued time series. The proposed methods\nare illustrated with data on human mortality and distributional time series of\nhouse prices.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:12:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:24:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Yaqing", ""], ["Lin", "Zhenhua", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2006.09666", "submitter": "Xuan Yin", "authors": "Zenan Wang, Xuan Yin, Tianbo Li, Liangjie Hong", "title": "Causal Meta-Mediation Analysis: Inferring Dose-Response Function From\n  Summary Statistics of Many Randomized Experiments", "comments": "In Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "journal-ref": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "doi": "10.1145/3394486.3403313", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in the internet industry to use offline-developed algorithms to\npower online products that contribute to the success of a business.\nOffline-developed algorithms are guided by offline evaluation metrics, which\nare often different from online business key performance indicators (KPIs). To\nmaximize business KPIs, it is important to pick a north star among all\navailable offline evaluation metrics. By noting that online products can be\nmeasured by online evaluation metrics, the online counterparts of offline\nevaluation metrics, we decompose the problem into two parts. As the offline A/B\ntest literature works out the first part: counterfactual estimators of offline\nevaluation metrics that move the same way as their online counterparts, we\nfocus on the second part: causal effects of online evaluation metrics on\nbusiness KPIs. The north star of offline evaluation metrics should be the one\nwhose online counterpart causes the most significant lift in the business KPI.\nWe model the online evaluation metric as a mediator and formalize its causality\nwith the business KPI as dose-response function (DRF). Our novel approach,\ncausal meta-mediation analysis, leverages summary statistics of many existing\nrandomized experiments to identify, estimate, and test the mediator DRF. It is\neasy to implement and to scale up, and has many advantages over the literature\nof mediation analysis and meta-analysis. We demonstrate its effectiveness by\nsimulation and implementation on real data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:41:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Zenan", ""], ["Yin", "Xuan", ""], ["Li", "Tianbo", ""], ["Hong", "Liangjie", ""]]}, {"id": "2006.09676", "submitter": "Guido Imbens", "authors": "Susan Athey and Raj Chetty and Guido Imbens", "title": "Combining Experimental and Observational Data to Estimate Treatment\n  Effects on Long Term Outcomes", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increase in interest in experimental evaluations to\nestimate causal effects, partly because their internal validity tends to be\nhigh. At the same time, as part of the big data revolution, large, detailed,\nand representative, administrative data sets have become more widely available.\nHowever, the credibility of estimates of causal effects based on such data sets\nalone can be low.\n  In this paper, we develop statistical methods for systematically combining\nexperimental and observational data to obtain credible estimates of the causal\neffect of a binary treatment on a primary outcome that we only observe in the\nobservational sample. Both the observational and experimental samples contain\ndata about a treatment, observable individual characteristics, and a secondary\n(often short term) outcome. To estimate the effect of a treatment on the\nprimary outcome while addressing the potential confounding in the observational\nsample, we propose a method that makes use of estimates of the relationship\nbetween the treatment and the secondary outcome from the experimental sample.\nIf assignment to the treatment in the observational sample were unconfounded,\nwe would expect the treatment effects on the secondary outcome in the two\nsamples to be similar. We interpret differences in the estimated causal effects\non the secondary outcome between the two samples as evidence of unobserved\nconfounders in the observational sample, and develop control function methods\nfor using those differences to adjust the estimates of the treatment effects on\nthe primary outcome.\n  We illustrate these ideas by combining data on class size and third grade\ntest scores from the Project STAR experiment with observational data on class\nsize and both third and eighth grade test scores from the New York school\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 06:30:48 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Athey", "Susan", ""], ["Chetty", "Raj", ""], ["Imbens", "Guido", ""]]}, {"id": "2006.09949", "submitter": "Flavio Goncalves", "authors": "Flavio B. Gon\\c{c}alves, Livia M. Dutra, Roger W. C. Silva", "title": "Exact and computationally efficient Bayesian inference for generalized\n  Markov modulated Poisson processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of point patterns is an important and common problem in\nseveral areas. The Poisson process is the most common process used for this\npurpose, in particular, its generalization that considers the intensity\nfunction to be stochastic. This is called a Cox process and different choices\nto model the dynamics of the intensity gives rise to a wide range of models. We\npresent a new class of unidimensional Cox process models in which the intensity\nfunction assumes parametric functional forms that switch among them according\nto a continuous-time Markov chain. A novel methodology is proposed to perform\nexact Bayesian inference based on MCMC algorithms. The term exact refers to the\nfact that no discrete time approximation is used and Monte Carlo error is the\nonly source of inaccuracy. The reliability of the algorithms depends on a\nvariety of specifications which are carefully addressed, resulting in a\ncomputationally efficient (in terms of computing time) algorithm and enabling\nits use with large data sets. Simulated and real examples are presented to\nillustrate the efficiency and applicability of the proposed methodology. A\nspecific model to fit epidemic curves is proposed and used to analyze data from\nDengue Fever in Brazil and COVID-19 in some countries.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:57:48 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 23:03:17 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 15:54:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Gon\u00e7alves", "Flavio B.", ""], ["Dutra", "Livia M.", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2006.09966", "submitter": "Flavio Goncalves", "authors": "Juliane Venturelli S. L., Flavio B. Gon\\c{c}alves, Dalton F. Andrade", "title": "Multidimensional Bayesian IRT Model for Hierarchical Latent Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is reasonable to consider, in many cases, that individuals' latent traits\nhave a hierarchical structure such that more general traits are a suitable\ncomposition of more specific ones. Existing item response models that account\nfor such hierarchical structure feature have considerable limitations in terms\nof modelling and/or inference. Motivated by those limitations and the\nimportance of the theme, this paper aims at proposing an improved methodology\nin terms of both modelling and inference to deal with hierarchically structured\nlatent traits in an item response theory context. From a modelling perspective,\nthe proposed methodology allows for genuinely multidimensional items and all of\nthe latent traits in the assumed hierarchical structure are on the same scale.\nItems are allowed to be dichotomous or of graded response. An efficient MCMC\nalgorithm is carefully devised to sample from the joint posterior distribution\nof all the unknown quantities of the proposed model. In particular, all the\nlatent trait parameters are jointly sampled from their full conditional\ndistribution in a Gibbs sampling algorithm. The proposed methodology is applied\nto simulated data and a real dataset concerning the Enem exam in Brazil.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:24:12 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 23:24:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["L.", "Juliane Venturelli S.", ""], ["Gon\u00e7alves", "Flavio B.", ""], ["Andrade", "Dalton F.", ""]]}, {"id": "2006.10005", "submitter": "Esa Ollila", "authors": "Esa Ollila, Daniel P. Palomar and Fr\\'ed\\'eric Pascal", "title": "Shrinking the eigenvalues of M-estimators of covariance matrix", "comments": "A supplementary report is available at:\n  http://users.spa.aalto.fi/esollila/shrinkM/supplement.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly popular regularized (shrinkage) covariance matrix estimator is the\nshrinkage sample covariance matrix (SCM) which shares the same set of\neigenvectors as the SCM but shrinks its eigenvalues toward the grand mean of\nthe eigenvalues of the SCM. In this paper, a more general approach is\nconsidered in which the SCM is replaced by an M-estimator of scatter matrix and\na fully automatic data adaptive method to compute the optimal shrinkage\nparameter with minimum mean squared error is proposed. Our approach permits the\nuse of any weight function such as Gaussian, Huber's, Tyler's, or t-weight\nfunctions, all of which are commonly used in M-estimation framework. Our\nsimulation examples illustrate that shrinkage M-estimators based on the\nproposed optimal tuning combined with robust weight function do not loose in\nperformance to shrinkage SCM estimator when the data is Gaussian, but provide\nsignificantly improved performance when the data is sampled from an unspecified\nheavy-tailed elliptically symmetric distribution. Also, real-world and\nsynthetic stock market data validate the performance of the proposed method in\npractical applications.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:11:14 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 12:03:22 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ollila", "Esa", ""], ["Palomar", "Daniel P.", ""], ["Pascal", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2006.10018", "submitter": "Ahad Jamalizadeh", "authors": "Me'raj Abdi, Mohsen Madadi, N. Balakrishnan, Ahad Jamalizadeh", "title": "Family of mean-mixtures of multivariate normal distributions:\n  properties, inference and assessment of multivariate skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new mixture family of multivariate normal distributions,\nformed by mixing multivariate normal distribution and skewed distribution, is\nconstructed. Some properties of this family, such as characteristic function,\nmoment generating function, and the first four moments are derived. The\ndistributions of affine transformations and canonical forms of the model are\nalso derived. An EM type algorithm is developed for the maximum likelihood\nestimation of model parameters. We have considered in detail, some special\ncases of the family, using standard gamma and standard exponential mixture\ndistributions, denoted by MMNG and MMNE, respectively. For the proposed family\nof distributions, different multivariate measures of skewness are computed. In\norder to examine the performance of the developed estimation method, some\nsimulation studies are carried out to show that the maximum likelihood\nestimates based on the EM type algorithm do provide good performance. For\ndifferent choices of parameters of MMNE distribution, several multivariate\nmeasures of skewness are computed and compared. Because some measures of\nskewness are scalar and some are vectors, in order to evaluate them properly,\nwe have carried out a simulation study to determine the power of tests, based\non sample versions of skewness measures as test statistics to test the fit of\nthe MMNE distribution. Finally, two real data sets are used to illustrate the\nusefulness of the proposed family of distributions and the associated\ninferential method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:22:30 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 07:45:26 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Abdi", "Me'raj", ""], ["Madadi", "Mohsen", ""], ["Balakrishnan", "N.", ""], ["Jamalizadeh", "Ahad", ""]]}, {"id": "2006.10107", "submitter": "Marius Hofert", "authors": "Marius Hofert", "title": "Right-truncated Archimedean and related copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The copulas of random vectors with standard uniform univariate margins\ntruncated from the right are considered and a general formula for such\nright-truncated conditional copulas is derived. This formula is analytical for\ncopulas that can be inverted analytically as functions of each single argument.\nThis is the case, for example, for Archimedean and related copulas. The\nresulting right-truncated Archimedean copulas are not only analytically\ntractable but can also be characterized as tilted Archimedean copulas. This\nfinding allows one, for example, to more easily derive analytical properties\nsuch as the coefficients of tail dependence or sampling procedures of\nright-truncated Archimedean copulas. As another result, one can easily obtain a\nlimiting Clayton copula for a general vector of truncation points converging to\nzero; this is an important property for (re)insurance and a fact already known\nin the special case of equal truncation points, but harder to prove without\naforementioned characterization. Furthermore, right-truncated Archimax copulas\nwith logistic stable tail dependence functions are characterized as tilted\nouter power Archimedean copulas and an analytical form of right-truncated\nnested Archimedean copulas is also derived.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:15:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Hofert", "Marius", ""]]}, {"id": "2006.10126", "submitter": "Arvind Thiagarajan", "authors": "Arvind Thiagarajan", "title": "Using Weighted P-Values in Fisher's Method", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's method prescribes a way to combine p-values from multiple\nexperiments into a single p-value. However, the original method can only\ndetermine a combined p-value analytically if all constituent p-values are\nweighted equally. Here we present, with proof, a method to combine p-values\nwith arbitrary weights.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:56:09 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Thiagarajan", "Arvind", ""]]}, {"id": "2006.10163", "submitter": "Zhengjia Wang", "authors": "Zhengjia Wang, John Magnotti, Michael S. Beauchamp and Meng Li", "title": "Functional Group Bridge for Simultaneous Regression and Support\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by studying multisensory effects on brain\nactivities in intracranial electroencephalography (iEEG) experiments.\nDifferential brain activities to multisensory stimulus presentations are zero\nin most regions and non-zero in some local regions, yielding locally sparse\nfunctions. Such studies are essentially a function-on-scalar regression\nproblem, with interest being focused not only on estimating nonparametric\nfunctions but also on recovering the function supports. We propose a weighted\ngroup bridge approach for simultaneous function estimation and support recovery\nin function-on-scalar mixed effect models, while accounting for heterogeneity\npresent in functional data. We use B-splines to transform sparsity of functions\nto its sparse vector counterpart of increasing dimension, and propose a fast\nnon-convex optimization algorithm using nested alternative direction method of\nmultipliers (ADMM) for estimation. Large sample properties are established. In\nparticular, we show that the estimated coefficient functions are rate optimal\nin the minimax sense under the $L_2$ norm and resemble a phase transition\nphenomenon. For support estimation, we derive a convergence rate under the\n$L_{\\infty}$ norm that leads to a sparsistency property under\n$\\delta$-sparsity, and provide a simple sufficient regularity condition under\nwhich a strict sparsistency property is established. An adjusted extended\nBayesian information criterion is proposed for parameter tuning. The developed\nmethod is illustrated through simulation and an application to a novel iEEG\ndataset to study multisensory integration. We integrate the proposed method\ninto RAVE, an R package that gains increasing popularity in the iEEG community.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 21:18:53 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 18:53:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Zhengjia", ""], ["Magnotti", "John", ""], ["Beauchamp", "Michael S.", ""], ["Li", "Meng", ""]]}, {"id": "2006.10258", "submitter": "Chul Moon", "authors": "Chul Moon and Adel Bedoui", "title": "Bayesian Elastic Net based on Empirical Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Empirical likelihood is a popular nonparametric method for inference and\nestimation. In this article, we propose a Bayesian elastic net model that is\nbased on empirical likelihood for variable selection. The proposed method\nincorporates interpretability and robustness from Bayesian empirical likelihood\napproach. We derive asymptotic distributions of coefficients for credible\nintervals. The posterior distribution of Bayesian empirical likelihood does not\nhave a closed-form analytic expression and has nonconvex domain, which causes\nimplementation of MCMC challenging. To solve this problem, we implement the\nHamiltonian Monte Carlo approach. Simulation studies and real data analysis\ndemonstrate the advantages of the proposed method in variable selection and\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:43:00 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Moon", "Chul", ""], ["Bedoui", "Adel", ""]]}, {"id": "2006.10264", "submitter": "Hang Deng", "authors": "Hang Deng, Qiyang Han and Bodhisattva Sen", "title": "Inference for local parameters in convexity constrained models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference for local parameters of a convex\nregression function $f_0: [0,1] \\to \\mathbb{R}$ based on observations from a\nstandard nonparametric regression model, using the convex least squares\nestimator (LSE) $\\widehat{f}_n$. For $x_0 \\in (0,1)$, the local parameters\ninclude the pointwise function value $f_0(x_0)$, the pointwise derivative\n$f_0'(x_0)$, and the anti-mode (i.e., the smallest minimizer) of $f_0$. The\nexisting limiting distribution of the estimation error $(\\widehat{f}_n(x_0) -\nf_0(x_0), \\widehat{f}_n'(x_0) - f_0'(x_0) )$ depends on the unknown second\nderivative $f_0''(x_0)$, and is therefore not directly applicable for\ninference. To circumvent this impasse, we show that the following locally\nnormalized errors (LNEs) enjoy pivotal limiting behavior: Let\n$[\\widehat{u}(x_0), \\widehat{v}(x_0)]$ be the maximal interval containing $x_0$\nwhere $\\widehat{f}_n$ is linear. Then, under standard conditions, $$\\binom{\n\\sqrt{n(\\widehat{v}(x_0)-\\widehat{u}(x_0))}(\\widehat{f}_n(x_0)-f_0(x_0)) }{\n\\sqrt{n(\\widehat{v}(x_0)-\\widehat{u}(x_0))^3}(\\widehat{f}_n'(x_0)-f_0'(x_0))}\n\\rightsquigarrow \\sigma \\cdot \\binom{\\mathbb{L}^{(0)}_2}{\\mathbb{L}^{(1)}_2},$$\nwhere $n$ is the sample size, $\\sigma$ is the standard deviation of the errors,\nand $\\mathbb{L}^{(0)}_2, \\mathbb{L}^{(1)}_2$ are universal random variables.\nThis asymptotically pivotal LNE theory instantly yields a simple tuning-free\nprocedure for constructing CIs with asymptotically exact coverage and optimal\nlength for $f_0(x_0)$ and $f_0'(x_0)$. We also construct an asymptotically\npivotal LNE for the anti-mode of $f_0$, and its limiting distribution does not\neven depend on $\\sigma$. These asymptotically pivotal LNE theories are further\nextended to other convexity/concavity constrained models (e.g., log-concave\ndensity estimation) for which a limit distribution theory is available for\nproblem-specific estimators.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:02:20 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Deng", "Hang", ""], ["Han", "Qiyang", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2006.10265", "submitter": "Joann Jasiak", "authors": "Christian Gourieroux, Joann Jasiak", "title": "Analysis of Virus Propagation: A Transition Model Representation of\n  Stochastic Epidemiological Models", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing literature on the propagation of COVID-19 relies on various\ndynamic SIR-type models (Susceptible-Infected-Recovered) which yield\nmodel-dependent results. For transparency and ease of comparing the results, we\nintroduce a common representation of the SIR-type stochastic epidemiological\nmodels. This representation is a discrete time transition model, which allows\nus to classify the epidemiological models with respect to the number of states\n(compartments) and their interpretation. Additionally, the transition model\neliminates several limitations of the deterministic continuous time\nepidemiological models which are pointed out in the paper. We also show that\nall SIR-type models have a nonlinear (pseudo) state space representation and\nare easily estimable from an extended Kalman filter.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:03:05 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Gourieroux", "Christian", ""], ["Jasiak", "Joann", ""]]}, {"id": "2006.10266", "submitter": "Taylor Okonek", "authors": "Jon Wakefield, Taylor Okonek, Jon Pedersen", "title": "Small Area Estimation of Health Outcomes", "comments": "53 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small area estimation (SAE) entails estimating characteristics of interest\nfor domains, often geographical areas, in which there may be few or no samples\navailable. SAE has a long history and a wide variety of methods have been\nsuggested, from a bewildering range of philosophical standpoints. We describe\ndesign-based and model-based approaches and models that are specified at the\narea-level and at the unit-level, focusing on health applications and fully\nBayesian spatial models. The use of auxiliary information is a key ingredient\nfor successful inference when response data are sparse and we discuss a number\nof approaches that allow the inclusion of covariate data. SAE for HIV\nprevalence, using data collected from a Demographic Health Survey in Malawi in\n2015-2016, is used to illustrate a number of techniques. The potential use of\nSAE techniques for outcomes related to COVID-19 is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:04:22 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Wakefield", "Jon", ""], ["Okonek", "Taylor", ""], ["Pedersen", "Jon", ""]]}, {"id": "2006.10305", "submitter": "Hao Chen", "authors": "Hoseung Song and Hao Chen", "title": "Asymptotic distribution-free change-point detection for data with\n  repeated observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the regime of change-point detection, a nonparametric framework based on\nscan statistics utilizing graphs representing similarities among observations\nis gaining attention due to its flexibility and good performances for\nhigh-dimensional and non-Euclidean data sequences, which are ubiquitous in this\nbig data era. However, this graph-based framework encounters problems when\nthere are repeated observations in the sequence, which often happens for\ndiscrete data, such as network data. In this work, we extend the graph-based\nframework to solve this problem by averaging or taking union of all possible\n\"optimal\" graphs resulted from repeated observations. We consider both the\nsingle change-point alternative and the changed-interval alternative, and\nderive analytic formulas to control the type I error for the new methods,\nmaking them fast applicable to large data sets. The extended methods are\nillustrated on an application in detecting changes in a sequence of dynamic\nnetworks over time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 06:51:33 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Song", "Hoseung", ""], ["Chen", "Hao", ""]]}, {"id": "2006.10308", "submitter": "Ranjiva Munasinghe Dr.", "authors": "Ranjiva Munasinghe, Pathum Kossinna, Dovini Jayasinghe, Dilanka\n  Wijeratne", "title": "Fast Tail Index Estimation for Power Law Distributions in R", "comments": "54 pages, 14 figures, 26 tables, R-package ptsuite", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power law distributions, in particular Pareto distributions, describe data\nacross diverse areas of study. We have developed a package in R to estimate the\ntail index for such datasets focusing on speed (in particular with large\ndatasets), keeping in mind ease of use, as well as accuracy. In this document,\nwe provide a user guide to our package along with the results obtained\nhighlighting the speed advantages of our package.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 06:57:32 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Munasinghe", "Ranjiva", ""], ["Kossinna", "Pathum", ""], ["Jayasinghe", "Dovini", ""], ["Wijeratne", "Dilanka", ""]]}, {"id": "2006.10428", "submitter": "Tobias Siems", "authors": "Tobias Siems", "title": "Bayesian Changepoint Analysis", "comments": "My PhD Thesis can also be found here:\n  https://epub.ub.uni-greifswald.de/frontdoor/index/index/docId/3787", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In my PhD thesis, we elaborate upon Bayesian changepoint analysis, whereby\nour focus is on three big topics: approximate sampling via MCMC, exact\ninference and uncertainty quantification. Besides, modeling matters are\ndiscussed in an ongoing fashion. Our findings are underpinned through several\nchangepoint examples with a focus on a well-log drilling data.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 11:08:59 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Siems", "Tobias", ""]]}, {"id": "2006.10533", "submitter": "Thomas Jaki", "authors": "Lori E Dodd, Dean Follmann, Jing Wang, Franz Koenig, Lisa L Korn,\n  Christian Schoergenhofer, Michael Proschan, Sally Hunsberger, Tyler Bonnett,\n  Mat Makowski, Drifa Belhadi, Yeming Wang, Bin Cao, France Mentre, Thomas Jaki", "title": "Endpoints for randomized controlled clinical trials for COVID-19\n  treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Endpoint choice for randomized controlled trials of treatments\nfor COVID-19 is complex. A new disease brings many uncertainties, but trials\nmust start rapidly. COVID-19 is heterogeneous, ranging from mild disease that\nimproves within days to critical disease that can last weeks and can end in\ndeath. While improvement in mortality would provide unquestionable evidence\nabout clinical significance of a treatment, sample sizes for a study evaluating\nmortality are large and may be impractical. Furthermore, patient states in\nbetween \"cure\" and \"death\" represent meaningful distinctions. Clinical severity\nscores have been proposed as an alternative. However, the appropriate summary\nmeasure for severity scores has been the subject of debate, particularly in\nrelating to the uncertainty about the time-course of COVID-19. Outcomes\nmeasured at fixed time-points may risk missing the time of clinical benefit. An\nendpoint such as time-to-improvement (or recovery), avoids the timing problem.\nHowever, some have argued that power losses will result from reducing the\nordinal scale to a binary state of \"recovered\" vs \"not recovered.\"\n  Methods: We evaluate statistical power for possible trial endpoints for\nCOVID-19 treatment trials using simulation models and data from two recent\nCOVID-19 treatment trials.\n  Results: Power for fixed-time point methods depends heavily on the time\nselected for evaluation. Time-to-improvement (or recovery) analyses do not\nspecify a time-point. Time-to-event approaches have reasonable statistical\npower, even when compared to a fixed time-point method evaluated at the optimal\ntime.\n  Discussion: Time-to-event analyses methods have advantages in the COVID-19\nsetting, unless the optimal time for evaluating treatment effect is known in\nadvance. Even when the optimal time is known, a time-to-event approach may\nincrease power for interim analyses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:34:40 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Dodd", "Lori E", ""], ["Follmann", "Dean", ""], ["Wang", "Jing", ""], ["Koenig", "Franz", ""], ["Korn", "Lisa L", ""], ["Schoergenhofer", "Christian", ""], ["Proschan", "Michael", ""], ["Hunsberger", "Sally", ""], ["Bonnett", "Tyler", ""], ["Makowski", "Mat", ""], ["Belhadi", "Drifa", ""], ["Wang", "Yeming", ""], ["Cao", "Bin", ""], ["Mentre", "France", ""], ["Jaki", "Thomas", ""]]}, {"id": "2006.10564", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Aleksandr Podkopaev, Aaditya Ramdas", "title": "Distribution-free binary classification: prediction sets, confidence\n  intervals and calibration", "comments": "33 pages, 3 figures, appears as a spotlight at Neural Information\n  Processing Systems (NeurIPS) '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three notions of uncertainty quantification -- calibration,\nconfidence intervals and prediction sets -- for binary classification in the\ndistribution-free setting, that is without making any distributional\nassumptions on the data. With a focus towards calibration, we establish a\n'tripod' of theorems that connect these three notions for score-based\nclassifiers. A direct implication is that distribution-free calibration is only\npossible, even asymptotically, using a scoring function whose level sets\npartition the feature space into at most countably many sets. Parametric\ncalibration schemes such as variants of Platt scaling do not satisfy this\nrequirement, while nonparametric schemes based on binning do. To close the\nloop, we derive distribution-free confidence intervals for binned probabilities\nfor both fixed-width and uniform-mass binning. As a consequence of our 'tripod'\ntheorems, these confidence intervals for binned probabilities lead to\ndistribution-free calibration. We also derive extensions to settings with\nstreaming data and covariate shift.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:17:29 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:46:59 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 03:11:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Podkopaev", "Aleksandr", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.10593", "submitter": "Sai Li", "authors": "Sai Li and T. Tony Cai and Hongzhe Li", "title": "Transfer Learning for High-dimensional Linear Regression: Prediction,\n  Estimation, and Minimax Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the estimation and prediction of a high-dimensional\nlinear regression in the setting of transfer learning, using samples from the\ntarget model as well as auxiliary samples from different but possibly related\nregression models. When the set of \"informative\" auxiliary samples is known, an\nestimator and a predictor are proposed and their optimality is established. The\noptimal rates of convergence for prediction and estimation are faster than the\ncorresponding rates without using the auxiliary samples. This implies that\nknowledge from the informative auxiliary samples can be transferred to improve\nthe learning performance of the target problem. In the case that the set of\ninformative auxiliary samples is unknown, we propose a data-driven procedure\nfor transfer learning, called Trans-Lasso, and reveal its robustness to\nnon-informative auxiliary samples and its efficiency in knowledge transfer. The\nproposed procedures are demonstrated in numerical studies and are applied to a\ndataset concerning the associations among gene expressions. It is shown that\nTrans-Lasso leads to improved performance in gene expression prediction in a\ntarget tissue by incorporating the data from multiple different tissues as\nauxiliary samples.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:55:29 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Li", "Sai", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "2006.10646", "submitter": "Alejandro Calle-Saldarriaga", "authors": "Alejandro Calle-Saldarriaga, Henry Laniado, Francisco Zuluaga", "title": "Homogeneity Test for Functional Data based on Data-Depth Plots", "comments": "25 pages, 17 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classic concerns in statistics is determining if two samples come\nfrom thesame population, i.e. homogeneity testing. In this paper, we propose a\nhomogeneitytest in the context of Functional Data Analysis, adopting an idea\nfrom multivariatedata analysis: the data depth plot (DD-plot). This DD-plot is\na generalization of theunivariate Q-Q plot (quantile-quantile plot). We propose\nsome statistics based onthese DD-plots, and we use bootstrapping techniques to\nestimate their distributions.We estimate the finite-sample size and power of\nour test via simulation, obtainingbetter results than other homogeneity test\nproposed in the literature. Finally, weillustrate the procedure in samples of\nreal heterogeneous data and get consistent results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:16:51 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:39:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Calle-Saldarriaga", "Alejandro", ""], ["Laniado", "Henry", ""], ["Zuluaga", "Francisco", ""]]}, {"id": "2006.10845", "submitter": "Xueheng Shi", "authors": "Robert Lund, Xueheng Shi", "title": "Short Communication: Detecting Possibly Frequent Change-points: Wild\n  Binary Segmentation 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article comments on the new version of wild binary segmentation 2. Wild\nBinary Segmentation 2 and Steepest-drop Model Selection has made improvements\non changepoint analysis especially on reducing the computational cost. However,\nWBS2 tends to overestimate as WBS and the threshold does not work appropriately\non short sequences without changepoints.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:39:20 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Lund", "Robert", ""], ["Shi", "Xueheng", ""]]}, {"id": "2006.10968", "submitter": "Juho Lee", "authors": "Fadhel Ayed, Juho Lee, Fran\\c{c}ois Caron", "title": "The Normal-Generalised Gamma-Pareto process: A novel pure-jump L\\'evy\n  process with flexible tail and jump-activity properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure-jump L\\'evy processes are popular classes of stochastic processes which\nhave found many applications in finance, statistics or machine learning. In\nthis paper, we propose a novel family of self-decomposable L\\'evy processes\nwhere one can control separately the tail behavior and the jump activity of the\nprocess, via two different parameters. Crucially, we show that one can sample\nexactly increments of this process, at any time scale; this allows the\nimplementation of likelihood-free Markov chain Monte Carlo algorithms for\n(asymptotically) exact posterior inference. We use this novel process in\nL\\'evy-based stochastic volatility models to predict the returns of stock\nmarket data, and show that the proposed class of models leads to superior\npredictive performances compared to classical alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:26:28 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ayed", "Fadhel", ""], ["Lee", "Juho", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "2006.11041", "submitter": "Davide Ravagli", "authors": "Davide Ravagli and Georgi N. Boshnakov", "title": "Bayesian analysis of mixture autoregressive models covering the complete\n  parameter space", "comments": "27 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture autoregressive (MAR) models provide a flexible way to model time\nseries with predictive distributions which depend on the recent history of the\nprocess and are able to accommodate asymmetry and multimodality. Bayesian\ninference for such models offers the additional advantage of incorporating the\nuncertainty in the estimated models into the predictions. We introduce a new\nway of sampling from the posterior distribution of the parameters of MAR models\nwhich allows for covering the complete parameter space of the models, unlike\nprevious approaches. We also propose a relabelling algorithm to deal a\nposteriori with label switching. We apply our new method to simulated and real\ndatasets, discuss the accuracy and performance of our new method, as well as\nits advantages over previous studies. The idea of density forecasting using\nMCMC output is also introduced.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:44:01 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ravagli", "Davide", ""], ["Boshnakov", "Georgi N.", ""]]}, {"id": "2006.11088", "submitter": "Alexander McNeil", "authors": "Martin Bladt and Alexander J. McNeil", "title": "Time series copula models using d-vines and v-transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An approach to modelling volatile financial return series using stationary\nd-vine copula processes combined with Lebesgue-measure-preserving\ntransformations known as v-transforms is proposed. By developing a method of\nstochastically inverting v-transforms, models are constructed that can describe\nboth stochastic volatility in the magnitude of price movements and serial\ncorrelation in their directions. In combination with parametric marginal\ndistributions it is shown that these models can rival and sometimes outperform\nwell-known models in the extended GARCH family.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 11:48:42 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 09:03:42 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 14:24:10 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 18:14:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bladt", "Martin", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "2006.11123", "submitter": "Una Radoji\\v{c}i\\'c", "authors": "Una Radojicic, Klaus Nordhausen, Hannu Oja", "title": "Notion of information and independent component analysis", "comments": null, "journal-ref": "Applications of Mathematics in vol. 65, no. 3 (2020), pp. 311-330", "doi": "10.21136/AM.2020.0326-19", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial orderings and measures of information for continuous univariate\nrandom variables with special roles of Gaussian and uniform distributions are\ndiscussed. The information measures and measures of non-Gaussianity including\nthird and fourth cumulants are generally used as projection indices in the\nprojection pursuit approach for the independent component analysis. The\nconnections between information, non-Gaussianity and statistical independence\nin the context of independent component analysis is discussed in detail.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:26:37 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Radojicic", "Una", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "2006.11145", "submitter": "Gregory Gundersen", "authors": "Gregory W. Gundersen, Michael Minyi Zhang, Barbara E. Engelhardt", "title": "Latent variable modeling with random features", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process-based latent variable models are flexible and theoretically\ngrounded tools for nonlinear dimension reduction, but generalizing to\nnon-Gaussian data likelihoods within this nonlinear framework is statistically\nchallenging. Here, we use random features to develop a family of nonlinear\ndimension reduction models that are easily extensible to non-Gaussian data\nlikelihoods; we call these random feature latent variable models (RFLVMs). By\napproximating a nonlinear relationship between the latent space and the\nobservations with a function that is linear with respect to random features, we\ninduce closed-form gradients of the posterior distribution with respect to the\nlatent variable. This allows the RFLVM framework to support computationally\ntractable nonlinear latent variable models for a variety of data likelihoods in\nthe exponential family without specialized derivations. Our generalized RFLVMs\nproduce results comparable with other state-of-the-art dimension reduction\nmethods on diverse types of data, including neural spike train recordings,\nimages, and text data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:12:05 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Gundersen", "Gregory W.", ""], ["Zhang", "Michael Minyi", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "2006.11201", "submitter": "Sokbae Lee", "authors": "Le-Yu Chen, Sokbae Lee", "title": "Sparse Quantile Regression", "comments": "45 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider both $\\ell_{0}$-penalized and $\\ell_{0}$-constrained quantile\nregression estimators. For the $\\ell_{0}$-penalized estimator, we derive an\nexponential inequality on the tail probability of excess quantile prediction\nrisk and apply it to obtain non-asymptotic upper bounds on the mean-square\nparameter and regression function estimation errors. We also derive analogous\nresults for the $\\ell_{0}$-constrained estimator. The resulting rates of\nconvergence are nearly minimax-optimal and the same as those for\n$\\ell_{1}$-penalized estimators. Further, we characterize expected Hamming loss\nfor the $\\ell_{0}$-penalized estimator. We implement the proposed procedure via\nmixed integer linear programming and also a more scalable first-order\napproximation algorithm. We illustrate the finite-sample performance of our\napproach in Monte Carlo experiments and its usefulness in a real data\napplication concerning conformal prediction of infant birth weights (with\n$n\\approx 10^{3}$ and up to $p>10^{3}$). In sum, our $\\ell_{0}$-based method\nproduces a much sparser estimator than the $\\ell_{1}$-penalized approach\nwithout compromising precision.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 16:04:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 11:18:30 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""]]}, {"id": "2006.11265", "submitter": "Matteo Iacopini", "authors": "Matteo Iacopini and Francesco Ravazzolo and Luca Rossini", "title": "Proper scoring rules for evaluating asymmetry in density forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel asymmetric continuous probabilistic score (ACPS)\nfor evaluating and comparing density forecasts. It extends the proposed score\nand defines a weighted version, which emphasizes regions of interest, such as\nthe tails or the center of a variable's range. A test is also introduced to\nstatistically compare the predictive ability of different forecasts. The ACPS\nis of general use in any situation where the decision maker has asymmetric\npreferences in the evaluation of the forecasts. In an artificial experiment,\nthe implications of varying the level of asymmetry in the ACPS are illustrated.\nThen, the proposed score and test are applied to assess and compare density\nforecasts of macroeconomic relevant datasets (US employment growth) and of\ncommodity prices (oil and electricity prices) with particular focus on the\nrecent COVID-19 crisis period.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:53:02 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:17:35 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Iacopini", "Matteo", ""], ["Ravazzolo", "Francesco", ""], ["Rossini", "Luca", ""]]}, {"id": "2006.11346", "submitter": "Carrie Fry", "authors": "Carrie E. Fry and Laura A. Hatfield", "title": "Do Methodological Birds of a Feather Flock Together?", "comments": "21 pages, 2 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-experimental methods have proliferated over the last two decades, as\nresearchers develop causal inference tools for settings in which randomization\nis infeasible. Two popular such methods, difference-in-differences (DID) and\ncomparative interrupted time series (CITS), compare observations before and\nafter an intervention in a treated group to an untreated comparison group\nobserved over the same period. Both methods rely on strong, untestable\ncounterfactual assumptions. Despite their similarities, the methodological\nliterature on CITS lacks the mathematical formality of DID. In this paper, we\nuse the potential outcomes framework to formalize two versions of CITS - a\ngeneral version described by Bloom (2005) and a linear version often used in\nhealth services research. We then compare these to two corresponding DID\nformulations - one with time fixed effects and one with time fixed effects and\ngroup trends. We also re-analyze three previously published studies using these\nmethods. We demonstrate that the most general versions of CITS and DID impute\nthe same counterfactuals and estimate the same treatment effects. The only\ndifference between these two designs is the language used to describe them and\ntheir popularity in distinct disciplines. We also show that these designs\ndiverge when one constrains them using linearity (CITS) or parallel trends\n(DID). We recommend defaulting to the more flexible versions and provide advice\nto practitioners on choosing between the more constrained versions by\nconsidering the data-generating mechanism. We also recommend greater attention\nto specifying the outcome model and counterfactuals in papers, allowing for\ntransparent evaluation of the plausibility of causal assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:49:58 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:36:42 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Fry", "Carrie E.", ""], ["Hatfield", "Laura A.", ""]]}, {"id": "2006.11350", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Cyrus Diciccio, Divya Venugopalan, Heloise Logan,\n  Kinjal Basu, Noureddine El Karoui", "title": "Achieving Fairness via Post-Processing in Web-Scale Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building fair recommender systems is a challenging and extremely important\narea of study due to its immense impact on society. We focus on two commonly\naccepted notions of fairness for machine learning models powering such\nrecommender systems, namely equality of opportunity and equalized odds. These\nmeasures of fairness make sure that equally \"qualified\" (or \"unqualified\")\ncandidates are treated equally regardless of their protected attribute status\n(such as gender or race). In this paper, we propose scalable methods for\nachieving equality of opportunity and equalized odds in rankings in the\npresence of position bias, which commonly plagues data generated from\nrecommendation systems. Our algorithms are model agnostic in the sense that\nthey depend only on the final scores provided by a model, making them easily\napplicable to virtually all web-scale recommender systems. We conduct extensive\nsimulations as well as real-world experiments to show the efficacy of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 20:12:13 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:30:14 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Nandy", "Preetam", ""], ["Diciccio", "Cyrus", ""], ["Venugopalan", "Divya", ""], ["Logan", "Heloise", ""], ["Basu", "Kinjal", ""], ["Karoui", "Noureddine El", ""]]}, {"id": "2006.11380", "submitter": "Marian-Gabriel Hancean", "authors": "Marian-Gabriel H\\^ancean (1), Miranda J. Lubbers (2) and Jos\\'e Luis\n  Molina (2) ((1) Department of Sociology, University of Bucharest, (2)\n  Department of Social and Cultural Anthropology, Universitat Aut\\`onoma de\n  Barcelona)", "title": "Measuring transnational social fields through binational link-tracing\n  sampling", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0253042", "report-no": null, "categories": "cs.SI stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We advance binational link-tracing sampling design, an innovative data\ncollection methodology for sampling from transnational social fields, i.e.,\ntransnational networks embedding migrants and non-migrants. This paper shows\nthe practical challenges of such a design, the representativeness of the\nsamples and the qualities of the resulted networks. We performed 303\nface-to-face structured interviews on sociodemographic variables, migration\ntrajectories and personal networks of people living in a Romanian migration\nsending community (D\\^ambovi\\c{t}a) and in a migration receiving Spanish town\n(Castell\\'on), simultaneously in both sites. Inter-connecting the personal\nnetworks, we built a multi-layered complex network structure embedding 4,855\nnominated people, 5,477 directed ties (nominations) and 2,540 edges. Results\nindicate that the participants' unique identification is a particularly\ndifficult challenge, the representativeness of the data is not optimal\n(homophily on observed attributes was detected in the nomination patterns), and\nthe relational and attribute data allow to explore the social organization of\nthe Romanian migrant enclave in Castell\\'on, as well as its connectivity to\nother places. Furthermore, we provide methodological suggestions for improving\nlink-tracing sampling from transnational networks of migration. Our research\ncontributes to the emerging efforts of applying social network analysis to the\nstudy of international migration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:03:11 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["H\u00e2ncean", "Marian-Gabriel", ""], ["Lubbers", "Miranda J.", ""], ["Molina", "Jos\u00e9 Luis", ""]]}, {"id": "2006.11386", "submitter": "Jason Hartford", "authors": "Jason Hartford, Victor Veitch, Dhanya Sridhar, Kevin Leyton-Brown", "title": "Valid Causal Inference with (Some) Invalid Instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable methods provide a powerful approach to estimating\ncausal effects in the presence of unobserved confounding. But a key challenge\nwhen applying them is the reliance on untestable \"exclusion\" assumptions that\nrule out any relationship between the instrument variable and the response that\nis not mediated by the treatment. In this paper, we show how to perform\nconsistent IV estimation despite violations of the exclusion assumption. In\nparticular, we show that when one has multiple candidate instruments, only a\nmajority of these candidates---or, more generally, the modal candidate-response\nrelationship---needs to be valid to estimate the causal effect. Our approach\nuses an estimate of the modal prediction from an ensemble of instrumental\nvariable estimators. The technique is simple to apply and is \"black-box\" in the\nsense that it may be used with any instrumental variable estimator as long as\nthe treatment effect is identified for each valid instrument independently. As\nsuch, it is compatible with recent machine-learning based estimators that allow\nfor the estimation of conditional average treatment effects (CATE) on complex,\nhigh dimensional data. Experimentally, we achieve accurate estimates of\nconditional average treatment effects using an ensemble of deep network-based\nestimators, including on a challenging simulated Mendelian Randomization\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:09:26 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Hartford", "Jason", ""], ["Veitch", "Victor", ""], ["Sridhar", "Dhanya", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "2006.11418", "submitter": "Renato J Cintra", "authors": "D. R. Canterle, T. L. T. da Silveira, F. M. Bayer, R. J. Cintra", "title": "A Multiparametric Class of Low-complexity Transforms for Image and Video\n  Coding", "comments": "Fixed Figure 1 and typos in the reference list", "journal-ref": "Signal Processing, Volume 176, November 2020", "doi": "10.1016/j.sigpro.2020.107685", "report-no": null, "categories": "eess.SP cs.CV cs.MM eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete transforms play an important role in many signal processing\napplications, and low-complexity alternatives for classical transforms became\npopular in recent years. Particularly, the discrete cosine transform (DCT) has\nproven to be convenient for data compression, being employed in well-known\nimage and video coding standards such as JPEG, H.264, and the recent high\nefficiency video coding (HEVC). In this paper, we introduce a new class of\nlow-complexity 8-point DCT approximations based on a series of works published\nby Bouguezel, Ahmed and Swamy. Also, a multiparametric fast algorithm that\nencompasses both known and novel transforms is derived. We select the\nbest-performing DCT approximations after solving a multicriteria optimization\nproblem, and submit them to a scaling method for obtaining larger size\ntransforms. We assess these DCT approximations in both JPEG-like image\ncompression and video coding experiments. We show that the optimal DCT\napproximations present compelling results in terms of coding efficiency and\nimage quality metrics, and require only few addition or bit-shifting\noperations, being suitable for low-complexity and low-power systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:56:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Canterle", "D. R.", ""], ["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "2006.11430", "submitter": "Arun Sai Suggala", "authors": "Kartik Gupta, Arun Sai Suggala, Adarsh Prasad, Praneeth Netrapalli,\n  Pradeep Ravikumar", "title": "Learning Minimax Estimators via Online Learning", "comments": "60 pages. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing minimax estimators for estimating the\nparameters of a probability distribution. Unlike classical approaches such as\nthe MLE and minimum distance estimators, we consider an algorithmic approach\nfor constructing such estimators. We view the problem of designing minimax\nestimators as finding a mixed strategy Nash equilibrium of a zero-sum game. By\nleveraging recent results in online learning with non-convex losses, we provide\na general algorithm for finding a mixed-strategy Nash equilibrium of general\nnon-convex non-concave zero-sum games. Our algorithm requires access to two\nsubroutines: (a) one which outputs a Bayes estimator corresponding to a given\nprior probability distribution, and (b) one which computes the worst-case risk\nof any given estimator. Given access to these two subroutines, we show that our\nalgorithm outputs both a minimax estimator and a least favorable prior. To\ndemonstrate the power of this approach, we use it to construct provably minimax\nestimators for classical problems such as estimation in the finite Gaussian\nsequence model, and linear regression.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 22:49:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Gupta", "Kartik", ""], ["Suggala", "Arun Sai", ""], ["Prasad", "Adarsh", ""], ["Netrapalli", "Praneeth", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "2006.11439", "submitter": "Debarghya Mukherjee", "authors": "Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun", "title": "Two Simple Ways to Learn Individual Fairness Metrics from Data", "comments": "To appear in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual fairness is an intuitive definition of algorithmic fairness that\naddresses some of the drawbacks of group fairness. Despite its benefits, it\ndepends on a task specific fair metric that encodes our intuition of what is\nfair and unfair for the ML task at hand, and the lack of a widely accepted fair\nmetric for many ML tasks is the main barrier to broader adoption of individual\nfairness. In this paper, we present two simple ways to learn fair metrics from\na variety of data types. We show empirically that fair training with the\nlearned metrics leads to improved fairness on three machine learning tasks\nsusceptible to gender and racial biases. We also provide theoretical guarantees\non the statistical performance of both approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 23:47:15 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Mukherjee", "Debarghya", ""], ["Yurochkin", "Mikhail", ""], ["Banerjee", "Moulinath", ""], ["Sun", "Yuekai", ""]]}, {"id": "2006.11450", "submitter": "Norbert Remenyi", "authors": "Norbert Remenyi, Xiaodong Luo", "title": "Demand Estimation from Sales Transaction Data -- Practical Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss practical limitations of the standard choice-based\ndemand models used in the literature to estimate demand from sales transaction\ndata. We present modifications and extensions of the models and discuss data\npreprocessing and solution techniques which are useful for practitioners\ndealing with sales transaction data. Among these, we present an algorithm to\nsplit sales transaction data observed under partial availability, we extend a\npopular Expectation Maximization (EM) algorithm for non-homogeneous product\nsets, and we develop two iterative optimization algorithms which can handle\nmuch of the extensions discussed in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 00:48:29 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 18:30:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Remenyi", "Norbert", ""], ["Luo", "Xiaodong", ""]]}, {"id": "2006.11585", "submitter": "Yoav Zeevi", "authors": "Yoav Zeevi, Sofi Astashenko, Yoav Benjamini", "title": "Ignored evident multiplicity harms replicability -- adjusting for it\n  offers a remedy", "comments": "28 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a central dogma in science that a result of a study should be\nreplicable. Only 90 of the 190 replications attempts were successful. We\nattribute a substantial part of the problem to selective inference evident in\nthe paper, which is the practice of selecting some of the results from the\nmany. 100 papers in the Reproducibility Project in Psychology were analyzed. It\nwas evident that the reporting of many results is common (77.7 per paper on\naverage). It was further found that the selection from those multiple results\nis not adjusted for. We propose to account for selection using the hierarchical\nfalse discovery rate (FDR) controlling procedure TreeBH of Bogomolov et al.\n(2020), which exploits hierarchical structures to gain power. Results that were\nstatistically significant after adjustment were 97% of the replicable results\n(31 of 32). Additionally, only 1 of the 21 non-significant results after\nadjustment was replicated. Given the easy deployment of adjustment tools and\nthe minor loss of power involved, we argue that addressing multiplicity is an\nessential missing component in experimental psychology. It should become a\nrequired component in the arsenal of replicability enhancing methodologies in\nthe field.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 14:22:48 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 06:17:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zeevi", "Yoav", ""], ["Astashenko", "Sofi", ""], ["Benjamini", "Yoav", ""]]}, {"id": "2006.11628", "submitter": "Rahul Ladhania", "authors": "Rahul Ladhania, Amelia Haviland, Neeraj Sood, Edward Kennedy, Ateev\n  Mehrotra", "title": "Learning and Testing Sub-groups with Heterogeneous Treatment Effects:A\n  Sequence of Two Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is strong interest in estimating how the magnitude of treatment effects\nof an intervention vary across sub-groups of the population of interest. In our\npaper, we propose a two-study approach to first propose and then test\nheterogeneous treatment effects. In Study 1, we use a large observational\ndataset to learn sub-groups with the most distinctive treatment-outcome\nrelationships ('high/low-impact sub-groups'). We adopt a model-based recursive\npartitioning approach to propose the high/low impact sub-groups, and validate\nthem by using sample-splitting. While the first study rules out noise, there is\npotential bias in our estimated heterogeneous treatment effects. Study 2 uses\nan experimental design, and here we classify our sample units based on\nsub-groups learned in Study 1. We then estimate treatment effects within each\nof the groups, thereby testing the causal hypotheses proposed in Study 1. Using\npatient claims data from the NBER MarketScan database, we apply our approach to\nestimate heterogeneous effects of a switch to a high-deductible health\ninsurance plan on use of outpatient care by patients with a common chronic\ncondition. We extend the method to non-parametrically learn the sub-groups in\nStudy 1. We also compare the methods' performance to other state-of-the-art\nmethods in the literature that make use only of the Study 2 data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:01:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ladhania", "Rahul", ""], ["Haviland", "Amelia", ""], ["Sood", "Neeraj", ""], ["Kennedy", "Edward", ""], ["Mehrotra", "Ateev", ""]]}, {"id": "2006.11641", "submitter": "Jacques Balayla", "authors": "Jacques Balayla", "title": "Bayesian Updating and Sequential Testing: Overcoming Inferential\n  Limitations of Screening Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes' Theorem confers inherent limitations on the accuracy of screening\ntests as a function of disease prevalence. We have shown in previous work that\na testing system can tolerate significant drops in prevalence, up until a\ncertain well-defined point known as the $prevalence$ $threshold$, below which\nthe reliability of a positive screening test drops precipitously. Herein, we\nestablish a mathematical model to determine whether sequential testing\novercomes the aforementioned Bayesian limitations and thus improves the\nreliability of screening tests. We show that for a desired positive predictive\nvalue of $\\rho$ that approaches $k$, the number of positive test iterations\n$n_i$ needed is: $ n_i =\\lim_{\\rho \\to\nk}\\left\\lceil\\frac{ln\\left[\\frac{\\rho(\\phi-1)}{\\phi(\\rho-1)}\\right]}{ln\\left[\\frac{a}{1-b}\\right]}\\right\\rceil$\nwhere $n_i$ = number of testing iterations necessary to achieve $\\rho$, the\ndesired positive predictive value, a = sensitivity, b = specificity, $\\phi$ =\ndisease prevalence and $k$ = constant. Based on the aforementioned derivation,\nwe provide reference tables for the number of test iterations needed to obtain\na $\\rho(\\phi)$ of 50, 75, 95 and 99$\\%$ as a function of various levels of\nsensitivity, specificity and disease prevalence.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 19:54:20 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 20:15:56 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 22:02:07 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2020 19:08:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Balayla", "Jacques", ""]]}, {"id": "2006.11676", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou and Yuan Ji", "title": "A Unified Framework for Time-to-Event Dose-Finding Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dose-finding trials, due to staggered enrollment, it might be desirable to\nmake dose assignment decisions in real-time in the presence of pending toxicity\noutcomes, for example, when patient accrual is fast or the dose-limiting\ntoxicity is late-onset. Patients' time-to-event information may be utilized to\nfacilitate such decisions. We propose a unified statistical framework for\ntime-to-event modeling in dose-finding trials, which leads to two classes of\ntime-to-event designs: TITE deigns and POD designs. TITE designs are based on\ninference on toxicity probabilities, while POD designs are based on inference\non dose-finding decisions. These two classes of designs contain existing\ndesigns as special cases and also give rise to new designs. We discuss and\nstudy the theoretical properties of these designs, including large-sample\nconvergence properties, coherence principles, and the underlying decision\nrules. To facilitate the use of time-to-event designs in practice, we introduce\nefficient computational algorithms and review common practical considerations,\nsuch as safety rules and suspension rules. Finally, the operating\ncharacteristics of several designs are evaluated and compared through computer\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 23:30:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.11689", "submitter": "Haoqi Sun", "authors": "Haoqi Sun, Michael J. Leone, Lin Liu, Shabani S. Mukerji, Gregory K.\n  Robbins, M. Brandon Westover", "title": "Clinically Relevant Mediation Analysis using Controlled Indirect Effect", "comments": "15 pages, 2 figures in main text, 1 figure in supplemental, 4 tables\n  in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mediation analysis allows one to use observational data to estimate the\nimportance of each potential mediating pathway involved in the causal effect of\nan exposure on an outcome. However, current approaches to mediation analysis\nwith multiple mediators either involve assumptions not verifiable by\nexperiments, or estimate the effect when mediators are manipulated jointly\nwhich precludes the practical design of experiments due to curse of\ndimensionality, or are difficult to interpret when arbitrary causal\ndependencies are present. We propose a method for mediation analysis for\nmultiple manipulable mediators with arbitrary causal dependencies. The proposed\nmethod is clinically relevant because the decomposition of the total effect\ndoes not involve effects under cross-world assumptions and focuses on the\neffects after manipulating (i.e. treating) one single mediator, which is more\nrelevant in a clinical scenario. We illustrate the approach using simulated\ndata, the \"framing\" dataset from political science, and the HIV-Brain Age\ndataset from a clinical retrospective cohort study. Our results provide\npotential guidance for clinical practitioners to make justified choices to\nmanipulate one of the mediators to optimize the outcome.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 01:58:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sun", "Haoqi", ""], ["Leone", "Michael J.", ""], ["Liu", "Lin", ""], ["Mukerji", "Shabani S.", ""], ["Robbins", "Gregory K.", ""], ["Westover", "M. Brandon", ""]]}, {"id": "2006.11715", "submitter": "Shu Wei Chou-Chen", "authors": "Shu Wei Chou-Chen, Pedro A. Morettin", "title": "Indirect inference for locally stationary ARMA processes with stable\n  innovations", "comments": "31 pages, 14 figures. Submitted to the Journal of Statistical\n  Computation and Simulation", "journal-ref": "Journal of Statistical Computation and Simulation, 90:17,\n  3106-3134 (2020)", "doi": "10.1080/00949655.2020.1797030", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of locally stationary processes assumes that there is a\ntime-varying spectral representation, that is, the existence of finite second\nmoment. We propose the $\\alpha$-stable locally stationary process by modifying\nthe innovations into stable distributions and the indirect inference to\nestimate this type of model. Due to the infinite variance, some of interesting\nproperties such as time-varying auto-correlation cannot be defined. However,\nsince the $\\alpha$-stable family of distributions is closed under linear\ncombination which includes the possibility of handling asymmetry and thicker\ntails, the proposed model has the same tail behavior throughout the time. In\nthis paper, we propose this new model, present theoretical properties of the\nprocess and carry out simulations related to the indirect inference in order to\nestimate the parametric form of the model. Finally, an empirical application is\nillustrated.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:34:10 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chou-Chen", "Shu Wei", ""], ["Morettin", "Pedro A.", ""]]}, {"id": "2006.11754", "submitter": "Michael Schomaker", "authors": "Michael Schomaker", "title": "Regression and Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal effect of an intervention (treatment/exposure) on an outcome can\nbe estimated by: i) specifying knowledge about the data-generating process; ii)\nassessing under what assumptions a target quantity, such as for example a\ncausal odds ratio, can be identified given the specified knowledge (and given\nthe measured data); and then, iii) using appropriate statistical estimation\ntechniques to estimate the desired parameter of interest. As regression is the\ncornerstone of statistical analysis, it seems obvious to ask: is it appropriate\nto use estimated regression parameters for causal effect estimation? It turns\nout that using regression for effect estimation is possible, but typically\nrequires more assumptions than competing methods. This manuscript provides a\ncomprehensive summary of the assumptions needed to identify and estimate a\ncausal parameter using regression and, equally important, discusses the\nresulting implications for statistical practice.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:16:48 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:48:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schomaker", "Michael", ""]]}, {"id": "2006.11759", "submitter": "Pavel Krupskii", "authors": "Pavel Krupskii and Marc G. Genton", "title": "Conditional Normal Extreme-Value Copulas", "comments": "42 pages, 6 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of extreme-value copulas which are extreme-value\nlimits of conditional normal models. Conditional normal models are\ngeneralizations of conditional independence models, where the dependence among\nobserved variables is modeled using one unobserved factor. Conditional on this\nfactor, the distribution of these variables is given by the Gaussian copula.\nThis structure allows one to build flexible and parsimonious models for data\nwith complex dependence structures, such as data with spatial dependence or\nfactor structure. We study the extreme-value limits of these models and show\nsome interesting special cases of the proposed class of copulas. We develop\nestimation methods for the proposed models and conduct a simulation study to\nassess the performance of these algorithms. Finally, we apply these copula\nmodels to analyze data on monthly wind maxima and stock return minima.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:32:01 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 08:12:02 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Krupskii", "Pavel", ""], ["Genton", "Marc G.", ""]]}, {"id": "2006.11848", "submitter": "Alexander Volkmann", "authors": "Alexander Volkmann", "title": "On the Relationship between Treatment Effect Heterogeneity and the\n  Variability Ratio Effect Size Statistic", "comments": "6 pages, 8 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the variability ratio (VR) effect size statistic has been used with\nincreasing frequency in the study of differences in variation of a measured\nvariable between two study populations. More specifically, the VR effect size\nstatistic allows for the detection of treatment effect heterogeneity (TEH) of\nmedical interventions. While a VR that is different from 1 is widely\nacknowledged to implicate a treatment effect heterogeneity (TEH) the exact\nrelationship between those two quantities has not been discussed in detail thus\nfar.\n  In this note we derive a precise connection between TEH and VR. In\nparticular, we derive precise upper and lower bounds on the TEH in terms of VR.\nMoreover, we provide an exemplary simulation for which VR is equal to 1 and\nthere exist TEH.\n  Our result has implications for the interpretation of VR effect size\nestimates regarding its connection to treatment effect heterogeneity of\n(medical) interventions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 17:02:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Volkmann", "Alexander", ""]]}, {"id": "2006.11908", "submitter": "Henrique Bolfarine", "authors": "Henrique Bolfarine, Carlos M. Carvalho, Hedibert F. Lopes, Jared S.\n  Murray", "title": "Decoupling Shrinkage and Selection in Gaussian Linear Factor Analysis", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor Analysis is a popular method for modeling dependence in multivariate\ndata. However, determining the number of factors and obtaining a sparse\norientation of the loadings are still major challenges. In this paper, we\npropose a decision-theoretic approach that brings to light the relation between\na sparse representation of the loadings and factor dimension. This relation is\ndone through a summary from information contained in the multivariate\nposterior. To construct such summary, we introduce a three-step approach. In\nthe first step, the model is fitted with a conservative factor dimension. In\nthe second step, a series of sparse point-estimates, with a decreasing number\nof factors, is obtained by minimizing an expected predictive loss function. In\nstep three, the degradation in utility in relation to the sparse loadings and\nfactor dimensions is displayed in the posterior summary. The findings are\nillustrated with applications in classical data from the Factor Analysis\nliterature. We used different prior choices and factor dimensions to\ndemonstrate the flexibility of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 20:44:52 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 00:56:19 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 15:57:43 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 12:30:17 GMT"}, {"version": "v5", "created": "Sat, 24 Jul 2021 05:37:20 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bolfarine", "Henrique", ""], ["Carvalho", "Carlos M.", ""], ["Lopes", "Hedibert F.", ""], ["Murray", "Jared S.", ""]]}, {"id": "2006.12039", "submitter": "Xinyu Song", "authors": "Donggyu Kim, Xinyu Song, Yazhen Wang", "title": "Unified Discrete-Time Factor Stochastic Volatility and Continuous-Time\n  Ito Models for Combining Inference Based on Low-Frequency and High-Frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces unified models for high-dimensional factor-based Ito\nprocess, which can accommodate both continuous-time Ito diffusion and\ndiscrete-time stochastic volatility (SV) models by embedding the discrete SV\nmodel in the continuous instantaneous factor volatility process. We call it the\nSV-Ito model. Based on the series of daily integrated factor volatility matrix\nestimators, we propose quasi-maximum likelihood and least squares estimation\nmethods. Their asymptotic properties are established. We apply the proposed\nmethod to predict future vast volatility matrix whose asymptotic behaviors are\nstudied. A simulation study is conducted to check the finite sample performance\nof the proposed estimation and prediction method. An empirical analysis is\ncarried out to demonstrate the advantage of the SV-Ito model in volatility\nprediction and portfolio allocation problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 07:20:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kim", "Donggyu", ""], ["Song", "Xinyu", ""], ["Wang", "Yazhen", ""]]}, {"id": "2006.12180", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Luca Bagnato", "title": "The multivariate tail-inflated normal distribution and its application\n  in finance", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the multivariate tail-inflated normal (MTIN)\ndistribution, an elliptical heavy-tails generalization of the multivariate\nnormal (MN). The MTIN belongs to the family of MN scale mixtures by choosing a\nconvenient continuous uniform as mixing distribution. Moreover, it has a\nclosed-form for the probability density function characterized by only one\nadditional ``inflation'' parameter, with respect to the nested MN, governing\nthe tail-weight. The first four moments are also computed; interestingly, they\nalways exist and the excess kurtosis can assume any positive value. The method\nof moments and maximum likelihood (ML) are considered for estimation. As\nconcerns the latter, a direct approach, as well as a variant of the EM\nalgorithm, are illustrated. The existence of the ML estimates is also\nevaluated. Since the inflation parameter is estimated from the data, robust\nestimates of the mean vector and covariance matrix of the nested MN\ndistribution are automatically obtained by down-weighting. Simulations are\nperformed to compare the estimation methods/algorithms and to investigate the\nability of AIC and BIC to select among a set of candidate elliptical models.\nFor illustrative purposes, the MTIN distribution is finally fitted to\nmultivariate financial data where its usefulness is also shown in comparison\nwith other well-established multivariate elliptical distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:25:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Punzo", "Antonio", ""], ["Bagnato", "Luca", ""]]}, {"id": "2006.12269", "submitter": "Fiammetta Menchetti", "authors": "Fiammetta Menchetti and Iavor Bojinov", "title": "Estimating the effectiveness of permanent price reductions for competing\n  products using multivariate Bayesian structural time series models", "comments": "21 pages; further robustness checks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Florence branch of an Italian supermarket chain recently implemented a\nstrategy that permanently lowered the price of numerous store brands in several\nproduct categories. To quantify the impact of such a policy change, researchers\noften use synthetic control methods for estimating causal effects when a subset\nof units receive a single persistent treatment, and the rest are unaffected by\nthe change. In our applications, however, competitor brands not assigned to\ntreatment are likely impacted by the intervention because of substitution\neffects; more broadly, this type of interference occurs whenever the treatment\nassignment of one unit affects the outcome of another. This paper extends the\nsynthetic control methods to accommodate partial interference, allowing\ninterference within predefined groups but not between them. Focusing on a class\nof causal estimands that capture the effect both on the treated and control\nunits, we develop a multivariate Bayesian structural time series model for\ngenerating synthetic controls that would have occurred in the absence of an\nintervention enabling us to estimate our novel effects. In a simulation study,\nwe explore our Bayesian procedure's empirical properties and show that it\nachieves good frequentists coverage even when the model is misspecified. We use\nour new methodology to make causal statements about the impact on sales of the\naffected store brands and their direct competitors. Our proposed approach is\nimplemented in the CausalMBSTS R package.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:03:55 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 20:57:50 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 17:33:52 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 15:38:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Menchetti", "Fiammetta", ""], ["Bojinov", "Iavor", ""]]}, {"id": "2006.12335", "submitter": "Yuling Yao", "authors": "Yuling Yao, Aki Vehtari, Andrew Gelman", "title": "Stacking for Non-mixing Bayesian Computations: The Curse and Blessing of\n  Multimodal Posteriors", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with multimodal Bayesian posterior distributions, Markov chain\nMonte Carlo (MCMC) algorithms can have difficulty moving between modes, and\ndefault variational or mode-based approximate inferences will understate\nposterior uncertainty. And, even if the most important modes can be found, it\nis difficult to evaluate their relative weights in the posterior. Here we\npropose an approach using parallel runs of MCMC, variational, or mode-based\ninference to hit as many modes or separated regions as possible and then\ncombine these using Bayesian stacking, a scalable method for constructing a\nweighted average of distributions so as to minimize cross validation prediction\nerror. The result from stacking is not necessarily equivalent, even\nasymptotically, to fully Bayesian inference, but it serves many of the same\ngoals. Under misspecified models, stacking can give better predictive\nperformance than full Bayesian inference, hence the multimodality can be\nconsidered a blessing rather than a curse. We explore theoretical consistency\nwith an examples where the stacked inference can approximate the true data\ngenerating process from the misspecified model and a non-mixing sampler. We\nconsider practical implementation in several model families: latent Dirichlet\nallocation, Gaussian process regression, hierarchical regression, horseshoe\nvariable selection, and neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 15:26:59 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 21:18:48 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yao", "Yuling", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2006.12460", "submitter": "Johann Gaebler", "authors": "Johann Gaebler, William Cai, Guillaume Basse, Ravi Shroff, Sharad\n  Goel, Jennifer Hill", "title": "A Causal Framework for Observational Studies of Discrimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies of discrimination, researchers often seek to estimate a causal\neffect of race or gender on outcomes. For example, in the criminal justice\ncontext, one might ask whether arrested individuals would have been\nsubsequently charged or convicted had they been a different race. It has long\nbeen known that such counterfactual questions face measurement challenges\nrelated to omitted-variable bias, and conceptual challenges related to the\ndefinition of causal estimands for largely immutable characteristics. Another\nconcern, which has been the subject of recent debates, is post-treatment bias:\nmany studies of discrimination condition on apparently intermediate outcomes,\nlike being arrested, that themselves may be the product of discrimination,\npotentially corrupting statistical estimates. There is, however, reason to be\noptimistic. By carefully defining the estimand---and by considering the precise\ntiming of events---we show that a primary causal quantity of interest in\ndiscrimination studies can be estimated under an ignorability condition that is\nplausible in many observational settings. We illustrate these ideas by\nanalyzing both simulated data and the charging decisions of a prosecutor's\noffice in a large county in the United States.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:43:14 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:03:39 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gaebler", "Johann", ""], ["Cai", "William", ""], ["Basse", "Guillaume", ""], ["Shroff", "Ravi", ""], ["Goel", "Sharad", ""], ["Hill", "Jennifer", ""]]}, {"id": "2006.12573", "submitter": "Riddhiman Adib", "authors": "Riddhiman Adib, Paul Griffin, Sheikh Iqbal Ahamed, Mohammad\n  Adibuzzaman", "title": "A Causally Formulated Hazard Ratio Estimation through Backdoor\n  Adjustment on Structural Causal Model", "comments": "19 pages, Accepted at Machine Learning for Healthcare 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal relationships for a treatment intervention is a\nfundamental problem in health sciences. Randomized controlled trials (RCTs) are\nconsidered the gold standard for identifying causal relationships. However,\nrecent advancements in the theory of causal inference based on the foundations\nof structural causal models (SCMs) have allowed the identification of causal\nrelationships from observational data, under certain assumptions. Survival\nanalysis provides standard measures, such as the hazard ratio, to quantify the\neffects of an intervention. While hazard ratios are widely used in clinical and\nepidemiological studies for RCTs, a principled approach does not exist to\ncompute hazard ratios for observational studies with SCMs. In this work, we\nreview existing approaches to compute hazard ratios as well as their causal\ninterpretation, if it exists. We also propose a novel approach to compute\nhazard ratios from observational studies using backdoor adjustment through SCMs\nand do-calculus. Finally, we evaluate the approach using experimental data for\nEwing's sarcoma.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:10:16 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Adib", "Riddhiman", ""], ["Griffin", "Paul", ""], ["Ahamed", "Sheikh Iqbal", ""], ["Adibuzzaman", "Mohammad", ""]]}, {"id": "2006.12592", "submitter": "Xiaodong Cai", "authors": "Xin Zhou, Chunlei Du, and Xiaodong Cai", "title": "An Efficient Smoothing Proximal Gradient Algorithm for Convex Clustering", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis organizes data into sensible groupings and is one of\nfundamental modes of understanding and learning. The widely used K-means and\nhierarchical clustering methods can be dramatically suboptimal due to local\nminima. Recently introduced convex clustering approach formulates clustering as\na convex optimization problem and ensures a globally optimal solution. However,\nthe state-of-the-art convex clustering algorithms, based on the alternating\ndirection method of multipliers (ADMM) or the alternating minimization\nalgorithm (AMA), require large computation and memory space, which limits their\napplications. In this paper, we develop a very efficient smoothing proximal\ngradient algorithm (Sproga) for convex clustering. Our Sproga is faster than\nADMM- or AMA-based convex clustering algorithms by one to two orders of\nmagnitude. The memory space required by Sproga is less than that required by\nADMM and AMA by at least one order of magnitude. Computer simulations and real\ndata analysis show that Sproga outperforms several well known clustering\nalgorithms including K-means and hierarchical clustering. The efficiency and\nsuperior performance of our algorithm will help convex clustering to find its\nwide application.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 20:02:59 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhou", "Xin", ""], ["Du", "Chunlei", ""], ["Cai", "Xiaodong", ""]]}, {"id": "2006.12623", "submitter": "Francesca Greselin", "authors": "Francesca Greselin, Simone Pellegrino, Achille Vernizzi", "title": "The Social Welfare Implications of the Zenga Index", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the social welfare implications of the Zenga index, a recently\nproposed index of inequality. Our proposal is derived by following the seminal\nbook by Son (2011) and the recent working paper by Kakwani and Son (2019). We\ncompare the Zenga based approach with the classical one, based on the Lorenz\ncurve and the Gini coefficient, as well as the Bonferroni index. We show that\nthe social welfare specification based on the Zenga uniformity curve presents\nsome peculiarities that distinguish it from the other considered indexes. The\nsocial welfare specification presented here provides a deeper understanding of\nhow the Zenga index evaluates the inequality in a distribution.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:01:31 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Greselin", "Francesca", ""], ["Pellegrino", "Simone", ""], ["Vernizzi", "Achille", ""]]}, {"id": "2006.12640", "submitter": "Chao Zhang", "authors": "Chao Zhang, Piotr Kokoszka, Alexander Petersen", "title": "Wasserstein Autoregressive Models for Density Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data consisting of time-indexed distributions of cross-sectional or intraday\nreturns have been extensively studied in finance, and provide one example in\nwhich the data atoms consist of serially dependent probability distributions.\nMotivated by such data, we propose an autoregressive model for density time\nseries by exploiting the tangent space structure on the space of distributions\nthat is induced by the Wasserstein metric. The densities themselves are not\nassumed to have any specific parametric form, leading to flexible forecasting\nof future unobserved densities. The main estimation targets in the order-$p$\nWasserstein autoregressive model are Wasserstein autocorrelations and the\nvector-valued autoregressive parameter. We propose suitable estimators and\nestablish their asymptotic normality, which is verified in a simulation study.\nThe new order-$p$ Wasserstein autoregressive model leads to a prediction\nalgorithm, which includes a data driven order selection procedure. Its\nperformance is compared to existing prediction procedures via application to\nfour financial return data sets, where a variety of metrics are used to\nquantify forecasting accuracy. For most metrics, the proposed model outperforms\nexisting methods in two of the data sets, while the best empirical performance\nin the other two data sets is attained by existing methods based on functional\ntransformations of the densities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 21:56:12 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhang", "Chao", ""], ["Kokoszka", "Piotr", ""], ["Petersen", "Alexander", ""]]}, {"id": "2006.12669", "submitter": "Soumya Ghosh", "authors": "Soumya Ghosh and William T. Stephenson and Tin D. Nguyen and Sameer K.\n  Deshpande and Tamara Broderick", "title": "Approximate Cross-Validation for Structured Models", "comments": "25 pages, 8 figures. NeurIPS 2020 camera ready. v2 fixes typos and\n  provides additional empirical results. Code:\n  https://github.com/SoumyaTGhosh/structured-infinitesimal-jackknife", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data analyses benefit from explicitly modeling dependence\nstructure in data -- such as measurements across time or space, ordered words\nin a sentence, or genes in a genome. A gold standard evaluation technique is\nstructured cross-validation (CV), which leaves out some data subset (such as\ndata within a time interval or data in a geographic region) in each fold. But\nCV here can be prohibitively slow due to the need to re-run already-expensive\nlearning algorithms many times. Previous work has shown approximate\ncross-validation (ACV) methods provide a fast and provably accurate alternative\nin the setting of empirical risk minimization. But this existing ACV work is\nrestricted to simpler models by the assumptions that (i) data across CV folds\nare independent and (ii) an exact initial model fit is available. In structured\ndata analyses, both these assumptions are often untrue. In the present work, we\naddress (i) by extending ACV to CV schemes with dependence structure between\nthe folds. To address (ii), we verify -- both theoretically and empirically --\nthat ACV quality deteriorates smoothly with noise in the initial fit. We\ndemonstrate the accuracy and computational benefits of our proposed methods on\na diverse set of real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:06:03 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:37:42 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ghosh", "Soumya", ""], ["Stephenson", "William T.", ""], ["Nguyen", "Tin D.", ""], ["Deshpande", "Sameer K.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2006.12778", "submitter": "Lu Xia", "authors": "Lu Xia, Bin Nan and Yi Li", "title": "A Revisit to De-biased Lasso for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-biased lasso has emerged as a popular tool to draw statistical inference\nfor high-dimensional regression models. However, simulations indicate that for\ngeneralized linear models (GLMs), de-biased lasso inadequately removes biases\nand yields unreliable confidence intervals. This motivates us to scrutinize the\napplication of de-biased lasso in high-dimensional GLMs. When $p >n$, we detect\nthat a key sparsity condition on the inverse information matrix generally does\nnot hold in a GLM setting, which likely explains the subpar performance of\nde-biased lasso. Even in a less challenging \"large $n$, diverging $p$\"\nscenario, we find that de-biased lasso and the maximum likelihood method often\nyield confidence intervals with unsatisfactory coverage probabilities. In this\nscenario, we examine an alternative approach for further bias correction by\ndirectly inverting the Hessian matrix without imposing the matrix sparsity\nassumption. We establish the asymptotic distributions of any linear\ncombinations of the resulting estimates, which lay the theoretical groundwork\nfor drawing inference. Simulations show that this refined de-biased estimator\nperforms well in removing biases and yields an honest confidence interval\ncoverage. We illustrate the method by analyzing a prospective hospital-based\nBoston Lung Cancer Study, a large scale epidemiology cohort investigating the\njoint effects of genetic variants on lung cancer risk.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 06:03:26 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Xia", "Lu", ""], ["Nan", "Bin", ""], ["Li", "Yi", ""]]}, {"id": "2006.12806", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Peter B\\\"uhlmann", "title": "Seeded intervals and noise level estimation in change point detection: A\n  discussion of Fryzlewicz (2020)", "comments": "To appear in the Journal of the Korean Statistical Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this discussion, we compare the choice of seeded intervals and that of\nrandom intervals for change point segmentation from practical, statistical and\ncomputational perspectives. Furthermore, we investigate a novel estimator of\nthe noise level, which improves many existing model selection procedures\n(including the steepest drop to low levels), particularly for challenging\nfrequent change point scenarios with low signal-to-noise ratios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:51:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2006.12822", "submitter": "Fabian Hinder", "authors": "Fabian Hinder, Barbara Hammer", "title": "Counterfactual Explanations of Concept Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of concept drift refers to the phenomenon that the distribution,\nwhich is underlying the observed data, changes over time; as a consequence\nmachine learning models may become inaccurate and need adjustment. While there\ndo exist methods to detect concept drift or to adjust models in the presence of\nobserved drift, the question of explaining drift has hardly been considered so\nfar. This problem is of importance, since it enables an inspection of the most\nprominent features where drift manifests itself; hence it enables human\nunderstanding of the necessity of change and it increases acceptance of\nlife-long learning models. In this paper we present a novel technology, which\ncharacterizes concept drift in terms of the characteristic change of spatial\nfeatures represented by typical examples based on counterfactual explanations.\nWe establish a formal definition of this problem, derive an efficient\nalgorithmic solution based on counterfactual explanations, and demonstrate its\nusefulness in several examples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:27:57 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Hinder", "Fabian", ""], ["Hammer", "Barbara", ""]]}, {"id": "2006.12839", "submitter": "Fran\\c{c}ois Portier", "authors": "Pascal Bianchi and Kevin Elgui and Fran\\c{c}ois Portier", "title": "Conditional independence testing via weighted partial copulas and\n  nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the \\textit{weighted partial copula} function for\ntesting conditional independence. The proposed test procedure results from\nthese two ingredients: (i) the test statistic is an explicit Cramer-von Mises\ntransformation of the \\textit{weighted partial copula}, (ii) the regions of\nrejection are computed using a bootstrap procedure which mimics conditional\nindependence by generating samples from the product measure of the estimated\nconditional marginals. Under conditional independence, the weak convergence of\nthe \\textit{weighted partial copula proces}s is established when the marginals\nare estimated using a smoothed local linear estimator. Finally, an experimental\nsection demonstrates that the proposed test has competitive power compared to\nrecent state-of-the-art methods such as kernel-based test.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:02:29 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 08:02:46 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 15:43:04 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Bianchi", "Pascal", ""], ["Elgui", "Kevin", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "2006.12871", "submitter": "Jes Frellsen", "authors": "Niels Bruun Ipsen, Pierre-Alexandre Mattei, Jes Frellsen", "title": "not-MIWAE: Deep Generative Modelling with Missing not at Random Data", "comments": "Camera-ready version for ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a missing process depends on the missing values themselves, it needs to\nbe explicitly modelled and taken into account while doing likelihood-based\ninference. We present an approach for building and fitting deep latent variable\nmodels (DLVMs) in cases where the missing process is dependent on the missing\ndata. Specifically, a deep neural network enables us to flexibly model the\nconditional distribution of the missingness pattern given the data. This allows\nfor incorporating prior information about the type of missingness (e.g.\nself-censoring) into the model. Our inference technique, based on\nimportance-weighted variational inference, involves maximising a lower bound of\nthe joint likelihood. Stochastic gradients of the bound are obtained by using\nthe reparameterisation trick both in latent space and data space. We show on\nvarious kinds of data sets and missingness patterns that explicitly modelling\nthe missing process can be invaluable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:06:21 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 13:32:09 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ipsen", "Niels Bruun", ""], ["Mattei", "Pierre-Alexandre", ""], ["Frellsen", "Jes", ""]]}, {"id": "2006.12904", "submitter": "Veli Safak", "authors": "Veli Safak", "title": "Min-Mid-Max Scaling, Limits of Agreement, and Agreement Score", "comments": "Cohen's kappa, limits of agreement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I solve a 60-year old question posed by Cohen's seminal paper\n(1960) and offer an agreement measure centered around the chance-expected\nagreement while isolating marginally forced agreement and disagreement. To\nachieve this, I formulate the minimum feasible agreement given row and column\nmarginals by devising a new algorithm that minimizes the sum of diagonals in\ncontingency tables. Based on this result, I also formulate the lower limit of\nthe most common agreement measure-Cohen's kappa. Finally, I study the lower\nlimit of maximum feasible agreement and devise two statistics of distribution\nsimilarity for agreement analysis.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:21:02 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 05:20:19 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 10:59:05 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 08:08:29 GMT"}, {"version": "v5", "created": "Sat, 13 Mar 2021 23:10:39 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Safak", "Veli", ""]]}, {"id": "2006.13039", "submitter": "Lun Wang", "authors": "Lun Wang, Ruoxi Jia and Dawn Song", "title": "D2P-Fed: Differentially Private Federated Learning With Efficient\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the discrete Gaussian based differentially private\nfederated learning (D2P-Fed), a unified scheme to achieve both differential\nprivacy (DP) and communication efficiency in federated learning (FL). In\nparticular, compared with the only prior work taking care of both aspects,\nD2P-Fed provides stronger privacy guarantee, better composability and smaller\ncommunication cost. The key idea is to apply the discrete Gaussian noise to the\nprivate data transmission. We provide complete analysis of the privacy\nguarantee, communication cost and convergence rate of D2P-Fed. We evaluated\nD2P-Fed on INFIMNIST and CIFAR10. The results show that D2P-Fed outperforms\nthe-state-of-the-art by 4.7% to 13.0% in terms of model accuracy while saving\none third of the communication cost.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 06:46:11 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:30:40 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 22:22:17 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 03:20:21 GMT"}, {"version": "v5", "created": "Sat, 2 Jan 2021 22:02:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Lun", ""], ["Jia", "Ruoxi", ""], ["Song", "Dawn", ""]]}, {"id": "2006.13094", "submitter": "Claudia Furlan", "authors": "Claudia Furlan and Cinzia Mortarino", "title": "The role of swabs in modeling the COVID-19 outbreak in the most affected\n  regions of Italy", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The daily fluctuations in the released number of Covid-19 cases played a big\nrole both at the beginning and in the most critical weeks of the outbreak, when\nlocal authorities in Italy had to decide whether to impose a lockdown and at\nwhich level. Public opinion was focused on this information as well, to\nunderstand how quickly the epidemic was spreading. When an increase/decrease\nwas communicated, especially a large one, it was not easy to understand if it\nwas due to a change in the epidemic evolution or if it was a fluctuation due to\nother reasons, such as an increase in the number of swabs or a delay in the\nswab processing. In this work, we propose a nonlinear asymmetric diffusion\nmodel, which includes information on the daily number of swabs, to describe\ndaily fluctuations in the number of confirmed cases in addition to the main\ntrend of the outbreak evolution. The class of diffusion models has been\nselected to develop our proposal, as it also allows estimation of the total\nnumber of confirmed cases at the end of the outbreak. The proposed model is\ncompared to six existing models, among which the logistic and the SIRD models\nare used as benchmarks, in the five most affected Italian regions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:35:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Furlan", "Claudia", ""], ["Mortarino", "Cinzia", ""]]}, {"id": "2006.13104", "submitter": "Yongqiang Tang Dr.", "authors": "Yongqiang Tang, Liang Zhu, Jiezhun Gu", "title": "An improved sample size calculation method for score tests in\n  generalized linear models", "comments": "26 pages, 1 figure. Statistics in Biopharmaceutical Research 2020", "journal-ref": null, "doi": "10.1080/19466315.2020.1756398", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self and Mauritsen (1988) developed a sample size determination procedure for\nscore tests in generalized linear models under contiguous alternatives. Its\nperformance may deteriorate when the effect size is large. We propose a\nmodification of the Self-Mauritsen method by taking into account of the\nvariance of the score statistic under both the null and alternative hypotheses,\nand extend the method to noninferiority trials. The modified approach is\nemployed to calculate the sample size for the logistic regression and negative\nbinomial regression in superiority and noninferiority trials. We further\nexplain why the formulae recently derived by Zhu and Lakkis tend to\nunderestimate the required sample size for the negative binomial regression.\nNumerical examples are used to demonstrate the accuracy of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:51:34 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tang", "Yongqiang", ""], ["Zhu", "Liang", ""], ["Gu", "Jiezhun", ""]]}, {"id": "2006.13105", "submitter": "Erik Scharw\\\"achter", "authors": "Erik Scharw\\\"achter and Jonathan Lennartz and Emmanuel M\\\"uller", "title": "Differentiable Segmentation of Sequences", "comments": "source codes available at https://github.com/diozaka/diffseg", "journal-ref": "International Conference on Learning Representations (ICLR) 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmented models are widely used to describe non-stationary sequential data\nwith discrete change points. Their estimation usually requires solving a mixed\ndiscrete-continuous optimization problem, where the segmentation is the\ndiscrete part and all other model parameters are continuous. A number of\nestimation algorithms have been developed that are highly specialized for their\nspecific model assumptions. The dependence on non-standard algorithms makes it\nhard to integrate segmented models in state-of-the-art deep learning\narchitectures that critically depend on gradient-based optimization techniques.\nIn this work, we formulate a relaxed variant of segmented models that enables\njoint estimation of all model parameters, including the segmentation, with\ngradient descent. We build on recent advances in learning continuous warping\nfunctions and propose a novel family of warping functions based on the\ntwo-sided power (TSP) distribution. TSP-based warping functions are\ndifferentiable, have simple closed-form expressions, and can represent\nsegmentation functions exactly. Our formulation includes the important class of\nsegmented generalized linear models as a special case, which makes it highly\nversatile. We use our approach to model the spread of COVID-19 with Poisson\nregression, apply it on a change point detection task, and learn classification\nmodels with concept drift. The experiments show that our approach effectively\nlearns all these tasks with standard algorithms for gradient descent.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:51:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 11:11:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Scharw\u00e4chter", "Erik", ""], ["Lennartz", "Jonathan", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "2006.13107", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Fast, Optimal, and Targeted Predictions using Parametrized Decision\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2021.1891926", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is critical for decision-making under uncertainty and lends\nvalidity to statistical inference. With targeted prediction, the goal is to\noptimize predictions for specific decision tasks of interest, which we\nrepresent via functionals. Although classical decision analysis extracts\npredictions from a Bayesian model, these predictions are often difficult to\ninterpret and slow to compute. Instead, we design a class of parametrized\nactions for Bayesian decision analysis that produce optimal, scalable, and\nsimple targeted predictions. For a wide variety of action parametrizations and\nloss functions--including linear actions with sparsity constraints for targeted\nvariable selection--we derive a convenient representation of the optimal\ntargeted prediction that yields efficient and interpretable solutions.\nCustomized out-of-sample predictive metrics are developed to evaluate and\ncompare among targeted predictors. Through careful use of the posterior\npredictive distribution, we introduce a procedure that identifies a set of\nnear-optimal, or acceptable targeted predictors, which provide unique insights\ninto the features and level of complexity needed for accurate targeted\nprediction. Simulations demonstrate excellent prediction, estimation, and\nvariable selection capabilities. Targeted predictions are constructed for\nphysical activity data from the National Health and Nutrition Examination\nSurvey (NHANES) to better predict and understand the characteristics of\nintraday physical activity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:55:47 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 16:39:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2006.13135", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl, Christian Wachinger", "title": "Estimation of Causal Effects in the Presence of Unobserved Confounding\n  in the Alzheimer's Continuum", "comments": "Accepted as oral presentation at 2021 International Conference on\n  Information Processing in Medical Imaging (IPMI)", "journal-ref": "Information Processing in Medical Imaging (2021) 45-57", "doi": "10.1007/978-3-030-78191-0_4", "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the relationship between neuroanatomy and cognitive decline due to\nAlzheimer's has been a major research focus in the last decade. However, to\ninfer cause-effect relationships rather than simple associations from\nobservational data, we need to (i) express the causal relationships leading to\ncognitive decline in a graphical model, and (ii) ensure the causal effect of\ninterest is identifiable from the collected data. We derive a causal graph from\nthe current clinical knowledge on cause and effect in the Alzheimer's disease\ncontinuum, and show that identifiability of the causal effect requires all\nconfounders to be known and measured. However, in complex neuroimaging studies,\nwe neither know all potential confounders nor do we have data on them. To\nalleviate this requirement, we leverage the dependencies among multiple causes\nby deriving a substitute confounder via a probabilistic latent factor model. In\nour theoretical analysis, we prove that using the substitute confounder enables\nidentifiability of the causal effect of neuroanatomy on cognition. We\nquantitatively evaluate the effectiveness of our approach on semi-synthetic\ndata, where we know the true causal effects, and illustrate its use on real\ndata on the Alzheimer's disease continuum, where it reveals important causes\nthat otherwise would have been missed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:29:54 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 15:18:05 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 16:11:32 GMT"}, {"version": "v4", "created": "Sun, 20 Jun 2021 08:42:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Wachinger", "Christian", ""]]}, {"id": "2006.13189", "submitter": "Aaron Sonabend", "authors": "Aaron Sonabend-W, Junwei Lu, Leo A. Celi, Tianxi Cai, Peter Szolovits", "title": "Expert-Supervised Reinforcement Learning for Offline Policy Learning and\n  Evaluation", "comments": "to be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline Reinforcement Learning (RL) is a promising approach for learning\noptimal policies in environments where direct exploration is expensive or\nunfeasible. However, the adoption of such policies in practice is often\nchallenging, as they are hard to interpret within the application context, and\nlack measures of uncertainty for the learned policy value and its decisions. To\novercome these issues, we propose an Expert-Supervised RL (ESRL) framework\nwhich uses uncertainty quantification for offline policy learning. In\nparticular, we have three contributions: 1) the method can learn safe and\noptimal policies through hypothesis testing, 2) ESRL allows for different\nlevels of risk averse implementations tailored to the application context, and\nfinally, 3) we propose a way to interpret ESRL's policy at every state through\nposterior distributions, and use this framework to compute off-policy value\nfunction posteriors. We provide theoretical guarantees for our estimators and\nregret bounds consistent with Posterior Sampling for RL (PSRL). Sample\nefficiency of ESRL is independent of the chosen risk aversion threshold and\nquality of the behavior policy.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:43:44 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 19:14:14 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sonabend-W", "Aaron", ""], ["Lu", "Junwei", ""], ["Celi", "Leo A.", ""], ["Cai", "Tianxi", ""], ["Szolovits", "Peter", ""]]}, {"id": "2006.13281", "submitter": "Chixiang Chen", "authors": "Chixiang Chen, Ming Wang, Rongling Wu, Runze Li", "title": "A Robust Consistent Information Criterion for Model Selection based on\n  Empirical Likelihood", "comments": "JSM Student Paper Award, ASA Nonparametric Section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional likelihood-based information criteria for model selection rely\non the distribution assumption of data. However, for complex data that are\nincreasingly available in many scientific fields, the specification of their\nunderlying distribution turns out to be challenging, and the existing criteria\nmay be limited and are not general enough to handle a variety of model\nselection problems. Here, we propose a robust and consistent model selection\ncriterion based upon the empirical likelihood function which is data-driven. In\nparticular, this framework adopts plug-in estimators that can be achieved by\nsolving external estimating equations, not limited to the empirical likelihood,\nwhich avoids potential computational convergence issues and allows versatile\napplications, such as generalized linear models, generalized estimating\nequations, penalized regressions and so on. The formulation of our proposed\ncriterion is initially derived from the asymptotic expansion of the marginal\nlikelihood under variable selection framework, but more importantly, the\nconsistent model selection property is established under a general context.\nExtensive simulation studies confirm the out-performance of the proposal\ncompared to traditional model selection criteria. Finally, an application to\nthe Atherosclerosis Risk in Communities Study illustrates the practical value\nof this proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 19:24:30 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chen", "Chixiang", ""], ["Wang", "Ming", ""], ["Wu", "Rongling", ""], ["Li", "Runze", ""]]}, {"id": "2006.13450", "submitter": "Hao Chen", "authors": "Yi-Wei Liu and Hao Chen", "title": "A Fast and Efficient Change-point Detection Framework based on\n  Approximate $k$-Nearest Neighbor Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point analysis is thriving in this big data era to address problems\narising in many fields where massive data sequences are collected to study\ncomplicated phenomena over time. It plays an important role in processing these\ndata by segmenting a long sequence into homogeneous parts for follow-up\nstudies. The task requires the method to be able to process large datasets\nquickly and deal with various types of changes for high-dimensional data. We\npropose a new approach making use of approximate $k$-nearest neighbor\ninformation from the observations, and derive an analytic formula to control\nthe type I error. The time complexity of our proposed method is $O(dn\\log\nn+nk^2)$ for an $n$-length sequence of $d$-dimensional data. The test statistic\nwe consider incorporates a useful pattern for moderate- to high- dimensional\ndata so that the proposed method could detect various types of changes in the\nsequence. The new approach is also asymptotic distribution-free, facilitating\nits usage for a broader community. We apply our method to an fMRI dataset and a\nNeuropixels dataset to illustrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 03:25:57 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 06:08:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Liu", "Yi-Wei", ""], ["Chen", "Hao", ""]]}, {"id": "2006.13489", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Yehua Li", "title": "Unified Principal Component Analysis for Sparse and Dense Functional\n  Data under Spatial Dependency", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2021.1938085", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spatially dependent functional data collected under a\ngeostatistics setting, where locations are sampled from a spatial point\nprocess. The functional response is the sum of a spatially dependent functional\neffect and a spatially independent functional nugget effect. Observations on\neach function are made on discrete time points and contaminated with\nmeasurement errors. Under the assumption of spatial stationarity and isotropy,\nwe propose a tensor product spline estimator for the spatio-temporal covariance\nfunction. When a coregionalization covariance structure is further assumed, we\npropose a new functional principal component analysis method that borrows\ninformation from neighboring functions. The proposed method also generates\nnonparametric estimators for the spatial covariance functions, which can be\nused for functional kriging. Under a unified framework for sparse and dense\nfunctional data, infill and increasing domain asymptotic paradigms, we develop\nthe asymptotic convergence rates for the proposed estimators. Advantages of the\nproposed approach are demonstrated through simulation studies and two real data\napplications representing sparse and dense functional data, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:26:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 05:30:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Haozhe", ""], ["Li", "Yehua", ""]]}, {"id": "2006.13516", "submitter": "Samvel Gasparyan", "authors": "Samvel Gasparyan", "title": "Second order asymptotic efficiency for a Poisson process", "comments": "8 pages", "journal-ref": "Journal of Contemporary Mathematical Analysis volume 50, pages\n  98-106 (2015)", "doi": "10.3103/S1068362315020065", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the estimation of the mean function of an\ninhomogeneous Poisson process when its intensity function is periodic. For the\nmean integrated squared error (MISE) there is a classical lower bound for all\nestimators and the empirical mean function attains that lower bound, thus it is\nasymptotically efficient. Following the ideas of the work by Golubev and Levit,\nwe compare asymptotically efficient estimators and propose an estimator which\nis second order asymptotically efficient. Second order efficiency is done over\nSobolev ellipsoids, following the idea of Pinsker.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:58:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Gasparyan", "Samvel", ""]]}, {"id": "2006.13548", "submitter": "Yaqing Chen", "authors": "Yaqing Chen and Hans-Georg M\\\"uller", "title": "Uniform convergence of local Fr\\'echet regression, with applications to\n  locating extrema and time warping for metric space valued trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Fr\\'echet regression is a nonparametric regression method for metric\nspace valued responses and Euclidean predictors, which can be utilized to\nobtain estimates of smooth trajectories taking values in general metric spaces\nfrom noisy metric space valued random objects. We derive uniform rates of\nconvergence, which so far have eluded theoretical analysis of this method, for\nboth fixed and random target trajectories, where we utilize tools from\nempirical processes. These results are shown to be widely applicable in metric\nspace valued data analysis. In addition to simulations, we provide two\npertinent examples where these results are important: The consistent estimation\nof the location of properly defined extrema in metric space valued\ntrajectories, which we illustrate with the problem of locating the age of\nminimum brain connectivity as obtained from fMRI data; Time warping for metric\nspace valued trajectories, illustrated with yearly age-at-death distributions\nfor different countries.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:15:12 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 23:29:25 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 01:47:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Yaqing", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2006.13698", "submitter": "Ick Hoon Jin", "authors": "Jaewoo Park, Yeseul Jeon, Minsuk Shin, Minjeong Jeon, Ick Hoon Jin", "title": "Bayesian Shrinkage for Functional Network Models with Intractable\n  Normalizing Constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal network models are widely used to study the time-varying\nrelationships between items (nodes), such as analyzing the relations among\nsurvey questions and studying friendship dynamics in school data over time. We\npropose a new model to study these temporal interactions among items by\nembedding the functional parameters within the exponential random graph model\nframework. Inference on such models is difficult because the likelihood\nfunctions contain intractable normalizing constants. Furthermore, the number of\nfunctional parameters grows exponentially as the number of items increases.\nVariable selection for such models is not trivial because standard shrinkage\napproaches do not consider temporal trends in functional parameters. To\novercome these challenges, we develop a novel Bayes approach by combining an\nauxiliary variable MCMC algorithm and a recently developed functional shrinkage\nmethod. We apply our algorithm to two survey data sets and hotel review data,\nillustrating that the proposed approach can avoid the evaluation of intractable\nnormalizing constants as well as detect significant temporal interactions among\nitems. Through a simulation study under different scenarios, we examine the\nperformance of our algorithm. Our method is, to our knowledge, the first\nattempt to select functional variables for models with intractable normalizing\nconstants.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:08:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Park", "Jaewoo", ""], ["Jeon", "Yeseul", ""], ["Shin", "Minsuk", ""], ["Jeon", "Minjeong", ""], ["Jin", "Ick Hoon", ""]]}, {"id": "2006.13700", "submitter": "Lorenzo Rimella", "authors": "Nick Whiteley, Lorenzo Rimella", "title": "Inference in Stochastic Epidemic Models via Multinomial Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for inference in stochastic epidemic models which\nuses recursive multinomial approximations to integrate over unobserved\nvariables and thus circumvent likelihood intractability. The method is\napplicable to a class of discrete-time, finite-population compartmental models\nwith partial, randomly under-reported or missing count observations. In\ncontrast to state-of-the-art alternatives such as Approximate Bayesian\nComputation techniques, no forward simulation of the model is required and\nthere are no tuning parameters. Evaluating the approximate marginal likelihood\nof model parameters is achieved through a computationally simple filtering\nrecursion. The accuracy of the approximation is demonstrated through analysis\nof real and simulated data using a model of the 1995 Ebola outbreak in the\nDemocratic Republic of Congo. We show how the method can be embedded within a\nSequential Monte Carlo approach to estimating the time-varying reproduction\nnumber of COVID-19 in Wuhan, China, recently published by Kucharski et al.\n2020.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:08:56 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 12:06:51 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Whiteley", "Nick", ""], ["Rimella", "Lorenzo", ""]]}, {"id": "2006.13786", "submitter": "Philo P\\\"ollmann", "authors": "Xiang Liu and Philo P\\\"ollmann", "title": "Dynamic Population Estimation Using Anonymized Mobility Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine population distribution both in space and in time is crucial for\nepidemic management, disaster prevention,urban planning and more. Human\nmobility data have a great potential for mapping population distribution at a\nhigh level of spatiotemporal resolution. Power law models are the most popular\nones for mapping mobility data to population. However,they fail to provide\nconsistent estimations under different spatial and temporal resolutions, i.e.\nthey have to be recalibrated whenever the spatial or temporal partitioning\nscheme changes. We propose a Bayesian model for dynamic population estimation\nusing static census data and anonymized mobility data. Our model gives\nconsistent population estimations under different spatial and temporal\nresolutions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:00:22 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Liu", "Xiang", ""], ["P\u00f6llmann", "Philo", ""]]}, {"id": "2006.13790", "submitter": "Juntao Wang", "authors": "Juntao Wang, Ningzhong Shi, Xue Zhang, Gongjun Xu", "title": "Sequential Gibbs Sampling Algorithm for Cognitive Diagnosis Models with\n  Many Attributes", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive diagnosis models (CDMs) are useful statistical tools to provide\nrich information relevant for intervention and learning. As a popular approach\nto estimate and make inference of CDMs, the Markov chain Monte Carlo (MCMC)\nalgorithm is widely used in practice. However, when the number of attributes,\n$K$, is large, the existing MCMC algorithm may become time-consuming, due to\nthe fact that $O(2^K)$ calculations are usually needed in the process of MCMC\nsampling to get the conditional distribution for each attribute profile. To\novercome this computational issue, motivated by Culpepper and Hudson (2018), we\npropose a computationally efficient sequential Gibbs sampling method, which\nneeds $O(K)$ calculations to sample each attribute profile. We use simulation\nand real data examples to show the good finite-sample performance of the\nproposed sequential Gibbs sampling, and its advantage over existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:02:48 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 06:12:04 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Juntao", ""], ["Shi", "Ningzhong", ""], ["Zhang", "Xue", ""], ["Xu", "Gongjun", ""]]}, {"id": "2006.13815", "submitter": "Primoz Kocbek", "authors": "Simon Kocbek, Primoz Kocbek, Leona Cilar, Gregor Stiglic", "title": "Local Interpretability of Calibrated Prediction Models: A Case of Type 2\n  Diabetes Mellitus Screening Test", "comments": "Submitted to the DSHealth 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) models are often complex and difficult to interpret due\nto their 'black-box' characteristics. Interpretability of a ML model is usually\ndefined as the degree to which a human can understand the cause of decisions\nreached by a ML model. Interpretability is of extremely high importance in many\nfields of healthcare due to high levels of risk related to decisions based on\nML models. Calibration of the ML model outputs is another issue often\noverlooked in the application of ML models in practice. This paper represents\nan early work in examination of prediction model calibration impact on the\ninterpretability of the results. We present a use case of a patient in diabetes\nscreening prediction scenario and visualize results using three different\ntechniques to demonstrate the differences between calibrated and uncalibrated\nregularized regression model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:14:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kocbek", "Simon", ""], ["Kocbek", "Primoz", ""], ["Cilar", "Leona", ""], ["Stiglic", "Gregor", ""]]}, {"id": "2006.13850", "submitter": "Matteo Fontana", "authors": "Matteo Fontana, Massimo Tavoni, Simone Vantini", "title": "Global Sensitivity and Domain-Selective Testing for Functional-Valued\n  Responses: An Application to Climate Economy Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex computational models are increasingly used by business and\ngovernments for making decisions, such as how and where to invest to transition\nto a low carbon world. Complexity arises with great evidence in the outputs\ngenerated by large scale models, and calls for the use of advanced Sensitivity\nAnalysis techniques. To our knowledge, there are no methods able to perform\nsensitivity analysis for outputs that are more complex than scalar ones and to\ndeal with model uncertainty using a sound statistical framework. The aim of\nthis work is to address these two shortcomings by combining sensitivity and\nfunctional data analysis. We express output variables as smooth functions,\nemploying a Functional Data Analysis (FDA) framework. We extend global\nsensitivity techniques to function-valued responses and perform significance\ntesting over sensitivity indices. We apply the proposed methods to computer\nmodels used in climate economics. While confirming the qualitative intuitions\nof previous works, we are able to test the significance of input assumptions\nand of their interactions. Moreover, the proposed method allows to identify the\ntime dynamics of sensitivity indices.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:25:44 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 17:58:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Fontana", "Matteo", ""], ["Tavoni", "Massimo", ""], ["Vantini", "Simone", ""]]}, {"id": "2006.13875", "submitter": "Grace Yoon", "authors": "Grace Yoon, Christian L. M\\\"uller, Irina Gaynanova", "title": "Fast computation of latent correlations", "comments": "Main text: 21 pages and 4 figures. Supplementary material: 24 pages\n  and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian copula models provide a powerful means to perform multi-view\ndata integration since these models can seamlessly express dependencies between\nmixed variable types (binary, continuous, zero-inflated) via latent Gaussian\ncorrelations. The estimation of these latent correlations, however, comes at\nconsiderable computational cost, having prevented the routine use of these\nmodels on high-dimensional data. Here, we propose a new computational approach\nfor estimating latent correlations via a hybrid multi-linear interpolation and\noptimization scheme. Our approach speeds up the current state of the art\ncomputation by several orders of magnitude, thus allowing fast computation of\nlatent Gaussian copula models even when the number of variables $p$ is large.\nWe provide theoretical guarantees for the approximation error of our numerical\nscheme and support its excellent performance on simulated and real-world data.\nWe illustrate the practical advantages of our method on high-dimensional sparse\nquantitative and relative abundance microbiome data as well as multi-view data\nfrom The Cancer Genome Atlas Project. Our method is implemented in the R\npackage mixedCCA, available at https://github.com/irinagain/mixedCCA.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:58:35 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:37:17 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yoon", "Grace", ""], ["M\u00fcller", "Christian L.", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2006.13887", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao, Ron D. Frostig and Hernando Ombao", "title": "Break Point Detection for Functional Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments record sequential trajectories that oscillate around zero.\nSuch trajectories can be viewed as zero-mean functional data. When there are\nstructural breaks (on the sequence of curves) in higher order moments, it is\noften difficult to spot these by mere visual inspection. Thus, we propose a\ndetection and testing procedure to find the change-points in functional\ncovariance. The method is fully functional in the sense that no dimension\nreduction is needed. We establish the asymptotic properties of the estimated\nchange-point. The effectiveness of the proposed method is numerically validated\nin the simulation studies and an application to study structural changes in rat\nbrain signals in a stroke experiment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:13:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 00:38:19 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 16:33:19 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Jiao", "Shuhao", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2006.13925", "submitter": "Peiyuan Zhu", "authors": "Peiyuan Zhu, Alexandre Bouchard-C\\^ot\\'e, and Trevor Campbell", "title": "Slice Sampling for General Completely Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely random measures provide a principled approach to creating flexible\nunsupervised models, where the number of latent features is infinite and the\nnumber of features that influence the data grows with the size of the data set.\nDue to the infinity the latent features, posterior inference requires either\nmarginalization---resulting in dependence structures that prevent efficient\ncomputation via parallelization and conjugacy---or finite truncation, which\narbitrarily limits the flexibility of the model. In this paper we present a\nnovel Markov chain Monte Carlo algorithm for posterior inference that\nadaptively sets the truncation level using auxiliary slice variables, enabling\nefficient, parallelized computation without sacrificing flexibility. In\ncontrast to past work that achieved this on a model-by-model basis, we provide\na general recipe that is applicable to the broad class of completely random\nmeasure-based priors. The efficacy of the proposed algorithm is evaluated on\nseveral popular nonparametric models, demonstrating a higher effective sample\nsize per second compared to algorithms using marginalization as well as a\nhigher predictive performance compared to models employing fixed truncations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:53:53 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:30:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhu", "Peiyuan", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Campbell", "Trevor", ""]]}, {"id": "2006.14126", "submitter": "David Frazier", "authors": "David T. Frazier", "title": "Robust and Efficient Approximate Bayesian Computation: A Minimum\n  Distance Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances, the application of approximate Bayesian methods is\nhampered by two practical features: 1) the requirement to project the data down\nto low-dimensional summary, including the choice of this projection, which\nultimately yields inefficient inference; 2) a possible lack of robustness to\ndeviations from the underlying model structure. Motivated by these efficiency\nand robustness concerns, we construct a new Bayesian method that can deliver\nefficient estimators when the underlying model is well-specified, and which is\nsimultaneously robust to certain forms of model misspecification. This new\napproach bypasses the calculation of summaries by considering a norm between\nempirical and simulated probability measures. For specific choices of the norm,\nwe demonstrate that this approach can deliver point estimators that are as\nefficient as those obtained using exact Bayesian inference, while also\nsimultaneously displaying robustness to deviations from the underlying model\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 01:11:02 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Frazier", "David T.", ""]]}, {"id": "2006.14211", "submitter": "Purushottam Kar", "authors": "Bhaskar Mukhoty and Govind Gopakumar and Prateek Jain and Purushottam\n  Kar", "title": "Globally-convergent Iteratively Reweighted Least Squares for Robust\n  Regression Problems", "comments": "30 pages, 5 figures, appeared as a publication in the 22nd\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2019", "journal-ref": "Proceedings of Machine Learning Research (PMLR) 89:313-322, 2019", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first global model recovery results for the IRLS (iteratively\nreweighted least squares) heuristic for robust regression problems. IRLS is\nknown to offer excellent performance, despite bad initializations and data\ncorruption, for several parameter estimation problems. Existing analyses of\nIRLS frequently require careful initialization, thus offering only local\nconvergence guarantees. We remedy this by proposing augmentations to the basic\nIRLS routine that not only offer guaranteed global recovery, but in practice\nalso outperform state-of-the-art algorithms for robust regression. Our routines\nare more immune to hyperparameter misspecification in basic regression tasks,\nas well as applied tasks such as linear-armed bandit problems. Our theoretical\nanalyses rely on a novel extension of the notions of strong convexity and\nsmoothness to weighted strong convexity and smoothness, and establishing that\nsub-Gaussian designs offer bounded weighted condition numbers. These notions\nmay be useful in analyzing other algorithms as well.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:16:13 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Mukhoty", "Bhaskar", ""], ["Gopakumar", "Govind", ""], ["Jain", "Prateek", ""], ["Kar", "Purushottam", ""]]}, {"id": "2006.14217", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti and Massimiliano Russo", "title": "Stratified stochastic variational inference for high-dimensional network\n  factor model", "comments": "25 pages, 1 figures. Corrected compilation issues and minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent interest in Bayesian modeling of\nhigh-dimensional networks via latent space approaches. When the number of nodes\nincreases, estimation based on Markov Chain Monte Carlo can be extremely slow\nand show poor mixing, thereby motivating research on alternative algorithms\nthat scale well in high-dimensional settings. In this article, we focus on the\nlatent factor model, a widely used approach for latent space modeling of\nnetwork data. We develop scalable algorithms to conduct approximate Bayesian\ninference via stochastic optimization. Leveraging sparse representations of\nnetwork data, the proposed algorithms show massive computational and storage\nbenefits, and allow to conduct inference in settings with thousands of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:27:47 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 10:09:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Russo", "Massimiliano", ""]]}, {"id": "2006.14273", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho and Claudia Kirch", "title": "Discussion of 'Detecting possibly frequent change-points: Wild Binary\n  Segmentation 2 and steepest-drop model selection'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the theoretical guarantee provided by the WBS2.SDLL proposed in\nFryzlewicz (2020) and explore an alternative, MOSUM-based candidate generation\nmethod for the SDLL.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:37:09 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 14:24:56 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Cho", "Haeran", ""], ["Kirch", "Claudia", ""]]}, {"id": "2006.14316", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Dennis Dobler, Markus Pauly", "title": "Inferring median survival differences in general factorial designs via\n  permutation tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial survival designs with right-censored observations are commonly\ninferred by Cox regression and explained by means of hazard ratios. However, in\ncase of non-proportional hazards, their interpretation can become cumbersome;\nespecially for clinicians. We therefore offer an alternative: median survival\ntimes are used to estimate treatment and interaction effects and null\nhypotheses are formulated in contrasts of their population versions.\nPermutation-based tests and confidence regions are proposed and shown to be\nasymptotically valid. Their type-1 error control and power behavior are\ninvestigated in extensive simulations, showing the new methods' wide\napplicability. The latter is complemented by an illustrative data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:33:37 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Dobler", "Dennis", ""], ["Pauly", "Markus", ""]]}, {"id": "2006.14343", "submitter": "Maxime Conjard", "authors": "Maxime Conjard, Henning Omre", "title": "Spatio-temporal Inversion using the Selection Kalman Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation in models representing spatio-temporal phenomena poses a\nchallenge, particularly if the spatial histogram of the variable appears with\nmultiple modes. The traditional Kalman model is based on a Gaussian initial\ndistribution and Gauss-linear dynamic and observation models. This model is\ncontained in the class of Gaussian distribution and is therefore analytically\ntractable. It is however unsuitable for representing multimodality. We define\nthe selection Kalman model that is based on a selection-Gaussian initial\ndistribution and Gauss-linear dynamic and observation models. The\nselection-Gaussian distribution can be seen as a generalization of the Gaussian\ndistribution and may represent multimodality, skewness and peakedness. This\nselection Kalman model is contained in the class of selection-Gaussian\ndistributions and therefore it is analytically tractable. An efficient\nrecursive algorithm for assessing the selection Kalman model is specified. The\nsynthetic case study of spatio-temporal inversion of an initial state, inspired\nby pollution monitoring, containing an extreme event suggests that the use of\nthe selection Kalman model offers significant improvements compared to the\ntraditional Kalman model when reconstructing discontinuous initial states.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 12:41:22 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Conjard", "Maxime", ""], ["Omre", "Henning", ""]]}, {"id": "2006.14411", "submitter": "Elizabeth Pei-Ting Chou", "authors": "Fushing Hsieh, Elizabeth P. Chou", "title": "Categorical Exploratory Data Analysis: From Multiclass Classification\n  and Response Manifold Analytics perspectives of baseball pitching dynamics", "comments": null, "journal-ref": null, "doi": "10.3390/e23070792", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From two coupled Multiclass Classification (MCC) and Response Manifold\nAnalytics (RMA) perspectives, we develop Categorical Exploratory Data Analysis\n(CEDA) on PITCHf/x database for the information content of Major League\nBaseball's (MLB) pitching dynamics. MCC and RMA information contents are\nrepresented by one collection of multi-scales pattern categories from mixing\ngeometries and one collection of global-to-local geometric localities from\nresponse-covariate manifolds, respectively. These collectives shed light on the\npitching dynamics and maps out uncertainty of popular machine learning\napproaches. On MCC setting, an indirect-distance-measure based label embedding\ntree leads to discover asymmetry of mixing geometries among labels'\npoint-clouds. A selected chain of complementary covariate feature groups\ncollectively brings out multi-order mixing geometric pattern categories. Such\ncategories then reveal the true nature of MCC predictive inferences. On RMA\nsetting, multiple response features couple with multiple major covariate\nfeatures to demonstrate physical principles bearing manifolds with a lattice of\nnatural localities. With minor features' heterogeneous effects being locally\nidentified, such localities jointly weave their focal characteristics into\nsystem understanding and provide a platform for RMA predictive inferences. Our\nCEDA works for universal data types, adopts non-linear associations and\nfacilitates efficient feature-selections and inferences.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:50:12 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Hsieh", "Fushing", ""], ["Chou", "Elizabeth P.", ""]]}, {"id": "2006.14621", "submitter": "Sinead Williamson", "authors": "Sinead A. Williamson", "title": "ANOVA exemplars for understanding data drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributions underlying complex datasets, such as images, text or\ntabular data, are often difficult to visualize in terms of summary statistics\nsuch as the mean or the marginal standard deviations. Instead, a small set of\nexemplars or prototypes---real or synthetic data points that are in some sense\nrepresentative of the entire distribution---can be used to provide a\nhuman-interpretable summary of the distribution. In many situations, we are\ninterested in understanding the \\textit{difference} between two distributions.\nFor example, we may be interested in identifying and characterizing data drift\nover time, or the difference between two related datasets. While exemplars are\noften more easily understood than high-dimensional summary statistics, they are\nharder to compare. To solve this problem, we introduce ANOVA exemplars. Rather\nthan independently find exemplars $S_X$ and $S_Y$ for two datasets $X$ and $Y$,\nwe aim to find exemplars that are both representative of $X$ and $Y$, and that\nmaximize the overlap $|S_X\\cap S_Y|$ between the two sets of exemplars. We can\nthen use the differences between the two sets of exemplars to describe the\ndifference between the distributions of $X$ and $Y$, in a concise,\ninterpretable manner.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 23:35:01 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Williamson", "Sinead A.", ""]]}, {"id": "2006.14732", "submitter": "Tatiana Komarova", "authors": "Tatiana Komarova and Denis Nekipelov", "title": "Identification and Formal Privacy Guarantees", "comments": "69 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical economic research crucially relies on highly sensitive individual\ndatasets. At the same time, increasing availability of public individual-level\ndata makes it possible for adversaries to potentially de-identify anonymized\nrecords in sensitive research datasets. Most commonly accepted formal\ndefinition of an individual non-disclosure guarantee is referred to as\ndifferential privacy. It restricts the interaction of researchers with the data\nby allowing them to issue queries to the data. The differential privacy\nmechanism then replaces the actual outcome of the query with a randomised\noutcome.\n  The impact of differential privacy on the identification of empirical\neconomic models and on the performance of estimators in nonlinear empirical\nEconometric models has not been sufficiently studied. Since privacy protection\nmechanisms are inherently finite-sample procedures, we define the notion of\nidentifiability of the parameter of interest under differential privacy as a\nproperty of the limit of experiments. It is naturally characterized by the\nconcepts from the random sets theory.\n  We show that particular instances of regression discontinuity design may be\nproblematic for inference with differential privacy as parameters turn out to\nbe neither point nor partially identified. The set of differentially private\nestimators converges weakly to a random set. Our analysis suggests that many\nother estimators that rely on nuisance parameters may have similar properties\nwith the requirement of differential privacy. We show that identification\nbecomes possible if the target parameter can be deterministically located\nwithin the random set. In that case, a full exploration of the random set of\nthe weak limits of differentially private estimators can allow the data curator\nto select a sequence of instances of differentially private estimators\nconverging to the target parameter in probability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 23:36:45 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 22:24:27 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Komarova", "Tatiana", ""], ["Nekipelov", "Denis", ""]]}, {"id": "2006.14737", "submitter": "Nokuthaba Sibanda", "authors": "Doaa Ayad and Nokuthaba Sibanda", "title": "Monitoring of process and risk-adjusted medical outcomes using a\n  multi-stage MEWMA chart", "comments": "17 pages, 3 figures Submitted to Statistical Methods in Medical\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statistical process control programmes in healthcare focus on\nsurveillance of outcomes at the final stage of a procedure, such as mortality\nor failure rates. Such an approach ignores the multi-stage nature of these\nprocedures, in which a patient progresses through several stages prior to the\nfinal stage. In this paper, we develop a multi-stage control chart based on a\nmultivariate exponentially weighted moving average (EWMA) test statistic\nderived from score equations. This allows simultaneous monitoring of all\nintermediate and final stage outcomes of a healthcare process, with adjustment\nfor underlying patient risk factors and dependence between outcome variables.\nUse of the EWMA test statistics allows quick detection of small gradual changes\nin any part of the process. Three advantages of the approach are: better\nunderstanding of how outcomes at different stages relate to each other,\nexplicit monitoring of upstream stage outcomes may help curtail trends that\nlead to poorer end-stage outcomes and understanding the impact of each stage\ncan help determine the most effective allocation of quality improvement\nresources. Simulations are performed to test the control charts under various\ntypes of hypothesised shifts, and the results are summarised using\nout-of-control average run lengths.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 00:31:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ayad", "Doaa", ""], ["Sibanda", "Nokuthaba", ""]]}, {"id": "2006.14820", "submitter": "Shonosuke Sugasawa", "authors": "Takumi Saegusa, Shonosuke Sugasawa, and Partha Lahiri", "title": "Parametric Bootstrap Confidence Intervals for the Multivariate\n  Fay-Herriot Model", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Fay-Herriot model is quite effective in combining\ninformation through correlations among small area survey estimates of related\nvariables or historical survey estimates of the same variable or both. Though\nthe literature on small area estimation is already very rich, construction of\nsecond-order efficient confidence intervals from multivariate models have so\nfar received very little attention. In this paper, we develop a parametric\nbootstrap method for constructing a second-order efficient confidence interval\nfor a general linear combination of small area means using the multivariate\nFay-Herriot normal model. The proposed parametric bootstrap method replaces\ndifficult and tedious analytical derivations by the power of efficient\nalgorithm and high speed computer. Moreover, the proposed method is more\nversatile than the analytical method because the parametric bootstrap method\ncan be easily applied to any method of model parameter estimation and any\nspecific structure of the variance-covariance matrix of the multivariate\nFay-Herriot model avoiding all the cumbersome and time-consuming calculations\nrequired in the analytical method. We apply our proposed methodology in\nconstructing confidence intervals for the median income of four-person families\nfor the fifty states and the District of Columbia in the United States. Our\ndata analysis demonstrates that the proposed parametric bootstrap method\ngenerally provides much shorter confidence intervals compared to the\ncorresponding traditional direct method. Moreover, the confidence intervals\nobtained from the multivariate model is generally shorter than the\ncorresponding univariate model indicating the potential advantage of exploiting\ncorrelations of median income of four-person families with median incomes of\nthree and five person families.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 06:41:08 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Saegusa", "Takumi", ""], ["Sugasawa", "Shonosuke", ""], ["Lahiri", "Partha", ""]]}, {"id": "2006.14831", "submitter": "Sungkyu Jung", "authors": "Zhao Ren and Sungkyu Jung and Xingye Qiao", "title": "Covariance-engaged Classification of Sets via Linear Programming", "comments": "86 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set classification aims to classify a set of observations as a whole, as\nopposed to classifying individual observations separately. To formally\nunderstand the unfamiliar concept of binary set classification, we first\ninvestigate the optimal decision rule under the normal distribution, which\nutilizes the empirical covariance of the set to be classified. We show that the\nnumber of observations in the set plays a critical role in bounding the Bayes\nrisk. Under this framework, we further propose new methods of set\nclassification. For the case where only a few parameters of the model drive the\ndifference between two classes, we propose a computationally-efficient approach\nto parameter estimation using linear programming, leading to the\nCovariance-engaged LInear Programming Set (CLIPS) classifier. Its theoretical\nproperties are investigated for both independent case and various (short-range\nand long-range dependent) time series structures among observations within each\nset. The convergence rates of estimation errors and risk of the CLIPS\nclassifier are established to show that having multiple observations in a set\nleads to faster convergence rates, compared to the standard classification\nsituation in which there is only one observation in the set. The applicable\ndomains in which the CLIPS performs better than competitors are highlighted in\na comprehensive simulation study. Finally, we illustrate the usefulness of the\nproposed methods in classification of real image data in histopathology.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 07:20:15 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ren", "Zhao", ""], ["Jung", "Sungkyu", ""], ["Qiao", "Xingye", ""]]}, {"id": "2006.14877", "submitter": "Santeri Karppinen", "authors": "Santeri Karppinen and Matti Vihola", "title": "Conditional particle filters with diffuse initial distributions", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional particle filters (CPFs) are powerful smoothing algorithms for\ngeneral nonlinear/non-Gaussian hidden Markov models. However, CPFs can be\ninefficient or difficult to apply with diffuse initial distributions, which are\ncommon in statistical applications. We propose a simple but generally\napplicable auxiliary variable method, which can be used together with the CPF\nin order to perform efficient inference with diffuse initial distributions. The\nmethod only requires simulatable Markov transitions that are reversible with\nrespect to the initial distribution, which can be improper. We focus in\nparticular on random-walk type transitions which are reversible with respect to\na uniform initial distribution (on some domain), and autoregressive kernels for\nGaussian initial distributions. We propose to use on-line adaptations within\nthe methods. In the case of random-walk transition, our adaptations use the\nestimated covariance and acceptance rate adaptation, and we detail their\ntheoretical validity. We tested our methods with a linear-Gaussian random-walk\nmodel, a stochastic volatility model, and a stochastic epidemic compartment\nmodel with time-varying transmission rate. The experimental findings\ndemonstrate that our method works reliably with little user specification, and\ncan be substantially better mixing than a direct particle Gibbs algorithm that\ntreats initial states as parameters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:21:13 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:16:41 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Karppinen", "Santeri", ""], ["Vihola", "Matti", ""]]}, {"id": "2006.14880", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Frank Schaarschmidt", "title": "A modified Armitage test for more than a linear trend on proportions", "comments": "5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The Armitage test for linear trend in proportions can be modified using the\nmultiple marginal model approach for three regression models with arithmetic,\nordinal and logarithmic dose scores simultaneously, to be powerful against a\nwide range of possible dose response relationships. Moreover, it can be used\nfor particular designs in the generalized linear (mixed) model for the three\ncommon effect sizes odds ratio, risk ratio and risk difference. The related R\npackage tukeytrend allows simple generalizations, e.g. the analysis 2-by-k\ntable data with a possible plateau shape or analysing overdispersed\nproportions. The evaluation of further real data examples are available in a\nvignette to that R package.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:26:47 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Schaarschmidt", "Frank", ""]]}, {"id": "2006.14888", "submitter": "Mattias Nordin", "authors": "Mattias Nordin and M{\\aa}rten Schultzberg", "title": "Properties of restricted randomization with implications for\n  experimental design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there as been an increasing interest in the use of heavily\nrestricted randomization designs which enforces balance on observed covariates\nin randomized controlled trials. However, when restrictions are too strict,\nthere is a risk that the treatment effect estimator will have a very high mean\nsquared error. In this paper, we formalize this risk and propose a novel\ncombinatoric-based approach to describe and address this issue. First, some\nknown properties of complete randomization and restricted randomization are\nre-proven using basic combinatorics. Second, a novel diagnostic measure that\nonly use the information embedded in the combinatorics of the design is\nproposed. Finally, we identify situations in which restricted designs can lead\nto an increased risk of getting a high mean squared error and discuss how our\ndiagnostic measure can be used to detect such designs. Our results have\nimplications for any restricted randomization design and can be used to\nevaluate the trade-off between enforcing balance on observed covariates and\navoiding too restrictive designs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:56:09 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:04:00 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Nordin", "Mattias", ""], ["Schultzberg", "M\u00e5rten", ""]]}, {"id": "2006.14998", "submitter": "Qingliang Fan", "authors": "Qingliang Fan, Yaqian Wu", "title": "Endogenous Treatment Effect Estimation with some Invalid and Irrelevant\n  Instruments", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables (IV) regression is a popular method for the estimation\nof the endogenous treatment effects. Conventional IV methods require all the\ninstruments are relevant and valid. However, this is impractical especially in\nhigh-dimensional models when we consider a large set of candidate IVs. In this\npaper, we propose an IV estimator robust to the existence of both the invalid\nand irrelevant instruments (called R2IVE) for the estimation of endogenous\ntreatment effects. This paper extends the scope of Kang et al. (2016) by\nconsidering a true high-dimensional IV model and a nonparametric reduced form\nequation. It is shown that our procedure can select the relevant and valid\ninstruments consistently and the proposed R2IVE is root-n consistent and\nasymptotically normal. Monte Carlo simulations demonstrate that the R2IVE\nperforms favorably compared to the existing high-dimensional IV estimators\n(such as, NAIVE (Fan and Zhong, 2018) and sisVIVE (Kang et al., 2016)) when\ninvalid instruments exist. In the empirical study, we revisit the classic\nquestion of trade and growth (Frankel and Romer, 1999).\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 14:11:43 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Fan", "Qingliang", ""], ["Wu", "Yaqian", ""]]}, {"id": "2006.15077", "submitter": "Jonathan Von Schroeder", "authors": "Jonathan von Schroeder", "title": "Stable Feature Selection with Applications to MALDI Imaging Mass\n  Spectrometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses an approach, based on the subsampling boostrap and FDR\ncontrol, to improve the stability of feature selection. It furthermore presents\nthe finite sample distribution of the correlation coefficient recently proposed\nby Chatterjee (2020) under the setting relevant for this paper. Finally an\napplication to matrix-assisted laser desorption/ionization (MALDI) imaging mass\nspectroscopy data is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:15:37 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["von Schroeder", "Jonathan", ""]]}, {"id": "2006.15282", "submitter": "Madan Kundu", "authors": "Madan Gopal Kundu and Samiran Ghosh", "title": "Survival trees for right-censored data based on score based parameter\n  instability test", "comments": "23 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis of right censored data arises often in many areas of\nresearch including medical research. Effect of covariates (and their\ninteractions) on survival distribution can be studied through existing methods\nwhich requires to pre-specify the functional form of the covariates including\ntheir interactions. Survival trees offer relatively flexible approach when the\nform of covariates' effects is unknown. Most of the currently available\nsurvival tree construction techniques are not based on a formal test of\nsignificance; however, recently proposed ctree algorithm (Hothorn et al., 2006)\nuses permutation test for splitting decision that may be conservative at times.\nWe consider parameter instability test of statistical significance of\nheterogeneity to guard against spurious findings of variation in covariates'\neffect without being overly conservative. We have proposed SurvCART algorithm\nto construct survival tree under conditional inference framework (Hothorn et\nal., 2006) that selects splitting variable via parameter instability test and\nsubsequently finds the optimal split based on some maximally chosen statistic.\nNotably, unlike the existing algorithms which focuses only on heterogeneity in\nevent time distribution, the proposed SurvCART algorithm can take splitting\ndecision based in censoring distribution as well along with heterogeneity in\nevent time distribution. The operating characteristics of parameter instability\ntest and comparative assessment of SurvCART algorithm were carried out via\nsimulation. Finally, SurvCART algorithm was applied to a real data setting. The\nproposed method is fully implemented in R package LongCART available on CRAN.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 04:31:55 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kundu", "Madan Gopal", ""], ["Ghosh", "Samiran", ""]]}, {"id": "2006.15387", "submitter": "Marco Felix Eigenmann", "authors": "Marco F. Eigenmann, Sach Mukherjee, Marloes H. Maathuis", "title": "Evaluation of Causal Structure Learning Algorithms via Risk Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen many advances in methods for causal structure learning\nfrom data. The empirical assessment of such methods, however, is much less\ndeveloped. Motivated by this gap, we pose the following question: how can one\nassess, in a given problem setting, the practical efficacy of one or more\ncausal structure learning methods? We formalize the problem in a\ndecision-theoretic framework, via a notion of expected loss or risk for the\ncausal setting. We introduce a theoretical notion of causal risk as well as\nsample quantities that can be computed from data, and study the relationship\nbetween the two, both theoretically and through an extensive simulation study.\nOur results provide an assumptions-light framework for assessing causal\nstructure learning methods that can be applied in a range of practical\nuse-cases.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 15:37:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Eigenmann", "Marco F.", ""], ["Mukherjee", "Sach", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "2006.15396", "submitter": "Taylor Brown", "authors": "Taylor R. Brown", "title": "Approximating Posterior Predictive Distributions by Averaging Output\n  From Many Particle Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the {\\it particle swarm filter} (not to be confused\nwith particle swarm optimization): a recursive and embarrassingly parallel\nalgorithm that targets an approximation to the sequence of posterior predictive\ndistributions by averaging expectation approximations from many particle\nfilters. A law of large numbers and a central limit theorem are provided, as\nwell as an numerical study of simulated data from a stochastic volatility\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 16:14:02 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 19:45:39 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 00:28:58 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Brown", "Taylor R.", ""]]}, {"id": "2006.15409", "submitter": "Chenchen Ma", "authors": "Chenchen Ma, Jimmy de la Torre, Gongjun Xu", "title": "Bridging Parametric and Nonparametric Methods in Cognitive Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of parametric and nonparametric methods for estimating cognitive\ndiagnosis models (CDMs) have been developed and applied in a wide range of\ncontexts. However, in the literature, a wide chasm exists between these two\nfamilies of methods, and their relationship to each other is not well\nunderstood. In this paper, we propose a unified estimation framework to bridge\nthe divide between parametric and nonparametric methods in cognitive diagnosis\nto better understand their relationship. We also develop iterative joint\nestimation algorithms and establish consistency properties within the proposed\nframework. Lastly, we present comprehensive simulation results to compare\ndifferent methods, and provide practical recommendations on the appropriate use\nof the proposed framework in various CDM contexts.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:26:34 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 21:32:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ma", "Chenchen", ""], ["de la Torre", "Jimmy", ""], ["Xu", "Gongjun", ""]]}, {"id": "2006.15424", "submitter": "Chenchen Ma", "authors": "Chengcheng Li, Chenchen Ma, Gongjun Xu", "title": "Learning Large $Q$-matrix by Restricted Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the large $Q$-matrix in Cognitive Diagnosis Models (CDMs) with\nmany items and latent attributes from observational data has been a huge\nchallenge due to its high computational cost. Borrowing ideas from deep\nlearning literature, we propose to learn the large $Q$-matrix by Restricted\nBoltzmann Machines (RBMs) to overcome the computational difficulties. In this\npaper, key relationships between RBMs and CDMs are identified. Consistent and\nrobust learning of the $Q$-matrix in various CDMs is shown to be valid under\ncertain conditions. Our simulation studies under different CDM settings show\nthat RBMs not only outperform the existing methods in terms of learning speed,\nbut also maintain good recovery accuracy of the $Q$-matrix. In the end, we\nillustrate the applicability and effectiveness of our method through a real\ndata analysis on the Cattell's 16 personality test data set.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 18:28:55 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 22:13:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Chengcheng", ""], ["Ma", "Chenchen", ""], ["Xu", "Gongjun", ""]]}, {"id": "2006.15532", "submitter": "Shaochuan Lu", "authors": "Lu Shaochuan", "title": "Scalable Bayesian Multiple Changepoint Detection via Auxiliary\n  Uniformization", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By attaching auxiliary event times to the chronologically ordered\nobservations, we formulate the Bayesian multiple changepoint problem of\ndiscrete-time observations into that of continuous-time ones. A version of\nforward-filtering backward-sampling (FFBS) algorithm is proposed for the\nsimulation of changepoints within a collapsed Gibbs sampling scheme. Ideally,\nboth the computational cost and memory cost of the FFBS algorithm can be\nquadratically scaled down to the number of changepoints, instead of the number\nof observations, which is otherwise prohibitive for a long sequence of\nobservations. The new formulation allows the number of changepoints accrue\nunboundedly upon the arrivals of new data. Also, a time-varying changepoint\nrecurrence rate across different segments is assumed to characterize diverse\nscales of run lengths of changepoints. We then suggest a continuous-time\nViterbi algorithm for obtaining the Maximum A Posteriori (MAP) estimates of\nchangepoints. We demonstrate the methods through simulation studies and real\ndata analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 07:52:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Shaochuan", "Lu", ""]]}, {"id": "2006.15638", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson, Ravi Varadhan, Thomas A. Louis", "title": "Improved Small Area Estimation via Compromise Regression Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimates of small domain parameters typically utilize a\ncombination of a noisy \"direct\" estimate that only uses data from a specific\nsmall domain and a more stable regression estimate. When the regression model\nis misspecified, estimation performance for the noisier domains can suffer due\nto substantial shrinkage towards a poorly estimated regression surface. In this\npaper, we introduce a new class of robust, empirically-driven regression\nweights that target estimation of the small domain means under potential\nmisspecification of the global regression model. Our regression weights are a\nconvex combination of the model-based weights associated with the best linear\nunbiased predictor (BLUP) and those associated with the observed best predictor\n(OBP). The compromise parameter in this convex combination is found by\nminimizing a novel, unbiased estimate of the mean-squared prediction error for\nthe small domain means, and we label the associated small domain estimates the\n\"compromise best predictor\", or CBP. Using a data-adaptive mixture for the\nregression weights enables the CBP to possess the robustness of the OBP while\nretaining the main advantages of the EBLUP whenever the regression model is\ncorrect. We demonstrate the use of the CBP in an application estimating gait\nspeed in older adults.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:51:15 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Varadhan", "Ravi", ""], ["Louis", "Thomas A.", ""]]}, {"id": "2006.15640", "submitter": "Ryan Martin", "authors": "Huiying Mao and Ryan Martin and Brian Reich", "title": "Valid model-free spatial prediction", "comments": "30 pages, 9 figures, 3 tables. Comments welcome at\n  https://www.researchers.one/article/2020-06-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the response at an unobserved location is a fundamental problem in\nspatial statistics. Given the difficulty in modeling spatial dependence,\nespecially in non-stationary cases, model-based prediction intervals are at\nrisk of misspecification bias that can negatively affect their validity. Here\nwe present a new approach for model-free spatial prediction based on the {\\em\nconformal prediction} machinery. Our key observation is that spatial data can\nbe treated as exactly or approximately exchangeable in a wide range of\nsettings. For example, when the spatial locations are deterministic, we prove\nthat the response values are, in a certain sense, locally approximately\nexchangeable for a broad class of spatial processes, and we develop a local\nspatial conformal prediction algorithm that yields valid prediction intervals\nwithout model assumptions. Numerical examples with both real and simulated data\nconfirm that the proposed conformal prediction intervals are valid and\ngenerally more efficient than existing model-based procedures across a range of\nnon-stationary and non-Gaussian settings.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 16:12:57 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mao", "Huiying", ""], ["Martin", "Ryan", ""], ["Reich", "Brian", ""]]}, {"id": "2006.15667", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng and Yifei Hu", "title": "High-Dimensional Inference for Unidentifiable Signals with False\n  Negative Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  False negative errors are of major concern in applications where missing a\nhigh proportion of true signals may cause serious consequences. False negative\ncontrol, however, raises a bottleneck challenge in high-dimensional inference\nwhen signals are not identifiable at individual levels. We develop a new\nanalytic framework to regulate false negative errors under measures tailored\ntowards modern applications with high-dimensional data. A new method is\nproposed in realistic settings with arbitrary covariance dependence between\nvariables. We explicate the joint effects of covariance dependence and signal\nsparsity on the new method and interpret the results using a phase diagram. It\nshows that signals that are not individually identifiable can be effectively\nretained by the proposed method without incurring excessive false positives.\nSimulation studies are conducted to compare the new method with several\nexisting methods. The new method outperforms the others in adapting to a\nuser-specified false negative control level. We apply the new method to analyze\nan fMRI dataset to locate voxels that are functionally relevant to saccadic eye\nmovements. The new method exhibits a nice balance in retaining signal voxels\nand avoiding excessive noise voxels.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 17:50:19 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 01:16:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Hu", "Yifei", ""]]}, {"id": "2006.15681", "submitter": "Mats Julius Stensrud", "authors": "Mats J. Stensrud, James M. Robins, Aaron Sarvet, Eric J. Tchetgen\n  Tchetgen, Jessica G. Young", "title": "Conditional separable effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in treatment effects on outcomes that are\nonly defined conditional on a post-treatment event status. For example, in a\nstudy of the effect of different cancer treatments on quality of life at end of\nfollow-up, the quality of life of individuals who die during the study is\nundefined. In these settings, a naive contrast of outcomes conditional on the\npost-treatment variable is not an average causal effect, even in a randomized\nexperiment. Therefore the effect in the principal stratum of those who would\nhave the same value of the post-treatment variable regardless of treatment,\nsuch as the always survivors in a truncation by death setting, is often\nadvocated for causal inference. While this principal stratum effect is a well\ndefined causal contrast, it is often hard to justify that it is relevant to\nscientists, patients or policy makers, and it cannot be identified without\nrelying on unfalsifiable assumptions. Here we formulate alternative estimands,\nthe conditional separable effects, that have a natural causal interpretation\nunder assumptions that can be falsified in a randomized experiment. We provide\nidentification results and introduce different estimators, including a doubly\nrobust estimator derived from the nonparametric influence function. As an\nillustration, we estimate a conditional separable effect of chemotherapies on\nquality of life in patients with prostate cancer, using data from a randomized\nclinical trial.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 19:09:50 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 14:10:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 15:46:08 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Stensrud", "Mats J.", ""], ["Robins", "James M.", ""], ["Sarvet", "Aaron", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Young", "Jessica G.", ""]]}, {"id": "2006.15689", "submitter": "Zhiyuan Huang", "authors": "Yuanlu Bai, Zhiyuan Huang, Henry Lam", "title": "A Distributionally Robust Optimization Approach to the NASA Langley\n  Uncertainty Quantification Challenge", "comments": "Published in the Proceedings of the 30th European Safety and\n  Reliability Conference and the 15th Probabilistic Safety Assessment and\n  Management Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a methodology to tackle the NASA Langley Uncertainty Quantification\nChallenge problem, based on an integration of robust optimization, more\nspecifically a recent line of research known as distributionally robust\noptimization, and importance sampling in Monte Carlo simulation. The main\ncomputation machinery in this integrated methodology boils down to solving\nsampled linear programs. We will illustrate both our numerical performances and\ntheoretical statistical guarantees obtained via connections to nonparametric\nhypothesis testing.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 19:44:22 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bai", "Yuanlu", ""], ["Huang", "Zhiyuan", ""], ["Lam", "Henry", ""]]}, {"id": "2006.15738", "submitter": "Pierre-Andr\\'e Maugis", "authors": "P-A. Maugis", "title": "Central limit theorems for local network statistics", "comments": "39 pages, 4 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counts - in particular the number of occurrences of small shapes\nsuch as triangles - characterize properties of random networks, and as a result\nhave seen wide use as network summary statistics. However, subgraphs are\ntypically counted globally, and existing approaches fail to describe\nvertex-specific characteristics. On the other hand, rooted subgraph counts -\ncounts focusing on any given vertex's neighborhood - are fundamental\ndescriptors of local network properties. We derive the asymptotic joint\ndistribution of rooted subgraph counts in inhomogeneous random graphs, a model\nwhich generalizes many popular statistical network models. This result enables\na shift in the statistical analysis of large graphs, from estimating network\nsummaries, to estimating models linking local network structure and\nvertex-specific covariates. As an example, we consider a school friendship\nnetwork and show that local friendship patterns are significant predictors of\ngender and race.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 22:50:51 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Maugis", "P-A.", ""]]}, {"id": "2006.15899", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele, Stijn Vansteelandt", "title": "A statistical test to reject the structural interpretation of a latent\n  factor model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factor analysis is often used to assess whether a single univariate latent\nvariable is sufficient to explain most of the covariance among a set of\nindicators for some underlying construct. When evidence suggests that a single\nfactor is adequate, research often proceeds by using a univariate summary of\nthe indicators in subsequent research. Implicit in such practices is the\nassumption that it is the underlying latent, rather than the indicators, that\nis causally efficacious. The assumption that the indicators do not have effects\non anything subsequent, and that they are themselves only affected by\nantecedents through the underlying latent is a strong assumption, effectively\nimposing a structural interpretation on the latent factor model. In this paper,\nwe show that this structural assumption has empirically testable implications,\neven though the latent is unobserved. We develop a statistical test to\npotentially reject the structural interpretation of a latent factor model. We\napply this test to data concerning associations between the\nSatisfaction-with-Life-Scale and subsequent all-cause mortality, which provides\nstrong evidence against a structural interpretation for a univariate latent\nunderlying the scale. Discussion is given to the implications of this result\nfor the development, evaluation, and use of measures related to latent factor\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 09:45:23 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 21:41:27 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2006.15907", "submitter": "Marco Scavino", "authors": "Renzo Caballero, Ahmed Kebaier, Marco Scavino and Ra\\'ul Tempone", "title": "Quantifying Uncertainty with a Derivative Tracking SDE Model and\n  Application to Wind Power Forecast Data", "comments": "28 pages and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a data-driven methodology based on parametric It\\^{o}'s Stochastic\nDifferential Equations (SDEs) to capture the real asymmetric dynamics of\nforecast errors. Our SDE framework features time-derivative tracking of the\nforecast, time-varying mean-reversion parameter, and an improved\nstate-dependent diffusion term. Proofs of the existence, strong uniqueness, and\nboundedness of the SDE solutions are shown under a principled condition for the\ntime-varying mean-reversion parameter. Inference based on approximate\nlikelihood, constructed through the moment-matching technique both in the\noriginal forecast error space and in the Lamperti space, is performed through\nnumerical optimization procedures. We propose another contribution based on the\nfixed-point likelihood optimization approach in the Lamperti space. All the\nprocedures are agnostic of the forecasting technology, and they enable\ncomparisons between different forecast providers. We apply our SDE framework to\nmodel historical Uruguayan normalized wind power production and forecast data\nbetween April and December 2019. Sharp empirical confidence bands of future\nwind power production are obtained for the best-selected model.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 09:53:05 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 23:50:12 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Caballero", "Renzo", ""], ["Kebaier", "Ahmed", ""], ["Scavino", "Marco", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "2006.16063", "submitter": "Matteo Iacopini", "authors": "Carlo Romano Marcello Alessandro Santagiustina and Matteo Iacopini", "title": "Visualizing and comparing distributions with half-disk density strips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a user-friendly graphical tool, the half-disk density strip\n(HDDS), for visualizing and comparing probability density functions. The HDDS\nexploits color shading for representing a distribution in an intuitive way. In\nunivariate settings, the half-disk density strip allows to immediately discern\nthe key characteristics of a density, such as symmetry, dispersion, and\nmulti-modality. In the multivariate settings, we define HDDS tables to\ngeneralize the concept of contingency tables. It is an array of half-disk\ndensity strips, which compactly displays the univariate marginal and\nconditional densities of a variable of interest, together with the joint and\nmarginal densities of the conditioning variables. Moreover, HDDSs are by\nconstruction well suited to easily compare pairs of densities. To highlight the\nconcrete benefits of the proposed methods, we show how to use HDDSs for\nanalyzing income distribution and life-satisfaction, conditionally on\ncontinuous and categorical controls, from survey data. The code for\nimplementing HDDS methods is made available through a dedicated R package.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:10:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Santagiustina", "Carlo Romano Marcello Alessandro", ""], ["Iacopini", "Matteo", ""]]}, {"id": "2006.16079", "submitter": "Hongshik Ahn", "authors": "Hongshik Ahn, Haoran Jiang and Xiaolin Li", "title": "Modeling and Computation of High Efficiency and Efficacy Multi-Step\n  Batch Testing for Infectious Diseases", "comments": "26 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mathematical model based on probability theory to optimize\nCOVID-19 testing by a multi-step batch testing approach with variable batch\nsizes. This model and simulation tool dramatically increase the efficiency and\nefficacy of the tests in a large population at a low cost, particularly when\nthe infection rate is low. The proposed method combines statistical modeling\nwith numerical methods to solve nonlinear equations and obtain optimal batch\nsizes at each step of tests, with the flexibility to incorporate geographic and\ndemographic information. In theory, this method substantially improves the\nfalse positive rate and positive predictive value as well. We also conducted a\nMonte Carlo simulation to verify this theory. Our simulation results show that\nour method significantly reduces the false negative rate. More accurate\nassessment can be made if the dilution effect or other practical factors are\ntaken into consideration. The proposed method will be particularly useful for\nthe early detection of infectious diseases and prevention of future pandemics.\nThe proposed work will have broader impacts on medical testing for contagious\ndiseases in general.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:34:15 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 02:03:28 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 13:12:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ahn", "Hongshik", ""], ["Jiang", "Haoran", ""], ["Li", "Xiaolin", ""]]}, {"id": "2006.16156", "submitter": "Graciela Boente Prof.", "authors": "Graciela Boente, Matias Salibian-Barrera, Pablo Vena", "title": "Robust estimation for semi-functional linear regression models", "comments": "In press", "journal-ref": "Computational Statistics and Data Analysis, 2020", "doi": "10.1016/j.csda.2020.107041", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-functional linear regression models postulate a linear relationship\nbetween a scalar response and a functional covariate, and also include a\nnon-parametric component involving a univariate explanatory variable. It is of\npractical importance to obtain estimators for these models that are robust\nagainst high-leverage outliers, which are generally difficult to identify and\nmay cause serious damage to least squares and Huber-type $M$-estimators. For\nthat reason, robust estimators for semi-functional linear regression models are\nconstructed combining $B$-splines to approximate both the functional regression\nparameter and the nonparametric component with robust regression estimators\nbased on a bounded loss function and a preliminary residual scale estimator.\nConsistency and rates of convergence for the proposed estimators are derived\nunder mild regularity conditions. The reported numerical experiments show the\nadvantage of the proposed methodology over the classical least squares and\nHuber-type $M$-estimators for finite samples. The analysis of real examples\nillustrate that the robust estimators provide better predictions for\nnon-outlying points than the classical ones, and that when potential outliers\nare removed from the training and test sets both methods behave very similarly.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:19:00 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 01:02:33 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 18:05:22 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Boente", "Graciela", ""], ["Salibian-Barrera", "Matias", ""], ["Vena", "Pablo", ""]]}, {"id": "2006.16214", "submitter": "Panos Toulis", "authors": "Panos Toulis", "title": "Estimation of Covid-19 Prevalence from Serology Tests: A Partial\n  Identification Approach", "comments": "Journal of Econometrics (2020), forthcoming", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a partial identification method for estimating disease prevalence\nfrom serology studies. Our data are results from antibody tests in some\npopulation sample, where the test parameters, such as the true/false positive\nrates, are unknown. Our method scans the entire parameter space, and rejects\nparameter values using the joint data density as the test statistic. The\nproposed method is conservative for marginal inference, in general, but its key\nadvantage over more standard approaches is that it is valid in finite samples\neven when the underlying model is not point identified. Moreover, our method\nrequires only independence of serology test results, and does not rely on\nasymptotic arguments, normality assumptions, or other approximations. We use\nrecent Covid-19 serology studies in the US, and show that the parameter\nconfidence set is generally wide, and cannot support definite conclusions.\nSpecifically, recent serology studies from California suggest a prevalence\nanywhere in the range 0%-2% (at the time of study), and are therefore\ninconclusive. However, this range could be narrowed down to 0.7%-1.5% if the\nactual false positive rate of the antibody test was indeed near its empirical\nestimate (~0.5%). In another study from New York state, Covid-19 prevalence is\nconfidently estimated in the range 13%-17% in mid-April of 2020, which also\nsuggests significant geographic variation in Covid-19 exposure across the US.\nCombining all datasets yields a 5%-8% prevalence range. Our results overall\nsuggest that serology testing on a massive scale can give crucial information\nfor future policy design, even when such tests are imperfect and their\nparameters unknown.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:25:25 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Toulis", "Panos", ""]]}, {"id": "2006.16357", "submitter": "Guorong Dai", "authors": "Guorong Dai, Ursula U. M\\\"uller and Raymond J. Carroll", "title": "Data integration in high dimension with multiple quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the analysis of high dimensional data that come from\nmultiple sources (experiments) and thus have different possibly correlated\nresponses, but share the same set of predictors. The measurements of the\npredictors may be different across experiments. We introduce a new regression\napproach with multiple quantiles to select those predictors that affect any of\nthe responses at any quantile level and estimate the nonzero parameters. Our\nestimator is a minimizer of a penalized objective function, which aggregates\nthe data from the different experiments. We establish model selection\nconsistency and asymptotic normality of the estimator. In addition we present\nan information criterion, which can also be used for consistent model\nselection. Simulations and two data applications illustrate the advantages of\nour method, which takes the group structure induced by the predictors across\nexperiments and quantile levels into account.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:22:17 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Dai", "Guorong", ""], ["M\u00fcller", "Ursula U.", ""], ["Carroll", "Raymond J.", ""]]}, {"id": "2006.16361", "submitter": "Guorong Dai", "authors": "Guorong Dai, Ursula U. M\\\"uller", "title": "Penalized regression with multiple loss functions and selection by vote", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers a linear model in a high dimensional data scenario. We\npropose a process which uses multiple loss functions both to select relevant\npredictors and to estimate parameters, and study its asymptotic properties.\nVariable selection is conducted by a procedure called \"vote\", which aggregates\nresults from penalized loss functions. Using multiple objective functions\nseparately simplifies algorithms and allows parallel computing, which is\nconvenient and fast. As a special example we consider a quantile regression\nmodel, which optimally combines multiple quantile levels. We show that the\nresulting estimators for the parameter vector are asymptotically efficient.\nSimulations and a data application confirm the three main advantages of our\napproach: (a) reducing the false discovery rate of variable selection; (b)\nimproving the quality of parameter estimation; (c) increasing the efficiency of\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:28:48 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Dai", "Guorong", ""], ["M\u00fcller", "Ursula U.", ""]]}, {"id": "2006.16388", "submitter": "Michele Azzone", "authors": "Michele Azzone and Roberto Baviera", "title": "Neural Network Middle-Term Probabilistic Forecasting of Daily Power\n  Consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Middle-term horizon (months to a year) power consumption prediction is a main\nchallenge in the energy sector, in particular when probabilistic forecasting is\nconsidered. We propose a new modelling approach that incorporates trend,\nseasonality and weather conditions, as explicative variables in a shallow\nNeural Network with an autoregressive feature. We obtain excellent results for\ndensity forecast on the one-year test set applying it to the daily power\nconsumption in New England U.S.A.. The quality of the achieved power\nconsumption probabilistic forecasting has been verified, on the one hand,\ncomparing the results to other standard models for density forecasting and, on\nthe other hand, considering measures that are frequently used in the energy\nsector as pinball loss and CI backtesting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:07:01 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Azzone", "Michele", ""], ["Baviera", "Roberto", ""]]}, {"id": "2006.16433", "submitter": "Runxue Bao", "authors": "Runxue Bao, Bin Gu, Heng Huang", "title": "Fast OSCAR and OWL Regression via Safe Screening Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered Weighted $L_{1}$ (OWL) regularized regression is a new regression\nanalysis for high-dimensional sparse learning. Proximal gradient methods are\nused as standard approaches to solve OWL regression. However, it is still a\nburning issue to solve OWL regression due to considerable computational cost\nand memory usage when the feature or sample size is large. In this paper, we\npropose the first safe screening rule for OWL regression by exploring the order\nof the primal solution with the unknown order structure via an iterative\nstrategy, which overcomes the difficulties of tackling the non-separable\nregularizer. It effectively avoids the updates of the parameters whose\ncoefficients must be zero during the learning process. More importantly, the\nproposed screening rule can be easily applied to standard and stochastic\nproximal gradient methods. Moreover, we prove that the algorithms with our\nscreening rule are guaranteed to have identical results with the original\nalgorithms. Experimental results on a variety of datasets show that our\nscreening rule leads to a significant computational gain without any loss of\naccuracy, compared to existing competitive algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 23:35:53 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bao", "Runxue", ""], ["Gu", "Bin", ""], ["Huang", "Heng", ""]]}, {"id": "2006.16464", "submitter": "Johan Koskinen", "authors": "Johan Koskinen and Galina Daraganova", "title": "Bayesian Analysis of Social Influence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network influence model is a model for binary outcome variables that\naccounts for dependencies between outcomes for units that are relationally\ntied. The basic influence model was previously extended to afford a suite of\nnew dependence assumptions and because of its relation to traditional Markov\nrandom field models it is often referred to as the auto logistic\nactor-attribute model (ALAAM). We extend on current approaches for fitting\nALAAMs by presenting a comprehensive Bayesian inference scheme that supports\ntesting of dependencies across subsets of data and the presence of missing\ndata. We illustrate different aspects of the procedures through three empirical\nexamples: masculinity attitudes in an all-male Australian school class,\neducational progression in Swedish schools, and un-employment among adults in a\ncommunity sample in Australia.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 01:36:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Koskinen", "Johan", ""], ["Daraganova", "Galina", ""]]}, {"id": "2006.16501", "submitter": "Dan Yang", "authors": "Xin Chen, Dan Yang, Yan Xu, Yin Xia, Dong Wang, Haipeng Shen", "title": "Testing and Support Recovery of Correlation Structures for Matrix-Valued\n  Observations with an Application to Stock Market Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix of asset returns is crucial to portfolio\nconstruction. As suggested by economic theories, the correlation structure\namong assets differs between emerging markets and developed countries. It is\ntherefore imperative to make rigorous statistical inference on correlation\nmatrix equality between the two groups of countries. However, if the\ntraditional vector-valued approach is undertaken, such inference is either\ninfeasible due to limited number of countries comparing to the relatively\nabundant assets, or invalid due to the violations of temporal independence\nassumption. This highlights the necessity of treating the observations as\nmatrix-valued rather than vector-valued. With matrix-valued observations, our\nproblem of interest can be formulated as statistical inference on covariance\nstructures under matrix normal distributions, i.e., testing independence and\ncorrelation equality, as well as the corresponding support estimations. We\ndevelop procedures that are asymptotically optimal under some regularity\nconditions. Simulation results demonstrate the computational and statistical\nadvantages of our procedures over certain existing state-of-the-art methods.\nApplication of our procedures to stock market data validates several economic\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:18:19 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Xin", ""], ["Yang", "Dan", ""], ["Xu", "Yan", ""], ["Xia", "Yin", ""], ["Wang", "Dong", ""], ["Shen", "Haipeng", ""]]}, {"id": "2006.16527", "submitter": "Johan Koskinen", "authors": "Johan Koskinen", "title": "Discussion of the paper \"Optimal treatment allocations in space and time\n  for on-line control of an emerging infectious disease'' by E. B. Laber, N. J.\n  Meyer, B. J. Reich, K. Pacifici, J. A. Collazo and J. Drake", "comments": "This is the pre-peer reviewed version of the following article:\n  Koskinen, J.H. (2018), Discussion of \"Optimal treatment allocations in space\n  and time for on-line control of an emerging infectious disease\" by Laber,\n  Meyer, Reich, Pacifici, Collazo and Drake, J.R.Statist.Soc. C 67, p779 which\n  has been published in final form at\n  https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12266", "journal-ref": "J.R.Statist.Soc. C 67, p779 (2018)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a written discussion of the paper \"Optimal treatment allocations in\nspace and time for on-line control of an emerging infectious disease\" by E. B.\nLaber, N. J. Meyer, B. J. Reich, K. Pacifici, J. A. Collazo and J. Drake,\ncontributed to the Journal of the Royal Statistical Society Series C\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 04:45:12 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Koskinen", "Johan", ""]]}, {"id": "2006.16539", "submitter": "Benny Ren", "authors": "Benny Ren, Ian Barnett", "title": "Autoregressive Mixture Models for Serial Correlation Clustering of Time\n  Series Data", "comments": "submitted to Technometrics\n  https://amstat.tandfonline.com/toc/utch20/current", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering individuals into similar groups in longitudinal studies can\nimprove time series models by combining information across like individuals.\nWhile there is a well developed literature for clustering of time series, these\napproaches tend to generate clusters independently of the model training\nprocedure which can lead to poor model fit. We propose a novel method that\nsimultaneously clusters and fits autoregression models for groups of similar\nindividuals. We apply a Wishart mixture model so as to cluster individuals\nwhile modeling the corresponding autocorrelation matrices at the same time. The\nfitted Wishart scale matrices map to cluster-level autoregressive coefficients\nthrough the Yule-Walker equations, fitting robust parsimonious autoregressive\nmixture models. This approach is able to discern differences in underlying\nserial variation of time series while accounting for an individual's intrinsic\nvariability. We prove consistency of our cluster membership estimator and\ncompare our approach against competing methods through simulation as well as by\nmodeling regional COVID-19 infection rates.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 05:38:27 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ren", "Benny", ""], ["Barnett", "Ian", ""]]}, {"id": "2006.16548", "submitter": "Gonzalo Mena E", "authors": "Gonzalo Mena, Amin Nejatbakhsh, Erdem Varol and Jonathan Niles-Weed", "title": "Sinkhorn EM: An Expectation-Maximization algorithm based on entropic\n  optimal transport", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Sinkhorn EM (sEM), a variant of the expectation maximization (EM)\nalgorithm for mixtures based on entropic optimal transport. sEM differs from\nthe classic EM algorithm in the way responsibilities are computed during the\nexpectation step: rather than assign data points to clusters independently, sEM\nuses optimal transport to compute responsibilities by incorporating prior\ninformation about mixing weights. Like EM, sEM has a natural interpretation as\na coordinate ascent procedure, which iteratively constructs and optimizes a\nlower bound on the log-likelihood. However, we show theoretically and\nempirically that sEM has better behavior than EM: it possesses better global\nconvergence guarantees and is less prone to getting stuck in bad local optima.\nWe complement these findings with experiments on simulated data as well as in\nan inference task involving C. elegans neurons and show that sEM learns cell\nlabels significantly better than other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 06:03:37 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mena", "Gonzalo", ""], ["Nejatbakhsh", "Amin", ""], ["Varol", "Erdem", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "2006.16638", "submitter": "Ilyas Bakbergenuly", "authors": "Elena Kulinskaya, David C. Hoaglin, and Ilyas Bakbergenuly", "title": "Exploring Consequences of Simulation Design for Apparent Performance of\n  Statistical Methods. 1: Results from simulations with constant sample sizes", "comments": "8 pages and full simulation results, comprising 400 figures, each\n  presenting 12 combinations of sample sizes and numbers of studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary statistical publications rely on simulation to evaluate\nperformance of new methods and compare them with established methods. In the\ncontext of meta-analysis of log-odds-ratios, we investigate how the ways in\nwhich simulations are implemented affect such conclusions. Choices of\ndistributions for sample sizes and/or control probabilities considerably affect\nconclusions about statistical methods. Here we report on the results for\nconstant sample sizes. Our two subsequent publications will cover normally and\nuniformly distributed sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:02:00 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 08:54:16 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kulinskaya", "Elena", ""], ["Hoaglin", "David C.", ""], ["Bakbergenuly", "Ilyas", ""]]}, {"id": "2006.16653", "submitter": "Kirill Neklyudov", "authors": "Kirill Neklyudov, Max Welling, Evgenii Egorov, Dmitry Vetrov", "title": "Involutive MCMC: a Unifying Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental\nproblems such as inference, integration, optimization, and simulation. The\nfield has developed a broad spectrum of algorithms, varying in the way they are\nmotivated, the way they are applied and how efficiently they sample. Despite\nall the differences, many of them share the same core principle, which we unify\nas the Involutive MCMC (iMCMC) framework. Building upon this, we describe a\nwide range of MCMC algorithms in terms of iMCMC, and formulate a number of\n\"tricks\" which one can use as design principles for developing new MCMC\nalgorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms,\nwhich facilitates the derivation of powerful extensions. We demonstrate the\nlatter with two examples where we transform known reversible MCMC algorithms\ninto more efficient irreversible ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:21:42 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Neklyudov", "Kirill", ""], ["Welling", "Max", ""], ["Egorov", "Evgenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "2006.16657", "submitter": "Peter Filzmoser", "authors": "Sukru Acitas, Peter Filzmoser, Birdal Senoglu", "title": "A Robust Adaptive Modified Maximum Likelihood Estimator for the Linear\n  Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear regression, the least squares (LS) estimator has certain optimality\nproperties if the errors are normally distributed. This assumption is often\nviolated in practice, partly caused by data outliers. Robust estimators can\ncope with this situation and thus they are widely used in practice. One example\nof robust estimators for regression are adaptive modified maximum likelihood\n(AMML) estimators (Donmez, 2010). However, they are not robust to $x$ outliers,\nso-called leverage points. In this study, we propose a new regression estimator\nby employing an appropriate weighting scheme in the AMML estimation method. The\nresulting estimator is called robust AMML (RAMML) since it is not only robust\nto y outliers but also to x outliers. A simulation study is carried out to\ncompare the performance of the RAMML estimator with some existing robust\nestimators such as MM, least trimmed squares (LTS) and S. The results show that\nthe RAMML estimator is preferable in most settings according to the mean\nsquared error (MSE) criterion. Two data sets taken from the literature are also\nanalyzed to show the implementation of the RAMML estimation methodology.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:23:45 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Acitas", "Sukru", ""], ["Filzmoser", "Peter", ""], ["Senoglu", "Birdal", ""]]}, {"id": "2006.16859", "submitter": "Arthur Chatton", "authors": "A. Chatton (1 and 2) and F. Le Borgne (1 and 2) and C. Leyrat (3 and\n  4) and Y. Foucher (1 and 5) ((1) INSERM UMR 1246 - SPHERE, Universit\\'e de\n  Nantes, Universit\\'e de Tours, (2) IDBC-A2COM, (3) Department of Medical\n  Statistics, London School of Hygiene and Tropical Medicine, London, (4)\n  Inequalities in Cancer Outcomes Network, London School of Hygiene and\n  Tropical Medicine, (5) Centre Hospitalier Universitaire de Nantes)", "title": "G-computation for continuous-time data: a comparison with inverse\n  probability weighting", "comments": "16 pages, including 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability weighting is increasingly used in causal inference, but\nthe g-computation constitutes a promising alternative. We aimed to compare the\nperformances of these methods with time-to-event outcomes. Given the\nlimitations of the interpretability of the hazard ratio for causal inference,\nour target estimand of interest is the difference in restricted mean survival\ntimes. We report the findings of an extensive simulation study showing that\nboth inverse probability weighting and g-computation are unbiased under a\ncorrect model specification, but g-computation is generally more efficient. We\nalso analyse two real-world datasets to illustrate these results. Finally, we\nupdate the R package RISCA to facilitate the implementation of g-computation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:51:51 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 14:46:51 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chatton", "A.", "", "1 and 2"], ["Borgne", "F. Le", "", "1 and 2"], ["Leyrat", "C.", "", "3 and\n  4"], ["Foucher", "Y.", "", "1 and 5"]]}, {"id": "2006.16901", "submitter": "Marcin Jurek", "authors": "Marcin Jurek and Matthias Katzfuss", "title": "Hierarchical sparse Cholesky decomposition with applications to\n  high-dimensional spatio-temporal filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial statistics often involves Cholesky decomposition of covariance\nmatrices. To ensure scalability to high dimensions, several recent\napproximations have assumed a sparse Cholesky factor of the precision matrix.\nWe propose a hierarchical Vecchia approximation, whose conditional-independence\nassumptions imply sparsity in the Cholesky factors of both the precision and\nthe covariance matrix. This remarkable property is crucial for applications to\nhigh-dimensional spatio-temporal filtering. We present a fast and simple\nalgorithm to compute our hierarchical Vecchia approximation, and we provide\nextensions to non-linear data assimilation with non-Gaussian data based on the\nLaplace approximation. In several numerical comparisons, our methods strongly\noutperformed alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:26:10 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Jurek", "Marcin", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "2006.16916", "submitter": "Amanda Coston", "authors": "Amanda Coston, Edward H. Kennedy, Alexandra Chouldechova", "title": "Counterfactual Predictions under Runtime Confounding", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems Vol 33, 2020.\n  pp. 4150--4162", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms are commonly used to predict outcomes under a particular decision\nor intervention, such as predicting whether an offender will succeed on parole\nif placed under minimal supervision. Generally, to learn such counterfactual\nprediction models from observational data on historical decisions and\ncorresponding outcomes, one must measure all factors that jointly affect the\noutcomes and the decision taken. Motivated by decision support applications, we\nstudy the counterfactual prediction task in the setting where all relevant\nfactors are captured in the historical data, but it is either undesirable or\nimpermissible to use some such factors in the prediction model. We refer to\nthis setting as runtime confounding. We propose a doubly-robust procedure for\nlearning counterfactual prediction models in this setting. Our theoretical\nanalysis and experimental results suggest that our method often outperforms\ncompeting approaches. We also present a validation procedure for evaluating the\nperformance of counterfactual prediction methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:49:05 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 01:29:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Coston", "Amanda", ""], ["Kennedy", "Edward H.", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2006.16941", "submitter": "Saeed Khaki", "authors": "Saeed Khaki and Dan Nettleton", "title": "Conformal Prediction Intervals for Neural Networks Using Cross\n  Validation", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are among the most powerful nonlinear models used to address\nsupervised learning problems. Similar to most machine learning algorithms,\nneural networks produce point predictions and do not provide any prediction\ninterval which includes an unobserved response value with a specified\nprobability. In this paper, we proposed the $k$-fold prediction interval method\nto construct prediction intervals for neural networks based on $k$-fold cross\nvalidation. Simulation studies and analysis of 10 real datasets are used to\ncompare the finite-sample properties of the prediction intervals produced by\nthe proposed method and the split conformal (SC) method. The results suggest\nthat the proposed method tends to produce narrower prediction intervals\ncompared to the SC method while maintaining the same coverage probability. Our\nexperimental results also reveal that the proposed $k$-fold prediction interval\nmethod produces effective prediction intervals and is especially advantageous\nrelative to competing approaches when the number of training observations is\nlimited.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:23:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Khaki", "Saeed", ""], ["Nettleton", "Dan", ""]]}]