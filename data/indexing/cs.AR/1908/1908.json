[{"id": "1908.00165", "submitter": "Song Chen", "authors": "Song Chen, Mengke Ge, Zhigang Li, Jinglei Huang, Qi Xu, and Feng Wu", "title": "Generalized Fault-Tolerance Topology Generation for Application Specific\n  Network-on-Chips", "comments": "14 pages, 14 figures, 7 tables, submitted to IEEE T-CAD for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network-on-Chips is a promising candidate for addressing communication\nbottlenecks in many-core processors and neural network processors. In this\nwork, we consider the generalized fault-tolerance topology generation problem,\nwhere the link or switch failures can happen, for application-specific\nnetwork-on-chips (ASNoC). With a user-defined number, K, we propose an integer\nlinear programming (ILP) based method to generate ASNoC topologies, which can\ntolerate at most K faults in switches or links. Given the communication\nrequirements between cores and their floorplan, we first propose a\nconvex-cost-flow based method to solve a core mapping problem for building\nconnections between the cores and switches. Second, an ILP based method is\nproposed to allocate K+1 switch-disjoint routing paths for every communication\nflow between the cores. Finally, to reduce switch sizes, we propose sharing the\nswitch ports for the connections between the cores and switches and formulate\nthe port sharing problem as a clique-partitioning problem Additionally, we\npropose an ILP-based method to simultaneously solve the core mapping and\nrouting path allocation problems when considering physical link failures only.\nExperimental results show that the power consumptions of fault-tolerance\ntopologies increase almost linearly with K because of the routing path\nredundancy. When both switch faults and link faults are considered, port\nsharing can reduce the average power consumption of fault-tolerance topologies\nwith K = 1, K = 2 and K = 3 by 18.08%, 28.88%, and 34.20%, respectively. When\nconsidering only the physical link faults, the experimental results show that\ncompared to the FTTG algorithm, the proposed method reduces power consumption\nand hop count by 10.58% and 6.25%, respectively; compared to the DBG based\nmethod, the proposed method reduces power consumption and hop count by 21.72%\nand 9.35%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 01:14:03 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Chen", "Song", ""], ["Ge", "Mengke", ""], ["Li", "Zhigang", ""], ["Huang", "Jinglei", ""], ["Xu", "Qi", ""], ["Wu", "Feng", ""]]}, {"id": "1908.00289", "submitter": "N Prasad", "authors": "N Prasad, Navonil Chatterjee, Santanu Chattopadhyay, Indrajit\n  Chakrabarti", "title": "Runtime Mitigation of Packet Drop Attacks in Fault-tolerant\n  Networks-on-Chip", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault-tolerant routing (FTR) in Networks-on-Chip (NoCs) has become a common\npractice to sustain the performance of multi-core systems with an increasing\nnumber of faults on a chip. On the other hand, usage of third-party\nintellectual property blocks has made security a primary concern in modern day\ndesigns. This article presents a mechanism to mitigate a denial-of-service\nattack, namely packet drop attack, which may arise due to the hardware Trojans\n(HTs) in NoCs that adopt FTR algorithms. HTs, associated with external kill\nswitches, are conditionally triggered to enable the attack scenario. Security\nmodules, such as authentication unit, buffer shuffler, and control unit, have\nbeen proposed to thwart the attack in runtime and restore secure packet flow in\nthe NoC. These units work together as a shield to safeguard the packets from\nproceeding towards the output ports with faulty links. Synthesis results show\nthat the proposed secure FT router, when compared with a baseline FT router,\nhas area and power overheads of at most 4.04% and 0.90%, respectively.\nPerformance evaluation shows that SeFaR has acceptable overheads in the\nexecution time, energy consumption, average packet latency, and power-latency\nproduct metrics when compared with a baseline FT router while running real\nbenchmarks, as well as synthetic traffic. Further, a possible design of a\ncomprehensive secure router has been presented with a view to addressing and\nmitigating multiple attacks that can arise in the NoC routers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:30:46 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Prasad", "N", ""], ["Chatterjee", "Navonil", ""], ["Chattopadhyay", "Santanu", ""], ["Chakrabarti", "Indrajit", ""]]}, {"id": "1908.00314", "submitter": "Maksim Jenihhin", "authors": "Maksim Jenihhin, Xinhui Lai, Tara Ghasempouri, Jaan Raik", "title": "Towards Multidimensional Verification: Where Functional Meets\n  Non-Functional", "comments": "2018 IEEE Nordic Circuits and Systems Conference (NORCAS): NORCHIP\n  and International Symposium of System-on-Chip (SoC)", "journal-ref": null, "doi": "10.1109/NORCHIP.2018.8573495", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trends in advanced electronic systems' design have a notable impact on design\nverification technologies. The recent paradigms of Internet-of-Things (IoT) and\nCyber-Physical Systems (CPS) assume devices immersed in physical environments,\nsignificantly constrained in resources and expected to provide levels of\nsecurity, privacy, reliability, performance and low power features. In recent\nyears, numerous extra-functional aspects of electronic systems were brought to\nthe front and imply verification of hardware design models in multidimensional\nspace along with the functional concerns of the target system. However,\ndifferent from the software domain such a holistic approach remains\nunderdeveloped. The contributions of this paper are a taxonomy for\nmultidimensional hardware verification aspects, a state-of-the-art survey of\nrelated research works and trends towards the multidimensional verification\nconcept. The concept is motivated by an example for the functional and power\nverification dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 10:33:46 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Jenihhin", "Maksim", ""], ["Lai", "Xinhui", ""], ["Ghasempouri", "Tara", ""], ["Raik", "Jaan", ""]]}, {"id": "1908.01261", "submitter": "Seung Won Min", "authors": "Seung Won Min, Sitao Huang, Mohamed El-Hadedy, Jinjun Xiong, Deming\n  Chen, Wen-mei Hwu", "title": "Analysis and Optimization of I/O Cache Coherency Strategies for SoC-FPGA\n  Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional PCIe-based FPGA accelerators, heterogeneous SoC-FPGA\ndevices provide tighter integrations between software running on CPUs and\nhardware accelerators. Modern heterogeneous SoC-FPGA platforms support multiple\nI/O cache coherence options between CPUs and FPGAs, but these options can have\ninadvertent effects on the achieved bandwidths depending on applications and\ndata access patterns. To provide the most efficient communications between CPUs\nand accelerators, understanding the data transaction behaviors and selecting\nthe right I/O cache coherence method is essential. In this paper, we use Xilinx\nZynq UltraScale+ as the SoC platform to show how certain I/O cache coherence\nmethod can perform better or worse in different situations, ultimately\naffecting the overall accelerator performances as well. Based on our analysis,\nwe further explore possible software and hardware modifications to improve the\nI/O performances with different I/O cache coherence options. With our proposed\nmodifications, the overall performance of SoC design can be averagely improved\nby 20%.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 02:35:57 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Min", "Seung Won", ""], ["Huang", "Sitao", ""], ["El-Hadedy", "Mohamed", ""], ["Xiong", "Jinjun", ""], ["Chen", "Deming", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1908.01324", "submitter": "Thomas Melham", "authors": "Andreas Tiemeyer, Tom Melham, Daniel Kroening, John O'Leary", "title": "CREST: Hardware Formal Verification with ANSI-C Reference Specifications", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CREST, a prototype front-end tool intended as an add-on\nto commercial EDA formal verifcation environments. CREST is an adaptation of\nthe CBMC bounded model checker for C, an academic tool widely used in industry\nfor software analysis and property verification. It leverages the capabilities\nof CBMC to process hardware datapath specifications written in arbitrary\nANSI-C, without limiting restrictions to a synthesizable subset. We briefly\nsketch the architecture of our tool and show its use in a range of verification\ncase studies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 12:18:50 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tiemeyer", "Andreas", ""], ["Melham", "Tom", ""], ["Kroening", "Daniel", ""], ["O'Leary", "John", ""]]}, {"id": "1908.01466", "submitter": "Sugandha Tiwari", "authors": "Sugandha Tiwari, Neel Gala, Chester Rebeiro, V. Kamakoti", "title": "PERI: A Posit Enabled RISC-V Core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the failure of Dennard's scaling the last decade has seen a steep\ngrowth of prominent new paradigms leveraging opportunities in computer\narchitecture. Two technologies of interest are Posit and RISC-V. Posit was\nintroduced in mid-2017 as a viable alternative to IEEE 754-2008. Posit promises\nmore accuracy, higher dynamic range, and fewer unused states along with simpler\nhardware designs as compared to IEEE 754-2008. RISC-V, on the other hand,\nprovides a commercial-grade open-source ISA. It is not only elegant and simple\nbut also highly extensible and customizable, thereby facilitating novel\nmicro-architectural research and exploration. In this paper, we bring these two\ntechnologies together and propose the first Posit Enabled RISC-V core. The\npaper provides insights on how the current 'F' extension and the custom op-code\nspace of RISC-V can be leveraged/modified to support Posit arithmetic. We also\npresent implementation details of a parameterized and feature-complete Posit\nFPU which is integrated with the RISC-V compliant SHAKTI C-class core either as\nan execution unit or as an accelerator. To fully leverage the potential of\nPosit, we further enhance our Posit FPU, with minimal overheads, to support two\ndifferent exponent sizes (with posit-size being 32-bits). This allows\napplications to switch from high-accuracy computation mode to a mode with\nhigher dynamic-range at run-time. In the absence of viable software tool-chain\nto enable porting of applications in the Posit domain, we present a workaround\non how certain applications can be modified minimally to exploit the existing\nRISC-V tool-chain. We also provide examples of applications which can perform\nbetter with Posit as compared to IEEE 754-2008. The proposed Posit FPU consumes\n3507 slice LUTs and 1294 slice registers on an Artix-7-100T Xilinx FPGA while\ncapable of operating at 100 MHz.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 04:49:01 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tiwari", "Sugandha", ""], ["Gala", "Neel", ""], ["Rebeiro", "Chester", ""], ["Kamakoti", "V.", ""]]}, {"id": "1908.01806", "submitter": "Ravikiran Yeleswarapu", "authors": "Ravikiran Yeleswarapu and Arun K. Somani", "title": "Addressing multiple bit/symbol errors in DRAM subsystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As DRAM technology continues to evolve towards smaller feature sizes and\nincreased densities, faults in DRAM subsystem are becoming more severe. Current\nservers mostly use CHIPKILL based schemes to tolerate up-to one/two symbol\nerrors per DRAM beat. Multi-symbol errors arising due to faults in multiple\ndata buses and chips may not be detected by these schemes. In this paper, we\nintroduce Single Symbol Correction Multiple Symbol Detection (SSCMSD) - a novel\nerror handling scheme to correct single-symbol errors and detect multi-symbol\nerrors. Our scheme makes use of a hash in combination with Error Correcting\nCode (ECC) to avoid silent data corruptions (SDCs). SSCMSD can also enhance the\ncapability of detecting errors in address bits. We employ 32-bit CRC along with\nReed-Solomon code to implement SSCMSD for a x4 based DDRx system. Our\nsimulations show that the proposed scheme effectively prevents SDCs in the\npresence of multiple symbol errors. Our novel design enabled us to achieve this\nwithout introducing additional READ latency. Also, we need 19 chips per rank\n(storage overhead of 18.75 percent), 76 data bus-lines and additional\nhash-logic at the memory controller.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:01:58 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 18:38:44 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Yeleswarapu", "Ravikiran", ""], ["Somani", "Arun K.", ""]]}, {"id": "1908.02472", "submitter": "Dmitri Strukov B", "authors": "Mohammad Bavandpour, Shubham Sahay, Mohammad Reza Mahmoodi, Dmitri B.\n  Strukov", "title": "3D-aCortex: An Ultra-Compact Energy-Efficient Neurocomputing Platform\n  Based on Commercial 3D-NAND Flash Memories", "comments": "14 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first contribution of this paper is the development of extremely dense,\nenergy-efficient mixed-signal vector-by-matrix-multiplication (VMM) circuits\nbased on the existing 3D-NAND flash memory blocks, without any need for their\nmodification. Such compatibility is achieved using time-domain-encoded VMM\ndesign. Our detailed simulations have shown that, for example, the 5-bit VMM of\n200-element vectors, using the commercially available 64-layer gate-all-around\nmacaroni-type 3D-NAND memory blocks designed in the 55-nm technology node, may\nprovide an unprecedented area efficiency of 0.14 um2/byte and energy efficiency\nof ~10 fJ/Op, including the input/output and other peripheral circuitry\noverheads. Our second major contribution is the development of 3D-aCortex, a\nmulti-purpose neuromorphic inference processor that utilizes the proposed\n3D-VMM blocks as its core processing units. We have performed rigorous\nperformance simulations of such a processor on both circuit and system levels,\ntaking into account non-idealities such as drain-induced barrier lowering,\ncapacitive coupling, charge injection, parasitics, process variations, and\nnoise. Our modeling of the 3D-aCortex performing several state-of-the-art\nneuromorphic-network benchmarks has shown that it may provide the\nrecord-breaking storage efficiency of 4.34 MB/mm2, the peak energy efficiency\nof 70.43 TOps/J, and the computational throughput up to 10.66 TOps/s. The\nstorage efficiency can be further improved seven-fold by aggressively sharing\nVMM peripheral circuits at the cost of slight decrease in energy efficiency and\nthroughput.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 07:38:34 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Bavandpour", "Mohammad", ""], ["Sahay", "Shubham", ""], ["Mahmoodi", "Mohammad Reza", ""], ["Strukov", "Dmitri B.", ""]]}, {"id": "1908.02640", "submitter": "Ahsan Javed Awan Dr", "authors": "Gagandeep Singh, Lorenzo Chelini, Stefano Corda, Ahsan Javed Awan,\n  Sander Stuijk, Roel Jordans, Henk Corporaal and Albert-Jan Boonstra", "title": "Near-Memory Computing: Past, Present, and Future", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional approach of moving data to the CPU for computation has\nbecome a significant performance bottleneck for emerging scale-out\ndata-intensive applications due to their limited data reuse. At the same time,\nthe advancement in 3D integration technologies has made the decade-old concept\nof coupling compute units close to the memory --- called near-memory computing\n(NMC) --- more viable. Processing right at the \"home\" of data can significantly\ndiminish the data movement problem of data-intensive applications.\n  In this paper, we survey the prior art on NMC across various dimensions\n(architecture, applications, tools, etc.) and identify the key challenges and\nopen issues with future research directions. We also provide a glimpse of our\napproach to near-memory computing that includes i) NMC specific\nmicroarchitecture independent application characterization ii) a compiler\nframework to offload the NMC kernels on our target NMC platform and iii) an\nanalytical model to evaluate the potential of NMC.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:00:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Singh", "Gagandeep", ""], ["Chelini", "Lorenzo", ""], ["Corda", "Stefano", ""], ["Awan", "Ahsan Javed", ""], ["Stuijk", "Sander", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""], ["Boonstra", "Albert-Jan", ""]]}, {"id": "1908.02986", "submitter": "Maksim Jenihhin", "authors": "Adeboye Stephen Oyeniran, Raimund Ubar, Maksim Jenihhin, Cemil Cem\n  G\\\"ursoy, Jaan Raik", "title": "High-Level Combined Deterministic and Pseudoexhuastive Test Generation\n  for RISC Processors", "comments": "2019 IEEE European Test Symposium (ETS). arXiv admin note: text\n  overlap with arXiv:1907.12325", "journal-ref": null, "doi": "10.1109/ETS.2019.8791526", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent safety standards set stringent requirements for the target fault\ncoverage in embedded microprocessors, with the objective to guarantee\nrobustness and functional safety of the critical electronic systems. This\nmotivates the need for improving the quality of test generation for\nmicroprocessors. A new high-level implementation-independent test generation\nmethod for RISC processors is proposed. The set of instructions of the\nprocessor is partitioned nto groups. For each group, a dedicated test template\nis created, to be used for generating two test programs, for testing the\ncontrol and the data paths respectively. For testing the control part, a novel\nhigh-level control fault model is proposed. Using this model, a set of\ndeterministic test data operands are generated for each instruction of the\ngiven group. The advantage of the high-level fault model is that it covers\nlarger than SAF fault class including multiple fault coverage in the control\npart. For generating the data path test, pseudoexhaustive data operands are\nused. We investigated the feasibility of the approach and demonstrated high\nefficiency of the generated test programs for testing the execute module of the\nminiMIPS RISC processor.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 09:29:59 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Oyeniran", "Adeboye Stephen", ""], ["Ubar", "Raimund", ""], ["Jenihhin", "Maksim", ""], ["G\u00fcrsoy", "Cemil Cem", ""], ["Raik", "Jaan", ""]]}, {"id": "1908.03072", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "TensorDIMM: A Practical Near-Memory Processing Architecture for\n  Embeddings and Tensor Operations in Deep Learning", "comments": "Accepted for publication at the 52nd IEEE/ACM International Symposium\n  on Microarchitecture (MICRO-52), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies from several hyperscalars pinpoint to embedding layers as the\nmost memory-intensive deep learning (DL) algorithm being deployed in today's\ndatacenters. This paper addresses the memory capacity and bandwidth challenges\nof embedding layers and the associated tensor operations. We present our\nvertically integrated hardware/software co-design, which includes a custom DIMM\nmodule enhanced with near-data processing cores tailored for DL tensor\noperations. These custom DIMMs are populated inside a GPU-centric system\ninterconnect as a remote memory pool, allowing GPUs to utilize for scalable\nmemory bandwidth and capacity expansion. A prototype implementation of our\nproposal on real DL systems shows an average 6.2-17.6x performance improvement\non state-of-the-art recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 13:45:33 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 11:15:05 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1908.03664", "submitter": "Anish N K", "authors": "Samet E. Arda, Anish NK, A. Alper Goksoy, Joshua Mack, Nirmal\n  Kumbhare, Anderson L. Sartor, Ali Akoglu, Radu Marculescu and Umit Y. Ogras", "title": "Work-in-Progress: A Simulation Framework for Domain-Specific\n  System-on-Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous system-on-chips (SoCs) have become the standard embedded\ncomputing platforms due to their potential to deliver superior performance and\nenergy efficiency compared to homogeneous architectures. They can be\nparticularly suited to target a specific domain of applications. However, this\npotential is contingent upon optimizing the SoC for the target domain and\nutilizing its resources effectively at run-time. Cycle-accurate instruction set\nsimulators are not suitable for this optimization, since meaningful temperature\nand power consumption evaluations require simulating seconds, if not minutes,\nof workloads.\n  This paper presents a system-level domain-specific SoC simulation (DS3)\nframework to address this need. DS3 enables both design space exploration and\ndynamic resource management for power-performance optimization for domain\napplications with$~600\\times$ speedup compared to commonly used gem5 simulator.\nWe showcase DS3 using five applications from wireless communications and radar\nprocessing domain. DS3, as well as the reference applications, will be shared\nas open-source software to stimulate research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 01:54:08 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Arda", "Samet E.", ""], ["NK", "Anish", ""], ["Goksoy", "A. Alper", ""], ["Mack", "Joshua", ""], ["Kumbhare", "Nirmal", ""], ["Sartor", "Anderson L.", ""], ["Akoglu", "Ali", ""], ["Marculescu", "Radu", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "1908.04484", "submitter": "C. H. Huck Yang", "authors": "Sheng-Chun Kao, Chao-Han Huck Yang, Pin-Yu Chen, Xiaoli Ma, Tushar\n  Krishna", "title": "Reinforcement Learning based Interconnection Routing for Adaptive\n  Traffic Optimization", "comments": null, "journal-ref": null, "doi": "10.1145/3313231.335236", "report-no": null, "categories": "cs.NI cs.AI cs.AR cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying Machine Learning (ML) techniques to design and optimize computer\narchitectures is a promising research direction. Optimizing the runtime\nperformance of a Network-on-Chip (NoC) necessitates a continuous learning\nframework. In this work, we demonstrate the promise of applying reinforcement\nlearning (RL) to optimize NoC runtime performance. We present three RL-based\nmethods for learning optimal routing algorithms. The experimental results show\nthe algorithms can successfully learn a near-optimal solution across different\nenvironment states. Reproducible Code:\ngithub.com/huckiyang/interconnect-routing-gym\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 04:35:40 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Kao", "Sheng-Chun", ""], ["Yang", "Chao-Han Huck", ""], ["Chen", "Pin-Yu", ""], ["Ma", "Xiaoli", ""], ["Krishna", "Tushar", ""]]}, {"id": "1908.06362", "submitter": "Benjamin Cho", "authors": "Benjamin Y. Cho, Yongkee Kwon, Sangkug Lym, Mattan Erez", "title": "Near Data Acceleration with Concurrent Host Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-data accelerators (NDAs) that are integrated with main memory have the\npotential for significant power and performance benefits. Fully realizing these\nbenefits requires the large available memory capacity to be shared between the\nhost and the NDAs in a way that permits both regular memory access by some\napplications and accelerating others with an NDA, avoids copying data, enables\ncollaborative processing, and simultaneously offers high performance for both\nhost and NDA. We identify and solve new challenges in this context: mitigating\nrow-locality interference from host to NDAs, reducing read/write-turnaround\noverhead caused by fine-grain interleaving of host and NDA requests,\narchitecting a memory layout that supports the locality required for NDAs and\nsophisticated address interleaving for host performance, and supporting both\npacketized and traditional memory interfaces. We demonstrate our approach in a\nsimulated system that consists of a multi-core CPU and NDA-enabled DDR4 memory\nmodules. We show that our mechanisms enable effective and efficient concurrent\naccess using a set of microbenchmarks, and then demonstrate the potential of\nthe system for the important stochastic variance-reduced gradient (SVRG)\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 01:36:35 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 23:27:51 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cho", "Benjamin Y.", ""], ["Kwon", "Yongkee", ""], ["Lym", "Sangkug", ""], ["Erez", "Mattan", ""]]}, {"id": "1908.06519", "submitter": "Sahand Salamat", "authors": "Sahand Salamat, Behnam Khaleghi, Mohsen Imani, Tajana Rosing", "title": "Workload-Aware Opportunistic Energy Efficiency in Multi-FPGA Platforms", "comments": "The paper will be published in ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous growth of big data applications with high computational and\nscalability demands has resulted in increasing popularity of cloud computing.\nOptimizing the performance and power consumption of cloud resources is\ntherefore crucial to relieve the costs of data centers. In recent years,\nmulti-FPGA platforms have gained traction in data centers as low-cost yet\nhigh-performance solutions particularly as acceleration engines, thanks to the\nhigh degree of parallelism they provide. Nonetheless, the size of data centers\nworkloads varies during service time, leading to significant underutilization\nof computing resources while consuming a large amount of power, which turns out\nas a key factor of data center inefficiency, regardless of the underlying\nhardware structure. In this paper, we propose an efficient framework to\nthrottle the power consumption of multi-FPGA platforms by dynamically scaling\nthe voltage and hereby frequency during runtime according to prediction of, and\nadjustment to the workload level, while maintaining the desired Quality of\nService (QoS). This is in contrast to, and more efficient than, conventional\napproaches that merely scale (i.e., power-gate) the computing nodes or\nfrequency. The proposed framework carefully exploits a pre-characterized\nlibrary of delay-voltage, and power-voltage information of FPGA resources,\nwhich we show is indispensable to obtain the efficient operating point due to\nthe different sensitivity of resources w.r.t. voltage scaling, particularly\nconsidering multiple power rails residing in these devices. Our evaluations by\nimplementing state-of-the-art deep neural network accelerators revealed that,\nproviding an average power reduction of 4.0X, the proposed framework surpasses\nthe previous works by 33.6% (up to 83%).\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 21:44:37 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 00:00:22 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Salamat", "Sahand", ""], ["Khaleghi", "Behnam", ""], ["Imani", "Mohsen", ""], ["Rosing", "Tajana", ""]]}, {"id": "1908.06649", "submitter": "Francesco Silvestri", "authors": "Rezaul Chowdhury and Francesco Silvestri and Flavio Vella", "title": "A Computational Model for Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To respond to the need of efficient training and inference of deep neural\nnetworks, a plethora of domain-specific hardware architectures have been\nintroduced, such as Google Tensor Processing Units and NVIDIA Tensor Cores. A\ncommon feature of these architectures is a hardware circuit for efficiently\ncomputing a dense matrix multiplication of a given small size. In order to\nbroaden the class of algorithms that exploit these systems, we propose a\ncomputational model, named the TCU model, that captures the ability to natively\nmultiply small matrices. We then use the TCU model for designing fast\nalgorithms for several problems, including matrix operations (dense and sparse\nmultiplication, Gaussian Elimination), graph algorithms (transitive closure,\nall pairs shortest distances), Discrete Fourier Transform, stencil\ncomputations, integer multiplication, and polynomial evaluation. We finally\nhighlight a relation between the TCU model and the external memory model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 08:59:46 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 07:25:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chowdhury", "Rezaul", ""], ["Silvestri", "Francesco", ""], ["Vella", "Flavio", ""]]}, {"id": "1908.06757", "submitter": "arXiv Admin", "authors": "Karthik Ganesan, Srinivasa Shashank Nuthakki", "title": "Boosting the Bounds of Symbolic QED for Effective Pre-Silicon\n  Verification of Processor Cores", "comments": "arXiv admin note: withdrawn by arXiv administrators due to incomplete\n  author list", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to ensure functional correctness and hardware trust\nduring pre-silicon verification face severe limitations. In this work, we\nsystematically leverage two key ideas: 1) Symbolic Quick Error Detection\n(Symbolic QED or SQED), a recent bug detection and localization technique using\nBounded Model Checking (BMC); and 2) Symbolic starting states, to present a\nmethod that: i) Effectively detects both \"difficult\" logic bugs and Hardware\nTrojans, even with long activation sequences where traditional BMC techniques\nfail; and ii) Does not need skilled manual guidance for writing testbenches,\nwriting design-specific assertions, or debugging spurious counter-examples.\nUsing open-source RISC-V cores, we demonstrate the following: 1. Quick (<5\nminutes for an in-order scalar core and <2.5 hours for an out-of-order\nsuperscalar core) detection of 100% of hundreds of logic bug and hardware\nTrojan scenarios from commercial chips and research literature, and 97.9% of\n\"extremal\" bugs (randomly-generated bugs requiring ~100,000 activation\ninstructions taken from random test programs). 2. Quick (~1 minute) detection\nof several previously unknown bugs in open-source RISC-V designs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:26:58 GMT"}, {"version": "v2", "created": "Sun, 20 Oct 2019 00:02:36 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 17:01:55 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 14:22:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ganesan", "Karthik", ""], ["Nuthakki", "Srinivasa Shashank", ""]]}, {"id": "1908.06841", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Ternary circuits: why R=3 is not the Optimal Radix for Computation", "comments": "9 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A demonstration that e=2.718 rounded to 3 is the best radix for computation\nis disproved. The MOSFET-like CNTFET technology is used to compare inverters,\nNand, adders, multipliers, D Flip-Flops and SRAM cells. The transistor count\nratio between ternary and binary circuits is generally greater than the\nlog(3)/log(2) information ratio. The only exceptions concern a circuit approach\nthat combines two circuit drawbacks (an additional power supply and a circuit\nconflict between transistors) and only when it implements circuits based on the\nternary inverter. For arithmetic circuits such as adders and multipliers, the\nternary circuits are always outperformed by the binary ones using the same\ntechnology.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:50:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "1908.06869", "submitter": "Cheng Li", "authors": "Cheng Li, Abdul Dakkak, Jinjun Xiong, Wei Wei, Lingjie Xu, Wen-mei Hwu", "title": "XSP: Across-Stack Profiling and Analysis of Machine Learning Models on\n  GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00042", "report-no": null, "categories": "cs.LG cs.AR cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a rapid proliferation of machine learning/deep learning (ML)\nmodels and wide adoption of them in many application domains. This has made\nprofiling and characterization of ML model performance an increasingly pressing\ntask for both hardware designers and system providers, as they would like to\noffer the best possible system to serve ML models with the target latency,\nthroughput, cost, and energy requirements while maximizing resource\nutilization. Such an endeavor is challenging as the characteristics of an ML\nmodel depend on the interplay between the model, framework, system libraries,\nand the hardware (or the HW/SW stack). Existing profiling tools are disjoint,\nhowever, and only focus on profiling within a particular level of the stack,\nwhich limits the thoroughness and usefulness of the profiling results.\n  This paper proposes XSP - an across-stack profiling design that gives a\nholistic and hierarchical view of ML model execution. XSP leverages distributed\ntracing to aggregate and correlates profile data from different sources. XSP\nintroduces a leveled and iterative measurement approach that accurately\ncaptures the latencies at all levels of the HW/SW stack in spite of the\nprofiling overhead. We couple the profiling design with an automated analysis\npipeline to systematically analyze 65 state-of-the-art ML models. We\ndemonstrate that XSP provides insights which would be difficult to discern\notherwise.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:05:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:42:42 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 01:31:35 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Li", "Cheng", ""], ["Dakkak", "Abdul", ""], ["Xiong", "Jinjun", ""], ["Wei", "Wei", ""], ["Xu", "Lingjie", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "1908.07299", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Comparing ternary and binary adders and multipliers", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many papers have proposed implementations of ternary adders and ternary\nmultipliers, no comparisons have generally been done with the corresponding\nbinary ones. We compare the implementations of binary and ternary adders and\nmultipliers with the same computing capability according to the basic blocks\nthat are 1-bit and 1-trit adders and 1-bit and 1-trit multipliers. Then we\ncompare the complexity of these basic blocks by using the same CNTFET\ntechnology to evaluate the overall complexity of N-bit adders and M-trit adders\non one side, and NxN bit multipliers and MxM trits multipliers with M = N/IR\n(IR = log(3)/log(2) is the information ratio). While ternary adders and\nmultipliers have less input and output connections and use less basic building\nblocks, the complexity of the ternary building blocks is too high and the\nternary adders and multipliers cannot compete with the binary ones.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:27:06 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "1908.07413", "submitter": "Ning Qiao", "authors": "Ning Qiao and Giacomo Indiveri", "title": "A bi-directional Address-Event transceiver block for low-latency\n  inter-chip communication in neuromorphic systems", "comments": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic systems typically use the Address-Event Representation (AER) to\ntransmit signals among nodes, cores, and chips. Communication of Address-Events\n(AEs) between neuromorphic cores/chips typically requires two parallel digital\nsignal buses for Input/Output (I/O) operations. This requirement can become\nvery expensive for large-scale systems in terms of both dedicated I/O pins and\npower consumption. In this paper we present a compact fully asynchronous\nevent-driven transmitter/receiver block that is both power efficient and I/O\nefficient. This block implements high-throughput low-latency bi-directional\ncommunication through a parallel AER bus. We show that by placing the proposed\nAE transceiver block in two separate chips and linking them by a single AER\nbus, we can drive the communication and switch the transmission direction of\nthe shared bus on a single event basis, from either side with low-latency. We\npresent experimental results that validate the circuits proposed and\ndemonstrate reliable bi-directional event transmission with high-throughput.\nThe proposed AE block, integrated in a neuromorphic chip fabricated using a 28\nnm FDSOI process, occupies a silicon die area of 140 {\\mu}m x 70 {\\mu}m. The\nexperimental measurements show that the event-driven AE block combined with\nstandard digital I/Os has a direction switch latency of 5 ns and can achieve a\nworst-case bi-directional event transmission throughput of 28.6M Events/second\nwhile consuming 11 pJ per event (26-bit) delivery.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 22:43:19 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Qiao", "Ning", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1908.07966", "submitter": "Anup Das", "authors": "Shihao Song and Anup Das and Onur Mutlu and Nagarajan Kandasamy", "title": "Enabling and Exploiting Partition-Level Parallelism (PALP) in Phase\n  Change Memories", "comments": "13 pages, 16 figures, 71 references. Published at ACM CASES 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phase-change memory (PCM) devices have multiple banks to serve memory\nrequests in parallel. Unfortunately, if two requests go to the same bank, they\nhave to be served one after another, leading to lower system performance. We\nobserve that a modern PCM bank is implemented as a collection of partitions\nthat operate mostly independently while sharing a few global peripheral\nstructures, which include the sense amplifiers (to read) and the write drivers\n(to write). Based on this observation, we propose PALP, a new mechanism that\nenables partition-level parallelism within each PCM bank, and exploits such\nparallelism by using the memory controller's access scheduling decisions. PALP\nconsists of three new contributions. First, we introduce new PCM commands to\nenable parallelism in a bank's partitions in order to resolve the read-write\nbank conflicts, with minimal changes needed to PCM logic and its interface.\nSecond, we propose simple circuit modifications that introduce a new operating\nmode for the write drivers, in addition to their default mode of serving write\nrequests. When configured in this new mode, the write drivers can resolve the\nread-read bank conflicts, working jointly with the sense amplifiers. Finally,\nwe propose a new access scheduling mechanism in PCM that improves performance\nby prioritizing those requests that exploit partition-level parallelism over\nother requests, including the long outstanding ones. While doing so, the memory\ncontroller also guarantees starvation-freedom and the PCM's\nrunning-average-power-limit (RAPL). We evaluate PALP with workloads from the\nMiBench and SPEC CPU2017 Benchmark suites. Our results show that PALP reduces\naverage PCM access latency by 23%, and improves average system performance by\n28% compared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:19:25 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""], ["Mutlu", "Onur", ""], ["Kandasamy", "Nagarajan", ""]]}, {"id": "1908.09297", "submitter": "Projjal Gupta", "authors": "Projjal Gupta", "title": "4-Bit High-Speed Binary Ling Adder", "comments": "4 pages, 5 Figures", "journal-ref": null, "doi": "10.13140/RG.2.2.15095.68002", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary addition is one of the most primitive and most commonly used\napplications in computer arithmetic. A large variety of algorithms and\nimplementations have been proposed for binary addition. Huey Ling proposed a\nsimpler form of CLA equations which rely on adjacent pair bits. Along with bit\ngenerate and bit propagate, we introduce another prefix bit, the half sum bit.\nLing adder increases the speed of n-bit binary addition, which is an upgrade\nfrom the existing Carry-Look-Ahead adder. Several variants of the carry\nlook-ahead equations, like Ling carries, have been presented that simplify\ncarry computation and can lead to faster structures. Ling adders, make use of\nLing carry and propagate bits, in order to calculate the sum bit. As a result,\ndependency on the previous bit addition is reduced; that is, ripple effect is\nlowered. This paper provides a comparative study on the implementation of the\nabove mentioned high-speed adders.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 10:37:29 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Gupta", "Projjal", ""]]}, {"id": "1908.09922", "submitter": "Rajat Kateja", "authors": "Rajat Kateja, Nathan Beckmann, Gregory R. Ganger", "title": "Tvarak: Software-managed hardware offload for DAX NVM storage redundancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tvarak efficiently implements system-level redundancy for direct-access (DAX)\nNVM storage. Production storage systems complement device-level ECC (which\ncovers media errors) with system-checksums and cross-device parity. This\nsystem-level redundancy enables detection of and recovery from data corruption\ndue to device firmware bugs (e.g., reading data from the wrong physical\nlocation). Direct access to NVM penalizes software-only implementations of\nsystem-level redundancy, forcing a choice between lack of data protection or\nsignificant performance penalties. Offloading the update and verification of\nsystem-level redundancy to Tvarak, a hardware controller co-located with the\nlast-level cache, enables efficient protection of data from such bugs in memory\ncontroller and NVM DIMM firmware. Simulation-based evaluation with seven\ndata-intensive applications shows Tvarak's performance and energy efficiency.\nFor example, Tvarak reduces Redis set-only performance by only 3%, compared to\n50% reduction for a state-of-the-art software-only approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 21:04:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kateja", "Rajat", ""], ["Beckmann", "Nathan", ""], ["Ganger", "Gregory R.", ""]]}, {"id": "1908.09930", "submitter": "Timothy Molteno", "authors": "P. A. Suggate, R. W. Ward, T. C. A. Molteno", "title": "Cyclic Sequence Generators as Program Counters for High-Speed FPGA-based\n  Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper compares the performance of conventional radix-2 program counters\nwith program counters based on Feedback Shift Registers (FSRs), a class of\ncyclic sequence generator. FSR counters have constant time scaling with\nbit-width, $N$, whereas FPGA-based radix-2 counters typically have $O(N)$\ntime-complexity due to the carry-chain. Program counter performance is measured\nby synthesis of standalone counter circuits, as well as synthesis of three\nFPGA-based processor designs modified to incorporate FSR program counters.\nHybrid counters, combining both an FSR and a radix-2 counter, are presented as\na solution to the potential cache-coherency issues of FSR program counters.\nResults show that high-speed processor designs benefit more from FSR program\ncounters, allowing both greater operating frequency and the use of fewer logic\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 21:30:44 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Suggate", "P. A.", ""], ["Ward", "R. W.", ""], ["Molteno", "T. C. A.", ""]]}, {"id": "1908.09992", "submitter": "Michel Kinsy", "authors": "Sahan Bandara, Alan Ehret, Donato Kava and Michel A. Kinsy", "title": "BRISC-V: An Open-Source Architecture Design Space Exploration Toolbox", "comments": "In Proceedings of the 2019 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays (FPGA '19)", "journal-ref": null, "doi": "10.1145/3289602.3293991", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a platform for register-transfer level (RTL)\narchitecture design space exploration. The platform is an open-source,\nparameterized, synthesizable set of RTL modules for designing RISC-V based\nsingle and multi-core architecture systems. The platform is designed with a\nhigh degree of modularity. It provides highly-parameterized, composable RTL\nmodules for fast and accurate exploration of different RISC-V based core\ncomplexities, multi-level caching and memory organizations, system topologies,\nrouter architectures, and routing schemes. The platform can be used for both\nRTL simulation and FPGA based emulation. The hardware modules are implemented\nin synthesizable Verilog using no vendor-specific blocks. The platform includes\na RISC-V compiler toolchain to assist in developing software for the cores, a\nweb-based system configuration graphical user interface (GUI) and a web-based\nRISC-V assembly simulator. The platform supports a myriad of RISC-V\narchitectures, ranging from a simple single cycle processor to a multi-core SoC\nwith a complex memory hierarchy and a network-on-chip. The modules are designed\nto support incremental additions and modifications. The interfaces between\ncomponents are particularly designed to allow parts of the processor such as\nwhole cache modules, cores or individual pipeline stages, to be modified or\nreplaced without impacting the rest of the system. The platform allows\nresearchers to quickly instantiate complete working RISC-V multi-core systems\nwith synthesizable RTL and make targeted modifications to fit their needs. The\ncomplete platform (including Verilog source code) can be downloaded at\nhttps://ascslab.org/research/briscv/explorer/explorer.html.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:35:09 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Bandara", "Sahan", ""], ["Ehret", "Alan", ""], ["Kava", "Donato", ""], ["Kinsy", "Michel A.", ""]]}, {"id": "1908.10017", "submitter": "Xiaolong Ma", "authors": "Xiaolong Ma, Geng Yuan, Sheng Lin, Caiwen Ding, Fuxun Yu, Tao Liu,\n  Wujie Wen, Xiang Chen, Yanzhi Wang", "title": "Tiny but Accurate: A Pruned, Quantized and Optimized Memristor Crossbar\n  Framework for Ultra Efficient DNN Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-art DNN structures involve intensive computation and high memory\nstorage. To mitigate the challenges, the memristor crossbar array has emerged\nas an intrinsically suitable matrix computation and low-power acceleration\nframework for DNN applications. However, the high accuracy solution for extreme\nmodel compression on memristor crossbar array architecture is still waiting for\nunraveling. In this paper, we propose a memristor-based DNN framework which\ncombines both structured weight pruning and quantization by incorporating\nalternating direction method of multipliers (ADMM) algorithm for better pruning\nand quantization performance. We also discover the non-optimality of the ADMM\nsolution in weight pruning and the unused data path in a structured pruned\nmodel. Motivated by these discoveries, we design a software-hardware\nco-optimization framework which contains the first proposed Network\nPurification and Unused Path Removal algorithms targeting on post-processing a\nstructured pruned model after ADMM steps. By taking memristor hardware\nconstraints into our whole framework, we achieve extreme high compression ratio\non the state-of-art neural network structures with minimum accuracy loss. For\nquantizing structured pruned model, our framework achieves nearly no accuracy\nloss after quantizing weights to 8-bit memristor weight representation. We\nshare our models at anonymous link https://bit.ly/2VnMUy0.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 04:19:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ma", "Xiaolong", ""], ["Yuan", "Geng", ""], ["Lin", "Sheng", ""], ["Ding", "Caiwen", ""], ["Yu", "Fuxun", ""], ["Liu", "Tao", ""], ["Wen", "Wujie", ""], ["Chen", "Xiang", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1908.11373", "submitter": "Salonik Resch", "authors": "Salonik Resch, S. Karen Khatamifard, Zamshed Iqbal Chowdhury, Masoud\n  Zabihi, Zhengyang Zhao, Jian-Ping Wang, Sachin S. Sapatnekar, Ulya R.\n  Karpuzcu", "title": "A Machine Learning Accelerator In-Memory for Energy Harvesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing demand to bring machine learning capabilities to low\npower devices. By integrating the computational power of machine learning with\nthe deployment capabilities of low power devices, a number of new applications\nbecome possible. In some applications, such devices will not even have a\nbattery, and must rely solely on energy harvesting techniques. This puts\nextreme constraints on the hardware, which must be energy efficient and capable\nof tolerating interruptions due to power outages. Here, as a representative\nexample, we propose an in-memory support vector machine learning accelerator\nutilizing non-volatile spintronic memory. The combination of\nprocessing-in-memory and non-volatility provides a key advantage in that\nprogress is effectively saved after every operation. This enables instant shut\ndown and restart capabilities with minimal overhead. Additionally, the\noperations are highly energy efficient leading to low power consumption.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 02:32:05 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Resch", "Salonik", ""], ["Khatamifard", "S. Karen", ""], ["Chowdhury", "Zamshed Iqbal", ""], ["Zabihi", "Masoud", ""], ["Zhao", "Zhengyang", ""], ["Wang", "Jian-Ping", ""], ["Sapatnekar", "Sachin S.", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "1908.11645", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Georg Rutishauser, Luca Benini", "title": "EBPC: Extended Bit-Plane Compression for Deep Neural Network Inference\n  and Training Accelerators", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.03979", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wake of the success of convolutional neural networks in image\nclassification, object recognition, speech recognition, etc., the demand for\ndeploying these compute-intensive ML models on embedded and mobile systems with\ntight power and energy constraints at low cost, as well as for boosting\nthroughput in data centers, is growing rapidly. This has sparked a surge of\nresearch into specialized hardware accelerators. Their performance is typically\nlimited by I/O bandwidth, power consumption is dominated by I/O transfers to\noff-chip memory, and on-chip memories occupy a large part of the silicon area.\nWe introduce and evaluate a novel, hardware-friendly, and lossless compression\nscheme for the feature maps present within convolutional neural networks. We\npresent hardware architectures and synthesis results for the compressor and\ndecompressor in 65nm. With a throughput of one 8-bit word/cycle at 600MHz, they\nfit into 2.8kGE and 3.0kGE of silicon area, respectively - together the size of\nless than seven 8-bit multiply-add units at the same throughput. We show that\nan average compression ratio of 5.1x for AlexNet, 4x for VGG-16, 2.4x for\nResNet-34 and 2.2x for MobileNetV2 can be achieved - a gain of 45-70% over\nexisting methods. Our approach also works effectively for various number\nformats, has a low frame-to-frame variance on the compression ratio, and\nachieves compression factors for gradient map compression during training that\nare even better than for inference.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 10:47:31 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 14:47:32 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Rutishauser", "Georg", ""], ["Benini", "Luca", ""]]}, {"id": "1908.11691", "submitter": "Geng Yuan", "authors": "Geng Yuan, Xiaolong Ma, Caiwen Ding, Sheng Lin, Tianyun Zhang, Zeinab\n  S. Jalali, Yilong Zhao, Li Jiang, Sucheta Soundarajan, Yanzhi Wang", "title": "An Ultra-Efficient Memristor-Based DNN Framework with Structured Weight\n  Pruning and Quantization Using ADMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high computation and memory storage of large deep neural networks (DNNs)\nmodels pose intensive challenges to the conventional Von-Neumann architecture,\nincurring substantial data movements in the memory hierarchy. The memristor\ncrossbar array has emerged as a promising solution to mitigate the challenges\nand enable low-power acceleration of DNNs. Memristor-based weight pruning and\nweight quantization have been seperately investigated and proven effectiveness\nin reducing area and power consumption compared to the original DNN model.\nHowever, there has been no systematic investigation of memristor-based\nneuromorphic computing (NC) systems considering both weight pruning and weight\nquantization. In this paper, we propose an unified and systematic\nmemristor-based framework considering both structured weight pruning and weight\nquantization by incorporating alternating direction method of multipliers\n(ADMM) into DNNs training. We consider hardware constraints such as crossbar\nblocks pruning, conductance range, and mismatch between weight value and real\ndevices, to achieve high accuracy and low power and small area footprint. Our\nframework is mainly integrated by three steps, i.e., memristor-based ADMM\nregularized optimization, masked mapping and retraining. Experimental results\nshow that our proposed framework achieves 29.81X (20.88X) weight compression\nratio, with 98.38% (96.96%) and 98.29% (97.47%) power and area reduction on\nVGG-16 (ResNet-18) network where only have 0.5% (0.76%) accuracy loss, compared\nto the original DNN models. We share our models at link http://bit.ly/2Jp5LHJ.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 03:32:41 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Ding", "Caiwen", ""], ["Lin", "Sheng", ""], ["Zhang", "Tianyun", ""], ["Jalali", "Zeinab S.", ""], ["Zhao", "Yilong", ""], ["Jiang", "Li", ""], ["Soundarajan", "Sucheta", ""], ["Wang", "Yanzhi", ""]]}]