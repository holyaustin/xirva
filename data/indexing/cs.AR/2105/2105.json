[{"id": "2105.00334", "submitter": "Hanieh Hashemi", "authors": "Hanieh Hashemi, Yongqin Wang, Murali Annavaram", "title": "Privacy and Integrity Preserving Training Using Trusted Hardware", "comments": null, "journal-ref": "Distributed and Private Machine Learning ICLR 2021 Workshop", "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy and security-related concerns are growing as machine learning reaches\ndiverse application domains. The data holders want to train with private data\nwhile exploiting accelerators, such as GPUs, that are hosted in the cloud.\nHowever, Cloud systems are vulnerable to attackers that compromise the privacy\nof data and integrity of computations. This work presents DarKnight, a\nframework for large DNN training while protecting input privacy and computation\nintegrity. DarKnight relies on cooperative execution between trusted execution\nenvironments (TEE) and accelerators, where the TEE provides privacy and\nintegrity verification, while accelerators perform the computation heavy linear\nalgebraic operations.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 19:33:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hashemi", "Hanieh", ""], ["Wang", "Yongqin", ""], ["Annavaram", "Murali", ""]]}, {"id": "2105.00378", "submitter": "Deeksha Dangwal", "authors": "Deeksha Dangwal, Meghan Cowan, Armin Alaghi, Vincent T. Lee, Brandon\n  Reagen, Caroline Trippel", "title": "SoK: Opportunities for Software-Hardware-Security Codesign for Next\n  Generation Secure Computing", "comments": "9 pages", "journal-ref": null, "doi": "10.1145/3458903.345891", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users are demanding increased data security. As a result, security is rapidly\nbecoming a first-order design constraint in next generation computing systems.\nResearchers and practitioners are exploring various security technologies to\nmeet user demand such as trusted execution environments (e.g., Intel SGX, ARM\nTrustZone), homomorphic encryption, and differential privacy. Each technique\nprovides some degree of security, but differs with respect to threat coverage,\nperformance overheads, as well as implementation and deployment challenges. In\nthis paper, we present a systemization of knowledge (SoK) on these design\nconsiderations and trade-offs using several prominent security technologies.\nOur study exposes the need for \\textit{software-hardware-security} codesign to\nrealize efficient and effective solutions of securing user data. In particular,\nwe explore how design considerations across applications, hardware, and\nsecurity mechanisms must be combined to overcome fundamental limitations in\ncurrent technologies so that we can minimize performance overhead while\nachieving sufficient threat model coverage. Finally, we propose a set of\nguidelines to facilitate putting these secure computing technologies into\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 02:19:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dangwal", "Deeksha", ""], ["Cowan", "Meghan", ""], ["Alaghi", "Armin", ""], ["Lee", "Vincent T.", ""], ["Reagen", "Brandon", ""], ["Trippel", "Caroline", ""]]}, {"id": "2105.00619", "submitter": "Salman Ahmed", "authors": "Salman Ahmed, Hammad Naveed", "title": "OpTorch: Optimized deep learning architectures for resource limited\n  environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning algorithms have made many breakthroughs and have various\napplications in real life. Computational resources become a bottleneck as the\ndata and complexity of the deep learning pipeline increases. In this paper, we\npropose optimized deep learning pipelines in multiple aspects of training\nincluding time and memory. OpTorch is a machine learning library designed to\novercome weaknesses in existing implementations of neural network training.\nOpTorch provides features to train complex neural networks with limited\ncomputational resources. OpTorch achieved the same accuracy as existing\nlibraries on Cifar-10 and Cifar-100 datasets while reducing memory usage to\napproximately 50%. We also explore the effect of weights on total memory usage\nin deep learning pipelines. In our experiments, parallel encoding-decoding\nalong with sequential checkpoints results in much improved memory and time\nusage while keeping the accuracy similar to existing pipelines. OpTorch python\npackage is available at available at https://github.com/cbrl-nuces/optorch\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 03:58:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 09:25:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ahmed", "Salman", ""], ["Naveed", "Hammad", ""]]}, {"id": "2105.00789", "submitter": "Heiner Bauer", "authors": "Heiner Bauer, Sebastian H\\\"oppner, Chris Iatrou, Zohra Charania,\n  Stephan Hartmann, Saif-Ur Rehman, Andreas Dixius, Georg Ellguth, Dennis\n  Walter, Johannes Uhlig, Felix Neum\\\"arker, Marc Berthel, Marco Stolba,\n  Florian Kelber, Leon Urbas, Christian Mayr", "title": "Hardware Implementation of an OPC UA Server for Industrial Field Devices", "comments": "5 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial plants suffer from a high degree of complexity and incompatibility\nin their communication infrastructure, caused by a wild mix of proprietary\ntechnologies. This prevents transformation towards Industry 4.0 and the\nIndustrial Internet of Things. Open Platform Communications Unified\nArchitecture (OPC UA) is a standardized protocol that addresses these problems\nwith uniform and semantic communication across all levels of the hierarchy.\nHowever, its adoption in embedded field devices, such as sensors and actors, is\nstill lacking due to prohibitive memory and power requirements of software\nimplementations. We have developed a dedicated hardware engine that offloads\nprocessing of the OPC UA protocol and enables realization of compact and\nlow-power field devices with OPC UA support. As part of a proof-of-concept\nembedded system we have implemented this engine in a 22 nm FDSOI technology. We\nmeasured performance, power consumption, and memory footprint of our test chip\nand compared it with a software implementation based on open62541 and a\nRaspberry Pi 2B. Our OPC UA hardware engine is 50 times more energy efficient\nand only requires 36 KiB of memory. The complete chip consumes only 24 mW under\nfull load, making it suitable for low-power embedded applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 12:43:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bauer", "Heiner", ""], ["H\u00f6ppner", "Sebastian", ""], ["Iatrou", "Chris", ""], ["Charania", "Zohra", ""], ["Hartmann", "Stephan", ""], ["Rehman", "Saif-Ur", ""], ["Dixius", "Andreas", ""], ["Ellguth", "Georg", ""], ["Walter", "Dennis", ""], ["Uhlig", "Johannes", ""], ["Neum\u00e4rker", "Felix", ""], ["Berthel", "Marc", ""], ["Stolba", "Marco", ""], ["Kelber", "Florian", ""], ["Urbas", "Leon", ""], ["Mayr", "Christian", ""]]}, {"id": "2105.01280", "submitter": "Feng Shi", "authors": "Feng Shi, Ahren Yiqiao Jin, Song-Chun Zhu", "title": "VersaGNN: a Versatile accelerator for Graph neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Graph Neural Network} (GNN) is a promising approach for analyzing\ngraph-structured data that tactfully captures their dependency information via\nnode-level message passing. It has achieved state-of-the-art performances in\nmany tasks, such as node classification, graph matching, clustering, and graph\ngeneration. As GNNs operate on non-Euclidean data, their irregular data access\npatterns cause considerable computational costs and overhead on conventional\narchitectures, such as GPU and CPU. Our analysis shows that GNN adopts a hybrid\ncomputing model. The \\textit{Aggregation} (or \\textit{Message Passing}) phase\nperforms vector additions where vectors are fetched with irregular strides. The\n\\textit{Transformation} (or \\textit{Node Embedding}) phase can be either dense\nor sparse-dense matrix multiplication. In this work, We propose\n\\textit{VersaGNN}, an ultra-efficient, systolic-array-based versatile hardware\naccelerator that unifies dense and sparse matrix multiplication. By applying\nthis single optimized systolic array to both aggregation and transformation\nphases, we have significantly reduced chip sizes and energy consumption. We\nthen divide the computing engine into blocked systolic arrays to support the\n\\textit{Strassen}'s algorithm for dense matrix multiplication, dramatically\nscaling down the number of multiplications and enabling high-throughput\ncomputation of GNNs. To balance the workload of sparse-dense matrix\nmultiplication, we also introduced a greedy algorithm to combine sparse\nsub-matrices of compressed format into condensed ones to reduce computational\ncycles. Compared with current state-of-the-art GNN software frameworks,\n\\textit{VersaGNN} achieves on average 3712$\\times$ speedup with 1301.25$\\times$\nenergy reduction on CPU, and 35.4$\\times$ speedup with 17.66$\\times$ energy\nreduction on GPU.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 04:10:48 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shi", "Feng", ""], ["Jin", "Ahren Yiqiao", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2105.01585", "submitter": "Qingcheng Xiao", "authors": "Qingcheng Xiao, Size Zheng, Bingzhe Wu, Pengcheng Xu, Xuehai Qian, Yun\n  Liang", "title": "HASCO: Towards Agile HArdware and Software CO-design for Tensor\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor computations overwhelm traditional general-purpose computing devices\ndue to the large amounts of data and operations of the computations. They call\nfor a holistic solution composed of both hardware acceleration and software\nmapping. Hardware/software (HW/SW) co-design optimizes the hardware and\nsoftware in concert and produces high-quality solutions. There are two main\nchallenges in the co-design flow. First, multiple methods exist to partition\ntensor computation and have different impacts on performance and energy\nefficiency. Besides, the hardware part must be implemented by the intrinsic\nfunctions of spatial accelerators. It is hard for programmers to identify and\nanalyze the partitioning methods manually. Second, the overall design space\ncomposed of HW/SW partitioning, hardware optimization, and software\noptimization is huge. The design space needs to be efficiently explored.\n  To this end, we propose an agile co-design approach HASCO that provides an\nefficient HW/SW solution to dense tensor computation. We use tensor syntax\ntrees as the unified IR, based on which we develop a two-step approach to\nidentify partitioning methods. For each method, HASCO explores the hardware and\nsoftware design spaces. We propose different algorithms for the explorations,\nas they have distinct objectives and evaluation costs. Concretely, we develop a\nmulti-objective Bayesian optimization algorithm to explore hardware\noptimization. For software optimization, we use heuristic and Q-learning\nalgorithms. Experiments demonstrate that HASCO achieves a 1.25X to 1.44X\nlatency reduction through HW/SW co-design compared with developing the hardware\nand software separately.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:48:27 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Xiao", "Qingcheng", ""], ["Zheng", "Size", ""], ["Wu", "Bingzhe", ""], ["Xu", "Pengcheng", ""], ["Qian", "Xuehai", ""], ["Liang", "Yun", ""]]}, {"id": "2105.01795", "submitter": "Anup Das", "authors": "Adarsha Balaji and Shihao Song and Twisha Titirsha and Anup Das and\n  Jeffrey Krichmar and Nikil Dutt and James Shackleford and Nagarajan Kandasamy\n  and Francky Catthoor", "title": "NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration\n  with Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, both industry and academia have proposed many different\nneuromorphic architectures to execute applications that are designed with\nSpiking Neural Network (SNN). Consequently, there is a growing need for an\nextensible simulation framework that can perform architectural explorations\nwith SNNs, including both platform-based design of today's hardware, and\nhardware-software co-design and design-technology co-optimization of the\nfuture. We present NeuroXplorer, a fast and extensible framework that is based\non a generalized template for modeling a neuromorphic architecture that can be\ninfused with the specific details of a given hardware and/or technology.\nNeuroXplorer can perform both low-level cycle-accurate architectural\nsimulations and high-level analysis with data-flow abstractions. NeuroXplorer's\noptimization engine can incorporate hardware-oriented metrics such as energy,\nthroughput, and latency, as well as SNN-oriented metrics such as inter-spike\ninterval distortion and spike disorder, which directly impact SNN performance.\nWe demonstrate the architectural exploration capabilities of NeuroXplorer\nthrough case studies with many state-of-the-art machine learning models.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 23:31:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Balaji", "Adarsha", ""], ["Song", "Shihao", ""], ["Titirsha", "Twisha", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Shackleford", "James", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2105.01892", "submitter": "Liqiang Lu", "authors": "Liqiang Lu, Naiqing Guan, Yuyue Wang, Liancheng Jia, Zizhang Luo,\n  Jieming Yin, Jason Cong, Yun Liang", "title": "TENET: A Framework for Modeling Tensor Dataflow Based on\n  Relation-centric Notation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating tensor applications on spatial architectures provides high\nperformance and energy-efficiency, but requires accurate performance models for\nevaluating various dataflow alternatives. Such modeling relies on the notation\nof tensor dataflow and the formulation of performance metrics. Recent proposed\ncompute-centric and data-centric notations describe the dataflow using\nimperative directives. However, these two notations are less expressive and\nthus lead to limited optimization opportunities and inaccurate performance\nmodels.\n  In this paper, we propose a framework TENET that models hardware dataflow of\ntensor applications. We start by introducing a relation-centric notation, which\nformally describes the hardware dataflow for tensor computation. The\nrelation-centric notation specifies the hardware dataflow, PE interconnection,\nand data assignment in a uniform manner using relations. The relation-centric\nnotation is more expressive than the compute-centric and data-centric notations\nby using more sophisticated affine transformations. Another advantage of\nrelation-centric notation is that it inherently supports accurate metrics\nestimation, including data reuse, bandwidth, latency, and energy. TENET\ncomputes each performance metric by counting the relations using integer set\nstructures and operators. Overall, TENET achieves 37.4\\% and 51.4\\% latency\nreduction for CONV and GEMM kernels compared with the state-of-the-art\ndata-centric notation by identifying more sophisticated hardware dataflows.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:50:57 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lu", "Liqiang", ""], ["Guan", "Naiqing", ""], ["Wang", "Yuyue", ""], ["Jia", "Liancheng", ""], ["Luo", "Zizhang", ""], ["Yin", "Jieming", ""], ["Cong", "Jason", ""], ["Liang", "Yun", ""]]}, {"id": "2105.01898", "submitter": "Qijing Huang", "authors": "Qijing Huang, Minwoo Kang, Grace Dinh, Thomas Norell, Aravind Kalaiah,\n  James Demmel, John Wawrzynek, Yakun Sophia Shao", "title": "CoSA: Scheduling by Constrained Optimization for Spatial Accelerators", "comments": "in Proceedings of the International Symposium on Computer\n  Architecture (ISCA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Neural Networks (DNNs) have led to active development\nof specialized DNN accelerators, many of which feature a large number of\nprocessing elements laid out spatially, together with a multi-level memory\nhierarchy and flexible interconnect. While DNN accelerators can take advantage\nof data reuse and achieve high peak throughput, they also expose a large number\nof runtime parameters to the programmers who need to explicitly manage how\ncomputation is scheduled both spatially and temporally. In fact, different\nscheduling choices can lead to wide variations in performance and efficiency,\nmotivating the need for a fast and efficient search strategy to navigate the\nvast scheduling space.\n  To address this challenge, we present CoSA, a constrained-optimization-based\napproach for scheduling DNN accelerators. As opposed to existing approaches\nthat either rely on designers' heuristics or iterative methods to navigate the\nsearch space, CoSA expresses scheduling decisions as a constrained-optimization\nproblem that can be deterministically solved using mathematical optimization\ntechniques. Specifically, CoSA leverages the regularities in DNN operators and\nhardware to formulate the DNN scheduling space into a mixed-integer programming\n(MIP) problem with algorithmic and architectural constraints, which can be\nsolved to automatically generate a highly efficient schedule in one shot. We\ndemonstrate that CoSA-generated schedules significantly outperform\nstate-of-the-art approaches by a geometric mean of up to 2.5x across a wide\nrange of DNN networks while improving the time-to-solution by 90x.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 07:17:25 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Huang", "Qijing", ""], ["Kang", "Minwoo", ""], ["Dinh", "Grace", ""], ["Norell", "Thomas", ""], ["Kalaiah", "Aravind", ""], ["Demmel", "James", ""], ["Wawrzynek", "John", ""], ["Shao", "Yakun Sophia", ""]]}, {"id": "2105.02038", "submitter": "Anup Das", "authors": "Shihao Song, Jui Hanamshet, Adarsha Balaji, Anup Das, Jeffrey L.\n  Krichmar, Nikil D. Dutt, Nagarajan Kandasamy, Francky Catthoor", "title": "Dynamic Reliability Management in Neuromorphic Computing", "comments": "Accepted in ACM JETC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems uses non-volatile memory (NVM) to implement\nhigh-density and low-energy synaptic storage. Elevated voltages and currents\nneeded to operate NVMs cause aging of CMOS-based transistors in each neuron and\nsynapse circuit in the hardware, drifting the transistor's parameters from\ntheir nominal values. Aggressive device scaling increases power density and\ntemperature, which accelerates the aging, challenging the reliable operation of\nneuromorphic systems. Existing reliability-oriented techniques periodically\nde-stress all neuron and synapse circuits in the hardware at fixed intervals,\nassuming worst-case operating conditions, without actually tracking their aging\nat run time. To de-stress these circuits, normal operation must be interrupted,\nwhich introduces latency in spike generation and propagation, impacting the\ninter-spike interval and hence, performance, e.g., accuracy. We propose a new\narchitectural technique to mitigate the aging-related reliability problems in\nneuromorphic systems, by designing an intelligent run-time manager (NCRTM),\nwhich dynamically destresses neuron and synapse circuits in response to the\nshort-term aging in their CMOS transistors during the execution of machine\nlearning workloads, with the objective of meeting a reliability target. NCRTM\nde-stresses these circuits only when it is absolutely necessary to do so,\notherwise reducing the performance impact by scheduling de-stress operations\noff the critical path. We evaluate NCRTM with state-of-the-art machine learning\nworkloads on a neuromorphic hardware. Our results demonstrate that NCRTM\nsignificantly improves the reliability of neuromorphic hardware, with marginal\nimpact on performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:17:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Song", "Shihao", ""], ["Hanamshet", "Jui", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey L.", ""], ["Dutt", "Nikil D.", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2105.02295", "submitter": "Hanieh Hashemi", "authors": "Hanieh Hashemi, Yongqin Wang, Chuan Guo, Murali Annavaram", "title": "Byzantine-Robust and Privacy-Preserving Framework for FedML", "comments": null, "journal-ref": "Security and Safety in Machine Learning Systems Workshop in ICLR\n  2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has emerged as a popular paradigm for collaboratively\ntraining a model from data distributed among a set of clients. This learning\nsetting presents, among others, two unique challenges: how to protect privacy\nof the clients' data during training, and how to ensure integrity of the\ntrained model. We propose a two-pronged solution that aims to address both\nchallenges under a single framework. First, we propose to create secure\nenclaves using a trusted execution environment (TEE) within the server. Each\nclient can then encrypt their gradients and send them to verifiable enclaves.\nThe gradients are decrypted within the enclave without the fear of privacy\nbreaches. However, robustness check computations in a TEE are computationally\nprohibitive. Hence, in the second step, we perform a novel gradient encoding\nthat enables TEEs to encode the gradients and then offloading Byzantine check\ncomputations to accelerators such as GPUs. Our proposed approach provides\ntheoretical bounds on information leakage and offers a significant speed-up\nover the baseline in empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 19:36:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Hashemi", "Hanieh", ""], ["Wang", "Yongqin", ""], ["Guo", "Chuan", ""], ["Annavaram", "Murali", ""]]}, {"id": "2105.02613", "submitter": "Hamid Tabani", "authors": "Hamid Tabani, Ajay Balasubramaniam, Elahe Arani, Bahram Zonooz", "title": "Challenges and Obstacles Towards Deploying Deep Learning Models on\n  Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  From computer vision and speech recognition to forecasting trajectories in\nautonomous vehicles, deep learning approaches are at the forefront of so many\ndomains. Deep learning models are developed using plethora of high-level,\ngeneric frameworks and libraries. Running those models on the mobile devices\nrequire hardware-aware optimizations and in most cases converting the models to\nother formats or using a third-party framework. In reality, most of the\ndeveloped models need to undergo a process of conversion, adaptation, and, in\nsome cases, full retraining to match the requirements and features of the\nframework that is deploying the model on the target platform. Variety of\nhardware platforms with heterogeneous computing elements, from wearable devices\nto high-performance GPU clusters are used to run deep learning models. In this\npaper, we present the existing challenges, obstacles, and practical solutions\ntowards deploying deep learning models on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 12:40:28 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tabani", "Hamid", ""], ["Balasubramaniam", "Ajay", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2105.02917", "submitter": "Gino Chacon", "authors": "Tapojyoti Mandal, Gino Chacon, Johann Knechtel, Ozgur Sinanoglu, Paul\n  Gratz, Vassos Soteriou", "title": "Interposer-Based Root of Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industry is moving towards large-scale system-on-chip (SoC) designs where\nheterogeneous components such as processor cores, DSPs, memory controllers, and\naccelerator units are bundled via 2.5D integration. That is, these components\nare fabricated separately onto chiplets and then integrated using an\ninterconnect carrier, a so-called interposer. Independently, however,\ngeneral-purpose SoC architectures have raised significant security concerns.\nTherefore, with many IP modules and hardware components coming from various\nthird-party vendors and manufacturers, ensuring security and integrity of\nchiplets-based system is a grand challenge. Further, malicious software running\nwithin a chiplet can pose significant risks as well. In this work, we propose\nto leverage an active interposer as secure-by-construction, generic root of\ntrust platform for such modern systems. Our work presents a new architectural\nframework where untrusted processing elements, running untrusted code, are\nintegrated on top of such an interposer-based root of trust, allowing us to\ndetect and prevent any form of malicious messages exchanged between the\nheterogeneous components. Our technique has limited design overhead that is\nfurthermore restricted to the active interposer, allowing the heterogeneous\ncomponents within chiplets to remain untouched. We show that our scheme\ncorrectly handles attempted security violations with little impact on system\nperformance, around 4%.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:06:04 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Mandal", "Tapojyoti", ""], ["Chacon", "Gino", ""], ["Knechtel", "Johann", ""], ["Sinanoglu", "Ozgur", ""], ["Gratz", "Paul", ""], ["Soteriou", "Vassos", ""]]}, {"id": "2105.03176", "submitter": "Matthias Wess", "authors": "Matthias Wess, Matvey Ivanov, Anvesh Nookala, Christoph Unger,\n  Alexander Wendt, Axel Jantsch", "title": "ANNETTE: Accurate Neural Network Execution Time Estimation with Stacked\n  Models", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 3545-3556, 2021", "doi": "10.1109/ACCESS.2020.3047259", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With new accelerator hardware for DNN, the computing power for AI\napplications has increased rapidly. However, as DNN algorithms become more\ncomplex and optimized for specific applications, latency requirements remain\nchallenging, and it is critical to find the optimal points in the design space.\nTo decouple the architectural search from the target hardware, we propose a\ntime estimation framework that allows for modeling the inference latency of\nDNNs on hardware accelerators based on mapping and layer-wise estimation\nmodels. The proposed methodology extracts a set of models from micro-kernel and\nmulti-layer benchmarks and generates a stacked model for mapping and network\nexecution time estimation. We compare estimation accuracy and fidelity of the\ngenerated mixed models, statistical models with the roofline model, and a\nrefined roofline model for evaluation. We test the mixed models on the ZCU102\nSoC board with DNNDK and Intel Neural Compute Stick 2 on a set of 12\nstate-of-the-art neural networks. It shows an average estimation error of 3.47%\nfor the DNNDK and 7.44% for the NCS2, outperforming the statistical and\nanalytical layer models for almost all selected networks. For a randomly\nselected subset of 34 networks of the NASBench dataset, the mixed model reaches\nfidelity of 0.988 in Spearman's rank correlation coefficient metric. The code\nof ANNETTE is publicly available at\nhttps://github.com/embedded-machine-learning/annette.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:39:05 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wess", "Matthias", ""], ["Ivanov", "Matvey", ""], ["Nookala", "Anvesh", ""], ["Unger", "Christoph", ""], ["Wendt", "Alexander", ""], ["Jantsch", "Axel", ""]]}, {"id": "2105.03725", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Geraldo F. Oliveira and Juan G\\'omez-Luna and Lois Orosa and Saugata\n  Ghose and Nandita Vijaykumar and Ivan Fernandez and Mohammad Sadrosadati and\n  Onur Mutlu", "title": "DAMOV: A New Methodology and Benchmark Suite for Evaluating Data\n  Movement Bottlenecks", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/DAMOV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data movement between the CPU and main memory is a first-order obstacle\nagainst improving performance, scalability, and energy efficiency in modern\nsystems. Computer systems employ a range of techniques to reduce overheads tied\nto data movement, spanning from traditional mechanisms (e.g., deep multi-level\ncache hierarchies, aggressive hardware prefetchers) to emerging techniques such\nas Near-Data Processing (NDP), where some computation is moved close to memory.\nOur goal is to methodically identify potential sources of data movement over a\nbroad set of applications and to comprehensively compare traditional\ncompute-centric data movement mitigation techniques to more memory-centric\ntechniques, thereby developing a rigorous understanding of the best techniques\nto mitigate each source of data movement.\n  With this goal in mind, we perform the first large-scale characterization of\na wide variety of applications, across a wide range of application domains, to\nidentify fundamental program properties that lead to data movement to/from main\nmemory. We develop the first systematic methodology to classify applications\nbased on the sources contributing to data movement bottlenecks. From our\nlarge-scale characterization of 77K functions across 345 applications, we\nselect 144 functions to form the first open-source benchmark suite (DAMOV) for\nmain memory data movement studies. We select a diverse range of functions that\n(1) represent different types of data movement bottlenecks, and (2) come from a\nwide range of application domains. Using NDP as a case study, we identify new\ninsights about the different data movement bottlenecks and use these insights\nto determine the most suitable data movement mitigation mechanism for a\nparticular application. We open-source DAMOV and the complete source code for\nour new characterization methodology at https://github.com/CMU-SAFARI/DAMOV.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:02:53 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:03:34 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 15:57:09 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 17:05:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Oliveira", "Geraldo F.", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Orosa", "Lois", ""], ["Ghose", "Saugata", ""], ["Vijaykumar", "Nandita", ""], ["Fernandez", "Ivan", ""], ["Sadrosadati", "Mohammad", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.03736", "submitter": "Sourjya Roy", "authors": "Sourjya Roy, Mustafa Ali and Anand Raghunathan", "title": "PIM-DRAM: Accelerating Machine Learning Workloads using Processing in\n  Commodity DRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) have transformed the field of machine learning\nand are widely deployed in many applications involving image, video, speech and\nnatural language processing. The increasing compute demands of DNNs have been\nwidely addressed through Graphics Processing Units (GPUs) and specialized\naccelerators. However, as model sizes grow, these von Neumann architectures\nrequire very high memory bandwidth to keep the processing elements utilized as\na majority of the data resides in the main memory. Processing in memory has\nbeen proposed as a promising solution for the memory wall bottleneck for ML\nworkloads. In this work, we propose a new DRAM-based processing-in-memory (PIM)\nmultiplication primitive coupled with intra-bank accumulation to accelerate\nmatrix vector operations in ML workloads. The proposed multiplication primitive\nadds < 1% area overhead and does not require any change in the DRAM\nperipherals. Therefore, the proposed multiplication can be easily adopted in\ncommodity DRAM chips. Subsequently, we design a DRAM-based PIM architecture,\ndata mapping scheme and dataflow for executing DNNs within DRAM. System\nevaluations performed on networks like AlexNet, VGG16 and ResNet18 show that\nthe proposed architecture, mapping, and data flow can provide up to 23x speedup\nover an NVIDIA Titan Xp GPU. Furthermore, it achieves upto 6.5x speedup over an\nideal von Neumann architecture with infinite computational throughput,\nhighlighting the need to overcome the memory bottleneck in future generations\nof DNN hardware.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:39:24 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 19:47:44 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Roy", "Sourjya", ""], ["Ali", "Mustafa", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2105.03814", "submitter": "Juan G\\'omez-Luna", "authors": "Juan G\\'omez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula,\n  Geraldo F. Oliveira, Onur Mutlu", "title": "Benchmarking a New Paradigm: An Experimental Analysis of a Real\n  Processing-in-Memory Architecture", "comments": "Our open source software is available at\n  https://github.com/CMU-SAFARI/prim-benchmarks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many modern workloads, such as neural networks, databases, and graph\nprocessing, are fundamentally memory-bound. For such workloads, the data\nmovement between main memory and CPU cores imposes a significant overhead in\nterms of both latency and energy. A major reason is that this communication\nhappens through a narrow bus with high latency and limited bandwidth, and the\nlow data reuse in memory-bound workloads is insufficient to amortize the cost\nof main memory access. Fundamentally addressing this data movement bottleneck\nrequires a paradigm where the memory system assumes an active role in computing\nby integrating processing capabilities. This paradigm is known as\nprocessing-in-memory (PIM).\n  Recent research explores different forms of PIM architectures, motivated by\nthe emergence of new 3D-stacked memory technologies that integrate memory with\na logic layer where processing elements can be easily placed. Past works\nevaluate these architectures in simulation or, at best, with simplified\nhardware prototypes. In contrast, the UPMEM company has designed and\nmanufactured the first publicly-available real-world PIM architecture.\n  This paper provides the first comprehensive analysis of the first\npublicly-available real-world PIM architecture. We make two key contributions.\nFirst, we conduct an experimental characterization of the UPMEM-based PIM\nsystem using microbenchmarks to assess various architecture limits such as\ncompute throughput and memory bandwidth, yielding new insights. Second, we\npresent PrIM, a benchmark suite of 16 workloads from different application\ndomains (e.g., linear algebra, databases, graph processing, neural networks,\nbioinformatics).\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:57:47 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:05:02 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 20:58:40 GMT"}, {"version": "v4", "created": "Tue, 6 Jul 2021 16:39:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["G\u00f3mez-Luna", "Juan", ""], ["Hajj", "Izzat El", ""], ["Fernandez", "Ivan", ""], ["Giannoula", "Christina", ""], ["Oliveira", "Geraldo F.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.03859", "submitter": "Ruben Gran Tejero", "authors": "Yamilka Toca-D\\'iaz, Alejandro Valero, Rub\\'en Gran-Tejero and Dar\\'io\n  Su\\'arez-Gracia", "title": "RRCD: Redirecci\\'on de Registros Basada en Compresi\\'on de Datos para\n  Tolerar FallosPermanentes en una GPU", "comments": "10 page, in Spanish, 6 Figures, to be submitted to Jornadas SARTECO\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ever-increasing parallelism demand of General-Purpose Graphics Processing\nUnit (GPGPU) applications pushes toward larger and more energy-hungry register\nfiles in successive GPU generations. Reducing the supply voltage beyond its\nsafe limit is an effective way to improve the energy efficiency of register\nfiles. However, at these operating voltages, the reliability of the circuit is\ncompromised. This work aims to tolerate permanent faults from process\nvariations in large GPU register files operating below the safe supply voltage\nlimit. To do so, this paper proposes a microarchitectural patching technique,\nDC-Patch, exploiting the inherent data redundancy of applications to compress\nregisters at run-time with neither compiler assistance nor instruction set\nmodifications. Instead of disabling an entire faulty register file entry,\nDC-Patch leverages the reliable cells within a faulty entry to store compressed\nregister values. Experimental results show that, with more than a third of\nfaulty register entries, DC-Patch ensures a reliable operation of the register\nfile and reduces the energy consumption by 47% with respect to a conventional\nregister file working at nominal supply voltage. The energy savings are 21%\ncompared to a voltage noise smoothing scheme operating at the safe supply\nvoltage limit. These benefits are obtained with less than 2 and 6% impact on\nthe system performance and area, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 07:29:31 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Toca-D\u00edaz", "Yamilka", ""], ["Valero", "Alejandro", ""], ["Gran-Tejero", "Rub\u00e9n", ""], ["Su\u00e1rez-Gracia", "Dar\u00edo", ""]]}, {"id": "2105.04003", "submitter": "Abhiroop Bhattacharjee", "authors": "Abhiroop Bhattacharjee, Abhishek Moitra and Priyadarshini Panda", "title": "Efficiency-driven Hardware Optimization for Adversarially Robust Neural\n  Networks", "comments": "6 pages, 8 figures, 3 tables; Accepted in DATE 2021 conference. arXiv\n  admin note: text overlap with arXiv:2008.11298", "journal-ref": "2021 Design, Automation and Test in Europe (DATE) Conference", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a growing need to enable intelligence in embedded devices in the\nInternet of Things (IoT) era, secure hardware implementation of Deep Neural\nNetworks (DNNs) has become imperative. We will focus on how to address\nadversarial robustness for DNNs through efficiency-driven hardware\noptimizations. Since memory (specifically, dot-product operations) is a key\nenergy-spending component for DNNs, hardware approaches in the past have\nfocused on optimizing the memory. One such approach is approximate digital CMOS\nmemories with hybrid 6T-8T SRAM cells that enable supply voltage (Vdd) scaling\nyielding low-power operation, without significantly affecting the performance\ndue to read/write failures incurred in the 6T cells. In this paper, we show how\nthe bit-errors in the 6T cells of hybrid 6T-8T memories minimize the\nadversarial perturbations in a DNN. Essentially, we find that for different\nconfigurations of 8T-6T ratios and scaledVdd operation, noise incurred in the\nhybrid memory architectures is bound within specific limits. This hardware\nnoise can potentially interfere in the creation of adversarial attacks in DNNs\nyielding robustness. Another memory optimization approach involves using analog\nmemristive crossbars that perform Matrix-Vector-Multiplications (MVMs)\nefficiently with low energy and area requirements. However, crossbars generally\nsuffer from intrinsic non-idealities that cause errors in performing MVMs,\nleading to degradation in the accuracy of the DNNs. We will show how the\nintrinsic hardware variations manifested through crossbar non-idealities yield\nadversarial robustness to the mapped DNNs without any additional optimization.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 19:26:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bhattacharjee", "Abhiroop", ""], ["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2105.04151", "submitter": "Xinyu Chen", "authors": "Xinyu Chen, Hongshi Tan, Yao Chen, Bingsheng He, Weng-Fai Wong, Deming\n  Chen", "title": "Skew-Oblivious Data Routing for Data-Intensive Applications on FPGAs\n  with HLS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  FPGAs have become emerging computing infrastructures for accelerating\napplications in datacenters. Meanwhile, high-level synthesis (HLS) tools have\nbeen proposed to ease the programming of FPGAs. Even with HLS, irregular\ndata-intensive applications require explicit optimizations, among which\nmultiple processing elements (PEs) with each owning a private BRAM-based buffer\nare usually adopted to process multiple data per cycle. Data routing, which\ndynamically dispatches multiple data to designated PEs, avoids data replication\nin buffers compared to statically assigning data to PEs, hence saving BRAM\nusage. However, the workload imbalance among PEs vastly diminishes performance\nwhen processing skew datasets. In this paper, we propose a skew-oblivious data\nrouting architecture that allocates secondary PEs and schedules them to share\nthe workload of the overloaded PEs at run-time. In addition, we integrate the\nproposed architecture into a framework called Ditto to minimize the development\nefforts for applications that require skew handling. We evaluate Ditto on five\ncommonly used applications: histogram building, data partitioning, pagerank,\nheavy hitter detection and hyperloglog. The results demonstrate that the\ngenerated implementations are robust to skew datasets and outperform the\nstateof-the-art designs in both throughput and BRAM usage efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 07:14:43 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Xinyu", ""], ["Tan", "Hongshi", ""], ["Chen", "Yao", ""], ["He", "Bingsheng", ""], ["Wong", "Weng-Fai", ""], ["Chen", "Deming", ""]]}, {"id": "2105.04212", "submitter": "Orian Leitersdorf", "authors": "Orian Leitersdorf, Ben Perach, Ronny Ronen, Shahar Kvatinsky", "title": "Efficient Error-Correcting-Code Mechanism for High-Throughput Memristive\n  Processing-in-Memory", "comments": "Accepted to 58th Design Automation Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inefficient data transfer between computation and memory inspired emerging\nprocessing-in-memory (PIM) technologies. Many PIM solutions enable storage and\nprocessing using memristors in a crossbar-array structure, with techniques such\nas memristor-aided logic (MAGIC) used for computation. This approach provides\nhighly-paralleled logic computation with minimal data movement. However,\nmemristors are vulnerable to soft errors and standard error-correcting-code\n(ECC) techniques are difficult to implement without moving data outside the\nmemory. We propose a novel technique for efficient ECC implementation along\ndiagonals to support reliable computation inside the memory without explicitly\nreading the data. Our evaluation demonstrates an improvement of over eight\norders of magnitude in reliability (mean time to failure) for an increase of\nabout 26% in computation latency.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 09:13:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Leitersdorf", "Orian", ""], ["Perach", "Ben", ""], ["Ronen", "Ronny", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "2105.05588", "submitter": "Jorge Echavarria", "authors": "Jorge Echavarria and Stefan Wildermann and Oliver Keszocze and\n  Faramarz Khosravi and Andreas Becher and J\\\"urgen Teich", "title": "On the Approximation of Accuracy-configurable Sequential Multipliers via\n  Segmented Carry Chains", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a multiplier based on a sequence of approximated\naccumulations. According to a given splitting point of the carry chains, the\ntechnique herein introduced allows varying the quality of the accumulations\nand, consequently, the overall product. Our approximate multiplier trades-off\naccuracy for a reduced latency (with respect to an accurate sequential\nmultiplier) and exploits the inherent area savings of sequential over\ncombinatorial approaches. We implemented multiple versions with different\nbit-width and accuracy configurations, targeting an FPGA and a 45nm ASIC to\nestimate resources, power consumption, and latency. We also present two error\nanalyses of the proposed design based on closed-form analysis and simulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:06:37 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 12:11:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Echavarria", "Jorge", ""], ["Wildermann", "Stefan", ""], ["Keszocze", "Oliver", ""], ["Khosravi", "Faramarz", ""], ["Becher", "Andreas", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "2105.05821", "submitter": "Lingda Li", "authors": "Lingda Li, Santosh Pandey, Thomas Flynn, Hang Liu, Noel Wheeler,\n  Adolfy Hoisie", "title": "SimNet: Computer Architecture Simulation using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cycle-accurate simulators are essential tools for architecture\nresearch, design, and development, their practicality is limited by an\nextremely long time-to-solution for realistic problems under investigation.\nThis work describes a concerted effort, where machine learning (ML) is used to\naccelerate discrete-event simulation. First, an ML-based instruction latency\nprediction framework that accounts for both static instruction/architecture\nproperties and dynamic execution context is constructed. Then, a\nGPU-accelerated parallel simulator is implemented based on the proposed\ninstruction latency predictor, and its simulation accuracy and throughput are\nvalidated and evaluated against a state-of-the-art simulator. Leveraging modern\nGPUs, the ML-based simulator outperforms traditional simulators significantly.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:31:52 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Li", "Lingda", ""], ["Pandey", "Santosh", ""], ["Flynn", "Thomas", ""], ["Liu", "Hang", ""], ["Wheeler", "Noel", ""], ["Hoisie", "Adolfy", ""]]}, {"id": "2105.05962", "submitter": "Pedro Antonino", "authors": "Pedro Antonino and Wojciech Aleksander Wo{\\l}oszyn and A. W. Roscoe", "title": "Guardian: symbolic validation of orderliness in SGX enclaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors can offer hardware primitives that allow a process to run\nin isolation. These primitives implement a trusted execution environment (TEE)\nin which a program can run such that the integrity and confidentiality of its\nexecution are guaranteed. Intel's Software Guard eXtensions (SGX) is an example\nof such primitives and its isolated processes are called \\emph{enclaves}. These\nguarantees, however, can be easily thwarted if the enclave has not been\nproperly designed. Its interface with the untrusted software stack is arguably\nthe largest attack surface that adversaries can exploit; unintended\ninteractions with untrusted code can expose the enclave to memory corruption\nattacks, for instance. In this paper, we propose a notion of an \\emph{orderly}\nenclave which splits its behaviour into several execution phases each of which\nimposes a set of restrictions on accesses to untrusted memory, phase\ntransitions and registers sanitisation. A violation to these restrictions\nindicates an undesired behaviour which could be harnessed to perpetrate attacks\nagainst the enclave. We also introduce \\Analyser{}: a tool that uses symbolic\nexecution to carry out the validation of an enclave against our notion of an\norderly enclave; in this process, it also looks for some typical\nmemory-corruption vulnerabilities. We discuss how our approach can prevent and\nflag enclave vulnerabilities that have been identified in the literature.\nMoreover, we have evaluated how our approach fares in the analysis of some\npractical enclaves. \\Analyser{} was able to identify real vulnerabilities on\nthese enclaves which have been acknowledged and fixed by their maintainers.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:50:33 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Antonino", "Pedro", ""], ["Wo\u0142oszyn", "Wojciech Aleksander", ""], ["Roscoe", "A. W.", ""]]}, {"id": "2105.06250", "submitter": "Yao Chen", "authors": "Yao Chen, Cole Hawkins, Kaiqi Zhang, Zheng Zhang, Cong Hao", "title": "3U-EdgeAI: Ultra-Low Memory Training, Ultra-Low BitwidthQuantization,\n  and Ultra-Low Latency Acceleration", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The deep neural network (DNN) based AI applications on the edge require both\nlow-cost computing platforms and high-quality services. However, the limited\nmemory, computing resources, and power budget of the edge devices constrain the\neffectiveness of the DNN algorithms. Developing edge-oriented AI algorithms and\nimplementations (e.g., accelerators) is challenging. In this paper, we\nsummarize our recent efforts for efficient on-device AI development from three\naspects, including both training and inference. First, we present on-device\ntraining with ultra-low memory usage. We propose a novel rank-adaptive\ntensor-based tensorized neural network model, which offers orders-of-magnitude\nmemory reduction during training. Second, we introduce an ultra-low bitwidth\nquantization method for DNN model compression, achieving the state-of-the-art\naccuracy under the same compression ratio. Third, we introduce an ultra-low\nlatency DNN accelerator design, practicing the software/hardware co-design\nmethodology. This paper emphasizes the importance and efficacy of training,\nquantization and accelerator design, and calls for more research breakthroughs\nin the area for AI on the edge.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 03:22:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Chen", "Yao", ""], ["Hawkins", "Cole", ""], ["Zhang", "Kaiqi", ""], ["Zhang", "Zheng", ""], ["Hao", "Cong", ""]]}, {"id": "2105.06594", "submitter": "Maya Gokhale", "authors": "Joshua Landgraf, Scott Lloyd, Maya Gokhale", "title": "Combining Emulation and Simulation to Evaluate a Near Memory Key/Value\n  Lookup Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-738643", "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing large numbers of key/value lookups is an integral part of modern\nserver databases and other \"Big Data\" applications. Prior work has shown that\nhash table based key/value lookups can benefit significantly from using a\ndedicated hardware lookup accelerator placed near memory. However, previous\nevaluations of this design on the Logic in Memory Emulator (LiME) were limited\nby the capabilities of the hardware on which it was emulated, which only\nsupports a single CPU core and a single near-memory lookup engine. We extend\nthe emulation results by incorporating simulation to evaluate this design in\nadditional scenarios. By incorporating an HMC simulation model, we design\noptimizations that better mitigate the effects of the HMC closed page policy\nand that better utilize the HMC's parallelism, improving predicted performance\nby an order of magnitude. Additionally, we use simulation to evaluate the\nscaling performance of multiple near-memory lookup accelerators. Our work\nemploys an open source emulator LiME, open source simulatation infrastructure\nSST, and the open source HMC-Sim simulator.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:32:59 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Landgraf", "Joshua", ""], ["Lloyd", "Scott", ""], ["Gokhale", "Maya", ""]]}, {"id": "2105.06872", "submitter": "Oleksii Oleksenko", "authors": "Oleksii Oleksenko, Christof Fetzer, Boris K\\\"opf, Mark Silberstein", "title": "Revizor: Fuzzing for Leaks in Black-box CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared microarchitectural state has become a prime target for side-channel\nattacks that leverage timing measurements to leak information across security\ndomains. Combined with speculative execution, they cause vulnerabilities like\nSpectre and Meltdown. Such vulnerabilities often stay undetected for a long\ntime because we lack the tools for systematic testing of CPUs against them.\n  In this paper, we propose an approach to automatically detect\nmicroarchitectural information leakage in commercial black-box CPUs. We base\nour approach on speculation contracts, which we employ to specify the permitted\nside effects of program execution on the microarchitectural state. We propose a\ntechnique, called Model-based Relational Fuzzing (MRF), that enables testing of\nCPUs against these specifications.\n  We implement MRF in a fuzzing framework called Revizor, and showcase its\neffectiveness on real Intel x86 CPUs: It automatically detects violations of a\nrich set of contracts, or indicates their absence. A highlight of our findings\nis that Revizor managed to automatically surface Spectre, MDS, and LVI by\nfuzzing against increasingly liberal contracts.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:56:15 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Oleksenko", "Oleksii", ""], ["Fetzer", "Christof", ""], ["K\u00f6pf", "Boris", ""], ["Silberstein", "Mark", ""]]}, {"id": "2105.07115", "submitter": "Syed Mohsin Abbas Dr.", "authors": "Syed Mohsin Abbas, Thibaud Tonnellier, Furkan Ercan, Marwan\n  Jalaleddine and Warren J. Gross", "title": "High-Throughput VLSI architecture for Soft-Decision decoding with\n  ORBGRAND", "comments": "Please note that a mislabeling in Fig. 1 has occurred in the IEEE\n  Xplore version of this paper. This error has been corrected in this version\n  of the manuscript. (Accepted in ICASSP 2021)", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414908", "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guessing Random Additive Noise Decoding (GRAND) is a recently proposed\napproximate Maximum Likelihood (ML) decoding technique that can decode any\nlinear error-correcting block code. Ordered Reliability Bits GRAND (ORBGRAND)\nis a powerful variant of GRAND, which outperforms the original GRAND technique\nby generating error patterns in a specific order. Moreover, their simplicity at\nthe algorithm level renders GRAND family a desirable candidate for applications\nthat demand very high throughput. This work reports the first-ever hardware\narchitecture for ORBGRAND, which achieves an average throughput of up to $42.5$\nGbps for a code length of $128$ at an SNR of $10$ dB. Moreover, the proposed\nhardware can be used to decode any code provided the length and rate\nconstraints. Compared to the state-of-the-art fast dynamic successive\ncancellation flip decoder (Fast-DSCF) using a 5G polar $(128,105)$ code, the\nproposed VLSI implementation has $49\\times$ more average throughput while\nmaintaining similar decoding performance.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:38:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Abbas", "Syed Mohsin", ""], ["Tonnellier", "Thibaud", ""], ["Ercan", "Furkan", ""], ["Jalaleddine", "Marwan", ""], ["Gross", "Warren J.", ""]]}, {"id": "2105.07131", "submitter": "Reza Sameni", "authors": "Amir-Hossein Kiamarzi, Pezhman Torabi, Reza Sameni", "title": "Hardware Synthesis of State-Space Equations; Application to FPGA\n  Implementation of Shallow and Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, shallow and deep Neural Networks (NNs) have vast applications\nincluding biomedical engineering, image processing, computer vision, and speech\nrecognition. Many researchers have developed hardware accelerators including\nfield-programmable gate arrays (FPGAs) for implementing high-performance and\nenergy efficient NNs. Apparently, the hardware architecture design process is\nspecific and time-consuming for each NN. Therefore, a systematic way to design,\nimplement and optimize NNs is highly demanded. The paper presents a systematic\napproach to implement state-space models in register transfer level (RTL), with\nspecial interest for NN implementation. The proposed design flow is based on\nthe iterative nature of state-space models and the analogy between state-space\nformulations and finite-state machines. The method can be used in\nlinear/nonlinear and time-varying/time-invariant systems. It can also be used\nto implement either intrinsically iterative systems (widely used in various\ndomains such as signal processing, numerical analysis, computer arithmetic, and\ncontrol engineering), or systems that could be rewritten in equivalent\niterative forms. The implementation of recurrent NNs such as long short-term\nmemory (LSTM) NNs, which have intrinsic state-space forms, are another major\napplications for this framework. As a case study, it is shown that state-space\nsystems can be used for the systematic implementation and optimization of NNs\n(as nonlinear and time-varying dynamic systems). An RTL code generating\nsoftware is also provided online, which simplifies the automatic generation of\nNNs of arbitrary size.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 04:00:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kiamarzi", "Amir-Hossein", ""], ["Torabi", "Pezhman", ""], ["Sameni", "Reza", ""]]}, {"id": "2105.07428", "submitter": "Xianjun Jiao", "authors": "Xianjun Jiao, Michael Mehari, Wei Liu, Muhammad Aslam, Ingrid Moerman", "title": "Openwifi CSI fuzzer for authorized sensing and covert channels", "comments": "Accepted by ACM WiSec 2021", "journal-ref": null, "doi": "10.1145/3448300.3468255", "report-no": null, "categories": "cs.CR cs.AR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  CSI (Channel State Information) of WiFi systems contains the environment\nchannel response between the transmitter and the receiver, so the\npeople/objects and their movement in between can be sensed. To get CSI, the\nreceiver performs channel estimation based on the pre-known training field of\nthe transmitted WiFi signal. CSI related technology is useful in many cases,\nbut it also brings concerns on privacy and security. In this paper, we open\nsourced a CSI fuzzer to enhance the privacy and security of WiFi CSI\napplications. It is built and embedded into the transmitter of openwifi, which\nis an open source full-stack WiFi chip design, to prevent unauthorized sensing\nwithout sacrificing the WiFi link performance. The CSI fuzzer imposes an\nartificial channel response to the signal before it is transmitted, so the CSI\nseen by the receiver will indicate the actual channel response combined with\nthe artificial response. Only the authorized receiver, that knows the\nartificial response, can calculate the actual channel response and perform the\nCSI sensing. Another potential application of the CSI fuzzer is covert channels\nbased on a set of pre-defined artificial response patterns. Our work resolves\nthe pain point of implementing the anti-sensing idea based on the commercial\noff-the-shelf WiFi devices.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 13:00:00 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 21:30:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jiao", "Xianjun", ""], ["Mehari", "Michael", ""], ["Liu", "Wei", ""], ["Aslam", "Muhammad", ""], ["Moerman", "Ingrid", ""]]}, {"id": "2105.07432", "submitter": "Chandan Jha Dr.", "authors": "Chandan Kumar Jha, Shreyas Singh, Riddhi Thakker, Manu Awasthi and\n  Joycee Mekie", "title": "Zero Aware Configurable Data Encoding by Skipping Transfer for Error\n  Resilient Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Zero Aware Configurable Data Encoding by Skipping\nTransfer (ZAC-DEST), a data encoding scheme to reduce the energy consumption of\nDRAM channels, specifically targeted towards approximate computing and error\nresilient applications. ZAC-DEST exploits the similarity between recent data\ntransfers across channels and information about the error resilience behavior\nof applications to reduce on-die termination and switching energy by reducing\nthe number of 1's transmitted over the channels. ZAC-DEST also provides a\nnumber of knobs for trading off the application's accuracy for energy savings,\nand vice versa, and can be applied to both training and inference.\n  We apply ZAC-DEST to five machine learning applications. On average, across\nall applications and configurations, we observed a reduction of $40$% in\ntermination energy and $37$% in switching energy as compared to the state of\nthe art data encoding technique BD-Coder with an average output quality loss of\n$10$%. We show that if both training and testing are done assuming the presence\nof ZAC-DEST, the output quality of the applications can be improved upto 9\ntimes as compared to when ZAC-DEST is only applied during testing leading to\nenergy savings during training and inference with increased output quality.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 13:05:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jha", "Chandan Kumar", ""], ["Singh", "Shreyas", ""], ["Thakker", "Riddhi", ""], ["Awasthi", "Manu", ""], ["Mekie", "Joycee", ""]]}, {"id": "2105.07784", "submitter": "Alexandros Dimopoulos", "authors": "A. C. Dimopoulos, C. Pavlatos, G. Papakonstantinou", "title": "Multi-output, multi-level, multi-gate design using non-linear\n  programming", "comments": "14 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using logic gates is the traditional way of designing logic circuits.\nHowever, most of the minimization algorithms concern a limited set of gates\n(complete sets), like sum of products, exclusive-or sum of products, NAND\ngates, NOR gates e.t.c.. In this paper, a method is proposed for minimizing\nmulti-output Boolean functions using any kind of two-input gates although it\ncan easily be extended to multi-input gates. The method is based on non-linear\nmixed integer programming. The experimental results show that the method gives\nthe same or better results compared to other methods available in the\nliterature. However, other methods do not ensure that they produce the minimal\nsolution, while the main advantages of the proposed method are that it does\nguarantee minimality and it can also handle Boolean functions for incompletely\nspecified functions. The method is general enough and can easily be extended to\nmore complicated design modules than just basic gates.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 12:50:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dimopoulos", "A. C.", ""], ["Pavlatos", "C.", ""], ["Papakonstantinou", "G.", ""]]}, {"id": "2105.08123", "submitter": "Ataberk Olgun", "authors": "Nandita Vijaykumar, Ataberk Olgun, Konstantinos Kanellopoulos, Nisa\n  Bostanc{\\i}, Hasan Hassan, Mehrshad Lotfi, Phillip B. Gibbons, Onur Mutlu", "title": "MetaSys: A Practical Open-Source Metadata Management System to Implement\n  and Evaluate Cross-Layer Optimizations", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the first open-source FPGA-based infrastructure,\nMetaSys, with a prototype in a RISC-V core, to enable the rapid implementation\nand evaluation of a wide range of cross-layer techniques in real hardware.\nHardware-software cooperative techniques are powerful approaches to improve the\nperformance, quality of service, and security of general-purpose processors.\nThey are however typically challenging to rapidly implement and evaluate in\nreal hardware as they require full-stack changes to the hardware, OS, system\nsoftware, and instruction-set architecture (ISA).\n  MetaSys implements a rich hardware-software interface and lightweight\nmetadata support that can be used as a common basis to rapidly implement and\nevaluate new cross-layer techniques. We demonstrate MetaSys's versatility and\nease-of-use by implementing and evaluating three cross-layer techniques for:\n(i) prefetching for graph analytics; (ii) bounds checking in memory unsafe\nlanguages, and (iii) return address protection in stack frames; each technique\nonly requiring ~100 lines of Chisel code over MetaSys.\n  Using MetaSys, we perform the first detailed experimental study to quantify\nthe performance overheads of using a single metadata management system to\nenable multiple cross-layer optimizations in CPUs. We identify the key sources\nof bottlenecks and system inefficiency of a general metadata management system.\nWe design MetaSys to minimize these inefficiencies and provide increased\nversatility compared to previously-proposed metadata systems. Using three use\ncases and a detailed characterization, we demonstrate that a common metadata\nmanagement system can be used to efficiently support diverse cross-layer\ntechniques in CPUs.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:27:48 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 08:41:33 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Vijaykumar", "Nandita", ""], ["Olgun", "Ataberk", ""], ["Kanellopoulos", "Konstantinos", ""], ["Bostanc\u0131", "Nisa", ""], ["Hassan", "Hasan", ""], ["Lotfi", "Mehrshad", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.08217", "submitter": "Amogh Agrawal", "authors": "Amogh Agrawal, Mustafa Ali, Minsuk Koo, Nitin Rathi, Akhilesh Jaiswal,\n  Kaushik Roy", "title": "IMPULSE: A 65nm Digital Compute-in-Memory Macro with Fused Weights and\n  Membrane Potential for Spike-based Sequential Learning Tasks", "comments": null, "journal-ref": null, "doi": "10.1109/LSSC.2021.3092727", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inherent dynamics of the neuron membrane potential in Spiking Neural\nNetworks (SNNs) allows processing of sequential learning tasks, avoiding the\ncomplexity of recurrent neural networks. The highly-sparse spike-based\ncomputations in such spatio-temporal data can be leveraged for\nenergy-efficiency. However, the membrane potential incurs additional memory\naccess bottlenecks in current SNN hardware. To that effect, we propose a\n10T-SRAM compute-in-memory (CIM) macro, specifically designed for\nstate-of-the-art SNN inference. It consists of a fused weight (WMEM) and\nmembrane potential (VMEM) memory and inherently exploits sparsity in input\nspikes leading to 97.4% reduction in energy-delay-product (EDP) at 85% sparsity\n(typical of SNNs considered in this work) compared to the case of no sparsity.\nWe propose staggered data mapping and reconfigurable peripherals for handling\ndifferent bit-precision requirements of WMEM and VMEM, while supporting\nmultiple neuron functionalities. The proposed macro was fabricated in 65nm CMOS\ntechnology, achieving an energy-efficiency of 0.99TOPS/W at 0.85V supply and\n200MHz frequency for signed 11-bit operations. We evaluate the SNN for\nsentiment classification from the IMDB dataset of movie reviews and achieve\nwithin 1% accuracy of an LSTM network with 8.5x lower parameters.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 01:14:56 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Agrawal", "Amogh", ""], ["Ali", "Mustafa", ""], ["Koo", "Minsuk", ""], ["Rathi", "Nitin", ""], ["Jaiswal", "Akhilesh", ""], ["Roy", "Kaushik", ""]]}, {"id": "2105.08239", "submitter": "Yangjie Qi", "authors": "Yangjie Qi, Shuo Zhang and Tarek M. Taha", "title": "TRIM: A Design Space Exploration Model for Deep Neural Networks\n  Inference and Training Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing demand for specialized hardware for training deep neural\nnetworks, both in edge/IoT environments and in high-performance computing\nsystems. The design space of such hardware is very large due to the wide range\nof processing architectures, deep neural network configurations, and dataflow\noptions. This makes developing deep neural network processors quite complex,\nespecially for training. We present TRIM, an infrastructure to help hardware\narchitects explore the design space of deep neural network accelerators for\nboth inference and training in the early design stages. The model evaluates at\nthe whole network level, considering both inter-layer and intra-layer\nactivities. Given applications, essential hardware specifications, and a design\ngoal, TRIM can quickly explore different hardware design options, select the\noptimal dataflow and guide new hardware architecture design. We validated TRIM\nwith FPGA-based implementation of deep neural network accelerators and\nASIC-based architectures. We also show how to use TRIM to explore the design\nspace through several case studies. TRIM is a powerful tool to help architects\nevaluate different hardware choices to develop efficient inference and training\narchitecture design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:38:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Qi", "Yangjie", ""], ["Zhang", "Shuo", ""], ["Taha", "Tarek M.", ""]]}, {"id": "2105.08712", "submitter": "Asmit De", "authors": "Asmit De, Swaroop Ghosh", "title": "HeapSafe: Securing Unprotected Heaps in RISC-V", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RISC-V is a promising open-source architecture primarily targeted for\nembedded systems. Programs compiled using the RISC-V toolchain can run\nbare-metal on the system, and, as such, can be vulnerable to several memory\ncorruption vulnerabilities. In this work, we present HeapSafe, a lightweight\nhardware assisted heap-buffer protection scheme to mitigate heap overflow and\nuse-after-free vulnerabilities in a RISC-V SoC. The proposed scheme tags\npointers associated with heap buffers with metadata indices and enforces tag\npropagation for commonly used pointer operations. The HeapSafe hardware is\ndecoupled from the core and is designed as a configurable coprocessor and is\nresponsible for validating the heap buffer accesses. Benchmark results show a\n1.5X performance overhead and 1.59% area overhead, while being 22% faster than\na software protection. We further implemented a HeapSafe-nb, an asynchronous\nvalidation design, which improves performance by 27% over the synchronous\nHeapSafe.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:52:16 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["De", "Asmit", ""], ["Ghosh", "Swaroop", ""]]}, {"id": "2105.08784", "submitter": "Sachin Sapatnekar", "authors": "Mohammad Abdullah Al Shohel, Vidya A. Chhabria, and Sachin S.\n  Sapatnekar", "title": "A New, Computationally Efficient \"Blech Criterion\" for Immortality in\n  General Interconnects", "comments": "Accepted for publication in the Proceedings of the ACM/IEEE Design\n  Automation Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methodologies for analyzing electromigration (EM) in VLSI\ncircuits first filter immortal wires using Blech's criterion, and then perform\ndetailed EM analysis on the remaining wires. However, Blech's criterion was\ndesigned for two-terminal wires and does not extend to general structures. This\npaper demonstrates a first-principles-based solution technique for determining\nthe steady-state stress at all the nodes of a general interconnect structure,\nand develops an immortality test whose complexity is linear in the number of\nedges of an interconnect structure. The proposed model is applied to a variety\nof structures. The method is shown to match well with results from numerical\nsolvers, to be scalable to large structures.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 19:08:16 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Shohel", "Mohammad Abdullah Al", ""], ["Chhabria", "Vidya A.", ""], ["Sapatnekar", "Sachin S.", ""]]}, {"id": "2105.08820", "submitter": "Udit Gupta", "authors": "Udit Gupta, Samuel Hsia, Jeff Zhang, Mark Wilkening, Javin Pombra,\n  Hsien-Hsin S. Lee, Gu-Yeon Wei, Carole-Jean Wu, David Brooks", "title": "RecPipe: Co-designing Models and Hardware to Jointly Optimize\n  Recommendation Quality and Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation systems must provide high quality, personalized\ncontent under strict tail-latency targets and high system loads. This paper\npresents RecPipe, a system to jointly optimize recommendation quality and\ninference performance. Central to RecPipe is decomposing recommendation models\ninto multi-stage pipelines to maintain quality while reducing compute\ncomplexity and exposing distinct parallelism opportunities. RecPipe implements\nan inference scheduler to map multi-stage recommendation engines onto\ncommodity, heterogeneous platforms (e.g., CPUs, GPUs).While the hardware-aware\nscheduling improves ranking efficiency, the commodity platforms suffer from\nmany limitations requiring specialized hardware. Thus, we design RecPipeAccel\n(RPAccel), a custom accelerator that jointly optimizes quality, tail-latency,\nand system throughput. RPAc-cel is designed specifically to exploit the\ndistinct design space opened via RecPipe. In particular, RPAccel processes\nqueries in sub-batches to pipeline recommendation stages, implements dual\nstatic and dynamic embedding caches, a set of top-k filtering units, and a\nreconfigurable systolic array. Com-pared to prior-art and at iso-quality, we\ndemonstrate that RPAccel improves latency and throughput by 3x and 6x.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:44:04 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 17:41:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gupta", "Udit", ""], ["Hsia", "Samuel", ""], ["Zhang", "Jeff", ""], ["Wilkening", "Mark", ""], ["Pombra", "Javin", ""], ["Lee", "Hsien-Hsin S.", ""], ["Wei", "Gu-Yeon", ""], ["Wu", "Carole-Jean", ""], ["Brooks", "David", ""]]}, {"id": "2105.08937", "submitter": "Gang Li", "authors": "Gang Li, Zejian Liu, Fanrong Li, Jian Cheng", "title": "Block Convolution: Towards Memory-Efficient Inference of Large-Scale\n  CNNs on FPGA", "comments": "Accepted to IEEE Transactions on Computer-Aided Design of Integrated\n  Circuits and Systems (TCAD), 2021. This is an extended version of the\n  conference paper published on DATE'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks have achieved remarkable progress in\nrecent years. However, the large volume of intermediate results generated\nduring inference poses a significant challenge to the accelerator design for\nresource-constraint FPGA. Due to the limited on-chip storage, partial results\nof intermediate layers are frequently transferred back and forth between\non-chip memory and off-chip DRAM, leading to a non-negligible increase in\nlatency and energy consumption. In this paper, we propose block convolution, a\nhardware-friendly, simple, yet efficient convolution operation that can\ncompletely avoid the off-chip transfer of intermediate feature maps at\nrun-time. The fundamental idea of block convolution is to eliminate the\ndependency of feature map tiles in the spatial dimension when spatial tiling is\nused, which is realized by splitting a feature map into independent blocks so\nthat convolution can be performed separately on individual blocks. We conduct\nextensive experiments to demonstrate the efficacy of the proposed block\nconvolution on both the algorithm side and the hardware side. Specifically, we\nevaluate block convolution on 1) VGG-16, ResNet-18, ResNet-50, and MobileNet-V1\nfor ImageNet classification task; 2) SSD, FPN for COCO object detection task,\nand 3) VDSR for Set5 single image super-resolution task. Experimental results\ndemonstrate that comparable or higher accuracy can be achieved with block\nconvolution. We also showcase two CNN accelerators via algorithm/hardware\nco-design based on block convolution on memory-limited FPGAs, and evaluation\nshows that both accelerators substantially outperform the baseline without\noff-chip transfer of intermediate feature maps.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:03:59 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Gang", ""], ["Liu", "Zejian", ""], ["Li", "Fanrong", ""], ["Cheng", "Jian", ""]]}, {"id": "2105.08955", "submitter": "Ataberk Olgun", "authors": "Ataberk Olgun, Minesh Patel, A. Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}, Haocong\n  Luo, Jeremie S. Kim, Nisa Bostanc{\\i}, Nandita Vijaykumar, O\\u{g}uz Ergin,\n  Onur Mutlu", "title": "QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple\n  Row Activation in Commodity DRAM Chips", "comments": "15 pages, 14 figures. A shorter version of this work is to appear at\n  the 48th IEEE International Symposium on Computer Architecture (ISCA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  True random number generators (TRNG) sample random physical processes to\ncreate large amounts of random numbers for various use cases, including\nsecurity-critical cryptographic primitives, scientific simulations, machine\nlearning applications, and even recreational entertainment. Unfortunately, not\nevery computing system is equipped with dedicated TRNG hardware, limiting the\napplication space and security guarantees for such systems. To open the\napplication space and enable security guarantees for the overwhelming majority\nof computing systems that do not necessarily have dedicated TRNG hardware, we\ndevelop QUAC-TRNG.\n  QUAC-TRNG exploits the new observation that a carefully-engineered sequence\nof DRAM commands activates four consecutive DRAM rows in rapid succession. This\nQUadruple ACtivation (QUAC) causes the bitline sense amplifiers to\nnon-deterministically converge to random values when we activate four rows that\nstore conflicting data because the net deviation in bitline voltage fails to\nmeet reliable sensing margins.\n  We experimentally demonstrate that QUAC reliably generates random values\nacross 136 commodity DDR4 DRAM chips from one major DRAM manufacturer. We\ndescribe how to develop an effective TRNG (QUAC-TRNG) based on QUAC. We\nevaluate the quality of our TRNG using NIST STS and find that QUAC-TRNG\nsuccessfully passes each test. Our experimental evaluations show that QUAC-TRNG\ngenerates true random numbers with a throughput of 3.44 Gb/s (per DRAM\nchannel), outperforming the state-of-the-art DRAM-based TRNG by 15.08x and\n1.41x for basic and throughput-optimized versions, respectively. We show that\nQUAC-TRNG utilizes DRAM bandwidth better than the state-of-the-art, achieving\nup to 2.03x the throughput of a throughput-optimized baseline when scaling bus\nfrequencies to 12 GT/s.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 06:58:46 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 11:17:00 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Olgun", "Ataberk", ""], ["Patel", "Minesh", ""], ["Ya\u011fl\u0131k\u00e7\u0131", "A. Giray", ""], ["Luo", "Haocong", ""], ["Kim", "Jeremie S.", ""], ["Bostanc\u0131", "Nisa", ""], ["Vijaykumar", "Nandita", ""], ["Ergin", "O\u011fuz", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.09163", "submitter": "Hongxiang Fan", "authors": "Hongxiang Fan, Martin Ferianc, Miguel Rodrigues, Hongyu Zhou, Xinyu\n  Niu and Wayne Luk", "title": "High-Performance FPGA-based Accelerator for Bayesian Neural Networks", "comments": "Design Automation Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NNs) have demonstrated their potential in a wide range of\napplications such as image recognition, decision making or recommendation\nsystems. However, standard NNs are unable to capture their model uncertainty\nwhich is crucial for many safety-critical applications including healthcare and\nautonomous vehicles. In comparison, Bayesian neural networks (BNNs) are able to\nexpress uncertainty in their prediction via a mathematical grounding.\nNevertheless, BNNs have not been as widely used in industrial practice, mainly\nbecause of their expensive computational cost and limited hardware performance.\nThis work proposes a novel FPGA-based hardware architecture to accelerate BNNs\ninferred through Monte Carlo Dropout. Compared with other state-of-the-art BNN\naccelerators, the proposed accelerator can achieve up to 4 times higher energy\nefficiency and 9 times better compute efficiency. Considering partial Bayesian\ninference, an automatic framework is proposed, which explores the trade-off\nbetween hardware and algorithmic performance. Extensive experiments are\nconducted to demonstrate that our proposed framework can effectively find the\noptimal points in the design space.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:20:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:01:12 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Fan", "Hongxiang", ""], ["Ferianc", "Martin", ""], ["Rodrigues", "Miguel", ""], ["Zhou", "Hongyu", ""], ["Niu", "Xinyu", ""], ["Luk", "Wayne", ""]]}, {"id": "2105.09187", "submitter": "Adri\\'an Castell\\'o", "authors": "Adri\\'an Castell\\'o, Sergio Barrachina, Manuel F. Dolz, Enrique S.\n  Quintana-Ort\\'i, Pau San Juan", "title": "High performance and energy efficient inference for deep learning on ARM\n  processors", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We evolve PyDTNN, a framework for distributed parallel training of Deep\nNeural Networks (DNNs), into an efficient inference tool for convolutional\nneural networks. Our optimization process on multicore ARM processors involves\nseveral high-level transformations of the original framework, such as the\ndevelopment and integration of Cython routines to exploit thread-level\nparallelism; the design and development of micro-kernels for the matrix\nmultiplication, vectorized with ARMs NEON intrinsics, that can accommodate\nlayer fusion; and the appropriate selection of several cache configuration\nparameters tailored to the memory hierarchy of the target ARM processors. Our\nexperiments evaluate both inference throughput (measured in processed images/s)\nand inference latency (i.e., time-to-response) as well as energy consumption\nper image when varying the level of thread parallelism and the processor power\nmodes. The experiments with the new inference engine are reported for the\nResNet50 v1.5 model on the ImageNet dataset from the MLPerf suite using the ARM\nv8.2 cores in the NVIDIA Jetson AGX Xavier board. These results show superior\nperformance compared with the well-spread TFLite from Google and slightly\ninferior results when compared with ArmNN, the native library from ARM for DNN\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:05:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Castell\u00f3", "Adri\u00e1n", ""], ["Barrachina", "Sergio", ""], ["Dolz", "Manuel F.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Juan", "Pau San", ""]]}, {"id": "2105.09282", "submitter": "Ganapati Bhat", "authors": "Aryan Deshwal, Syrine Belakaria, Ganapati Bhat, Janardhan Rao Doppa,\n  Partha Pratim Pande", "title": "Learning Pareto-Frontier Resource Management Policies for Heterogeneous\n  SoCs: An Information-Theoretic Approach", "comments": "To be published in proceedings DAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile system-on-chips (SoCs) are growing in their complexity and\nheterogeneity (e.g., Arm's Big-Little architecture) to meet the needs of\nemerging applications, including games and artificial intelligence. This makes\nit very challenging to optimally manage the resources (e.g., controlling the\nnumber and frequency of different types of cores) at runtime to meet the\ndesired trade-offs among multiple objectives such as performance and energy.\nThis paper proposes a novel information-theoretic framework referred to as\nPaRMIS to create Pareto-optimal resource management policies for given target\napplications and design objectives. PaRMIS specifies parametric policies to\nmanage resources and learns statistical models from candidate policy evaluation\ndata in the form of target design objective values. The key idea is to select a\ncandidate policy for evaluation in each iteration guided by statistical models\nthat maximize the information gain about the true Pareto front. Experiments on\na commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts\nand is easily usable to optimize complex objectives (e.g., performance per\nWatt) when compared to prior methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:14:52 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Deshwal", "Aryan", ""], ["Belakaria", "Syrine", ""], ["Bhat", "Ganapati", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""]]}, {"id": "2105.09564", "submitter": "Yang Wang", "authors": "Yang Wang, Chen Zhang, Zhiqiang Xie, Cong Guo, Yunxin Liu, Jingwen\n  Leng", "title": "Dual-side Sparse Tensor Core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Leveraging sparsity in deep neural network (DNN) models is promising for\naccelerating model inference. Yet existing GPUs can only leverage the sparsity\nfrom weights but not activations, which are dynamic, unpredictable, and hence\nchallenging to exploit. In this work, we propose a novel architecture to\nefficiently harness the dual-side sparsity (i.e., weight and activation\nsparsity). We take a systematic approach to understand the (dis)advantages of\nprevious sparsity-related architectures and propose a novel, unexplored\nparadigm that combines outer-product computation primitive and bitmap-based\nencoding format. We demonstrate the feasibility of our design with minimal\nchanges to the existing production-scale inner-product-based Tensor Core. We\npropose a set of novel ISA extensions and co-design the matrix-matrix\nmultiplication and convolution algorithms, which are the two dominant\ncomputation patterns in today's DNN models, to exploit our new dual-side sparse\nTensor Core. Our evaluation shows that our design can fully unleash the\ndual-side DNN sparsity and improve the performance by up to one order of\nmagnitude with \\hl{small} hardware overhead.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 07:36:16 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wang", "Yang", ""], ["Zhang", "Chen", ""], ["Xie", "Zhiqiang", ""], ["Guo", "Cong", ""], ["Liu", "Yunxin", ""], ["Leng", "Jingwen", ""]]}, {"id": "2105.09666", "submitter": "Christian Pilato", "authors": "Christian Pilato, Luca Collini, Luca Cassano, Donatella Sciuto,\n  Siddharth Garg, Ramesh Karri", "title": "On the Optimization of Behavioral Logic Locking for High-Level Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The globalization of the electronics supply chain is requiring effective\nmethods to thwart reverse engineering and IP theft. Logic locking is a\npromising solution but there are still several open concerns. Even when applied\nat high level of abstraction, logic locking leads to large overhead without\nguaranteeing that the obfuscation metric is actually maximized. We propose a\nframework to optimize the use of behavioral logic locking for a given security\nmetric. We explore how to apply behavioral logic locking techniques during the\nHLS of IP cores. Operating on the chip behavior, our method is compatible with\ncommercial HLS tools, complementing existing industrial design flows. We offer\na framework where the designer can implement different meta-heuristics to\nexplore the design space and select where to apply logic locking. Our method\noptimizes a given security metric better than complete obfuscation, allows us\nto 1) obtain better protection, 2) reduce the obfuscation cost.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:53:20 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Pilato", "Christian", ""], ["Collini", "Luca", ""], ["Cassano", "Luca", ""], ["Sciuto", "Donatella", ""], ["Garg", "Siddharth", ""], ["Karri", "Ramesh", ""]]}, {"id": "2105.09696", "submitter": "Samuel Pagliarini", "authors": "Mateus Saquetti, Raphael M. Brum, Bruno Zatt, Samuel Pagliarini,\n  Weverton Cordeiro, Jose R. Azambuja", "title": "A Terabit Hybrid FPGA-ASIC Platform for Switch Virtualization", "comments": "ISVLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The roll-out of technologies like 5G and the need for multi-terabit bandwidth\nin backbone networks requires networking companies to make significant\ninvestments to keep up with growing service demands. For lower capital\nexpenditure and faster time-to-market, companies can resort to\nanything-as-a-service providers to lease virtual resources. Nevertheless,\nexisting virtualization technologies are still lagging behind next-generation\nnetworks' requirements. This paper breaks the terabit barrier by introducing a\nhybrid FPGA-ASIC architecture to virtualize programmable forwarding planes. In\ncontrast to existing solutions, our architecture involves an ASIC that\nmultiplexes network flows between programmable virtual switches running in an\nFPGA capable of full and partial reconfiguration, enabling virtual switch\nhot-swapping. Our evaluation shows the feasibility of a switch virtualization\narchitecture capable of achieving a combined throughput of 3.2 Tbps by having\nup to 26 virtual switch instances in parallel with low resource occupation\noverhead.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:17:49 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Saquetti", "Mateus", ""], ["Brum", "Raphael M.", ""], ["Zatt", "Bruno", ""], ["Pagliarini", "Samuel", ""], ["Cordeiro", "Weverton", ""], ["Azambuja", "Jose R.", ""]]}, {"id": "2105.10397", "submitter": "R\\'emi Dulong", "authors": "R\\'emi Dulong, Rafael Pires, Andreia Correia, Valerio Schiavoni, Pedro\n  Ramalhete, Pascal Felber, Ga\\\"el Thomas", "title": "NVCache: A Plug-and-Play NVMM-based I/O Booster for Legacy Systems", "comments": "13 pages, 7 figures, to be published in the 51th IEEE/IFIP\n  International Conference on Dependable Systems and Networks (DSN 21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces NVCache, an approach that uses a non-volatile main\nmemory (NVMM) as a write cache to improve the write performance of legacy\napplications. We compare NVCache against file systems tailored for NVMM\n(Ext4-DAX and NOVA) and with I/O-heavy applications (SQLite, RocksDB). Our\nevaluation shows that NVCache reaches the performance level of the existing\nstate-of-the-art systems for NVMM, but without their limitations: NVCache does\nnot limit the size of the stored data to the size of the NVMM, and works\ntransparently with unmodified legacy applications, providing additional\npersistence guarantees even when their source code is not available.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:08:10 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Dulong", "R\u00e9mi", ""], ["Pires", "Rafael", ""], ["Correia", "Andreia", ""], ["Schiavoni", "Valerio", ""], ["Ramalhete", "Pedro", ""], ["Felber", "Pascal", ""], ["Thomas", "Ga\u00ebl", ""]]}, {"id": "2105.10427", "submitter": "Saurabh Jaiswal", "authors": "Saurabh Jaiswal, Shailendra Kumar Gupta, Soumya Soubhagya Dandapat", "title": "Prefetcher-based DRAM Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancement in Processor technology has made it easy to handle data-intensive\nworkloads, but limiting main memory advances has created performance\nbottlenecks. In DRAM, there have been improvements in DRAM access latency as\nwell as reduction in cost-per-bit with the increase in cell density. But still\nDRAM data transfer rate lags behind the processing speed of the current\ngeneration processors. As Memory advancements based on hardware have been\nprogressing at a slower pace, to cope up with High-end Processors,\nArchitectural level advancements such as Prediction techniques, Replacement\npolicies, etc are the major subject.\n  In the recent field of research, Data prediction is a sought out topic as\ncorrect prediction can boost performance by decreasing the amount of excess\nmemory access by predicting data beforehand using data access trends and\nbehaviors. Though prediction techniques have been implemented at most of the\nComputer Architecture, We propose implementing data prediction in DRAM level\narchitectures like TL-DRAM and CROW. Both of these method distributes the DRAM\ninto different parts which contain a smaller section which is faster and larger\nsection which contains the bulk of data but is comparatively slower. We wish to\nuse data prediction in between these sections of memory to have predicted data\ntransferred to the faster sections to improve the overall performance by\nreducing the memory access time.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:53:36 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Jaiswal", "Saurabh", ""], ["Gupta", "Shailendra Kumar", ""], ["Dandapat", "Soumya Soubhagya", ""]]}, {"id": "2105.10554", "submitter": "Sachin Sapatnekar", "authors": "Sudipta Mondal, Susmita Dey Manasi, Kishor Kunal, and Sachin S.\n  Sapatnekar", "title": "GNNIE: GNN Inference Engine with Load-balancing and Graph-Specific\n  Caching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis engines based on Graph Neural Networks (GNNs) are vital for many\nreal-world problems that model relationships using large graphs. Challenges for\na GNN hardware platform include the ability to (a) host a variety of GNNs, (b)\nhandle high sparsity in input node feature vectors and the graph adjacency\nmatrix and the accompanying random memory access patterns, and (c) maintain\nload-balanced computation in the face of uneven workloads induced by high\nsparsity and power-law vertex degree distributions in real datasets. The\nproposes GNNIE, an accelerator designed to run a broad range of GNNs. It\ntackles workload imbalance by (i) splitting node feature operands into blocks,\n(ii) reordering and redistributing computations, and (iii) using a flexible MAC\narchitecture with low communication overheads among the processing elements. In\naddition, it adopts a graph partitioning scheme and a graph-specific caching\npolicy that efficiently uses off-chip memory bandwidth that is well suited to\nthe characteristics of real-world graphs. Random memory access effects are\nmitigated by partitioning and degree-aware caching to enable the reuse of\nhigh-degree vertices. GNNIE achieves average speedups of over 8890x over a CPU\nand 295x over a GPU over multiple datasets on graph attention networks (GATs),\ngraph convolutional networks (GCNs), GraphSAGE, GINConv, and DiffPool, Compared\nto prior approaches, GNNIE achieves an average speedup of 9.74x over HyGCN for\nGCN, GraphSAGE, and GINConv; HyGCN cannot implement GATs. GNNIE achieves an\naverage speedup of 2.28x over AWB-GCN (which runs only GCNs), despite using\n3.4x fewer processing units.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:07:14 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mondal", "Sudipta", ""], ["Manasi", "Susmita Dey", ""], ["Kunal", "Kishor", ""], ["Sapatnekar", "Sachin S.", ""]]}, {"id": "2105.10712", "submitter": "Harsh Tataria Dr.", "authors": "Harsh Tataria and Erik L. Bengtsson and Ove Edfors and Fredrik\n  Tufvesson", "title": "27.5-29.5 GHz Switched Array Sounder for Dynamic Channel\n  Characterization: Design, Implementation and Measurements", "comments": "IEEE Transactions on Wireless Communications, 30 pages, 16 figures, 2\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pre-requisite for the design of wireless systems is the understanding of\nthe propagation channel. While a wealth of propagation knowledge exists for\nbands below 6 GHz, the same can not be said for bands approaching\nmillimeter-wave frequencies. In this paper, we present the design,\nimplementation and measurement-based verification of a re-configurable\n27.5-29.5 GHz channel sounder for measuring dynamic directional channels. Based\non the switched array principle, our design is capable of characterizing\n128$\\times$256 dual-polarized channels with snapshot times of around 600 ms.\nThis is in sharp contrast to measurement times on the order of tens-of-minutes\nwith rotating horn antenna sounders. Our design lends itself to high angular\nresolution at both link ends with calibrated antenna arrays sampled at\n2$^\\circ$ and 5$^\\circ$ intervals in the azimuth and elevation domains. This is\ncomplemented with a bandwidth of up to 2 GHz, enabling nanosecond-level delay\nresolution. The short measurement times and stable radio frequency design\nfacilitates real-time processing and averaging of the received wavefronts to\ngain measurement signal-to-noise ratio and dynamic range. After disclosing the\nsounder design and implementation, we demonstrate its capabilities by\npresenting dynamic and static measurements at 28 GHz over a 1 GHz bandwidth in\nan office corridor environment.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 12:32:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tataria", "Harsh", ""], ["Bengtsson", "Erik L.", ""], ["Edfors", "Ove", ""], ["Tufvesson", "Fredrik", ""]]}, {"id": "2105.10852", "submitter": "Mouhamed Abdulla Ph.D.", "authors": "Alvin Ramoutar and Zohreh Motamedi and Mouhamed Abdulla", "title": "Latency of Concatenating Unlicensed LPWAN with Cellular IoT: An\n  Experimental QoE Study", "comments": "Experimental dataset is openly available here:\n  https://dx.doi.org/10.21227/zzax-g919", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR cs.IT cs.PF math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing low-power wide-area network (LPWAN) solutions that are efficient\nto adopt, deploy and maintain are vital for smart cities. The poor\nquality-of-service of unlicensed LPWAN, and the high service cost of\nLTE-M/NB-IoT are key disadvantages of these technologies. Concatenating\nunlicensed with licensed LPWANs can overcome these limitations and harness\ntheir benefits. However, a concatenated LPWAN architecture will inevitably\nresult in excess latency which may impact users' quality-of-experience (QoE).\nTo evaluate the real-life feasibility of this system, we first propose a\nconcatenated LPWAN architecture and experimentally measure the statistics of\nend-to-end (E2E) latencies. The concatenated delay margin is determined by\nbenchmarking the latencies with different LPWAN architecture schemes, namely\nwith unlicensed IoT (standalone LoRa), cellular IoT (standalone LTE-M), and\nconcatenated IoT (LoRa interfaced with LTE-M). Through extensive experimental\nmeasurement campaigns of 30,000 data points of E2E latencies, we show that the\nexcess delay due to LPWAN interfacing introduces on average less than 300\nmilliseconds. The proof-of-concept results suggest that the latency for\nconcatenating unlicensed LPWAN with cellular IoT is negligible for smart city\nuse cases where human perception and decision making is in the loop.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 03:53:37 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 19:06:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ramoutar", "Alvin", ""], ["Motamedi", "Zohreh", ""], ["Abdulla", "Mouhamed", ""]]}, {"id": "2105.11010", "submitter": "Gil Shomron", "authors": "Gil Shomron, Freddy Gabbay, Samer Kurzum, Uri Weiser", "title": "Post-Training Sparsity-Aware Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization is a technique used in deep neural networks (DNNs) to increase\nexecution performance and hardware efficiency. Uniform post-training\nquantization (PTQ) methods are common, since they can be implemented\nefficiently in hardware and do not require extensive hardware resources or a\ntraining set. Mapping FP32 models to INT8 using uniform PTQ yields models with\nnegligible accuracy degradation; however, reducing precision below 8 bits with\nPTQ is challenging, as accuracy degradation becomes noticeable, due to the\nincrease in quantization noise. In this paper, we propose a sparsity-aware\nquantization (SPARQ) method, in which the unstructured and dynamic activation\nsparsity is leveraged in different representation granularities. 4-bit\nquantization, for example, is employed by dynamically examining the bits of\n8-bit values and choosing a window of 4 bits, while first skipping zero-value\nbits. Moreover, instead of quantizing activation-by-activation to 4 bits, we\nfocus on pairs of 8-bit activations and examine whether one of the two is equal\nto zero. If one is equal to zero, the second can opportunistically use the\nother's 4-bit budget; if both do not equal zero, then each is dynamically\nquantized to 4 bits, as described. SPARQ achieves minor accuracy degradation,\n2x speedup over widely used hardware architectures, and a practical hardware\nimplementation. The code is available at https://github.com/gilshm/sparq.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 20:12:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shomron", "Gil", ""], ["Gabbay", "Freddy", ""], ["Kurzum", "Samer", ""], ["Weiser", "Uri", ""]]}, {"id": "2105.11754", "submitter": "Chenhao Liu", "authors": "Chenhao Liu, Zhiyuan Shao, Zeke Wang, Kexin Li, Minkang Wu, Jiajie\n  Chen, Xiaofei Liao, Hai Jin", "title": "ScalaBFS: A Scalable BFS Accelerator on HBM-Enhanced FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High Bandwidth Memory (HBM) provides massive aggregated memory bandwidth by\nexposing multiple memory channels to the processing units. To achieve high\nperformance, an accelerator built on top of an FPGA configured with HBM (i.e.,\nFPGA-HBM platform) needs to scale its performance according to the available\nmemory channels. In this paper, we propose an accelerator for BFS\n(Breadth-First Search) algorithm, named as ScalaBFS, that builds multiple\nprocessing elements to sufficiently exploit the high bandwidth of HBM to\nimprove efficiency. We implement the prototype system of ScalaBFS and conduct\nBFS in both real-world and synthetic scale-free graphs on Xilinx Alveo U280\nFPGA card real hardware. The experimental results show that ScalaBFS scales its\nperformance almost linearly according to the available memory pseudo channels\n(PCs) from the HBM2 subsystem of U280. By fully using the 32 PCs and building\n64 processing elements (PEs) on U280, ScalaBFS achieves a performance up to\n19.7 GTEPS (Giga Traversed Edges Per Second). When conducting BFS in sparse\nreal-world graphs, ScalaBFS achieves equivalent GTEPS to Gunrock running on the\nstate-of-art Nvidia V100 GPU that features 64-PC HBM2 (twice memory bandwidth\nthan U280).\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:43:22 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Liu", "Chenhao", ""], ["Shao", "Zhiyuan", ""], ["Wang", "Zeke", ""], ["Li", "Kexin", ""], ["Wu", "Minkang", ""], ["Chen", "Jiajie", ""], ["Liao", "Xiaofei", ""], ["Jin", "Hai", ""]]}, {"id": "2105.12676", "submitter": "Zhaoxia Deng", "authors": "Zhaoxia (Summer) Deng, Jongsoo Park, Ping Tak Peter Tang, Haixin Liu,\n  Jie (Amy) Yang, Hector Yuen, Jianyu Huang, Daya Khudia, Xiaohan Wei, Ellie\n  Wen, Dhruv Choudhary, Raghuraman Krishnamoorthi, Carole-Jean Wu, Satish\n  Nadathur, Changkyu Kim, Maxim Naumov, Sam Naghshineh, Mikhail Smelyanskiy", "title": "Low-Precision Hardware Architectures Meet Recommendation Model Inference\n  at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.IR cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous success of machine learning (ML) and the unabated growth in ML\nmodel complexity motivated many ML-specific designs in both CPU and accelerator\narchitectures to speed up the model inference. While these architectures are\ndiverse, highly optimized low-precision arithmetic is a component shared by\nmost. Impressive compute throughputs are indeed often exhibited by these\narchitectures on benchmark ML models. Nevertheless, production models such as\nrecommendation systems important to Facebook's personalization services are\ndemanding and complex: These systems must serve billions of users per month\nresponsively with low latency while maintaining high prediction accuracy,\nnotwithstanding computations with many tens of billions parameters per\ninference. Do these low-precision architectures work well with our production\nrecommendation systems? They do. But not without significant effort. We share\nin this paper our search strategies to adapt reference recommendation models to\nlow-precision hardware, our optimization of low-precision compute kernels, and\nthe design and development of tool chain so as to maintain our models' accuracy\nthroughout their lifespan during which topic trends and users' interests\ninevitably evolve. Practicing these low-precision technologies helped us save\ndatacenter capacities while deploying models with up to 5X complexity that\nwould otherwise not be deployed on traditional general-purpose CPUs. We believe\nthese lessons from the trenches promote better co-design between hardware\narchitecture and software engineering and advance the state of the art of ML in\nindustry.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:42:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Zhaoxia", "", "", "Summer"], ["Deng", "", "", "Amy"], ["Park", "Jongsoo", "", "Amy"], ["Tang", "Ping Tak Peter", "", "Amy"], ["Liu", "Haixin", "", "Amy"], ["Jie", "", "", "Amy"], ["Yang", "", ""], ["Yuen", "Hector", ""], ["Huang", "Jianyu", ""], ["Khudia", "Daya", ""], ["Wei", "Xiaohan", ""], ["Wen", "Ellie", ""], ["Choudhary", "Dhruv", ""], ["Krishnamoorthi", "Raghuraman", ""], ["Wu", "Carole-Jean", ""], ["Nadathur", "Satish", ""], ["Kim", "Changkyu", ""], ["Naumov", "Maxim", ""], ["Naghshineh", "Sam", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2105.12781", "submitter": "Supreeth Mysore Shivanandamurthy", "authors": "Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad\n  Salehi", "title": "ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for\n  In-DRAM CNN Processing", "comments": "Preprint accepted in ISVLSI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the rapidly growing use of Convolutional Neural Networks (CNNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator designs for CNN inference and training have\nbeen proposed recently. In this paper, we present ATRIA, a novel bit-pArallel\nsTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and\nhigh-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM\ncell arrays to implement bit-parallel stochastic arithmetic based acceleration\nof multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly\nimproves the latency, throughput, and efficiency of processing CNN inferences\nby performing 16 MAC operations in only five consecutive memory operation\ncycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to\ncompare its performance with five state-of-the-art in-DRAM CNN accelerators\nfrom prior work. The results of our analysis show that ATRIA exhibits only 3.5%\ndrop in CNN inference accuracy and still achieves improvements of up to 3.2x in\nframes-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to\nthe best-performing in-DRAM accelerator from prior work.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:36:01 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shivanandamurthy", "Supreeth Mysore", ""], ["Thakkar", "Ishan. G.", ""], ["Salehi", "Sayed Ahmad", ""]]}, {"id": "2105.12839", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, Jo\\~ao\n  Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose,\n  Juan G\\'omez Luna, and Onur Mutlu", "title": "SIMDRAM: An End-to-End Framework for Bit-Serial SIMD Computing in DRAM", "comments": "This is an extended version of the paper that appeared at ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Processing-using-DRAM has been proposed for a limited set of basic operations\n(i.e., logic operations, addition). However, in order to enable full adoption\nof processing-using-DRAM, it is necessary to provide support for more complex\noperations. In this paper, we propose SIMDRAM, a flexible general-purpose\nprocessing-using-DRAM framework that (1) enables the efficient implementation\nof complex operations, and (2) provides a flexible mechanism to support the\nimplementation of arbitrary user-defined operations. The SIMDRAM framework\ncomprises three key steps. The first step builds an efficient MAJ/NOT\nrepresentation of a given desired operation. The second step allocates DRAM\nrows that are reserved for computation to the operation's input and output\noperands, and generates the required sequence of DRAM commands to perform the\nMAJ/NOT implementation of the desired operation in DRAM. The third step uses\nthe SIMDRAM control unit located inside the memory controller to manage the\ncomputation of the operation from start to end, by executing the DRAM commands\ngenerated in the second step of the framework. We design the hardware and ISA\nsupport for SIMDRAM framework to (1) address key system integration challenges,\nand (2) allow programmers to employ new SIMDRAM operations without hardware\nchanges.\n  We evaluate SIMDRAM for reliability, area overhead, throughput, and energy\nefficiency using a wide range of operations and seven real-world applications\nto demonstrate SIMDRAM's generality. Using 16 DRAM banks, SIMDRAM provides (1)\n88x and 5.8x the throughput, and 257x and 31x the energy efficiency, of a CPU\nand a high-end GPU, respectively, over 16 operations; (2) 21x and 2.1x the\nperformance of the CPU and GPU, over seven real-world applications. SIMDRAM\nincurs an area overhead of only 0.2% in a high-end CPU.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:06:55 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 13:48:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hajinazar", "Nastaran", ""], ["Oliveira", "Geraldo F.", ""], ["Gregorio", "Sven", ""], ["Ferreira", "Jo\u00e3o", ""], ["Ghiasi", "Nika Mansouri", ""], ["Patel", "Minesh", ""], ["Alser", "Mohammed", ""], ["Ghose", "Saugata", ""], ["Luna", "Juan G\u00f3mez", ""], ["Mutlu", "Onur", ""]]}, {"id": "2105.12842", "submitter": "Dan Zhang", "authors": "Dan Zhang, Safeen Huda, Ebrahim Songhori, Quoc Le, Anna Goldie, Azalia\n  Mirhoseini", "title": "A Full-stack Accelerator Search Technique for Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly-changing ML model landscape presents a unique opportunity for\nbuilding hardware accelerators optimized for specific datacenter-scale\nworkloads. We propose Full-stack Accelerator Search Technique (FAST), a\nhardware accelerator search framework that defines a broad optimization\nenvironment covering key design decisions within the hardware-software stack,\nincluding hardware datapath, software scheduling, and compiler passes such as\noperation fusion and tensor padding. Although FAST can be used on any number\nand type of deep learning workload, in this paper we focus on optimizing for a\nsingle or small set of vision models, resulting in significantly faster and\nmore power-efficient designs relative to a general purpose ML accelerator. When\nevaluated on EfficientNet, ResNet50v2, and OCR inference performance relative\nto a TPU-v3, designs generated by FAST optimized for single workloads can\nimprove Perf/TDP (peak power) by over 6x in the best case and 4x on average. On\na limited workload subset, FAST improves Perf/TDP 2.85x on average, with a\nreduction to 2.35x for a single design optimized over the set of workloads. In\naddition, we demonstrate a potential 1.8x speedup opportunity for TPU-v3 with\nimproved scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:10:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Dan", ""], ["Huda", "Safeen", ""], ["Songhori", "Ebrahim", ""], ["Le", "Quoc", ""], ["Goldie", "Anna", ""], ["Mirhoseini", "Azalia", ""]]}, {"id": "2105.12858", "submitter": "Jeff Setter", "authors": "Qiaoyi Liu, Dillon Huff, Jeff Setter, Maxwell Strange, Kathleen Feng,\n  Kavya Sreedhar, Ziheng Wang, Keyi Zhang, Mark Horowitz, Priyanka Raina, and\n  Fredrik Kjolstad", "title": "Compiling Halide Programs to Push-Memory Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image processing and machine learning applications benefit tremendously from\nhardware acceleration, but existing compilers target either FPGAs, which\nsacrifice power and performance for flexible hardware, or ASICs, which rapidly\nbecome obsolete as applications change. Programmable domain-specific\naccelerators have emerged as a promising middle-ground between these two\nextremes, but such architectures have traditionally been difficult compiler\ntargets.\n  The main obstacle is that these accelerators often use a different memory\nabstraction than CPUs and GPUs: push memories that send a data stream from one\ncomputation kernel to other kernels, possibly reordered. To address the\ncompilation challenges caused by push memories, we propose that the\nrepresentation of memory in the middle and backend of the compiler be altered\nto combine storage with address generation and control logic in a single\nstructure -- a unified buffer. We show that this compiler abstraction can be\nimplemented efficiently on a programmable accelerator, and design a memory\nmapping algorithm that combines polyhedral analysis and software vectorization\ntechniques to target our accelerator.\n  Our evaluation shows that the compiler supports programmability while\nmaintaining high performance. It can compile a wide range of image processing\nand machine learning applications to our accelerator with 4.7x better runtime\nand 4.3x better energy-efficiency as compared to an FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:52:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liu", "Qiaoyi", ""], ["Huff", "Dillon", ""], ["Setter", "Jeff", ""], ["Strange", "Maxwell", ""], ["Feng", "Kathleen", ""], ["Sreedhar", "Kavya", ""], ["Wang", "Ziheng", ""], ["Zhang", "Keyi", ""], ["Horowitz", "Mark", ""], ["Raina", "Priyanka", ""], ["Kjolstad", "Fredrik", ""]]}, {"id": "2105.13258", "submitter": "Yujun Lin", "authors": "Yujun Lin, Mengtian Yang and Song Han", "title": "NAAS: Neural Accelerator Architecture Search", "comments": "Accepted by DAC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven, automatic design space exploration of neural accelerator\narchitecture is desirable for specialization and productivity. Previous\nframeworks focus on sizing the numerical architectural hyper-parameters while\nneglect searching the PE connectivities and compiler mappings. To tackle this\nchallenge, we propose Neural Accelerator Architecture Search (NAAS) which\nholistically searches the neural network architecture, accelerator\narchitecture, and compiler mapping in one optimization loop. NAAS composes\nhighly matched architectures together with efficient mapping. As a data-driven\napproach, NAAS rivals the human design Eyeriss by 4.4x EDP reduction with 2.7%\naccuracy improvement on ImageNet under the same computation resource, and\noffers 1.4x to 3.5x EDP reduction than only sizing the architectural\nhyper-parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:56:41 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lin", "Yujun", ""], ["Yang", "Mengtian", ""], ["Han", "Song", ""]]}, {"id": "2105.13262", "submitter": "Harideep Nair", "authors": "Harideep Nair, John Paul Shen and James E. Smith", "title": "A Microarchitecture Implementation Framework for Online Learning with\n  Temporal Neural Networks", "comments": "To be published in ISVLSI 2021. arXiv admin note: substantial text\n  overlap with arXiv:2009.00457", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Neural Networks (TNNs) are spiking neural networks that use time as\na resource to represent and process information, similar to the mammalian\nneocortex. In contrast to compute-intensive deep neural networks that employ\nseparate training and inference phases, TNNs are capable of extremely efficient\nonline incremental/continual learning and are excellent candidates for building\nedge-native sensory processing units. This work proposes a microarchitecture\nframework for implementing TNNs using standard CMOS. Gate-level implementations\nof three key building blocks are presented: 1) multi-synapse neurons, 2)\nmulti-neuron columns, and 3) unsupervised and supervised online learning\nalgorithms based on Spike Timing Dependent Plasticity (STDP). The proposed\nmicroarchitecture is embodied in a set of characteristic scaling equations for\nassessing the gate count, area, delay and power for any TNN design.\nPost-synthesis results (in 45nm CMOS) for the proposed designs are presented,\nand their online incremental learning capability is demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:59:54 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 21:51:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Nair", "Harideep", ""], ["Shen", "John Paul", ""], ["Smith", "James E.", ""]]}, {"id": "2105.13434", "submitter": "Surya Selvam", "authors": "Surya Selvam, Vinod Ganesan and Pratyush Kumar", "title": "FuSeConv: Fully Separable Convolutions for Fast Inference on Systolic\n  Arrays", "comments": "To appear in the Proceedings of the Design, Automation & Test in\n  Europe (DATE), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Both efficient neural networks and hardware accelerators are being explored\nto speed up DNN inference on edge devices. For example, MobileNet uses\ndepthwise separable convolution to achieve much lower latency, while systolic\narrays provide much higher performance per watt. Interestingly however, the\ncombination of these two ideas is inefficient: The computational patterns of\ndepth-wise separable convolution are not systolic and lack data reuse to\nsaturate the systolic array's constrained dataflow. In this paper, we propose\nFuSeConv (Fully-Separable Convolution) as a drop-in replacement for depth-wise\nseparable convolution. FuSeConv generalizes the decomposition of convolutions\nfully to separable 1D convolutions along spatial and depth dimensions. The\nresultant computation is systolic and efficiently utilizes the systolic array\nwith a slightly modified dataflow. With FuSeConv, we achieve a significant\nspeed-up of 3x-7x with the MobileNet family of networks on a systolic array of\nsize 64x64, with comparable accuracy on the ImageNet dataset. The high speed-up\nmotivates exploration of hardware-aware Neural Operator Search (NOS) in\ncomplement to ongoing efforts on Neural Architecture Search (NAS).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:19:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Selvam", "Surya", ""], ["Ganesan", "Vinod", ""], ["Kumar", "Pratyush", ""]]}, {"id": "2105.13904", "submitter": "Mohammed Elbtity", "authors": "Mohammed Elbtity, Abhishek Singh, Brendan Reidy, Xiaochen Guo, Ramtin\n  Zand", "title": "An In-Memory Analog Computing Co-Processor for Energy-Efficient CNN\n  Inference on Mobile Devices", "comments": "6 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2012.02695, arXiv:2006.01238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an in-memory analog computing (IMAC) architecture\nrealizing both synaptic behavior and activation functions within non-volatile\nmemory arrays. Spin-orbit torque magnetoresistive random-access memory\n(SOT-MRAM) devices are leveraged to realize sigmoidal neurons as well as\nbinarized synapses. First, it is shown the proposed IMAC architecture can be\nutilized to realize a multilayer perceptron (MLP) classifier achieving orders\nof magnitude performance improvement compared to previous mixed-signal and\ndigital implementations. Next, a heterogeneous mixed-signal and mixed-precision\nCPU-IMAC architecture is proposed for convolutional neural networks (CNNs)\ninference on mobile processors, in which IMAC is designed as a co-processor to\nrealize fully-connected (FC) layers whereas convolution layers are executed in\nCPU. Architecture-level analytical models are developed to evaluate the\nperformance and energy consumption of the CPU-IMAC architecture. Simulation\nresults exhibit 6.5% and 10% energy savings for CPU-IMAC based realizations of\nLeNet and VGG CNN models, for MNIST and CIFAR-10 pattern recognition tasks,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 23:01:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Elbtity", "Mohammed", ""], ["Singh", "Abhishek", ""], ["Reidy", "Brendan", ""], ["Guo", "Xiaochen", ""], ["Zand", "Ramtin", ""]]}, {"id": "2105.14156", "submitter": "Kaustubh Shivdikar", "authors": "Kaustubh Shivdikar", "title": "SMASH: Sparse Matrix Atomic Scratchpad Hashing", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.17515.87840", "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse matrices, more specifically SpGEMM kernels, are commonly found in a\nwide range of applications, spanning graph-based path-finding to machine\nlearning algorithms (e.g., neural networks). A particular challenge in\nimplementing SpGEMM kernels has been the pressure placed on DRAM memory. One\napproach to tackle this problem is to use an inner product method for the\nSpGEMM kernel implementation. While the inner product produces fewer\nintermediate results, it can end up saturating the memory bandwidth, given the\nhigh number of redundant fetches of the input matrix elements. Using an outer\nproduct-based SpGEMM kernel can reduce redundant fetches, but at the cost of\nincreased overhead due to extra computation and memory accesses for\nproducing/managing partial products.\n  In this thesis, we introduce a novel SpGEMM kernel implementation based on\nthe row-wise product approach. We leverage atomic instructions to merge\nintermediate partial products as they are generated. The use of atomic\ninstructions eliminates the need to create partial product matrices.\n  To evaluate our row-wise product approach, we map an optimized SpGEMM kernel\nto a custom accelerator designed to accelerate graph-based applications. The\ntargeted accelerator is an experimental system named PIUMA, being developed by\nIntel. PIUMA provides several attractive features, including fast context\nswitching, user-configurable caches, globally addressable memory, non-coherent\ncaches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many\nof the features of the PIUMA fabric.\n  This thesis compares our SpGEMM implementation against prior solutions, all\nmapped to the PIUMA framework. We briefly describe some of the PIUMA\narchitecture features and then delve into the details of our optimized SpGEMM\nkernel. Our SpGEMM kernel can achieve 9.4x speedup as compared to competing\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:22:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shivdikar", "Kaustubh", ""]]}, {"id": "2105.14295", "submitter": "Muhui Jiang", "authors": "Muhui Jiang, Lin Ma, Yajin Zhou, Qiang Liu, Cen Zhang, Zhi Wang, Xiapu\n  Luo, Lei Wu, Kui Ren", "title": "ECMO: Peripheral Transplantation to Rehost Embedded Linux Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic analysis based on the full-system emulator QEMU is widely used for\nvarious purposes. However, it is challenging to run firmware images of embedded\ndevices in QEMU, especially theprocess to boot the Linux kernel (we call this\nprocess rehosting the Linux kernel in this paper.) That's because embedded\ndevices usually use different system-on-chips (SoCs) from multiple vendors\nandonly a limited number of SoCs are currently supported in QEMU.\n  In this work, we propose a technique calledperipheral transplantation. The\nmain idea is to transplant the device drivers of designated peripherals into\nthe Linux kernel binary. By doing so, it can replace the peripherals in the\nkernel that are currently unsupported in QEMU with supported ones, thus making\nthe Linux kernel rehostable. After that, various applications can be built\nupon.\n  We implemented this technique inside a prototype system called ECMO and\napplied it to 824 firmware images, which consist of 17 kernel versions, 37\ndevice models, and 24 vendors. The resultshows that ECMO can successfully\ntransplant peripherals for all the 824 Linux kernels. Among them, 719 kernels\ncan be successfully rehosted, i.e., launching a user-space shell (87.3% success\nrate). The failed cases are mainly because the root file system format (ramfs)\nis not supported by the kernel. We further build three applications, i.e.,\nkernel crash analysis, rootkit forensic analysis, and kernel fuzzing, based on\nthe rehosted kernels to demonstrate the usage scenarios of ECMO.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 13:14:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jiang", "Muhui", ""], ["Ma", "Lin", ""], ["Zhou", "Yajin", ""], ["Liu", "Qiang", ""], ["Zhang", "Cen", ""], ["Wang", "Zhi", ""], ["Luo", "Xiapu", ""], ["Wu", "Lei", ""], ["Ren", "Kui", ""]]}, {"id": "2105.14331", "submitter": "Shriya Gupta", "authors": "Shriya T.P. Gupta, Pablo Linares-Serrano, Basabdatta Sen Bhattacharya,\n  Teresa Serrano-Gotarredona", "title": "Foveal-pit inspired filtering of DVS spike response", "comments": "6 pages, 4 figures, 2 tables. 2021 55th Annual Conference on\n  Information Sciences and Systems (CISS), 2021", "journal-ref": null, "doi": "10.1109/CISS50987.2021.9400245", "report-no": null, "categories": "cs.CV cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present results of processing Dynamic Vision Sensor (DVS)\nrecordings of visual patterns with a retinal model based on foveal-pit inspired\nDifference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying\nnumber of vertical white and black bars of different spatial frequencies moving\nhorizontally at a constant velocity. The output spikes generated by the DVS\nsensor were applied as input to a set of DoG filters inspired by the receptive\nfield structure of the primate visual pathway. In particular, these filters\nmimic the receptive fields of the midget and parasol ganglion cells (spiking\nneurons of the retina) that sub-serve the photo-receptors of the foveal-pit.\nThe features extracted with the foveal-pit model are used for further\nclassification using a spiking convolutional neural network trained with a\nbackpropagation variant adapted for spiking neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 16:01:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gupta", "Shriya T. P.", ""], ["Linares-Serrano", "Pablo", ""], ["Bhattacharya", "Basabdatta Sen", ""], ["Serrano-Gotarredona", "Teresa", ""]]}, {"id": "2105.14401", "submitter": "Lucius Schoenbaum", "authors": "Lucius T. Schoenbaum", "title": "A Tapered Floating Point Extension for the Redundant Signed Radix 2\n  System Using the Canonical Recoding", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A tapered floating point encoding is proposed which uses the redundant signed\nradix 2 system and is based on the canonical recoding. By making use of ternary\ntechnology, the encoding has a dynamic range exceeding that of the\nrecently-proposed Posit number system and the IEEE 754-1985 Standard for\nFloating Point Arithmetic (IEEE-754-1985), and precision equal to or better\nthan that of the IEEE-754-1985 system and the recently proposed Posit system\nwhen equal input sizes are compared. In addition, the encoding is capable of\nsupporting several proposed extensions, including extensions to integers,\nboolean values, complex numbers, higher number systems, low-dimensional\nvectors, and system artifacts such as machine instructions. A detailed analytic\ncomparison is provided between the proposed encoding, the IEEE-754-1985 system,\nand the recently proposed Posit number system.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 01:12:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Schoenbaum", "Lucius T.", ""]]}, {"id": "2105.14442", "submitter": "Chundong Wang", "authors": "Rui Wang, Chundong Wang, Chongnan Ye", "title": "Reuse Distance-based Copy-backs of Clean Cache Lines to Lower-level\n  Caches", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cache plays a critical role in reducing the performance gap between CPU and\nmain memory. A modern multi-core CPU generally employs a multi-level hierarchy\nof caches, through which the most recently and frequently used data are\nmaintained in each core's local private caches while all cores share the\nlast-level cache (LLC). For inclusive caches, clean cache lines replaced in\nhigher-level caches are not necessarily copied back to lower levels, as the\ninclusiveness implies their existences in lower levels. For exclusive and\nnon-inclusive caches that are widely utilized by Intel, AMD, and ARM today,\neither indiscriminately copying back all or none of replaced clean cache lines\nto lower levels raises no violation to exclusiveness and non-inclusiveness\ndefinitions.\n  We have conducted a quantitative study and found that, copying back all or\nnone of clean cache lines to lower-level cache of exclusive caches entails\nsuboptimal performance. The reason is that only a part of cache lines would be\nreused and others turn to be dead in a long run. This observation motivates us\nto selectively copy back some clean cache lines to LLC in an architecture of\nexclusive or non-inclusive caches. We revisit the concept of reuse distance of\ncache lines. In a nutshell, a clean cache line with a shorter reuse distance is\ncopied back to lower-level cache as it is likely to be re-referenced in the\nnear future, while cache lines with much longer reuse distances would be\ndiscarded or sent to memory if they are dirty. We have implemented and\nevaluated our proposal with non-volatile (STT-MRAM) LLC. Experimental results\nwith gem5 and SPEC CPU 2017 benchmarks show that on average our proposal yields\nup to 12.8% higher throughput of IPC (instructions per cycle) than the\nleast-recently-used (LRU) replacement policy with copying back all clean cache\nlines for STT-MRAM LLC.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 06:50:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Rui", ""], ["Wang", "Chundong", ""], ["Ye", "Chongnan", ""]]}]