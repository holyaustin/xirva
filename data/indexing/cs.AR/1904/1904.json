[{"id": "1904.02183", "submitter": "Krishna Gnawali", "authors": "Krishna Prasad Gnawali, Seyed Nima Mozaffari, Spyros Tragoudas", "title": "Low Power Artificial Neural Network Architecture", "comments": "6 pages, 2 figures", "journal-ref": "IEEE VLSI Circuits and Systems Letter Vol.4, Issue.4, Nov. 2018", "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent artificial neural network architectures improve performance and power\ndissipation by leveraging resistive devices to store and multiply synaptic\nweights with input data. Negative and positive synaptic weights are stored on\nthe memristors of a reconfigurable crossbar array (MCA). Existing MCA-based\nneural network architectures use high power consuming voltage converters or\noperational amplifiers to generate the total synaptic current through each\ncolumn of the crossbar array. This paper presents a low power MCA-based\nfeedforward neural network architecture that uses a spintronic device per pair\nof columns to generate the synaptic current for each neuron. It is shown\nexperimentally that the proposed architecture dissipates significantly less\npower compared to existing feedforward memristive neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 18:08:48 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Gnawali", "Krishna Prasad", ""], ["Mozaffari", "Seyed Nima", ""], ["Tragoudas", "Spyros", ""]]}, {"id": "1904.02327", "submitter": "Huazi Zhang", "authors": "Jiajie Tong, Huazi Zhang, Lingchen Huang, Xiaocheng Liu, Jun Wang", "title": "An Asymmetric Adaptive SCL Decoder Hardware for Ultra-Low-Error-Rate\n  Polar Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, Polar codes do not exhibit an error floor under\nsuccessive-cancellation (SC) decoding. In practice, frame error rate (FER) down\nto $10^{-12}$ has not been reported with a real SC list (SCL) decoder hardware.\nThis paper presents an asymmetric adaptive SCL (A2SCL) decoder, implemented in\nreal hardware, for high-throughput and ultra-reliable communications. We\npropose to concatenate multiple SC decoders with an SCL decoder, in which the\nnumbers of SC/SCL decoders are balanced with respect to their area and latency.\nIn addition, a novel unequal-quantization technique is adopted. The two\noptimizations are crucial for improving SCL throughput within limited chip\narea. As an application, we build a link-level FPGA emulation platform to\nmeasure ultra-low FERs of 3GPP NR Polar codes (with parity-check and CRC bits).\nIt is flexible to support all list sizes up to $8$, code lengths up to $1024$\nand arbitrary code rates. With the proposed hardware, decoding speed is 7000\ntimes faster than a CPU core. For the first time, FER as low as $10^{-12}$ is\nmeasured and quantization effect is analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:13:31 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Tong", "Jiajie", ""], ["Zhang", "Huazi", ""], ["Huang", "Lingchen", ""], ["Liu", "Xiaocheng", ""], ["Wang", "Jun", ""]]}, {"id": "1904.03428", "submitter": "Somnath Mazumdar", "authors": "Somnath Mazumdar and Alberto Scionti", "title": "Ring-Mesh: A Scalable and High-Performance Approach for Manycore\n  Accelerators", "comments": "35 pages, Accepted to Journal of Supercomputing", "journal-ref": null, "doi": "10.1007/s11227-019-03072-5", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are increasing number of works addressing the design challenges of\nfast, scalable solutions for the growing number of new type of applications.\nRecently, many of the solutions aimed at improving processing element\ncapabilities to speed up the execution of machine learning application domain.\nHowever, only a few works focused on the interconnection subsystem as a\npotential source of performance improvement. Wrapping many cores together offer\nexcellent parallelism, but it brings other challenges (e.g., adequate\ninterconnections). Scalable, power-aware interconnects are required to support\nsuch a growing number of processing elements, as well as modern applications.\nIn this paper, we propose a scalable and energy efficient Network-on-Chip\narchitecture fusing the advantages of rings as well as the 2D-mesh without\nusing any bridge router to provide high-performance. A dynamic adaptation\nmechanism allows to better adapt to the application requirements. Simulation\nresults show efficient power consumption (up to 141.3% saving for connecting\n1024 cores), 2x (on average) throughput growth with better scalability (up to\n1024 processing elements) compared to popular 2D-mesh while tested in multiple\nstatistical traffic pattern scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 12:23:40 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 18:40:45 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Mazumdar", "Somnath", ""], ["Scionti", "Alberto", ""]]}, {"id": "1904.03756", "submitter": "Issam Damaj", "authors": "Issam Damaj (Rafik Hariri University)", "title": "Higher-Level Hardware Synthesis of The KASUMI Algorithm", "comments": null, "journal-ref": "Jrnl. Comp. Sc. & Tech. Springer. 22(2007) 60-70", "doi": "10.1016/j.advengsoft.2006.01.009", "report-no": null, "categories": "cs.AR cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable Logic Devices (PLDs) continue to grow in size and currently\ncontain several millions of gates. At the same time, research effort is going\ninto higher-level hardware synthesis methodologies for reconfigurable computing\nthat can exploit PLD technology. In this paper, we explore the effectiveness\nand extend one such formal methodology in the design of massively parallel\nalgorithms. We take a step-wise refinement approach to the development of\ncorrect reconfigurable hardware circuits from formal specifications. A\nfunctional programming notation is used for specifying algorithms and for\nreasoning about them. The specifications are realised through the use of a\ncombination of function decomposition strategies, data refinement techniques,\nand off-the-shelf refinements based upon higher-order functions. The\noff-the-shelf refinements are inspired by the operators of Communicating\nSequential Processes (CSP) and map easily to programs in Handel-C (a hardware\ndescription language). The Handel-C descriptions are directly compiled into\nreconfigurable hardware. The practical realisation of this methodology is\nevidenced by a case studying the third generation mobile communication security\nalgorithms. The investigated algorithm is the KASUM} block cipher. In this\npaper, we obtain several hardware implementations with different performance\ncharacteristics by applying different refinements to the algorithm. The\ndeveloped designs are compiled and tested under Celoxica's RC-1000\nreconfigurable computer with its 2 million gates Virtex-E FPGA. Performance\nanalysis and evaluation of these implementations are included.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 22:15:36 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Damaj", "Issam", "", "Rafik Hariri University"]]}, {"id": "1904.04024", "submitter": "Fan Yang", "authors": "Fan Yang, Zhan Wang, Xiaoxiao Ma, Guojun Yuan, Xuejun An", "title": "SwitchAgg:A Further Step Towards In-Network Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed applications adopt a partition/aggregation pattern to\nachieve high performance and scalability. The aggregation process, which\nusually takes a large portion of the overall execution time, incurs large\namount of network traffic and bottlenecks the system performance. To reduce\nnetwork traffic,some researches take advantage of network devices to commit\ninnetwork aggregation. However, these approaches use either special topology or\nmiddle-boxes, which cannot be easily deployed in current datacenters. The\nemerging programmable RMT switch brings us new opportunities to implement\nin-network computation task. However, we argue that the architecture of RMT\nswitch is not suitable for in-network aggregation since it is designed\nprimarily for implementing traditional network functions. In this paper, we\nfirst give a detailed analysis of in-network aggregation, and point out the key\nfactor that affects the data reduction ratio. We then propose SwitchAgg, which\nis an innetwork aggregation system that is compatible with current datacenter\ninfrastructures. We also evaluate the performance improvement we have gained\nfrom SwitchAgg. Our results show that, SwitchAgg can process data aggregation\ntasks at line rate and gives a high data reduction rate, which helps us to cut\ndown network traffic and alleviate pressure on server CPU. In the system\nperformance test, the job-completion-time can be reduced as much as 50%\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 03:09:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yang", "Fan", ""], ["Wang", "Zhan", ""], ["Ma", "Xiaoxiao", ""], ["Yuan", "Guojun", ""], ["An", "Xuejun", ""]]}, {"id": "1904.04782", "submitter": "Jianping Zhu", "authors": "Jianping Zhu, Rui Hou, XiaoFeng Wang, Wenhao Wang, Jiangfeng Cao,\n  Lutan Zhao, Fengkai Yuan, Peinan Li, Zhongpu Wang, Boyan Zhao, Lixin Zhang,\n  Dan Meng", "title": "Enabling Privacy-Preserving, Compute- and Data-Intensive Computing using\n  Heterogeneous Trusted Execution Environment", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an urgent demand for privacy-preserving techniques capable of\nsupporting compute and data intensive (CDI) computing in the era of big data.\nHowever, none of existing TEEs can truly support CDI computing tasks, as CDI\nrequires high throughput accelerators like GPU and TPU but TEEs do not offer\nsecurity protection of such accelerators. This paper present HETEE\n(Heterogeneous TEE), the first design of TEE capable of strongly protecting\nheterogeneous computing with unsecure accelerators. HETEE is uniquely\nconstructed to work with today's servers, and does not require any changes for\nexisting commercial CPUs or accelerators. The key idea of our design runs\nsecurity controller as a stand-alone computing system to dynamically adjust the\nboundary of between secure and insecure worlds through the PCIe switches,\nrendering the control of an accelerator to the host OS when it is not needed\nfor secure computing, and shifting it back when it is. The controller is the\nonly trust unit in the system and it runs the custom OS and accelerator\nruntimes, together with the encryption, authentication and remote attestation\ncomponents. The host server and other computing systems communicate with\ncontroller through an in memory task queue that accommodates the computing\ntasks offloaded to HETEE, in the form of encrypted and signed code and data.\nAlso, HETEE offers a generic and efficient programming model to the host CPU.\nWe have implemented the HETEE design on a hardware prototype system, and\nevaluated it with large-scale Neural Networks inference and training tasks. Our\nevaluations show that HETEE can easily support such secure computing tasks and\nonly incurs a 12.34% throughput overhead for inference and 9.87% overhead for\ntraining on average.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:56:21 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 07:19:40 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Zhu", "Jianping", ""], ["Hou", "Rui", ""], ["Wang", "XiaoFeng", ""], ["Wang", "Wenhao", ""], ["Cao", "Jiangfeng", ""], ["Zhao", "Lutan", ""], ["Yuan", "Fengkai", ""], ["Li", "Peinan", ""], ["Wang", "Zhongpu", ""], ["Zhao", "Boyan", ""], ["Zhang", "Lixin", ""], ["Meng", "Dan", ""]]}, {"id": "1904.04953", "submitter": "Issam Damaj", "authors": "Issam Damaj (Dhofar University)", "title": "High Performance Reconfigurable Computing Systems", "comments": "53 pages, 14 tables, 15 figures", "journal-ref": "Supercomp. Res. Adv. (2008) 116-143", "doi": null, "report-no": null, "categories": "cs.AR cs.GR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress and advancement in electronic chips technology provide a\nvariety of new implementation options for system engineers. The choice varies\nbetween the flexible programs running on a general-purpose processor (GPP) and\nthe fixed hardware implementation using an application specific integrated\ncircuit (ASIC). Many other implementation options present, for instance, a\nsystem with a RISC processor and a DSP core. Other options include graphics\nprocessors and microcontrollers. Specialist processors certainly improve\nperformance over general-purpose ones, but this comes as a quid pro quo for\nflexibility. Combining the flexibility of GPPs and the high performance of\nASICs leads to the introduction of reconfigurable computing (RC) as a new\nimplementation option with a balance between versatility and speed. The focus\nof this chapter is on introducing reconfigurable computers as modern super\ncomputing architectures. The chapter also investigates the main reasons behind\nthe current advancement in the development of RC-systems. Furthermore, a\ntechnical survey of various RC-systems is included laying common grounds for\ncomparisons. In addition, this chapter mainly presents case studies implemented\nunder the MorphoSys RC-system. The selected case studies belong to different\nareas of application, such as, computer graphics and information coding.\nParallel versions of the studied algorithms are developed to match the\ntopologies supported by the MorphoSys. Performance evaluation and results\nanalyses are included for implementations with different characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:24:46 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Damaj", "Issam", "", "Dhofar University"]]}, {"id": "1904.05106", "submitter": "Andreas Bytyn", "authors": "Andreas Bytyn, Rainer Leupers and Gerd Ascheid", "title": "An Application-Specific VLIW Processor with Vector Instruction Set for\n  CNN Acceleration", "comments": "Accepted for publication in the proceedings of the 2019 IEEE\n  International Symposium on Circuits and Systems (ISCAS)", "journal-ref": null, "doi": "10.1109/ISCAS.2019.8702357", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural networks have surpassed classical algorithms in areas\nsuch as object recognition, e.g. in the well-known ImageNet challenge. As a\nresult, great effort is being put into developing fast and efficient\naccelerators, especially for Convolutional Neural Networks (CNNs). In this work\nwe present ConvAix, a fully C-programmable processor, which -- contrary to many\nexisting architectures -- does not rely on a hard-wired array of\nmultiply-and-accumulate (MAC) units. Instead it maps computations onto\nindependent vector lanes making use of a carefully designed vector instruction\nset. The presented processor is targeted towards latency-sensitive applications\nand is capable of executing up to 192 MAC operations per cycle. ConvAix\noperates at a target clock frequency of 400 MHz in 28nm CMOS, thereby offering\nstate-of-the-art performance with proper flexibility within its target domain.\nSimulation results for several 2D convolutional layers from well known CNNs\n(AlexNet, VGG-16) show an average ALU utilization of 72.5% using vector\ninstructions with 16 bit fixed-point arithmetic. Compared to other well-known\ndesigns which are less flexible, ConvAix offers competitive energy efficiency\nof up to 497 GOP/s/W while even surpassing them in terms of area efficiency and\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 10:59:07 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bytyn", "Andreas", ""], ["Leupers", "Rainer", ""], ["Ascheid", "Gerd", ""]]}, {"id": "1904.05279", "submitter": "Mohammad Hemmati", "authors": "Mohammad Hemmati, Vahid Rashtchi, Ahmad Maleki, Siroos Toofan", "title": "A Configurable Memristor-based Finite Impulse Response Filter", "comments": "9 pages, 18 figures, 4 tables, and 8 equations, 44 high quality\n  references, brief biographies of the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main methods to implement FIR filters: software and hardware.\nIn the software method, an FIR filter can be implemented within the processor\nby programming; it uses too much memory and it is extremely time-consuming\nwhile it gives the design more configurability. In most hardware-based\nimplementations of FIR filters, Analog-to-Digital (A/D) and Digital-to-Analog\n(D/A) converters are mandatory and increase the cost. The most important\nadvantage of hardware implementation of a FIR filter is its higher speed\ncompared to its software counterpart. In this work, considering the advantages\nof software and hardware approaches, a method to implement direct form FIR\nfilters using analog components and memristors is proposed. Not only the A/D\nand D/A converters are omitted, but also using memristors avails\nconfigurability. A new circuit is presented to handle negative coefficients of\nthe filter and memristance values are calculated using a heuristic method in\norder to achieve a better accuracy in setting coefficients. Moreover, an\nappropriate sample and delay topology is employed which overcomes the\nlimitations of the previous research in implementation of high-order filters.\nProper operation and usefulness of the proposed structures are all validated\nvia simulation in Cadence.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 16:28:16 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Hemmati", "Mohammad", ""], ["Rashtchi", "Vahid", ""], ["Maleki", "Ahmad", ""], ["Toofan", "Siroos", ""]]}, {"id": "1904.05442", "submitter": "Florian Zaruba", "authors": "Florian Zaruba and Luca Benini", "title": "The Cost of Application-Class Processing: Energy and Performance\n  Analysis of a Linux-ready 1.7GHz 64bit RISC-V Core in 22nm FDSOI Technology", "comments": "11 pages, submitted to IEEE Transaction on Very Large Scale\n  Integration (VLSI) Systems", "journal-ref": null, "doi": "10.1109/TVLSI.2019.2926114", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The open-source RISC-V ISA is gaining traction, both in industry and\nacademia. The ISA is designed to scale from micro-controllers to server-class\nprocessors. Furthermore, openness promotes the availability of various\nopen-source and commercial implementations. Our main contribution in this work\nis a thorough power, performance, and efficiency analysis of the RISC-V ISA\ntargeting baseline \"application class\" functionality, i.e. supporting the Linux\nOS and its application environment based on our open-source single-issue\nin-order implementation of the 64 bit ISA variant (RV64GC) called Ariane. Our\nanalysis is based on a detailed power and efficiency analysis of the RISC-V ISA\nextracted from silicon measurements and calibrated simulation of an Ariane\ninstance (RV64IMC) taped-out in GlobalFoundries 22 FDX technology. Ariane runs\nat up to 1.7 GHz and achieves up to 40 Gop/sW peak efficiency. We give insight\ninto the interplay between functionality required for application-class\nexecution (e.g. virtual memory, caches, multiple modes of privileged operation)\nand energy cost. Our analysis indicates that ISA heterogeneity and simpler\ncores with a few critical instruction extensions (e.g. packed SIMD) can\nsignificantly boost a RISC-V core's compute energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:07:21 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zaruba", "Florian", ""], ["Benini", "Luca", ""]]}, {"id": "1904.05782", "submitter": "Shaahin Angizi", "authors": "Shaahin Angizi and Deliang Fan", "title": "Accelerating Bulk Bit-Wise X(N)OR Operation in Processing-in-DRAM\n  Platform", "comments": "7 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With Von-Neumann computing architectures struggling to address\ncomputationally- and memory-intensive big data analytic task today,\nProcessing-in-Memory (PIM) platforms are gaining growing interests. In this\nway, processing-in-DRAM architecture has achieved remarkable success by\ndramatically reducing data transfer energy and latency. However, the\nperformance of such system unavoidably diminishes when dealing with more\ncomplex applications seeking bulk bit-wise X(N)OR- or addition operations,\ndespite utilizing maximum internal DRAM bandwidth and in-memory parallelism. In\nthis paper, we develop DRIM platform that harnesses DRAM as computational\nmemory and transforms it into a fundamental processing unit. DRIM uses the\nanalog operation of DRAM sub-arrays and elevates it to implement bit-wise\nX(N)OR operation between operands stored in the same bit-line, based on a new\ndual-row activation mechanism with a modest change to peripheral circuits such\nsense amplifiers. The simulation results show that DRIM achieves on average 71x\nand 8.4x higher throughput for performing bulk bit-wise X(N)OR-based operations\ncompared with CPU and GPU, respectively. Besides, DRIM outperforms recent\nprocessing-in-DRAM platforms with up to 3.7x better performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 15:29:44 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Angizi", "Shaahin", ""], ["Fan", "Deliang", ""]]}, {"id": "1904.07725", "submitter": "Estela Suarez", "authors": "Anke Kreuzer, Norbert Eicker, Jorge Amaya, Raphael Leger, Estela\n  Suarez", "title": "The DEEP-ER project: I/O and resiliency extensions for the\n  Cluster-Booster architecture", "comments": "8 pages, 10 figures, HPCC conference. arXiv admin note: text overlap\n  with arXiv:1904.05275", "journal-ref": "2018 IEEE 20th International Conference on High Performance\n  Computing and Communications (HPCC)", "doi": "10.1109/HPCC/SmartCity/DSS.2018.00046", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently completed research project DEEP-ER has developed a variety of\nhardware and software technologies to improve the I/O capabilities of next\ngeneration high-performance computers, and to enable applications recovering\nfrom the larger hardware failure rates expected on these machines.\n  The heterogeneous Cluster-Booster architecture --first introduced in the\npredecessor DEEP project-- has been extended by a multi-level memory hierarchy\nemploying non-volatile and network-attached memory devices. Based on this\nhardware infrastructure, an I/O and resiliency software stack has been\nimplemented combining and extending well established libraries and software\ntools, and sticking to standard user-interfaces. Real-world scientific codes\nhave tested the projects' developments and demonstrated the improvements\nachieved without compromising the portability of the applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:54:51 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kreuzer", "Anke", ""], ["Eicker", "Norbert", ""], ["Amaya", "Jorge", ""], ["Leger", "Raphael", ""], ["Suarez", "Estela", ""]]}, {"id": "1904.07864", "submitter": "Arman Roohi", "authors": "Arman Roohi, Shaahin Angizi, Deliang Fan, and Ronald F DeMara", "title": "Processing-In-Memory Acceleration of Convolutional Neural Networks for\n  Energy-Efficiency, and Power-Intermittency Resilience", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein, a bit-wise Convolutional Neural Network (CNN) in-memory accelerator\nis implemented using Spin-Orbit Torque Magnetic Random Access Memory (SOT-MRAM)\ncomputational sub-arrays. It utilizes a novel AND-Accumulation method capable\nof significantly-reduced energy consumption within convolutional layers and\nperforms various low bit-width CNN inference operations entirely within MRAM.\nPower-intermittence resiliency is also enhanced by retaining the partial state\ninformation needed to maintain computational forward-progress, which is\nadvantageous for battery-less IoT nodes. Simulation results indicate\n$\\sim$5.4$\\times$ higher energy-efficiency and 9$\\times$ speedup over\nReRAM-based acceleration, or roughly $\\sim$9.7$\\times$ higher energy-efficiency\nand 13.5$\\times$ speedup over recent CMOS-only approaches, while maintaining\ninference accuracy comparable to baseline designs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:22:16 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Roohi", "Arman", ""], ["Angizi", "Shaahin", ""], ["Fan", "Deliang", ""], ["DeMara", "Ronald F", ""]]}, {"id": "1904.08233", "submitter": "Issam Damaj", "authors": "Issam Damaj (1), Hassan Diab (2) ((1) London South Bank University,\n  (2) American University of Beirut)", "title": "Performance Analysis of Linear Algebraic Functions using Reconfigurable\n  Computing", "comments": "22 pages, 17 figures, 5 tables. arXiv admin note: substantial text\n  overlap with arXiv:1904.04953; text overlap with arXiv:1904.06198", "journal-ref": "Intl. Jrnl. of. Super. Comp. 24(2003) 91-107", "doi": "10.1023/A:1020993510939", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new mapping of geometrical transformation on the\nMorphoSys (M1) reconfigurable computing (RC) system. New mapping techniques for\nsome linear algebraic functions are recalled. A new mapping for geometrical\ntransformation operations is introduced and their performance on the M1 system\nis evaluated. The translation and scaling transformation addressed in this\nmapping employ some vector-vector and vector-scalar operations [6-7]. A\nperformance analysis study of the M1 RC system is also presented to evaluate\nthe efficiency of the algorithm execution. Numerical examples were simulated to\nvalidate our results, using the MorphoSys mULATE program, which emulates M1\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 20:15:29 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Damaj", "Issam", ""], ["Diab", "Hassan", ""]]}, {"id": "1904.08762", "submitter": "Ahsan Javed Awan Dr", "authors": "Stefano Corda, Gagandeep Singh, Ahsan Javed Awan, Roel Jordans and\n  Henk Corporaal", "title": "Memory and Parallelism Analysis Using a Platform-Independent Approach", "comments": "22nd ACM International Workshop on Software and Compilers for\n  Embedded Systems (SCOPES '19), May 2019", "journal-ref": null, "doi": "10.1145/3323439.3323988", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging computing architectures such as near-memory computing (NMC) promise\nimproved performance for applications by reducing the data movement between CPU\nand memory. However, detecting such applications is not a trivial task. In this\nongoing work, we extend the state-of-the-art platform-independent software\nanalysis tool with NMC related metrics such as memory entropy, spatial\nlocality, data-level, and basic-block-level parallelism. These metrics help to\nidentify the applications more suitable for NMC architectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:25:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Corda", "Stefano", ""], ["Singh", "Gagandeep", ""], ["Awan", "Ahsan Javed", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "1904.09363", "submitter": "Kyle Kuan", "authors": "Kyle Kuan and Tosiron Adegbija", "title": "Energy-Efficient Runtime Adaptable L1 STT-RAM Cache Design", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2019.2912920", "report-no": null, "categories": "cs.AR cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has shown that applications have variable runtime cache\nrequirements. In the context of the increasingly popular Spin-Transfer Torque\nRAM (STT-RAM) cache, the retention time, which defines how long the cache can\nretain a cache block in the absence of power, is one of the most important\ncache requirements that may vary for different applications. In this paper, we\npropose a Logically Adaptable Retention Time STT-RAM (LARS) cache that allows\nthe retention time to be dynamically adapted to applications' runtime\nrequirements. LARS cache comprises of multiple STT-RAM units with different\nretention times, with only one unit being used at a given time. LARS\ndynamically determines which STT-RAM unit to use during runtime, based on\nexecuting applications' needs. As an integral part of LARS, we also explore\ndifferent algorithms to dynamically determine the best retention time based on\ndifferent cache design tradeoffs. Our experiments show that by adapting the\nretention time to different applications' requirements, LARS cache can reduce\nthe average cache energy by 25.31%, compared to prior work, with minimal\noverheads.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 23:03:04 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Kuan", "Kyle", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "1904.09495", "submitter": "Aleksandr Romanov Yur'evich", "authors": "Aleksandr Yu. Romanov", "title": "Development of routing algorithms in networks-on-chip based on ring\n  circulant topologies", "comments": "23 p., 10 fig", "journal-ref": "Heliyon 5 (2019) e01516", "doi": "10.1016/j.heliyon.2019.e01516", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is devoted to the study of communication subsystem of\nnetworks-onchip (NoCs) development with an emphasis on their topologies. The\nmain characteristics of NoC topologies and the routing problem in NoCs with\nvarious topologies are considered. It is proposed to use two-dimensional\ncirculant topologies for NoC design, since they have significantly better\ncharacteristics than most common mesh and torus topologies, and, in contrast to\nmany other approaches to improving topologies, have a regular structure. The\nemphasis is on using ring circulants which although in some cases have somewhat\nworse characteristics than the optimal circulants, compensate by one-length\nfirst generatrix in such graphs that greatly facilitate routing in them. The\npaper considers three different approaches to routing in NoCs with ring\ncirculant topology: Table routing, Clockwise routing, and Adaptive routing. The\nalgorithms of routing are proposed, the results of synthesis of routers, based\non them, are presented, and the cost of chip resources for the implementation\nof such communication subsystems in NoCs is estimated.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 20:31:15 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 08:27:33 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Romanov", "Aleksandr Yu.", ""]]}, {"id": "1904.09554", "submitter": "Yawen Zhang", "authors": "Yawen Zhang, Runsheng Wang, Xinyue Zhang, Zherui Zhang, Jiahao Song,\n  Zuodong Zhang, Yuan Wang, and Ru Huang", "title": "A Parallel Bitstream Generator for Stochastic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic computing (SC) presents high error tolerance and low hardware\ncost, and has great potential in applications such as neural networks and image\nprocessing. However, the bitstream generator, which converts a binary number to\nbitstreams, occupies a large area and energy consumption, thus weakening the\nsuperiority of SC. In this paper, we propose a novel technique for generating\nbitstreams in parallel, which needs only one clock for conversion and\nsignificantly reduces the hardware cost. Synthesis results demonstrate that the\nproposed parallel bitstream generator improves 2.5x area and 712x energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:16:06 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "Yawen", ""], ["Wang", "Runsheng", ""], ["Zhang", "Xinyue", ""], ["Zhang", "Zherui", ""], ["Song", "Jiahao", ""], ["Zhang", "Zuodong", ""], ["Wang", "Yuan", ""], ["Huang", "Ru", ""]]}, {"id": "1904.09724", "submitter": "Jeremie Kim", "authors": "Onur Mutlu and Jeremie S. Kim", "title": "RowHammer: A Retrospective", "comments": "A version of this work is to appear at IEEE Transactions on\n  Computer-Aided Design of Integrated Circuits and Systems (TCAD) Special Issue\n  on Top Picks in Hardware and Embedded Security, 2019. arXiv admin note:\n  substantial text overlap with arXiv:1703.00626, arXiv:1903.11056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This retrospective paper describes the RowHammer problem in Dynamic Random\nAccess Memory (DRAM), which was initially introduced by Kim et al. at the ISCA\n2014 conference~\\cite{rowhammer-isca2014}. RowHammer is a prime (and perhaps\nthe first) example of how a circuit-level failure mechanism can cause a\npractical and widespread system security vulnerability. It is the phenomenon\nthat repeatedly accessing a row in a modern DRAM chip causes bit flips in\nphysically-adjacent rows at consistently predictable bit locations. RowHammer\nis caused by a hardware failure mechanism called {\\em DRAM disturbance errors},\nwhich is a manifestation of circuit-level cell-to-cell interference in a scaled\nmemory technology.\n  Researchers from Google Project Zero demonstrated in 2015 that this hardware\nfailure mechanism can be effectively exploited by user-level programs to gain\nkernel privileges on real systems. Many other follow-up works demonstrated\nother practical attacks exploiting RowHammer. In this article, we\ncomprehensively survey the scientific literature on RowHammer-based attacks as\nwell as mitigation techniques to prevent RowHammer. We also discuss what other\nrelated vulnerabilities may be lurking in DRAM and other types of memories,\ne.g., NAND flash memory or Phase Change Memory, that can potentially threaten\nthe foundations of secure systems, as the memory technologies scale to higher\ndensities. We conclude by describing and advocating a principled approach to\nmemory reliability and security research that can enable us to better\nanticipate and prevent such vulnerabilities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:21:51 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Mutlu", "Onur", ""], ["Kim", "Jeremie S.", ""]]}, {"id": "1904.10564", "submitter": "Arman Roohi", "authors": "Arman Roohi, and Ronald F DeMara", "title": "IRC: Cross-layer design exploration of Intermittent Robust Computation\n  units for IoTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-harvesting-powered computing offers intriguing and vast opportunities\nto dramatically transform the landscape of the Internet of Things (IoT) devices\nby utilizing ambient sources of energy to achieve battery-free computing. In\norder to operate within the restricted energy capacity and intermittency\nprofile, it is proposed to innovate Intermittent Robust Computation (IRC) Unit\nas a new duty-cycle-variable computing approach leveraging the non-volatility\ninherent in spin-based switching devices. The foundations of IRC will be\nadvanced from the device-level upwards, by extending a Spin Hall Effect\nMagnetic Tunnel Junction (SHE-MTJ) device. The device will then be used to\nrealize SHE-MTJ Majority/Polymorphic Gate (MG/PG) logic approaches and\nlibraries. Then a Logic-Embedded Flip-Flop (LE-FF) is developed to realize\nrudimentary Boolean logic functions along with an inherent state-holding\ncapability within a compact footprint. Finally, the NV-Clustering synthesis\nprocedure and corresponding tool module are proposed to instantiate the LE-FF\nlibrary cells within conventional Register Transfer Language (RTL)\nspecifications. This selectively clusters together logic and NV state-holding\nfunctionality, based on energy and area minimization criteria. It also realizes\nmiddleware-coherent, intermittent computation without checkpointing,\nmicro-tasking, or software bloat and energy overheads vital to IoT. Simulation\nresults for various benchmark circuits including ISCAS-89 validate\nfunctionality and power dissipation, area, and delay benefits.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 23:03:54 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Roohi", "Arman", ""], ["DeMara", "Ronald F", ""]]}, {"id": "1904.10646", "submitter": "Norbert Deak", "authors": "Norbert Deak, Octavian Cre\\c{t}, Horia Hede\\c{s}iu", "title": "Efficient FPGA Floorplanning for Partial Reconfiguration-Based\n  Applications", "comments": "9 pages, 7 figures, a one page summary published to FCCM 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Reconfiguration (PR) is a technique that allows reconfiguring the\nFPGA chip at runtime. However, current design support tools require manual\nfloorplanning of the partial modules. Several approaches have been proposed in\nthis field, but only a few of them consider all aspects of PR, like the shape\nand the aspect ratio of the reconfigurable region. Most of them are defined for\nold FPGA architectures and have a high computational time. This paper\nintroduces an efficient automatic floorplanning algorithm, which takes into\naccount the heterogeneous architectures of modern FPGA families, as well as PR\nconstraints, introducing the aspect ratio constraint to optimize routing. The\nalgorithm generates possible placements of the partial modules, then applies a\nrecursive pseudo-bipartitioning heuristic search to find the best floorplan.\nThe experiments showed that the algorithm's performance is significantly better\nthan the one of other algorithms in this field.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 05:26:39 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Deak", "Norbert", ""], ["Cre\u0163", "Octavian", ""], ["Hede\u015fiu", "Horia", ""]]}, {"id": "1904.11200", "submitter": "Shan Shen", "authors": "Shan Shen, Tianxiang Shao, Xiaojing Shang, Yichen Guo, Ming Ling, Jun\n  Yang, Longxing Shi", "title": "TS Cache: A Fast Cache with Timing-speculation Mechanism Under Low\n  Supply Voltages", "comments": "This version have been submitted to Transaction on VLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the ever-worsening Power Wall problem, more and more applications\nneed to expand their power supply to the wide-voltage range including the\nnear-threshold region. However, the read delay distribution of the SRAM cells\nunder the near-threshold voltage shows a more serious long-tail characteristic\nthan that under the nominal voltage due to the process fluctuation. Such\ndegradation of SRAM delay makes the SRAM-based cache a performance bottleneck\nof systems as well. To avoid the unreliable data reading, circuit-level studies\nuse larger/more transistors in a bitcell by scarifying chip area and the static\npower of cache arrays. Architectural studies propose the auxiliary error\ncorrection or block disabling/remapping methods in fault-tolerant caches, which\nworsen both the hit latency and energy efficiency due to the complex accessing\nlogic. This paper proposes the Timing-Speculation (TS) cache to boost the cache\nfrequency and improve energy efficiency under low supply voltages. In the TS\ncache, the voltage differences of bitlines are continuously evaluated twice by\na sense amplifier (SA), and the access timing error can be detected much\nearlier than that in prior methods. According to the measurement results from\nthe fabricated chips, the TS L1 cache aggressively increases its frequency to\n1.62X and 1.92X compared with the conventional scheme at 0.5V and 0.6V supply\nvoltages, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:19:01 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Shen", "Shan", ""], ["Shao", "Tianxiang", ""], ["Shang", "Xiaojing", ""], ["Guo", "Yichen", ""], ["Ling", "Ming", ""], ["Yang", "Jun", ""], ["Shi", "Longxing", ""]]}, {"id": "1904.11560", "submitter": "Morteza Hoseinzadeh", "authors": "Morteza Hoseinzadeh", "title": "A Survey on Tiering and Caching in High-Performance Storage Systems", "comments": "Ph.D. Research Exam Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although every individual invented storage technology made a big step towards\nperfection, none of them is spotless. Different data store essentials such as\nperformance, availability, and recovery requirements have not met together in a\nsingle economically affordable medium, yet. One of the most influential factors\nis price. So, there has always been a trade-off between having a desired set of\nstorage choices and the costs. To address this issue, a network of various\ntypes of storing media is used to deliver the high performance of expensive\ndevices such as solid state drives and non-volatile memories, along with the\nhigh capacity of inexpensive ones like hard disk drives. In software, caching\nand tiering are long-established concepts for handling file operations and\nmoving data automatically within such a storage network and manage data backup\nin low-cost media. Intelligently moving data around different devices based on\nthe needs is the key insight for this matter. In this survey, we discuss some\nrecent pieces of research that have been done to improve high-performance\nstorage systems with caching and tiering techniques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:57:31 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Hoseinzadeh", "Morteza", ""]]}, {"id": "1904.12729", "submitter": "Omer Khan", "authors": "Hamza Omar and Omer Khan", "title": "IRONHIDE: A Secure Multicore that Efficiently Mitigates\n  Microarchitecture State Attacks for Interactive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microprocessors enable aggressive hardware virtualization by means of which\nmultiple processes temporally execute on the system. These security-critical\nand ordinary processes interact with each other to assure application progress.\nHowever, temporal sharing of hardware resources exposes the processor to\nvarious microarchitecture state attacks. State-of-the-art secure processors,\nsuch as MI6 adopt Intel's SGX enclave execution model. MI6 architects strong\nisolation by statically isolating shared memory state, and purging the\nmicroarchitecture state of private core, cache, and TLB resources on every\nenclave entry and exit. The purging overhead significantly impacts performance\nas the interactivity across the secure and insecure processes increases. This\npaper proposes IRONHIDE that implements strong isolation in the context of\nmulticores to form spatially isolated secure and insecure clusters of cores.\nFor an interactive application comprising of secure and insecure processes,\nIRONHIDE pins the secure process(es) to the secure cluster, where they execute\nand interact with the insecure process(es) without incurring the\nmicroarchitecture state purging overheads on every interaction event. IRONHIDE\nimproves performance by 2.1x over the MI6 baseline for a set of user and OS\ninteractive applications. Moreover, IRONHIDE improves performance by 20% over\nan SGX-like baseline, while also ensuring strong isolation guarantees against\nmicroarchitecture state attacks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 14:14:02 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 23:28:55 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 14:50:42 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Omar", "Hamza", ""], ["Khan", "Omer", ""]]}]