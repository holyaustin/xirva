[{"id": "1506.03160", "submitter": "Donghyuk Lee", "authors": "Donghyuk Lee, Gennady Pekhimenko, Samira Khan, Saugata Ghose, Onur\n  Mutlu", "title": "Simultaneous Multi Layer Access: A High Bandwidth and Low Cost\n  3D-Stacked Memory Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited memory bandwidth is a critical bottleneck in modern systems.\n3D-stacked DRAM enables higher bandwidth by leveraging wider\nThrough-Silicon-Via (TSV) channels, but today's systems cannot fully exploit\nthem due to the limited internal bandwidth of DRAM. DRAM reads a whole row\nsimultaneously from the cell array to a row buffer, but can transfer only a\nfraction of the data from the row buffer to peripheral IO circuit, through a\nlimited and expensive set of wires referred to as global bitlines. In presence\nof wider memory channels, the major bottleneck becomes the limited data\ntransfer capacity through these global bitlines. Our goal in this work is to\nenable higher bandwidth in 3D-stacked DRAM without the increased cost of adding\nmore global bitlines. We instead exploit otherwise-idle resources, such as\nglobal bitlines, already existing within the multiple DRAM layers by accessing\nthe layers simultaneously. Our architecture, Simultaneous Multi Layer Access\n(SMLA), provides higher bandwidth by aggregating the internal bandwidth of\nmultiple layers and transferring the available data at a higher IO frequency.\n  To implement SMLA, simultaneous data transfer from multiple layers through\nthe same IO TSVs requires coordination between layers to avoid channel\nconflict. We first study coordination by static partitioning, which we call\nDedicated-IO, that assigns groups of TSVs to each layer. We then provide a\nsimple, yet sophisticated mechanism, called Cascaded-IO, which enables\nsimultaneous access to each layer by time-multiplexing the IOs. By operating at\na frequency proportional to the number of layers, SMLA provides a higher\nbandwidth (4X for a four-layer stacked DRAM). Our evaluations show that SMLA\nprovides significant performance improvement and energy reduction (55%/18% on\naverage for multi-programmed workloads, respectively) over a baseline\n3D-stacked DRAM with very low area overhead.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:14:29 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Lee", "Donghyuk", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Ghose", "Saugata", ""], ["Mutlu", "Onur", ""]]}, {"id": "1506.03181", "submitter": "Mohammad Shihabul Haque", "authors": "Mohammad Shihabul Haque, Jorgen Peddersen, Andhi Janapsatya, Sri\n  Parameswaran", "title": "DEW: A Fast Level 1 Cache Simulation Approach for Embedded Processors\n  with FIFO Replacement Policy", "comments": null, "journal-ref": null, "doi": "10.1109/DATE.2010.5457153", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the speed of cache simulation to obtain hit/miss rates en- ables\nperformance estimation, cache exploration for embedded sys- tems and energy\nestimation. Previously, such simulations, particu- larly exact approaches, have\nbeen exclusively for caches which uti- lize the least recently used (LRU)\nreplacement policy. In this paper, we propose a new, fast and exact cache\nsimulation method for the First In First Out(FIFO) replacement policy. This\nmethod, called DEW, is able to simulate multiple level 1 cache configurations\n(dif- ferent set sizes, associativities, and block sizes) with FIFO replace-\nment policy. DEW utilizes a binomial tree based representation of cache\nconfigurations and a novel searching method to speed up sim- ulation over\nsingle cache simulators like Dinero IV. Depending on different cache block\nsizes and benchmark applications, DEW oper- ates around 8 to 40 times faster\nthan Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW\nto determine accu- rate miss rates.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 06:14:33 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 18:28:46 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Haque", "Mohammad Shihabul", ""], ["Peddersen", "Jorgen", ""], ["Janapsatya", "Andhi", ""], ["Parameswaran", "Sri", ""]]}, {"id": "1506.03182", "submitter": "Mohammad Shihabul Haque", "authors": "Mohammad Shihabul Haque, Akash Kumar, Yajun Ha, Qiang Wu and Shaobo\n  Luo", "title": "TRISHUL: A Single-pass Optimal Two-level Inclusive Data Cache Hierarchy\n  Selection Process for Real-time MPSoCs", "comments": null, "journal-ref": null, "doi": "10.1109/ASPDAC.2013.6509615", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hitherto discovered approaches analyze the execution time of a real time\napplication on all the possible cache hierarchy setups to find the application\nspecific optimal two level inclusive data cache hierarchy to reduce cost, space\nand energy consumption while satisfying the time deadline in real time\nMultiprocessor Systems on Chip. These brute force like approaches can take\nyears to complete. Alternatively, memory access trace driven crude estimation\nmethods can find a cache hierarchy quickly by compromising the accuracy of\nresults. In this article, for the first time, we propose a fast and accurate\ntrace driven approach to find the optimal real time application specific two\nlevel inclusive data cache hierarchy. Our proposed approach TRISHUL predicts\nthe optimal cache hierarchy performance first and then utilizes that\ninformation to find the optimal cache hierarchy quickly. TRISHUL can suggest a\ncache hierarchy, which has up to 128 times smaller size, up to 7 times faster\ncompared to the suggestion of the state of the art crude trace driven two level\ninclusive cache hierarchy selection approach for the application traces\nanalyzed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 06:26:02 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 18:09:02 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Haque", "Mohammad Shihabul", ""], ["Kumar", "Akash", ""], ["Ha", "Yajun", ""], ["Wu", "Qiang", ""], ["Luo", "Shaobo", ""]]}, {"id": "1506.03186", "submitter": "Mohammad Shihabul Haque", "authors": "Mohammad Shihabul Haque, Jorgen Peddersen, Sri Parameswaran", "title": "CIPARSim: Cache Intersection Property Assisted Rapid Single-pass FIFO\n  Cache Simulation Technique", "comments": null, "journal-ref": null, "doi": "10.1109/ICCAD.2011.6105316", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the first time, we introduce a cache property called the\nIntersection Property that helps to reduce singlepass simulation time in a\nmanner similar to inclusion property. An intersection property defines\nconditions that if met, prove a particular element exists in larger caches,\nthus avoiding further search time. We have discussed three such intersection\nproperties for caches using the FIFO replacement policy in this paper. A rapid\nsinglepass FIFO cache simulator CIPARSim has also been proposed. CIPARSim is\nthe first singlepass simulator dependent on the FIFO cache properties to reduce\nsimulation time significantly. CIPARSim simulation time was up to 5 times\nfaster compared to the state of the art singlepass FIFO cache simulator for the\ncache configurations tested. CIPARSim produces the cache hit and miss rates of\nan application accurately on various cache configurations. During simulation,\nCIPARSim intersection properties alone predict up to 90% of the total hits,\nreducing simulationtime immensely\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 06:57:55 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 18:21:08 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Haque", "Mohammad Shihabul", ""], ["Peddersen", "Jorgen", ""], ["Parameswaran", "Sri", ""]]}, {"id": "1506.03193", "submitter": "Mohammad Shihabul Haque", "authors": "Mohammad Shihabul Haque, Ang Li, Akash Kumar, Qingsong Wei", "title": "Accelerating Non-volatile/Hybrid Processor Cache Design Space\n  Exploration for Application Specific Embedded Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ASPDAC.2015.7059045", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a technique to accelerate nonvolatile or hybrid\nof volatile and nonvolatile processor cache design space exploration for\napplication specific embedded systems. Utilizing a novel cache behavior\nmodeling equation and a new accurate cache miss prediction mechanism, our\nproposed technique can accelerate NVM or hybrid FIFO processor cache design\nspace exploration for SPEC CPU 2000 applications up to 249 times compared to\nthe conventional approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 07:14:45 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2015 17:51:47 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Haque", "Mohammad Shihabul", ""], ["Li", "Ang", ""], ["Kumar", "Akash", ""], ["Wei", "Qingsong", ""]]}]