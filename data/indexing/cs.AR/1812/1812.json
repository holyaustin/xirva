[{"id": "1812.00182", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Michael Schaffner, Luca Benini", "title": "NTX: An Energy-efficient Streaming Accelerator for Floating-point\n  Generalized Reduction Workloads in 22nm FD-SOI", "comments": "6 pages, invited paper at DATE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized coprocessors for Multiply-Accumulate (MAC) intensive workloads\nsuch as Deep Learning are becoming widespread in SoC platforms, from GPUs to\nmobile SoCs. In this paper we revisit NTX (an efficient accelerator developed\nfor training Deep Neural Networks at scale) as a generalized MAC and reduction\nstreaming engine. The architecture consists of a set of 32 bit floating-point\nstreaming co-processors that are loosely coupled to a RISC-V core in charge of\norchestrating data movement and computation. Post-layout results of a recent\nsilicon implementation in 22 nm FD-SOI technology show the accelerator's\ncapability to deliver up to 20 Gflop/s at 1.25 GHz and 168 mW. Based on these\nresults we show that a version of NTX scaled down to 14 nm can achieve a 3x\nenergy efficiency improvement over contemporary GPUs at 10.4x less silicon\narea, and a compute performance of 1.4 Tflop/s for training large\nstate-of-the-art networks with full floating-point precision. An extended\nevaluation of MAC-intensive kernels shows that NTX can consistently achieve up\nto 87% of its peak performance across general reduction workloads beyond\nmachine learning. Its modular architecture enables deployment at different\nscales ranging from high-performance GPU-class to low-power embedded scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 09:58:51 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Schuiki", "Fabian", ""], ["Schaffner", "Michael", ""], ["Benini", "Luca", ""]]}, {"id": "1812.01209", "submitter": "Soroush Khaleghi", "authors": "Soroush Khaleghi and Wenjing Rao", "title": "Repairability Enhancement of Scalable Systems with Locally Shared Spares", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future systems based on nano-scale devices will provide great potentials for\nscaling up in system complexity, yet they will be highly susceptible to\noperational faults. While spare units can be generally used to enhance\nreliability, they must be shared in a limited way among functional units to\nensure low-cost overheads when systems scale up. Furthermore, the efficiency of\nachieving reliability using spare units heavily depends on the replacement\nmechanisms of such spares. While global and chained replacement approaches can\ntake advantage of the entire replacement capabilities in the network, they\nusually impose some sort of disturbance to all the functional units in the\nsystem during the repair process, thus are dreadfully expensive in terms of\nperformance overhead for systems with high fault rates. In this paper, we focus\non a low-cost, fast, immediate replacement mechanism that can be implemented\nlocally with minimum disturbance to the system. The proposed schemes aim for\nmaintaining the system with high fault rates in such a low-cost, fast\nrepairable status for many faults before invoking the more expensive, yet\noptimal, approaches. First, we propose an online repair algorithm: as faults\noccur during the run-time of the system, the proposed algorithm makes a choice\nof a spare unit (among several candidates), such that the overall impact on\nsystem repairability in the future is minimized. Second, we propose a network\nenhancement approach, which identifies and connects the vulnerable units to the\nexploitable spares, thus strengthening the entire system at a low cost.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 04:15:05 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Khaleghi", "Soroush", ""], ["Rao", "Wenjing", ""]]}, {"id": "1812.02770", "submitter": "Imran Abbasi", "authors": "Imran Hafeez Abbassi, Faiq Khalid, Semeen Rehman, Awais Mehmood\n  Kamboh, Axel Jantsch, Siddharth Garg and Muhammad Shafique", "title": "TrojanZero: Switching Activity-Aware Design of Undetectable Hardware\n  Trojans with Zero Power and Area Footprint", "comments": "Design, Automation and Test in Europe (DATE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Hardware Trojan (HT) detection techniques are based on the\nvalidation of integrated circuits to determine changes in their functionality,\nand on non-invasive side-channel analysis to identify the variations in their\nphysical parameters. In particular, almost all the proposed side-channel\npower-based detection techniques presume that HTs are detectable because they\nonly add gates to the original circuit with a noticeable increase in power\nconsumption. This paper demonstrates how undetectable HTs can be realized with\nzero impact on the power and area footprint of the original circuit. Towards\nthis, we propose a novel concept of TrojanZero and a systematic methodology for\ndesigning undetectable HTs in the circuits, which conceals their existence by\ngate-level modifications. The crux is to salvage the cost of the HT from the\noriginal circuit without being detected using standard testing techniques. Our\nmethodology leverages the knowledge of transition probabilities of the circuit\nnodes to identify and safely remove expendable gates, and embeds malicious\ncircuitry at the appropriate locations with zero power and area overheads when\ncompared to the original circuit. We synthesize these designs and then embed in\nmultiple ISCAS85 benchmarks using a 65nm technology library, and perform a\ncomprehensive power and area characterization. Our experimental results\ndemonstrate that the proposed TrojanZero designs are undetectable by the\nstate-of-the-art power-based detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:48:44 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Abbassi", "Imran Hafeez", ""], ["Khalid", "Faiq", ""], ["Rehman", "Semeen", ""], ["Kamboh", "Awais Mehmood", ""], ["Jantsch", "Axel", ""], ["Garg", "Siddharth", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1812.03991", "submitter": "Shoeb Shaikh", "authors": "Shoeb Shaikh, Rosa So, Tafadzwa Sibindi, Camilo Libedinsky and Arindam\n  Basu", "title": "Real-time Closed Loop Neural Decoding on a Neuromorphic Chip", "comments": "accepted at Neural Engineering Conference (NER), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents for the first time a real-time closed loop neuromorphic\ndecoder chip-driven intra-cortical brain machine interface (iBMI) in a\nnon-human primate (NHP) based experimental setup. Decoded results show trial\nsuccess rates and mean times to target comparable to those obtained by\nhand-controlled joystick. Neural control trial success rates of approximately\n96% of those obtained by hand-controlled joystick have been demonstrated. Also,\nneural control has shown mean target reach speeds of approximately 85% of those\nobtained by hand-controlled joystick . These results pave the way for fast and\naccurate, fully implantable neuromorphic neural decoders in iBMIs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 09:31:18 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Shaikh", "Shoeb", ""], ["So", "Rosa", ""], ["Sibindi", "Tafadzwa", ""], ["Libedinsky", "Camilo", ""], ["Basu", "Arindam", ""]]}, {"id": "1812.04122", "submitter": "Reza Salkhordeh", "authors": "Reza Salkhordeh and Mostafa Hadizadeh and Hossein Asadi", "title": "An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs", "comments": null, "journal-ref": "R. Salkhordeh, M. Hadizadeh and H. Asadi, \"An Efficient Hybrid I/O\n  Caching Architecture Using Heterogeneous SSDs,\" in IEEE Transactions on\n  Parallel and Distributed Systems. doi: 10.1109/TPDS.2018.2883745", "doi": "10.1109/TPDS.2018.2883745", "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSDs are emerging storage devices which unlike HDDs, do not have mechanical\nparts and therefore, have superior performance compared to HDDs. Due to the\nhigh cost of SSDs, entirely replacing HDDs with SSDs is not economically\njustified. Additionally, SSDs can endure a limited number of writes before\nfailing. To mitigate the shortcomings of SSDs while taking advantage of their\nhigh performance, SSD caching is practiced in both academia and industry.\nPreviously proposed caching architectures have only focused on either\nperformance or endurance and neglected to address both parameters in suggested\narchitectures. Moreover, the cost, reliability, and power consumption of such\narchitectures is not evaluated. This paper proposes a hybrid I/O caching\narchitecture that while offers higher performance than previous studies, it\nalso improves power consumption with a similar budget. The proposed\narchitecture uses DRAM, Read-Optimized SSD, and Write-Optimized SSD in a\nthree-level cache hierarchy and tries to efficiently redirect read requests to\neither DRAM or RO-SSD while sending writes to WO-SSD. To provide high\nreliability, dirty pages are written to at least two devices which removes any\nsingle point of failure. The power consumption is also managed by reducing the\nnumber of accesses issued to SSDs. The proposed architecture reconfigures\nitself between performance- and endurance-optimized policies based on the\nworkload characteristics to maintain an effective tradeoff between performance\nand endurance. We have implemented the proposed architecture on a server\nequipped with industrial SSDs and HDDs. The experimental results show that as\ncompared to state-of-the-art studies, the proposed architecture improves\nperformance and power consumption by an average of 8% and 28%, respectively,\nand reduces the cost by 5% while increasing the endurance cost by 4.7% and\nnegligible reliability penalty.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 22:00:39 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Salkhordeh", "Reza", ""], ["Hadizadeh", "Mostafa", ""], ["Asadi", "Hossein", ""]]}, {"id": "1812.04514", "submitter": "Sushant Kondguli", "authors": "Sushant Kondguli and Michael Huang", "title": "R3-DLA (Reduce, Reuse, Recycle): A More Efficient Approach to Decoupled\n  Look-Ahead Architectures", "comments": "16 pages, 16 Figures, Scheduled to appear in 25th IEEE International\n  Symposium on High-Performance Computer Architecture 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern societies have developed insatiable demands for more computation\ncapabilities. Exploiting implicit parallelism to provide automatic performance\nimprovement remains a central goal in engineering future general-purpose\ncomputing systems. One approach is to use a separate thread context to perform\ncontinuous look-ahead to improve the data and instruction supply to the main\npipeline. Such a decoupled look-ahead (DLA) architecture can be quite effective\nin accelerating a broad range of applications in a relatively straightforward\nimplementation. It also has broad design flexibility as the look-ahead agent\nneed not be concerned with correctness constraints. In this paper, we explore a\nnumber of optimizations that make the look-ahead agent more efficient and yet\nextract more utility from it. With these optimizations, a DLA architecture can\nachieve an average speedup of 1.4 over a state-of-the-art microarchitecture for\na broad set of benchmark suites, making it a powerful tool to enhance\nsingle-thread performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 16:13:35 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 17:19:36 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 16:05:14 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Kondguli", "Sushant", ""], ["Huang", "Michael", ""]]}, {"id": "1812.05446", "submitter": "Faiq Khalid", "authors": "Faiq Khalid, Imran Hafeez Abbassi, Semeen Rehman, Awais Mehmood\n  Kamboh, Osman Hasan, Muhammad Shafique", "title": "ForASec: Formal Analysis of Security Vulnerabilities in Sequential\n  Circuits", "comments": "(Accepted in 2021)", "journal-ref": "IEEE Transactions on Computer-Aided Design of Integrated Circuits\n  and Systems 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security vulnerability analysis of Integrated Circuits using conventional\ndesign-time validation and verification techniques (like simulations,\nemulations, etc.) is generally a computationally intensive task and incomplete\nby nature, especially under limited resources and time constraints. To overcome\nthis limitation, we propose a novel methodology based on model checking to\nformally analyze security vulnerabilities in sequential circuits while\nconsidering side-channel parameters like propagation delay, switching power,\nand leakage power. In particular, we present a novel algorithm to efficiently\npartition the state-space into corresponding smaller state-spaces to enable\ndistributed security analysis of complex sequential circuits and thereby\nmitigating the associated state-space explosion due to their feedback loops. We\nanalyze multiple ISCAS89 and trust-hub benchmarks to demonstrate the efficacy\nof our framework in identifying security vulnerabilities. The experimental\nresults show that ForASec successfully performs the complete analysis of the\ngiven complex and large sequential circuits, and provides approximately 11x to\n16x speedup in analysis time compared to state-of-the-art model checking-based\ntechniques. Moreover, it also identifies the number of gates required by an HT\nthat can go undetected for a given design and variability conditions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 00:22:15 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 07:51:04 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 11:03:48 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Khalid", "Faiq", ""], ["Abbassi", "Imran Hafeez", ""], ["Rehman", "Semeen", ""], ["Kamboh", "Awais Mehmood", ""], ["Hasan", "Osman", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1812.06377", "submitter": "Justin Meza", "authors": "Justin Meza and Jing Li and Onur Mutlu", "title": "Evaluating Row Buffer Locality in Future Non-Volatile Main Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-SAFARI-2012-002", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DRAM-based main memories have read operations that destroy the read data, and\nas a result, must buffer large amounts of data on each array access to keep\nchip costs low. Unfortunately, system-level trends such as increased memory\ncontention in multi-core architectures and data mapping schemes that improve\nmemory parallelism may cause only a small amount of the buffered data to be\naccessed. This makes buffering large amounts of data on every memory array\naccess energy-inefficient.\n  Emerging non-volatile memories (NVMs) such as PCM, STT-RAM, and RRAM,\nhowever, do not have destructive read operations, opening up opportunities for\nemploying small row buffers without incurring additional area penalty and/or\ndesign complexity.\n  In this work, we discuss architectural changes to enable small row buffers at\na low cost in NVMs. We provide a memory access protocol, energy model, and\ntiming model to enable further system-level evaluation. We evaluate the\nsystem-level tradeoffs of employing different row buffer sizes in NVM main\nmemories in terms of energy, performance, and endurance, with different data\nmapping schemes. We find that on a multi-core CMP system, reducing the row\nbuffer size can greatly reduce main memory dynamic energy compared to a DRAM\nbaseline with large row sizes, without greatly affecting endurance, and for\nsome memories, leads to improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 02:42:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Meza", "Justin", ""], ["Li", "Jing", ""], ["Mutlu", "Onur", ""]]}, {"id": "1812.07012", "submitter": "Young-Kyu Choi", "authors": "Yuze Chi and Young-kyu Choi and Jason Cong and Jie Wang", "title": "Rapid Cycle-Accurate Simulator for High-Level Synthesis", "comments": "This paper is an extended version of a paper that has been accepted\n  for FPGA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large semantic gap between the high-level synthesis (HLS) design and the\nlow-level (on-board or RTL) simulation environment often creates a barrier for\nthose who are not FPGA experts. Moreover, such low-level simulation takes a\nlong time to complete. Software-based HLS simulators can help bridge this gap\nand accelerate the simulation process; however, we found that the current FPGA\nHLS commercial software simulators sometimes produce incorrect results. In\norder to solve this correctness issue while maintaining the high speed of a\nsoftware-based simulator, this paper proposes a new HLS simulation flow named\nFLASH. The main idea behind the proposed flow is to extract the scheduling\ninformation from the HLS tool and automatically construct an equivalent\ncycle-accurate simulation model while preserving C semantics. Experimental\nresults show that FLASH runs three orders of magnitude faster than the RTL\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 19:30:57 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 02:10:37 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Chi", "Yuze", ""], ["Choi", "Young-kyu", ""], ["Cong", "Jason", ""], ["Wang", "Jie", ""]]}, {"id": "1812.07517", "submitter": "Hyunbin Park", "authors": "Hyunbin Park, Dohyun Kim, and Shiho Kim", "title": "Digital Neuron: A Hardware Inference Accelerator for Convolutional Deep\n  Neural Networks", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Digital Neuron, a hardware inference accelerator for\nconvolutional deep neural networks with integer inputs and integer weights for\nembedded systems. The main idea to reduce circuit area and power consumption is\nmanipulating dot products between input feature and weight vectors by Barrel\nshifters and parallel adders. The reduced area allows the more computational\nengines to be mounted on an inference accelerator, resulting in high throughput\ncompared to prior HW accelerators. We verified that the multiplication of\ninteger numbers with 3-partial sub-integers does not cause significant loss of\ninference accuracy compared to 32-bit floating point calculation. The proposed\ndigital neuron can perform 800 MAC operations in one clock for computation for\nconvolution as well as full-connection. This paper provides a scheme that\nreuses input, weight, and output of all layers to reduce DRAM access. In\naddition, this paper proposes a configurable architecture that can provide\ninference of adaptable feature of convolutional neural networks. The throughput\nin terms of Watt of the digital neuron is achieved 754.7 GMACs/W.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 14:18:57 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 08:07:25 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Park", "Hyunbin", ""], ["Kim", "Dohyun", ""], ["Kim", "Shiho", ""]]}, {"id": "1812.07609", "submitter": "Mohammad Samavatian", "authors": "Mohammad Hossein Samavatian, Anys Bacha, Li Zhou, Radu Teodorescu", "title": "RNNFast: An Accelerator for Recurrent Neural Networks Using Domain Wall\n  Memory", "comments": "26 pages", "journal-ref": "JETC January 2020 Volume 1 26 pages", "doi": "10.1145/3399670", "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) are an important class of neural networks\ndesigned to retain and incorporate context into current decisions. RNNs are\nparticularly well suited for machine learning problems in which context is\nimportant, such as speech recognition and language translation. This work\npresents RNNFast, a hardware accelerator for RNNs that leverages an emerging\nclass of non-volatile memory called domain-wall memory (DWM). We show that DWM\nis very well suited for RNN acceleration due to its very high density and low\nread/write energy. At the same time, the sequential nature of input/weight\nprocessing of RNNs mitigates one of the downsides of DWM, which is the linear\n(rather than constant) data access time.RNNFast is very efficient and highly\nscalable, with flexible mapping of logical neurons to RNN hardware blocks. The\nbasic hardware primitive, the RNN processing element (PE) includes custom\nDWM-based multiplication, sigmoid and tanh units for high density and\nlow-energy. The accelerator is designed to minimize data movement by closely\ninterleaving DWM storage and computation. We compare our design with a\nstate-of-the-art GPGPU and find21.8x higher performance with70x lower energy\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:14:50 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 01:04:04 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Samavatian", "Mohammad Hossein", ""], ["Bacha", "Anys", ""], ["Zhou", "Li", ""], ["Teodorescu", "Radu", ""]]}, {"id": "1812.08918", "submitter": "Zamshed Chowdhury", "authors": "Zamshed I. Chowdhury, S. Karen Khatamifard, Zhengyang Zhao, Masoud\n  Zabihi, Salonik Resch, Meisam Razaviyayn, Jian-Ping Wang, Sachin Sapatnekar,\n  Ulya R. Karpuzcu", "title": "Computational RAM to Accelerate String Matching at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Von Neumann computing is falling apart in the era of exploding\ndata volumes as the overhead of data transfer becomes forbidding. Instead, it\nis more energy-efficient to fuse compute capability with memory where the data\nreside. This is particularly critical for pattern matching, a key computational\nstep in large-scale data analytics, which involves repetitive search over very\nlarge databases residing in memory. Emerging spintronic technologies show\nremarkable versatility for the tight integration of logic and memory. In this\npaper, we introduce CRAM-PM, a novel high-density, reconfigurable spintronic\nin-memory compute substrate for pattern matching.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 02:41:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Chowdhury", "Zamshed I.", ""], ["Khatamifard", "S. Karen", ""], ["Zhao", "Zhengyang", ""], ["Zabihi", "Masoud", ""], ["Resch", "Salonik", ""], ["Razaviyayn", "Meisam", ""], ["Wang", "Jian-Ping", ""], ["Sapatnekar", "Sachin", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "1812.09357", "submitter": "Onur Dizdar PhD", "authors": "Onur Dizdar", "title": "A Complexity Reduction Method for Successive Cancellation List Decoding", "comments": "6 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief introduces a hardware complexity reduction method for successive\ncancellation list (SCL) decoders. Specifically, we propose to use a sorting\nscheme so that L paths with smallest path metrics are also sorted according to\ntheir path indexes for path pruning. We prove that such sorting scheme reduces\nthe input number of multiplexers in any hardware implementation of SCL decoding\nfrom L to (L/2+1) without any changes in the decoding latency. We also propose\nsorter architectures for the proposed sorting method. Field programmable gate\narray (FPGA) implementations show that the proposed method achieves significant\ngain in hardware consumptions of SCL decoder implementations, especially for\nlarge list sizes and block lengths.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:05:42 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 17:48:47 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Dizdar", "Onur", ""]]}, {"id": "1812.10011", "submitter": "Ghasem Pasandi", "authors": "Ghasem Pasandi and Sied Mehdi Fakhraei", "title": "A 256kb 9T Near-Threshold SRAM With 1k Cells per Bit-Line and Enhanced\n  Write and Read Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new 9T SRAM cell that has good write-ability and\nimproves read stability at the same time. Simulation results show that the\nproposed design increases Read SNM (RSNM) and Ion/Ioff of read path by 219% and\n113%, respectively at supply voltage of 300mV over conventional 6T SRAM cell in\na 90nm CMOS technology. Proposed design lets us to reduce minimum operating\nvoltage of SRAM (VDDmin) to 350mV, whereas conventional 6T SRAM cannot operate\nsuccessfully with acceptable failure rate at supply voltages bellow 725mV. We\nalso compared our design with three other SRAM cells from recent literature. To\nverify the proposed design, a 256kb SRAM is designed using new 9T and\nconventional 6T SRAM cells. Operating at their minimum possible VDDs, the\nproposed design decreases write and read power per operation by 92%, and 93%,\nrespectively over the conventional rival. Area of proposed SRAM cell is\nincreased by 83% over conventional 6T one. However, due to large Ion/Ioff of\nread path for 9T cell, we are able to put 1k cells in each column of 256kb SRAM\nblock, resulting in the possibility for sharing write and read circuitries of\neach column between more cells compared to conventional 6T. Thus, area overhead\nof 256kb SRAM based on new 9T cell is reduced to 37% compared to 6T SRAM.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 01:51:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 19:23:20 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Pasandi", "Ghasem", ""], ["Fakhraei", "Sied Mehdi", ""]]}, {"id": "1812.11677", "submitter": "Ao Ren", "authors": "Ao Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian,\n  Xue Lin, Yanzhi Wang", "title": "ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using\n  Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate efficient embedded and hardware implementations of deep neural\nnetworks (DNNs), two important categories of DNN model compression techniques:\nweight pruning and weight quantization are investigated. The former leverages\nthe redundancy in the number of weights, whereas the latter leverages the\nredundancy in bit representation of weights. However, there lacks a systematic\nframework of joint weight pruning and quantization of DNNs, thereby limiting\nthe available model compression ratio. Moreover, the computation reduction,\nenergy efficiency improvement, and hardware performance overhead need to be\naccounted for besides simply model size reduction.\n  To address these limitations, we present ADMM-NN, the first\nalgorithm-hardware co-optimization framework of DNNs using Alternating\nDirection Method of Multipliers (ADMM), a powerful technique to deal with\nnon-convex optimization problems with possibly combinatorial constraints. The\nfirst part of ADMM-NN is a systematic, joint framework of DNN weight pruning\nand quantization using ADMM. It can be understood as a smart regularization\ntechnique with regularization target dynamically updated in each ADMM\niteration, thereby resulting in higher performance in model compression than\nprior work. The second part is hardware-aware DNN optimizations to facilitate\nhardware-level implementations.\n  Without accuracy loss, we can achieve 85$\\times$ and 24$\\times$ pruning on\nLeNet-5 and AlexNet models, respectively, significantly higher than prior work.\nThe improvement becomes more significant when focusing on computation\nreductions. Combining weight pruning and quantization, we achieve 1,910$\\times$\nand 231$\\times$ reductions in overall model size on these two benchmarks, when\nfocusing on data storage. Highly promising results are also observed on other\nrepresentative DNNs such as VGGNet and ResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 02:26:48 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ren", "Ao", ""], ["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Li", "Jiayu", ""], ["Xu", "Wenyao", ""], ["Qian", "Xuehai", ""], ["Lin", "Xue", ""], ["Wang", "Yanzhi", ""]]}]