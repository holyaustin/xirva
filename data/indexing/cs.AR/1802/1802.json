[{"id": "1802.00320", "submitter": "Saugata Ghose", "authors": "Saugata Ghose, Kevin Hsieh, Amirali Boroumand, Rachata\n  Ausavarungnirun, Onur Mutlu", "title": "Enabling the Adoption of Processing-in-Memory: Challenges, Mechanisms,\n  Future Research Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor DRAM technology scaling over the course of many years has caused\nDRAM-based main memory to increasingly become a larger system bottleneck. A\nmajor reason for the bottleneck is that data stored within DRAM must be moved\nacross a pin-limited memory channel to the CPU before any computation can take\nplace. This requires a high latency and energy overhead, and the data often\ncannot benefit from caching in the CPU, making it difficult to amortize the\noverhead.\n  Modern 3D-stacked DRAM architectures include a logic layer, where compute\nlogic can be integrated underneath multiple layers of DRAM cell arrays within\nthe same chip. Architects can take advantage of the logic layer to perform\nprocessing-in-memory (PIM), or near-data processing. In a PIM architecture, the\nlogic layer within DRAM has access to the high internal bandwidth available\nwithin 3D-stacked DRAM (which is much greater than the bandwidth available\nbetween DRAM and the CPU). Thus, PIM architectures can effectively free up\nvaluable memory channel bandwidth while reducing system energy consumption.\n  A number of important issues arise when we add compute logic to DRAM. In\nparticular, the logic does not have low-latency access to common CPU structures\nthat are essential for modern application execution, such as the virtual memory\nand cache coherence mechanisms. To ease the widespread adoption of PIM, we\nideally would like to maintain traditional virtual memory abstractions and the\nshared memory programming model. This requires efficient mechanisms that can\nprovide logic in DRAM with access to CPU structures without having to\ncommunicate frequently with the CPU. To this end, we propose and evaluate two\ngeneral-purpose solutions that minimize unnecessary off-chip communication for\nPIM architectures. We show that both mechanisms improve the performance and\nenergy consumption of many important memory-intensive applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 15:00:38 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Ghose", "Saugata", ""], ["Hsieh", "Kevin", ""], ["Boroumand", "Amirali", ""], ["Ausavarungnirun", "Rachata", ""], ["Mutlu", "Onur", ""]]}, {"id": "1802.00438", "submitter": "Hamid Reza Zohouri", "authors": "Hamid Reza Zohouri, Artur Podobas, Satoshi Matsuoka", "title": "Combined Spatial and Temporal Blocking for High-Performance Stencil\n  Computation on FPGAs Using OpenCL", "comments": "FPGA '18: 2018 ACM/SIGDA International Symposium on\n  Field-Programmable Gate Arrays", "journal-ref": null, "doi": "10.1145/3174243.3174248", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in High Level Synthesis tools have attracted software\nprogrammers to accelerate their high-performance computing applications on\nFPGAs. Even though it has been shown that FPGAs can compete with GPUs in terms\nof performance for stencil computation, most previous work achieve this by\navoiding spatial blocking and restricting input dimensions relative to FPGA\non-chip memory. In this work we create a stencil accelerator using Intel FPGA\nSDK for OpenCL that achieves high performance without having such restrictions.\nWe combine spatial and temporal blocking to avoid input size restrictions, and\nemploy multiple FPGA-specific optimizations to tackle issues arisen from the\nadded design complexity. Accelerator parameter tuning is guided by our\nperformance model, which we also use to project performance for the upcoming\nIntel Stratix 10 devices. On an Arria 10 GX 1150 device, our accelerator can\nreach up to 760 and 375 GFLOP/s of compute performance, for 2D and 3D stencils,\nrespectively, which rivals the performance of a highly-optimized GPU\nimplementation. Furthermore, we estimate that the upcoming Stratix 10 devices\ncan achieve a performance of up to 3.5 TFLOP/s and 1.6 TFLOP/s for 2D and 3D\nstencil computation, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 10:55:40 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Zohouri", "Hamid Reza", ""], ["Podobas", "Artur", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1802.00580", "submitter": "Carlo Condo", "authors": "Gabriele Coppolino, Carlo Condo, Guido Masera, Warren J. Gross", "title": "A Multi-Kernel Multi-Code Polar Decoder Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polar codes have received increasing attention in the past decade, and have\nbeen selected for the next generation of wireless communication standard. Most\nresearch on polar codes has focused on codes constructed from a $2\\times2$\npolarization matrix, called binary kernel: codes constructed from binary\nkernels have code lengths that are bound to powers of $2$. A few recent works\nhave proposed construction methods based on multiple kernels of different\ndimensions, not only binary ones, allowing code lengths different from powers\nof $2$. In this work, we design and implement the first multi-kernel successive\ncancellation polar code decoder in literature. It can decode any code\nconstructed with binary and ternary kernels: the architecture, sized for a\nmaximum code length $N_{max}$, is fully flexible in terms of code length, code\nrate and kernel sequence. The decoder can achieve frequency of more than $1$\nGHz in $65$ nm CMOS technology, and a throughput of $615$ Mb/s. The area\noccupation ranges between $0.11$ mm$^2$ for $N_{max}=256$ and $2.01$ mm$^2$ for\n$N_{max}=4096$. Implementation results show an unprecedented degree of\nflexibility: with $N_{max}=4096$, up to $55$ code lengths can be decoded with\nthe same hardware, along with any kernel sequence and code rate.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 07:02:19 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Coppolino", "Gabriele", ""], ["Condo", "Carlo", ""], ["Masera", "Guido", ""], ["Gross", "Warren J.", ""]]}, {"id": "1802.00822", "submitter": "Ruizhe Cai", "authors": "Ruizhe Cai, Ao Ren, Ning Liu, Caiwen Ding, Luhao Wang, Xuehai Qian,\n  Massoud Pedram, Yanzhi Wang", "title": "VIBNN: Hardware Acceleration of Bayesian Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3173162.3173212", "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Neural Networks (BNNs) have been proposed to address the problem of\nmodel uncertainty in training and inference. By introducing weights associated\nwith conditioned probability distributions, BNNs are capable of resolving the\noverfitting issue commonly seen in conventional neural networks and allow for\nsmall-data training, through the variational inference process. Frequent usage\nof Gaussian random variables in this process requires a properly optimized\nGaussian Random Number Generator (GRNG). The high hardware cost of conventional\nGRNG makes the hardware implementation of BNNs challenging.\n  In this paper, we propose VIBNN, an FPGA-based hardware accelerator design\nfor variational inference on BNNs. We explore the design space for massive\namount of Gaussian variable sampling tasks in BNNs. Specifically, we introduce\ntwo high performance Gaussian (pseudo) random number generators: the RAM-based\nLinear Feedback Gaussian Random Number Generator (RLF-GRNG), which is inspired\nby the properties of binomial distribution and linear feedback logics; and the\nBayesian Neural Network-oriented Wallace Gaussian Random Number Generator. To\nachieve high scalability and efficient memory access, we propose a deep\npipelined accelerator architecture with fast execution and good hardware\nutilization. Experimental results demonstrate that the proposed VIBNN\nimplementations on an FPGA can achieve throughput of 321,543.4 Images/s and\nenergy efficiency upto 52,694.8 Images/J while maintaining similar accuracy as\nits software counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 19:26:59 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Cai", "Ruizhe", ""], ["Ren", "Ao", ""], ["Liu", "Ning", ""], ["Ding", "Caiwen", ""], ["Wang", "Luhao", ""], ["Qian", "Xuehai", ""], ["Pedram", "Massoud", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1802.02138", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Jiashen Cao, Matthew Woodward, Michael S. Ryoo, Hyesoon\n  Kim", "title": "Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of Internet of things (IoT) devices and abundance of sensor\ndata has created an increase in real-time data processing such as recognition\nof speech, image, and video. While currently such processes are offloaded to\nthe computationally powerful cloud system, a localized and distributed approach\nis desirable because (i) it preserves the privacy of users and (ii) it omits\nthe dependency on cloud services. However, IoT networks are usually composed of\nresource-constrained devices, and a single device is not powerful enough to\nprocess real-time data. To overcome this challenge, we examine data and model\nparallelism for such devices in the context of deep neural networks. We propose\nMusical Chair to enable efficient, localized, and dynamic real-time recognition\nby harvesting the aggregated computational power from the resource-constrained\ndevices in the same IoT network as input sensors. Musical chair adapts to the\navailability of computing devices at runtime and adjusts to the inherit\ndynamics of IoT networks. To demonstrate Musical Chair, on a network of\nRaspberry PIs (up to 12) each connected to a camera, we implement a\nstate-of-the-art action recognition model for videos and two recognition models\nfor images. Compared to the Tegra TX2, an embedded low-power platform with a\nsix-core CPU and a GPU, our distributed action recognition system achieves not\nonly similar energy consumption but also twice the performance of the TX2.\nFurthermore, in image recognition, Musical Chair achieves similar performance\nand saves dynamic energy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:32:35 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 23:45:14 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 16:21:24 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Cao", "Jiashen", ""], ["Woodward", "Matthew", ""], ["Ryoo", "Michael S.", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1802.02573", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar, Kevin Hsieh, Gennady Pekhimenko, Samira Khan,\n  Ashish Shrestha, Saugata Ghose, Phillip B. Gibbons, Onur Mutlu", "title": "Zorua: Enhancing Programming Ease, Portability, and Performance in GPUs\n  by Decoupling Programming Models from Resource Management", "comments": null, "journal-ref": null, "doi": null, "report-no": "SAFARI Technical Report 2016-005", "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application resource specification--a static specification of several\nparameters such as the number of threads and the scratchpad memory usage per\nthread block--forms a critical component of the existing GPU programming\nmodels. This specification determines the performance of the application during\nexecution because the corresponding on-chip hardware resources are allocated\nand managed purely based on this specification. This tight coupling between the\nsoftware-provided resource specification and resource management in hardware\nleads to significant challenges in programming ease, portability, and\nperformance, as we demonstrate in this work.\n  Our goal in this work is to reduce the dependence of performance on the\nsoftware-provided resource specification to simultaneously alleviate the above\nchallenges. To this end, we introduce Zorua, a new resource virtualization\nframework, that decouples the programmer-specified resource usage of a GPU\napplication from the actual allocation in the on-chip hardware resources. Zorua\nenables this decoupling by virtualizing each resource transparently to the\nprogrammer.\n  We demonstrate that by providing the illusion of more resources than\nphysically available, Zorua offers several important benefits: (i) Programming\nEase: Zorua eases the burden on the programmer to provide code that is tuned to\nefficiently utilize the physically available on-chip resources. (ii)\nPortability: Zorua alleviates the necessity of re-tuning an application's\nresource usage when porting the application across GPU generations. (iii)\nPerformance: By dynamically allocating resources and carefully oversubscribing\nthem when necessary, Zorua improves or retains the performance of applications\nthat are already highly tuned to best utilize the resources. The holistic\nvirtualization provided by Zorua has many other potential uses which we\ndescribe in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:07:48 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Vijaykumar", "Nandita", ""], ["Hsieh", "Kevin", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Shrestha", "Ashish", ""], ["Ghose", "Saugata", ""], ["Gibbons", "Phillip B.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1802.03310", "submitter": "Naveen Sai Madiraju", "authors": "Naveen Sai Madiraju, Naresh Kurella, Rama Valapudasu", "title": "FPGA Implementation of ECG feature extraction using Time domain analysis", "comments": "4 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electrocardiogram (ECG) feature extraction system has been developed and\nevaluated using Virtex-6 FPGA kit which belongs to Xilinx Ltd. In time domain,\nPan-Tompkins algorithm is used for QRS detection and it is followed by a\nfeature extractor block to extract ECG features. This whole system can be used\nto detect cardiac arrhythmia. The completed algorithm was implemented on\nVirtex-6(XC6VLX240-T) device and tested using hardware co-simulation in\nModelsim and simulink environment. The software generated ECG signals are\nobtained from MIT-BIH arrhythmia Database [1]. The memory and time complexities\nof the implemented design were recorded and feature extraction has been done.\nWe have achieved satisfactory results which is mainly due to parallel\nimplementation. Therefore accurate arrhythmia detection using hardware\nimplementation a viable approach.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 07:14:44 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Madiraju", "Naveen Sai", ""], ["Kurella", "Naresh", ""], ["Valapudasu", "Rama", ""]]}, {"id": "1802.03650", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, Ranjani Narayan", "title": "Achieving Efficient Realization of Kalman Filter on CGRA through\n  Algorithm-Architecture Co-design", "comments": "Accepted in ARC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present efficient realization of Kalman Filter (KF) that\ncan achieve up to 65% of the theoretical peak performance of underlying\narchitecture platform. KF is realized using Modified Faddeeva Algorithm (MFA)\nas a basic building block due to its versatility and REDEFINE Coarse Grained\nReconfigurable Architecture (CGRA) is used as a platform for experiments since\nREDEFINE is capable of supporting realization of a set algorithmic compute\nstructures at run-time on a Reconfigurable Data-path (RDP). We perform several\nhardware and software based optimizations in the realization of KF to achieve\n116% improvement in terms of Gflops over the first realization of KF. Overall,\nwith the presented approach for KF, 4-105x performance improvement in terms of\nGflops/watt over several academically and commercially available realizations\nof KF is attained. In REDEFINE, we show that our implementation is scalable and\nthe performance attained is commensurate with the underlying hardware resources\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 20:51:30 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1802.03802", "submitter": "Caroline Trippel", "authors": "Caroline Trippel, Daniel Lustig, and Margaret Martonosi", "title": "MeltdownPrime and SpectrePrime: Automatically-Synthesized Attacks\n  Exploiting Invalidation-Based Coherence Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent Meltdown and Spectre attacks highlight the importance of automated\nverification techniques for identifying hardware security vulnerabilities. We\nhave developed a tool for synthesizing microarchitecture-specific programs\ncapable of producing any user-specified hardware execution pattern of interest.\nOur tool takes two inputs: a formal description of (i) a microarchitecture in a\ndomain-specific language, and (ii) a microarchitectural execution pattern of\ninterest, e.g. a threat pattern. All programs synthesized by our tool are\ncapable of producing the specified execution pattern on the supplied\nmicroarchitecture.\n  We used our tool to specify a hardware execution pattern common to\nFlush+Reload attacks and automatically synthesized security litmus tests\nrepresentative of those that have been publicly disclosed for conducting\nMeltdown and Spectre attacks. We also formulated a Prime+Probe threat pattern,\nenabling our tool to synthesize a new variant of each---MeltdownPrime and\nSpectrePrime. Both of these new exploits use Prime+Probe approaches to conduct\nthe timing attack. They are both also novel in that they are 2-core attacks\nwhich leverage the cache line invalidation mechanism in modern cache coherence\nprotocols. These are the first proposed Prime+Probe variants of Meltdown and\nSpectre. But more importantly, both Prime attacks exploit invalidation-based\ncoherence protocols to achieve the same level of precision as a Flush+Reload\nattack. While mitigation techniques in software (e.g., barriers that prevent\nspeculation) will likely be the same for our Prime variants as for original\nSpectre and Meltdown, we believe that hardware protection against them will be\ndistinct. As a proof of concept, we implemented SpectrePrime as a C program and\nran it on an Intel x86 processor, averaging about the same accuracy as Spectre\nover 100 runs---97.9% for Spectre and 99.95% for SpectrePrime.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:36:53 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Trippel", "Caroline", ""], ["Lustig", "Daniel", ""], ["Martonosi", "Margaret", ""]]}, {"id": "1802.03806", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Kartheek Rangineni, Zahra Ghodsi, Siddharth Garg", "title": "ThUnderVolt: Enabling Aggressive Voltage Underscaling and Timing Error\n  Resilience for Energy Efficient Deep Neural Network Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerators are being increasingly deployed to boost the\nperformance and energy efficiency of deep neural network (DNN) inference. In\nthis paper we propose Thundervolt, a new framework that enables aggressive\nvoltage underscaling of high-performance DNN accelerators without compromising\nclassification accuracy even in the presence of high timing error rates. Using\npost-synthesis timing simulations of a DNN accelerator modeled on the Google\nTPU, we show that Thundervolt enables between 34%-57% energy savings on\nstate-of-the-art speech and image recognition benchmarks with less than 1% loss\nin classification accuracy and no performance loss. Further, we show that\nThundervolt is synergistic with and can further increase the energy efficiency\nof commonly used run-time DNN pruning techniques like Zero-Skip.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:51:59 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 04:10:47 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhang", "Jeff", ""], ["Rangineni", "Kartheek", ""], ["Ghodsi", "Zahra", ""], ["Garg", "Siddharth", ""]]}, {"id": "1802.03885", "submitter": "Michel Kinsy", "authors": "Mihailo Isakov and Michel A. Kinsy", "title": "ClosNets: a Priori Sparse Topologies for Faster DNN Training", "comments": "Boston Area Architecture 2018 Workshop (BARC18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-connected layers in deep neural networks (DNN) are often the throughput\nand power bottleneck during training. This is due to their large size and low\ndata reuse. Pruning dense layers can significantly reduce the size of these\nnetworks, but this approach can only be applied after training. In this work we\npropose a novel fully-connected layer that reduces the memory requirements of\nDNNs without sacrificing accuracy. We replace a dense matrix with products of\nsparse matrices whose topologies we pick in advance. This allows us to: (1)\ntrain significantly smaller networks without a loss in accuracy, and (2) store\nthe network weights without having to store connection indices. We therefore\nachieve significant training speedups due to the smaller network size, and a\nreduced amount of computation per epoch. We tested several sparse layer\ntopologies and found that Clos networks perform well due to their high path\ndiversity, shallowness, and high model accuracy. With the ClosNets, we are able\nto reduce dense layer sizes by as much as an order of magnitude without hurting\nmodel accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:16:21 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Isakov", "Mihailo", ""], ["Kinsy", "Michel A.", ""]]}, {"id": "1802.04259", "submitter": "Michel Kinsy", "authors": "Michel A. Kinsy, Donato Kava, Alan Ehret, Miguel Mark", "title": "Sphinx: A Secure Architecture Based on Binary Code Diversification and\n  Execution Obfuscation", "comments": "Boston Area Architecture 2018 Workshop (BARC18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sphinx, a hardware-software co-design architecture for binary code and\nruntime obfuscation. The Sphinx architecture uses binary code diversification\nand self-reconfigurable processing elements to maintain application\nfunctionality while obfuscating the binary code and architecture states to\nattackers. This approach dramatically reduces an attacker's ability to exploit\ninformation gained from one deployment to attack another deployment. Our\nresults show that the Sphinx is able to decouple the program's execution time,\npower and memory and I/O activities from its functionality. It is also\npractical in the sense that the system (both software and hardware) overheads\nare minimal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 04:52:53 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Kinsy", "Michel A.", ""], ["Kava", "Donato", ""], ["Ehret", "Alan", ""], ["Mark", "Miguel", ""]]}, {"id": "1802.04576", "submitter": "Chuan Zhang", "authors": "Haochuan Song (1 and 2 and 3), Frankie Fu (4), Cloud Zeng (4), Jin Sha\n  (5), Zaichen Zhang (2 and 3), Xiaohu You (3), Chuan Zhang (1 and 2 and 3)\n  ((1) Lab of Efficient Architectures for Digital-communication and\n  Signal-processing (LEADS), (2) Quantum Information Center of Southeast\n  University, (3) National Mobile Communications Research Laboratory, Southeast\n  University, China, (4) Lite-On Technology Corporation, Guangzhou, China, (5)\n  School of Electronic Science and Engineering, Nanjing University, China)", "title": "Polar-Coded Forward Error Correction for MLC NAND Flash Memory Polar FEC\n  for NAND Flash Memory", "comments": "submitted to SCIENCE CHINA: Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing storage density, high-speed, and low-cost data access,\nflash memory has inevitably become popular. Multi-level cell (MLC) NAND flash\nmemory, which can well balance the data density and memory stability, has\noccupied the largest market share of flash memory. With the aggressive memory\nscaling, however, the reliability decays sharply owing to multiple\ninterferences. Therefore, the control system should be embedded with a suitable\nerror correction code (ECC) to guarantee the data integrity and accuracy. We\nproposed the pre-check scheme which is a multi-strategy polar code scheme to\nstrike a balance between reasonable frame error rate (FER) and decoding\nlatency. Three decoders namely binary-input, quantized-soft, and pure-soft\ndecoders are embedded in this scheme. Since the calculation of soft\nlog-likelihood ratio (LLR) inputs needs multiple sensing operations and\noptional quantization boundaries, a 2-bit quantized hard-decision decoder is\nproposed to outperform the hard-decoded LDPC bit-flipping decoder with fewer\nsensing operations. We notice that polar codes have much lower computational\ncomplexity compared to LDPC codes. The stepwise maximum mutual information\n(SMMI) scheme is also proposed to obtain overlapped boundaries without\nexhausting search. The mapping scheme using Gray code is employed and proved to\nachieve better raw error performance compared to other alternatives. Hardware\narchitectures are also given in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 11:55:06 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Song", "Haochuan", "", "1 and 2 and 3"], ["Fu", "Frankie", "", "2 and 3"], ["Zeng", "Cloud", "", "2 and 3"], ["Sha", "Jin", "", "2 and 3"], ["Zhang", "Zaichen", "", "2 and 3"], ["You", "Xiaohu", "", "1 and 2 and 3"], ["Zhang", "Chuan", "", "1 and 2 and 3"]]}, {"id": "1802.04657", "submitter": "Jeff  (Jun) Zhang", "authors": "Jeff Zhang, Tianyu Gu, Kanad Basu, Siddharth Garg", "title": "Analyzing and Mitigating the Impact of Permanent Faults on a Systolic\n  Array Based Neural Network Accelerator", "comments": "To appear at IEEE VLSI Test Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their growing popularity and computational cost, deep neural networks\n(DNNs) are being targeted for hardware acceleration. A popular architecture for\nDNN acceleration, adopted by the Google Tensor Processing Unit (TPU), utilizes\na systolic array based matrix multiplication unit at its core. This paper deals\nwith the design of fault-tolerant, systolic array based DNN accelerators for\nhigh defect rate technologies. To this end, we empirically show that the\nclassification accuracy of a baseline TPU drops significantly even at extremely\nlow fault rates (as low as $0.006\\%$). We then propose two novel strategies,\nfault-aware pruning (FAP) and fault-aware pruning+retraining (FAP+T), that\nenable the TPU to operate at fault rates of up to $50\\%$, with negligible drop\nin classification accuracy (as low as $0.1\\%$) and no run-time performance\noverhead. The FAP+T does introduce a one-time retraining penalty per TPU chip\nbefore it is deployed, but we propose optimizations that reduce this one-time\npenalty to under 12 minutes. The penalty is then amortized over the entire\nlifetime of the TPU's operation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:51:35 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 06:19:12 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Zhang", "Jeff", ""], ["Gu", "Tianyu", ""], ["Basu", "Kanad", ""], ["Garg", "Siddharth", ""]]}, {"id": "1802.05100", "submitter": "Michel Kinsy", "authors": "Michel A. Kinsy, Mihailo Isakov, Alan Ehret, Donato Kava", "title": "SAPA: Self-Aware Polymorphic Architecture", "comments": "Boston Area Architecture 2018 Workshop (BARC18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a Self-Aware Polymorphic Architecture (SAPA)\ndesign approach to support emerging context-aware applications and mitigate the\nprogramming challenges caused by the ever-increasing complexity and\nheterogeneity of high performance computing systems. Through the SAPA design,\nwe examined the salient software-hardware features of adaptive computing\nsystems that allow for (1) the dynamic allocation of computing resources\ndepending on program needs (e.g., the amount of parallelism in the program) and\n(2) automatic approximation to meet program and system goals (e.g., execution\ntime budget, power constraints and computation resiliency) without the\nprogramming complexity of current multicore and many-core systems. The proposed\nadaptive computer architecture framework applies machine learning algorithms\nand control theory techniques to the application execution based on information\ncollected about the system runtime performance trade-offs. It has heterogeneous\nreconfigurable cores with fast hardware-level migration capability,\nself-organizing memory structures and hierarchies, an adaptive\napplication-aware network-on-chip, and a built-in hardware layer for dynamic,\nautonomous resource management. Our prototyped architecture performs extremely\nwell on a large pool of applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 05:13:16 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kinsy", "Michel A.", ""], ["Isakov", "Mihailo", ""], ["Ehret", "Alan", ""], ["Kava", "Donato", ""]]}, {"id": "1802.05982", "submitter": "Chuan Zhang", "authors": "Chuan Zhang (1 and 2 and 3), Yufeng Yang (1 and 2 and 3), Shunqing\n  Zhang (4), Zaichen Zhang (2 and 3), Xiaohu You (2) ((1) Lab of Efficient\n  Architectures for Digital-communication and Signal-processing (LEADS), (2)\n  National Mobile Communications Research Laboratory, (3) Quantum Information\n  Center, Southeast University, China, (4) Shanghai Institute for Advanced\n  Communications and Data Science, Shanghai University, Shanghai, China)", "title": "Residual-Based Detections and Unified Architecture for Massive MIMO\n  Uplink", "comments": "submitted to Journal of Signal Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multiple-input multiple-output (M-MIMO) technique brings better\nenergy efficiency and coverage but higher computational complexity than\nsmall-scale MIMO. For linear detections such as minimum mean square error\n(MMSE), prohibitive complexity lies in solving large-scale linear equations.\nFor a better trade-off between bit-error-rate (BER) performance and\ncomputational complexity, iterative linear algorithms like conjugate gradient\n(CG) have been applied and have shown their feasibility in recent years. In\nthis paper, residual-based detection (RBD) algorithms are proposed for M-MIMO\ndetection, including minimal residual (MINRES) algorithm, generalized minimal\nresidual (GMRES) algorithm, and conjugate residual (CR) algorithm. RBD\nalgorithms focus on the minimization of residual norm per iteration, whereas\nmost existing algorithms focus on the approximation of exact signal. Numerical\nresults have shown that, for $64$-QAM $128\\times 8$ MIMO, RBD algorithms are\nonly $0.13$ dB away from the exact matrix inversion method when BER$=10^{-4}$.\nStability of RBD algorithms has also been verified in various correlation\nconditions. Complexity comparison has shown that, CR algorithm require $87\\%$\nless complexity than the traditional method for $128\\times 60$ MIMO. The\nunified hardware architecture is proposed with flexibility, which guarantees a\nlow-complexity implementation for a family of RBD M-MIMO detectors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:54:31 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Zhang", "Chuan", "", "1 and 2 and 3"], ["Yang", "Yufeng", "", "1 and 2 and 3"], ["Zhang", "Shunqing", "", "2 and 3"], ["Zhang", "Zaichen", "", "2 and 3"], ["You", "Xiaohu", ""]]}, {"id": "1802.06195", "submitter": "Jonti Talukdar", "authors": "Bhavana Mehta, Jonti Talukdar, Sachin Gajjar", "title": "High Speed SRT Divider for Intelligent Embedded System", "comments": "IEEE Int. Conf. Soft Comp. 17 (5 Pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing development in embedded systems, VLSI and processor design have\ngiven rise to increased demands from the system in terms of power, speed, area,\nthroughput etc. Most of the sophisticated embedded system applications consist\nof processors, which now need an arithmetic unit with the ability to execute\ncomplex division operations with maximum efficiency. Hence the speed of the\narithmetic unit is critically dependent on division operation. Most of the\ndividers use the SRT division algorithm for division. In IoT and other embedded\napplications, typically radix 2 and radix 4 division algorithms are used. The\nproposed algorithm lies on parallel execution of various steps so as to reduce\ntime critical path, use fuzzy logic to solve the overlap problem in quotient\nselection, hence reducing maximum delay and increasing the accuracy. Every\nlogical circuit has a maximum delay on which the timing of the circuit is\ndependent and the path, causing the maximum delay is known as the critical\npath. Our approach uses the previous SRT algorithm methods to make a highly\nparallel pipelined design and use Mamdani model to determine a solution to the\noverlapping problem to reduce the overall execution time of radix 4 SRT\ndivision on 64 bits double precision floating point numbers to 281ns. The\ndesign is made using Bluespec System Verilog, synthesized and simulated using\nVivado v.2016.1 and implemented on Xilinx Virtex UltraScale FPGA board.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 05:20:34 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Mehta", "Bhavana", ""], ["Talukdar", "Jonti", ""], ["Gajjar", "Sachin", ""]]}, {"id": "1802.08919", "submitter": "Shahrzad Keshavarz", "authors": "Shahrzad Keshavarz and Daniel Holcomb", "title": "Privacy Leakages in Approximate Adders", "comments": "2017 IEEE International Symposium on Circuits and Systems (ISCAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing has recently emerged as a promising method to meet the\nlow power requirements of digital designs. The erroneous outputs produced in\napproximate computing can be partially a function of each chip's process\nvariation. We show that, in such schemes, the erroneous outputs produced on\neach chip instance can reveal the identity of the chip that performed the\ncomputation, possibly jeopardizing user privacy. In this work, we perform\nsimulation experiments on 32-bit Ripple Carry Adders, Carry Lookahead Adders,\nand Han-Carlson Adders running at over-scaled operating points. Our results\nshow that identification is possible, we contrast the identifiability of each\ntype of adder, and we quantify how success of identification varies with the\nextent of over-scaling and noise. Our results are the first to show that\napproximate digital computations may compromise privacy. Designers of future\napproximate computing systems should be aware of the possible privacy leakages\nand decide whether mitigation is warranted in their application.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 21:41:24 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Keshavarz", "Shahrzad", ""], ["Holcomb", "Daniel", ""]]}, {"id": "1802.09517", "submitter": "Kostya Serebryany", "authors": "Kostya Serebryany, Evgenii Stepanov, Aleksey Shlyapnikov, Vlad\n  Tsyrklevich, Dmitry Vyukov", "title": "Memory Tagging and how it improves C/C++ memory safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory safety in C and C++ remains largely unresolved. A technique usually\ncalled \"memory tagging\" may dramatically improve the situation if implemented\nin hardware with reasonable overhead. This paper describes two existing\nimplementations of memory tagging: one is the full hardware implementation in\nSPARC; the other is a partially hardware-assisted compiler-based tool for\nAArch64. We describe the basic idea, evaluate the two implementations, and\nexplain how they improve memory safety. This paper is intended to initiate a\nwider discussion of memory tagging and to motivate the CPU and OS vendors to\nadd support for it in the near future.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:58:46 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Serebryany", "Kostya", ""], ["Stepanov", "Evgenii", ""], ["Shlyapnikov", "Aleksey", ""], ["Tsyrklevich", "Vlad", ""], ["Vyukov", "Dmitry", ""]]}, {"id": "1802.10444", "submitter": "Chuan Zhang", "authors": "Chuan Zhang (1 and 2 and 3), Xiao Liang (1 and 2 and 3), Zhizhen Wu (1\n  and 2 and 3), Feng Wang (1 and 2 and 3), Shunqing Zhang (4), Zaichen Zhang (2\n  and 3), Xiaohu You (2) ((1) Lab of Efficient Architectures for\n  Digital-communication and Signal-processing (LEADS), (2) National Mobile\n  Communications Research Laboratory, (3) Quantum Information Center, Southeast\n  University, China, (4) Shanghai Institute for Advanced Communications and\n  Data Science, Shanghai University, Shanghai, China)", "title": "On the Low-Complexity, Hardware-Friendly Tridiagonal Matrix Inversion\n  for Correlated Massive MIMO Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In massive MIMO (M-MIMO) systems, one of the key challenges in the\nimplementation is the large-scale matrix inversion operation, as widely used in\nchannel estimation, equalization, detection, and decoding procedures.\nTraditionally, to handle this complexity issue, several low-complexity matrix\ninversion approximation methods have been proposed, including the classic\nCholesky decomposition and the Neumann series expansion (NSE). However, the\nconventional approaches failed to exploit neither the special structure of\nchannel matrices nor the critical issues in the hardware implementation, which\nresults in poorer throughput performance and longer processing delay. In this\npaper, by targeting at the correlated M-MIMO systems, we propose a modified NSE\nbased on tridiagonal matrix inversion approximation (TMA) to accommodate the\ncomplexity as well as the performance issue in the conventional hardware\nimplementation, and analyze the corresponding approximation errors. Meanwhile,\nwe investigate the VLSI implementation for the proposed detection algorithm\nbased on a Xilinx Virtex-7 XC7VX690T FPGA platform. It is shown that for\ncorrelated massive MIMO systems, it can achieve near-MMSE performance and $630$\nMb/s throughput. Compared with other benchmark systems, the proposed pipelined\nTMA detector can get high throughput-to-hardware ratio. Finally, we also\npropose a fast iteration structure for further research.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:51:48 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Zhang", "Chuan", "", "1 and 2 and 3"], ["Liang", "Xiao", "", "1 and 2 and 3"], ["Wu", "Zhizhen", "", "1\n  and 2 and 3"], ["Wang", "Feng", "", "1 and 2 and 3"], ["Zhang", "Shunqing", "", "2\n  and 3"], ["Zhang", "Zaichen", "", "2\n  and 3"], ["You", "Xiaohu", ""]]}]