[{"id": "1906.00327", "submitter": "Pareesa Golnari", "authors": "Pareesa Ameneh Golnari and Sharad Malik", "title": "Sparse Matrix to Matrix Multiplication: A Representation and\n  Architecture for Acceleration (long version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerators for sparse matrix multiplication are important components in\nemerging systems. In this paper, we study the main challenges of accelerating\nSparse Matrix Multiplication (SpMM). For the situations that data is not stored\nin the desired order (row/column order), we propose a compact high performance\nsparse format, which allows for random access to a dataset with low memory\naccess overhead. We show that using this format results in a 14-49 times\nspeedup for SpMM. Next, we propose a high performance systolic architecture for\nSpMM, which uses a mesh of comparators to locate the useful (non-zero)\ncomputation. This design maximizes data reuse by sharing the input data among a\nrow/column of the mesh. We also show that, with similar memory access\nassumptions, the proposed architecture results in a 9-30 times speedup in\ncomparison with the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 02:26:30 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Golnari", "Pareesa Ameneh", ""], ["Malik", "Sharad", ""]]}, {"id": "1906.00478", "submitter": "Matheus Cavalcante", "authors": "Matheus Cavalcante, Fabian Schuiki, Florian Zaruba, Michael Schaffner,\n  Luca Benini", "title": "Ara: A 1 GHz+ Scalable and Energy-Efficient RISC-V Vector Processor with\n  Multi-Precision Floating Point Support in 22 nm FD-SOI", "comments": "13 pages. Accepted for publication in IEEE Transactions on Very Large\n  Scale Integration Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Ara, a 64-bit vector processor based on the version\n0.5 draft of RISC-V's vector extension, implemented in GlobalFoundries 22FDX\nFD-SOI technology. Ara's microarchitecture is scalable, as it is composed of a\nset of identical lanes, each containing part of the processor's vector register\nfile and functional units. It achieves up to 97% FPU utilization when running a\n256 x 256 double precision matrix multiplication on sixteen lanes. Ara runs at\nmore than 1 GHz in the typical corner (TT/0.80V/25 oC) achieving a performance\nup to 33 DP-GFLOPS. In terms of energy efficiency, Ara achieves up to 41\nDP-GFLOPS/W under the same conditions, which is slightly superior to similar\nvector processors found in literature. An analysis on several vectorizable\nlinear algebra computation kernels for a range of different matrix and vector\nsizes gives insight into performance limitations and bottlenecks for vector\nprocessors and outlines directions to maintain high energy efficiency even for\nsmall matrix sizes where the vector architecture achieves suboptimal\nutilization of the available FPUs.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 20:33:22 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 09:53:17 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 17:30:24 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cavalcante", "Matheus", ""], ["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Schaffner", "Michael", ""], ["Benini", "Luca", ""]]}, {"id": "1906.00877", "submitter": "Philippos Papaphilippou", "authors": "Philippos Papaphilippou, Paul H. J. Kelly, Wayne Luk", "title": "Pangloss: a novel Markov chain prefetcher", "comments": "Accepted in The Third Data Prefetching Championship (DPC3), held in\n  conjunction with ISCA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Pangloss, an efficient high-performance data prefetcher that\napproximates a Markov chain on delta transitions. With a limited information\nscope and space/logic complexity, it is able to reconstruct a variety of both\nsimple and complex access patterns. This is achieved by a highly-efficient\nrepresentation of the Markov chain to provide accurate values for transition\nprobabilities. In addition, we have added a mechanism to reconstruct delta\ntransitions originally obfuscated by the out-of-order execution or page\ntransitions, such as when streaming data from multiple sources. Our\nsingle-level (L2) prefetcher achieves a geometric speedup of 1.7% and 3.2% over\nselected state-of-the-art baselines (KPCP and BOP). When combined with an\nequivalent for the L1 cache (L1 & L2), the speedups rise to 6.8% and 8.4%, and\n40.4% over non-prefetch. In the multi-core evaluation, there seems to be a\nconsiderable performance improvement as well.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:34:25 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Papaphilippou", "Philippos", ""], ["Kelly", "Paul H. J.", ""], ["Luk", "Wayne", ""]]}, {"id": "1906.01838", "submitter": "Hiroshi Sasaki", "authors": "Hiroshi Sasaki, Miguel A. Arroyo, M. Tarek Ibn Ziad, Koustubha Bhat,\n  Kanad Sinha, Simha Sethumadhavan", "title": "Practical Byte-Granular Memory Blacklisting using Califorms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent rapid strides in memory safety tools and hardware have improved\nsoftware quality and security. While coarse-grained memory safety has improved,\nachieving memory safety at the granularity of individual objects remains a\nchallenge due to high performance overheads which can be between ~1.7x-2.2x. In\nthis paper, we present a novel idea called Califorms, and associated program\nobservations, to obtain a low overhead security solution for practical,\nbyte-granular memory safety.\n  The idea we build on is called memory blacklisting, which prohibits a program\nfrom accessing certain memory regions based on program semantics. State of the\nart hardware-supported memory blacklisting while much faster than software\nblacklisting creates memory fragmentation (of the order of few bytes) for each\nuse of the blacklisted location. In this paper, we observe that metadata used\nfor blacklisting can be stored in dead spaces in a program's data memory and\nthat this metadata can be integrated into microarchitecture by changing the\ncache line format. Using these observations, Califorms based system proposed in\nthis paper reduces the performance overheads of memory safety to ~1.02x-1.16x\nwhile providing byte-granular protection and maintaining very low hardware\noverheads.\n  The low overhead offered by Califorms enables always on, memory safety for\nsmall and large objects alike, and the fundamental idea of storing metadata in\nempty spaces, and microarchitecture can be used for other security and\nperformance applications.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 05:52:23 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 02:00:48 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 07:10:07 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sasaki", "Hiroshi", ""], ["Arroyo", "Miguel A.", ""], ["Ziad", "M. Tarek Ibn", ""], ["Bhat", "Koustubha", ""], ["Sinha", "Kanad", ""], ["Sethumadhavan", "Simha", ""]]}, {"id": "1906.04258", "submitter": "Mehdi Safarpour", "authors": "Mehdi Safarpour, Ilkka Hautala, Miguel Bordallo Lopez, Olli Silven", "title": "Transport Triggered Array Processor for Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-level sensory data processing in many Internet-of-Things (IoT) devices\npursue energy efficiency by utilizing sleep modes or slowing the clocking to\nthe minimum. To curb the share of stand-by power dissipation in those designs,\nnear-threshold/sub-threshold operational points or ultra-low-leakage processes\nin fabrication are employed. Those limit the clocking rates significantly,\nreducing the computing throughputs of individual processing cores. In this\ncontribution we explore compensating for the performance loss of operating in\nnear-threshold region (Vdd =0.6V) through massive parallelization. Benefits of\nnear-threshold operation and massive parallelism are optimum energy consumption\nper instruction operation and minimized memory roundtrips, respectively. The\nProcessing Elements (PE) of the design are based on Transport Triggered\nArchitecture. The fine grained programmable parallel solution allows for fast\nand efficient computation of learnable low-level features (e.g. local binary\ndescriptors and convolutions). Other operations, including Max-pooling have\nalso been implemented. The programmable design achieves excellent energy\nefficiency for Local Binary Patterns computations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 20:07:16 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Safarpour", "Mehdi", ""], ["Hautala", "Ilkka", ""], ["Lopez", "Miguel Bordallo", ""], ["Silven", "Olli", ""]]}, {"id": "1906.05351", "submitter": "Sergi Abadal", "authors": "Sergi Abadal, and Eduard Alarc\\'on", "title": "Data Conversion in Area-Constrained Applications: the Wireless\n  Network-on-Chip Case", "comments": "Presented at DCIS 2018", "journal-ref": null, "doi": "10.1109/DCIS.2018.8681465", "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-on-Chip (NoC) is currently the paradigm of choice to interconnect the\ndifferent components of System-on-Chips (SoCs) or Chip Multiprocessors (CMPs).\nAs the levels of integration continue to grow, however, current NoCs face\nsignificant scalability limitations and have prompted research in novel\ninterconnect technologies. Among these, wireless intra-chip communications have\nbeen under intense scrutiny due to their low latency broadcast and\narchitectural flexibility. Thus far, the practicality of the idea has been\nstudied from the RF front-end and the network interface perspectives, whereas\nlittle to no attention has been placed on another essential component: the data\nconverters. This article aims to fill this gap by providing a comprehensive\nanalysis of the requirements of the scenario, as well as of the current\nperformance and cost trends of Analog-to-Digital Converters (ADCs). Based on\nMurmann's data, we demonstrate that ADCs will not be a roadblock for the\nrealization of wireless intra-chip communications although current designs do\nnot meet their demands fully.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:46:03 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Abadal", "Sergi", ""], ["Alarc\u00f3n", "Eduard", ""]]}, {"id": "1906.05922", "submitter": "Bing Li Dr.", "authors": "Bing Li, Mengjie Mao, Xiaoxiao Liu, Tao Liu, Zihao Liu, Wujie Wen,\n  Yiran Chen, Hai (Helen) Li", "title": "Thread Batching for High-performance Energy-efficient GPU Memory Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multi-threading in GPU imposes tremendous pressure on memory\nsubsystems. Due to rapid growth in thread-level parallelism of GPU and slowly\nimproved peak memory bandwidth, the memory becomes a bottleneck of GPU's\nperformance and energy efficiency. In this work, we propose an integrated\narchitectural scheme to optimize the memory accesses and therefore boost the\nperformance and energy efficiency of GPU. Firstly, we propose a thread batch\nenabled memory partitioning (TEMP) to improve GPU memory access parallelism. In\nparticular, TEMP groups multiple thread blocks that share the same set of pages\ninto a thread batch and applies a page coloring mechanism to bound each stream\nmultiprocessor (SM) to the dedicated memory banks. After that, TEMP dispatches\nthe thread batch to an SM to ensure high-parallel memory-access streaming from\nthe different thread blocks. Secondly, a thread batch-aware scheduling (TBAS)\nscheme is introduced to improve the GPU memory access locality and to reduce\nthe contention on memory controllers and interconnection networks. Experimental\nresults show that the integration of TEMP and TBAS can achieve up to 10.3%\nperformance improvement and 11.3% DRAM energy reduction across diverse GPU\napplications. We also evaluate the performance interference of the mixed\nCPU+GPU workloads when they are run on a heterogeneous system that employs our\nproposed schemes. Our results show that a simple solution can effectively\nensure the efficient execution of both GPU and CPU applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:44:51 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Li", "Bing", "", "Helen"], ["Mao", "Mengjie", "", "Helen"], ["Liu", "Xiaoxiao", "", "Helen"], ["Liu", "Tao", "", "Helen"], ["Liu", "Zihao", "", "Helen"], ["Wen", "Wujie", "", "Helen"], ["Chen", "Yiran", "", "Helen"], ["Hai", "", "", "Helen"], ["Li", "", ""]]}, {"id": "1906.06603", "submitter": "Bing Li Dr.", "authors": "Bing Li, Bonan Yan, Hai (Helen) Li", "title": "An Overview of In-memory Processing with Emerging Non-volatile Memory\n  for Data-intensive Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3299874.3319452", "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional von Neumann architecture has been revealed as a major\nperformance and energy bottleneck for rising data-intensive applications. %,\ndue to the intensive data movements. The decade-old idea of leveraging\nin-memory processing to eliminate substantial data movements has returned and\nled extensive research activities. The effectiveness of in-memory processing\nheavily relies on memory scalability, which cannot be satisfied by traditional\nmemory technologies. Emerging non-volatile memories (eNVMs) that pose appealing\nqualities such as excellent scaling and low energy consumption, on the other\nhand, have been heavily investigated and explored for realizing in-memory\nprocessing architecture. In this paper, we summarize the recent research\nprogress in eNVM-based in-memory processing from various aspects, including the\nadopted memory technologies, locations of the in-memory processing in the\nsystem, supported arithmetics, as well as applied applications.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 18:46:40 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Li", "Bing", "", "Helen"], ["Yan", "Bonan", "", "Helen"], ["Hai", "", "", "Helen"], ["Li", "", ""]]}, {"id": "1906.08170", "submitter": "Chit-Kwan Lin", "authors": "Chit-Kwan Lin and Stephen J. Tarsa", "title": "Branch Prediction Is Not a Solved Problem: Measurements, Opportunities,\n  and Future Directions", "comments": "11 pages, 10 figures. Submitted to IISWC 2019", "journal-ref": null, "doi": "10.1109/IISWC47752.2019.9042108", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern branch predictors predict the vast majority of conditional branch\ninstructions with near-perfect accuracy, allowing superscalar, out-of-order\nprocessors to maximize speculative efficiency and thus performance. However,\nthis impressive overall effectiveness belies a substantial missed opportunity\nin single-threaded instructions per cycle (IPC). For example, we show that\ncorrecting the mispredictions made by the state-of-the-art TAGE-SC-L branch\npredictor on SPECint 2017 would improve IPC by margins similar to an advance in\nprocess technology node.\n  In this work, we measure and characterize these mispredictions. We find that\nthey categorically arise from either (1) a small number of systematically\nhard-to-predict (H2P) branches; or (2) rare branches with low dynamic execution\ncounts. Using data from SPECint 2017 and additional large code footprint\napplications, we quantify the occurrence and IPC impact of these two\ncategories. We then demonstrate that increasing the resources afforded to\nexisting branch predictors does not alone address the root causes of most\nmispredictions. This leads us to reexamine basic assumptions in branch\nprediction and to propose new research directions that, for example, deploy\nmachine learning to improve pattern matching for H2Ps, and use on-chip phase\nlearning to track long-term statistics for rare branches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:50:31 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lin", "Chit-Kwan", ""], ["Tarsa", "Stephen J.", ""]]}, {"id": "1906.09380", "submitter": "Omer Anjum", "authors": "Omer Anjum, Wen-Mei Hwu, Jinjun Xiong", "title": "A Retrospective Recount of Computer Architecture Research with a\n  Data-Driven Study of Over Four Decades of ISCA Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study began with a research project, called DISCvR, conducted at the\nIBM-ILLINOIS Center for Cognitive Computing Systems Reseach. The goal of DISCvR\nwas to build a practical NLP based AI pipeline for document understanding which\nwill help us better understand the computation patterns and requirements of\nmodern computing systems. While building such a prototype, an early use case\ncame to us thanks to the 2017 IEEE/ACM International Symposium on\nMicroarchitecture (MICRO-50) Program Co-chairs, Drs. Hillery Hunter and Jaime\nMoreno. They asked us if we can perform some data-driven analysis of the past\n50 years of MICRO papers and show some interesting historical perspectives on\nMICRO's 50 years of publication. We learned two important lessons from that\nexperience: (1) building an AI solution to truly understand unstructured data\nis hard in spite of the many claimed successes in natural language\nunderstanding; and (2) providing a data-driven perspective on computer\narchitecture research is a very interesting and fun project. Recently we\ndecided to conduct a more thorough study based on all past papers of\nInternational Symposium on Computer Architecture (ISCA) from 1973 to 2018,\nwhich resulted this article. We recognize that we have just scratched the\nsurface of natural language understanding of unstructured data, and there are\nmany more aspects that we can improve. But even with our current study, we felt\nthere were enough interesting findings that may be worthwhile to share with the\ncommunity. Hence we decided to write this article to summarize our findings so\nfar based only on ISCA publications. Our hope is to generate further interests\nfrom the community in this topic, and we welcome collaboration from the\ncommunity to deepen our understanding both of the computer architecture\nresearch and of the challenges of NLP-based AI solutions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 03:38:41 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Anjum", "Omer", ""], ["Hwu", "Wen-Mei", ""], ["Xiong", "Jinjun", ""]]}, {"id": "1906.09395", "submitter": "Jason Kamran Jr Eshraghian", "authors": "Jaeheum Lee, Jason K. Eshraghian, Kyoungrok Cho, Kamran Eshraghian", "title": "Adaptive Precision CNN Accelerator Using Radix-X Parallel Connected\n  Memristor Crossbars", "comments": "12 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural processor development is reducing our reliance on remote server access\nto process deep learning operations in an increasingly edge-driven world. By\nemploying in-memory processing, parallelization techniques, and\nalgorithm-hardware co-design, memristor crossbar arrays are known to\nefficiently compute large scale matrix-vector multiplications. However,\nstate-of-the-art implementations of negative weights require duplicative column\nwires, and high precision weights using single-bit memristors further\ndistributes computations. These constraints dramatically increase chip area and\nresistive losses, which lead to increased power consumption and reduced\naccuracy. In this paper, we develop an adaptive precision method by varying the\nnumber of memristors at each crosspoint. We also present a weight mapping\nalgorithm designed for implementation on our crossbar array. This novel\nalgorithm-hardware solution is described as the radix-X Convolutional Neural\nNetwork Crossbar Array, and demonstrate how to efficiently represent negative\nweights using a single column line, rather than double the number of additional\ncolumns. Using both simulation and experimental results, we verify that our\nradix-5 CNN array achieves a validation accuracy of 90.5% on the CIFAR-10\ndataset, a 4.5% improvement over binarized neural networks whilst\nsimultaneously reducing crossbar area by 46% over conventional arrays by\nremoving the need for duplicate columns to represent signed weights.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 06:14:24 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lee", "Jaeheum", ""], ["Eshraghian", "Jason K.", ""], ["Cho", "Kyoungrok", ""], ["Eshraghian", "Kamran", ""]]}, {"id": "1906.10666", "submitter": "Huimei Cheng", "authors": "Huimei Cheng, Yichen Gu, Peter A. Beerel", "title": "Automatic Conversion from Flip-flop to 3-phase Latch-based Designs", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latch-based designs have many benefits over their flip-flop based\ncounterparts but have limited use partially because most RTL specifications are\nflop-centric and automatic conversion of FF to latch-based designs is\nchallenging. Conventional conversion algorithms target master-slave latch-based\ndesigns with two non-overlapping clocks. This paper presents a novel automated\ndesign flow that converts flip-flop to 3-phase latch-based designs. The\nresulting circuits have the same performance as the master-slave based designs\nbut require significantly less latches. Our experimental results demonstrate\nthe potential for savings in the number of latches (21.3%), area (5.8%), and\npower (16.3%) on a variety of ISCAS, CEP, and CPU benchmark circuits, compared\nto the master-slave conversions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 17:04:39 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Cheng", "Huimei", ""], ["Gu", "Yichen", ""], ["Beerel", "Peter A.", ""]]}, {"id": "1906.11175", "submitter": "Yann Beilliard", "authors": "Yann Beilliard, Maxime Godard, Aggelos Ioannou, Astrinos Damianakis,\n  Michael Ligerakis, Iakovos Mavroidis, Pierre-Yves Martinez, David Danovitch,\n  Julien Sylvestre, Dominique Drouin", "title": "FPGA-based Multi-Chip Module for High-Performance Computing", "comments": "HiPEAC 2019 - ExaNoDe Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current integration, architectural design and manufacturing technologies are\nnot suited for the computing density and power efficiency requested by Exascale\ncomputing. New approaches in hardware architecture are thus needed to overcome\nthe technological barriers preventing the transition to the Exascale era. In\nthat scope, we report successful fabrication of first ExaNoDe's MCM prototypes\ndedicated to Exascale computing applications. Each MCM was composed of 2 Xilinx\nZynq Ultrascale+ MPSoC, assembled on advanced 68.5 mm x 55 mm laminate\nsubstrates specifically designed and fabricated for the project. Acoustic\nmicroscopy, x-ray, cross-section and Thermo-Moire investigations revealed no\nvoids, shorts, delamination, cracks or warpage issues. Two MCMs were mounted on\na daughter board by FORTH for testing purposes. The DDR memories on the 4\nSODIMMs of the daughter board were successfully tested by running extensive\nXilinx memory tests with clock frequencies of 1866 MHz and 2133 MHz. All 4\nFPGAs were programmed with the Xilinx integrated bit error ratio test (IBERT)\ntailored for this board for links testing. All intra-board high-speed links\nbetween all FPGAs were stable at 10 Gbps, even under the more demanding 31-bit\nPRBS (Pseudorandom Binary Sequence) tests.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:43:34 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Beilliard", "Yann", ""], ["Godard", "Maxime", ""], ["Ioannou", "Aggelos", ""], ["Damianakis", "Astrinos", ""], ["Ligerakis", "Michael", ""], ["Mavroidis", "Iakovos", ""], ["Martinez", "Pierre-Yves", ""], ["Danovitch", "David", ""], ["Sylvestre", "Julien", ""], ["Drouin", "Dominique", ""]]}, {"id": "1906.11915", "submitter": "Soroush Ghodrati", "authors": "Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh,\n  Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh", "title": "Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through\n  Interleaved Bit-Partitioned Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-power potential of mixed-signal design makes it an alluring option to\naccelerate Deep Neural Networks (DNNs). However, mixed-signal circuitry suffers\nfrom limited range for information encoding, susceptibility to noise, and\nAnalog to Digital (A/D) conversion overheads. This paper aims to address these\nchallenges by offering and leveraging the insight that a vector dot-product\n(the basic operation in DNNs) can be bit-partitioned into groups of spatially\nparallel low-bitwidth operations, and interleaved across multiple elements of\nthe vectors. As such, the building blocks of our accelerator become a group of\nwide, yet low-bitwidth multiply-accumulate units that operate in the analog\ndomain and share a single A/D converter. The low-bitwidth operation tackles the\nencoding range limitation and facilitates noise mitigation. Moreover, we\nutilize the switched-capacitor design for our bit-level reformulation of DNN\noperations. The proposed switched-capacitor circuitry performs the group\nmultiplications in the charge domain and accumulates the results of the group\nin its capacitors over multiple cycles. The capacitive accumulation combined\nwith wide bit-partitioned operations alleviate the need for A/D conversion per\noperation. With such mathematical reformulation and its switched-capacitor\nimplementation, we define a 3D-stacked microarchitecture, dubbed BIHIWE.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:09:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 16:14:50 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 17:03:50 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Ghodrati", "Soroush", ""], ["Sharma", "Hardik", ""], ["Kinzer", "Sean", ""], ["Yazdanbakhsh", "Amir", ""], ["Samadi", "Kambiz", ""], ["Kim", "Nam Sung", ""], ["Burger", "Doug", ""], ["Esmaeilzadeh", "Hadi", ""]]}]