[{"id": "1907.00048", "submitter": "Georg Hager", "authors": "Johannes Hofmann, Christie L. Alappat, Georg Hager, Dietmar Fey,\n  Gerhard Wellein", "title": "Bridging the Architecture Gap: Abstracting Performance-Relevant\n  Properties of Modern Server Processors", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.14529/jsfi200204", "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a universal modeling approach for predicting single- and\nmulticore runtime of steady-state loops on server processors. To this end we\nstrictly differentiate between application and machine models: An application\nmodel comprises the loop code, problem sizes, and other runtime parameters,\nwhile a machine model is an abstraction of all performance-relevant properties\nof a CPU. We introduce a generic method for determining machine models and\npresent results for relevant server-processor architectures by Intel, AMD, IBM,\nand Marvell/Cavium. Considering this wide range of architectures, the set of\nfeatures required for adequate performance modeling is surprisingly small. To\nvalidate our approach, we compare performance predictions to empirical data for\nan OpenMP-parallel preconditioned CG algorithm, which includes compute- and\nmemory-bound kernels. Both single- and multicore analysis shows that the model\nexhibits average and maximum relative errors of 5% and 10%. Deviations from the\nmodel and insights gained are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:04:32 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hofmann", "Johannes", ""], ["Alappat", "Christie L.", ""], ["Hager", "Georg", ""], ["Fey", "Dietmar", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1907.00271", "submitter": "Kartik Hegde", "authors": "Kartik Hegde, Abhishek Srivastava, Rohit Agrawal", "title": "HTS: A Hardware Task Scheduler for Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the Moore's scaling era comes to an end, application specific hardware\naccelerators appear as an attractive way to improve the performance and power\nefficiency of our computing systems. A massively heterogeneous system with a\nlarge number of hardware accelerators along with multiple general purpose CPUs\nis a promising direction, but pose several challenges in terms of the run-time\nscheduling of tasks on the accelerators and design granularity of accelerators.\nThis paper addresses these challenges by developing an example heterogeneous\nsystem to enable multiple applications to share the available accelerators. We\npropose to design accelerators at a lower abstraction to enable applications to\nbe broken down into tasks that can be mapped on several accelerators. We\nobserve that several real-life workloads can be broken down into common\nprimitives that are shared across many workloads. Finally, we propose and\ndesign a hardware task scheduler inspired by the hardware schedulers in\nout-of-order superscalar processors to efficiently utilize the accelerators in\nthe system by scheduling tasks in out-of-order and even speculatively. We\nevaluate the proposed system on both real-life and synthetic benchmarks based\non Digital Signal Processing~(DSP) applications. Compared to executing the\nbenchmark on a system with sequential scheduling, proposed scheduler achieves\nup to 12x improvement in performance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:59:21 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hegde", "Kartik", ""], ["Srivastava", "Abhishek", ""], ["Agrawal", "Rohit", ""]]}, {"id": "1907.01112", "submitter": "Yongjune Kim", "authors": "Yongjune Kim, Won Ho Choi, Cyril Guyot, Yuval Cassuto", "title": "On the Optimal Refresh Power Allocation for Energy-Efficient Memories", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/GLOBECOM38437.2019.9013465", "report-no": null, "categories": "cs.AR cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refresh is an important operation to prevent loss of data in dynamic\nrandom-access memory (DRAM). However, frequent refresh operations incur\nconsiderable power consumption and degrade system performance. Refresh power\ncost is especially significant in high-capacity memory devices and\nbattery-powered edge/mobile applications. In this paper, we propose a\nprincipled approach to optimizing the refresh power allocation. Given a model\nfor the bit error rate dependence on power, we formulate a convex optimization\nproblem to minimize the word mean squared error for a refresh power constraint;\nhence we can guarantee the optimality of the obtained refresh power\nallocations. In addition, we provide an integer programming problem to optimize\nthe discrete refresh interval assignments. For an 8-bit accessed word,\nnumerical results show that the optimized nonuniform refresh intervals reduce\nthe refresh power by 29% at a peak signal-to-noise ratio of 50dB compared to\nthe uniform assignment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:49:11 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 17:44:05 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kim", "Yongjune", ""], ["Choi", "Won Ho", ""], ["Guyot", "Cyril", ""], ["Cassuto", "Yuval", ""]]}, {"id": "1907.01522", "submitter": "Kaiqi Zhang", "authors": "Kaiqi Zhang, Xiyuan Zhang and Zheng Zhang", "title": "Tucker Tensor Decomposition on FPGA", "comments": "Accepted by ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor computation has emerged as a powerful mathematical tool for solving\nhigh-dimensional and/or extreme-scale problems in science and engineering. The\nlast decade has witnessed tremendous advancement of tensor computation and its\napplications in machine learning and big data. However, its hardware\noptimization on resource-constrained devices remains an (almost) unexplored\nfield. This paper presents an hardware accelerator for a classical tensor\ncomputation framework, Tucker decomposition. We study three modules of this\narchitecture: tensor-times-matrix (TTM), matrix singular value decomposition\n(SVD), and tensor permutation, and implemented them on Xilinx FPGA for\nprototyping. In order to further reduce the computing time, a warm-start\nalgorithm for the Jacobi iterations in SVD is proposed. A fixed-point simulator\nis used to evaluate the performance of our design. Some synthetic data sets and\na real MRI data set are used to validate the design and evaluate its\nperformance. We compare our work with state-of-the-art software toolboxes\nrunning on both CPU and GPU, and our work shows 2.16 - 30.2x speedup on the\ncardiac MRI data set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:22:41 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Zhang", "Kaiqi", ""], ["Zhang", "Xiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1907.02167", "submitter": "Vinson Young", "authors": "Vinson Young, Moinuddin K. Qureshi", "title": "To Update or Not To Update?: Bandwidth-Efficient Intelligent Replacement\n  Policies for DRAM Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates intelligent replacement policies for improving the\nhit-rate of gigascale DRAM caches. Cache replacement policies are commonly used\nto improve the hit-rate of on-chip caches. The most effective replacement\npolicies often require the cache to track per-line reuse state to inform their\ndecision. A fundamental challenge on DRAM caches, however, is that stateful\npolicies would require significant bandwidth to maintain per-line DRAM cache\nstate. As such, DRAM cache replacement policies have primarily been stateless\npolicies, such as always-install or probabilistic bypass. Unfortunately, we\nfind that stateless policies are often too coarse-grain and become ineffective\nat the size and associativity of DRAM caches. Ideally, we want a replacement\npolicy that can obtain the hit-rate benefits of stateful replacement policies,\nbut keep the bandwidth-efficiency of stateless policies.\n  In our study, we find that tracking per-line reuse state can enable an\neffective replacement policy that can mitigate common thrashing patterns seen\nin gigascale caches. We propose a stateful replacement/bypass policy called\nRRIP Age-On-Bypass (RRIP-AOB), that tracks reuse state for high-reuse lines,\nprotects such lines by bypassing other lines, and Ages the state On cache\nBypass. Unfortunately, such a stateful technique requires significant bandwidth\nto update state. To this end, we propose Efficient Tracking of Reuse (ETR). ETR\nmakes state tracking efficient by accurately tracking the state of only one\nline from a region, and using the state of that line to guide the replacement\ndecisions for other lines in that region. ETR reduces the bandwidth for\ntracking replacement state by 70%, and makes stateful policies practical for\nDRAM caches. Our evaluations with a 2GB DRAM cache, show that our RRIP-AOB and\nETR techniques provide 18% speedup while needing less than 1KB of SRAM.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 00:28:51 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Young", "Vinson", ""], ["Qureshi", "Moinuddin K.", ""]]}, {"id": "1907.02184", "submitter": "Vinson Young", "authors": "Vinson Young, Zeshan Chishti, Moinuddin K. Qureshi", "title": "TicToc: Enabling Bandwidth-Efficient DRAM Caching for both Hits and\n  Misses in Hybrid Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates bandwidth-efficient DRAM caching for hybrid DRAM +\n3D-XPoint memories. 3D-XPoint is becoming a viable alternative to DRAM as it\nenables high-capacity and non-volatile main memory systems; however, 3D-XPoint\nhas 4-8x slower read, and worse writes. As such, effective DRAM caching in\nfront of 3D-XPoint is important to enable a high-capacity, low-latency, and\nhigh-write-bandwidth memory. There are two major approaches for DRAM cache\ndesign: (1) a Tag-Inside-Cacheline (TIC) organization that optimizes for hits,\nby storing tag next to each line such that one access gets both tag and data,\nand (2) a Tag-Outside-Cacheline (TOC) organization that optimizes for misses,\nby storing tags from multiple data-lines together such that one tag-access gets\ninfo for several data-lines. Ideally, we desire the low hit-latency of TIC, and\nthe low miss-bandwidth of TOC. To this end, we propose TicToc, an organization\nthat provisions both TIC and TOC to get hit and miss benefits of both.\n  However, we find that naively combining both actually performs worse than\nTIC, because one needs to pay bandwidth to maintain both metadata. The main\ncontribution of this work is developing architectural techniques to reduce the\nbandwidth of maintaining both TIC and TOC metadata. We find the majority of the\nbandwidth cost is due to maintaining TOC dirty bits. We propose DRAM Cache\nDirtiness Bit, which carries DRAM cache dirty info to last-level caches, to\nprune repeated dirty-bit checks for known dirty lines. We then propose\nPreemptive Dirty Marking, which predicts which lines will be written and\nproactively marks dirty bit at install time, to amortize the initial dirty-bit\nupdate. Our evaluations on a 4GB DRAM cache with 3D-XPoint memory show that\nTicToc enables 10% speedup over baseline TIC, nearing 14% speedup possible with\nan idealized DRAM cache w/ 64MB of SRAM tags, while needing only 34KB SRAM.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 01:39:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Young", "Vinson", ""], ["Chishti", "Zeshan", ""], ["Qureshi", "Moinuddin K.", ""]]}, {"id": "1907.02217", "submitter": "Shi Shi", "authors": "Shi Shi", "title": "FusionAccel: A General Re-configurable Deep Learning Inference\n  Accelerator on FPGA for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep learning accelerator is one of the methods to accelerate deep\nlearning network computations, which is mainly based on convolutional neural\nnetwork acceleration. To address the fact that concurrent convolutional neural\nnetwork accelerators are not solely open-source and the exclusiveness of\nplatforms, FusionAccel, a scalable convolutional neural network accelerator\nhardware architecture with supporting software is proposed. It can adapt to\ndifferent network structures and can be reconstructed before compilation and\nreconfigured at runtime. This paper realizes this RTL convolutional neural\nnetwork accelerator design and functional verifications on a Xilinx Spartan-6\nFPGA. The result is identical to that of Caffe-CPU. Since the entire project is\nbased on RTL, it can be migrated to ASIC after replacing some FPGA-specific\nIPs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 04:48:04 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Shi", "Shi", ""]]}, {"id": "1907.04504", "submitter": "Hossein Pourmeidani", "authors": "Hossein Pourmeidani, Mehdi Habibi", "title": "A Range Matching CAM for Hierarchical Defect Tolerance Technique in NRAM\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the small size of nanoscale devices, they are highly prone to process\ndisturbances which results in manufacturing defects. Some of the defects are\nrandomly distributed throughout the nanodevice layer. Other disturbances tend\nto be local and lead to cluster defects caused by factors such as layer\nmisintegration and line width variations. In this paper, we propose a method\nfor identifying cluster defects from random ones. The motivation is to repair\nthe cluster defects using rectangular ranges in a range matching\ncontent-addressable memory (RM-CAM) and random defects using triple-modular\nredundancy (TMR). It is believed a combination of these two approaches is more\neffective for repairing defects at high error rate with less resource. With the\nproposed fault repairing technique, defect recovery results are examined for\ndifferent fault distribution scenarios. Also the mapping circuit structure\nrequired for two conceptual 32*32 and 64*64 bit RAMs are presented and their\nspeed, power and transistor count are reported.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 04:38:06 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Pourmeidani", "Hossein", ""], ["Habibi", "Mehdi", ""]]}, {"id": "1907.05068", "submitter": "Ming Ling", "authors": "Ming Ling, Jiancong Ge, Guangmin Wang", "title": "Fast Modeling L2 Cache Reuse Distance Histograms Using Combined Locality\n  Information from Software Traces", "comments": "This manuscript has been major revised and re-submitted to Journal of\n  Systems Architecture", "journal-ref": "Volume 108 , 2020, Journal of Systems Architecture", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the performance gap between CPU and the main memory, multi-level\ncache architectures are widely used in modern processors. Therefore, modeling\nthe behaviors of the downstream caches becomes a critical part of the processor\nperformance evaluation in the early stage of Design Space Exploration (DSE). In\nthis paper, we propose a fast and accurate L2 cache reuse distance histogram\nmodel, which can be used to predict the behaviors of the multi-level cache\narchitectures where the L1 cache uses the LRU replacement policy and the L2\ncache uses LRU/Random replacement policies. We use the profiled L1 reuse\ndistance histogram and two newly proposed metrics, namely the RST table and the\nHit-RDH, that describing more detailed information of the software traces as\nthe inputs. For a given L1 cache configuration, the profiling results can be\nreused for different configurations of the L2 cache. The output of our model is\nthe L2 cache reuse distance histogram, based on which the L2 cache miss rates\ncan be evaluated. We compare the L2 cache miss rates with the results from gem5\ncycle-accurate simulations of 15 benchmarks chosen from SPEC CPU 2006 and 9\nbenchmarks from SPEC CPU 2017. The average absolute error is less than 5%,\nwhile the evaluation time for each L2 configuration can be sped up almost 30X\nfor four L2 cache candidates.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:20:34 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:24:16 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 10:23:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ling", "Ming", ""], ["Ge", "Jiancong", ""], ["Wang", "Guangmin", ""]]}, {"id": "1907.05700", "submitter": "Zichang He", "authors": "Zichang He, Weilong Cui, Chunfeng Cui, Timothy Sherwood, Zheng Zhang", "title": "Efficient Uncertainty Modeling for System Design via Mixed Integer\n  Programming", "comments": "International Conf. Computer Aided Design (ICCAD), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The post-Moore era casts a shadow of uncertainty on many aspects of computer\nsystem design. Managing that uncertainty requires new algorithmic tools to make\nquantitative assessments. While prior uncertainty quantification methods, such\nas generalized polynomial chaos (gPC), show how to work precisely under the\nuncertainty inherent to physical devices, these approaches focus solely on\nvariables from a continuous domain. However, as one moves up the system stack\nto the architecture level many parameters are constrained to a discrete\n(integer) domain. This paper proposes an efficient and accurate uncertainty\nmodeling technique, named mixed generalized polynomial chaos (M-gPC), for\narchitectural uncertainty analysis. The M-gPC technique extends the generalized\npolynomial chaos (gPC) theory originally developed in the uncertainty\nquantification community, such that it can efficiently handle the mixed-type\n(i.e., both continuous and discrete) uncertainties in computer architecture\ndesign. Specifically, we employ some stochastic basis functions to capture the\narchitecture-level impact caused by uncertain parameters in a simulator. We\nalso develop a novel mixed-integer programming method to select a small number\nof uncertain parameter samples for detailed simulations. With a few highly\ninformative simulation samples, an accurate surrogate model is constructed in\nplace of cycle-level simulators for various architectural uncertainty analysis.\nIn the chip-multiprocessor (CMP) model, we are able to estimate the propagated\nuncertainties with only 95 samples whereas Monte Carlo requires 5*10^4 samples\nto achieve the similar accuracy. We also demonstrate the efficiency and\neffectiveness of our method on a detailed DRAM subsystem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:50:01 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 02:56:35 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["He", "Zichang", ""], ["Cui", "Weilong", ""], ["Cui", "Chunfeng", ""], ["Sherwood", "Timothy", ""], ["Zhang", "Zheng", ""]]}, {"id": "1907.06948", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Coprocessors: failures and successes", "comments": "8 pages, 8 figures, Conf\\'erence d'Informatique en Parall\\'elisme,\n  Architecture et Syst\\`eme (COMPAS'2019), June 25-28, Anglet, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appearance and disappearance of coprocessors by integration into the CPU,\nthe success or failure of coprocessors are examined by summarizing their\ncharacteristics from the mainframes of the 1960s. The coprocessors most\nparticularly reviewed are the IBM 360 and CDC-6600 I/O processors, the Intel\n8087 math coprocessor, the Cell processor, the Intel Xeon Phi coprocessors, the\nGPUs, the FPGAs, and the coprocessors of manycores SW26010 and Pezy SC-2 used\nin high-ranked supercomputers in the TOP500 or Green500. The conditions for a\ncoprocessor to be viable in the medium or long-term are defined.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 11:48:15 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "1907.07776", "submitter": "Eduardo Olmedo Sanchez", "authors": "Eduardo Olmedo Sanchez, Xian-He Sun", "title": "CADS: Core-Aware Dynamic Scheduler for Multicore Memory Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory controller scheduling is crucial in multicore processors, where DRAM\nbandwidth is shared. Since increased number of requests from multiple cores of\nprocessors becomes a source of bottleneck, scheduling the requests efficiently\nis necessary to utilize all the computing power these processors offer.\nHowever, current multicore processors are using traditional memory controllers,\nwhich are designed for single-core processors. They are unable to adapt to\nchanging characteristics of memory workloads that run simultaneously on\nmultiple cores. Existing schedulers may disrupt locality and bank parallelism\namong data requests coming from different cores. Hence, novel memory\ncontrollers that consider and adapt to the memory access characteristics, and\nshare memory resources efficiently and fairly are necessary. We introduce\nCore-Aware Dynamic Scheduler (CADS) for multicore memory controller. CADS uses\nReinforcement Learning (RL) to alter its scheduling strategy dynamically at\nruntime. Our scheduler utilizes locality among data requests from multiple\ncores and exploits parallelism in accessing multiple banks of DRAM. CADS is\nalso able to share the DRAM while guaranteeing fairness to all cores accessing\nmemory. Using CADS policy, we achieve 20% better cycles per instruction (CPI)\nin running memory intensive and compute intensive PARSEC parallel benchmarks\nsimultaneously, and 16% better CPI with SPEC 2006 benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 21:14:24 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sanchez", "Eduardo Olmedo", ""], ["Sun", "Xian-He", ""]]}, {"id": "1907.08641", "submitter": "Oscar Casta\\~neda", "authors": "Oscar Casta\\~neda, Maria Bobbett, Alexandra Gallyas-Sanhueza,\n  Christoph Studer", "title": "PPAC: A Versatile In-Memory Accelerator for Matrix-Vector-Product-Like\n  Operations", "comments": "Presented at the IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing in memory (PIM) moves computation into memories with the goal of\nimproving throughput and energy-efficiency compared to traditional von\nNeumann-based architectures. Most existing PIM architectures are either\ngeneral-purpose but only support atomistic operations, or are specialized to\naccelerate a single task. We propose the Parallel Processor in Associative\nContent-addressable memory (PPAC), a novel in-memory accelerator that supports\na range of matrix-vector-product (MVP)-like operations that find use in\ntraditional and emerging applications. PPAC is, for example, able to accelerate\nlow-precision neural networks, exact/approximate hash lookups, cryptography,\nand forward error correction. The fully-digital nature of PPAC enables its\nimplementation with standard-cell-based CMOS, which facilitates automated\ndesign and portability among technology nodes. To demonstrate the efficacy of\nPPAC, we provide post-layout implementation results in 28nm CMOS for different\narray sizes. A comparison with recent digital and mixed-signal PIM accelerators\nreveals that PPAC is competitive in terms of throughput and energy-efficiency,\nwhile accelerating a wide range of applications and simplifying development.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 18:25:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Casta\u00f1eda", "Oscar", ""], ["Bobbett", "Maria", ""], ["Gallyas-Sanhueza", "Alexandra", ""], ["Studer", "Christoph", ""]]}, {"id": "1907.09078", "submitter": "KyoungRok Cho", "authors": "Seungbum Baek", "title": "Reconfigurable multiplier architecture based on memristor-cmos with\n  higher flexibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is an indispensable operation in most of digital signal\nprocessing systems. Recently, many systems need to execute different types of\nalgorithms on a multiplier. Therefore, it needs complicated computation and\nlarge area occupation. In this regard a fixed multiplier is inefficient and the\ndevelopment of a reconfigurable multiplier becomes increasingly important. The\nadvent of memristor-CMOS hybrid circuits provides an opportunity for reducing\narea occupation. This paper introduces memristor-CMOS based reconfigurable\nmultiplier which provides flexible multiplication according to various\nbit-width. Performance of the proposed multiplier is estimated with some\napplications and comparison with conventional multipliers, using memristor\nSPICE model and proprietary 180-nm CMOS process.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 01:52:36 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Baek", "Seungbum", ""]]}, {"id": "1907.10826", "submitter": "P Balasubramanian", "authors": "P Balasubramanian", "title": "Performance Comparison of Quasi-Delay-Insensitive Asynchronous Adders", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.09433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical note, we provide a comparison of the design metrics of\nvarious quasi-delay-insensitive (QDI) asynchronous adders, where the adders\ncorrespond to diverse architectures. QDI adders are robust, and the objective\nof this technical note is to point to those QDI adders which are suitable for\nlow power/energy and less area. This information could be valuable for a\nresource-constrained low power VLSI design scenario. Non-QDI adders are\nexcluded from the comparison since they are not robust although they may have\noptimized design metrics. All the QDI adders were realized using a 32/28nm CMOS\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:31:54 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Balasubramanian", "P", ""]]}, {"id": "1907.12014", "submitter": "Takahiro Hirofuchi", "authors": "Takahiro Hirofuchi and Ryousei Takano", "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization\n  Mechanism for Intel Optane DC Persistent Memory Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies, being accessible in the same manner\nas DRAM, are considered indispensable for expanding main memory capacities.\nIntel Optane DCPMM is a long-awaited product that drastically increases main\nmemory capacities. However, a substantial performance gap exists between DRAM\nand DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and\n407% higher than those of DRAM, respectively. The read/write bandwidths were\n37% and 8% of those of DRAM. This performance gap in main memory presents a new\nchallenge to researchers; we need a new system software technology supporting\nemerging hybrid memory architecture. In this paper, we present RAMinate, a\nhypervisor-based virtualization mechanism for hybrid memory systems, and a key\ntechnology to address the performance gap in main memory systems. It provides\ngreat flexibility in memory management and maximizes the performance of virtual\nmachines (VMs) by dynamically optimizing memory mappings. Through experiments,\nwe confirmed that even though a VM has only 1% of DRAM in its RAM, the\nperformance degradation of the VM was drastically alleviated by memory mapping\noptimization. The elapsed time to finish the build of Linux Kernel in the VM\nwas 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495\nseconds). When the optimization mechanism was disabled, the elapsed time\nincreased to 624 seconds (i.e. 26% increase from the 100% DRAM case).\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 05:06:24 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hirofuchi", "Takahiro", ""], ["Takano", "Ryousei", ""]]}, {"id": "1907.12325", "submitter": "Maksim Jenihhin", "authors": "Adeboye Stephen Oyeniran, Raimund Ubar, Maksim Jenihhin, Cemil Cem\n  Gursoy, Jaan Raik", "title": "Mixed-level identification of fault redundancy in microprocessors", "comments": "2019 IEEE Latin American Test Symposium (LATS)", "journal-ref": "2019 IEEE Latin American Test Symposium (LATS), Santiago, Chile,\n  2019, pp. 1-6", "doi": "10.1109/LATW.2019.8704591", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new high-level implementation independent functional fault model for\ncontrol faults in microprocessors is introduced. The fault model is based on\nthe instruction set, and is specified as a set of data constraints to be\nsatisfied by test data generation. We show that the high-level test, which\nsatisfies these data constraints, will be sufficient to guarantee the detection\nof all non-redundant low level faults. The paper proposes a simple and fast\nsimulation based method of generating test data, which satisfy the constraints\nprescribed by the proposed fault model, and a method of evaluating the\nhigh-level control fault coverage for the proposed fault model and for the\ngiven test. A method is presented for identification of the high-level\nredundant faults, and it is shown that a test, which provides 100% coverage of\nnon-redundant high-level faults, will also guarantee 100% non-redundant SAF\ncoverage, whereas all gate-level SAF not covered by the test are identified as\nredundant. Experimental results of test generation for the execution part of a\nmicroprocessor support the results presented in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:47:27 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Oyeniran", "Adeboye Stephen", ""], ["Ubar", "Raimund", ""], ["Jenihhin", "Maksim", ""], ["Gursoy", "Cemil Cem", ""], ["Raik", "Jaan", ""]]}, {"id": "1907.12901", "submitter": "Hao Zheng", "authors": "Yuting Cao, Hao Zheng, Sandip Ray", "title": "A Communication-Centric Observability Selection for Post-Silicon\n  System-on-Chip Integration Debug", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of how components communicate with each other during system\nexecution is crucial for debugging system-on-chip designs. However, limited\nobservability is the major obstacle to the efficient and accurate\nreconstruction in the post-silicon validation stage. This paper addresses that\nproblem by proposing several communication event selection methods guided by\nsystem-level communication protocols. Such methods are optimized for on-chip\ncommunication event tracing infrastructure to enhance observability. The\neffectiveness of these methods are demonstrated with experiments on a\nnon-trivial multicore SoC prototype. The results show that with the proposed\nmethod, more comprehensive information on system internal execution can be\ninferred from traces under limited observability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:23:44 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Cao", "Yuting", ""], ["Zheng", "Hao", ""], ["Ray", "Sandip", ""]]}, {"id": "1907.12947", "submitter": "Saugata Ghose", "authors": "Saugata Ghose, Amirali Boroumand, Jeremie S. Kim, Juan G\\'omez-Luna,\n  Onur Mutlu", "title": "A Workload and Programming Ease Driven Perspective of\n  Processing-in-Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern and emerging applications must process increasingly large volumes\nof data. Unfortunately, prevalent computing paradigms are not designed to\nefficiently handle such large-scale data: the energy and performance costs to\nmove this data between the memory subsystem and the CPU now dominate the total\ncosts of computation. This forces system architects and designers to\nfundamentally rethink how to design computers. Processing-in-memory (PIM) is a\ncomputing paradigm that avoids most data movement costs by bringing computation\nto the data. New opportunities in modern memory systems are enabling\narchitectures that can perform varying degrees of processing inside the memory\nsubsystem. However, there are many practical system-level issues that must be\ntackled to construct PIM architectures, including enabling workloads and\nprogrammers to easily take advantage of PIM. This article examines three key\ndomains of work towards the practical construction and widespread adoption of\nPIM architectures. First, we describe our work on systematically identifying\nopportunities for PIM in real applications, and quantify potential gains for\npopular emerging applications (e.g., machine learning, data analytics, genome\nanalysis). Second, we aim to solve several key issues on programming these\napplications for PIM architectures. Third, we describe challenges that remain\nfor the widespread adoption of PIM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 20:58:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ghose", "Saugata", ""], ["Boroumand", "Amirali", ""], ["Kim", "Jeremie S.", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Mutlu", "Onur", ""]]}, {"id": "1907.12952", "submitter": "Hosein Mohammadi Makrani", "authors": "Hosein Mohammadi Makrani, Farnoud Farahmand, Hossein Sayadi, Sara\n  Bondi, Sai Manoj Pudukotai Dinakarrao, Liang Zhao, Avesta Sasan, Houman\n  Homayoun, Setareh Rafatirad", "title": "Pyramid: Machine Learning Framework to Estimate the Optimal Timing and\n  Resource Usage of a High-Level Synthesis Design", "comments": "This paper has been accepted in The International Conference on\n  Field-Programmable Logic and Applications 2019 (FPL'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of High-Level Synthesis (HLS) tools shifted the paradigm of\nhardware design by making the process of mapping high-level programming\nlanguages to hardware design such as C to VHDL/Verilog feasible. HLS tools\noffer a plethora of techniques to optimize designs for both area and\nperformance, but resource usage and timing reports of HLS tools mostly deviate\nfrom the post-implementation results. In addition, to evaluate a hardware\ndesign performance, it is critical to determine the maximum achievable clock\nfrequency. Obtaining such information using static timing analysis provided by\nCAD tools is difficult, due to the multitude of tool options. Moreover, a\nbinary search to find the maximum frequency is tedious, time-consuming, and\noften does not obtain the optimal result. To address these challenges, we\npropose a framework, called Pyramid, that uses machine learning to accurately\nestimate the optimal performance and resource utilization of an HLS design. For\nthis purpose, we first create a database of C-to-FPGA results from a diverse\nset of benchmarks. To find the achievable maximum clock frequency, we use\nMinerva, which is an automated hardware optimization tool. Minerva determines\nthe close-to-optimal settings of tools, using static timing analysis and a\nheuristic algorithm, and targets either optimal throughput or\nthroughput-to-area. Pyramid uses the database to train an ensemble machine\nlearning model to map the HLS-reported features to the results of Minerva. To\nthis end, Pyramid re-calibrates the results of HLS to bridge the accuracy gap\nand enable developers to estimate the throughput or throughput-to-area of\nhardware design with more than 95% accuracy and alleviates the need to perform\nactual implementation for estimation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 01:34:20 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Makrani", "Hosein Mohammadi", ""], ["Farahmand", "Farnoud", ""], ["Sayadi", "Hossein", ""], ["Bondi", "Sara", ""], ["Dinakarrao", "Sai Manoj Pudukotai", ""], ["Zhao", "Liang", ""], ["Sasan", "Avesta", ""], ["Homayoun", "Houman", ""], ["Rafatirad", "Setareh", ""]]}]