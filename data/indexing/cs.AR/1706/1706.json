[{"id": "1706.02344", "submitter": "Vincent T. Lee", "authors": "Vincent T. Lee, Armin Alaghi, John P. Hayes, Visvesh Sathe, Luis Ceze", "title": "Energy-Efficient Hybrid Stochastic-Binary Neural Networks for\n  Near-Sensor Computing", "comments": "6 pages, 3 figures, Design, Automata and Test in Europe (DATE) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks (NNs) exhibit unprecedented success at\ntransforming large, unstructured data streams into compact higher-level\nsemantic information for tasks such as handwriting recognition, image\nclassification, and speech recognition. Ideally, systems would employ\nnear-sensor computation to execute these tasks at sensor endpoints to maximize\ndata reduction and minimize data movement. However, near- sensor computing\npresents its own set of challenges such as operating power constraints, energy\nbudgets, and communication bandwidth capacities. In this paper, we propose a\nstochastic- binary hybrid design which splits the computation between the\nstochastic and binary domains for near-sensor NN applications. In addition, our\ndesign uses a new stochastic adder and multiplier that are significantly more\naccurate than existing adders and multipliers. We also show that retraining the\nbinary portion of the NN computation can compensate for precision losses\nintroduced by shorter stochastic bit-streams, allowing faster run times at\nminimal accuracy losses. Our evaluation shows that our hybrid stochastic-binary\ndesign can achieve 9.8x energy efficiency savings, and application-level\naccuracies within 0.05% compared to conventional all-binary designs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:07:38 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Lee", "Vincent T.", ""], ["Alaghi", "Armin", ""], ["Hayes", "John P.", ""], ["Sathe", "Visvesh", ""], ["Ceze", "Luis", ""]]}, {"id": "1706.02725", "submitter": "Ramyad Hadidi", "authors": "Ramyad Hadidi, Bahar Asgari, Burhan Ahmad Mudassar, Saibal\n  Mukhopadhyay, Sudhakar Yalamanchili, and Hyesoon Kim", "title": "Demystifying the Characteristics of 3D-Stacked Memories: A Case Study\n  for Hybrid Memory Cube", "comments": "EEE Catalog Number: CFP17236-USB ISBN 13: 978-1-5386-1232-3", "journal-ref": "Proceedings of the 2017 IEEE International Symposium on Workload\n  Characterization", "doi": "10.1109/IISWC.2017.8167757", "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D)-stacking technology, which enables the integration of\nDRAM and logic dies, offers high bandwidth and low energy consumption. This\ntechnology also empowers new memory designs for executing tasks not\ntraditionally associated with memories. A practical 3D-stacked memory is Hybrid\nMemory Cube (HMC), which provides significant access bandwidth and low power\nconsumption in a small area. Although several studies have taken advantage of\nthe novel architecture of HMC, its characteristics in terms of latency and\nbandwidth or their correlation with temperature and power consumption have not\nbeen fully explored. This paper is the first, to the best of our knowledge, to\ncharacterize the thermal behavior of HMC in a real environment using the AC-510\naccelerator and to identify temperature as a new limitation for this\nstate-of-the-art design space. Moreover, besides bandwidth studies, we\ndeconstruct factors that contribute to latency and reveal their sources for\nhigh- and low-load accesses. The results of this paper demonstrates essential\nbehaviors and performance bottlenecks for future explorations of\npacket-switched and 3D-stacked memories.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 18:16:11 GMT"}, {"version": "v2", "created": "Sun, 24 Sep 2017 22:42:01 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 16:39:16 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hadidi", "Ramyad", ""], ["Asgari", "Bahar", ""], ["Mudassar", "Burhan Ahmad", ""], ["Mukhopadhyay", "Saibal", ""], ["Yalamanchili", "Sudhakar", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1706.03162", "submitter": "Amirali Boroumand", "authors": "Amirali Boroumand, Saugata Ghose, Minesh Patel, Hasan Hassan, Brandon\n  Lucia, Nastaran Hajinazar, Kevin Hsieh, Krishna T. Malladi, Hongzhong Zheng,\n  Onur Mutlu", "title": "LazyPIM: Efficient Support for Cache Coherence in Processing-in-Memory\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing-in-memory (PIM) architectures have seen an increase in popularity\nrecently, as the high internal bandwidth available within 3D-stacked memory\nprovides greater incentive to move some computation into the logic layer of the\nmemory. To maintain program correctness, the portions of a program that are\nexecuted in memory must remain coherent with the portions of the program that\ncontinue to execute within the processor. Unfortunately, PIM architectures\ncannot use traditional approaches to cache coherence due to the high off-chip\ntraffic consumed by coherence messages, which, as we illustrate in this work,\ncan undo the benefits of PIM execution for many data-intensive applications. We\npropose LazyPIM, a new hardware cache coherence mechanism designed specifically\nfor PIM. Prior approaches for coherence in PIM are ill-suited to applications\nthat share a large amount of data between the processor and the PIM logic.\nLazyPIM uses a combination of speculative cache coherence and compressed\ncoherence signatures to greatly reduce the overhead of keeping PIM coherent\nwith the processor, even when a large amount of sharing exists.We find that\nLazyPIM improves average performance across a range of data-intensive PIM\napplications by 19.6%, reduces off-chip traffic by 30.9%, and reduces energy\nconsumption by 18.0%, over the best prior approaches to PIM coherence.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:52:10 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Boroumand", "Amirali", ""], ["Ghose", "Saugata", ""], ["Patel", "Minesh", ""], ["Hassan", "Hasan", ""], ["Lucia", "Brandon", ""], ["Hajinazar", "Nastaran", ""], ["Hsieh", "Kevin", ""], ["Malladi", "Krishna T.", ""], ["Zheng", "Hongzhong", ""], ["Mutlu", "Onur", ""]]}, {"id": "1706.03251", "submitter": "Eric Olsen", "authors": "Eric B. Olsen", "title": "Proposal for a High Precision Tensor Processing Unit", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This whitepaper proposes the design and adoption of a new generation of\nTensor Processing Unit which has the performance of Google's TPU, yet performs\noperations on wide precision data. The new generation TPU is made possible by\nimplementing arithmetic circuits which compute using a new general purpose,\nfractional arithmetic based on the residue number system.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 16:40:04 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Olsen", "Eric B.", ""]]}, {"id": "1706.03864", "submitter": "Amrita Mazumdar", "authors": "Amrita Mazumdar, Thierry Moreau, Sung Kim, Meghan Cowan, Armin Alaghi,\n  Luis Ceze, Mark Oskin and Visvesh Sathe", "title": "Exploring Computation-Communication Tradeoffs in Camera Systems", "comments": null, "journal-ref": "2017 IEEE International Symposium on Workload Characterization\n  (IISWC)", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras are the defacto sensor. The growing demand for real-time and\nlow-power computer vision, coupled with trends towards high-efficiency\nheterogeneous systems, has given rise to a wide range of image processing\nacceleration techniques at the camera node and in the cloud. In this paper, we\ncharacterize two novel camera systems that use acceleration techniques to push\nthe extremes of energy and performance scaling, and explore the\ncomputation-communication tradeoffs in their design. The first case study\ntargets a camera system designed to detect and authenticate individual faces,\nrunning solely on energy harvested from RFID readers. We design a\nmulti-accelerator SoC design operating in the sub-mW range, and evaluate it\nwith real-world workloads to show performance and energy efficiency\nimprovements over a general purpose microprocessor. The second camera system\nsupports a 16-camera rig processing over 32 Gb/s of data to produce real-time\n3D-360 degree virtual reality video. We design a multi-FPGA processing pipeline\nthat outperforms CPU and GPU configurations by up to 10x in computation time,\nproducing panoramic stereo video directly from the camera rig at 30 frames per\nsecond. We find that an early data reduction step, either before complex\nprocessing or offloading, is the most critical optimization for in-camera\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 22:11:55 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 21:32:05 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mazumdar", "Amrita", ""], ["Moreau", "Thierry", ""], ["Kim", "Sung", ""], ["Cowan", "Meghan", ""], ["Alaghi", "Armin", ""], ["Ceze", "Luis", ""], ["Oskin", "Mark", ""], ["Sathe", "Visvesh", ""]]}, {"id": "1706.04487", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, K Prasad", "title": "Latency Optimized Asynchronous Early Output Ripple Carry Adder based on\n  Delay-Insensitive Dual-Rail Data Encoding", "comments": "arXiv admin note: text overlap with arXiv:1704.07619", "journal-ref": "International Journal of Circuits, Systems and Signal Processing,\n  vol. 11, pp. 65-74 (2017)", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous circuits employing delay-insensitive codes for data\nrepresentation i.e. encoding and following a 4-phase return-to-zero protocol\nfor handshaking are generally robust. Depending upon whether a single\ndelay-insensitive code or multiple delay-insensitive code(s) are used for data\nencoding, the encoding scheme is called homogeneous or heterogeneous\ndelay-insensitive data encoding. This article proposes a new latency optimized\nearly output asynchronous ripple carry adder (RCA) that utilizes single-bit\nasynchronous full adders (SAFAs) and dual-bit asynchronous full adders (DAFAs)\nwhich incorporate redundant logic and are based on the delay-insensitive\ndual-rail code i.e. homogeneous data encoding, and follow a 4-phase\nreturn-to-zero handshaking. Amongst various RCA, carry lookahead adder (CLA),\nand carry select adder (CSLA) designs, which are based on homogeneous or\nheterogeneous delay-insensitive data encodings which correspond to the\nweak-indication or the early output timing model, the proposed early output\nasynchronous RCA that incorporates SAFAs and DAFAs with redundant logic is\nfound to result in reduced latency for a dual-operand addition operation. In\nparticular, for a 32-bit asynchronous RCA, utilizing 15 stages of DAFAs and 2\nstages of SAFAs leads to reduced latency. The theoretical worst-case latencies\nof the different asynchronous adders were calculated by taking into account the\ntypical gate delays of a 32/28nm CMOS digital cell library, and a comparison is\nmade with their practical worst-case latencies estimated. The theoretical and\npractical worst-case latencies show a close correlation....\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 17:47:23 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Balasubramanian", "P", ""], ["Prasad", "K", ""]]}, {"id": "1706.07568", "submitter": "Mohamed Hassan Dr.", "authors": "Nivedita Sritharan, Anirudh M. Kaushik, Mohamed Hassan, and Hiren\n  Patel", "title": "HourGlass: Predictable Time-based Cache Coherence Protocol for\n  Dual-Critical Multi-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hardware mechanism called HourGlass to predictably share data in\na multi-core system where cores are explicitly designated as critical or\nnon-critical. HourGlass is a time-based cache coherence protocol for\ndual-critical multi-core systems that ensures worst-case latency (WCL) bounds\nfor memory requests originating from critical cores. Although HourGlass does\nnot provide either WCL or bandwidth guarantees for memory requests from\nnon-critical cores, it promotes the use of timers to improve its bandwidth\nutilization while still maintaining WCL bounds for critical cores. This\nencourages a trade-off between the WCL bounds for critical cores, and the\nimproved memory bandwidth for non-critical cores via timer configurations. We\nevaluate HourGlass using gem5, and with multithreaded benchmark suites\nincluding SPLASH-2, and synthetic workloads. Our results show that the WCL for\ncritical cores with HourGlass is always within the analytical WCL bounds, and\nprovides a tighter WCL bound on critical cores compared to the state-of-the-art\nreal-time cache coherence protocol. Further, we show that HourGlass enables a\ntrade-off between provable WCL bounds for critical cores, and improved\nbandwidth utilization for non-critical cores. The average-case performance of\nHourGlass is comparable to the state-of-the-art real-time cache coherence\nprotocol, and suffers a slowdown of 1.43x and 1.46x compared to the\nconventional MSI and MESI protocols.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 05:36:19 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 20:58:37 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 21:10:32 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Sritharan", "Nivedita", ""], ["Kaushik", "Anirudh M.", ""], ["Hassan", "Mohamed", ""], ["Patel", "Hiren", ""]]}, {"id": "1706.07853", "submitter": "Sayeh Sharify", "authors": "Sayeh Sharify, Alberto Delmas Lascorz, Kevin Siu, Patrick Judd,\n  Andreas Moshovos", "title": "Loom: Exploiting Weight and Activation Precisions to Accelerate\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loom (LM), a hardware inference accelerator for Convolutional Neural Networks\n(CNNs) is presented. In LM every bit of data precision that can be saved\ntranslates to proportional performance gains. Specifically, for convolutional\nlayers LM's execution time scales inversely proportionally with the precisions\nof both weights and activations. For fully-connected layers LM's performance\nscales inversely proportionally with the precision of the weights. LM targets\narea- and bandwidth-constrained System-on-a-Chip designs such as those found on\nmobile devices that cannot afford the multi-megabyte buffers that would be\nneeded to store each layer on-chip. Accordingly, given a data bandwidth budget,\nLM boosts energy efficiency and performance over an equivalent bit-parallel\naccelerator. For both weights and activations LM can exploit profile-derived\nperlayer precisions. However, at runtime LM further trims activation precisions\nat a much smaller than a layer granularity. Moreover, it can naturally exploit\nweight precision variability at a smaller granularity than a layer. On average,\nacross several image classification CNNs and for a configuration that can\nperform the equivalent of 128 16b x 16b multiply-accumulate operations per\ncycle LM outperforms a state-of-the-art bit-parallel accelerator [1] by 4.38x\nwithout any loss in accuracy while being 3.54x more energy efficient. LM can\ntrade-off accuracy for additional improvements in execution performance and\nenergy efficiency and compares favorably to an accelerator that targeted only\nactivation precisions. We also study 2- and 4-bit LM variants and find the the\n2-bit per cycle variant is the most energy efficient.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 20:35:42 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 19:31:40 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Sharify", "Sayeh", ""], ["Lascorz", "Alberto Delmas", ""], ["Siu", "Kevin", ""], ["Judd", "Patrick", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1706.08642", "submitter": "Saugata Ghose", "authors": "Yu Cai, Saugata Ghose, Erich F. Haratsch, Yixin Luo, Onur Mutlu", "title": "Error Characterization, Mitigation, and Recovery in Flash Memory Based\n  Solid-State Drives", "comments": null, "journal-ref": "Proceedings of the IEEE 105 (2017) 1666 - 1704", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NAND flash memory is ubiquitous in everyday life today because its capacity\nhas continuously increased and cost has continuously decreased over decades.\nThis positive growth is a result of two key trends: (1) effective process\ntechnology scaling, and (2) multi-level (e.g., MLC, TLC) cell data coding.\nUnfortunately, the reliability of raw data stored in flash memory has also\ncontinued to become more difficult to ensure, because these two trends lead to\n(1) fewer electrons in the flash memory cell (floating gate) to represent the\ndata and (2) larger cell-to-cell interference and disturbance effects. Without\nmitigation, worsening reliability can reduce the lifetime of NAND flash memory.\nAs a result, flash memory controllers in solid-state drives (SSDs) have become\nmuch more sophisticated: they incorporate many effective techniques to ensure\nthe correct interpretation of noisy data stored in flash memory cells.\n  In this article, we review recent advances in SSD error characterization,\nmitigation, and data recovery techniques for reliability and lifetime\nimprovement. We provide rigorous experimental data from state-of-the-art MLC\nand TLC NAND flash devices on various types of flash memory errors, to motivate\nthe need for such techniques. Based on the understanding developed by the\nexperimental characterization, we describe several mitigation and recovery\ntechniques, including (1) cell-to-cell interference mitigation, (2) optimal\nmulti-level cell sensing, (3) error correction using state-of-the-art\nalgorithms and methods, and (4) data recovery when error correction fails. We\nquantify the reliability improvement provided by each of these techniques.\nLooking forward, we briefly discuss how flash memory and these techniques could\nevolve into the future.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 01:54:06 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 17:07:39 GMT"}, {"version": "v3", "created": "Fri, 22 Sep 2017 12:49:41 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Cai", "Yu", ""], ["Ghose", "Saugata", ""], ["Haratsch", "Erich F.", ""], ["Luo", "Yixin", ""], ["Mutlu", "Onur", ""]]}, {"id": "1706.08870", "submitter": "Yixin Luo", "authors": "Yixin Luo, Saugata Ghose, Tianshi Li, Sriram Govindan, Bikash Sharma,\n  Bryan Kelly, Amirali Boroumand, Onur Mutlu", "title": "Using ECC DRAM to Adaptively Increase Memory Capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern DRAM modules are often equipped with hardware error correction\ncapabilities, especially for DRAM deployed in large-scale data centers, as\nprocess technology scaling has increased the susceptibility of these devices to\nerrors. To provide fast error detection and correction, error-correcting codes\n(ECC) are placed on an additional DRAM chip in a DRAM module. This additional\nchip expands the raw capacity of a DRAM module by 12.5%, but the applications\nare unable to use any of this extra capacity, as it is used exclusively to\nprovide reliability for all data. In reality, there are a number of\napplications that do not need such strong reliability for all their data\nregions (e.g., some user batch jobs executing on a public cloud), and can\ninstead benefit from using additional DRAM capacity to store extra data. Our\ngoal in this work is to provide the additional capacity within an ECC DRAM\nmodule to applications when they do not need the high reliability of error\ncorrection.\n  In this paper, we propose Capacity- and Reliability-Adaptive Memory (CREAM),\na hardware mechanism that adapts error correcting DRAM modules to offer\nmultiple levels of error protection, and provides the capacity saved from using\nweaker protection to applications. For regions of memory that do not require\nstrong error correction, we either provide no ECC protection or provide error\ndetection using multibit parity. We evaluate several layouts for arranging the\ndata within ECC DRAM in these reduced-protection modes, taking into account the\nvarious trade-offs exposed from exploiting the extra chip. Our experiments show\nthat the increased capacity provided by CREAM improves performance by 23.0% for\na memory caching workload, and by 37.3% for a commercial web search workload\nexecuting production query traces. In addition, CREAM can increase bank-level\nparallelism within DRAM, offering further performance improvements.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:13:20 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 05:31:13 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Luo", "Yixin", ""], ["Ghose", "Saugata", ""], ["Li", "Tianshi", ""], ["Govindan", "Sriram", ""], ["Sharma", "Bikash", ""], ["Kelly", "Bryan", ""], ["Boroumand", "Amirali", ""], ["Mutlu", "Onur", ""]]}, {"id": "1706.09953", "submitter": "Michel Kinsy", "authors": "Michel A. Kinsy, Rashmi S. Agrawal and Hien D. Nguyen", "title": "Fast Processing of Large Graph Applications Using Asynchronous\n  Architecture", "comments": "Boston Area Architecture 2017 Workshop (BARC17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms and techniques are increasingly being used in scientific and\ncommercial applications to express relations and explore large data sets.\nAlthough conventional or commodity computer architectures, like CPU or GPU, can\ncompute fairly well dense graph algorithms, they are often inadequate in\nprocessing large sparse graph applications. Memory access patterns, memory\nbandwidth requirements and on-chip network communications in these applications\ndo not fit in the conventional program execution flow. In this work, we propose\nand design a new architecture for fast processing of large graph applications.\nTo leverage the lack of the spatial and temporal localities in these\napplications and to support scalable computational models, we design the\narchitecture around two key concepts. (1) The architecture is a multicore\nprocessor of independently clocked processing elements. These elements\ncommunicate in a self-timed manner and use handshaking to perform\nsynchronization, communication, and sequencing of operations. By being\nasynchronous, the operating speed at each processing element is determined by\nactual local latencies rather than global worst-case latencies. We create a\nspecialized ISA to support these operations. (2) The application compilation\nand mapping process uses a graph clustering algorithm to optimize parallel\ncomputing of graph operations and load balancing. Through the clustering\nprocess, we make scalability an inherent property of the architecture where\ntask-to-element mapping can be done at the graph node level or at node cluster\nlevel. A prototyped version of the architecture outperforms a comparable CPU by\n10~20x across all benchmarks and provides 2~5x better power efficiency when\ncompared to a GPU.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 21:07:14 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Kinsy", "Michel A.", ""], ["Agrawal", "Rashmi S.", ""], ["Nguyen", "Hien D.", ""]]}]