[{"id": "1602.00722", "submitter": "Kevin Chang", "authors": "Kevin K. Chang, Gabriel H. Loh, Mithuna Thottethodi, Yasuko Eckert,\n  Mike O'Connor, Srilatha Manne, Lisa Hsu, Lavanya Subramanian, Onur Mutlu", "title": "Enabling Efficient Dynamic Resizing of Large DRAM Caches via A Hardware\n  Consistent Hashing Mechanism", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Die-stacked DRAM has been proposed for use as a large, high-bandwidth,\nlast-level cache with hundreds or thousands of megabytes of capacity. Not all\nworkloads (or phases) can productively utilize this much cache space, however.\nUnfortunately, the unused (or under-used) cache continues to consume power due\nto leakage in the peripheral circuitry and periodic DRAM refresh. Dynamically\nadjusting the available DRAM cache capacity could largely eliminate this energy\noverhead. However, the current proposed DRAM cache organization introduces new\nchallenges for dynamic cache resizing. The organization differs from a\nconventional SRAM cache organization because it places entire cache sets and\ntheir tags within a single bank to reduce on-chip area and power overhead.\nHence, resizing a DRAM cache requires remapping sets from the powered-down\nbanks to active banks.\n  In this paper, we propose CRUNCH (Cache Resizing Using Native Consistent\nHashing), a hardware data remapping scheme inspired by consistent hashing, an\nalgorithm originally proposed to uniformly and dynamically distribute Internet\ntraffic across a changing population of web servers. CRUNCH provides a\nload-balanced remapping of data from the powered-down banks alone to the active\nbanks, without requiring sets from all banks to be remapped, unlike naive\nschemes to achieve load balancing. CRUNCH remaps only sets from the\npowered-down banks, so it achieves this load balancing with low bank\npower-up/down transition latencies. CRUNCH's combination of good load balancing\nand low transition latencies provides a substrate to enable efficient DRAM\ncache resizing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 21:43:40 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Chang", "Kevin K.", ""], ["Loh", "Gabriel H.", ""], ["Thottethodi", "Mithuna", ""], ["Eckert", "Yasuko", ""], ["O'Connor", "Mike", ""], ["Manne", "Srilatha", ""], ["Hsu", "Lisa", ""], ["Subramanian", "Lavanya", ""], ["Mutlu", "Onur", ""]]}, {"id": "1602.01329", "submitter": "Leonid Yavits", "authors": "Leonid Yavits, Amir Morad, Ran Ginosar", "title": "Effect of Data Sharing on Private Cache Design in Chip Multiprocessors", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multithreaded applications with high degree of data sharing, the miss rate\nof private cache is shown to exhibit a compulsory miss component. It manifests\nbecause at least some of the shared data originates from other cores and can\nonly be accessed in a shared cache. The compulsory component does not change\nwith the private cache size, causing its miss rate to diminish slower as the\ncache size grows. As a result, the peak performance of a Chip Multiprocessor\n(CMP) for workloads with high degree of data sharing is achieved with a smaller\nprivate cache, compared to workloads with no data sharing. The CMP performance\ncan be improved by reassigning some of the constrained area or power resource\nfrom private cache to core. Alternatively, the area or power budget of a CMP\ncan be reduced without a performance hit.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 15:02:05 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Ginosar", "Ran", ""]]}, {"id": "1602.01348", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar, Gennady Pekhimenko, Adwait Jog, Saugata Ghose,\n  Abhishek Bhowmick, Rachata Ausavarangnirun, Chita Das, Mahmut Kandemir, Todd\n  C. Mowry, Onur Mutlu", "title": "A Framework for Accelerating Bottlenecks in GPU Execution with Assist\n  Warps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Graphics Processing Units (GPUs) are well provisioned to support the\nconcurrent execution of thousands of threads. Unfortunately, different\nbottlenecks during execution and heterogeneous application requirements create\nimbalances in utilization of resources in the cores. For example, when a GPU is\nbottlenecked by the available off-chip memory bandwidth, its computational\nresources are often overwhelmingly idle, waiting for data from memory to\narrive.\n  This work describes the Core-Assisted Bottleneck Acceleration (CABA)\nframework that employs idle on-chip resources to alleviate different\nbottlenecks in GPU execution. CABA provides flexible mechanisms to\nautomatically generate \"assist warps\" that execute on GPU cores to perform\nspecific tasks that can improve GPU performance and efficiency.\n  CABA enables the use of idle computational units and pipelines to alleviate\nthe memory bandwidth bottleneck, e.g., by using assist warps to perform data\ncompression to transfer less data from memory. Conversely, the same framework\ncan be employed to handle cases where the GPU is bottlenecked by the available\ncomputational units, in which case the memory pipelines are idle and can be\nused by CABA to speed up computation, e.g., by performing memoization using\nassist warps.\n  We provide a comprehensive design and evaluation of CABA to perform effective\nand flexible data compression in the GPU memory hierarchy to alleviate the\nmemory bandwidth bottleneck. Our extensive evaluations show that CABA, when\nused to implement data compression, provides an average performance improvement\nof 41.7% (as high as 2.6X) across a variety of memory-bandwidth-sensitive GPGPU\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 15:57:59 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Vijaykumar", "Nandita", ""], ["Pekhimenko", "Gennady", ""], ["Jog", "Adwait", ""], ["Ghose", "Saugata", ""], ["Bhowmick", "Abhishek", ""], ["Ausavarangnirun", "Rachata", ""], ["Das", "Chita", ""], ["Kandemir", "Mahmut", ""], ["Mowry", "Todd C.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1602.01528", "submitter": "Song Han", "authors": "Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A.\n  Horowitz, William J. Dally", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "comments": "External Links: TheNextPlatform: http://goo.gl/f7qX0L ; O'Reilly:\n  https://goo.gl/Id1HNT ; Hacker News: https://goo.gl/KM72SV ; Embedded-vision:\n  http://goo.gl/joQNg8 ; Talk at NVIDIA GTC'16: http://goo.gl/6wJYvn ; Talk at\n  Embedded Vision Summit: https://goo.gl/7abFNe ; Talk at Stanford University:\n  https://goo.gl/6lwuer. Published as a conference paper in ISCA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks (DNNs) have hundreds of millions of\nconnections and are both computationally and memory intensive, making them\ndifficult to deploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation, fetching weights\nfrom DRAM is two orders of magnitude more expensive than ALU operations, and\ndominates the required power.\n  Previously proposed 'Deep Compression' makes it possible to fit large DNNs\n(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by\npruning the redundant connections and having multiple connections share the\nsame weight. We propose an energy efficient inference engine (EIE) that\nperforms inference on this compressed network model and accelerates the\nresulting sparse matrix-vector multiplication with weight sharing. Going from\nDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;\nWeight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.\nEvaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to\nCPU and GPU implementations of the same DNN without compression. EIE has a\nprocessing power of 102GOPS/s working directly on a compressed network,\ncorresponding to 3TOPS/s on an uncompressed network, and processes FC layers of\nAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is\n24,000x and 3,400x more energy efficient than a CPU and GPU respectively.\nCompared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy\nefficiency and area efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 01:28:28 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 04:27:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Han", "Song", ""], ["Liu", "Xingyu", ""], ["Mao", "Huizi", ""], ["Pu", "Jing", ""], ["Pedram", "Ardavan", ""], ["Horowitz", "Mark A.", ""], ["Dally", "William J.", ""]]}, {"id": "1602.01616", "submitter": "Jinhwan Park", "authors": "Jinhwan Park and Wonyong Sung", "title": "FPGA Based Implementation of Deep Neural Networks Using On-chip Memory\n  Only", "comments": "Published in ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) demand a very large amount of computation and\nweight storage, and thus efficient implementation using special purpose\nhardware is highly desired. In this work, we have developed an FPGA based\nfixed-point DNN system using only on-chip memory not to access external DRAM.\nThe execution time and energy consumption of the developed system is compared\nwith a GPU based implementation. Since the capacity of memory in FPGA is\nlimited, only 3-bit weights are used for this implementation, and training\nbased fixed-point weight optimization is employed. The implementation using\nXilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark\nand a phoneme recognition task on TIMIT corpus. The obtained speed is about one\nquarter of a GPU based implementation and much better than that of a PC based\none. The power consumption is less than 5 Watt at the full speed operation\nresulting in much higher efficiency compared to GPU based systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 10:16:13 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 06:24:25 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Park", "Jinhwan", ""], ["Sung", "Wonyong", ""]]}, {"id": "1602.02517", "submitter": "Jose Nunez-Yanez Dr", "authors": "Jose Nunez-Yanez, Tom Sun", "title": "Energy Efficient Video Fusion with Heterogeneous CPU-FPGA Devices", "comments": "Presented at HIP3ES, 2016", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2016/3", "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a complete video fusion system with hardware acceleration\nand investigates the energy trade-offs between computing in the CPU or the FPGA\ndevice. The video fusion application is based on the Dual-Tree Complex Wavelet\nTransforms (DT-CWT). In this work the transforms are mapped to a hardware\naccelerator using high-level synthesis tools for the FPGA and also vectorized\ncode for the single instruction multiple data (SIMD) engine available in the\nCPU. The accelerated system reduces computation time and energy by a factor of\n2. Moreover, the results show a key finding that the FPGA is not always the\nbest choice for acceleration, and the SIMD engine should be selected when the\nwavelet decomposition reduces the frame size below a certain threshold. This\ndependency on workload size means that an adaptive system that intelligently\nselects between the SIMD engine and the FPGA achieves the most energy and\nperformance efficiency point.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:28:44 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Nunez-Yanez", "Jose", ""], ["Sun", "Tom", ""]]}, {"id": "1602.03016", "submitter": "Marcelo A. Montemurro", "authors": "Francisco Ortega-Zamorano, Marcelo A. Montemurro, Sergio A. Cannas,\n  Jos\\'e M. Jerez, and Leonardo Franco", "title": "FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising\n  Model", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TPDS.2015.2505725", "report-no": null, "categories": "cs.AR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-dimensional Ising model with nearest-neighbors ferromagnetic\ninteractions is implemented in a Field Programmable Gate Array (FPGA)\nboard.Extensive Monte Carlo simulations were carried out using an efficient\nhardware representation of individual spins and a combined global-local LFSR\nrandom number generator. Consistent results regarding the descriptive\nproperties of magnetic systems, like energy, magnetization and susceptibility\nare obtained while a speed-up factor of approximately 6 times is achieved in\ncomparison to previous FPGA-based published works and almost $10^4$ times in\ncomparison to a standard CPU simulation. A detailed description of the logic\ndesign used is given together with a careful analysis of the quality of the\nrandom number generator used. The obtained results confirm the potential of\nFPGAs for analyzing the statistical mechanics of magnetic systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 15:04:11 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Ortega-Zamorano", "Francisco", ""], ["Montemurro", "Marcelo A.", ""], ["Cannas", "Sergio A.", ""], ["Jerez", "Jos\u00e9 M.", ""], ["Franco", "Leonardo", ""]]}, {"id": "1602.03095", "submitter": "Kai Cong", "authors": "Kai Cong, Li Lei, Zhenkun Yang, Fei Xie", "title": "OpenRISC System-on-Chip Design Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the hardware emulation technique has emerged as a promising approach\nto accelerating hardware verification/debugging process. To fully evaluate the\npowerfulness of the emulation approach and demonstrate its potential impact, we\npropose to emulate a system-on-chip (SoC) design using Mentor Graphics Veloce\nemulation platform. This article presents our project setup and the results we\nhave achieved. The results are encouraging. ORPSoC emulation with Veloce has\nmore than ten times faster than hardware simulation. Our experimental results\ndemonstrate that Mentor Graphics Veloce has major advantages in emulation,\nverification, and debugging of complicated real hardware designs, especially in\nthe context of SoC complexity. Through our three major tasks, we will\ndemonstrate that (1) Veloce can successfully emulate large-scale SoC designs;\n(2) it has much better performance comparing to the state-of-the-art simulation\ntools; (3) it can significantly accelerate the process of hardware verification\nand debugging while maintaining full signal visibility.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 17:59:24 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Cong", "Kai", ""], ["Lei", "Li", ""], ["Yang", "Zhenkun", ""], ["Xie", "Fei", ""]]}, {"id": "1602.04183", "submitter": "Ardavan Pedram", "authors": "Ardavan Pedram, Stephen Richardson, Sameh Galal, Shahar Kvatinsky, and\n  Mark A. Horowitz", "title": "Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon\n  Era", "comments": "8 pages, To appear in IEEE Design and Test Journal", "journal-ref": "IEEE Design & Test ( Volume: 34, Issue: 2, April 2017 )", "doi": "10.1109/MDAT.2016.2573586", "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge to improving performance in the age of Dark Silicon is how\nto leverage transistors when they cannot all be used at the same time. In\nmodern SOCs, these transistors are often used to create specialized\naccelerators which improve energy efficiency for some applications by 10-1000X.\nWhile this might seem like the magic bullet we need, for most CPU applications\nmore energy is dissipated in the memory system than in the processor: these\nlarge gains in efficiency are only possible if the DRAM and memory hierarchy\nare mostly idle. We refer to this desirable state as Dark Memory, and it only\noccurs for applications with an extreme form of locality.\n  To show our findings, we introduce Pareto curves in the energy/op and\nmm$^2$/(ops/s) metric space for compute units, accelerators, and on-chip\nmemory/interconnect. These Pareto curves allow us to solve the power,\nperformance, area constrained optimization problem to determine which\naccelerators should be used, and how to set their design parameters to optimize\nthe system. This analysis shows that memory accesses create a floor to the\nachievable energy-per-op. Thus high performance requires Dark Memory, which in\nturn requires co-design of the algorithm for parallelism and locality, with the\nhardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 19:48:31 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 20:06:16 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 00:49:56 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Pedram", "Ardavan", ""], ["Richardson", "Stephen", ""], ["Galal", "Sameh", ""], ["Kvatinsky", "Shahar", ""], ["Horowitz", "Mark A.", ""]]}, {"id": "1602.04414", "submitter": "Tosiron Adegbija", "authors": "Tosiron Adegbija and Ann Gordon-Ross", "title": "Temperature-aware Dynamic Optimization of Embedded Systems", "comments": "24 pages, 12 figures, Extended version of paper \"Thermal-aware\n  Phase-based Tuning of Embedded Systems\" published in GLSVLSI 2014, Submitted\n  to ACM TODAES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to embedded systems` stringent design constraints, much prior work\nfocused on optimizing energy consumption and/or performance. Since embedded\nsystems typically have fewer cooling options, rising temperature, and thus\ntemperature optimization, is an emergent concern. Most embedded systems only\ndissipate heat by passive convection, due to the absence of dedicated thermal\nmanagement hardware mechanisms. The embedded system`s temperature not only\naffects the system`s reliability, but could also affect the performance, power,\nand cost. Thus, embedded systems require efficient thermal management\ntechniques. However, thermal management can conflict with other optimization\nobjectives, such as execution time and energy consumption. In this paper, we\nfocus on managing the temperature using a synergy of cache optimization and\ndynamic frequency scaling, while also optimizing the execution time and energy\nconsumption. This paper provides new insights on the impact of cache parameters\non efficient temperature-aware cache tuning heuristics. In addition, we present\ntemperature-aware phase-based tuning, TaPT, which determines Pareto optimal\nclock frequency and cache configurations for fine-grained execution time,\nenergy, and temperature tradeoffs. TaPT enables autonomous system optimization\nand also allows designers to specify temperature constraints and optimization\npriorities. Experiments show that TaPT can effectively reduce execution time,\nenergy, and temperature, while imposing minimal hardware overhead.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 04:42:19 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Adegbija", "Tosiron", ""], ["Gordon-Ross", "Ann", ""]]}, {"id": "1602.04415", "submitter": "Tosiron Adegbija", "authors": "Tosiron Adegbija, Ann Gordon-Ross, and Arslan Munir", "title": "Phase distance mapping: a phase-based cache tuning methodology for\n  embedded systems", "comments": "26 pages, Springer Design Automation for Embedded Systems, Special\n  Issue on Networked Embedded Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networked embedded systems typically leverage a collection of low-power\nembedded systems (nodes) to collaboratively execute applications spanning\ndiverse application domains (e.g., video, image processing, communication,\netc.) with diverse application requirements. The individual networked nodes\nmust operate under stringent constraints (e.g., energy, memory, etc.) and\nshould be specialized to meet varying application requirements in order to\nadhere to these constraints. Phase-based tuning specializes system tunable\nparameters to the varying runtime requirements of different execution phases to\nmeet optimization goals. Since the design space for tunable systems can be very\nlarge, one of the major challenges in phase-based tuning is determining the\nbest configuration for each phase without incurring significant tuning overhead\n(e.g., energy and/or performance) during design space exploration. In this\npaper, we propose phase distance mapping, which directly determines the best\nconfiguration for a phase, thereby eliminating design space exploration. Phase\ndistance mapping applies the correlation between the characteristics and best\nconfiguration of a known phase to determine the best configuration of a new\nphase. Experimental results verify that our phase distance mapping approach,\nwhen applied to cache tuning, determines cache configurations within 1 % of the\noptimal configurations on average and yields an energy delay product savings of\n27 % on average.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 04:42:38 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Adegbija", "Tosiron", ""], ["Gordon-Ross", "Ann", ""], ["Munir", "Arslan", ""]]}, {"id": "1602.06038", "submitter": "Yu Zhang", "authors": "Yu Zhang, Wenlong Feng, Mengxing Huang", "title": "Automatic Generation of High-Coverage Tests for RTL Designs using\n  Software Techniques and Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Register Transfer Level (RTL) design validation is a crucial stage in the\nhardware design process. We present a new approach to enhancing RTL design\nvalidation using available software techniques and tools. Our approach converts\nthe source code of a RTL design into a C++ software program. Then a powerful\nsymbolic execution engine is employed to execute the converted C++ program\nsymbolically to generate test cases. To better generate efficient test cases,\nwe limit the number of cycles to guide symbolic execution. Moreover, we add\nbit-level symbolic variable support into the symbolic execution engine.\nGenerated test cases are further evaluated by simulating the RTL design to get\naccurate coverage. We have evaluated the approach on a floating point unit\n(FPU) design. The preliminary results show that our approach can deliver\nhigh-quality tests to achieve high coverage.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 03:49:44 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Zhang", "Yu", ""], ["Feng", "Wenlong", ""], ["Huang", "Mengxing", ""]]}]