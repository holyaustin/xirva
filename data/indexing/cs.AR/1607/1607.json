[{"id": "1607.00064", "submitter": "Yongming Shen", "authors": "Yongming Shen, Michael Ferdman, and Peter Milder", "title": "Maximizing CNN Accelerator Efficiency Through Resource Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are revolutionizing machine learning,\nbut they present significant computational challenges. Recently, many\nFPGA-based accelerators have been proposed to improve the performance and\nefficiency of CNNs. Current approaches construct a single processor that\ncomputes the CNN layers one at a time; the processor is optimized to maximize\nthe throughput at which the collection of layers is computed. However, this\napproach leads to inefficient designs because the same processor structure is\nused to compute CNN layers of radically varying dimensions.\n  We present a new CNN accelerator paradigm and an accompanying automated\ndesign methodology that partitions the available FPGA resources into multiple\nprocessors, each of which is tailored for a different subset of the CNN\nconvolutional layers. Using the same FPGA resources as a single large\nprocessor, multiple smaller specialized processors increase computational\nefficiency and lead to a higher overall throughput. Our design methodology\nachieves 3.8x higher throughput than the state-of-the-art approach on\nevaluating the popular AlexNet CNN on a Xilinx Virtex-7 FPGA. For the more\nrecent SqueezeNet and GoogLeNet, the speedups are 2.2x and 2.0x.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 22:10:13 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 05:36:34 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Shen", "Yongming", ""], ["Ferdman", "Michael", ""], ["Milder", "Peter", ""]]}, {"id": "1607.00667", "submitter": "Sai Zhang", "authors": "Sai Zhang, Mingu Kang, Charbel Sakr, Naresh Shanbhag", "title": "Reducing the Energy Cost of Inference via In-sensor Information\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much interest in incorporating inference capabilities into\nsensor-rich embedded platforms such as autonomous vehicles, wearables, and\nothers. A central problem in the design of such systems is the need to extract\ninformation locally from sensed data on a severely limited energy budget. This\nnecessitates the design of energy-efficient sensory embedded system. A typical\nsensory embedded system enforces a physical separation between sensing and\ncomputational subsystems - a separation mandated by the differing requirements\nof the sensing and computational functions. As a consequence, the energy\nconsumption in such systems tends to be dominated by the energy consumed in\ntransferring data over the sensor-processor interface (communication energy)\nand the energy consumed in processing the data in digital processor\n(computational energy). In this article, we propose an in-sensor computing\narchitecture which (mostly) eliminates the sensor-processor interface by\nembedding inference computations in the noisy sensor fabric in analog and\nretraining the hyperparameters in order to compensate for non-ideal\ncomputations. The resulting architecture referred to as the Compute Sensor - a\nsensor that computes in addition to sensing - represents a radical departure\nfrom the conventional. We show that a Compute Sensor for image data can be\ndesigned by embedding both feature extraction and classification functions in\nthe analog domain in close proximity to the CMOS active pixel sensor (APS)\narray. Significant gains in energy-efficiency are demonstrated using behavioral\nand energy models in a commercial semiconductor process technology. In the\nprocess, the Compute Sensor creates a unique opportunity to develop machine\nlearning algorithms for information extraction from data on a noisy underlying\ncomputational fabric.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 18:43:55 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Zhang", "Sai", ""], ["Kang", "Mingu", ""], ["Sakr", "Charbel", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "1607.01643", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "A configurable accelerator for manycores: the Explicitly Many-Processor\n  Approach", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A new approach to designing processor accelerators is presented. A new\ncomputing model and a special kind of accelerator with dynamic (end-user\nprogrammable) architecture is suggested. The new model considers a processor,\nin which a newly introduced supervisor layer coordinates the job of the cores.\nThe cores have the ability (based on the parallelization information provided\nby the compiler, and using the help of the supervisor) to outsource part of the\njob they received to some neighbouring core. The introduced changes essentially\nand advantageously modify the architecture and operation of the computing\nsystems. The computing throughput drastically increases, the efficiency of the\ntechnological implementation (computing performance per logic gates) increases,\nthe non-payload activity for using operating system services decreases, the\nreal-time behavior changes advantageously, and connecting accelerators to the\nprocessor greatly simplifies. Here only some details of the architecture and\noperation of the processor are discussed, the rest is described elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:40:03 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1607.02318", "submitter": "Christopher Celio", "authors": "Christopher Celio, Palmer Dabbelt, David A. Patterson, Krste\n  Asanovi\\'c", "title": "The Renewed Case for the Reduced Instruction Set Computer: Avoiding ISA\n  Bloat with Macro-Op Fusion for RISC-V", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCB/EECS-2016-130", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report makes the case that a well-designed Reduced Instruction Set\nComputer (RISC) can match, and even exceed, the performance and code density of\nexisting commercial Complex Instruction Set Computers (CISC) while maintaining\nthe simplicity and cost-effectiveness that underpins the original RISC goals.\n  We begin by comparing the dynamic instruction counts and dynamic instruction\nbytes fetched for the popular proprietary ARMv7, ARMv8, IA-32, and x86-64\nInstruction Set Architectures (ISAs) against the free and open RISC-V RV64G and\nRV64GC ISAs when running the SPEC CINT2006 benchmark suite. RISC-V was designed\nas a very small ISA to support a wide range of implementations, and has a less\nmature compiler toolchain. However, we observe that on SPEC CINT2006 RV64G\nexecutes on average 16% more instructions than x86-64, 3% more instructions\nthan IA-32, 9% more instructions than ARMv8, but 4% fewer instructions than\nARMv7.\n  CISC x86 implementations break up complex instructions into smaller internal\nRISC-like micro-ops, and the RV64G instruction count is within 2% of the x86-64\nretired micro-op count. RV64GC, the compressed variant of RV64G, is the densest\nISA studied, fetching 8% fewer dynamic instruction bytes than x86-64. We\nobserved that much of the increased RISC-V instruction count is due to a small\nset of common multi-instruction idioms.\n  Exploiting this fact, the RV64G and RV64GC effective instruction count can be\nreduced by 5.4% on average by leveraging macro-op fusion. Combining the\ncompressed RISC-V ISA extension with macro-op fusion provides both the densest\nISA and the fewest dynamic operations retired per program, reducing the\nmotivation to add more instructions to the ISA. This approach retains a single\nsimple ISA suitable for both low-end and high-end implementations, where\nhigh-end implementations can boost performance through microarchitectural\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:15:31 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Celio", "Christopher", ""], ["Dabbelt", "Palmer", ""], ["Patterson", "David A.", ""], ["Asanovi\u0107", "Krste", ""]]}, {"id": "1607.03238", "submitter": "Vishwesh Jatala", "authors": "Vishwesh Jatala, Jayvant Anantpur, Amey Karkare", "title": "Scratchpad Sharing in GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPGPU applications exploit on-chip scratchpad memory available in the\nGraphics Processing Units (GPUs) to improve performance. The amount of thread\nlevel parallelism present in the GPU is limited by the number of resident\nthreads, which in turn depends on the availability of scratchpad memory in its\nstreaming multiprocessor (SM). Since the scratchpad memory is allocated at\nthread block granularity, part of the memory may remain unutilized. In this\npaper, we propose architectural and compiler optimizations to improve the\nscratchpad utilization. Our approach, Scratchpad Sharing, addresses scratchpad\nunder-utilization by launching additional thread blocks in each SM. These\nthread blocks use unutilized scratchpad and also share scratchpad with other\nresident blocks. To improve the performance of scratchpad sharing, we propose\nOwner Warp First (OWF) scheduling that schedules warps from the additional\nthread blocks effectively. The performance of this approach, however, is\nlimited by the availability of the shared part of scratchpad.\n  We propose compiler optimizations to improve the availability of shared\nscratchpad. We describe a scratchpad allocation scheme that helps in allocating\nscratchpad variables such that shared scratchpad is accessed for short\nduration. We introduce a new instruction, relssp, that when executed, releases\nthe shared scratchpad. Finally, we describe an analysis for optimal placement\nof relssp instructions such that shared scratchpad is released as early as\npossible.\n  We implemented the hardware changes using the GPGPU-Sim simulator and\nimplemented the compiler optimizations in Ocelot framework. We evaluated the\neffectiveness of our approach on 19 kernels from 3 benchmarks suites: CUDA-SDK,\nGPGPU-Sim, and Rodinia. The kernels that underutilize scratchpad memory show an\naverage improvement of 19% and maximum improvement of 92.17% compared to the\nbaseline approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 06:45:08 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 05:58:07 GMT"}, {"version": "v3", "created": "Sat, 15 Oct 2016 08:05:51 GMT"}, {"version": "v4", "created": "Sat, 17 Dec 2016 13:55:19 GMT"}, {"version": "v5", "created": "Sun, 12 Feb 2017 06:50:55 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Jatala", "Vishwesh", ""], ["Anantpur", "Jayvant", ""], ["Karkare", "Amey", ""]]}, {"id": "1607.04549", "submitter": "Philipp Wagner", "authors": "Philipp Wagner, Thomas Wild, Andreas Herkersdorf", "title": "DiaSys: Improving SoC Insight Through On-Chip Diagnosis", "comments": null, "journal-ref": null, "doi": "10.1016/j.sysarc.2017.01.005", "report-no": null, "categories": "cs.DC cs.AR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To find the cause of a functional or non-functional defect (bug) in software\nrunning on a multi-processor System-on-Chip (MPSoC), developers need insight\ninto the chip. Tracing systems provide this insight non-intrusively, at the\ncost of high off-chip bandwidth requirements. This I/O bottleneck limits the\nobservability, a problem becoming more severe as more functionality is\nintegrated on-chip. In this paper, we present DiaSys, an MPSoC diagnosis system\nwith the potential to replace today's tracing systems. Its main idea is to\npartially execute the analysis of observation data on the chip; in consequence,\nmore information and less data is sent to the attached host PC. With DiaSys,\nthe data analysis is performed by the diagnosis application. Its input are\nevents, which are generated by observation hardware at interesting points in\nthe program execution (like a function call). Its outputs are events with\nhigher information density. The event transformation is modeled as dataflow\napplication. For execution, it is mapped in part to dedicated and distributed\non-chip components, and in part to the host PC; the off-chip boundary is\ntransparent to the developer of the diagnosis application. We implement DiaSys\nas extension to an existing SoC with four tiles and a mesh network running on\nan FPGA platform. Two usage examples confirm that DiaSys is flexible enough to\nreplace a tracing system, while significantly lowering the off-chip bandwidth\nrequirements. In our examples, the debugging of a race-condition bug, and the\ncreation of a lock contention profile, we see a reduction of trace bandwidth of\nmore than three orders of magnitude, compared to a full trace created by a\ncommon tracing system.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:16:03 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 14:38:51 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Wagner", "Philipp", ""], ["Wild", "Thomas", ""], ["Herkersdorf", "Andreas", ""]]}, {"id": "1607.06541", "submitter": "Jeremy Kepner", "authors": "William S. Song, Vitaliy Gleyzer, Alexei Lomakin, Jeremy Kepner", "title": "Novel Graph Processor Architecture, Prototype System, and Results", "comments": "7 pages, 8 figures, IEEE HPEC 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761635", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms are increasingly used in applications that exploit large\ndatabases. However, conventional processor architectures are inadequate for\nhandling the throughput and memory requirements of graph computation. Lincoln\nLaboratory's graph-processor architecture represents a rethinking of parallel\narchitectures for graph problems. Our processor utilizes innovations that\ninclude a sparse matrix-based graph instruction set, a cacheless memory system,\naccelerator-based architecture, a systolic sorter, high-bandwidth\nmulti-dimensional toroidal communication network, and randomized\ncommunications. A field-programmable gate array (FPGA) prototype of the new\ngraph processor has been developed with significant performance enhancement\nover conventional processors in graph computational throughput.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 02:22:44 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Song", "William S.", ""], ["Gleyzer", "Vitaliy", ""], ["Lomakin", "Alexei", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1607.07766", "submitter": "Georgios Passas", "authors": "Giorgos Passas", "title": "Uber: Utilizing Buffers to Simplify NoCs for Hundreds-Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaching ideal wire latency using a network-on-chip (NoC) is an important\npractical problem for many-core systems, particularly hundreds-cores. Although\nother researchers have focused on optimizing large meshes, bypassing or\nspeculating router pipelines, or creating more intricate logarithmic\ntopologies, this paper proposes a balanced combination that trades queue\nbuffers for simplicity. Preliminary analysis of nine benchmarks from PARSEC and\nSPLASH using execution-driven simulation shows that utilization rises from 2%\nwhen connecting a single core per mesh port to at least 50%, as slack for delay\nin concentrator and router queues is around 6x higher compared to the ideal\nlatency of just 20 cycles. That is, a 16-port mesh suffices because queueing is\nthe uncommon case for system performance. In this way, the mesh hop count is\nbounded to three, as load becomes uniform via extended concentration, and ideal\nlatency is approached using conventional four-stage pipelines for the mesh\nrouters together with minor logarithmic edges. A realistic Uber is also\ndetailed, featuring the same performance as a 64-port mesh that employs\noptimized router pipelines, improving the baseline by 12%. Ongoing work\ndevelops techniques to better balance load by tuning the placement of cache\nblocks, and compares Uber with bufferless routing.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:53:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 10:47:17 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Passas", "Giorgos", ""]]}, {"id": "1607.08086", "submitter": "Navid Khoshavi", "authors": "Navid Khoshavi, Xunchao Chen, Jun Wang, and Ronald F. DeMara", "title": "Read-Tuned STT-RAM and eDRAM Cache Hierarchies for Throughput and Energy\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As capacity and complexity of on-chip cache memory hierarchy increases, the\nservice cost to the critical loads from Last Level Cache (LLC), which are\nfrequently repeated, has become a major concern. The processor may stall for a\nconsiderable interval while waiting to access the data stored in the cache\nblocks in LLC, if there are no independent instructions to execute. To provide\naccelerated service to the critical loads requests from LLC, this work\nconcentrates on leveraging the additional capacity offered by replacing\nSRAM-based L2 with Spin-Transfer Torque Random Access Memory (STT-RAM) to\naccommodate frequently accessed cache blocks in exclusive read mode in favor of\nreducing the overall read service time. Our proposed technique partitions L2\ncache into two STT-RAM arrangements with different write performance and data\nretention time. The retention-relaxed STT-RAM arrays are utilized to\neffectively deal with the regular L2 cache requests while the high retention\nSTT-RAM arrays in L2 are selected for maintaining repeatedly read accessed\ncache blocks from LLC by incurring negligible energy consumption for data\nretention. Our experimental results show that the proposed technique can reduce\nthe mean L2 read miss ratio by 51.4% and increase the IPC by 11.7% on average\nacross PARSEC benchmark suite while significantly decreasing the total L2\nenergy consumption compared to conventional SRAM-based L2 design.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:37:30 GMT"}, {"version": "v2", "created": "Sat, 6 Aug 2016 23:30:40 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Khoshavi", "Navid", ""], ["Chen", "Xunchao", ""], ["Wang", "Jun", ""], ["DeMara", "Ronald F.", ""]]}, {"id": "1607.08523", "submitter": "Navid Khoshavi", "authors": "Navid Khoshavi, Armin Samiei", "title": "The Study of Transient Faults Propagation in Multithread Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas contemporary Error Correcting Codes (ECC) designs occupy a\nsignificant fraction of total die area in chip-multiprocessors (CMPs),\napproaches to deal with the vulnerability increase of CMP architecture against\nSingle Event Upsets (SEUs) and Multi-Bit Upsets (MBUs) are sought. In this\npaper, we focus on reliability assessment of multithreaded applications running\non CMPs to propose an adaptive application-relevant architecture design to\naccommodate the impact of both SEUs and MBUs in the entire CMP architecture.\nThis work concentrates on leveraging the intrinsic soft-error-immunity feature\nof Spin-Transfer Torque RAM (STT-RAM) as an alternative for SRAM-based storage\nand operation components. We target a specific portion of working set for\nreallocation to improve the reliability level of the CMP architecture design. A\nselected portion of instructions in multithreaded program which experience high\nrate of referencing with the lowest memory modification are ideal candidate to\nbe stored and executed in STT-RAM based components. We argue about why we\ncannot use STT-RAM for the global storage and operation counterparts and\ndescribe the obtained resiliency compared to the baseline setup. In addition, a\ndetail study of the impact of SEUs and MBUs on multithreaded programs will be\npresented in the Appendix.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 16:33:31 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Khoshavi", "Navid", ""], ["Samiei", "Armin", ""]]}, {"id": "1607.08635", "submitter": "Zhengdong Zhang", "authors": "Amr Suleiman, Zhengdong Zhang, Vivienne Sze", "title": "A 58.6mW Real-Time Programmable Object Detector with Multi-Scale\n  Multi-Object Support Using Deformable Parts Model on 1920x1080 Video at 30fps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a programmable, energy-efficient and real-time object\ndetection accelerator using deformable parts models (DPM), with 2x higher\naccuracy than traditional rigid body models. With 8 deformable parts detection,\nthree methods are used to address the high computational complexity:\nclassification pruning for 33x fewer parts classification, vector quantization\nfor 15x memory size reduction, and feature basis projection for 2x reduction of\nthe cost of each classification. The chip is implemented in 65nm CMOS\ntechnology, and can process HD (1920x1080) images at 30fps without any off-chip\nstorage while consuming only 58.6mW (0.94nJ/pixel, 1168 GOPS/W). The chip has\ntwo classification engines to simultaneously detect two different classes of\nobjects. With a tested high throughput of 60fps, the classification engines can\nbe time multiplexed to detect even more than two object classes. It is energy\nscalable by changing the pruning factor or disabling the parts classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 19:20:33 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Suleiman", "Amr", ""], ["Zhang", "Zhengdong", ""], ["Sze", "Vivienne", ""]]}]