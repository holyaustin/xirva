[{"id": "1902.00478", "submitter": "Ghasem Pasandi", "authors": "Ghasem Pasandi and Shahin Nazarian and Massoud Pedram", "title": "Approximate Logic Synthesis: A Reinforcement Learning-Based Technology\n  Mapping Approach", "comments": "20th International Symposium on Quality Electronic Design (ISQED\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Logic Synthesis (ALS) is the process of synthesizing and mapping\na given Boolean network to a library of logic cells so that the magnitude/rate\nof error between outputs of the approximate and initial (exact) Boolean\nnetlists is bounded from above by a predetermined total error threshold. In\nthis paper, we present Q-ALS, a novel framework for ALS with focus on the\ntechnology mapping phase. Q-ALS incorporates reinforcement learning and\nutilizes Boolean difference calculus to estimate the maximum error rate that\neach node of the given network can tolerate such that the total error rate at\nnon of the outputs of the mapped netlist exceeds a predetermined maximum error\nrate, and the worst case delay and the total area are minimized. Maximum\nHamming Distance (MHD) between exact and approximate truth tables of cuts of\neach node is used as the error metric. In Q-ALS, a Q-Learning agent is trained\nwith a sufficient number of iterations aiming to select the fittest values of\nMHD for each node, and in a cut-based technology mapping approach, the best\nsupergates (in terms of delay and area, bounded further by the fittest MHD) are\nselected towards implementing each node. Experimental results show that having\nset the required accuracy of 95% at the primary outputs, Q-ALS reduces the\ntotal cost in terms of area and delay by up to 70% and 36%, respectively, and\nalso reduces the run-time by 2.21 times on average, when compared to the best\nstate-of-the-art academic ALS tools.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:53:17 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Pasandi", "Ghasem", ""], ["Nazarian", "Shahin", ""], ["Pedram", "Massoud", ""]]}, {"id": "1902.00484", "submitter": "Ghasem Pasandi", "authors": "Ghasem Pasandi, Raghav Mehta, Massoud Pedram, Shahin Nazarian", "title": "Hybrid Cell Assignment and Sizing for Power, Area, Delay Product\n  Optimization of SRAM Arrays", "comments": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II: EXPRESS BRIEF (DOI:\n  10.1109/TCSII.2019.2896794)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory accounts for a considerable portion of the total power budget and area\nof digital systems. Furthermore, it is typically the performance bottleneck of\nthe processing units. Therefore, it is critical to optimize the memory with\nrespect to the product of power, area, and delay (PAD). We propose a hybrid\ncell assignment method based on multi-sized and dual-Vth SRAM cells which\nimproves the PAD cost function by 34% compared to the conventional cell\nassignment. We also utilize the sizing of SRAM cells for minimizing the Data\nRetention Voltage (DRV), and voltages for the read and write operations in the\nSRAM array. Experimental results in a 32nm technology show that combining the\nproposed hybrid cell assignment and the cell sizing methods can lower PAD by up\nto 41% when compared to the conventional cell design and assignment.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:02:53 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Pasandi", "Ghasem", ""], ["Mehta", "Raghav", ""], ["Pedram", "Massoud", ""], ["Nazarian", "Shahin", ""]]}, {"id": "1902.01151", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio, Muhammad Abdullah Hanif, Mohammad Taghi Teimoori,\n  Muhammad Shafique", "title": "CapStore: Energy-Efficient Design and Management of the On-Chip Memory\n  for CapsuleNet Inference Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been established as the state-of-the-art\nalgorithm for advanced machine learning applications. Recently, CapsuleNets\nhave improved the generalization ability, as compared to DNNs, due to their\nmulti-dimensional capsules. However, they pose high computational and memory\nrequirements, which makes energy-efficient inference a challenging task. In\nthis paper, we perform an extensive analysis to demonstrate their key\nlimitations due to intense memory accesses and large on-chip memory\nrequirements. To enable efficient CaspuleNet inference accelerators, we propose\na specialized on-chip memory hierarchy which minimizes the off-chip memory\naccesses, while efficiently feeding the data to the accelerator. We analyze the\non-chip memory requirements for each memory component of the architecture. By\nleveraging this analysis, we propose a methodology to explore different on-chip\nmemory designs and a power-gating technique to further reduce the energy\nconsumption, depending upon the utilization across different operations of a\nCapsuleNet. Our memory designs can significantly reduce the energy consumption\nof the on-chip memory by up to 86%, when compared to a state-of-the-art memory\ndesign. Since the power consumption of the memory elements is the major\ncontributor in the power breakdown of the CapsuleNet accelerator, as we will\nalso show in our analyses, the proposed memory design can effectively reduce\nthe overall energy consumption of the complete CapsuleNet accelerator\narchitecture.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 12:38:40 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 14:24:06 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Marchisio", "Alberto", ""], ["Hanif", "Muhammad Abdullah", ""], ["Teimoori", "Mohammad Taghi", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1902.03314", "submitter": "Aleksandr Romanov Yur'evich", "authors": "Shchegoleva M.A., Romanov A.Yu., Lezhnev E.V., Amerikanov A.A", "title": "Routing in Networks on Chip with Multiplicative Circulant Topology", "comments": "7 p., 4 fig., International Conference on Computer Simulation in\n  Physics and beyond", "journal-ref": null, "doi": "10.1088/1742-6596/1163/1/012027", "report-no": null, "categories": "cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of multi-core processor systems is a demanded branch of\nscience and technology. The appearance of processors with dozens and hundreds\nof cores poses to the developers the question of choosing the optimal topology\ncapable to provide efficient routing in a network with a large number of nodes.\nIn this paper, we consider the possibility of using multiplicative circulants\nas a topology for networks-on-chip. A specialized routing algorithm for\nnetworks with multiplicative circulant topology, taking into account topology\nfeatures and having a high scalability, has been developed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:33:18 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["A.", "Shchegoleva M.", ""], ["Yu.", "Romanov A.", ""], ["V.", "Lezhnev E.", ""], ["A", "Amerikanov A.", ""]]}, {"id": "1902.03518", "submitter": "Fan Yao", "authors": "Fan Yao and Guru Venkataramani", "title": "Architecting Non-Volatile Main Memory to Guard Against Persistence-based\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DRAM-based main memory and its associated components increasingly account for\na significant portion of application performance bottlenecks and power budget\ndemands inside the computing ecosystem. To alleviate the problems of storage\ndensity and power constraints associated with DRAM, system architects are\ninvestigating alternative non-volatile memory technologies such as Phase Change\nMemory (PCM) to either replace or be used alongside DRAM memory. While such\nalternative memory types offer many promises to overcome the DRAM-related\nissues, they present a significant security threat to the users due to\npersistence of memory data even after power down.\n  In this paper, we investigate smart mechanisms to obscure the data left in\nnon-volatile memory after power down. In particular, we analyze the effect of\nusing a single encryption algorithm versus differentiated encryption based on\nthe security needs of the application phases. We also explore the effect of\nencryption on a hybrid main memory that has a DRAM buffer cache plus PCM main\nmemory. Our mechanism takes into account the limited write endurance problem\nassociated with several non-volatile memory technologies including PCM, and\navoids any additional writes beyond those originally issued by the\napplications. We evaluate using Gem5 simulator and SPEC 2006 applications, and\nshow the performance and power overheads of our proposed design.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 23:59:47 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Yao", "Fan", ""], ["Venkataramani", "Guru", ""]]}, {"id": "1902.04641", "submitter": "Swamit Tannu", "authors": "Swamit S. Tannu, Poulami Das, Michael L. Lewis, Robert Krick, Douglas\n  M. Carmean, Moinuddin K. Qureshi", "title": "A Case for Superconducting Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the scaling of conventional CMOS-based technologies slows down, there is\ngrowing interest in alternative technologies that can improve performance or\nenergy-efficiency. Superconducting circuits based on Josephson Junction (JJ) is\nan emerging technology that can provide devices which can be switched with\npico-second latencies and consuming two orders of magnitude lower switching\nenergy compared to CMOS. While JJ-based circuits can provide high operating\nfrequency and energy-efficiency, this technology faces three critical\nchallenges: limited device density and lack of area-efficient technology for\nmemory structures, reduced gate fanout compared to CMOS, and new failure modes\nof Flux-Traps that occurs due to the operating environment.\n  The lack of dense memory technology restricts the use of superconducting\ntechnology in the near term to application domains that have high compute\nintensity but require negligible amount of memory. In this paper, we study the\nuse of superconducting technology to build an accelerator for SHA-256 engines\ncommonly used in Bitcoin mining applications. We show that merely porting\nexisting CMOS-based accelerator to superconducting technology provides 10.6X\nimprovement in energy efficiency. Redesigning the accelerator to suit the\nunique constraints of superconducting technology (such as low fanout) improves\nthe energy efficiency to 12.2X. We also investigate solutions to make the\naccelerator tolerant of new fault modes and show how this fault-tolerant design\ncan be leveraged to reduce the operating current, thereby increasing the\noverall energy-efficiency to 46X compared to CMOS. Our paper also develops a\nworkflow for evaluating area, performance, and power for accelerators built in\nsuperconducting technology, and this workflow can help other researchers\nexplore designs using this technology.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 21:21:03 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 17:07:46 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Tannu", "Swamit S.", ""], ["Das", "Poulami", ""], ["Lewis", "Michael L.", ""], ["Krick", "Robert", ""], ["Carmean", "Douglas M.", ""], ["Qureshi", "Moinuddin K.", ""]]}, {"id": "1902.05067", "submitter": "Duggirala Ravi", "authors": "Duggirala Meher Krishna and Duggirala Ravi", "title": "Fast Parallel Integer Adder in Binary Representation", "comments": null, "journal-ref": "International Journal of Electronics Engineering Research, Vol 10,\n  No. 1, pp. 9--18, 2018 (ISSN 0975-6450)", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integer adder for integers in the binary representation is one of the\nbasic operations of any digital processor. For adding two integers of N bits\neach, the serial adder takes as many clock ticks. For achieving higher speeds,\nparallel circuits are discussed in the literature, and these circuits usually\noperate in two levels. At the lower level, integers represented by blocks of\nsmaller number of bits are added, and in a cascade of stages in the next level,\nthe carries produced in previous addition operations are summed to the augends.\nIn this paper, we describe a fast method and an improvement of it. The first\nattempt resembles the operation method of the merge sort algorithm, from which\nsome important properties of carries produced in each stage are analysed and\nassimilated, resulting in a parallel adder that runs in time comparable to the\nexisting methods. After that, the crucial insights are brought to fruition in\nan improved design, which takes 2 clock ticks to perform the addition operation\nrequiring only O(square(N)) space. The number of bits N is chosen usually to be\na positive integer power of 2. The speedup is achieved by special purpose\ncircuits for increment operations by i-th power of 2 , for i = 0, 1, ..., N-1,\neach operation taking only a single clock tick to complete. The usefulness of\nthis adder for multiplication operation is discussed. The standard\nmultiplication method utilizes quantizer and 3-bit to 2-bit consolidation\ncircuits to produce an integer that represents in binary the number of 1s in a\ncolumn corresponding to a place (weighted coefficient) of nonnegative integer\npower of 2. The last two consolidated integers are added by an adder in the\nend.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 06:04:43 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 14:54:00 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 13:14:39 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Krishna", "Duggirala Meher", ""], ["Ravi", "Duggirala", ""]]}, {"id": "1902.06468", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Minsoo Rhu", "title": "Beyond the Memory Wall: A Case for Memory-centric HPC System for Deep\n  Learning", "comments": "Published as a conference paper at the 51st IEEE/ACM International\n  Symposium on Microarchitecture (MICRO-51), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the models and the datasets to train deep learning (DL) models scale,\nsystem architects are faced with new challenges, one of which is the memory\ncapacity bottleneck, where the limited physical memory inside the accelerator\ndevice constrains the algorithm that can be studied. We propose a\nmemory-centric deep learning system that can transparently expand the memory\ncapacity available to the accelerators while also providing fast inter-device\ncommunication for parallel training. Our proposal aggregates a pool of memory\nmodules locally within the device-side interconnect, which are decoupled from\nthe host interface and function as a vehicle for transparent memory capacity\nexpansion. Compared to conventional systems, our proposal achieves an average\n2.8x speedup on eight DL applications and increases the system-wide memory\ncapacity to tens of TBs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 09:07:07 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kwon", "Youngeun", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1902.06655", "submitter": "Elisardo Antelo", "authors": "Elisardo Antelo", "title": "ENBB Processor: Towards the ExaScale Numerical Brain Box [Position\n  Paper]", "comments": "This paper describes an idea for a new processor that I wanted to\n  develop but I was not able to got support for this", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  ExaScale systems will be a key driver for simulations that are essential for\nadvance of science and economic growth. We aim to present a new concept of\nmicroprocessor for floating-point computations useful for being a basic\nbuilding block of ExaScale systems and beyond. The proposed microprocessor\narchitecture has a frontend for programming interface based on the concept of\nevent-driven simulation. The user program is executed as an event-driven\nsimulation using a hardware/software co-designed simulator. This is the\nflexible part of the system. The back-end exploits the concept of uniform\ntopology as in a brain: a massive packet switched interconnection network with\nflit credit-based flow control with virtual channels that incorporates\nseamlessly communication, arithmetic and storage. Floating-point computations\nare incorporated as on-line arithmetic operators in the output ports of the\nswitches as virtual arithmetic output channels, and storage as virtual input\nchannels. The front-end carries out the event-driven simulation of the user\nprogram, and uses the arithmetic network for the hard floating-point work by\nmeans of virtual dataflows. We expect to reduce significantly the needs of main\nmemory due to the execution model proposed, where variables are just virtual\ninterconnections in the network or signals stored in the virtual channels.\nMoreover, we have the hypothesis that the problem size assigned to a\nmicroprocessor should allow maximum concurrency and it should not be oversized.\nThis may lead to systems composed of microprocessors with main memory\nincorporated in 3D chips. We identified several challenges that a research to\ndevelop this microprocessor should address, and several hypothesis that should\nbe demonstrated by means of scientific evidence.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 17:21:04 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Antelo", "Elisardo", ""]]}, {"id": "1902.06742", "submitter": "Reza Faghih Mirzaee", "authors": "Aida Ghorbani Asibelagh, Reza Faghih Mirzaee", "title": "Applicability of Partial Ternary Full Adder in Ternary Arithmetic Units", "comments": "11 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper explores whether or not a complete ternary full adder, whose input\nvariables can independently be '0', '1', or '2', is indispensable in the\narithmetic blocks of adder, subtractor, and multiplier. Our investigations show\nthat none of the mentioned arithmetic units require a complete ternary full\nadder. Instead, they can be designed by use of partial ternary full adder,\nwhose input carry never becomes '2'. Furthermore, some new ternary compressors\nare proposed in this paper without the requirement of complete ternary full\nadder. The usage of partial ternary full adder can help circuit designers to\nsimplify their designs, especially in transistor level.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 20:34:42 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Asibelagh", "Aida Ghorbani", ""], ["Mirzaee", "Reza Faghih", ""]]}, {"id": "1902.06886", "submitter": "Jianlei Yang", "authors": "Xiaotao Jia, Jianlei Yang, Pengcheng Dai, Runze Liu, Yiran Chen,\n  Weisheng Zhao", "title": "SPINBIS: Spintronics based Bayesian Inference System with Stochastic\n  Computing", "comments": "14 pages, 26 figures, accepted by IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems", "journal-ref": null, "doi": "10.1109/TCAD.2019.2897631", "report-no": null, "categories": "cs.ET cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian inference is an effective approach for solving statistical learning\nproblems, especially with uncertainty and incompleteness. However, Bayesian\ninference is a computing-intensive task whose efficiency is physically limited\nby the bottlenecks of conventional computing platforms. In this work, a\nspintronics based stochastic computing approach is proposed for efficient\nBayesian inference. The inherent stochastic switching behaviors of spintronic\ndevices are exploited to build stochastic bitstream generator (SBG) for\nstochastic computing with hybrid CMOS/MTJ circuits design. Aiming to improve\nthe inference efficiency, an SBG sharing strategy is leveraged to reduce the\nrequired SBG array scale by integrating a switch network between SBG array and\nstochastic computing logic. A device-to-architecture level framework is\nproposed to evaluate the performance of spintronics based Bayesian inference\nsystem (SPINBIS). Experimental results on data fusion applications have shown\nthat SPINBIS could improve the energy efficiency about 12X than MTJ-based\napproach with 45% design area overhead and about 26X than FPGA-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 04:10:51 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Jia", "Xiaotao", ""], ["Yang", "Jianlei", ""], ["Dai", "Pengcheng", ""], ["Liu", "Runze", ""], ["Chen", "Yiran", ""], ["Zhao", "Weisheng", ""]]}, {"id": "1902.07609", "submitter": "Saugata Ghose", "authors": "Saugata Ghose, Tianshi Li, Nastaran Hajinazar, Damla Senol Cali, Onur\n  Mutlu", "title": "Understanding the Interactions of Workloads and DRAM Types: A\n  Comprehensive Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become increasingly difficult to understand the complex interaction\nbetween modern applications and main memory, composed of DRAM chips.\nManufacturers are now selling and proposing many different types of DRAM, with\neach DRAM type catering to different needs (e.g., high throughput, low power,\nhigh memory density). At the same time, the memory access patterns of prevalent\nand emerging workloads are rapidly diverging, as these applications manipulate\nlarger data sets in very different ways. As a result, the combined\nDRAM-workload behavior is often difficult to intuitively determine today, which\ncan hinder memory optimizations in both hardware and software.\n  In this work, we identify important families of workloads, as well as\nprevalent types of DRAM chips, and rigorously analyze the combined\nDRAM--workload behavior. To this end, we perform a comprehensive experimental\nstudy of the interaction between nine different DRAM types and 115 modern\napplications and multiprogrammed workloads. We draw 12 key observations from\nour characterization, enabled in part by our development of new metrics that\ntake into account contention between memory requests due to hardware design.\nNotably, we find that (1) newer DRAM types such as DDR4 and HMC often do not\noutperform older types such as DDR3, due to higher access latencies and, in the\ncase of HMC, poor exploitation of locality; (2) there is no single DRAM type\nthat can cater to all components of a heterogeneous system (e.g., GDDR5\nsignificantly outperforms other memories for multimedia acceleration, while HMC\nsignificantly outperforms other memories for network acceleration); and (3)\nthere is still a strong need to lower DRAM latency, but unfortunately the\ncurrent design trend of commodity DRAM is toward higher latencies to obtain\nother benefits. We hope that the trends we identify can drive optimizations in\nboth hardware and software design.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 15:52:01 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 18:09:09 GMT"}, {"version": "v3", "created": "Sat, 6 Jul 2019 18:21:14 GMT"}, {"version": "v4", "created": "Mon, 29 Jul 2019 15:51:38 GMT"}, {"version": "v5", "created": "Fri, 18 Oct 2019 13:34:25 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ghose", "Saugata", ""], ["Li", "Tianshi", ""], ["Hajinazar", "Nastaran", ""], ["Cali", "Damla Senol", ""], ["Mutlu", "Onur", ""]]}, {"id": "1902.07836", "submitter": "Igor Vernik", "authors": "A. F. Kirichenko, M. Y. Kamkar, J. Walter, and I. V. Vernik", "title": "ERSFQ 8-bit Parallel Binary Shifter for Energy-Efficient Superconducting\n  CPU", "comments": "4 pages, 6 figures, 22 references. Presented at Applied\n  Superconductivity Conference 2018 (ASC 2018), Oct. 28 - Nov. 2, 2018,\n  Seattle, WA, USA. Paper 1EOr1C-07", "journal-ref": "IEEE Trans. Appl. Supercon., 29(5), 1302704, Aug. 2019", "doi": "10.1109/TASC.2019.2904490", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed and tested a parallel 8-bit ERSFQ binary shifter that is one\nof the essential circuits in the design of the energy-efficient superconducting\nCPU. The binary shifter performs a bi-directional SHIFT instruction of an 8-bit\nargument. It consists of a bi-direction triple-port shift register controlled\nby two (left and right) shift pulse generators asynchronously generating a set\nnumber of shift pulses. At first clock cycle, an 8-bit word is loaded into the\nbinary shifter and a 3-bit shift argument is loaded into the desired\nshift-pulse generator. Next, the generator produces the required number of\nshift SFQ pulses (from 0 to 7) asynchronously, with a repetition rate set by\nthe internal generator delay of ~ 30 ps. These SFQ pulses are applied to the\nleft (positive) or the right (negative) input of the binary shifter. Finally,\nafter the shift operation is completed, the resulting 8-bit word goes to the\nparallel output. The complete 8-bit ERSFQ binary shifter, consisting of 820\nJosephson junctions, was simulated and optimized using PSCAN2. It was\nfabricated in MIT Lincoln Lab 10-kA/cm2 SFQ5ee fabrication process with a\nhigh-kinetic inductance layer. We have successfully tested the binary shifter\nat both the LSB-to-MSB and MSB-to-LSB propagation regimes for all eight shift\narguments. A single shift operation on a single input word demonstrated\noperational margins of +/-16% of the dc bias current. The correct functionality\nof the 8-bit ERSFQ binary shifter with the large, exhaustive data pattern was\nobserved within +/-10% margins of the dc bias current. In this paper, we\ndescribe the design and present the test results for the ERSFQ 8-bit parallel\nbinary shifter.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:01:46 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 19:22:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kirichenko", "A. F.", ""], ["Kamkar", "M. Y.", ""], ["Walter", "J.", ""], ["Vernik", "I. V.", ""]]}, {"id": "1902.09500", "submitter": "Igor Vernik", "authors": "A. F. Kirichenko, I. V. Vernik, M. Y. Kamkar, J. Walter, M. Miller, L.\n  R. Albu, and O. A. Mukhanov", "title": "ERSFQ 8-bit Parallel Arithmetic Logic Unit", "comments": "7 pages, 10 figures, 2 tables, 41 references. Presented at Applied\n  Superconductivity Conference 2018 (ASC 2018), Oct. 28 - Nov. 2, 2018,\n  Seattle, WA, USA. Paper 1EOr1C-06", "journal-ref": "IEEE Trans. Appl. Supercon., 29(5), 1302407, Aug. 2019", "doi": "10.1109/TASC.2019.2904484", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed and tested a parallel 8-bit ERSFQ arithmetic logic unit\n(ALU). The ALU design employs wave-pipelined instruction execution and features\nmodular bit-slice architecture that is easily extendable to any number of bits\nand adaptable to current recycling. A carry signal synchronized with an\nasynchronous instruction propagation provides the wave-pipeline operation of\nthe ALU. The ALU instruction set consists of 14 arithmetical and logical\ninstructions. It has been designed and simulated for operation up to a 10 GHz\nclock rate at the 10-kA/cm2 fabrication process. The ALU is embedded into a\nshift-register-based high-frequency testbed with on-chip clock generator to\nallow for comprehensive high frequency testing for all possible operands. The\n8-bit ERSFQ ALU, comprising 6840 Josephson junctions, has been fabricated with\nMIT Lincoln Lab 10-kA/cm2 SFQ5ee fabrication process featuring eight Nb wiring\nlayers and a high-kinetic inductance layer needed for ERSFQ technology. We\nevaluated the bias margins for all instructions and various operands at both\nlow and high frequency clock. At low frequency, clock and all instruction\npropagation through ALU were observed with bias margins of +/-11% and +/-9%,\nrespectively. Also at low speed, the ALU exhibited correct functionality for\nall arithmetical and logical instructions with +/-6% bias margins. We tested\nthe 8-bit ALU for all instructions up to 2.8 GHz clock frequency.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:01:32 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 19:22:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kirichenko", "A. F.", ""], ["Vernik", "I. V.", ""], ["Kamkar", "M. Y.", ""], ["Walter", "J.", ""], ["Miller", "M.", ""], ["Albu", "L. R.", ""], ["Mukhanov", "O. A.", ""]]}, {"id": "1902.10222", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad\n  Shafique", "title": "ROMANet: Fine-Grained Reuse-Driven Off-Chip Memory Access Management and\n  Data Organization for Deep Neural Network Accelerators", "comments": "Submitted to the IEEE-TVLSI journal, 14 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling high energy efficiency is crucial for embedded implementations of\ndeep learning. Several studies have shown that the DRAM-based off-chip memory\naccesses are one of the most energy-consuming operations in deep neural network\n(DNN) accelerators, and thereby limit the designs from achieving efficiency\ngains at the full potential. DRAM access energy varies depending upon the\nnumber of accesses required as well as the energy consumed per-access.\nTherefore, searching for a solution towards the minimum DRAM access energy is\nan important optimization problem. Towards this, we propose the ROMANet\nmethodology that aims at reducing the number of memory accesses, by searching\nfor the appropriate data partitioning and scheduling for each layer of a\nnetwork using a design space exploration, based on the knowledge of the\navailable on-chip memory and the data reuse factors. Moreover, ROMANet also\ntargets decreasing the number of DRAM row buffer conflicts and misses, by\nexploiting the DRAM multi-bank burst feature to improve the energy-per-access.\nBesides providing the energy benefits, our proposed DRAM data mapping also\nresults in an increased effective DRAM throughput, which is useful for\nlatency-constraint scenarios. Our experimental results show that the ROMANet\nsaves DRAM access energy by 12% for the AlexNet, by 36% for the VGG-16, and by\n46% for the MobileNet, while also improving the DRAM throughput by 10%, as\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:04:37 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 11:40:06 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1902.11128", "submitter": "Chuteng Zhou", "authors": "Paul N. Whatmough, Chuteng Zhou, Patrick Hansen, Shreyas Kolala\n  Venkataramanaiah, Jae-sun Seo, Matthew Mattina", "title": "FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer\n  Learning", "comments": "10 pages, 8 figures, paper accepted at SysML2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational demands of computer vision tasks based on state-of-the-art\nConvolutional Neural Network (CNN) image classification far exceed the energy\nbudgets of mobile devices. This paper proposes FixyNN, which consists of a\nfixed-weight feature extractor that generates ubiquitous CNN features, and a\nconventional programmable CNN accelerator which processes a dataset-specific\nCNN. Image classification models for FixyNN are trained end-to-end via transfer\nlearning, with the common feature extractor representing the transfered part,\nand the programmable part being learnt on the target dataset. Experimental\nresults demonstrate FixyNN hardware can achieve very high energy efficiencies\nup to 26.6 TOPS/W ($4.81 \\times$ better than iso-area programmable\naccelerator). Over a suite of six datasets we trained models via transfer\nlearning with an accuracy loss of $<1\\%$ resulting in up to 11.2 TOPS/W -\nnearly $2 \\times$ more efficient than a conventional programmable CNN\naccelerator of the same area.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 02:42:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Whatmough", "Paul N.", ""], ["Zhou", "Chuteng", ""], ["Hansen", "Patrick", ""], ["Venkataramanaiah", "Shreyas Kolala", ""], ["Seo", "Jae-sun", ""], ["Mattina", "Matthew", ""]]}]