[{"id": "1703.00073", "submitter": "Tetsufumi Tanamoto", "authors": "Tetsufumi Tanamoto, Satoshi Takaya, Nobuaki Sakamoto, Hirotsugu Kasho,\n  Shinichi Yasuda, Takao Marukame, Shinobu Fujita, and Yuichiro Mitani", "title": "Physically unclonable function using initial waveform of ring\n  oscillators on 65 nm CMOS technology", "comments": "5 pages, 9 figures", "journal-ref": "Jpn. J. Appl. Phys. 56, 04CF13 (2017)", "doi": "10.7567/JJAP.56.04CF13", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A silicon physically unclonable function (PUF) using ring oscillators (ROs)\nhas the advantage of easy application in both an application specific\nintegrated circuit (ASIC) and a field-programmable gate array (FPGA). Here, we\nprovide a RO-PUF using the initial waveform of the ROs based on 65 nm CMOS\ntechnology. Compared with the conventional RO-PUF, the number of ROs is greatly\nreduced and the time needed to generate an ID is within a couple of system\nclocks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 16:38:17 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Tanamoto", "Tetsufumi", ""], ["Takaya", "Satoshi", ""], ["Sakamoto", "Nobuaki", ""], ["Kasho", "Hirotsugu", ""], ["Yasuda", "Shinichi", ""], ["Marukame", "Takao", ""], ["Fujita", "Shinobu", ""], ["Mitani", "Yuichiro", ""]]}, {"id": "1703.00897", "submitter": "Gene Cooperman", "authors": "Rohan Garg and Kapil Arya and Jiajun Cao and Gene Cooperman and Jeff\n  Evans and Ankit Garg and Neil A. Rosenberg and K. Suresh", "title": "Adapting the DMTCP Plugin Model for Checkpointing of Hardware Emulation", "comments": "5 pages, 11 figure, 1 listing; SELSE '17, March 21--22, 2017, Boston,\n  MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Checkpoint-restart is now a mature technology. It allows a user to save and\nlater restore the state of a running process. The new plugin model for the\nupcoming version 3.0 of DMTCP (Distributed MultiThreaded Checkpointing) is\ndescribed here. This plugin model allows a target application to disconnect\nfrom the hardware emulator at checkpoint time and then re-connect to a possibly\ndifferent hardware emulator at the time of restart. The DMTCP plugin model is\nimportant in allowing three distinct parties to seamlessly inter-operate. The\nthree parties are: the EDA designer, who is concerned with formal verification\nof a circuit design; the DMTCP developers, who are concerned with providing\ntransparent checkpointing during the circuit emulation; and the hardware\nemulator vendor, who provides a plugin library that responds to checkpoint,\nrestart, and other events.\n  The new plugin model is an example of process-level virtualization:\nvirtualization of external abstractions from within a process. This capability\nis motivated by scenarios for testing circuit models with the help of a\nhardware emulator. The plugin model enables a three-way collaboration: allowing\na circuit designer and emulator vendor to each contribute separate proprietary\nplugins while sharing an open source software framework from the DMTCP\ndevelopers. This provides a more flexible platform, where different fault\ninjection models based on plugins can be designed within the DMTCP\ncheckpointing framework. After initialization, one restarts from a checkpointed\nstate under the control of the desired plugin. This restart saves the time\nspent in simulating the initialization phase, while enabling fault injection\nexactly at the region of interest. Upon restart, one can inject faults or\notherwise modify the remainder of the simulation. The work concludes with a\nbrief survey of checkpointing and process-level virtualization.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 18:52:45 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Garg", "Rohan", ""], ["Arya", "Kapil", ""], ["Cao", "Jiajun", ""], ["Cooperman", "Gene", ""], ["Evans", "Jeff", ""], ["Garg", "Ankit", ""], ["Rosenberg", "Neil A.", ""], ["Suresh", "K.", ""]]}, {"id": "1703.01457", "submitter": "Dajiang Zhou", "authors": "Shihao Wang, Dajiang Zhou, Xushen Han, Takeshi Yoshimura", "title": "Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating\n  Deep Convolutional Neural Networks", "comments": "DATE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have shown their good performances\nin many computer vision tasks. However, the high computational complexity of\nCNN involves a huge amount of data movements between the computational\nprocessor core and memory hierarchy which occupies the major of the power\nconsumption. This paper presents Chain-NN, a novel energy-efficient 1D chain\narchitecture for accelerating deep CNNs. Chain-NN consists of the dedicated\ndual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D\nsystolic primitives composed of a group of adjacent PEs. These systolic\nprimitives, together with the proposed column-wise scan input pattern, can\nfully reuse input operand to reduce the memory bandwidth requirement for energy\nsaving. Moreover, the 1D chain architecture allows the systolic primitives to\nbe easily reconfigured according to specific CNN parameters with fewer design\ncomplexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It\ncosts 3751k logic gates and 352KB on-chip memory. The results show a 576-PE\nChain-NN can be scaled up to 700MHz. This achieves a peak throughput of\n806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers\nin AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at\nleast 2.5 to 4.1x times better than the state-of-the-art works.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 14:14:14 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Wang", "Shihao", ""], ["Zhou", "Dajiang", ""], ["Han", "Xushen", ""], ["Yoshimura", "Takeshi", ""]]}, {"id": "1703.05769", "submitter": "Reza Ghanaatian", "authors": "Reza Ghanaatian, Alexios Balatsoukas-Stimming, Christoph Muller,\n  Michael Meidlinger, Gerald Matz, Adam Teman, and Andreas Burg", "title": "A 588 Gbps LDPC Decoder Based on Finite-Alphabet Message Passing", "comments": "Published in IEEE Transactions on Very Large Scale Integration\n  (TVLSI) Systems, Nov. 2017", "journal-ref": null, "doi": "10.1109/TVLSI.2017.2766925", "report-no": null, "categories": "cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ultra-high throughput low-density parity check (LDPC) decoder with an\nunrolled full-parallel architecture is proposed, which achieves the highest\ndecoding throughput compared to previously reported LDPC decoders in the\nliterature. The decoder benefits from a serial message-transfer approach\nbetween the decoding stages to alleviate the well-known routing congestion\nproblem in parallel LDPC decoders. Furthermore, a finite-alphabet message\npassing algorithm is employed to replace the variable node update rule of the\nstandard min-sum decoder with look-up tables, which are designed in a way that\nmaximizes the mutual information between decoding messages. The proposed\nalgorithm results in an architecture with reduced bit-width messages, leading\nto a significantly higher decoding throughput and to a lower area as compared\nto a min-sum decoder when serial message-transfer is used. The architecture is\nplaced and routed for the standard min-sum reference decoder and for the\nproposed finite-alphabet decoder using a custom pseudo-hierarchical backend\ndesign strategy to further alleviate routing congestions and to handle the\nlarge design. Post-layout results show that the finite-alphabet decoder with\nthe serial message-transfer architecture achieves a throughput as large as 588\nGbps with an area of 16.2 mm$^2$ and dissipates an average power of 22.7 pJ per\ndecoded bit in a 28 nm FD-SOI library. Compared to the reference min-sum\ndecoder, this corresponds to 3.1 times smaller area and 2 times better energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 18:01:55 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 20:34:46 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Ghanaatian", "Reza", ""], ["Balatsoukas-Stimming", "Alexios", ""], ["Muller", "Christoph", ""], ["Meidlinger", "Michael", ""], ["Matz", "Gerald", ""], ["Teman", "Adam", ""], ["Burg", "Andreas", ""]]}, {"id": "1703.06320", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa and Marina Chicheva", "title": "Hardware-Efficient Schemes of Quaternion Multiplying Units for 2D\n  Discrete Quaternion Fourier Transform Processors", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we offer and discuss three efficient structural solutions for\nthe hardware-oriented implementation of discrete quaternion Fourier transform\nbasic operations with reduced implementation complexities. The first solution:\na scheme for calculating sq product, the second solution: a scheme for\ncalculating qt product, and the third solution: a scheme for calculating sqt\nproduct, where s is a so-called i-quaternion, t is an j-quaternion, and q is an\nusual quaternion. The direct multiplication of two usual quaternions requires\n16 real multiplications (or two-operand multipliers in the case of fully\nparallel hardware implementation) and 12 real additions (or binary adders). At\nthe same time, our solutions allow to design the computation units, which\nconsume only 6 multipliers plus 6 two input adders for implementation of sq or\nqt basic operations and 9 binary multipliers plus 6 two-input adders and 4\nfour-input adders for implementation of sqt basic operation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 17:21:14 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""], ["Chicheva", "Marina", ""]]}, {"id": "1703.06571", "submitter": "EPTCS", "authors": "Reto Achermann (Systems Group, Department of Computer Science, ETH\n  Zurich), Lukas Humbel (Systems Group, Department of Computer Science, ETH\n  Zurich), David Cock (Systems Group, Department of Computer Science, ETH\n  Zurich), Timothy Roscoe (Systems Group, Department of Computer Science, ETH\n  Zurich)", "title": "Formalizing Memory Accesses and Interrupts", "comments": "In Proceedings MARS 2017, arXiv:1703.05812", "journal-ref": "EPTCS 244, 2017, pp. 66-116", "doi": "10.4204/EPTCS.244.4", "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardware/software boundary in modern heterogeneous multicore computers is\nincreasingly complex, and diverse across different platforms. A single memory\naccess by a core or DMA engine traverses multiple hardware translation and\ncaching steps, and the destination memory cell or register often appears at\ndifferent physical addresses for different cores. Interrupts pass through a\ncomplex topology of interrupt controllers and remappers before delivery to one\nor more cores, each with specific constraints on their configurations. System\nsoftware must not only correctly understand the specific hardware at hand, but\nalso configure it appropriately at runtime. We propose a formal model of\naddress spaces and resources in a system that allows us to express and verify\ninvariants of the system's runtime configuration, and illustrate (and motivate)\nit with several real platforms we have encountered in the process of OS\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 02:47:57 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Achermann", "Reto", "", "Systems Group, Department of Computer Science, ETH\n  Zurich"], ["Humbel", "Lukas", "", "Systems Group, Department of Computer Science, ETH\n  Zurich"], ["Cock", "David", "", "Systems Group, Department of Computer Science, ETH\n  Zurich"], ["Roscoe", "Timothy", "", "Systems Group, Department of Computer Science, ETH\n  Zurich"]]}, {"id": "1703.07348", "submitter": "Dajiang Zhou", "authors": "Xushen Han, Dajiang Zhou, Shihao Wang, and Shinji Kimura", "title": "CNN-MERP: An FPGA-Based Memory-Efficient Reconfigurable Processor for\n  Forward and Backward Propagation of Convolutional Neural Networks", "comments": null, "journal-ref": "ICCD 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale deep convolutional neural networks (CNNs) are widely used in\nmachine learning applications. While CNNs involve huge complexity, VLSI (ASIC\nand FPGA) chips that deliver high-density integration of computational\nresources are regarded as a promising platform for CNN's implementation. At\nmassive parallelism of computational units, however, the external memory\nbandwidth, which is constrained by the pin count of the VLSI chip, becomes the\nsystem bottleneck. Moreover, VLSI solutions are usually regarded as a lack of\nthe flexibility to be reconfigured for the various parameters of CNNs. This\npaper presents CNN-MERP to address these issues. CNN-MERP incorporates an\nefficient memory hierarchy that significantly reduces the bandwidth\nrequirements from multiple optimizations including on/off-chip data allocation,\ndata flow optimization and data reuse. The proposed 2-level reconfigurability\nis utilized to enable fast and efficient reconfiguration, which is based on the\ncontrol logic and the multiboot feature of FPGA. As a result, an external\nmemory bandwidth requirement of 1.94MB/GFlop is achieved, which is 55% lower\nthan prior arts. Under limited DRAM bandwidth, a system throughput of\n1244GFlop/s is achieved at the Vertex UltraScale platform, which is 5.48 times\nhigher than the state-of-the-art FPGA implementations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 01:31:23 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Han", "Xushen", ""], ["Zhou", "Dajiang", ""], ["Wang", "Shihao", ""], ["Kimura", "Shinji", ""]]}, {"id": "1703.07706", "submitter": "Zelalem Aweke", "authors": "Zelalem Birhanu Aweke, Todd Austin", "title": "Ozone: Efficient Execution with Zero Timing Leakage for Modern\n  Microarchitectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time variation during program execution can leak sensitive information. Time\nvariations due to program control flow and hardware resource contention have\nbeen used to steal encryption keys in cipher implementations such as AES and\nRSA. A number of approaches to mitigate timing-based side-channel attacks have\nbeen proposed including cache partitioning, control-flow obfuscation and\ninjecting timing noise into the outputs of code. While these techniques make\ntiming-based side-channel attacks more difficult, they do not eliminate the\nrisks. Prior techniques are either too specific or too expensive, and all leave\nremnants of the original timing side channel for later attackers to attempt to\nexploit.\n  In this work, we show that the state-of-the-art techniques in timing\nside-channel protection, which limit timing leakage but do not eliminate it,\nstill have significant vulnerabilities to timing-based side-channel attacks. To\nprovide a means for total protection from timing-based side-channel attacks, we\ndevelop Ozone, the first zero timing leakage execution resource for a modern\nmicroarchitecture. Code in Ozone execute under a special hardware thread that\ngains exclusive access to a single core's resources for a fixed (and limited)\nnumber of cycles during which it cannot be interrupted. Memory access under\nOzone thread execution is limited to a fixed size uncached scratchpad memory,\nand all Ozone threads begin execution with a known fixed microarchitectural\nstate. We evaluate Ozone using a number of security sensitive kernels that have\npreviously been targets of timing side-channel attacks, and show that Ozone\neliminates timing leakage with minimal performance overhead.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 19:53:26 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Aweke", "Zelalem Birhanu", ""], ["Austin", "Todd", ""]]}, {"id": "1703.07725", "submitter": "Lei Liu", "authors": "Lei Liu, Mengyao Xie and Hao Yang", "title": "Memos: Revisiting Hybrid Memory Management in Modern Operating System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging hybrid DRAM-NVM architecture is challenging the existing memory\nmanagement mechanism in operating system. In this paper, we introduce memos,\nwhich can schedule memory resources over the entire memory hierarchy including\ncache, channels, main memory comprising DRAM and NVM simultaneously. Powered by\nour newly designed kernel-level monitoring module and page migration engine,\nmemos can dynamically optimize the data placement at the memory hierarchy in\nterms of the on-line memory patterns, current resource utilization and feature\nof memory medium. Our experimental results show that memos can achieve high\nmemory utilization, contributing to system throughput by 19.1% and QoS by 23.6%\non average. Moreover, memos can reduce the NVM side memory latency by 3~83.3%,\nenergy consumption by 25.1~99%, and benefit the NVM lifetime significantly (40X\nimprovement on average).\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:07:11 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Liu", "Lei", ""], ["Xie", "Mengyao", ""], ["Yang", "Hao", ""]]}, {"id": "1703.10725", "submitter": "Wei-Che Wang", "authors": "Wei-Che Wang, Zhuoqi Li, Joseph Skudlarek, Mario Larouche, Michael\n  Chen and Puneet Gupta", "title": "UNBIAS PUF: A Physical Implementation Bias Agnostic Strong PUF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Physical Unclonable Function (PUF) is a promising hardware security\nprimitive because of its inherent uniqueness and low cost. To extract the\ndevice-specific variation from delay-based strong PUFs, complex routing\nconstraints are imposed to achieve symmetric path delays; and systematic\nvariations can severely compromise the uniqueness of the PUF. In addition, the\nmetastability of the arbiter circuit of an Arbiter PUF can also degrade the\nquality of the PUF due to the induced instability. In this paper we propose a\nnovel strong UNBIAS PUF that can be implemented purely by Register Transfer\nLanguage (RTL), such as verilog, without imposing any physical design\nconstraints or delay characterization effort to solve the aforementioned\nissues. Efficient inspection bit prediction models for unbiased response\nextraction are proposed and validated. Our experimental results of the strong\nUNBIAS PUF show 5.9% intra-Fractional Hamming Distance (FHD) and 45.1%\ninter-FHD on 7 Field Programmable Gate Array (FPGA) boards without applying any\nphysical layout constraints or additional XOR gates. The UNBIAS PUF is also\nscalable because no characterization cost is required for each challenge to\ncompensate the implementation bias. The averaged intra-FHD measured at worst\ntemperature and voltage variation conditions is 12%, which is still below the\nmargin of practical Error Correction Code (ECC) with error reduction techniques\nfor PUFs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 01:24:52 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Wang", "Wei-Che", ""], ["Li", "Zhuoqi", ""], ["Skudlarek", "Joseph", ""], ["Larouche", "Mario", ""], ["Chen", "Michael", ""], ["Gupta", "Puneet", ""]]}]