[{"id": "1604.01789", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Hasan Hassan, Hongyi Xin, O\\u{g}uz Ergin, Onur Mutlu,\n  and Can Alkan", "title": "GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment\n  in DNA Short Read Mapping", "comments": null, "journal-ref": "Bioinformatics. Nov 1;33(21):3355-3363, 2017", "doi": "10.1093/bioinformatics/btx342", "report-no": null, "categories": "q-bio.GN cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: High throughput DNA sequencing (HTS) technologies generate an\nexcessive number of small DNA segments -- called short reads -- that cause\nsignificant computational burden. To analyze the entire genome, each of the\nbillions of short reads must be mapped to a reference genome based on the\nsimilarity between a read and \"candidate\" locations in that reference genome.\nThe similarity measurement, called alignment, formulated as an approximate\nstring matching problem, is the computational bottleneck because: (1) it is\nimplemented using quadratic-time dynamic programming algorithms, and (2) the\nmajority of candidate locations in the reference genome do not align with a\ngiven read due to high dissimilarity. Calculating the alignment of such\nincorrect candidate locations consumes an overwhelming majority of a modern\nread mapper's execution time. Therefore, it is crucial to develop a fast and\neffective filter that can detect incorrect candidate locations and eliminate\nthem before invoking computationally costly alignment operations. Results: We\npropose GateKeeper, a new hardware accelerator that functions as a\npre-alignment step that quickly filters out most incorrect candidate locations.\nGateKeeper is the first design to accelerate pre-alignment using\nField-Programmable Gate Arrays (FPGAs), which can perform pre-alignment much\nfaster than software. GateKeeper can be integrated with any mapper that\nperforms sequence alignment for verification. When implemented on a single FPGA\nchip, GateKeeper maintains high accuracy (on average >96%) while providing up\nto 90-fold and 130-fold speedup over the state-of-the-art software\npre-alignment techniques, Adjacency Filter and Shifted Hamming Distance (SHD),\nrespectively. The addition of GateKeeper as a pre-alignment step can reduce the\nverification time of the mrFAST mapper by a factor of 10. Availability:\nhttps://github.com/BilkentCompGen/GateKeeper\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 20:04:56 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:57:38 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 00:31:25 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Alser", "Mohammed", ""], ["Hassan", "Hasan", ""], ["Xin", "Hongyi", ""], ["Ergin", "O\u011fuz", ""], ["Mutlu", "Onur", ""], ["Alkan", "Can", ""]]}, {"id": "1604.03062", "submitter": "Eric Cheng", "authors": "Eric Cheng, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\n  Hyungmin Cho, Kevin Skadron, Mircea R. Stan, Klas Lilja, Jacob A. Abraham,\n  Pradip Bose, Subhasish Mitra", "title": "CLEAR: Cross-Layer Exploration for Architecting Resilience - Combining\n  Hardware and Software Techniques to Tolerate Soft Errors in Processor Cores", "comments": "Extended version of paper published in Proceedings of the 53rd Annual\n  Design Automation Conference", "journal-ref": null, "doi": "10.1145/2897937.2897996", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first of its kind framework which overcomes a major challenge in\nthe design of digital systems that are resilient to reliability failures:\nachieve desired resilience targets at minimal costs (energy, power, execution\ntime, area) by combining resilience techniques across various layers of the\nsystem stack (circuit, logic, architecture, software, algorithm). This is also\nreferred to as cross-layer resilience. In this paper, we focus on\nradiation-induced soft errors in processor cores. We address both single-event\nupsets (SEUs) and single-event multiple upsets (SEMUs) in terrestrial\nenvironments. Our framework automatically and systematically explores the large\nspace of comprehensive resilience techniques and their combinations across\nvarious layers of the system stack (586 cross-layer combinations in this\npaper), derives cost-effective solutions that achieve resilience targets at\nminimal costs, and provides guidelines for the design of new resilience\ntechniques. We demonstrate the practicality and effectiveness of our framework\nusing two diverse designs: a simple, in-order processor core and a complex,\nout-of-order processor core. Our results demonstrate that a carefully optimized\ncombination of circuit-level hardening, logic-level parity checking, and\nmicro-architectural recovery provides a highly cost-effective soft error\nresilience solution for general-purpose processor cores. For example, a 50x\nimprovement in silent data corruption rate is achieved at only 2.1% energy cost\nfor an out-of-order core (6.1% for an in-order core) with no speed impact.\nHowever, selective circuit-level hardening alone, guided by a thorough analysis\nof the effects of soft errors on application benchmarks, provides a\ncost-effective soft error resilience solution as well (with ~1% additional\nenergy cost for a 50x improvement in silent data corruption rate).\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 18:44:27 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 23:28:33 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cheng", "Eric", ""], ["Mirkhani", "Shahrzad", ""], ["Szafaryn", "Lukasz G.", ""], ["Cher", "Chen-Yong", ""], ["Cho", "Hyungmin", ""], ["Skadron", "Kevin", ""], ["Stan", "Mircea R.", ""], ["Lilja", "Klas", ""], ["Abraham", "Jacob A.", ""], ["Bose", "Pradip", ""], ["Mitra", "Subhasish", ""]]}, {"id": "1604.04006", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, S Yamashita", "title": "Area/latency optimized early output asynchronous full adders and\n  relative-timed ripple carry adders", "comments": null, "journal-ref": "SpringerPlus, vol. 5:440, pages 26, April 2016", "doi": "10.1186/s40064-016-2074-z", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents two area/latency optimized gate level asynchronous full\nadder designs which correspond to early output logic. The proposed full adders\nare constructed using the delay-insensitive dual-rail code and adhere to the\nfour-phase return-to-zero handshaking. For an asynchronous ripple carry adder\n(RCA) constructed using the proposed early output full adders, the\nrelative-timing assumption becomes necessary and the inherent advantages of the\nrelative-timed RCA are: (1) computation with valid inputs, i.e., forward\nlatency is data-dependent, and (2) computation with spacer inputs involves a\nbare minimum constant reverse latency of just one full adder delay, thus\nresulting in the optimal cycle time. With respect to different 32-bit RCA\nimplementations, and in comparison with the optimized strong-indication,\nweak-indication, and early output full adder designs, one of the proposed early\noutput full adders achieves respective reductions in latency by 67.8, 12.3 and\n6.1 %, while the other proposed early output full adder achieves corresponding\nreductions in area by 32.6, 24.6 and 6.9 %, with practically no power penalty.\nFurther, the proposed early output full adders based asynchronous RCAs enable\nminimum reductions in cycle time by 83.4, 15, and 8.8 % when considering\ncarry-propagation over the entire RCA width of 32-bits, and maximum reductions\nin cycle time by 97.5, 27.4, and 22.4 % for the consideration of a typical\ncarry chain length of 4 full adder stages, when compared to the least of the\ncycle time estimates of various strong-indication, weak-indication, and early\noutput asynchronous RCAs of similar size. All the asynchronous full adders and\nRCAs were realized using standard cells in a semi-custom design fashion based\non a 32/28 nm CMOS process technology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 01:01:41 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Balasubramanian", "P", ""], ["Yamashita", "S", ""]]}, {"id": "1604.05897", "submitter": "Valentin Puente", "authors": "Valentin Puente, Jos\\'e \\'Angel Gregorio", "title": "CLAASIC: a Cortex-Inspired Hardware Accelerator", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores the feasibility of specialized hardware implementing the\nCortical Learning Algorithm (CLA) in order to fully exploit its inherent\nadvantages. This algorithm, which is inspired in the current understanding of\nthe mammalian neo-cortex, is the basis of the Hierarchical Temporal Memory\n(HTM). In contrast to other machine learning (ML) approaches, the structure is\nnot application dependent and relies on fully unsupervised continuous learning.\nWe hypothesize that a hardware implementation will be able not only to extend\nthe already practical uses of these ideas to broader scenarios but also to\nexploit the hardware-friendly CLA characteristics. The architecture proposed\nwill enable an unfeasible scalability for software solutions and will fully\ncapitalize on one of the many CLA advantages: low computational requirements\nand reduced storage utilization. Compared to a state-of-the-art CLA software\nimplementation it could be possible to improve by 4 orders of magnitude in\nperformance and up to 8 orders of magnitude in energy efficiency. We propose to\nuse a packet-switched network to tackle this. The paper addresses the\nfundamental issues of such an approach, proposing solutions to achieve scalable\nsolutions. We will analyze cost and performance when using well-known\narchitecture techniques and tools. The results obtained suggest that even with\nCMOS technology, under constrained cost, it might be possible to implement a\nlarge-scale system. We found that the proposed solutions enable a saving of 90%\nof the original communication costs running either synthetic or realistic\nworkloads.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 11:18:06 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 14:28:28 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Puente", "Valentin", ""], ["Gregorio", "Jos\u00e9 \u00c1ngel", ""]]}, {"id": "1604.08041", "submitter": "Donghyuk Lee", "authors": "Donghyuk Lee", "title": "Reducing DRAM Latency at Low Cost by Exploiting Heterogeneity", "comments": "159 pages, PhD thesis, CMU 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern systems, DRAM-based main memory is significantly slower than the\nprocessor. Consequently, processors spend a long time waiting to access data\nfrom main memory, making the long main memory access latency one of the most\ncritical bottlenecks to achieving high system performance. Unfortunately, the\nlatency of DRAM has remained almost constant in the past decade. This is mainly\nbecause DRAM has been optimized for cost-per-bit, rather than access latency.\nAs a result, DRAM latency is not reducing with technology scaling, and\ncontinues to be an important performance bottleneck in modern and future\nsystems.\n  This dissertation seeks to achieve low latency DRAM-based memory systems at\nlow cost in three major directions. First, based on the observation that long\nbitlines in DRAM are one of the dominant sources of DRAM latency, we propose a\nnew DRAM architecture, Tiered-Latency DRAM (TL-DRAM), which divides the long\nbitline into two shorter segments using an isolation transistor, allowing one\nsegment to be accessed with reduced latency. Second, we propose a fine-grained\nDRAM latency reduction mechanism, Adaptive-Latency DRAM, which optimizes DRAM\nlatency for the common operating conditions for individual DRAM module. Third,\nwe propose a new technique, Architectural-Variation-Aware DRAM (AVA-DRAM),\nwhich reduces DRAM latency at low cost, by profiling and identifying only the\ninherently slower regions in DRAM to dynamically determine the lowest latency\nDRAM can operate at without causing failures.\n  This dissertation provides a detailed analysis of DRAM latency by using both\ncircuit-level simulation with a detailed DRAM model and FPGA-based profiling of\nreal DRAM modules. Our latency analysis shows that our low latency DRAM\nmechanisms enable significant latency reductions, leading to large improvement\nin both system performance and energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 12:35:05 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lee", "Donghyuk", ""]]}, {"id": "1604.08484", "submitter": "Ahsan Javed Awan", "authors": "Ahsan Javed Awan, Mats Brorsson, Vladimir Vlassov and Eduard Ayguade", "title": "Architectural Impact on Performance of In-memory Data Analytics: Apache\n  Spark Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cluster computing frameworks are continuously evolving to provide\nreal-time data analysis capabilities, Apache Spark has managed to be at the\nforefront of big data analytics for being a unified framework for both, batch\nand stream data processing. However, recent studies on micro-architectural\ncharacterization of in-memory data analytics are limited to only batch\nprocessing workloads. We compare micro-architectural performance of batch\nprocessing and stream processing workloads in Apache Spark using hardware\nperformance counters on a dual socket server. In our evaluation experiments, we\nhave found that batch processing are stream processing workloads have similar\nmicro-architectural characteristics and are bounded by the latency of frequent\ndata access to DRAM. For data accesses we have found that simultaneous\nmulti-threading is effective in hiding the data latencies. We have also\nobserved that (i) data locality on NUMA nodes can improve the performance by\n10% on average and(ii) disabling next-line L1-D prefetchers can reduce the\nexecution time by up-to 14\\% and (iii) multiple small executors can provide\nup-to 36\\% speedup over single large executor.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 16:00:38 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Awan", "Ahsan Javed", ""], ["Brorsson", "Mats", ""], ["Vlassov", "Vladimir", ""], ["Ayguade", "Eduard", ""]]}]