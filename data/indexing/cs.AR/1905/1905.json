[{"id": "1905.02438", "submitter": "George Constantinides", "authors": "George A. Constantinides", "title": "Rethinking Arithmetic for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1098/rsta.2019.0051", "report-no": null, "categories": "cs.LG cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficiency in the implementation of deep neural networks.\nHardware accelerators are gaining interest as machine learning becomes one of\nthe drivers of high-performance computing. In these accelerators, the directed\ngraph describing a neural network can be implemented as a directed graph\ndescribing a Boolean circuit. We make this observation precise, leading\nnaturally to an understanding of practical neural networks as discrete\nfunctions, and show that so-called binarised neural networks are functionally\ncomplete. In general, our results suggest that it is valuable to consider\nBoolean circuits as neural networks, leading to the question of which circuit\ntopologies are promising. We argue that continuity is central to generalisation\nin learning, explore the interaction between data coding, network topology, and\nnode functionality for continuity, and pose some open questions for future\nresearch. As a first step to bridging the gap between continuous and Boolean\nviews of neural network accelerators, we present some recent results from our\nwork on LUTNet, a novel Field-Programmable Gate Array inference approach.\nFinally, we conclude with additional possible fruitful avenues for research\nbridging the continuous and discrete views of neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:36:48 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:00:24 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Constantinides", "George A.", ""]]}, {"id": "1905.02487", "submitter": "Zhangyu Chen", "authors": "Zhangyu Chen, Yu Hua, Pengfei Zuo, Yuanyuan Sun, Yuncheng Guo", "title": "Efficient Similarity-aware Compression to Reduce Bit-writes in\n  Non-Volatile Main Memory for Image-based Applications", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image bitmaps have been widely used in in-memory applications, which consume\nlots of storage space and energy. Compared with legacy DRAM, non-volatile\nmemories (NVMs) are suitable for bitmap storage due to the salient features in\ncapacity and power savings. However, NVMs suffer from higher latency and energy\nconsumption in writes compared with reads. Although compressing data in write\naccesses to NVMs on-the-fly reduces the bit-writes in NVMs, existing precise or\napproximate compression schemes show limited performance improvements for data\nof bitmaps, due to the irregular data patterns and variance in data. We observe\nthat the data containing bitmaps show the pixel-level similarity due to the\nanalogous contents in adjacent pixels. By exploiting the pixel-level\nsimilarity, we propose SimCom, an efficient similarity-aware compression scheme\nin hardware layer, to compress data for each write access on-the-fly. The idea\nbehind SimCom is to compress continuous similar words into the pairs of base\nwords with runs. With the aid of domain knowledge of images, SimCom adaptively\nselects an appropriate compression mode to achieve an efficient trade-off\nbetween image quality and memory performance. We implement SimCom on GEM5 with\nNVMain and evaluate the performance with real-world workloads. Our results\ndemonstrate that SimCom reduces 33.0%, 34.8% write latency and saves 28.3%,\n29.0% energy than state-of-the-art FPC and BDI with minor quality loss of 3%.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:57:29 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Chen", "Zhangyu", ""], ["Hua", "Yu", ""], ["Zuo", "Pengfei", ""], ["Sun", "Yuanyuan", ""], ["Guo", "Yuncheng", ""]]}, {"id": "1905.02871", "submitter": "Jianming Huang", "authors": "Jianming Huang, Yu Hua, Pengfei Zuo, Wen Zhou, Fangting Huang", "title": "SAWL:A Self-adaptive Wear-leveling NVM Scheme for High Performance\n  Storage Systems", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to meet the needs of high performance computing (HPC) in terms of\nlarge memory, high throughput and energy savings, the non-volatile memory (NVM)\nhas been widely studied due to its salient features of high density, near-zero\nstandby power, byte-addressable and non-volatile properties. In HPC systems,\nthe multi-level cell (MLC) technique is used to significantly increase device\ndensity and decrease the cost, which however leads to much weaker endurance\nthan the single-level cell (SLC) counterpart. Although wear-leveling techniques\ncan mitigate this weakness in MLC, the improvements upon MLC-based NVM become\nvery limited due to not achieving uniform write distribution before some cells\nare really worn out. To address this problem, our paper proposes a\nself-adaptive wear-leveling (SAWL) scheme for MLC-based NVM. The idea behind\nSAWL is to dynamically tune the wear-leveling granularities and balance the\nwrites across the cells of entire memory, thus achieving suitable tradeoff\nbetween the lifetime and cache hit rate. Moreover, to reduce the size of the\naddress-mapping table, SAWL maintains a few recently-accessed mappings in a\nsmall on-chip cache. Experimental results demonstrate that SAWL significantly\nimproves the NVM lifetime and the performance for HPC systems, compared with\nstate-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 02:18:40 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Huang", "Jianming", ""], ["Hua", "Yu", ""], ["Zuo", "Pengfei", ""], ["Zhou", "Wen", ""], ["Huang", "Fangting", ""]]}, {"id": "1905.04423", "submitter": "Lizhong Chen", "authors": "Ting-Ru Lin, Drew Penney, Massoud Pedram, Lizhong Chen", "title": "Optimizing Routerless Network-on-Chip Designs: An Innovative\n  Learning-Based Framework", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applied to architecture design presents a promising\nopportunity with broad applications. Recent deep reinforcement learning (DRL)\ntechniques, in particular, enable efficient exploration in vast design spaces\nwhere conventional design strategies may be inadequate. This paper proposes a\nnovel deep reinforcement framework, taking routerless networks-on-chip (NoC) as\nan evaluation case study. The new framework successfully resolves problems with\nprior design approaches being either unreliable due to random searches or\ninflexible due to severe design space restrictions. The framework learns\n(near-)optimal loop placement for routerless NoCs with various design\nconstraints. A deep neural network is developed using parallel threads that\nefficiently explore the immense routerless NoC design space with a Monte Carlo\nsearch tree. Experimental results show that, compared with conventional mesh,\nthe proposed deep reinforcement learning (DRL) routerless design achieves a\n3.25x increase in throughput, 1.6x reduction in packet latency, and 5x\nreduction in power. Compared with the state-of-the-art routerless NoC, DRL\nachieves a 1.47x increase in throughput, 1.18x reduction in packet latency, and\n1.14x reduction in average hop count albeit with slightly more power overhead.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 02:15:44 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Lin", "Ting-Ru", ""], ["Penney", "Drew", ""], ["Pedram", "Massoud", ""], ["Chen", "Lizhong", ""]]}, {"id": "1905.04890", "submitter": "Peng Gao", "authors": "Qi Ni, Fei Wang, Ziwei Zhao, Peng Gao", "title": "FPGA-based Binocular Image Feature Extraction and Matching System", "comments": "Accepted for the 4th International Conference on Multimedia Systems\n  and Signal Processing (ICMSSP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image feature extraction and matching is a fundamental but computation\nintensive task in machine vision. This paper proposes a novel FPGA-based\nembedded system to accelerate feature extraction and matching. It implements\nSURF feature point detection and BRIEF feature descriptor construction and\nmatching. For binocular stereo vision, feature matching includes both tracking\nmatching and stereo matching, which simultaneously provide feature point\ncorrespondences and parallax information. Our system is evaluated on a ZYNQ\nXC7Z045 FPGA. The result demonstrates that it can process binocular video data\nat a high frame rate (640$\\times$480 @ 162fps). Moreover, an extensive test\nproves our system has robustness for image compression, blurring and\nillumination.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:31:26 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 01:03:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ni", "Qi", ""], ["Wang", "Fei", ""], ["Zhao", "Ziwei", ""], ["Gao", "Peng", ""]]}, {"id": "1905.05359", "submitter": "Tong Geng", "authors": "Chen Yang, Tong Geng, Tianqi Wang, Rushi Patel, Qingqing Xiong, Ahmed\n  Sanaullah, Jiayi Sheng, Charles Lin, Vipin Sachdeva, Woody Sherman, Martin C.\n  Herbordt", "title": "Fully Integrated On-FPGA Molecular Dynamics Simulations", "comments": "13 pages, 17 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of Molecular Dynamics (MD) on FPGAs has received\nsubstantial attention. Previous work, however, has consisted of either\nproof-of-concept implementations of components, usually the range-limited\nforce; full systems, but with much of the work shared by the host CPU; or\nprototype demonstrations, e.g., using OpenCL, that neither implement a whole\nsystem nor have competitive performance. In this paper, we present what we\nbelieve to be the first full-scale FPGA-based simulation engine, and show that\nits performance is competitive with a GPU (running Amber in an industrial\nproduction environment). The system features on-chip particle data storage and\nmanagement, short- and long-range force evaluation, as well as bonded forces,\nmotion update, and particle migration. Other contributions of this work include\nexploring numerous architectural trade-offs and analysis on various mappings\nschemes among particles/cells and the various on-chip compute units. The\npotential impact is that this system promises to be the basis for long\ntimescale Molecular Dynamics with a commodity cluster.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:29:27 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Yang", "Chen", ""], ["Geng", "Tong", ""], ["Wang", "Tianqi", ""], ["Patel", "Rushi", ""], ["Xiong", "Qingqing", ""], ["Sanaullah", "Ahmed", ""], ["Sheng", "Jiayi", ""], ["Lin", "Charles", ""], ["Sachdeva", "Vipin", ""], ["Sherman", "Woody", ""], ["Herbordt", "Martin C.", ""]]}, {"id": "1905.05904", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, D L Maskell", "title": "Indicating Asynchronous Array Multipliers", "comments": "arXiv admin note: text overlap with arXiv:1903.09433", "journal-ref": "International Journal of Circuits, Systems and Signal Processing,\n  vol. 13, pp. 464-471, 2019", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is an important arithmetic operation that is frequently\nencountered in microprocessing and digital signal processing applications, and\nmultiplication is physically realized using a multiplier. This paper discusses\nthe physical implementation of many indicating asynchronous array multipliers,\nwhich are inherently elastic and modular and are robust to timing, process and\nparametric variations. We consider the physical realization of many indicating\nasynchronous array multipliers using a 32/28nm CMOS technology. The\nweak-indication array multipliers comprise strong-indication or weak-indication\nfull adders, and strong-indication 2-input AND functions to realize the partial\nproducts. The multipliers were synthesized in a semi-custom ASIC design style\nusing standard library cells including a custom-designed 2-input C-element. 4x4\nand 8x8 multiplication operations were considered for the physical\nimplementations. The 4-phase return-to-zero (RTZ) and the 4-phase return-to-one\n(RTO) handshake protocols were utilized for data communication, and the\ndelay-insensitive dual-rail code was used for data encoding. Among several\nweak-indication array multipliers, a weak-indication array multiplier utilizing\na biased weak-indication full adder and the strong-indication 2-input AND\nfunction is found to have reduced cycle time and power-cycle time product with\nrespect to RTZ and RTO handshaking for 4x4 and 8x8 multiplications. Further,\nthe 4-phase RTO handshaking is found to be preferable to the 4-phase RTZ\nhandshaking for achieving enhanced optimizations of the design metrics.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:19:39 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Balasubramanian", "P", ""], ["Maskell", "D L", ""]]}, {"id": "1905.06238", "submitter": "Jian Weng", "authors": "Jian Weng, Vidushi Dadu, Tony Nowatzki", "title": "Exploiting Fine-Grain Ordered Parallelism in Dense Matrix Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense linear algebra kernels are critical for wireless applications, and the\noncoming proliferation of 5G only amplifies their importance. Many such matrix\nalgorithms are inductive, and exhibit ample amounts of fine-grain ordered\nparallelism -- when multiple computations flow with fine-grain\nproducer/consumer dependences, and where the iteration domain is not easily\ntileable. Synchronization overheads make multi-core parallelism ineffective and\nthe non-tileable iterations make the vector-VLIW approach less effective,\nespecially for the typically modest-sized matrices. Because CPUs and DSPs lose\norder-of-magnitude performance/hardware utilization, costly and inflexible\nASICs are often employed in signal processing pipelines. A programmable\naccelerator with similar performance/power/area would be highly desirable. We\nfind that fine-grain ordered parallelism can be exploited by supporting: 1.\nfine-grain stream-based communication/synchronization; 2. inductive data-reuse\nand memory access patterns; 3. implicit vector-masking for partial vectors; 4.\nhardware specialization of dataflow criticality. In this work, we propose,\nREVEL, as a next-generation DSP architecture. It supports the above features in\nits ISA and microarchitecture, and further uses a novel vector-stream control\nparadigm to reduce control overheads. Across a suite of linear algebra kernels,\nREVEL outperforms equally provisioned DSPs by 4.6x-37x in latency and achieves\na performance per mm 2 of 8.3x. It is only 2.2x higher power to achieve the\nsame performance as ideal ASICs, at about 55% of the combined area.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 20:28:30 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Weng", "Jian", ""], ["Dadu", "Vidushi", ""], ["Nowatzki", "Tony", ""]]}, {"id": "1905.06314", "submitter": "Inaik Yoon", "authors": "Insik Yoon, Aqeel Anwar, Titash Rakshit, Arijit Raychowdhury", "title": "Transfer and Online Reinforcement Learning in STT-MRAM Based Embedded\n  Systems for Autonomous Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm-hardware codesign for camera-based\nautonomous flight in small drones. We show that the large write-latency and\nwrite-energy for nonvolatile memory (NVM) based embedded systems makes them\nunsuitable for real-time reinforcement learning (RL). We address this by\nperforming transfer learning (TL) on metaenvironments and RL on the last few\nlayers of a deep convolutional network. While the NVM stores the meta-model\nfrom TL, an on-die SRAM stores the weights of the last few layers. Thus all the\nreal-time updates via RL are carried out on the SRAM arrays. This provides us\nwith a practical platform with comparable performance as end-to-end RL and\n83.4% lower energy per image frame\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 00:18:09 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Yoon", "Insik", ""], ["Anwar", "Aqeel", ""], ["Rakshit", "Titash", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1905.06825", "submitter": "Xuan Guo", "authors": "Xuan Guo, Robert Mullins", "title": "Fast TLB Simulation for RISC-V Systems", "comments": "To be published in the Third Workshop on Computer Architecture\n  Research with RISC-V", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Address translation and protection play important roles in today's\nprocessors, supporting multiprocessing and enforcing security. Historically,\nthe design of the address translation mechanisms has been closely tied to the\ninstruction set. In contrast, RISC-V defines its privileged specification in a\nway that permits a variety of designs.\n  An important part of the design space is the organisation of Translation\nLookaside Buffers (TLBs). This paper presents our recent work on simulating TLB\nbehaviours in multi-core RISC-V systems. Our TLB simulation framework allows\nrapid, flexible and versatile prototyping of various hardware TLB design\nchoices, and enables validation, profiling and benchmarking of software running\non RISC-V systems. We show how this framework can be integrated with the\ndynamic binary translated emulator QEMU to perform online simulation. When\nsimulating complicated multi-level shared TLB designs, the framework runs at\naround 400 million instructions per second (MIPS) when simulating an 8-core\nsystem. The performance overhead compared to unmodified QEMU is only 18% when\nthe benchmark's L1 TLB miss rate is 1%.\n  We also demonstrate how this tool can be used to explore the instruction-set\nlevel design space. We test a shared last-level TLB design that is not\ncurrently permitted by the RISC-V's privileged specification. We then propose\nan extension to RISC-V's virtual memory system design based on these\nexperimental results.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 15:08:18 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Guo", "Xuan", ""], ["Mullins", "Robert", ""]]}, {"id": "1905.06844", "submitter": "Issam Damaj", "authors": "Safaa Kasbah (1), Ramzi Haraty (1), Issam Damaj (2) ((1) Lebanese\n  American University, (2) Dhofar University)", "title": "Reconfigurable Hardware Implementation of the Successive Overrelaxation\n  Method", "comments": "15 pages, 5 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1904.00629", "journal-ref": "Lec. Notes. in. Elec. Eng. Springer. 5(2008) 453-466", "doi": "10.1007/978-0-387-74905-1_32", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we study the feasibility of implementing SOR in\nreconfigurable hardware. We use Handel-C, a higher level design tool, to code\nour design, which is analyzed, synthesized, and placed and routed using the\nFPGAs proprietary software (DK Design Suite, Xilinx ISE 8.1i, and Quartus II\n5.1). We target Virtex II Pro, Altera Stratix, and Spartan3L, which is embedded\nin the RC10 FPGA-based system from Celoxica. We report our timing results when\ntargeting Virtex II Pro and compare them to software version results written in\nC++ and running on a general purpose processor (GPP).\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 12:18:34 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Kasbah", "Safaa", ""], ["Haraty", "Ramzi", ""], ["Damaj", "Issam", ""]]}, {"id": "1905.07511", "submitter": "Kyle Kuan", "authors": "Kyle Kuan and Tosiron Adegbija", "title": "HALLS: An Energy-Efficient Highly Adaptable Last Level STT-RAM Cache for\n  Multicore Systems", "comments": "To Appear on IEEE Transactions on Computers (TC)", "journal-ref": null, "doi": "10.1109/TC.2019.2918153", "report-no": null, "categories": "cs.AR cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spin-Transfer Torque RAM (STT-RAM) is widely considered a promising\nalternative to SRAM in the memory hierarchy due to STT-RAM's non-volatility,\nlow leakage power, high density, and fast read speed. The STT-RAM's small\nfeature size is particularly desirable for the last-level cache (LLC), which\ntypically consumes a large area of silicon die. However, long write latency and\nhigh write energy still remain challenges of implementing STT-RAMs in the CPU\ncache. An increasingly popular method for addressing this challenge involves\ntrading off the non-volatility for reduced write speed and write energy by\nrelaxing the STT-RAM's data retention time. However, in order to maximize\nenergy saving potential, the cache configurations, including STT-RAM's\nretention time, must be dynamically adapted to executing applications' variable\nmemory needs. In this paper, we propose a highly adaptable last level STT-RAM\ncache (HALLS) that allows the LLC configurations and retention time to be\nadapted to applications' runtime execution requirements. We also propose\nlow-overhead runtime tuning algorithms to dynamically determine the best\n(lowest energy) cache configurations and retention times for executing\napplications. Compared to prior work, HALLS reduced the average energy\nconsumption by 60.57% in a quad-core system, while introducing marginal latency\noverhead.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 00:42:54 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kuan", "Kyle", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "1905.08239", "submitter": "Jakub \\v{Z}\\'adn\\'ik", "authors": "Jakub \\v{Z}\\'adn\\'ik, Jarmo Takala", "title": "Low-power Programmable Processor for Fast Fourier Transform Based on\n  Transport Triggered Architecture", "comments": "5 pages, 4 figures, 1 table, ICASSP 2019 conference", "journal-ref": null, "doi": "10.1109/ICASSP.2019.8682289", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a low-power processor tailored for fast Fourier\ntransform computations where transport triggering template is exploited. The\nprocessor is software-programmable while retaining an energy-efficiency\ncomparable to existing fixed-function implementations. The power savings are\nachieved by compressing the computation kernel into one instruction word. The\nword is stored in an instruction loop buffer, which is more power-efficient\nthan regular instruction memory storage. The processor supports all\npower-of-two FFT sizes from 64 to 16384 and given 1 mJ of energy, it can\ncompute 20916 transforms of size 1024.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 21:56:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["\u017d\u00e1dn\u00edk", "Jakub", ""], ["Takala", "Jarmo", ""]]}, {"id": "1905.08624", "submitter": "Apollos Ezeogu C", "authors": "Apollos Ezeogu", "title": "Performance Analysis of 6T and 9T SRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SRAM cell is made up of latch, which ensures that the cell data is\npreserved as long as power is turned on and refresh operation is not required\nfor the SRAM cell. SRAM is widely used for on-chip cache memory in\nmicroprocessors, game software, computers, workstations, portable handheld\ndevices due to high data speed, low power consumption, low voltage supply,\nno-refresh needed. Therefore, to build a reliable cache/memory, the individual\ncell (SRAM) must be designed to have high Static Noise Margin (SNM). In\nsub-threshold region, conventional 6T-cell SRAM experiences poor read and write\nability, and reduction in the SNM at various fluctuation of the threshold\nvoltage, supply voltage down scaling, and technology scaling in nano-meter\nranges (180nm, 90nm, 45nm, 22nm, 16nm and 10nm). Thus, noise margin becomes\nworse during read and write operations compared to hold operation which the\ninternal feedback operates independent of the access transistors. Due to these\nlimitations of the conventional 6T SRAM cell, we have proposed a 9T SRAM that\nwill drastically minimize these limitations; the extra three transistors added\nto the 6T topology will improve the read, hold and write SNM. The design and\nsimulation results were carried out using Cadence Virtuoso to evaluate the\nperformance of 6T and 9T SRAM cells.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 07:53:51 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ezeogu", "Apollos", ""]]}, {"id": "1905.09822", "submitter": "Vivek Seshadri", "authors": "Vivek Seshadri and Onur Mutlu", "title": "In-DRAM Bulk Bitwise Execution Engine", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.06483,\n  arXiv:1610.09603, arXiv:1611.09988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications heavily use bitwise operations on large bitvectors as part\nof their computation. In existing systems, performing such bulk bitwise\noperations requires the processor to transfer a large amount of data on the\nmemory channel, thereby consuming high latency, memory bandwidth, and energy.\nIn this paper, we describe Ambit, a recently-proposed mechanism to perform bulk\nbitwise operations completely inside main memory. Ambit exploits the internal\norganization and analog operation of DRAM-based memory to achieve low cost,\nhigh performance, and low energy. Ambit exposes a new bulk bitwise execution\nmodel to the host processor. Evaluations show that Ambit significantly improves\nthe performance of several applications that use bulk bitwise operations,\nincluding databases.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 06:17:39 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 05:17:50 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 14:08:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Seshadri", "Vivek", ""], ["Mutlu", "Onur", ""]]}, {"id": "1905.10336", "submitter": "Rekha Singhal Dr.", "authors": "Rekha Singhal, Nathan Zhang, Luigi Nardi, Muhammad Shahbaz, Kunle\n  Olukotun", "title": "Polystore++: Accelerated Polystore System for Heterogeneous Workloads", "comments": "11 pages, Accepted in ICDCS 2019", "journal-ref": "ICDCS 2019", "doi": null, "report-no": null, "categories": "cs.AR cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern real-time business analytic consist of heterogeneous workloads (e.g,\ndatabase queries, graph processing, and machine learning). These analytic\napplications need programming environments that can capture all aspects of the\nconstituent workloads (including data models they work on and movement of data\nacross processing engines). Polystore systems suit such applications; however,\nthese systems currently execute on CPUs and the slowdown of Moore's Law means\nthey cannot meet the performance and efficiency requirements of modern\nworkloads. We envision Polystore++, an architecture to accelerate existing\npolystore systems using hardware accelerators (e.g, FPGAs, CGRAs, and GPUs).\nPolystore++ systems can achieve high performance at low power by identifying\nand offloading components of a polystore system that are amenable to\nacceleration using specialized hardware. Building a Polystore++ system is\nchallenging and introduces new research problems motivated by the use of\nhardware accelerators (e.g, optimizing and mapping query plans across\nheterogeneous computing units and exploiting hardware pipelining and\nparallelism to improve performance). In this paper, we discuss these challenges\nin detail and list possible approaches to address these problems.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:01:36 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Singhal", "Rekha", ""], ["Zhang", "Nathan", ""], ["Nardi", "Luigi", ""], ["Shahbaz", "Muhammad", ""], ["Olukotun", "Kunle", ""]]}, {"id": "1905.11231", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, D L Maskell, N E Mastorakis", "title": "Indicating Asynchronous Multipliers", "comments": "arXiv admin note: text overlap with arXiv:1903.09433 and\n  arXiv:1905.05904", "journal-ref": "Proceedings of 2nd European Conference on Electrical Engineering\n  and Computer Science, pp. 1-7, 2018, Switzerland", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication is a basic arithmetic operation that is encountered in almost\nall general-purpose microprocessing and digital signal processing applications,\nand multiplication is physically realized using a multiplier. This paper\ndiscusses the physical implementation of indicating asynchronous multipliers,\nwhich are inherently elastic and are robust to timing, process, and parametric\nvariations, and are modular. We consider the physical implementation of many\nweak-indication asynchronous multipliers using a 32/28-nm CMOS technology by\nadopting the array multiplier architecture. The multipliers are synthesized in\na semi-custom ASIC-design style. The 4-phase return-to-zero (RTZ) and the\n4-phase return-to-one (RTO) handshake protocols are considered for the data\ncommunication. The multipliers are realized using strong-indication or\nweak-indication full adders. Strong-indication 2-input AND function is used to\ngenerate the partial products in the case of both RTZ and RTO handshaking. The\nfull adders considered are derived from different indicating asynchronous logic\ndesign methods. Among the multipliers considered, a weak-indication\nasynchronous multiplier utilizing the biased weak-indication full adder is\nfound to be efficient in terms of the cycle time and the power-cycle time\nproduct with respect to both RTZ and RTO handshaking. Also, the 4-phase RTO\nhandshake protocol is found to be preferable than the 4-phase RTZ handshake\nprotocol for achieving enhanced optimizations in the design metrics.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:45:10 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Balasubramanian", "P", ""], ["Maskell", "D L", ""], ["Mastorakis", "N E", ""]]}, {"id": "1905.12701", "submitter": "Ahmad Moghimi", "authors": "Marina Minkin, Daniel Moghimi, Moritz Lipp, Michael Schwarz, Jo Van\n  Bulck, Daniel Genkin, Daniel Gruss, Frank Piessens, Berk Sunar, Yuval Yarom", "title": "Fallout: Reading Kernel Writes From User Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, out-of-order execution, an important performance optimization in\nmodern high-end processors, has been revealed to pose a significant security\nthreat, allowing information leaks across security domains. In particular, the\nMeltdown attack leaks information from the operating system kernel to user\nspace, completely eroding the security of the system. To address this and\nsimilar attacks, without incurring the performance costs of software\ncountermeasures, Intel includes hardware-based defenses in its recent Coffee\nLake R processors.\n  In this work, we show that the recent hardware defenses are not sufficient.\nSpecifically, we present Fallout, a new transient execution attack that leaks\ninformation from a previously unexplored microarchitectural component called\nthe store buffer. We show how unprivileged user processes can exploit Fallout\nto reconstruct privileged information recently written by the kernel. We\nfurther show how Fallout can be used to bypass kernel address space\nrandomization. Finally, we identify and explore microcode assists as a hitherto\nignored cause of transient execution.\n  Fallout affects all processor generations we have tested. However, we notice\na worrying regression, where the newer Coffee Lake R processors are more\nvulnerable to Fallout than older generations.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 20:02:55 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Minkin", "Marina", ""], ["Moghimi", "Daniel", ""], ["Lipp", "Moritz", ""], ["Schwarz", "Michael", ""], ["Van Bulck", "Jo", ""], ["Genkin", "Daniel", ""], ["Gruss", "Daniel", ""], ["Piessens", "Frank", ""], ["Sunar", "Berk", ""], ["Yarom", "Yuval", ""]]}, {"id": "1905.12812", "submitter": "Saraju Mohanty", "authors": "Saraju P. Mohanty and Elias Kougianos", "title": "iVAMS 1.0: Polynomial-Metamodel-Integrated Intelligent Verilog-AMS for\n  Fast, Accurate Mixed-Signal Design Optimization", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic circuit behavioral models built with hardware description/modeling\nlanguages such as Verilog-AMS for system-level simulations are typically\nfunctional models. They do not capture the physical design (layout) information\nof the target design. Numerous iterations of post-layout design adjustments are\nusually required to ensure that design specifications are met with the presence\nof layout parasitics. In this paper a paradigm shift of the current trend is\npresented that integrates layout-level information in Verilog-AMS through\nmetamodels such that system-level simulation of a mixed-signal circuit/system\nis realistic and as accurate as true parasitic netlist simulation. The\nsimulations performed with these parasitic-aware models can be used to estimate\nsystem performance without layout iterations. We call this new form of\nVerilog-AMS as iVAMS (i.e. Intelligent Verilog-AMS). We call this iVAMS 1.0 as\nit is simple polynomial-metamodel integrated Intelligent Verilog-AMS. As a\nspecific case study, a voltage-controlled oscillator (VCO) Verilog-AMS\nbehavioral model and design flow are proposed to assist fast PLL design space\nexploration. The PLL simulation employing quadratic metamodels achieves\napproximately 10X speedup compared to that employing the layout extracted,\nparasitic netlist. The simulations using this behavioral model attain high\naccuracy. The observed error for the simulated lock time and average power\ndissipation are 0.7% and 3%, respectively. This behavioral metamodel approach\nbridges the gap between layout-accurate but fast simulation and design space\nexploration. The proposed method also allows much shorter design verification\nand optimization to meet stringent time-to-market requirements. Compared to the\noptimization using the layout netlist, the runtime using the behavioral model\nis reduced by 88.9%.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 01:21:07 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mohanty", "Saraju P.", ""], ["Kougianos", "Elias", ""]]}, {"id": "1905.13503", "submitter": "Behnaz Pourmohseni", "authors": "Behnaz Pourmohseni, Fedor Smirnov, Stefan Wildermann, J\\\"urgen Teich", "title": "Isolation-Aware Timing Analysis and Design Space Exploration for\n  Predictable and Composable Many-Core Systems", "comments": "Final version published at the 31st Euromicro Conference on Real-Time\n  Systems (ECRTS 2019)", "journal-ref": null, "doi": "10.4230/LIPIcs.ECRTS.2019.12", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composable many-core systems enable the independent development and analysis\nof applications which will be executed on a shared platform where the mix of\nconcurrently executed applications may change dynamically at run time. For each\nindividual application, an off-line Design Space Exploration (DSE) is performed\nto compute several mapping alternatives on the platform, offering\nPareto-optimal trade-offs in terms of real-time guarantees, resource usage,\netc. At run time, one mapping is then chosen to launch the application on\ndemand. In this context, to enable an independent analysis of each individual\napplication at design time, so-called inter-application isolation schemes are\napplied which specify temporal or spatial isolation policies between\napplications. S.o.t.a. composable many-core systems are developed based on a\nfixed isolation scheme that is exclusively applied to every resource in every\nmapping of every application and use a timing analysis tailored to that\nisolation scheme to derive timing guarantees for each mapping. A fixed\nisolation scheme, however, heavily restricts the explored space of solutions\nand can, therefore, lead to suboptimality. Lifting this restriction\nnecessitates a timing analysis that is applicable to mappings with an arbitrary\nmix of isolation schemes on different resources. To address this issue, we\npresent an isolation-aware timing analysis that unlike existing analyses can\nhandle multiple isolation schemes in combination within one mapping and\ndelivers safe yet tight timing bounds by identifying and excluding interference\nscenarios that can never happen under the given combination of isolation\nschemes. Based on the timing analysis, we present a DSE which explores the\nchoices of isolation scheme per resource within each mapping. Experimental\nresults demonstrate the advantage of the proposed approach over approaches\nbased on a fixed isolation scheme.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:43:21 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 12:07:28 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 08:43:05 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 10:48:06 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pourmohseni", "Behnaz", ""], ["Smirnov", "Fedor", ""], ["Wildermann", "Stefan", ""], ["Teich", "J\u00fcrgen", ""]]}]