[{"id": "1810.00307", "submitter": "Sangkug Lym", "authors": "Sangkug Lym, Armand Behroozi, Wei Wen, Ge Li, Yongkee Kwon, Mattan\n  Erez", "title": "Mini-batch Serialization: CNN Training with Inter-layer Data Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Training convolutional neural networks (CNNs) requires intense computations\nand high memory bandwidth. We find that bandwidth today is over-provisioned\nbecause most memory accesses in CNN training can be eliminated by rearranging\ncomputation to better utilize on-chip buffers and avoid traffic resulting from\nlarge per-layer memory footprints. We introduce the MBS CNN training approach\nthat significantly reduces memory traffic by partially serializing mini-batch\nprocessing across groups of layers. This optimizes reuse within on-chip buffers\nand balances both intra-layer and inter-layer reuse. We also introduce the\nWaveCore CNN training accelerator that effectively trains CNNs in the MBS\napproach with high functional-unit utilization. Combined, WaveCore and MBS\nreduce DRAM traffic by 75%, improve performance by 53%, and save 26% system\nenergy for modern deep CNN training compared to conventional training\nmechanisms and accelerators.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 03:48:56 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 02:10:17 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 04:15:49 GMT"}, {"version": "v4", "created": "Sat, 4 May 2019 04:31:18 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Lym", "Sangkug", ""], ["Behroozi", "Armand", ""], ["Wen", "Wei", ""], ["Li", "Ge", ""], ["Kwon", "Yongkee", ""], ["Erez", "Mattan", ""]]}, {"id": "1810.00722", "submitter": "Daniel Ziener", "authors": "Thorbj\\\"orn Posewsky and Daniel Ziener", "title": "Throughput Optimizations for FPGA-based Deep Neural Network Inference", "comments": null, "journal-ref": "Microprocessors and Microsystems 60C (2018) pp. 151-161", "doi": "10.1016/j.micpro.2018.04.004", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks are an extremely successful and widely used technique\nfor various pattern recognition and machine learning tasks. Due to power and\nresource constraints, these computationally intensive networks are difficult to\nimplement in embedded systems. Yet, the number of applications that can benefit\nfrom the mentioned possibilities is rapidly rising. In this paper, we propose\nnovel architectures for the inference of previously learned and arbitrary deep\nneural networks on FPGA-based SoCs that are able to overcome these limitations.\nOur key contributions include the reuse of previously transferred weight\nmatrices across multiple input samples, which we refer to as batch processing,\nand the usage of compressed weight matrices, also known as pruning. An\nextensive evaluation of these optimizations is presented. Both techniques allow\na significant mitigation of data transfers and speed-up the network inference\nby one order of magnitude. At the same time, we surpass the data throughput of\nfully-featured x86-based systems while only using a fraction of their energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 17:08:22 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Posewsky", "Thorbj\u00f6rn", ""], ["Ziener", "Daniel", ""]]}, {"id": "1810.01115", "submitter": "P Balasubramanian", "authors": "P Balasubramanian", "title": "Performance Comparison of some Synchronous Adders", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note compares the performance of some synchronous adders which\ncorrespond to the following architectures: i) ripple carry adder (RCA), ii)\nrecursive carry lookahead adder (RCLA), iii) hybrid RCLA-RCA with the RCA used\nin the least significant adder bit positions, iv) block carry lookahead adder\n(BCLA), v) hybrid BCLA-RCA with the RCA used in the least significant adder bit\npositions, and vi) non-uniform input partitioned carry select adders (CSLAs)\nwithout and with the binary to excess-1 code (BEC) converter. The 32-bit\naddition was considered as an example operation. The adder architectures\nmentioned were implemented by targeting a typical case PVT specification (high\nthreshold voltage, supply voltage of 1.05V and operating temperature of 25\ndegrees Celsius) of the Synopsys 32/28nm CMOS technology. The comparison leads\nto the following observations: i) the hybrid CCLA-RCA is preferable to the\nother adders in terms of the speed, the power-delay product, and the\nenergy-delay product, ii) the non-uniform input partitioned CSLA without the\nBEC converter is preferable to the other adders in terms of the area-delay\nproduct, and iii) the RCA incorporating the full adder present in the standard\ndigital cell library is preferable to the other adders in terms of the\npower-delay-area product.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 08:34:26 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Balasubramanian", "P", ""]]}, {"id": "1810.01486", "submitter": "Gerhard Dueck", "authors": "Evandro C. Ferraz and Jeferson de Lima Muniz and Alexandre C. R. da\n  Silva and Gerhard W. Dueck", "title": "Synthesis of Majority Expressions through Primitive Function\n  Manipulation", "comments": "16 pages", "journal-ref": "13th International Workshop on Boolean Problems, September 19-21,\n  2018, Bremen, Germany, pp. 117-132", "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to technology advancements and circuits miniaturization, the study of\nlogic systems that can be applied to nanotechnology has been progressing\nsteadily. Among the creation of nanoeletronic circuits reversible and majority\nlogic stand out. This paper proposes the MPC (Majority Primitives Combination)\nalgorithm, used for majority logic synthesis. The algorithm receives a truth\ntable as input and returns a majority function that covers the same set of\nminterms. The formulation of a valid output function is made with the\ncombination of previously optimized functions. As cost criteria the algorithm\nsearches for a function with the least number of levels, followed by the least\nnumber of gates, inverters, and gate inputs. In this paper it's also presented\na comparison between the MPC and the exact_mig, currently considered the best\nalgorithm for majority synthesis. The exact_mig encode the exact synthesis of\nmajority functions using the number of levels and gates as cost criteria. The\nMPC considers two additional cost criteria, the number of inverters and the\nnumber of gate inputs, with the goal to further improve exact_mig results.\nTests have shown that both algorithms return optimal solutions for all\nfunctions with 3 input variables. For functions with 4 inputs, the MPC is able\nto further improve 42,987 (66%) functions and achieves equal results for 7,198\n(11%). For functions with 5 input variables, out of a sample of 1,000 randomly\ngenerated functions, the MPC further improved 477 (48%) functions and achieved\nequal results for 112 (11%).\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:07:53 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Ferraz", "Evandro C.", ""], ["Muniz", "Jeferson de Lima", ""], ["da Silva", "Alexandre C. R.", ""], ["Dueck", "Gerhard W.", ""]]}, {"id": "1810.02068", "submitter": "Cheng Fu", "authors": "Cheng Fu, Shilin Zhu, Hao Su, Ching-En Lee, Jishen Zhao", "title": "Towards Fast and Energy-Efficient Binarized Neural Network Inference on\n  FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN\nby using a single bit (-1/+1) for network parameters and intermediate\nrepresentations, which has greatly reduced the off-chip data transfer and\nstorage overhead. However, a large amount of computation redundancy still\nexists in BNN inference. By analyzing local properties of images and the\nlearned BNN kernel weights, we observe an average of $\\sim$78% input similarity\nand $\\sim$59% weight similarity among weight kernels, measured by our proposed\nmetric in common network architectures. Thus there does exist redundancy that\ncan be exploited to further reduce the amount of on-chip computations.\n  Motivated by the observation, in this paper, we proposed two types of fast\nand energy-efficient architectures for BNN inference. We also provide analysis\nand insights to pick the better strategy of these two for different datasets\nand network models. By reusing the results from previous computation, much\ncycles for data buffer access and computations can be skipped. By experiments,\nwe demonstrate that 80% of the computation and 40% of the buffer access can be\nskipped by exploiting BNN similarity. Thus, our design can achieve 17%\nreduction in total power consumption, 54% reduction in on-chip power\nconsumption and 2.4$\\times$ maximum speedup, compared to the baseline without\napplying our reuse technique. Our design also shows 1.9$\\times$ more\narea-efficiency compared to state-of-the-art BNN inference design. We believe\nour deployment of BNN on FPGA leads to a promising future of running deep\nlearning models on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 06:29:59 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Fu", "Cheng", ""], ["Zhu", "Shilin", ""], ["Su", "Hao", ""], ["Lee", "Ching-En", ""], ["Zhao", "Jishen", ""]]}, {"id": "1810.03979", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Extended Bit-Plane Compression for Convolutional Neural Network\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the tremendous success of convolutional neural networks in image\nclassification, object detection, speech recognition, etc., there is now rising\ndemand for deployment of these compute-intensive ML models on tightly power\nconstrained embedded and mobile systems at low cost as well as for pushing the\nthroughput in data centers. This has triggered a wave of research towards\nspecialized hardware accelerators. Their performance is often constrained by\nI/O bandwidth and the energy consumption is dominated by I/O transfers to\noff-chip memory. We introduce and evaluate a novel, hardware-friendly\ncompression scheme for the feature maps present within convolutional neural\nnetworks. We show that an average compression ratio of 4.4x relative to\nuncompressed data and a gain of 60% over existing method can be achieved for\nResNet-34 with a compression block requiring <300 bit of sequential cells and\nminimal combinational logic.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 21:02:53 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1810.04610", "submitter": "Andreas Abel", "authors": "Andreas Abel and Jan Reineke", "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of\n  Instructions on Intel Microarchitectures", "comments": null, "journal-ref": null, "doi": "10.1145/3297858.3304062", "report-no": null, "categories": "cs.PF cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern microarchitectures are some of the world's most complex man-made\nsystems. As a consequence, it is increasingly difficult to predict, explain,\nlet alone optimize the performance of software running on such\nmicroarchitectures. As a basis for performance predictions and optimizations,\nwe would need faithful models of their behavior, which are, unfortunately,\nseldom available.\n  In this paper, we present the design and implementation of a tool to\nconstruct faithful models of the latency, throughput, and port usage of x86\ninstructions. To this end, we first discuss common notions of instruction\nthroughput and port usage, and introduce a more precise definition of latency\nthat, in contrast to previous definitions, considers dependencies between\ndifferent pairs of input and output operands. We then develop novel algorithms\nto infer the latency, throughput, and port usage based on\nautomatically-generated microbenchmarks that are more accurate and precise than\nexisting work.\n  To facilitate the rapid construction of optimizing compilers and tools for\nperformance prediction, the output of our tool is provided in a\nmachine-readable format. We provide experimental results for processors of all\ngenerations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake,\nand discuss various cases where the output of our tool differs considerably\nfrom prior work.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:42:17 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 18:10:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Abel", "Andreas", ""], ["Reineke", "Jan", ""]]}, {"id": "1810.05670", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Ikuo Hoshi, Tomoyoshi Shimobaba, Takashi Kakue, Tomoyoshi Ito", "title": "Computational ghost imaging using a field-programmable gate array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational ghost imaging is a promising technique for single-pixel imaging\nbecause it is robust to disturbance and can be operated over broad wavelength\nbands, unlike common cameras. However, one disadvantage of this method is that\nit has a long calculation time for image reconstruction. In this paper, we have\ndesigned a dedicated calculation circuit that accelerated the process of\ncomputational ghost imaging. We implemented this circuit by using a\nfield-programmable gate array, which reduced the calculation time for the\ncircuit compared to a CPU. The dedicated circuit reconstructs images at a frame\nrate of 300 Hz.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 21:32:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Hoshi", "Ikuo", ""], ["Shimobaba", "Tomoyoshi", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1810.06472", "submitter": "Luc Jaulmes", "authors": "Luc Jaulmes, Miquel Moret\\'o, Mateo Valero, Marc Casas", "title": "Memory Vulnerability: A Case for Delaying Error Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To face future reliability challenges, it is necessary to quantify the risk\nof error in any part of a computing system. To this goal, the Architectural\nVulnerability Factor (AVF) has long been used for chips. However, this metric\nis used for offline characterisation, which is inappropriate for memory. We\nsurvey the literature and formalise one of the metrics used, the Memory\nVulnerability Factor, and extend it to take into account false errors. These\nare reported errors which would have no impact on the program if they were\nignored. We measure the False Error Aware MVF (FEA) and related metrics\nprecisely in a cycle-accurate simulator, and compare them with the effects of\ninjecting faults in a program's data, in native parallel runs. Our findings\nshow that MVF and FEA are the only two metrics that are safe to use at runtime,\nas they both consistently give an upper bound on the probability of incorrect\nprogram outcome. FEA gives a tighter bound than MVF, and is the metric that\ncorrelates best with the incorrect outcome probability of all considered\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 15:44:27 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Jaulmes", "Luc", ""], ["Moret\u00f3", "Miquel", ""], ["Valero", "Mateo", ""], ["Casas", "Marc", ""]]}, {"id": "1810.06807", "submitter": "Kartik Hegde", "authors": "Kartik Hegde, Rohit Agrawal, Yulun Yao, Christopher W. Fletcher", "title": "Morph: Flexible Acceleration for 3D CNN-based Video Understanding", "comments": "Appears in the proceedings of the 51st Annual IEEE/ACM International\n  Symposium on Microarchitecture (MICRO), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past several years have seen both an explosion in the use of\nConvolutional Neural Networks (CNNs) and the design of accelerators to make CNN\ninference practical. In the architecture community, the lion share of effort\nhas targeted CNN inference for image recognition. The closely related problem\nof video recognition has received far less attention as an accelerator target.\nThis is surprising, as video recognition is more computationally intensive than\nimage recognition, and video traffic is predicted to be the majority of\ninternet traffic in the coming years.\n  This paper fills the gap between algorithmic and hardware advances for video\nrecognition by providing a design space exploration and flexible architecture\nfor accelerating 3D Convolutional Neural Networks (3D CNNs) - the core kernel\nin modern video understanding. When compared to (2D) CNNs used for image\nrecognition, efficiently accelerating 3D CNNs poses a significant engineering\nchallenge due to their large (and variable over time) memory footprint and\nhigher dimensionality.\n  To address these challenges, we design a novel accelerator, called Morph,\nthat can adaptively support different spatial and temporal tiling strategies\ndepending on the needs of each layer of each target 3D CNN. We codesign a\nsoftware infrastructure alongside the Morph hardware to find good-fit\nparameters to control the hardware. Evaluated on state-of-the-art 3D CNNs,\nMorph achieves up to 3.4x (2.5x average) reduction in energy consumption and\nimproves performance/watt by up to 5.1x (4x average) compared to a baseline 3D\nCNN accelerator, with an area overhead of 5%. Morph further achieves a 15.9x\naverage energy reduction on 3D CNNs when compared to Eyeriss.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 04:49:15 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hegde", "Kartik", ""], ["Agrawal", "Rohit", ""], ["Yao", "Yulun", ""], ["Fletcher", "Christopher W.", ""]]}, {"id": "1810.06819", "submitter": "Takashi Morie", "authors": "Quan Wang, Hakaru Tamukoh, and Takashi Morie", "title": "A Time-domain Analog Weighted-sum Calculation Model for Extremely Low\n  Power VLSI Implementation of Multi-layer Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time-domain analog weighted-sum calculation model is proposed based on an\nintegrate-and-fire-type spiking neuron model. The proposed calculation model is\napplied to multi-layer feedforward networks, in which weighted summations with\npositive and negative weights are separately performed in each layer and\nsummation results are then fed into the next layers without their subtraction\noperation. We also propose very large-scale integrated (VLSI) circuits to\nimplement the proposed model. Unlike the conventional analog voltage or current\nmode circuits, the time-domain analog circuits use transient operation in\ncharging/discharging processes to capacitors. Since the circuits can be\ndesigned without operational amplifiers, they can operate with extremely low\npower consumption. However, they have to use very high resistance devices on\nthe order of G$\\rm \\Omega$. We designed a proof-of-concept (PoC) CMOS VLSI chip\nto verify weighted-sum operation with the same weights and evaluated it by\npost-layout circuit simulation using 250-nm fabrication technology. High\nresistance operation was realized by using the subthreshold operation region of\nMOS transistors. Simulation results showed that energy efficiency for the\nweighted-sum calculation was 290~TOPS/W, more than one order of magnitude\nhigher than that in state-of-the-art digital AI processors, even though the\nminimum width of interconnection used in the PoC chip was several times larger\nthan that in such digital processors. If state-of-the-art VLSI technology is\nused to implement the proposed model, an energy efficiency of more than\n1,000~TOPS/W will be possible. For practical applications, development of\nemerging analog memory devices such as ferroelectric-gate FETs is necessary.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 05:41:01 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Wang", "Quan", ""], ["Tamukoh", "Hakaru", ""], ["Morie", "Takashi", ""]]}, {"id": "1810.06885", "submitter": "Debesh Choudhury", "authors": "Atin Mukherjee, Debesh Choudhury", "title": "An Area Efficient 2D Fourier Transform Architecture for FPGA\n  Implementation", "comments": "7 pages, 8 figures, 6 tables", "journal-ref": "IEEE VLSI Circuits & Systems Letters, Volume 4, Issue 3, August\n  2018, pages 2-8", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional Fourier transform plays a significant role in a variety of\nimage processing problems, such as medical image processing, digital\nholography, correlation pattern recognition, hybrid digital optical processing,\noptical computing etc. 2D spatial Fourier transformation involves large number\nof image samples and hence it requires huge hardware resources of field\nprogrammable gate arrays (FPGA). In this paper, we present an area efficient\narchitecture of 2D FFT processor that reuses the butterfly units multiple\ntimes. This is achieved by using a control unit that sends back the previous\ncomputed data of N/2 butterfly units to itself for {log_2(N) - 1} times. A RAM\ncontroller is used to synchronize the flow of data samples between the\nfunctional blocks.The 2D FFT processor is simulated by VHDL and the results are\nverified on a Virtex-6 FPGA. The proposed method outperforms the conventional\nNxN point 2D FFT in terms of area which is reduced by a factor of log_N(2) with\nnegligible increase in computation time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:58:19 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Mukherjee", "Atin", ""], ["Choudhury", "Debesh", ""]]}, {"id": "1810.07059", "submitter": "Mohamed Hassan Dr.", "authors": "Mohamed Hassan", "title": "On the Off-chip Memory Latency of Real-Time Systems: Is DDR DRAM Really\n  the Best Option?", "comments": "Accepted in IEEE Real Time Systems Symposium (RTSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictable execution time upon accessing shared memories in multi-core\nreal-time systems is a stringent requirement. A plethora of existing works\nfocus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR\nDRAMs), or redesigning its memory to provide predictable memory behavior. In\nthis paper, we show that DDR DRAMs by construction suffer inherent limitations\nassociated with achieving such predictability. These limitations lead to 1)\nhighly variable access latencies that fluctuate based on various factors such\nas access patterns and memory state from previous accesses, and 2) overly\npessimistic latency bounds. As a result, DDR DRAMs can be ill-suited for some\nreal-time systems that mandate a strict predictable performance with tight\ntiming constraints. Targeting these systems, we promote an alternative off-chip\nmemory solution that is based on the emerging Reduced Latency DRAM (RLDRAM)\nprotocol, and propose a predictable memory controller (RLDC) managing accesses\nto this memory. Comparing with the state-of-the-art predictable DDR\ncontrollers, the proposed solution provides up to 11x less timing variability\nand 6.4x reduction in the worst case memory latency.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:02:12 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hassan", "Mohamed", ""]]}, {"id": "1810.07269", "submitter": "Mahmoud Khairy", "authors": "Mahmoud Khairy, Jain Akshay, Tor Aamodt and Timothy G. Rogers", "title": "Exploring Modern GPU Memory System Design Challenges through Accurate\n  Modeling", "comments": null, "journal-ref": null, "doi": "10.1109/ISCA45697.2020.00047", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the impact of simulator accuracy on architecture design\ndecisions in the general-purpose graphics processing unit (GPGPU) space. We\nperform a detailed, quantitative analysis of the most popular publicly\navailable GPU simulator, GPGPU-Sim, against our enhanced version of the\nsimulator, updated to model the memory system of modern GPUs in more detail.\nOur enhanced GPU model is able to describe the NVIDIA Volta architecture in\nsufficient detail to reduce error in memory system even counters by as much as\n66X. The reduced error in the memory system further reduces execution time\nerror versus real hardware by 2.5X. To demonstrate the accuracy of our enhanced\nmodel against a real machine, we perform a counter-by-counter validation\nagainst an NVIDIA TITAN V Volta GPU, demonstrating the relative accuracy of the\nnew simulator versus the publicly available model.\n  We go on to demonstrate that the simpler model discounts the importance of\nadvanced memory system designs such as out-of-order memory access scheduling,\nwhile overstating the impact of more heavily researched areas like L1 cache\nbypassing. Our results demonstrate that it is important for the academic\ncommunity to enhance the level of detail in architecture simulators as system\ncomplexity continues to grow. As part of this detailed correlation and modeling\neffort, we developed a new Correlator toolset that includes a consolidation of\napplications from a variety of popular GPGPU benchmark suites, designed to run\nin reasonable simulation times. The Correlator also includes a database of\nhardware profiling results for all these applications on NVIDIA cards ranging\nfrom Fermi to Volta and a toolchain that enables users to gather correlation\nstatistics and create detailed counter-by-counter hardware correlation plots\nwith minimal effort.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 20:52:59 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Khairy", "Mahmoud", ""], ["Akshay", "Jain", ""], ["Aamodt", "Tor", ""], ["Rogers", "Timothy G.", ""]]}, {"id": "1810.09661", "submitter": "Swagata Mandal", "authors": "Swagata Mandal, Sreetama Sarkar, Wong Ming Ming, Anupam Chattopadhyay,\n  and Amlan Chakrabarti", "title": "Criticality Aware Soft Error Mitigation in the Configuration Memory of\n  SRAM based FPGA", "comments": "6 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient low complexity error correcting code(ECC) is considered as an\neffective technique for mitigation of multi-bit upset (MBU) in the\nconfiguration memory(CM)of static random access memory (SRAM) based Field\nProgrammable Gate Array (FPGA) devices. Traditional multi-bit ECCs have large\noverhead and complex decoding circuit to correct adjacent multibit error. In\nthis work, we propose a simple multi-bit ECC which uses Secure Hash Algorithm\nfor error detection and parity based two dimensional Erasure Product Code for\nerror correction. Present error mitigation techniques perform error correction\nin the CM without considering the criticality or the execution period of the\ntasks allocated in different portion of CM. In most of the cases, error\ncorrection is not done in the right instant, which sometimes either suspends\nnormal system operation or wastes hardware resources for less critical tasks.\nIn this paper,we advocate for a dynamic priority-based hardware scheduling\nalgorithm which chooses the tasks for error correction based on their area,\nexecution period and criticality. The proposed method has been validated in\nterms of overhead due to redundant bits, error correction time and system\nreliability\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 05:07:24 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Mandal", "Swagata", ""], ["Sarkar", "Sreetama", ""], ["Ming", "Wong Ming", ""], ["Chattopadhyay", "Anupam", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "1810.12573", "submitter": "Marcos Horro Varela", "authors": "M. Horro, G. Rodr\\'iguez, J. Touri\\~no, M. T. Kandemir", "title": "Architectural exploration of heterogeneous memory systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous systems appear as a viable design alternative for the dark\nsilicon era. In this paradigm, a processor chip includes several different\ntechnological alternatives for implementing a certain logical block (e.g.,\ncore, on-chip memories) which cannot be used at the same time due to power\nconstraints. The programmer and compiler are then responsible for selecting\nwhich of the alternatives should be used for maximizing performance and/or\nenergy efficiency for a given application. This paper presents an initial\napproach for the exploration of different technological alternatives for the\nimplementation of on-chip memories. It hinges on a linear programming-based\nmodel for theoretically comparing the performance offered by the available\nalternatives, namely SRAM and STT-RAM scratchpads or caches. Experimental\nresults using a cycle-accurate simulation tool confirm that this is a viable\nmodel for implementation into production compilers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 08:25:12 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Horro", "M.", ""], ["Rodr\u00edguez", "G.", ""], ["Touri\u00f1o", "J.", ""], ["Kandemir", "M. T.", ""]]}, {"id": "1810.12640", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Bernard Girau, Nicolas Rougier, Andres Upegui, Benoit\n  Miramond", "title": "Neuromorphic hardware as a self-organizing computing system", "comments": "Published in IEEE World Congress on Computational Intelligence\n  (WCCI), International Workshop: Neuromorphic Hardware in Practice and Use\n  (NHPU), Jul. 2018, Rio de Janeiro, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the self-organized neuromorphic architecture named SOMA.\nThe objective is to study neural-based self-organization in computing systems\nand to prove the feasibility of a self-organizing hardware structure.\nConsidering that these properties emerge from large scale and fully connected\nneural maps, we will focus on the definition of a self-organizing hardware\narchitecture based on digital spiking neurons that offer hardware efficiency.\nFrom a biological point of view, this corresponds to a combination of the\nso-called synaptic and structural plasticities. We intend to define\ncomputational models able to simultaneously self-organize at both computation\nand communication levels, and we want these models to be hardware-compliant,\nfault tolerant and scalable by means of a neuro-cellular structure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:35:07 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Khacef", "Lyes", ""], ["Girau", "Bernard", ""], ["Rougier", "Nicolas", ""], ["Upegui", "Andres", ""], ["Miramond", "Benoit", ""]]}, {"id": "1810.12910", "submitter": "Muhammad Abdullah Hanif", "authors": "Muhammad Abdullah Hanif, Rachmad Vidya Wicaksana Putra, Muhammad\n  Tanvir, Rehan Hafiz, Semeen Rehman, Muhammad Shafique", "title": "MPNA: A Massively-Parallel Neural Array Accelerator with Dataflow\n  Optimization for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art accelerators for Convolutional Neural Networks (CNNs)\ntypically focus on accelerating only the convolutional layers, but do not\nprioritize the fully-connected layers much. Hence, they lack a synergistic\noptimization of the hardware architecture and diverse dataflows for the\ncomplete CNN design, which can provide a higher potential for\nperformance/energy efficiency. Towards this, we propose a novel\nMassively-Parallel Neural Array (MPNA) accelerator that integrates two\nheterogeneous systolic arrays and respective highly-optimized dataflow patterns\nto jointly accelerate both the convolutional (CONV) and the fully-connected\n(FC) layers. Besides fully-exploiting the available off-chip memory bandwidth,\nthese optimized dataflows enable high data-reuse of all the data types (i.e.,\nweights, input and output activations), and thereby enable our MPNA to achieve\nhigh energy savings. We synthesized our MPNA architecture using the ASIC design\nflow for a 28nm technology, and performed functional and timing validation\nusing multiple real-world complex CNNs. MPNA achieves 149.7GOPS/W at 280MHz and\nconsumes 239mW. Experimental results show that our MPNA architecture provides\n1.7x overall performance improvement compared to state-of-the-art accelerator,\nand 51% energy saving compared to the baseline architecture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:20:24 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hanif", "Muhammad Abdullah", ""], ["Putra", "Rachmad Vidya Wicaksana", ""], ["Tanvir", "Muhammad", ""], ["Hafiz", "Rehan", ""], ["Rehman", "Semeen", ""], ["Shafique", "Muhammad", ""]]}]