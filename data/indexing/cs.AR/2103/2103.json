[{"id": "2103.00007", "submitter": "Daniel Waddington", "authors": "Daniel Waddington and Clem Dickey and Moshik Hershcovitch and\n  Sangeetha Seshadri", "title": "An Architecture for Memory Centric Active Storage (MCAS)", "comments": "Revision 1.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of CPU-attached persistent memory technology, such as Intel's\nOptane Persistent Memory Modules (PMM), has brought with it new opportunities\nfor storage. In 2018, IBM Research Almaden began investigating and developing a\nnew enterprise-grade storage solution directly aimed at this emerging\ntechnology. MCAS (Memory Centric Active Storage) defines an evolved\nnetwork-attached key-value store that offers both near-data compute and the\nability to layer enterprise-grade data management services on shared persistent\nmemory. As a converged memory-storage tier, MCAS moves towards eliminating the\ntraditional separation of compute and storage, and thereby unifying the data\nspace. This paper provides an in-depth review of the MCAS architecture and\nimplementation, as well as general performance results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 18:12:41 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 01:56:24 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 20:31:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Waddington", "Daniel", ""], ["Dickey", "Clem", ""], ["Hershcovitch", "Moshik", ""], ["Seshadri", "Sangeetha", ""]]}, {"id": "2103.00194", "submitter": "Kingshuk Majumder", "authors": "Kingshuk Majumder, Uday Bondhugula", "title": "HIR: An MLIR-based Intermediate Representation for Hardware Accelerator\n  Description", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of machine learning, image and audio processing on edge devices\nhas motivated research towards power efficient custom hardware accelerators.\nThough FPGAs are an ideal target for energy efficient custom accelerators, the\ndifficulty of hardware design and the lack of vendor agnostic, standardized\nhardware compilation infrastructure has hindered their adoption.\n  This paper introduces HIR, an MLIR-based intermediate representation (IR) to\ndescribe hardware accelerator designs. HIR combines high level language\nfeatures, such as loops and multi-dimensional tensors, with programmer defined\nexplicit scheduling, to provide a high-level IR suitable for DSL compiler\npipelines without compromising control over the micro-architecture of the\naccelerator. HIR's explicit schedules allow it to express fine-grained,\nsynchronization-free parallelism and optimizations such as retiming and\npipelining. Built as a dialect in MLIR, it draws from best IR practices learnt\nfrom communities like those of LLVM. While offering rich optimization\nopportunities and a high level abstraction, HIR enables sharing of\noptimizations, utilities and passes with software compiler infrastructure.\n  Our implementation shows that the code generation time of the HIR code\ngenerator is on average 1112x lower than that of Xilinx Vivado HLS on a range\nof kernels without a compromise on the quality of the generated hardware. We\nbelieve that these are significant steps forward in the design of IRs for\nhardware synthesis and in equipping domain-specific languages with a productive\nand performing compilation path to custom hardware acceleration.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 11:34:30 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Majumder", "Kingshuk", ""], ["Bondhugula", "Uday", ""]]}, {"id": "2103.00216", "submitter": "Nimish Shah", "authors": "Nimish Shah, Laura I. Galindez Olascoaga, Wannes Meert and Marian\n  Verhelst", "title": "ProbLP: A framework for low-precision probabilistic inference", "comments": null, "journal-ref": "Proceedings of the 56th Annual Design Automation Conference (DAC)\n  2019", "doi": "10.1145/3316781.3317885", "report-no": null, "categories": "cs.AR cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian reasoning is a powerful mechanism for probabilistic inference in\nsmart edge-devices. During such inferences, a low-precision arithmetic\nrepresentation can enable improved energy efficiency. However, its impact on\ninference accuracy is not yet understood. Furthermore, general-purpose hardware\ndoes not natively support low-precision representation. To address this, we\npropose ProbLP, a framework that automates the analysis and design of\nlow-precision probabilistic inference hardware. It automatically chooses an\nappropriate energy-efficient representation based on worst-case error-bounds\nand hardware energy-models. It generates custom hardware for the resulting\ninference network exploiting parallelism, pipelining and low-precision\noperation. The framework is validated on several embedded-sensing benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 13:41:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shah", "Nimish", ""], ["Olascoaga", "Laura I. Galindez", ""], ["Meert", "Wannes", ""], ["Verhelst", "Marian", ""]]}, {"id": "2103.00266", "submitter": "Nimish Shah", "authors": "Nimish Shah, Laura I. Galindez Olascoaga, Wannes Meert and Marian\n  Verhelst", "title": "Acceleration of probabilistic reasoning through custom processor\n  architecture", "comments": null, "journal-ref": "Design, Automation & Test in Europe Conference & Exhibition (DATE)\n  2020", "doi": "10.23919/DATE48585.2020.9116326", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic reasoning is an essential tool for robust decision-making\nsystems because of its ability to explicitly handle real-world uncertainty,\nconstraints and causal relations. Consequently, researchers are developing\nhybrid models by combining Deep Learning with probabilistic reasoning for\nsafety-critical applications like self-driving vehicles, autonomous drones,\netc. However, probabilistic reasoning kernels do not execute efficiently on\nCPUs or GPUs. This paper, therefore, proposes a custom programmable processor\nto accelerate sum-product networks, an important probabilistic reasoning\nexecution kernel. The processor has an optimized datapath architecture and\nmemory hierarchy optimized for sum-product networks execution. Experimental\nresults show that the processor, while requiring fewer computational and memory\nunits, achieves a 12x throughput benefit over the Nvidia Jetson TX2 embedded\nGPU platform.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:57:20 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shah", "Nimish", ""], ["Olascoaga", "Laura I. Galindez", ""], ["Meert", "Wannes", ""], ["Verhelst", "Marian", ""]]}, {"id": "2103.00421", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Abdullah Hanif, Muhammad\n  Shafique", "title": "SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural\n  Network Inference using Approximate DRAM", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have the potential for achieving low energy\nconsumption due to their biologically sparse computation. Several studies have\nshown that the off-chip memory (DRAM) accesses are the most energy-consuming\noperations in SNN processing. However, state-of-the-art in SNN systems do not\noptimize the DRAM energy-per-access, thereby hindering achieving high\nenergy-efficiency. To substantially minimize the DRAM energy-per-access, a key\nknob is to reduce the DRAM supply voltage but this may lead to DRAM errors\n(i.e., the so-called approximate DRAM). Towards this, we propose SparkXD, a\nnovel framework that provides a comprehensive conjoint solution for resilient\nand energy-efficient SNN inference using low-power DRAMs subjected to\nvoltage-induced errors. The key mechanisms of SparkXD are: (1) improving the\nSNN error tolerance through fault-aware training that considers bit errors from\napproximate DRAM, (2) analyzing the error tolerance of the improved SNN model\nto find the maximum tolerable bit error rate (BER) that meets the targeted\naccuracy constraint, and (3) energy-efficient DRAM data mapping for the\nresilient SNN model that maps the weights in the appropriate DRAM location to\nminimize the DRAM access energy. Through these mechanisms, SparkXD mitigates\nthe negative impact of DRAM (approximation) errors, and provides the required\naccuracy. The experimental results show that, for a target accuracy within 1%\nof the baseline design (i.e., SNN without DRAM errors), SparkXD reduces the\nDRAM energy by ca. 40% on average across different network sizes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:12:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2103.00424", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Shafique", "title": "SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with\n  Continual and Unsupervised Learning Capabilities in Dynamic Environments", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised\nand continual learning capabilities because of their biological plausibility,\nbut their complexity still poses a serious research challenge to enable their\nenergy-efficient design for resource-constrained scenarios (like embedded\nsystems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for\nenergy-efficient SNNs with continual and unsupervised learning capabilities in\ndynamic environments, for both the training and inference phases. It is\nachieved through the following multiple diverse mechanisms: 1) reduction of\nneuronal operations, by replacing the inhibitory neurons with direct lateral\ninhibitions; 2) a memory- and energy-constrained SNN model search algorithm\nthat employs analytical models to estimate the memory footprint and energy\nconsumption of different candidate SNN models and selects a Pareto-optimal SNN\nmodel; and 3) a lightweight continual and unsupervised learning algorithm that\nemploys adaptive learning rates, adaptive membrane threshold potential, weight\ndecay, and reduction of spurious updates. Our experimental results show that,\nfor a network with 400 excitatory neurons, our SpikeDyn reduces the energy\nconsumption on average by 51% for training and by 37% for inference, as\ncompared to the state-of-the-art. Due to the improved learning algorithm,\nSpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art,\nfor classifying the most recently learned task, and by 8% on average for the\npreviously learned tasks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:26:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2103.00571", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Fabio Checconi, Douglas Doerfler, Fabrizio Petrini", "title": "Performance Optimization of SU3_Bench on Xeon and Programmable\n  Integrated Unified Memory Architecture", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SU3\\_Bench is a microbenchmark developed to explore performance portability\nacross multiple programming models/methodologies using a simple, but\nnontrivial, mathematical kernel. This kernel has been derived from the MILC\nlattice quantum chromodynamics (LQCD) code. SU3\\_Bench is bandwidth bound and\ngenerates regular compute and data access patterns. Therefore, on most\ntraditional CPU and GPU-based systems, its performance is mainly determined by\nthe achievable memory bandwidth. Although SU3\\_Bench is a simple kernel,\nexperience says its subtleties require a certain amount of tweaking to achieve\npeak performance for a given programming model and hardware, making performance\nportability challenging. In this paper, we share some of the challenges in\nobtaining the peak performance for SU3\\_Bench on a state-of-the-art Intel Xeon\nmachine, due to the nuances of variable definition, the nature of\ncompiler-provided default constructors, how memory is accessed at object\ncreation time, and the NUMA effects on the machine. We discuss how to tackle\nthose challenges to improve SU3\\_Bench's performance by \\(2\\times\\) compared to\nthe original OpenMP implementation available at Github. This provides a\nvaluable lesson for other similar kernels.\n  Expanding on the performance portability aspects, we also show early results\nobtained porting SU3\\_Bench to the new Intel Programmable Integrated Unified\nMemory Architecture (PIUMA), characterized by a more balanced flops-to-byte\nratio. This paper shows that it is not the usual bandwidth or flops, rather the\npipeline throughput, that determines SU3\\_Bench's performance on PIUMA.\nFinally, we show how to improve performance on PIUMA and how that compares with\nthe performance on Xeon, which has around one order of magnitude more\nflops-per-byte.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 17:33:26 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:15:07 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Checconi", "Fabio", ""], ["Doerfler", "Douglas", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2103.00686", "submitter": "Divya Mahajan", "authors": "Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, Prashant\n  J. Nair", "title": "High-Performance Training by Exploiting Hot-Embeddings in Recommendation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recommendation models are commonly used learning models that suggest relevant\nitems to a user for e-commerce and online advertisement-based applications.\nCurrent recommendation models include deep-learning-based (DLRM) and time-based\nsequence (TBSM) models. These models use massive embedding tables to store a\nnumerical representation of item's and user's categorical variables\n(memory-bound) while also using neural networks to generate outputs\n(compute-bound). Due to these conflicting compute and memory requirements, the\ntraining process for recommendation models is divided across CPU and GPU for\nembedding and neural network executions, respectively. Such a training process\nnaively assigns the same level of importance to each embedding entry. This\npaper observes that some training inputs and their accesses into the embedding\ntables are heavily skewed with certain entries being accessed up to 10000x\nmore. This paper tries to leverage skewed embedded table accesses to\nefficiently use the GPU resources during training. To this end, this paper\nproposes a Frequently Accessed Embeddings (FAE) framework that exposes a\ndynamic knob to the software based on the GPU memory capacity and the input\npopularity index. This framework efficiently estimates and varies the size of\nthe hot portions of the embedding tables within GPUs and reallocates the rest\nof the embeddings on the CPU. Overall, our framework speeds-up the training of\nthe recommendation models on Kaggle, Terabyte, and Alibaba datasets by 2.34x as\ncompared to a baseline that uses Intel-Xeon CPUs and Nvidia Tesla-V100 GPUs,\nwhile maintaining accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:43:26 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 19:16:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Adnan", "Muhammad", ""], ["Maboud", "Yassaman Ebrahimzadeh", ""], ["Mahajan", "Divya", ""], ["Nair", "Prashant J.", ""]]}, {"id": "2103.00768", "submitter": "Saugata Ghose", "authors": "Amirali Boroumand, Saugata Ghose, Berkin Akin, Ravi Narayanaswami,\n  Geraldo F. Oliveira, Xiaoyu Ma, Eric Shiu, Onur Mutlu", "title": "Mitigating Edge Machine Learning Inference Bottlenecks: An Empirical\n  Study on Accelerating Google Edge Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the need for edge computing grows, many modern consumer devices now\ncontain edge machine learning (ML) accelerators that can compute a wide range\nof neural network (NN) models while still fitting within tight resource\nconstraints. We analyze a commercial Edge TPU using 24 Google edge NN models\n(including CNNs, LSTMs, transducers, and RCNNs), and find that the accelerator\nsuffers from three shortcomings, in terms of computational throughput, energy\nefficiency, and memory access handling. We comprehensively study the\ncharacteristics of each NN layer in all of the Google edge models, and find\nthat these shortcomings arise from the one-size-fits-all approach of the\naccelerator, as there is a high amount of heterogeneity in key layer\ncharacteristics both across different models and across different layers in the\nsame model.\n  We propose a new acceleration framework called Mensa. Mensa incorporates\nmultiple heterogeneous ML edge accelerators (including both on-chip and\nnear-data accelerators), each of which caters to the characteristics of a\nparticular subset of models. At runtime, Mensa schedules each layer to run on\nthe best-suited accelerator, accounting for both efficiency and inter-layer\ndependencies. As we analyze the Google edge NN models, we discover that all of\nthe layers naturally group into a small number of clusters, which allows us to\ndesign an efficient implementation of Mensa for these models with only three\nspecialized accelerators. Averaged across all 24 Google edge models, Mensa\nimproves energy efficiency and throughput by 3.0x and 3.1x over the Edge TPU,\nand by 2.4x and 4.3x over Eyeriss v2, a state-of-the-art accelerator.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:49:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Boroumand", "Amirali", ""], ["Ghose", "Saugata", ""], ["Akin", "Berkin", ""], ["Narayanaswami", "Ravi", ""], ["Oliveira", "Geraldo F.", ""], ["Ma", "Xiaoyu", ""], ["Shiu", "Eric", ""], ["Mutlu", "Onur", ""]]}, {"id": "2103.00798", "submitter": "Saugata Ghose", "authors": "Amirali Boroumand, Saugata Ghose, Geraldo F. Oliveira, Onur Mutlu", "title": "Polynesia: Enabling Effective Hybrid Transactional/Analytical Databases\n  with Specialized Hardware/Software Co-Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exponential growth in data volume, combined with increasing demand for\nreal-time analysis (i.e., using the most recent data), has resulted in the\nemergence of database systems that concurrently support transactions and data\nanalytics. These hybrid transactional and analytical processing (HTAP) database\nsystems can support real-time data analysis without the high costs of\nsynchronizing across separate single-purpose databases. Unfortunately, for many\napplications that perform a high rate of data updates, state-of-the-art HTAP\nsystems incur significant drops in transactional (up to 74.6%) and/or\nanalytical (up to 49.8%) throughput compared to performing only transactions or\nonly analytics in isolation, due to (1) data movement between the CPU and\nmemory, (2) data update propagation, and (3) consistency costs.\n  We propose Polynesia, a hardware-software co-designed system for in-memory\nHTAP databases. Polynesia (1) divides the HTAP system into transactional and\nanalytical processing islands, (2) implements custom algorithms and hardware to\nreduce the costs of update propagation and consistency, and (3) exploits\nprocessing-in-memory for the analytical islands to alleviate data movement. Our\nevaluation shows that Polynesia outperforms three state-of-the-art HTAP\nsystems, with average transactional/analytical throughput improvements of\n1.70X/3.74X, and reduces energy consumption by 48% over the prior lowest-energy\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:41:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Boroumand", "Amirali", ""], ["Ghose", "Saugata", ""], ["Oliveira", "Geraldo F.", ""], ["Mutlu", "Onur", ""]]}, {"id": "2103.01176", "submitter": "Carlo Sau", "authors": "Giacomo Valente, Tiziana Fanni, Carlo Sau, Francesco Di Battista", "title": "Layering the monitoring action for improved flexibility and overhead\n  control: work-in-progress", "comments": null, "journal-ref": "2020 International Conference on Hardware/Software Codesign and\n  System Synthesis (CODES+ISSS), September 2020", "doi": "10.1109/CODESISSS51650.2020.9244018", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the diffusion of complex heterogeneous platforms and their need of\ncharacterization, monitoring the system gained increasing interest. This work\nproposes a framework to build custom and modular monitoring systems, flexible\nenough to face the heterogeneity of modern platforms, offering a predictable\nHW/SW impact.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:16:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Valente", "Giacomo", ""], ["Fanni", "Tiziana", ""], ["Sau", "Carlo", ""], ["Di Battista", "Francesco", ""]]}, {"id": "2103.01195", "submitter": "Carlo Sau", "authors": "Tiziana Fanni, Daniel Madronal, Claudio Rubattu, Carlo Sau, Francesca\n  Palumbo, Eduardo Juarez, Maxime Pelcat, Cesar Sanz, Luigi Raffo", "title": "Run-time Performance Monitoring of Heterogenous Hw/Sw Platforms Using\n  PAPI", "comments": "Sixth International Workshop on FPGAs for Software Programmers (FSP\n  Workshop) 2019 ISBN: 978-3-8007-5045-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Cyber Physical Systems, designers need to offer support for\nrun-time adaptivity considering different constraints, including the internal\nstatus of the system. This work presents a run-time monitoring approach, based\non the Performance Application Programming Interface, that offers a unified\ninterface to transparently access both the standard Performance Monitoring\nCounters (PMCs) in the CPUs and the custom ones integrated into hardware\naccelerators. Automatic tools offer to Sw programmers the support to design and\nimplement Coarse-Grain Virtual Reconfigurable Circuits, instrumented with\ncustom PMCs. This approach has been validated on a heterogeneous application\nfor image/video processing with an overhead of 6% of the execution time.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:42:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fanni", "Tiziana", ""], ["Madronal", "Daniel", ""], ["Rubattu", "Claudio", ""], ["Sau", "Carlo", ""], ["Palumbo", "Francesca", ""], ["Juarez", "Eduardo", ""], ["Pelcat", "Maxime", ""], ["Sanz", "Cesar", ""], ["Raffo", "Luigi", ""]]}, {"id": "2103.01489", "submitter": "Kartik Hegde", "authors": "Kartik Hegde, Po-An Tsai, Sitao Huang, Vikas Chandra, Angshuman\n  Parashar, and Christopher W. Fletcher", "title": "Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space\n  Search", "comments": "Appears in the proceedings of the 26th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems\n  (ASPLOS '21), April 19-23, 2021, Virtual, USA", "journal-ref": null, "doi": "10.1145/3445814.3446762", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern day computing increasingly relies on specialization to satiate growing\nperformance and efficiency requirements. A core challenge in designing such\nspecialized hardware architectures is how to perform mapping space search,\ni.e., search for an optimal mapping from algorithm to hardware. Prior work\nshows that choosing an inefficient mapping can lead to multiplicative-factor\nefficiency overheads. Additionally, the search space is not only large but also\nnon-convex and non-smooth, precluding advanced search techniques. As a result,\nprevious works are forced to implement mapping space search using expert\nchoices or sub-optimal search heuristics.\n  This work proposes Mind Mappings, a novel gradient-based search method for\nalgorithm-accelerator mapping space search. The key idea is to derive a smooth,\ndifferentiable approximation to the otherwise non-smooth, non-convex search\nspace. With a smooth, differentiable approximation, we can leverage efficient\ngradient-based search algorithms to find high-quality mappings. We extensively\ncompare Mind Mappings to black-box optimization schemes used in prior work.\nWhen tasked to find mappings for two important workloads (CNN and MTTKRP), the\nproposed search finds mappings that achieve an average $1.40\\times$,\n$1.76\\times$, and $1.29\\times$ (when run for a fixed number of steps) and\n$3.16\\times$, $4.19\\times$, and $2.90\\times$ (when run for a fixed amount of\ntime) better energy-delay product (EDP) relative to Simulated Annealing,\nGenetic Algorithms and Reinforcement Learning, respectively. Meanwhile, Mind\nMappings returns mappings with only $5.32\\times$ higher EDP than a possibly\nunachievable theoretical lower-bound, indicating proximity to the global\noptima.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:11:58 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hegde", "Kartik", ""], ["Tsai", "Po-An", ""], ["Huang", "Sitao", ""], ["Chandra", "Vikas", ""], ["Parashar", "Angshuman", ""], ["Fletcher", "Christopher W.", ""]]}, {"id": "2103.01705", "submitter": "Fangxin Liu", "authors": "Fangxin Liu, Wenbo Zhao, Yilong Zhao, Zongwu Wang, Tao Yang, Zhezhi\n  He, Naifeng Jing, Xiaoyao Liang, Li Jiang", "title": "SME: ReRAM-based Sparse-Multiplication-Engine to Squeeze-Out Bit\n  Sparsity of Neural Network", "comments": "7 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive Random-Access-Memory (ReRAM) crossbar is a promising technique for\ndeep neural network (DNN) accelerators, thanks to its in-memory and in-situ\nanalog computing abilities for Vector-Matrix Multiplication-and-Accumulations\n(VMMs). However, it is challenging for crossbar architecture to exploit the\nsparsity in the DNN. It inevitably causes complex and costly control to exploit\nfine-grained sparsity due to the limitation of tightly-coupled crossbar\nstructure. As the countermeasure, we developed a novel ReRAM-based DNN\naccelerator, named Sparse-Multiplication-Engine (SME), based on a hardware and\nsoftware co-design framework. First, we orchestrate the bit-sparse pattern to\nincrease the density of bit-sparsity based on existing quantization methods.\nSecond, we propose a novel weigh mapping mechanism to slice the bits of a\nweight across the crossbars and splice the activation results in peripheral\ncircuits. This mechanism can decouple the tightly-coupled crossbar structure\nand cumulate the sparsity in the crossbar. Finally, a superior squeeze-out\nscheme empties the crossbars mapped with highly-sparse non-zeros from the\nprevious two steps. We design the SME architecture and discuss its use for\nother quantization methods and different ReRAM cell technologies. Compared with\nprior state-of-the-art designs, the SME shrinks the use of crossbars up to 8.7x\nand 2.1x using Resent-50 and MobileNet-v2, respectively, with less than 0.3%\naccuracy drop on ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:27:15 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Liu", "Fangxin", ""], ["Zhao", "Wenbo", ""], ["Zhao", "Yilong", ""], ["Wang", "Zongwu", ""], ["Yang", "Tao", ""], ["He", "Zhezhi", ""], ["Jing", "Naifeng", ""], ["Liang", "Xiaoyao", ""], ["Jiang", "Li", ""]]}, {"id": "2103.01773", "submitter": "Sabah Al-Fedaghi Dr.", "authors": "Sabah Al-Fedaghi", "title": "Conceptual Modeling for Computer Organization and Architecture", "comments": "11 pages, 12 figures", "journal-ref": "Journal of Computer Science 2021, 17 (2): 123.134", "doi": "10.3844/jcssp.2021.123.1324", "report-no": null, "categories": "cs.CY cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding computer system hardware, including how computers operate, is\nessential for undergraduate students in computer engineering and science.\nLiterature shows students learning computer organization and assembly language\noften find fundamental concepts difficult to comprehend within the topic\nmaterials. Tools have been introduced to improve students comprehension of the\ninteraction between computer architecture, assembly language, and the operating\nsystem. One such tool is the Little Man Computer (LMC) model that operates in a\nway similar to a computer but that is easier to understand. Even though LMC\ndoes not have modern CPUs with multiple cores nor executes multiple\ninstructions, it nevertheless shows the basic principles of the von Neumann\narchitecture. LMC aims to introduce students to such concepts as code and\ninstruction sets. In this paper, LMC is used for an additional purpose: a tool\nwith which to experiment using a new modeling language (i.e., a thinging\nmachine; TM) in the area of computer organization and architecture without\ninvolving complexity in the subject. That is, the simplicity of LMC facilitates\nthe application of TM without going deep into computer\norganization/architecture materials. Accordingly, the paper (a) provides a new\nway for using the LMC model for whatever purpose (e.g., education) and (b)\ndemonstrates that TM can be used to build an abstract level of description in\nthe organization/architect field. The resultant schematics from the TM model of\nLMC offer an initial case study that supports our thesis that TM is a viable\nmethod for hardware/software-independent descriptions in the computer\norganization and architect field of study.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:52:49 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Al-Fedaghi", "Sabah", ""]]}, {"id": "2103.02176", "submitter": "Shaoshan Liu", "authors": "Shaoshan Liu, Bo Yu, Jie Tang, Qi Zhu", "title": "Towards Fully Intelligent Transportation through Infrastructure-Vehicle\n  Cooperative Autonomous Driving: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infrastructure-vehicle cooperative autonomous driving approach depends on\nthe cooperation between intelligent roads and intelligent vehicles. This\napproach is not only safer but also more economical compared to the traditional\non-vehicle-only autonomous driving approach. In this paper, we introduce our\nreal-world deployment experiences of cooperative autonomous driving, and delve\ninto the details of new challenges and opportunities. Specifically, based on\nour progress towards commercial deployment, we follow a three-stage development\nroadmap of the cooperative autonomous driving approach:infrastructure-augmented\nautonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and\ninfrastructure-planned autonomous driving (IPAD).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:50:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Shaoshan", ""], ["Yu", "Bo", ""], ["Tang", "Jie", ""], ["Zhu", "Qi", ""]]}, {"id": "2103.02800", "submitter": "Zejian Liu", "authors": "Zejian Liu, Gang Li and Jian Cheng", "title": "Hardware Acceleration of Fully Quantized BERT for Efficient Natural\n  Language Processing", "comments": null, "journal-ref": "Design, Automation & Test in Europe (DATE) 2021", "doi": null, "report-no": null, "categories": "cs.AR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BERT is the most recent Transformer-based model that achieves\nstate-of-the-art performance in various NLP tasks. In this paper, we\ninvestigate the hardware acceleration of BERT on FPGA for edge computing. To\ntackle the issue of huge computational complexity and memory footprint, we\npropose to fully quantize the BERT (FQ-BERT), including weights, activations,\nsoftmax, layer normalization, and all the intermediate results. Experiments\ndemonstrate that the FQ-BERT can achieve 7.94x compression for weights with\nnegligible performance loss. We then propose an accelerator tailored for the\nFQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a\nperformance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)\nCore(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 02:49:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Zejian", ""], ["Li", "Gang", ""], ["Cheng", "Jian", ""]]}, {"id": "2103.02889", "submitter": "Frederick Ziyang Hong", "authors": "Ziyang Hong and C. Patrick Yue", "title": "Efficient Training Convolutional Neural Networks on Edge Devices with\n  Gradient-pruned Sign-symmetric Feedback Alignment", "comments": "This work is published in the Proceedings of the 9th International\n  Conference on IT Convergence and Security (ICITCS2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperity of mobile devices, the distributed learning approach\nenabling model training with decentralized data has attracted wide research.\nHowever, the lack of training capability for edge devices significantly limits\nthe energy efficiency of distributed learning in real life. This paper\ndescribes a novel approach of training DNNs exploiting the redundancy and the\nweight asymmetry potential of conventional backpropagation. We demonstrate that\nwith negligible classification accuracy loss, the proposed approach outperforms\nthe prior arts by 5x in terms of energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:23:39 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 02:10:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Hong", "Ziyang", ""], ["Yue", "C. Patrick", ""]]}, {"id": "2103.02904", "submitter": "Qigong Sun", "authors": "Qigong Sun, Licheng Jiao, Yan Ren, Xiufang Li, Fanhua Shang, Fang Liu", "title": "Effective and Fast: A Novel Sequential Single Path Search for\n  Mixed-Precision Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since model quantization helps to reduce the model size and computation\nlatency, it has been successfully applied in many applications of mobile\nphones, embedded devices and smart chips. The mixed-precision quantization\nmodel can match different quantization bit-precisions according to the\nsensitivity of different layers to achieve great performance. However, it is a\ndifficult problem to quickly determine the quantization bit-precision of each\nlayer in deep neural networks according to some constraints (e.g., hardware\nresources, energy consumption, model size and computation latency). To address\nthis issue, we propose a novel sequential single path search (SSPS) method for\nmixed-precision quantization,in which the given constraints are introduced into\nits loss function to guide searching process. A single path search cell is used\nto combine a fully differentiable supernet, which can be optimized by\ngradient-based algorithms. Moreover, we sequentially determine the candidate\nprecisions according to the selection certainties to exponentially reduce the\nsearch space and speed up the convergence of searching process. Experiments\nshow that our method can efficiently search the mixed-precision models for\ndifferent architectures (e.g., ResNet-20, 18, 34, 50 and MobileNet-V2) and\ndatasets (e.g., CIFAR-10, ImageNet and COCO) under given constraints, and our\nexperimental results verify that SSPS significantly outperforms their uniform\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:15:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sun", "Qigong", ""], ["Jiao", "Licheng", ""], ["Ren", "Yan", ""], ["Li", "Xiufang", ""], ["Shang", "Fanhua", ""], ["Liu", "Fang", ""]]}, {"id": "2103.03443", "submitter": "Riccardo Paccagnella", "authors": "Riccardo Paccagnella and Licheng Luo and Christopher W. Fletcher", "title": "Lord of the Ring(s): Side Channel Attacks on the CPU On-Chip Ring\n  Interconnect Are Practical", "comments": "This is the extended version of a paper that appears in USENIX\n  Security 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first microarchitectural side channel attacks that leverage\ncontention on the CPU ring interconnect. There are two challenges that make it\nuniquely difficult to exploit this channel. First, little is known about the\nring interconnect's functioning and architecture. Second, information that can\nbe learned by an attacker through ring contention is noisy by nature and has\ncoarse spatial granularity. To address the first challenge, we perform a\nthorough reverse engineering of the sophisticated protocols that handle\ncommunication on the ring interconnect. With this knowledge, we build a\ncross-core covert channel over the ring interconnect with a capacity of over 4\nMbps from a single thread, the largest to date for a cross-core channel not\nrelying on shared memory. To address the second challenge, we leverage the\nfine-grained temporal patterns of ring contention to infer a victim program's\nsecrets. We demonstrate our attack by extracting key bits from vulnerable EdDSA\nand RSA implementations, as well as inferring the precise timing of keystrokes\ntyped by a victim user.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:44:20 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Paccagnella", "Riccardo", ""], ["Luo", "Licheng", ""], ["Fletcher", "Christopher W.", ""]]}, {"id": "2103.03500", "submitter": "Mark Zhao", "authors": "Mark Zhao, Mingyu Gao, and Christos Kozyrakis", "title": "ShEF: Shielded Enclaves for Cloud FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  FPGAs are now used in public clouds to accelerate a wide range of\napplications, including many that operate on sensitive data such as financial\nand medical records. We present ShEF, a trusted execution environment (TEE) for\ncloud-based reconfigurable accelerators. ShEF is independent from CPU-based\nTEEs and allows secure execution under a threat model where the adversary can\ncontrol all software running on the CPU connected to the FPGA, has physical\naccess to the FPGA, and can compromise the FPGA interface logic of the cloud\nprovider. ShEF provides a secure boot and remote attestation process that\nrelies solely on existing FPGA mechanisms for root of trust. It also includes a\nShield component that provides secure access to data while the accelerator is\nin use. The Shield is highly customizable and extensible, allowing users to\ncraft a bespoke security solution that fits their accelerator's memory access\npatterns, bandwidth, and security requirements at minimum performance and area\noverheads. We describe a prototype implementation of ShEF for existing cloud\nFPGAs and measure the performance benefits of customizable security using five\naccelerator designs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:02:26 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhao", "Mark", ""], ["Gao", "Mingyu", ""], ["Kozyrakis", "Christos", ""]]}, {"id": "2103.03564", "submitter": "Carlo Sau", "authors": "Carlo Sau, Tiziana Fanni, Claudio Rubattu, Luigi Raffo, Francesca\n  Palumbo", "title": "The Multi-Dataflow Composer Tool: an open-source tool suite for\n  Optimized Coarse-Grain Reconfigurable Hardware Accelerators and Platform\n  Design", "comments": null, "journal-ref": "Microprocessors and Microsystems, Volume 80, February 2021", "doi": "10.1016/j.micpro.2020.103326", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern embedded and cyber-physical systems require every day more\nperformance, power efficiency and flexibility, to execute several profiles and\nfunctionalities targeting the ever growing adaptivity needs and preserving\nexecution efficiency. Such requirements pushed designers towards the adoption\nof heterogeneous and reconfigurable substrates, which development and\nmanagement is not that straightforward. Despite acceleration and flexibility\nare desirable in many domains, the barrier of hardware deployment and operation\nis still there since specific advanced expertise and skills are needed. Related\nchallenges are effectively tackled by leveraging on automation strategies that\nin some cases, as in the proposed work, exploit model-based approaches. This\npaper is focused on the Multi-Dataflow Composer (MDC) tool, that intends to\nsolve issues related to design, optimization and operation of coarse-grain\nreconfigurable hardware accelerators and their easy adoption in modern\nheterogeneous substrates. MDC latest features and improvements are introduced\nin detail and have been assessed on the so far unexplored robotics application\nfield. A multi-profile trajectory generator for a robotic arm is implemented\nover a Xilinx FPGA board to show in which cases coarse-grain reconfiguration\ncan be applied and which can be the parameters and trade-offs MDC will allow\nusers to play with.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:47:11 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Sau", "Carlo", ""], ["Fanni", "Tiziana", ""], ["Rubattu", "Claudio", ""], ["Raffo", "Luigi", ""], ["Palumbo", "Francesca", ""]]}, {"id": "2103.03712", "submitter": "Carlo Sau", "authors": "Francesca Palumbo and Carlo Sau", "title": "Reconfigurable and approximate computing for video coding", "comments": "Chapater of the VLSI Architectures for Future Video Coding, IET\n  Digital Library, 2019\n  (https://digital-library.theiet.org/content/books/10.1049/pbcs053e_ch9)", "journal-ref": null, "doi": "10.1049/PBCS053E_ch9", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chapter begins with a discussion of the constraints and needs of video\ncoding systems. The lack in flexibility of traditional monolithic codec\nspecifications, not suitable to model commonalities among codecs and foster\nreusability among successive codec generations/updates, was the main trigger\nfor the development of a new standard initiative within the ISO/IEC MPEG\ncommittee, called reconfigurable video coding (RVC). The MPEG-RVC framework\nexploits the dataflow nature behind video coding to foster flexible and\nreconfigurable codec design, as well as to support dynamic reconfiguration. The\nChapter goes on to consider that the inherent resiliency of various functional\nblocks (like motion estimation in the high-efficiency video coding, HEVC) and\nthe varying levels of user perception make video coding suitable to apply\napproximate computing techniques. Approximate computing, if properly supported\nat design time, allows achieving run-time trade-offs, representing a new\ndirection in hardware-software codesign research. The main assumption behind\napproximate computing, exploited within video coding, is that the degree of\naccuracy (in this case during codec execution) is not required to be the same\nall the time. The final part of the Chapter attempts to put together the\nconcepts addressed and remarks on which are, in the authors' opinion, some\ninteresting research directions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:35:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Palumbo", "Francesca", ""], ["Sau", "Carlo", ""]]}, {"id": "2103.03953", "submitter": "Supreeth Mysore Shivanandamurthy", "authors": "Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad\n  Salehi", "title": "ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ\n  Neural Network Processing in Phase Change RAM", "comments": "6 pages, 6 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator de-signs for ANNs have been proposed\nrecently. In this paper, we present a novel processing-in-memory (PIM) engine\ncalled ODIN that employs hybrid binary-stochastic bit-parallel arithmetic\nin-side phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration\nof all essential ANN functions such as multiply-accumulate (MAC), nonlinear\nactivation, and pooling. We mapped four ANN benchmark applications on ODIN to\ncompare its performance with a conventional processor-centric design and a\ncrossbar-based in-situ ANN accelerator from prior work. The results of our\nanalysis for the considered ANN topologies indicate that our ODIN accelerator\ncan be at least 5.8x faster and 23.2x more energy-efficient, and up to 90.8x\nfaster and 1554x more energy-efficient, compared to the crossbar-based in-situ\nANN accelerator from prior work.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 21:47:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shivanandamurthy", "Supreeth Mysore", ""], ["Thakkar", "Ishan. G.", ""], ["Salehi", "Sayed Ahmad", ""]]}, {"id": "2103.04456", "submitter": "Stefan Tauner", "authors": "Mario Telesklav and Stefan Tauner", "title": "Comparative Analysis and Enhancement of CFG-based Hardware-Assisted CFI\n  Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Subverting the flow of instructions (e.g., by use of code-reuse attacks)\nstill poses a serious threat to the security of today's systems. Various\ncontrol flow integrity (CFI) schemes have been proposed as a powerful technique\nto detect and mitigate such attacks. In recent years, many hardware-assisted\nimplementations of CFI enforcement based on control flow graphs (CFGs) have\nbeen presented by academia. Such approaches check whether control flow\ntransfers follow the intended CFG by limiting the valid target addresses.\nHowever, these papers all target different platforms and were evaluated with\ndifferent sets of benchmark applications, which makes quantitative comparisons\nhardly possible.\n  For this paper, we have implemented multiple promising CFG-based CFI schemes\non a common platform comprising a RISC-V SoC within an FPGA. By porting almost\n40 benchmark applications to this system we can present a meaningful comparison\nof the various techniques in terms of run-time performance, hardware\nutilization, and binary size. In addition, we present an enhanced CFI approach\nthat is inspired by what we consider the best concepts and ideas of previously\nproposed mechanisms. We have made this approach more practical and\nfeature-complete by tackling some problems largely ignored previously. We show\nwith this fine-grained scheme that CFI can be achieved with even less overheads\nthan previously demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:53:33 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Telesklav", "Mario", ""], ["Tauner", "Stefan", ""]]}, {"id": "2103.04675", "submitter": "Grzegorz Ficht", "authors": "Grzegorz Ficht and Sven Behnke", "title": "Bipedal Humanoid Hardware Design: A Technology Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose of Review: As new technological advancements are made, humanoid\nrobots that utilise them are being designed and manufactured. For optimal\ndesign choices, a broad overview with insight on the advantages and\ndisadvantages of available technologies is necessary. This article intends to\nprovide an analysis on the established approaches and contrast them with\nemerging ones.\n  Recent Findings: A clear shift in the recent design features of humanoid\nrobots is developing, which is supported by literature. As humanoid robots are\nmeant to leave laboratories and traverse the world, compliance and more\nefficient locomotion is necessary. The limitations of highly rigid actuation\nare being tackled by different research groups in unique ways. Some focus on\nmodifying the kinematic structure, while others change the actuation scheme.\nWith new manufacturing capabilities, previously impossible designs are becoming\nfeasible.\n  Summary: A comprehensive review on the technologies crucial for bipedal\nhumanoid robots was performed. Different mechanical concepts have been\ndiscussed, along with the advancements in actuation, sensing and manufacturing.\nThe paper is supplemented with a list of the recently developed platforms along\nwith a selection of their specifications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:34:19 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ficht", "Grzegorz", ""], ["Behnke", "Sven", ""]]}, {"id": "2103.04808", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Luca Giuseppe Cellamare, Marco Siracusa, Marco\n  Domenico Santambrogio", "title": "Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding\n  Similarity on FPGAs", "comments": "To appear in Proceedings of the 58th Design Automation Conference\n  (DAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Top-K SpMV is a key component of similarity-search on sparse embeddings. This\nsparse workload does not perform well on general-purpose NUMA systems that\nemploy traditional caching strategies. Instead, modern FPGA accelerator cards\nhave a few tricks up their sleeve. We introduce a Top-K SpMV FPGA design that\nleverages reduced precision and a novel packet-wise CSR matrix compression,\nenabling custom data layouts and delivering bandwidth efficiency often\nunreachable even in architectures with higher peak bandwidth. With HBM-based\nboards, we are 100x faster than a multi-threaded CPU implementation and 2x\nfaster than a GPU with 20% higher bandwidth, with 14.2x higher\npower-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:09:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Parravicini", "Alberto", ""], ["Cellamare", "Luca Giuseppe", ""], ["Siracusa", "Marco", ""], ["Santambrogio", "Marco Domenico", ""]]}, {"id": "2103.04812", "submitter": "Georgios Zervakis", "authors": "Sami Salamin, Georgios Zervakis, Ourania Spantidi, Iraklis\n  Anagnostopoulos, J\\\"org Henkel, Hussam Amrouch", "title": "Reliability-Aware Quantization for Anti-Aging NPUs", "comments": "Accepted for publication at the 24th Design Automation and Test in\n  Europe Conference (DATE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transistor aging is one of the major concerns that challenges designers in\nadvanced technologies. It profoundly degrades the reliability of circuits\nduring its lifetime as it slows down transistors resulting in errors due to\ntiming violations unless large guardbands are included, which leads to\nconsiderable performance losses. When it comes to Neural Processing Units\n(NPUs), where increasing the inference speed is the primary goal, such\nperformance losses cannot be tolerated. In this work, we are the first to\npropose a reliability-aware quantization to eliminate aging effects in NPUs\nwhile completely removing guardbands. Our technique delivers a graceful\ninference accuracy degradation over time while compensating for the\naging-induced delay increase of the NPU. Our evaluation, over ten\nstate-of-the-art neural network architectures trained on the ImageNet dataset,\ndemonstrates that for an entire lifetime of 10 years, the average accuracy loss\nis merely 3%. In the meantime, our technique achieves 23% higher performance\ndue to the elimination of the aging guardband.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:13:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Salamin", "Sami", ""], ["Zervakis", "Georgios", ""], ["Spantidi", "Ourania", ""], ["Anagnostopoulos", "Iraklis", ""], ["Henkel", "J\u00f6rg", ""], ["Amrouch", "Hussam", ""]]}, {"id": "2103.04852", "submitter": "E K", "authors": "Eren Kurshan, Hai Li, Mingoo Seok, Yuan Xie", "title": "A Case for 3D Integrated System Design for Neuromorphic Computing & AI\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, artificial intelligence has found many applications\nareas in the society. As AI solutions have become more sophistication and the\nuse cases grew, they highlighted the need to address performance and energy\nefficiency challenges faced during the implementation process. To address these\nchallenges, there has been growing interest in neuromorphic chips. Neuromorphic\ncomputing relies on non von Neumann architectures as well as novel devices,\ncircuits and manufacturing technologies to mimic the human brain. Among such\ntechnologies, 3D integration is an important enabler for AI hardware and the\ncontinuation of the scaling laws. In this paper, we overview the unique\nopportunities 3D integration provides in neuromorphic chip design, discuss the\nemerging opportunities in next generation neuromorphic architectures and review\nthe obstacles. Neuromorphic architectures, which relied on the brain for\ninspiration and emulation purposes, face grand challenges due to the limited\nunderstanding of the functionality and the architecture of the human brain.\nYet, high-levels of investments are dedicated to develop neuromorphic chips. We\nargue that 3D integration not only provides strategic advantages to the\ncost-effective and flexible design of neuromorphic chips, it may provide design\nflexibility in incorporating advanced capabilities to further benefits the\ndesigns in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:50:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kurshan", "Eren", ""], ["Li", "Hai", ""], ["Seok", "Mingoo", ""], ["Xie", "Yuan", ""]]}, {"id": "2103.04958", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Dawei Wang, Pierce Chuang, Shugao Ma, Deming Chen,\n  Yuecheng Li", "title": "F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar\n  Decoding", "comments": "Published as a conference paper at Design Automation Conference 2021\n  (DAC'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Creating virtual avatars with realistic rendering is one of the most\nessential and challenging tasks to provide highly immersive virtual reality\n(VR) experiences. It requires not only sophisticated deep neural network (DNN)\nbased codec avatar decoders to ensure high visual quality and precise motion\nexpression, but also efficient hardware accelerators to guarantee smooth\nreal-time rendering using lightweight edge devices, like untethered VR\nheadsets. Existing hardware accelerators, however, fail to deliver sufficient\nperformance and efficiency targeting such decoders which consist of\nmulti-branch DNNs and require demanding compute and memory resources. To\naddress these problems, we propose an automation framework, called F-CAD\n(Facebook Codec avatar Accelerator Design), to explore and deliver optimized\nhardware accelerators for codec avatar decoding. Novel technologies include 1)\na new accelerator architecture to efficiently handle multi-branch DNNs; 2) a\nmulti-branch dynamic design space to enable fine-grained architecture\nconfigurations; and 3) an efficient architecture search for picking the\noptimized hardware design based on both application-specific demands and\nhardware resource constraints. To the best of our knowledge, F-CAD is the first\nautomation tool that supports the whole design flow of hardware acceleration of\ncodec avatar decoders, allowing joint optimization on decoder designs in\npopular machine learning frameworks and corresponding customized accelerator\ndesign with cycle-accurate evaluation. Results show that the accelerators\ngenerated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6%\nhardware efficiency when running the latest codec avatar decoder. Compared to\nthe state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput,\n62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting\nthe same hardware device.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:28:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Wang", "Dawei", ""], ["Chuang", "Pierce", ""], ["Ma", "Shugao", ""], ["Chen", "Deming", ""], ["Li", "Yuecheng", ""]]}, {"id": "2103.05106", "submitter": "Ahmet Cagri Bagbaba", "authors": "Ahmet Cagri Bagbaba, Maksim Jenihhin, Raimund Ubar, Christian Sauer", "title": "Representing Gate-Level SET Faults by Multiple SEU Faults at RTL", "comments": null, "journal-ref": null, "doi": "10.1109/IOLTS50870.2020.9159715", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advanced complex electronic systems increasingly demand safer and more\nsecure hardware parts. Correspondingly, fault injection became a major\nverification milestone for both safety- and security-critical applications.\nHowever, fault injection campaigns for gate-level designs suffer from huge\nexecution times. Therefore, designers need to apply early design evaluation\ntechniques to reduce the execution time of fault injection campaigns. In this\nwork, we propose a method to represent gate-level Single-Event Transient (SET)\nfaults by multiple Single-Event Upset (SEU) faults at the Register-Transfer\nLevel. Introduced approach is to identify true and false logic paths for each\nSET in the flip-flops fan-in logic cones to obtain more accurate sets of\nflip-flops for multiple SEUs injections at RTL. Experimental results\ndemonstrate the feasibility of the proposed method to successfully reduce the\nfault space and also its advantage with respect to state of the art. It was\nshown that the approach is able to reduce the fault space, and therefore the\nfault-injection effort, by up to tens to hundreds of times.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:40:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Bagbaba", "Ahmet Cagri", ""], ["Jenihhin", "Maksim", ""], ["Ubar", "Raimund", ""], ["Sauer", "Christian", ""]]}, {"id": "2103.05363", "submitter": "Qigong Sun", "authors": "Qigong Sun, Yan Ren, Licheng Jiao, Xiufang Li, Fanhua Shang, Fang Liu", "title": "MWQ: Multiscale Wavelet Quantized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model quantization can reduce the model size and computational latency, it\nhas become an essential technique for the deployment of deep neural networks on\nresourceconstrained hardware (e.g., mobile phones and embedded devices). The\nexisting quantization methods mainly consider the numerical elements of the\nweights and activation values, ignoring the relationship between elements. The\ndecline of representation ability and information loss usually lead to the\nperformance degradation. Inspired by the characteristics of images in the\nfrequency domain, we propose a novel multiscale wavelet quantization (MWQ)\nmethod. This method decomposes original data into multiscale frequency\ncomponents by wavelet transform, and then quantizes the components of different\nscales, respectively. It exploits the multiscale frequency and spatial\ninformation to alleviate the information loss caused by quantization in the\nspatial domain. Because of the flexibility of MWQ, we demonstrate three\napplications (e.g., model compression, quantized network optimization, and\ninformation enhancement) on the ImageNet and COCO datasets. Experimental\nresults show that our method has stronger representation ability and can play\nan effective role in quantized neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:21:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Sun", "Qigong", ""], ["Ren", "Yan", ""], ["Jiao", "Licheng", ""], ["Li", "Xiufang", ""], ["Shang", "Fanhua", ""], ["Liu", "Fang", ""]]}, {"id": "2103.05436", "submitter": "Pavlos Aimoniotis", "authors": "Pavlos Aimoniotis, Maria Rafaela Gkeka and Nikolaos Bellas", "title": "MapVisual: A Visualization Tool for Memory Access Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memory bandwidth is strongly correlated to the complexity of the memory\naccess pattern of a running application. To improve memory performance of\napplications with irregular and/or unpredictable memory patterns, we need tools\nto analyze these patterns during application development. In this work, we\npresent a software tool for the analysis and visualization of memory access\npatterns. We perform memory tracing and profiling, we do data processing and\nfiltering, and we use visualization algorithms to produce three dimensional\ngraphs that describe the patterns both in space and in time. Finally, we\nevaluate our toolflow on a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:55:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Aimoniotis", "Pavlos", ""], ["Gkeka", "Maria Rafaela", ""], ["Bellas", "Nikolaos", ""]]}, {"id": "2103.05579", "submitter": "Javier Duarte", "authors": "Farah Fahim, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo\n  Jindariani, Nhan Tran, Luca P. Carloni, Giuseppe Di Guglielmo, Philip Harris,\n  Jeffrey Krupa, Dylan Rankin, Manuel Blanco Valentin, Josiah Hester, Yingyi\n  Luo, John Mamish, Seda Orgrenci-Memik, Thea Aarrestad, Hamza Javed, Vladimir\n  Loncar, Maurizio Pierini, Adrian Alan Pol, Sioni Summers, Javier Duarte,\n  Scott Hauck, Shih-Chieh Hsu, Jennifer Ngadiuba, Mia Liu, Duc Hoang, Edward\n  Kreinar, Zhenbin Wu", "title": "hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power\n  Machine Learning Devices", "comments": "10 pages, 8 figures, TinyML Research Symposium 2021", "journal-ref": null, "doi": null, "report-no": "FERMILAB-CONF-21-080-SCD", "categories": "cs.LG cs.AR physics.ins-det", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Accessible machine learning algorithms, software, and diagnostic tools for\nenergy-efficient devices and systems are extremely valuable across a broad\nrange of application domains. In scientific domains, real-time near-sensor\nprocessing can drastically improve experimental design and accelerate\nscientific discoveries. To support domain scientists, we have developed hls4ml,\nan open-source software-hardware codesign workflow to interpret and translate\nmachine learning algorithms for implementation with both FPGA and ASIC\ntechnologies. We expand on previous hls4ml work by extending capabilities and\ntechniques towards low-power implementations and increased usability: new\nPython APIs, quantization-aware pruning, end-to-end FPGA workflows, long\npipeline kernels for low power, and new device backends include an ASIC\nworkflow. Taken together, these and continued efforts in hls4ml will arm a new\ngeneration of domain scientists with accessible, efficient, and powerful tools\nfor machine-learning-accelerated discovery.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:34:44 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:55:42 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 13:40:23 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Fahim", "Farah", ""], ["Hawks", "Benjamin", ""], ["Herwig", "Christian", ""], ["Hirschauer", "James", ""], ["Jindariani", "Sergo", ""], ["Tran", "Nhan", ""], ["Carloni", "Luca P.", ""], ["Di Guglielmo", "Giuseppe", ""], ["Harris", "Philip", ""], ["Krupa", "Jeffrey", ""], ["Rankin", "Dylan", ""], ["Valentin", "Manuel Blanco", ""], ["Hester", "Josiah", ""], ["Luo", "Yingyi", ""], ["Mamish", "John", ""], ["Orgrenci-Memik", "Seda", ""], ["Aarrestad", "Thea", ""], ["Javed", "Hamza", ""], ["Loncar", "Vladimir", ""], ["Pierini", "Maurizio", ""], ["Pol", "Adrian Alan", ""], ["Summers", "Sioni", ""], ["Duarte", "Javier", ""], ["Hauck", "Scott", ""], ["Hsu", "Shih-Chieh", ""], ["Ngadiuba", "Jennifer", ""], ["Liu", "Mia", ""], ["Hoang", "Duc", ""], ["Kreinar", "Edward", ""], ["Wu", "Zhenbin", ""]]}, {"id": "2103.05600", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris, Javier Fernandez-Marques, Nicholas D. Lane", "title": "unzipFPGA: Enhancing FPGA-based CNN Engines with On-the-Fly Weights\n  Generation", "comments": "Accepted at the 29th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines (FCCM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single computation engines have become a popular design choice for FPGA-based\nconvolutional neural networks (CNNs) enabling the deployment of diverse models\nwithout fabric reconfiguration. This flexibility, however, often comes with\nsignificantly reduced performance on memory-bound layers and resource\nunderutilisation due to suboptimal mapping of certain layers on the engine's\nfixed configuration. In this work, we investigate the implications in terms of\nCNN engine design for a class of models that introduce a pre-convolution stage\nto decompress the weights at run time. We refer to these approaches as\non-the-fly. To minimise the negative impact of limited bandwidth on\nmemory-bound layers, we present a novel hardware component that enables the\non-chip on-the-fly generation of weights. We further introduce an input\nselective processing element (PE) design that balances the load between PEs on\nsuboptimally mapped layers. Finally, we present unzipFPGA, a framework to train\non-the-fly models and traverse the design space to select the highest\nperforming CNN engine configuration. Quantitative evaluation shows that\nunzipFPGA yields an average speedup of 2.14x and 71% over optimised status-quo\nand pruned CNN engines under constrained bandwidth and up to 3.69x higher\nperformance density over the state-of-the-art FPGA-based CNN accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:19:41 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 14:15:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Fernandez-Marques", "Javier", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2103.05615", "submitter": "Saraju Mohanty", "authors": "Saswat K. Ram and Sauvagya R. Sahoo and Banee B. Das and Kamalakanta\n  Mahapatra and Saraju P. Mohanty", "title": "Eternal-Thing 2.0: Analog-Trojan Resilient Ripple-Less Solar Energy\n  Harvesting System for Sustainable IoT in Smart Cities and Smart Villages", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, harvesting natural energy is gaining more attention than other\nconventional approaches for sustainable Internet-of-Things (IoT). System on\nchip (SoC) power requirement for the IoT and generating higher voltages on-chip\nis a massive challenge for on-chip peripherals and systems. Many sensors are\nemployed in smart cities and smart villages in decision-making, whose power\nrequirement is an issue, and it must be uninterrupted. Previously, we presented\nSecurity-by-Design (SbD) principle to bring energy dissipation and\ncybersecurity together through our \"Eternal-Thing\". In this paper, an on-chip\nreliable energy harvesting system (EHS) is designed for IoT end node devices\nwhich is called \"Eternal-Thing 2.0\". The management section monitors the\nprocess load and also the recharging of the battery or super-capacitor. An\nefficient maximum power point tracking (MPPT) algorithm is used to avoid\nquiescent power consumption. The reliability of the proposed EHS is improved by\nusing an aging tolerant ring oscillator. The proposed EHS is intended and\nsimulated in CMOS 90nm technology. The output voltage is within the vary of\n3-3.55V with an input of 1-1.5V. The EHS consumes 22 micro Watt of power, that\nsatisfies the ultra-low-power necessities of IoT sensible nodes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:36:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ram", "Saswat K.", ""], ["Sahoo", "Sauvagya R.", ""], ["Das", "Banee B.", ""], ["Mahapatra", "Kamalakanta", ""], ["Mohanty", "Saraju P.", ""]]}, {"id": "2103.05707", "submitter": "Anup Das", "authors": "Twisha Titirsha, Shihao Song, Anup Das, Jeffrey Krichmar, Nikil Dutt,\n  Nagarajan Kandasamy, Francky Catthoor", "title": "Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic\n  Hardware", "comments": "Accepted for publication in IEEE Transactions on Parallel and\n  Distributed Systems (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems are embracing memristors to implement high\ndensity and low power synaptic storage as crossbar arrays in hardware. These\nsystems are energy efficient in executing Spiking Neural Networks (SNNs). We\nobserve that long bitlines and wordlines in a memristive crossbar are a major\nsource of parasitic voltage drops, which create current asymmetry. Through\ncircuit simulations, we show the significant endurance variation that results\nfrom this asymmetry. Therefore, if the critical memristors (ones with lower\nendurance) are overutilized, they may lead to a reduction of the crossbar's\nlifetime. We propose eSpine, a novel technique to improve lifetime by\nincorporating the endurance variation within each crossbar in mapping machine\nlearning workloads, ensuring that synapses with higher activation are always\nimplemented on memristors with higher endurance, and vice versa. eSpine works\nin two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to\npartition a workload into clusters of neurons and synapses, where each cluster\ncan fit in a crossbar. Second, it uses an instance of Particle Swarm\nOptimization (PSO) to map clusters to tiles, where the placement of synapses of\na cluster to memristors of a crossbar is performed by analyzing their\nactivation within the workload. We evaluate eSpine for a state-of-the-art\nneuromorphic hardware model with phase-change memory (PCM)-based memristors.\nUsing 10 SNN workloads, we demonstrate a significant improvement in the\neffective lifetime.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:43:28 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Titirsha", "Twisha", ""], ["Song", "Shihao", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2103.06453", "submitter": "Guangyuan Hu", "authors": "Guangyuan Hu, Zecheng He, Ruby B. Lee", "title": "Smartphone Impostor Detection with Behavioral Data Privacy and\n  Minimalist Hardware Support", "comments": "Accepted by tinyML 2021 Research Symposium. arXiv admin note:\n  substantial text overlap with arXiv:2002.03914", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Impostors are attackers who take over a smartphone and gain access to the\nlegitimate user's confidential and private information. This paper proposes a\ndefense-in-depth mechanism to detect impostors quickly with simple Deep\nLearning algorithms, which can achieve better detection accuracy than the best\nprior work which used Machine Learning algorithms requiring computation of\nmultiple features. Different from previous work, we then consider protecting\nthe privacy of a user's behavioral (sensor) data by not exposing it outside the\nsmartphone. For this scenario, we propose a Recurrent Neural Network (RNN)\nbased Deep Learning algorithm that uses only the legitimate user's sensor data\nto learn his/her normal behavior. We propose to use Prediction Error\nDistribution (PED) to enhance the detection accuracy. We also show how a\nminimalist hardware module, dubbed SID for Smartphone Impostor Detector, can be\ndesigned and integrated into smartphones for self-contained impostor detection.\nExperimental results show that SID can support real-time impostor detection, at\na very low hardware cost and energy consumption, compared to other RNN\naccelerators.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:39:53 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:31:49 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hu", "Guangyuan", ""], ["He", "Zecheng", ""], ["Lee", "Ruby B.", ""]]}, {"id": "2103.06506", "submitter": "Corey Lammie", "authors": "Corey Lammie, Jason K. Eshraghian, Wei D. Lu, Mostafa Rahimi Azghadi", "title": "Memristive Stochastic Computing for Deep Learning Parameter Optimization", "comments": "Accepted by IEEE Transactions on Circuits and Systems Part II:\n  Express Briefs", "journal-ref": "IEEE Transactions on Circuits and Systems Part II: Express Briefs,\n  2021", "doi": "10.1109/TCSII.2021.3065932", "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Computing (SC) is a computing paradigm that allows for the\nlow-cost and low-power computation of various arithmetic operations using\nstochastic bit streams and digital logic. In contrast to conventional\nrepresentation schemes used within the binary domain, the sequence of bit\nstreams in the stochastic domain is inconsequential, and computation is usually\nnon-deterministic. In this brief, we exploit the stochasticity during switching\nof probabilistic Conductive Bridging RAM (CBRAM) devices to efficiently\ngenerate stochastic bit streams in order to perform Deep Learning (DL)\nparameter optimization, reducing the size of Multiply and Accumulate (MAC)\nunits by 5 orders of magnitude. We demonstrate that in using a 40-nm\nComplementary Metal Oxide Semiconductor (CMOS) process our scalable\narchitecture occupies 1.55mm$^2$ and consumes approximately 167$\\mu$W when\noptimizing parameters of a Convolutional Neural Network (CNN) while it is being\ntrained for a character recognition task, observing no notable reduction in\naccuracy post-training.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:10:32 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Lammie", "Corey", ""], ["Eshraghian", "Jason K.", ""], ["Lu", "Wei D.", ""], ["Azghadi", "Mostafa Rahimi", ""]]}, {"id": "2103.06653", "submitter": "Xinfeng Xie", "authors": "Xinfeng Xie, Peng Gu, Yufei Ding, Dimin Niu, Hongzhong Zheng, Yuan Xie", "title": "MPU: Towards Bandwidth-abundant SIMT Processor via Near-bank Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing number of data-intensive workloads, GPU, which is the\nstate-of-the-art single-instruction-multiple-thread (SIMT) processor, is\nhindered by the memory bandwidth wall. To alleviate this bottleneck, previously\nproposed 3D-stacking near-bank computing accelerators benefit from abundant\nbank-internal bandwidth by bringing computations closer to the DRAM banks.\nHowever, these accelerators are specialized for certain application domains\nwith simple architecture data paths and customized software mapping schemes.\nFor general purpose scenarios, lightweight hardware designs for diverse data\npaths, architectural supports for the SIMT programming model, and end-to-end\nsoftware optimizations remain challenging.\n  To address these issues, we propose MPU (Memory-centric Processing Unit), the\nfirst SIMT processor based on 3D-stacking near-bank computing architecture.\nFirst, to realize diverse data paths with small overheads while leveraging\nbank-level bandwidth, MPU adopts a hybrid pipeline with the capability of\noffloading instructions to near-bank compute-logic. Second, we explore two\narchitectural supports for the SIMT programming model, including a near-bank\nshared memory design and a multiple activated row-buffers enhancement. Third,\nwe present an end-to-end compilation flow for MPU to support CUDA programs. To\nfully utilize MPU's hybrid pipeline, we develop a backend optimization for the\ninstruction offloading decision. The evaluation results of MPU demonstrate\n3.46x speedup and 2.57x energy reduction compared with an NVIDIA Tesla V100 GPU\non a set of representative data-intensive workloads.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 13:19:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xie", "Xinfeng", ""], ["Gu", "Peng", ""], ["Ding", "Yufei", ""], ["Niu", "Dimin", ""], ["Zheng", "Hongzhong", ""], ["Xie", "Yuan", ""]]}, {"id": "2103.06656", "submitter": "Stefan Wagner", "authors": "Ilia Polian and Jens Anders and Steffen Becker and Paolo Bernardi and\n  Krishnendu Chakrabarty and Nourhan ElHamawy and Matthias Sauer and Adit Singh\n  and Matteo Sonza Reorda and Stefan Wagner", "title": "Exploring the Mysteries of System-Level Test", "comments": "7 pages, 2 figures", "journal-ref": "2020 IEEE 29th Asian Test Symposium (ATS)", "doi": "10.1109/ATS49688.2020.9301557", "report-no": null, "categories": "cs.AR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System-level test, or SLT, is an increasingly important process step in\ntoday's integrated circuit testing flows. Broadly speaking, SLT aims at\nexecuting functional workloads in operational modes. In this paper, we\nconsolidate available knowledge about what SLT is precisely and why it is used\ndespite its considerable costs and complexities. We discuss the types or\nfailures covered by SLT, and outline approaches to quality assessment, test\ngeneration and root-cause diagnosis in the context of SLT. Observing that the\ntheoretical understanding for all these questions has not yet reached the level\nof maturity of the more conventional structural and functional test methods, we\noutline new and promising directions for methodical developments leveraging on\nrecent findings from software engineering.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 13:29:19 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Polian", "Ilia", ""], ["Anders", "Jens", ""], ["Becker", "Steffen", ""], ["Bernardi", "Paolo", ""], ["Chakrabarty", "Krishnendu", ""], ["ElHamawy", "Nourhan", ""], ["Sauer", "Matthias", ""], ["Singh", "Adit", ""], ["Reorda", "Matteo Sonza", ""], ["Wagner", "Stefan", ""]]}, {"id": "2103.06709", "submitter": "Toygun Basaklar", "authors": "Toygun Basaklar, Yigit Tuncel, Shruti Yadav Narayana, Suat Gumussoy,\n  and Umit Y. Ogras", "title": "Hypervector Design for Efficient Hyperdimensional Computing on Edge\n  Devices", "comments": "9 pages, 6 figures, accepted to tinyML 2021 Research Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hyperdimensional computing (HDC) has emerged as a new light-weight learning\nalgorithm with smaller computation and energy requirements compared to\nconventional techniques. In HDC, data points are represented by\nhigh-dimensional vectors (hypervectors), which are mapped to high-dimensional\nspace (hyperspace). Typically, a large hypervector dimension ($\\geq1000$) is\nrequired to achieve accuracies comparable to conventional alternatives.\nHowever, unnecessarily large hypervectors increase hardware and energy costs,\nwhich can undermine their benefits. This paper presents a technique to minimize\nthe hypervector dimension while maintaining the accuracy and improving the\nrobustness of the classifier. To this end, we formulate the hypervector design\nas a multi-objective optimization problem for the first time in the literature.\nThe proposed approach decreases the hypervector dimension by more than\n$32\\times$ while maintaining or increasing the accuracy achieved by\nconventional HDC. Experiments on a commercial hardware platform show that the\nproposed approach achieves more than one order of magnitude reduction in model\nsize, inference time, and energy consumption. We also demonstrate the trade-off\nbetween accuracy and robustness to noise and provide Pareto front solutions as\na design parameter in our hypervector design.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:25:45 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Basaklar", "Toygun", ""], ["Tuncel", "Yigit", ""], ["Narayana", "Shruti Yadav", ""], ["Gumussoy", "Suat", ""], ["Ogras", "Umit Y.", ""]]}, {"id": "2103.06743", "submitter": "McKenzie Van Der Hagen", "authors": "McKenzie van der Hagen, Brandon Lucia", "title": "Practical Encrypted Computing for IoT Clients", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy and energy are primary concerns for sensor devices that offload\ncompute to a potentially untrusted edge server or cloud. Homomorphic Encryption\n(HE) enables offload processing of encrypted data. HE offload processing\nretains data privacy, but is limited by the need for frequent communication\nbetween the client device and the offload server. Existing client-aided\nencrypted computing systems are optimized for performance on the offload\nserver, failing to sufficiently address client costs, and precluding HE offload\nfor low-resource (e.g., IoT) devices. We introduce Client-aided HE for Opaque\nCompute Offloading (CHOCO), a client-optimized system for encrypted offload\nprocessing. CHOCO introduces rotational redundancy, an algorithmic optimization\nto minimize computing and communication costs. We design Client-Aided HE for\nOpaque Compute Offloading Through Accelerated Cryptographic Operations\n(CHOCO-TACO), a comprehensive architectural accelerator for client-side\ncryptographic operations that eliminates most of their time and energy costs.\nOur evaluation shows that CHOCO makes client-aided HE offloading feasible for\nresource-constrained clients. Compared to existing encrypted computing\nsolutions, CHOCO reduces communication cost by up to 2948x. With hardware\nsupport, client-side encryption/decryption is faster by 1094x and uses 648x\nless energy. In our end-to-end implementation of a large-scale DNN (VGG16),\nCHOCO uses 37% less energy than local (unencrypted) computation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:44:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["van der Hagen", "McKenzie", ""], ["Lucia", "Brandon", ""]]}, {"id": "2103.07750", "submitter": "Thomas Luinaud", "authors": "Thomas Luinaud, Jeferson Santiago da Silva, J.M. Pierre Langlois, Yvon\n  Savaria", "title": "Design Principles for Packet Deparsers on FPGAs", "comments": "Presented at ISFPGA'21, 2021 Source code available at :\n  https://github.com/luinaudt/deparser/tree/FPGA_paper", "journal-ref": null, "doi": "10.1145/3431920.3439303", "report-no": null, "categories": "cs.AR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The P4 language has drastically changed the networking field as it allows to\nquickly describe and implement new networking applications. Although a large\nvariety of applications can be described with the P4 language, current\nprogrammable switch architectures impose significant constraints on P4\nprograms. To address this shortcoming, FPGAs have been explored as potential\ntargets for P4 applications. P4 applications are described using three\nabstractions: a packet parser, match-action tables, and a packet deparser,\nwhich reassembles the output packet with the result of the match-action tables.\nWhile implementations of packet parsers and match-action tables on FPGAs have\nbeen widely covered in the literature, no general design principles have been\npresented for the packet deparser. Indeed, implementing a high-speed and\nefficient deparser on FPGAs remains an open issue because it requires a large\namount of interconnections and the architecture must be tailored to a P4\nprogram. As a result, in several works where a P4 application is implemented on\nFPGAs, the deparser consumes a significant proportion of chip resources. Hence,\nin this paper, we address this issue by presenting design principles for\nefficient and high-speed deparsers on FPGAs. As an artifact, we introduce a\ntool that generates an efficient vendor-agnostic deparser architecture from a\nP4 program. Our design has been validated and simulated with a cocotb-based\nframework. The resulting architecture is implemented on Xilinx Ultrascale+\nFPGAs and supports a throughput of more than 200 Gbps while reducing resource\nusage by almost 10$\\times$ compared to other solutions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 16:58:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luinaud", "Thomas", ""], ["da Silva", "Jeferson Santiago", ""], ["Langlois", "J. M. Pierre", ""], ["Savaria", "Yvon", ""]]}, {"id": "2103.07977", "submitter": "Raveesh Garg", "authors": "Raveesh Garg, Eric Qin, Francisco Mu\\~noz Mart\\'inez, Robert Guirado,\n  Akshay Jain, Sergi Abadal, Jos\\'e L. Abell\\'an, Manuel E. Acacio, Eduard\n  Alarc\\'on, Sivasankaran Rajamanickam, Tushar Krishna", "title": "A Taxonomy for Classification and Comparison of Dataflows for GNN\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Graph Neural Networks (GNNs) have received a lot of interest\nbecause of their success in learning representations from graph structured\ndata. However, GNNs exhibit different compute and memory characteristics\ncompared to traditional Deep Neural Networks (DNNs). Graph convolutions require\nfeature aggregations from neighboring nodes (known as the aggregation phase),\nwhich leads to highly irregular data accesses. GNNs also have a very regular\ncompute phase that can be broken down to matrix multiplications (known as the\ncombination phase). All recently proposed GNN accelerators utilize different\ndataflows and microarchitecture optimizations for these two phases. Different\ncommunication strategies between the two phases have been also used. However,\nas more custom GNN accelerators are proposed, the harder it is to qualitatively\nclassify them and quantitatively contrast them. In this work, we present a\ntaxonomy to describe several diverse dataflows for running GNN inference on\naccelerators. This provides a structured way to describe and compare the\ndesign-space of GNN accelerators.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:14:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Garg", "Raveesh", ""], ["Qin", "Eric", ""], ["Mart\u00ednez", "Francisco Mu\u00f1oz", ""], ["Guirado", "Robert", ""], ["Jain", "Akshay", ""], ["Abadal", "Sergi", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Acacio", "Manuel E.", ""], ["Alarc\u00f3n", "Eduard", ""], ["Rajamanickam", "Sivasankaran", ""], ["Krishna", "Tushar", ""]]}, {"id": "2103.08392", "submitter": "Yexin Yan", "authors": "Sebastian H\\\"oppner, Yexin Yan, Andreas Dixius, Stefan Scholze,\n  Johannes Partzsch, Marco Stolba, Florian Kelber, Bernhard Vogginger, Felix\n  Neum\\\"arker, Georg Ellguth, Stephan Hartmann, Stefan Schiefer, Thomas Hocker,\n  Dennis Walter, Genting Liu, Jim Garside, Steve Furber, Christian Mayr", "title": "The SpiNNaker 2 Processing Element Architecture for Hybrid Digital\n  Neuromorphic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the processing element architecture of the second\ngeneration SpiNNaker chip, implemented in 22nm FDSOI. On circuit level, the\nchip features adaptive body biasing for near-threshold operation, and dynamic\nvoltage-and-frequency scaling driven by spiking activity. On system level,\nprocessing is centered around an ARM M4 core, similar to the processor-centric\narchitecture of the first generation SpiNNaker. To speed operation of subtasks,\nwe have added accelerators for numerical operations of both spiking (SNN) and\nrate based (deep) neural networks (DNN). PEs communicate via a dedicated,\ncustom-designed network-on-chip. We present three benchmarks showing operation\nof the whole processor element on SNN, DNN and hybrid SNN/DNN networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:59:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["H\u00f6ppner", "Sebastian", ""], ["Yan", "Yexin", ""], ["Dixius", "Andreas", ""], ["Scholze", "Stefan", ""], ["Partzsch", "Johannes", ""], ["Stolba", "Marco", ""], ["Kelber", "Florian", ""], ["Vogginger", "Bernhard", ""], ["Neum\u00e4rker", "Felix", ""], ["Ellguth", "Georg", ""], ["Hartmann", "Stephan", ""], ["Schiefer", "Stefan", ""], ["Hocker", "Thomas", ""], ["Walter", "Dennis", ""], ["Liu", "Genting", ""], ["Garside", "Jim", ""], ["Furber", "Steve", ""], ["Mayr", "Christian", ""]]}, {"id": "2103.08828", "submitter": "Sudeep Pasricha", "authors": "Febin Sunny, Asif Mirza, Ishan Thakkar, Mahdi Nikdast, and Sudeep\n  Pasricha", "title": "ARXON: A Framework for Approximate Communication over Photonic\n  Networks-on-Chip", "comments": "arXiv admin note: text overlap with arXiv:2002.11289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The approximate computing paradigm advocates for relaxing accuracy goals in\napplications to improve energy-efficiency and performance. Recently, this\nparadigm has been explored to improve the energy-efficiency of silicon photonic\nnetworks-on-chip (PNoCs). Silicon photonic interconnects suffer from high power\ndissipation because of laser sources, which generate carrier wavelengths, and\ntuning power required for regulating photonic devices under different\nuncertainties. In this paper, we propose a framework called ARXON to reduce\nsuch power dissipation overhead by enabling intelligent and aggressive\napproximation during communication over silicon photonic links in PNoCs. Our\nframework reduces laser and tuning-power overhead while intelligently\napproximating communication, such that application output quality is not\ndistorted beyond an acceptable limit. Simulation results show that our\nframework can achieve up to 56.4% lower laser power consumption and up to 23.8%\nbetter energy-efficiency than the best-known prior work on approximate\ncommunication with silicon photonic interconnects and for the same application\noutput quality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:25:04 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sunny", "Febin", ""], ["Mirza", "Asif", ""], ["Thakkar", "Ishan", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2103.08910", "submitter": "Mats Brorsson", "authors": "Bobby Sleeba and Mikael Collin and Mats Brorsson", "title": "An ASIC Implementation and Evaluation of a Profiled Low-Energy\n  Instruction Set Architecture Extension", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an extension to an existing instruction set architecture,\nwhich gains considerable reduction in power consumption. The reduction in power\nconsumption is achieved through coding of the most commonly executed\ninstructions in a short format done by the compiler based on a profile of\nprevious executions. This leads to fewer accesses to the instruction cache and\nthat more instructions can fit in the cache. As a secondary effect, this turned\nout to be very beneficial in terms of power. Another major advantage, which is\nthe main concern of this paper is the reduction in the number of instruction\nfetch cycles which will also contribute significantly towards reduction in\npower consumption. The work involves implementing the new processor\narchitecture in ASIC and estimation of power-consumption compared to the normal\narchitecture.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:34:39 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sleeba", "Bobby", ""], ["Collin", "Mikael", ""], ["Brorsson", "Mats", ""]]}, {"id": "2103.09301", "submitter": "Jacob Stevens", "authors": "Jacob R. Stevens, Rangharajan Venkatesan, Steve Dai, Brucek Khailany,\n  Anand Raghunathan", "title": "Softermax: Hardware/Software Co-Design of an Efficient Softmax for\n  Transformers", "comments": "To appear in Proceedings of the 58th Design Automation Conference\n  (DAC '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformers have transformed the field of natural language processing. This\nperformance is largely attributed to the use of stacked self-attention layers,\neach of which consists of matrix multiplies as well as softmax operations. As a\nresult, unlike other neural networks, the softmax operation accounts for a\nsignificant fraction of the total run-time of Transformers. To address this, we\npropose Softermax, a hardware-friendly softmax design. Softermax consists of\nbase replacement, low-precision softmax computations, and an online\nnormalization calculation. We show Softermax results in 2.35x the energy\nefficiency at 0.90x the size of a comparable baseline, with negligible impact\non network accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:04:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Stevens", "Jacob R.", ""], ["Venkatesan", "Rangharajan", ""], ["Dai", "Steve", ""], ["Khailany", "Brucek", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2103.09523", "submitter": "Keisuke Sugiura", "authors": "Keisuke Sugiura and Hiroki Matsutani", "title": "Particle Filter-based vs. Graph-based: SLAM Acceleration on Low-end\n  FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SLAM allows a robot to continuously perceive the surrounding environment and\nlocate itself correctly. However, its high computational complexity limits the\npractical use of SLAM in resource-constrained computing platforms. We propose a\nresource-efficient FPGA-based accelerator and apply it to two major SLAM\nmethods: particle filter-based and graph-based SLAM. We compare their\nperformances in terms of the latency, throughput gain, and memory consumption,\nconsidering their algorithmic characteristics, and confirm that the accelerator\nremoves the bottleneck without compromising the accuracy in both methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:12:31 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Sugiura", "Keisuke", ""], ["Matsutani", "Hiroki", ""]]}, {"id": "2103.09791", "submitter": "Mineto Tsukada", "authors": "Mineto Tsukada, Hiroki Matsutani", "title": "An Overflow/Underflow-Free Fixed-Point Bit-Width Optimization Method for\n  OS-ELM Digital Circuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there has been increasing demand for real-time training on\nresource-limited IoT devices such as smart sensors, which realizes standalone\nonline adaptation for streaming data without data transfers to remote servers.\nOS-ELM (Online Sequential Extreme Learning Machine) has been one of promising\nneural-network-based online algorithms for on-chip learning because it can\nperform online training at low computational cost and is easy to implement as a\ndigital circuit. Existing OS-ELM digital circuits employ fixed-point data\nformat and the bit-widths are often manually tuned, however, this may cause\noverflow or underflow which can lead to unexpected behavior of the circuit. For\non-chip learning systems, an overflow/underflow-free design has a great impact\nsince online training is continuously performed and the intervals of\nintermediate variables will dynamically change as time goes by. In this paper,\nwe propose an overflow/underflow-free bit-width optimization method for\nfixed-point digital circuits of OS-ELM. Experimental results show that our\nmethod realizes overflow/underflow-free OS-ELM digital circuits with 1.0x -\n1.5x more area cost compared to the baseline simulation method where overflow\nor underflow can happen.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:24:46 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 11:14:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Tsukada", "Mineto", ""], ["Matsutani", "Hiroki", ""]]}, {"id": "2103.10040", "submitter": "Francesco Sgherzi", "authors": "Francesco Sgherzi, Alberto Parravicini, Marco Siracusa, Marco Domenico\n  Santambrogio", "title": "Solving Large Top-K Graph Eigenproblems with a Memory and\n  Compute-optimized FPGA Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale eigenvalue computations on sparse matrices are a key component of\ngraph analytics techniques based on spectral methods. In such applications, an\nexhaustive computation of all eigenvalues and eigenvectors is impractical and\nunnecessary, as spectral methods can retrieve the relevant properties of\nenormous graphs using just the eigenvectors associated with the Top-K largest\neigenvalues.\n  In this work, we propose a hardware-optimized algorithm to approximate a\nsolution to the Top-K eigenproblem on sparse matrices representing large graph\ntopologies. We prototype our algorithm through a custom FPGA hardware design\nthat exploits HBM, Systolic Architectures, and mixed-precision arithmetic. We\nachieve a speedup of 6.22x compared to the highly optimized ARPACK library\nrunning on an 80-thread CPU, while keeping high accuracy and 49x better power\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 06:23:03 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sgherzi", "Francesco", ""], ["Parravicini", "Alberto", ""], ["Siracusa", "Marco", ""], ["Santambrogio", "Marco Domenico", ""]]}, {"id": "2103.10515", "submitter": "Robert Guirado", "authors": "Robert Guirado, Akshay Jain, Sergi Abadal and Eduard Alarc\\'on", "title": "Characterizing the Communication Requirements of GNN Accelerators: A\n  Model-Based Approach", "comments": "ISCAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data present in real world graph representations demands for tools\ncapable to study it accurately. In this regard Graph Neural Network (GNN) is a\npowerful tool, wherein various models for it have also been developed over the\npast decade. Recently, there has been a significant push towards creating\naccelerators that speed up the inference and training process of GNNs. These\naccelerators, however, do not delve into the impact of their dataflows on the\noverall data movement and, hence, on the communication requirements. In this\npaper, we formulate analytical models that capture the amount of data movement\nin the most recent GNN accelerator frameworks. Specifically, the proposed\nmodels capture the dataflows and hardware setup of these accelerator designs\nand expose their scalability characteristics for a set of hardware, GNN model\nand input graph parameters. Additionally, the proposed approach provides means\nfor the comparative analysis of the vastly different GNN accelerators.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:49:28 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Guirado", "Robert", ""], ["Jain", "Akshay", ""], ["Abadal", "Sergi", ""], ["Alarc\u00f3n", "Eduard", ""]]}, {"id": "2103.10692", "submitter": "Christos Sakalis", "authors": "Christos Sakalis, Stefanos Kaxiras and Magnus Sj\\\"alander", "title": "Selectively Delaying Instructions to Prevent Microarchitectural Replay\n  Attacks", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  MicroScope, and microarchitectural replay attacks in general, take advantage\nof the characteristics of speculative execution to trap the execution of the\nvictim application in an infinite loop, enabling the attacker to amplify a\nside-channel attack by executing it indefinitely. Due to the nature of the\nreplay, it can be used to effectively attack security critical trusted\nexecution environments (secure enclaves), even under conditions where a\nside-channel attack would not be possible. At the same time, unlike speculative\nside-channel attacks, MicroScope can be used to amplify the correct path of\nexecution, rendering many existing speculative side-channel defences\nineffective.\n  In this work, we generalize microarchitectural replay attacks beyond\nMicroScope and present an efficient defence against them. We make the\nobservation that such attacks rely on repeated squashes of so-called \"replay\nhandles\" and that the instructions causing the side-channel must reside in the\nsame reorder buffer window as the handles. We propose Delay-on-Squash, a\ntechnique for tracking squashed instructions and preventing them from being\nreplayed by speculative replay handles. Our evaluation shows that it is\npossible to achieve full security against microarchitectural replay attacks\nwith very modest hardware requirements, while still maintaining 97% of the\ninsecure baseline performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:53:02 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Sakalis", "Christos", ""], ["Kaxiras", "Stefanos", ""], ["Sj\u00e4lander", "Magnus", ""]]}, {"id": "2103.10836", "submitter": "Jacob Stevens", "authors": "Jacob R. Stevens, Dipankar Das, Sasikanth Avancha, Bharat Kaul, Anand\n  Raghunathan", "title": "GNNerator: A Hardware/Software Framework for Accelerating Graph Neural\n  Networks", "comments": "To appear in Proceedings of the 58th Design Automation Conference\n  (DAC '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph Neural Networks (GNNs) use a fully-connected layer to extract features\nfrom the nodes of a graph and aggregate these features using message passing\nbetween nodes, combining two distinct computational patterns: dense, regular\ncomputations and sparse, irregular computations.\n  To address this challenge, we propose GNNerator, an accelerator with\nheterogeneous compute engines optimized for these two patterns.\n  Further, GNNerator implements feature-blocking, a novel GNN dataflow that\nbeneficially trades off irregular memory accesses during aggregation for\nregular memory accesses during feature extraction. We show GNNerator achieves\nspeedups of 5.7-37x over an NVIDIA RTX 2080-Ti, and 2.3x-3.8x over HyGCN, a\nstate-of-the-art GNN accelerator.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:36:01 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Stevens", "Jacob R.", ""], ["Das", "Dipankar", ""], ["Avancha", "Sasikanth", ""], ["Kaul", "Bharat", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2103.12166", "submitter": "Anup Das", "authors": "Shamik Kundu, Kanad Basu, Mehdi Sadi, Twisha Titirsha, Shihao Song,\n  Anup Das, Ujjwal Guin", "title": "Special Session: Reliability Analysis for ML/AI Hardware", "comments": "To appear at VLSI Test Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive\nin today's applications, such as autonomous vehicles, healthcare, aerospace,\ncybersecurity, and many critical applications. Ensuring the reliability and\nrobustness of the underlying AI/ML hardware becomes our paramount importance.\nIn this paper, we explore and evaluate the reliability of different AI/ML\nhardware. The first section outlines the reliability issues in a commercial\nsystolic array-based ML accelerator in the presence of faults engendering from\ndevice-level non-idealities in the DRAM. Next, we quantified the impact of\ncircuit-level faults in the MSB and LSB logic cones of the Multiply and\nAccumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we\npresent two key reliability issues -- circuit aging and endurance in emerging\nneuromorphic hardware platforms and present our system-level approach to\nmitigate them.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:24:45 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 01:12:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kundu", "Shamik", ""], ["Basu", "Kanad", ""], ["Sadi", "Mehdi", ""], ["Titirsha", "Twisha", ""], ["Song", "Shihao", ""], ["Das", "Anup", ""], ["Guin", "Ujjwal", ""]]}, {"id": "2103.12231", "submitter": "Anup Das", "authors": "Twisha Titirsha, Shihao Song, Adarsha Balaji, Anup Das", "title": "On the Role of System Software in Energy Management of Neuromorphic\n  Computing", "comments": "To appear in 18th Computer Frontiers 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic computing systems such as DYNAPs and Loihi have recently been\nintroduced to the computing community to improve performance and energy\nefficiency of machine learning programs, especially those that are implemented\nusing Spiking Neural Network (SNN). The role of a system software for\nneuromorphic systems is to cluster a large machine learning model (e.g., with\nmany neurons and synapses) and map these clusters to the computing resources of\nthe hardware. In this work, we formulate the energy consumption of a\nneuromorphic hardware, considering the power consumed by neurons and synapses,\nand the energy consumed in communicating spikes on the interconnect. Based on\nsuch formulation, we first evaluate the role of a system software in managing\nthe energy consumption of neuromorphic systems. Next, we formulate a simple\nheuristic-based mapping approach to place the neurons and synapses onto the\ncomputing resources to reduce energy consumption. We evaluate our approach with\n10 machine learning applications and demonstrate that the proposed mapping\napproach leads to a significant reduction of energy consumption of neuromorphic\ncomputing systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:31:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Titirsha", "Twisha", ""], ["Song", "Shihao", ""], ["Balaji", "Adarsha", ""], ["Das", "Anup", ""]]}, {"id": "2103.12393", "submitter": "Mingzhe Zhang", "authors": "Taoran Xiang, Lunkai Zhang, Shuqian An, Xiaochun Ye, Mingzhe Zhang,\n  Yanhuan Liu, Mingyu Yan, Da Wang, Hao Zhang, Wenming Li, Ninghui Sun, Dongrui\n  Fan", "title": "RISC-NN: Use RISC, NOT CISC as Neural Network Hardware Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Networks (NN) have been proven to be powerful tools to analyze Big\nData. However, traditional CPUs cannot achieve the desired performance and/or\nenergy efficiency for NN applications. Therefore, numerous NN accelerators have\nbeen used or designed to meet these goals. These accelerators all fall into\nthree categories: GPGPUs, ASIC NN Accelerators and CISC NN Accelerators. Though\nCISC NN Accelerators can achieve considerable smaller memory footprint than\nGPGPU thus improve energy efficiency; they still fail to provide same level of\ndata reuse optimization achieved by ASIC NN Accelerators because of the\ninherited poor pragrammability of their CISC architecture. We argue that, for\nNN Accelerators, RISC is a better design choice than CISC, as is the case with\ngeneral purpose processors. We propose RISC-NN, a novel many-core RISC-based NN\naccelerator that achieves high expressiveness and high parallelism and features\nstrong programmability and low control-hardware costs. We show that, RISC-NN\ncan implement all the necessary instructions of state-of-the-art CISC NN\nAccelerators; in the meantime, RISC-NN manages to achieve advanced optimization\nsuch as multiple-level data reuse and support for Sparse NN applications which\npreviously only existed in ASIC NN Accelerators. Experiment results show that,\nRISC-NN achieves on average 11.88X performance efficiency compared with\nstate-of-the-art Nvidia TITAN Xp GPGPU for various NN applications. RISC-NN\nalso achieves on average 1.29X, 8.37X and 21.71X performance efficiency over\nCISC-based TPU in CNN, MLP and LSTM applications, respectively. Finally,\nRISC-NN can achieve additional 26.05% performance improvement and 33.13% energy\nreduction after applying pruning for Sparse NN applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:49:50 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Xiang", "Taoran", ""], ["Zhang", "Lunkai", ""], ["An", "Shuqian", ""], ["Ye", "Xiaochun", ""], ["Zhang", "Mingzhe", ""], ["Liu", "Yanhuan", ""], ["Yan", "Mingyu", ""], ["Wang", "Da", ""], ["Zhang", "Hao", ""], ["Li", "Wenming", ""], ["Sun", "Ninghui", ""], ["Fan", "Dongrui", ""]]}, {"id": "2103.12433", "submitter": "Dmytro Petryk", "authors": "Dmytro Petryk and Zoya Dyka and Peter Langendoerfer", "title": "Sensitivity of Standard Library Cells to Optical Fault Injection Attacks\n  in IHP 250 nm Technology", "comments": null, "journal-ref": null, "doi": "10.1109/MECO49872.2020.9134146", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The IoT consists of a lot of devices such as embedded systems, wireless\nsensor nodes (WSNs), control systems, etc. It is essential for some of these\ndevices to protect information that they process and transmit. The issue is\nthat an adversary may steal these devices to gain a physical access to the\ndevice. There is a variety of ways that allows to reveal cryptographic keys.\nOne of them are optical Fault Injection attacks. We performed successful\noptical Fault Injections into different type of gates, in particular INV, NAND,\nNOR, FF. In our work we concentrate on the selection of the parameters\nconfigured by an attacker and their influence on the success of the Fault\nInjections.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:23:58 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Petryk", "Dmytro", ""], ["Dyka", "Zoya", ""], ["Langendoerfer", "Peter", ""]]}, {"id": "2103.12435", "submitter": "Dmytro Petryk", "authors": "Dmytro Petryk and Zoya Dyka and Eduardo Perez and Mamathamba\n  Kalishettyhalli Mahadevaiaha and Ievgen Kabin and Christian Wenger and Peter\n  Langendoerfer", "title": "Evaluation of the Sensitivity of RRAM Cells to Optical Fault Injection\n  Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/DSD51259.2020.00047", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Resistive Random Access Memory (RRAM) is a type of Non-Volatile Memory (NVM).\nIn this paper we investigate the sensitivity of the TiN/Ti/Al:HfO2/TiN-based\n1T-1R RRAM cells implemented in a 250 nm CMOS IHP technology to the laser\nirradiation in detail. Experimental results show the feasibility to influence\nthe state of the cells under laser irradiation, i.e. successful optical Fault\nInjection. We focus on the selection of the parameters of the laser station and\ntheir influence on the success of optical Fault Injections.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:26:29 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Petryk", "Dmytro", ""], ["Dyka", "Zoya", ""], ["Perez", "Eduardo", ""], ["Mahadevaiaha", "Mamathamba Kalishettyhalli", ""], ["Kabin", "Ievgen", ""], ["Wenger", "Christian", ""], ["Langendoerfer", "Peter", ""]]}, {"id": "2103.12436", "submitter": "Dmytro Petryk", "authors": "Dmytro Petryk and Zoya Dyka and Jens Katzer and Peter Langendoerfer", "title": "Metal Fillers as Potential Low Cost Countermeasure against Optical Fault\n  Injection Attacks", "comments": null, "journal-ref": null, "doi": "10.1109/EWDTS50664.2020.9225092", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Physically accessible devices such as sensor nodes in Wireless Sensor\nNetworks or \"smart\" devices in the Internet of Things have to be resistant to a\nbroad spectrum of physical attacks, for example to Side Channel Analysis and to\nFault Injection attacks. In this work we concentrate on the vulnerability of\nASICs to precise optical Fault Injection attacks. Here we propose to use metal\nfillers as potential low-cost countermeasure that may be effective against a\nbroad spectrum of physical attacks. In our future work we plan to evaluate\ndifferent methods of metal fillers placement, to select an effective one and to\nintegrate it as additional design rules into automated design flows.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:28:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Petryk", "Dmytro", ""], ["Dyka", "Zoya", ""], ["Katzer", "Jens", ""], ["Langendoerfer", "Peter", ""]]}, {"id": "2103.12928", "submitter": "Sashidhar Jakkamsetti", "authors": "Ivan De Oliveira Nunes and Sashidhar Jakkamsetti and Gene Tsudik", "title": "DIALED: Data Integrity Attestation for Low-end Embedded Devices", "comments": "6 pages, to be published in DAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying integrity of software execution in low-end micro-controller units\n(MCUs) is a well-known open problem. The central challenge is how to securely\ndetect software exploits with minimal overhead, since these MCUs are designed\nfor low cost, low energy and small size. Some recent work yielded inexpensive\nhardware/software co-designs for remotely verifying code and execution\nintegrity. In particular, a means of detecting unauthorized code modifications\nand control-flow attacks were proposed, referred to as Remote Attestation (RA)\nand Control-Flow Attestation (CFA), respectively. Despite this progress,\ndetection of data-only attacks remains elusive. Such attacks exploit software\nvulnerabilities to corrupt intermediate computation results stored in data\nmemory, changing neither the program code nor its control flow. Motivated by\nlack of any current techniques (for low-end MCUs) that detect these attacks, in\nthis paper we propose, implement and evaluate DIALED, the first Data-Flow\nAttestation (DFA) technique applicable to the most resource-constrained\nembedded devices (e.g., TI MSP430). DIALED works in tandem with a companion CFA\nscheme to detect all (currently known) types of runtime software exploits at\nfairly low cost.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:56:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nunes", "Ivan De Oliveira", ""], ["Jakkamsetti", "Sashidhar", ""], ["Tsudik", "Gene", ""]]}, {"id": "2103.13060", "submitter": "Serena Curzel", "authors": "Serena Curzel, Nicol\\`o Ghielmetti, Michele Fiorito and Fabrizio\n  Ferrandi", "title": "De-specializing an HLS library for Deep Neural Networks: improvements\n  upon hls4ml", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom hardware accelerators for Deep Neural Networks are increasingly\npopular: in fact, the flexibility and performance offered by FPGAs are\nwell-suited to the computational effort and low latency constraints required by\nmany image recognition and natural language processing tasks. The gap between\nhigh-level Machine Learning frameworks (e.g., Tensorflow, Pytorch) and\nlow-level hardware design in Verilog/VHDL creates a barrier to widespread\nadoption of FPGAs, which can be overcome with the help of High-Level Synthesis.\nhls4ml is a framework that translates Deep Neural Networks into annotated C++\ncode for High-Level Synthesis, offering a complete and user-friendly design\nprocess that has been enthusiastically adopted in physics research. We analyze\nthe strengths and weaknesses of hls4ml, drafting a plan to enhance its core\nlibrary of components in order to allow more advanced optimizations, target a\nwider selection of FPGAs, and support larger Neural Network models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:16:45 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Curzel", "Serena", ""], ["Ghielmetti", "Nicol\u00f2", ""], ["Fiorito", "Michele", ""], ["Ferrandi", "Fabrizio", ""]]}, {"id": "2103.14007", "submitter": "Dana AbdulQader", "authors": "Dana AbdulQader, Shoba Krishnan, Claudionor N. Coelho Jr", "title": "Enabling Incremental Training with Forward Pass for Edge Devices", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are commonly deployed on end devices that exist\nin constantly changing environments. In order for the system to maintain it's\naccuracy, it is critical that it is able to adapt to changes and recover by\nretraining parts of the network. However, end devices have limited resources\nmaking it challenging to train on the same device. Moreover, training deep\nneural networks is both memory and compute intensive due to the backpropagation\nalgorithm. In this paper we introduce a method using evolutionary strategy (ES)\nthat can partially retrain the network enabling it to adapt to changes and\nrecover after an error has occurred. This technique enables training on an\ninference-only hardware without the need to use backpropagation and with\nminimal resource overhead. We demonstrate the ability of our technique to\nretrain a quantized MNIST neural network after injecting noise to the input.\nFurthermore, we present the micro-architecture required to enable training on\nHLS4ML (an inference hardware architecture) and implement it in Verilog. We\nsynthesize our implementation for a Xilinx Kintex Ultrascale Field Programmable\nGate Array (FPGA) resulting in less than 1% resource utilization required to\nimplement the incremental training.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:43:04 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["AbdulQader", "Dana", ""], ["Krishnan", "Shoba", ""], ["Coelho", "Claudionor N.", "Jr"]]}, {"id": "2103.14808", "submitter": "Majid Jalili", "authors": "Majid Jalili, Mattan Erez", "title": "Reducing Load Latency with Cache Level Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  High load latency that results from deep cache hierarchies and relatively\nslow main memory is an important limiter of single-thread performance. Data\nprefetch helps reduce this latency by fetching data up the hierarchy before it\nis requested by load instructions. However, data prefetching has shown to be\nimperfect in many situations. We propose cache-level prediction to complement\nprefetchers. Our method predicts which memory hierarchy level a load will\naccess allowing the memory loads to start earlier, and thereby saves many\ncycles. The predictor provides high prediction accuracy at the cost of just one\ncycle added latency to L1 misses. Experimental results show speedup of 7.8\\% on\ngeneric, graph, and HPC applications over a baseline with aggressive\nprefetchers.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 04:58:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jalili", "Majid", ""], ["Erez", "Mattan", ""]]}, {"id": "2103.14951", "submitter": "Jose Martins", "authors": "Bruno S\\'a, Jos\\'e Martins, Sandro Pinto", "title": "A First Look at RISC-V Virtualization from an Embedded Systems\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the first public implementation and evaluation of the\nlatest version of the RISC-V hypervisor extension (H-extension v0.6.1)\nspecification in a Rocket chip core. To perform a meaningful evaluation for\nmodern multi-core embedded and mixedcriticality systems, we have ported Bao, an\nopen-source static partitioning hypervisor, to RISC-V. We have also extended\nthe RISC-V platformlevel interrupt controller (PLIC) to enable direct guest\ninterrupt injection with low and deterministic latency and we have enhanced the\ntimer infrastructure to avoid trap and emulation overheads. Experiments were\ncarried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the\nsystem was also successfully deployed and tested in a Zynq UltraScale+ MPSoC\nZCU104. Our hardware implementation was opensourced and is currently in use by\nthe RISC-V community towards the ratification of the H-extension specification.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:44:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["S\u00e1", "Bruno", ""], ["Martins", "Jos\u00e9", ""], ["Pinto", "Sandro", ""]]}, {"id": "2103.14978", "submitter": "Z\\\"ulal Bing\\\"ol", "authors": "Z\\\"ulal Bing\\\"ol, Mohammed Alser, Onur Mutlu, Ozcan Ozturk, Can Alkan", "title": "GateKeeper-GPU: Fast and Accurate Pre-Alignment Filtering in Short Read\n  Mapping", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  At the last step of short read mapping, the candidate locations of the reads\non the reference genome are verified to compute their differences from the\ncorresponding reference segments using sequence alignment algorithms.\nCalculating the similarities and differences between two sequences is still\ncomputationally expensive since approximate string matching techniques\ntraditionally inherit dynamic programming algorithms with quadratic time and\nspace complexity. We introduce GateKeeper-GPU, a fast and accurate\npre-alignment filter that efficiently reduces the need for expensive sequence\nalignment. GateKeeper-GPU provides two main contributions: first, improving the\nfiltering accuracy of GateKeeper(state-of-the-art lightweight pre-alignment\nfilter), second, exploiting the massive parallelism provided by the large\nnumber of GPU threads of modern GPUs to examine numerous sequence pairs rapidly\nand concurrently. GateKeeper-GPU accelerates the sequence alignment by up to\n2.9x and provides up to 1.4x speedup to the end-to-end execution time of a\ncomprehensive read mapper (mrFAST). GateKeeper-GPU is available at\nhttps://github.com/BilkentCompGen/GateKeeper-GPU\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:01:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:55:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bing\u00f6l", "Z\u00fclal", ""], ["Alser", "Mohammed", ""], ["Mutlu", "Onur", ""], ["Ozturk", "Ozcan", ""], ["Alkan", "Can", ""]]}, {"id": "2103.15024", "submitter": "Xinbiao Gan", "authors": "Xinbiao Gan and Wen Tan", "title": "MT-lib: A Topology-aware Message Transfer Library for Graph500 on\n  Supercomputers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present MT-lib, an efficient message transfer library for messages gather\nand scatter in benchmarks like Graph500 for Supercomputers. Our library\nincludes MST version as well as new-MST version. The MT-lib is deliberately\nkept light-weight, efficient and friendly interfaces for massive graph\ntraverse. MST provides (1) a novel non-blocking communication scheme with\nsending and receiving messages asynchronously to overlap calculation and\ncommunication;(2) merging messages according to the target process for reducing\ncommunication overhead;(3) a new communication mode of gathering intra-group\nmessages before forwarding between groups for reducing communication traffic.\nIn MT-lib, there are (1) one-sided message; (2) two-sided messages; and (3)\ntwo-sided messages with buffer, in which dynamic buffer expansion is built for\nmessages delivery. We experimented with MST and then testing Graph500 with MST\non Tianhe supercomputers. Experimental results show high communication\nefficiency and high throughputs for both BFS and SSSP communication operations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 00:32:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gan", "Xinbiao", ""], ["Tan", "Wen", ""]]}, {"id": "2103.15103", "submitter": "Ruizhe Zhao", "authors": "Ruizhe Zhao, Jianyi Cheng", "title": "Phism: Polyhedral High-Level Synthesis in MLIR", "comments": "Will be presented at LATTE'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polyhedral optimisation, a methodology that views nested loops as polyhedra\nand searches for their optimal transformation regarding specific objectives\n(parallelism, locality, etc.), sounds promising for mitigating difficulties in\nautomatically optimising hardware designs described by high-level synthesis\n(HLS), which are typically software programs with nested loops. Nevertheless,\nexisting polyhedral tools cannot meet the requirements from HLS developers for\nplatform-specific customisation and software/hardware co-optimisation. This\npaper proposes $\\phi_{sm}$ (phism), a polyhedral HLS framework built on MLIR,\nto address these challenges through progressive lowering multi-level\nintermediate representations (IRs) from polyhedra to HLS designs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 10:21:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Ruizhe", ""], ["Cheng", "Jianyi", ""]]}, {"id": "2103.15750", "submitter": "Callie Hao", "authors": "Cong Hao, Jordan Dotzel, Jinjun Xiong, Luca Benini, Zhiru Zhang,\n  Deming Chen", "title": "Enabling Design Methodologies and Future Trends for Edge AI:\n  Specialization and Co-design", "comments": "Accepted by IEEE Design & Test (D&T)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) technologies have dramatically advanced in\nrecent years, resulting in revolutionary changes in people's lives. Empowered\nby edge computing, AI workloads are migrating from centralized cloud\narchitectures to distributed edge systems, introducing a new paradigm called\nedge AI. While edge AI has the promise of bringing significant increases in\nautonomy and intelligence into everyday lives through common edge devices, it\nalso raises new challenges, especially for the development of its algorithms\nand the deployment of its services, which call for novel design methodologies\ncatered to these unique challenges. In this paper, we provide a comprehensive\nsurvey of the latest enabling design methodologies that span the entire edge AI\ndevelopment stack. We suggest that the key methodologies for effective edge AI\ndevelopment are single-layer specialization and cross-layer co-design. We\ndiscuss representative methodologies in each category in detail, including\non-device training methods, specialized software design, dedicated hardware\ndesign, benchmarking and design automation, software/hardware co-design,\nsoftware/compiler co-design, and compiler/hardware co-design. Moreover, we\nattempt to reveal hidden cross-layer design opportunities that can further\nboost the solution quality of future edge AI and provide insights into future\ndirections and emerging areas that require increased research focus.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 16:29:55 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:04:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hao", "Cong", ""], ["Dotzel", "Jordan", ""], ["Xiong", "Jinjun", ""], ["Benini", "Luca", ""], ["Zhang", "Zhiru", ""], ["Chen", "Deming", ""]]}, {"id": "2103.15756", "submitter": "Baohua Sun", "authors": "Baohua Sun, Tao Zhang, Jiapeng Su, Hao Sha", "title": "GnetDet: Object Detection Optimized on a 224mW CNN Accelerator Chip at\n  the Speed of 106FPS", "comments": "5 pages, 2 figures, and 1 table. arXiv admin note: text overlap with\n  arXiv:2101.10444", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is widely used on embedded devices. With the wide\navailability of CNN (Convolutional Neural Networks) accelerator chips, the\nobject detection applications are expected to run with low power consumption,\nand high inference speed. In addition, the CPU load is expected to be as low as\npossible for a CNN accelerator chip working as a co-processor with a host CPU.\nIn this paper, we optimize the object detection model on the CNN accelerator\nchip by minimizing the CPU load. The resulting model is called GnetDet. The\nexperimental result shows that the GnetDet model running on a 224mW chip\nachieves the speed of 106FPS with excellent accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 06:16:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sun", "Baohua", ""], ["Zhang", "Tao", ""], ["Su", "Jiapeng", ""], ["Sha", "Hao", ""]]}, {"id": "2103.15960", "submitter": "Yannik Stradmann", "authors": "Yannik Stradmann, Sebastian Billaudelle, Oliver Breitwieser, Falk\n  Leonard Ebert, Arne Emmel, Dan Husmann, Joscha Ilmberger, Eric M\\\"uller,\n  Philipp Spilger, Johannes Weis, Johannes Schemmel", "title": "Demonstrating Analog Inference on the BrainScaleS-2 Mobile System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the BrainScaleS-2 mobile system as a compact analog inference\nengine based on the BrainScaleS-2 ASIC and demonstrate its capabilities at\nclassifying a medical electrocardiogram dataset. The analog network core of the\nASIC is utilized to perform the multiply-accumulate operations of a\nconvolutional deep neural network. We measure a total energy consumption of\n192uJ for the ASIC and achieve a classification time of 276us per\nelectrocardiographic patient sample. Patients with atrial fibrillation are\ncorrectly identified with a detection rate of 93.7(7)% at 14.0(10)% false\npositives. The system is directly applicable to edge inference applications due\nto its small size, power envelope and flexible I/O capabilities. Possible\nfuture applications can furthermore combine conventional machine learning\nlayers with online-learning in spiking neural networks on a single\nBrainScaleS-2 ASIC. The system has successfully participated and proven to\noperate reliably in the independently judged competition\n\"Pilotinnovationswettbewerb 'Energieeffizientes KI-System'\" of the German\nFederal Ministry of Education and Research (BMBF).\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:22:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Stradmann", "Yannik", ""], ["Billaudelle", "Sebastian", ""], ["Breitwieser", "Oliver", ""], ["Ebert", "Falk Leonard", ""], ["Emmel", "Arne", ""], ["Husmann", "Dan", ""], ["Ilmberger", "Joscha", ""], ["M\u00fcller", "Eric", ""], ["Spilger", "Philipp", ""], ["Weis", "Johannes", ""], ["Schemmel", "Johannes", ""]]}, {"id": "2103.15969", "submitter": "Giannis Kazdaridis", "authors": "Giannis Kazdaridis, Nikos Sidiropoulos, Ioannis Zografopoulos,\n  Thanasis Korakis", "title": "eWake: A Novel Architecture for Semi-Active Wake-Up Radios Attaining\n  Ultra-High Sensitivity at Extremely-Low Consumption", "comments": "4 pages, 4 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new scheme for semi-passive Wake-Up Receiver\ncircuits that exhibits remarkable sensitivity beyond -70 dBm, while\nstate-of-the-art receivers illustrate sensitivity of up to -55 dBm. The\nreceiver employs the typical principle of an envelope detector that harvests RF\nenergy from its antenna, while it employs a nano-power operation amplifier to\nintensify the obtained signal prior to the final decoding that is realized with\nthe aid of a comparator circuit. It operates at the 868 MHz ISM band using OOK\nsignals propagated through LoRa transceivers, while also supporting addressing\ncapabilities in order to awake only the specified network's nodes. The power\nexpenditure of the developed receiver is as low as 580 nA, remaining at the\nsame power consumption levels as the state-of-the-art implementations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:52:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kazdaridis", "Giannis", ""], ["Sidiropoulos", "Nikos", ""], ["Zografopoulos", "Ioannis", ""], ["Korakis", "Thanasis", ""]]}, {"id": "2103.16045", "submitter": "Shaoshan Liu", "authors": "Shaoshan Liu, Bo Yu, Yahui Liu, Kunai Zhang, Yisong Qiao, Thomas Yuang\n  Li, Jie Tang, Yuhao Zhu", "title": "The Matter of Time -- A General and Efficient System for Precise Sensor\n  Synchronization in Robotic Computing", "comments": "IEEE RTAS 2021 Brief Industry Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time synchronization is a critical task in robotic computing such as\nautonomous driving. In the past few years, as we developed advanced robotic\napplications, our synchronization system has evolved as well. In this paper, we\nfirst introduce the time synchronization problem and explain the challenges of\ntime synchronization, especially in robotic workloads. Summarizing these\nchallenges, we then present a general hardware synchronization system for\nrobotic computing, which delivers high synchronization accuracy while\nmaintaining low energy and resource consumption. The proposed hardware\nsynchronization system is a key building block in our future robotic products.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:04:01 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Shaoshan", ""], ["Yu", "Bo", ""], ["Liu", "Yahui", ""], ["Zhang", "Kunai", ""], ["Qiao", "Yisong", ""], ["Li", "Thomas Yuang", ""], ["Tang", "Jie", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2103.16141", "submitter": "Kazuo Aoyama", "authors": "Kazuo Aoyama and Kazumi Saito", "title": "Structured Inverted-File k-Means Clustering for High-Dimensional Sparse\n  Data", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture-friendly k-means clustering algorithm\ncalled SIVF for a large-scale and high-dimensional sparse data set. Algorithm\nefficiency on time is often measured by the number of costly operations such as\nsimilarity calculations. In practice, however, it depends greatly on how the\nalgorithm adapts to an architecture of the computer system which it is executed\non. Our proposed SIVF employs invariant centroid-pair based filter (ICP) to\ndecrease the number of similarity calculations between a data object and\ncentroids of all the clusters. To maximize the ICP performance, SIVF exploits\nfor a centroid set an inverted-file that is structured so as to reduce pipeline\nhazards. We demonstrate in our experiments on real large-scale document data\nsets that SIVF operates at higher speed and with lower memory consumption than\nexisting algorithms. Our performance analysis reveals that SIVF achieves the\nhigher speed by suppressing performance degradation factors of the number of\ncache misses and branch mispredictions rather than less similarity\ncalculations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:54:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Aoyama", "Kazuo", ""], ["Saito", "Kazumi", ""]]}]