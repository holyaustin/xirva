[{"id": "1708.00052", "submitter": "Evgenii Zheltonozhskii", "authors": "Chaim Baskin, Natan Liss, Evgenii Zheltonozhskii, Alex M. Bronshtein,\n  Avi Mendelson", "title": "Streaming Architecture for Large-Scale Quantized Neural Networks on an\n  FPGA-Based Dataflow Platform", "comments": "Will appear in RAW 2018", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00032", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are used by different applications that are\nexecuted on a range of computer architectures, from IoT devices to\nsupercomputers. The footprint of these networks is huge as well as their\ncomputational and communication needs. In order to ease the pressure on\nresources, research indicates that in many cases a low precision representation\n(1-2 bit per parameter) of weights and other parameters can achieve similar\naccuracy while requiring less resources. Using quantized values enables the use\nof FPGAs to run NNs, since FPGAs are well fitted to these primitives; e.g.,\nFPGAs provide efficient support for bitwise operations and can work with\narbitrary-precision representation of numbers.\n  This paper presents a new streaming architecture for running QNNs on FPGAs.\nThe proposed architecture scales out better than alternatives, allowing us to\ntake advantage of systems with multiple FPGAs. We also included support for\nskip connections, that are used in state-of-the art NNs, and shown that our\narchitecture allows to add those connections almost for free. All this allowed\nus to implement an 18-layer ResNet for 224x224 images classification, achieving\n57.5% top-1 accuracy.\n  In addition, we implemented a full-sized quantized AlexNet. In contrast to\nprevious works, we use 2-bit activations instead of 1-bit ones, which improves\nAlexNet's top-1 accuracy from 41.8% to 51.03% for the ImageNet classification.\nBoth AlexNet and ResNet can handle 1000-class real-time classification on an\nFPGA.\n  Our implementation of ResNet-18 consumes 5x less power and is 4x slower for\nImageNet, when compared to the same NN on the latest Nvidia GPUs. Smaller NNs,\nthat fit a single FPGA, are running faster then on GPUs on small (32x32)\ninputs, while consuming up to 20x less energy and power.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 19:53:48 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:44:36 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 13:20:13 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Baskin", "Chaim", ""], ["Liss", "Natan", ""], ["Zheltonozhskii", "Evgenii", ""], ["Bronshtein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1708.02579", "submitter": "Eugenio Culurciello", "authors": "Vinayak Gokhale, Aliasger Zaidy, Andre Xian Ming Chang, Eugenio\n  Culurciello", "title": "Snowflake: A Model Agnostic Accelerator for Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are the deep learning model of\nchoice for performing object detection, classification, semantic segmentation\nand natural language processing tasks. CNNs require billions of operations to\nprocess a frame. This computational complexity, combined with the inherent\nparallelism of the convolution operation make CNNs an excellent target for\ncustom accelerators. However, when optimizing for different CNN hierarchies and\ndata access patterns, it is difficult for custom accelerators to achieve close\nto 100% computational efficiency. In this work, we present Snowflake, a\nscalable and efficient accelerator that is agnostic to CNN workloads, and was\ndesigned to always perform at near-peak hardware utilization. Snowflake is able\nto achieve a computational efficiency of over 91% on modern CNN models.\nSnowflake, implemented on a Xilinx Zynq XC7Z045 SoC is capable of achieving a\npeak throughput of 128G-ops/s and a measured throughput of 100 frames per\nsecond and 120 G-ops/s on the AlexNet CNN model, 36 frames per second and 116G-\nops/s on the GoogLeNet CNN model and 17 frames per second and 122 G-ops/s on\nthe ResNet-50 CNN model. To the best of our knowledge, Snowflake is the only\nimplemented system capable of achieving over 91% efficiency on modern CNNs and\nthe only implemented system with GoogLeNet and ResNet as part of the benchmark\nsuite.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 17:49:44 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Gokhale", "Vinayak", ""], ["Zaidy", "Aliasger", ""], ["Chang", "Andre Xian Ming", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1708.03900", "submitter": "Prathmesh Kallurkar Mr.", "authors": "Prathmesh Kallurkar, Smruti R. Sarangi", "title": "Sensitivity Analysis of Core Specialization Techniques", "comments": "5 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instruction footprint of OS-intensive workloads such as web servers,\ndatabase servers, and file servers typically exceeds the size of the\ninstruction cache (32 KB). Consequently, such workloads incur a lot of i-cache\nmisses, which reduces their performance drastically. Several papers have\nproposed to improve the performance of such workloads using core\nspecialization. In this scheme, tasks with different instruction footprints are\nexecuted on different cores. In this report, we study the performance of five\nstate of the art core specialization techniques: SelectiveOffload [6], FlexSC\n[8], DisAggregateOS [5], SLICC [2], and SchedTask [3] for different system\nparameters. Our studies show that for a suite of 8 popular OS-intensive\nworkloads, SchedTask performs best for all evaluated configurations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 12:56:29 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Kallurkar", "Prathmesh", ""], ["Sarangi", "Smruti R.", ""]]}, {"id": "1708.04198", "submitter": "Saber Moradi", "authors": "Saber Moradi, Ning Qiao, Fabio Stefanini and Giacomo Indiveri", "title": "A scalable multi-core architecture with heterogeneous memory structures\n  for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TBCAS.2017.2759700", "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing systems comprise networks of neurons that use\nasynchronous events for both computation and communication. This type of\nrepresentation offers several advantages in terms of bandwidth and power\nconsumption in neuromorphic electronic systems. However, managing the traffic\nof asynchronous events in large scale systems is a daunting task, both in terms\nof circuit complexity and memory requirements. Here we present a novel routing\nmethodology that employs both hierarchical and mesh routing strategies and\ncombines heterogeneous memory structures for minimizing both memory\nrequirements and latency, while maximizing programming flexibility to support a\nwide range of event-based neural network architectures, through parameter\nconfiguration. We validated the proposed scheme in a prototype multi-core\nneuromorphic processor chip that employs hybrid analog/digital circuits for\nemulating synapse and neuron dynamics together with asynchronous digital\ncircuits for managing the address-event traffic. We present a theoretical\nanalysis of the proposed connectivity scheme, describe the methods and circuits\nused to implement such scheme, and characterize the prototype chip. Finally, we\ndemonstrate the use of the neuromorphic processor with a convolutional neural\nnetwork for the real-time classification of visual symbols being flashed to a\ndynamic vision sensor (DVS) at high speed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 16:28:02 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 17:50:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Moradi", "Saber", ""], ["Qiao", "Ning", ""], ["Stefanini", "Fabio", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1708.04485", "submitter": "Stephen Keckler", "authors": "Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli,\n  Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and\n  William J. Dally", "title": "SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology\nfor machine learning. High performance and extreme energy efficiency are\ncritical for deployments of CNNs in a wide range of situations, especially\nmobile platforms such as autonomous vehicles, cameras, and electronic personal\nassistants. This paper introduces the Sparse CNN (SCNN) accelerator\narchitecture, which improves performance and energy efficiency by exploiting\nthe zero-valued weights that stem from network pruning during training and\nzero-valued activations that arise from the common ReLU operator applied during\ninference. Specifically, SCNN employs a novel dataflow that enables maintaining\nthe sparse weights and activations in a compressed encoding, which eliminates\nunnecessary data transfers and reduces storage requirements. Furthermore, the\nSCNN dataflow facilitates efficient delivery of those weights and activations\nto the multiplier array, where they are extensively reused. In addition, the\naccumulation of multiplication products are performed in a novel accumulator\narray. Our results show that on contemporary neural networks, SCNN can improve\nboth performance and energy by a factor of 2.7x and 2.3x, respectively, over a\ncomparably provisioned dense CNN accelerator.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:11:11 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Parashar", "Angshuman", ""], ["Rhu", "Minsoo", ""], ["Mukkara", "Anurag", ""], ["Puglielli", "Antonio", ""], ["Venkatesan", "Rangharajan", ""], ["Khailany", "Brucek", ""], ["Emer", "Joel", ""], ["Keckler", "Stephen W.", ""], ["Dally", "William J.", ""]]}, {"id": "1708.04911", "submitter": "Rachata Ausavarungnirun", "authors": "Rachata Ausavarungnirun, Christopher J. Rossbach, Vance Miller, Joshua\n  Landgraf, Saugata Ghose, Jayneel Gnadhi, Adwait Jog, Onur Mutlu", "title": "Improving Multi-Application Concurrency Support Within the GPU Memory\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs exploit a high degree of thread-level parallelism to hide long-latency\nstalls. Due to the heterogeneous compute requirements of different\napplications, there is a growing need to share the GPU across multiple\napplications in large-scale computing environments. However, while CPUs offer\nrelatively seamless multi-application concurrency, and are an excellent fit for\nmultitasking and for virtualized environments, GPUs currently offer only\nprimitive support for multi-application concurrency. Much of the problem in a\ncontemporary GPU lies within the memory system, where multi-application\nexecution requires virtual memory support to manage the address spaces of each\napplication and to provide memory protection. In this work, we perform a\ndetailed analysis of the major problems in state-of-the-art GPU virtual memory\nmanagement that hinders multi-application execution. Existing GPUs are designed\nto share memory between the CPU and GPU, but do not handle multi-application\nsupport within the GPU well. We find that when multiple applications spatially\nshare the GPU, there is a significant amount of inter-core thrashing on the\nshared TLB within the GPU. The TLB contention is high enough to prevent the GPU\nfrom successfully hiding stall latencies, thus becoming a first-order\nperformance concern. We introduce MASK, a memory hierarchy design that provides\nlow-overhead virtual memory support for the concurrent execution of multiple\napplications. MASK extends the GPU memory hierarchy to efficiently support\naddress translation through the use of multi-level TLBs, and uses\ntranslation-aware memory and cache management to maximize throughput in the\npresence of inter-application contention.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 14:33:41 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ausavarungnirun", "Rachata", ""], ["Rossbach", "Christopher J.", ""], ["Miller", "Vance", ""], ["Landgraf", "Joshua", ""], ["Ghose", "Saugata", ""], ["Gnadhi", "Jayneel", ""], ["Jog", "Adwait", ""], ["Mutlu", "Onur", ""]]}, {"id": "1708.06248", "submitter": "Linghao Song", "authors": "Linghao Song, Youwei Zhuo, Xuehai Qian, Hai Li, Yiran Chen", "title": "GraphR: Accelerating Graph Processing Using ReRAM", "comments": "Accepted to HPCA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GRAPHR, the first ReRAM-based graph processing\naccelerator. GRAPHR follows the principle of near-data processing and explores\nthe opportunity of performing massive parallel analog operations with low\nhardware and energy cost. The analog computation is suit- able for graph\nprocessing because: 1) The algorithms are iterative and could inherently\ntolerate the imprecision; 2) Both probability calculation (e.g., PageRank and\nCollaborative Filtering) and typical graph algorithms involving integers (e.g.,\nBFS/SSSP) are resilient to errors. The key insight of GRAPHR is that if a\nvertex program of a graph algorithm can be expressed in sparse matrix vector\nmultiplication (SpMV), it can be efficiently performed by ReRAM crossbar. We\nshow that this assumption is generally true for a large set of graph\nalgorithms. GRAPHR is a novel accelerator architecture consisting of two\ncomponents: memory ReRAM and graph engine (GE). The core graph computations are\nperformed in sparse matrix format in GEs (ReRAM crossbars). The\nvector/matrix-based graph computation is not new, but ReRAM offers the unique\nopportunity to realize the massive parallelism with unprecedented energy\nefficiency and low hardware cost. With small subgraphs processed by GEs, the\ngain of performing parallel operations overshadows the wastes due to sparsity.\nThe experiment results show that GRAPHR achieves a 16.01x (up to 132.67x)\nspeedup and a 33.82x energy saving on geometric mean compared to a CPU baseline\nsystem. Com- pared to GPU, GRAPHR achieves 1.69x to 2.19x speedup and consumes\n4.77x to 8.91x less energy. GRAPHR gains a speedup of 1.16x to 4.12x, and is\n3.67x to 10.96x more energy efficiency compared to PIM-based architecture.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:21:36 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 04:23:14 GMT"}, {"version": "v3", "created": "Sun, 22 Oct 2017 19:18:28 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 22:02:14 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Song", "Linghao", ""], ["Zhuo", "Youwei", ""], ["Qian", "Xuehai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1708.07619", "submitter": "Fazel Sharifi", "authors": "Fazel Sharifi, Z. M. Saifullah and Abdel-Hameed Badawy", "title": "Design of Adiabatic MTJ-CMOS Hybrid Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-power designs are a necessity with the increasing demand of portable\ndevices which are battery operated. In many of such devices the operational\nspeed is not as important as battery life. Logic-in-memory structures using\nnano-devices and adiabatic designs are two methods to reduce the static and\ndynamic power consumption respectively. Magnetic tunnel junction (MTJ) is an\nemerging technology which has many advantages when used in logic-in-memory\nstructures in conjunction with CMOS. In this paper, we introduce a novel\nadiabatic hybrid MTJ/CMOS structure which is used to design AND/NAND, XOR/XNOR\nand 1-bit full adder circuits. We simulate the designs using HSPICE with 32nm\nCMOS technology and compared it with a non-adiabatic hybrid MTJ/CMOS circuits.\nThe proposed adiabatic MTJ/CMOS full adder design has more than 7 times lower\npower consumtion compared to the previous MTJ/CMOS full adder.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 06:11:17 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Sharifi", "Fazel", ""], ["Saifullah", "Z. M.", ""], ["Badawy", "Abdel-Hameed", ""]]}, {"id": "1708.07677", "submitter": "Xiang Fu", "authors": "X. Fu, M. A. Rol, C. C. Bultink, J. van Someren, N. Khammassi, I.\n  Ashraf, R. F. L. Vermeulen, J. C. de Sterke, W. J. Vlothuizen, R. N.\n  Schouten, C. G. Almudever, L. DiCarlo, K. Bertels", "title": "An Experimental Microarchitecture for a Superconducting Quantum\n  Processor", "comments": "13 pages including reference. 9 figures", "journal-ref": null, "doi": "10.1145/3123939.3123952", "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computers promise to solve certain problems that are intractable for\nclassical computers, such as factoring large numbers and simulating quantum\nsystems. To date, research in quantum computer engineering has focused\nprimarily at opposite ends of the required system stack: devising high-level\nprogramming languages and compilers to describe and optimize quantum\nalgorithms, and building reliable low-level quantum hardware. Relatively little\nattention has been given to using the compiler output to fully control the\noperations on experimental quantum processors. Bridging this gap, we propose\nand build a prototype of a flexible control microarchitecture supporting\nquantum-classical mixed code for a superconducting quantum processor. The\nmicroarchitecture is based on three core elements: (i) a codeword-based event\ncontrol scheme, (ii) queue-based precise event timing control, and (iii) a\nflexible multilevel instruction decoding mechanism for control. We design a set\nof quantum microinstructions that allows flexible control of quantum operations\nwith precise timing. We demonstrate the microarchitecture and microinstruction\nset by performing a standard gate-characterization experiment on a transmon\nqubit.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 10:17:54 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Fu", "X.", ""], ["Rol", "M. A.", ""], ["Bultink", "C. C.", ""], ["van Someren", "J.", ""], ["Khammassi", "N.", ""], ["Ashraf", "I.", ""], ["Vermeulen", "R. F. L.", ""], ["de Sterke", "J. C.", ""], ["Vlothuizen", "W. J.", ""], ["Schouten", "R. N.", ""], ["Almudever", "C. G.", ""], ["DiCarlo", "L.", ""], ["Bertels", "K.", ""]]}, {"id": "1708.09597", "submitter": "Cunxi Yu", "authors": "Cunxi Yu, Mihir Choudhury, Andrew Sullivan, Maciej Ciesielski", "title": "Advanced Datapath Synthesis using Graph Isomorphism", "comments": "6 pages, 8 figures. To appear in 2017 IEEE/ACM International\n  Conference on Computer-Aided Design (ICCAD'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an advanced DAG-based algorithm for datapath synthesis\nthat targets area minimization using logic-level resource sharing. The problem\nof identifying common specification logic is formulated using unweighted graph\nisomorphism problem, in contrast to a weighted graph isomorphism using AIGs. In\nthe context of gate-level datapath circuits, our algorithm solves the un-\nweighted graph isomorphism problem in linear time. The experiments are\nconducted within an industrial synthesis flow that includes the complete\nhigh-level synthesis, logic synthesis and placement and route procedures.\nExperimental results show a significant runtime improvements compared to the\nexisting datapath synthesis algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 07:34:00 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Yu", "Cunxi", ""], ["Choudhury", "Mihir", ""], ["Sullivan", "Andrew", ""], ["Ciesielski", "Maciej", ""]]}, {"id": "1708.09603", "submitter": "Pascal Giard", "authors": "Pascal Giard, Alexios Balatsoukas-Stimming, Thomas Christoph M\\\"uller,\n  Andrea Bonetti, Claude Thibeault, Warren J. Gross, Philippe Flatresse,\n  Andreas Burg", "title": "PolarBear: A 28-nm FD-SOI ASIC for Decoding of Polar Codes", "comments": "12 pages, 12 figures, 5 tables, to appear in IEEE Journal on Emerging\n  and Selected Topics in Circuits and Systems", "journal-ref": null, "doi": "10.1109/JETCAS.2017.2745704", "report-no": null, "categories": "cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polar codes are a recently proposed class of block codes that provably\nachieve the capacity of various communication channels. They received a lot of\nattention as they can do so with low-complexity encoding and decoding\nalgorithms, and they have an explicit construction. Their recent inclusion in a\n5G communication standard will only spur more research. However, only a couple\nof ASICs featuring decoders for polar codes were fabricated, and none of them\nimplements a list-based decoding algorithm. In this paper, we present ASIC\nmeasurement results for a fabricated 28 nm CMOS chip that implements two\ndifferent decoders: the first decoder is tailored toward error-correction\nperformance and flexibility. It supports any code rate as well as three\ndifferent decoding algorithms: successive cancellation (SC), SC flip and SC\nlist (SCL). The flexible decoder can also decode both non-systematic and\nsystematic polar codes. The second decoder targets speed and energy efficiency.\nWe present measurement results for the first silicon-proven SCL decoder, where\nits coded throughput is shown to be of 306.8 Mbps with a latency of 3.34 us and\nan energy per bit of 418.3 pJ/bit at a clock frequency of 721 MHz for a supply\nof 1.3 V. The energy per bit drops down to 178.1 pJ/bit with a more modest\nclock frequency of 308 MHz, lower throughput of 130.9 Mbps and a reduced supply\nvoltage of 0.9 V. For the other two operating modes, the energy per bit is\nshown to be of approximately 95 pJ/bit. The less flexible high-throughput\nunrolled decoder can achieve a coded throughput of 9.2 Gbps and a latency of\n628 ns for a measured energy per bit of 1.15 pJ/bit at 451 MHz.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 08:03:19 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 09:01:53 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Giard", "Pascal", ""], ["Balatsoukas-Stimming", "Alexios", ""], ["M\u00fcller", "Thomas Christoph", ""], ["Bonetti", "Andrea", ""], ["Thibeault", "Claude", ""], ["Gross", "Warren J.", ""], ["Flatresse", "Philippe", ""], ["Burg", "Andreas", ""]]}]