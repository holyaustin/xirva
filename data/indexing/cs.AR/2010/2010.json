[{"id": "2010.00289", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Weighing up the new kid on the block: Impressions of using Vitis for HPC\n  software development", "comments": "Pre-print of Weighing up the new kid on the block: Impressions of\n  using Vitis for HPC software development, paper in 30th International\n  Conference on Field Programmable Logic and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of reconfigurable computing, and FPGAs in particular, has strong\npotential in the field of High Performance Computing (HPC). However the\ntraditionally high barrier to entry when it comes to programming this\ntechnology has, until now, precluded widespread adoption. To popularise\nreconfigurable computing with communities such as HPC, Xilinx have recently\nreleased the first version of Vitis, a platform aimed at making the programming\nof FPGAs much more a question of software development rather than hardware\ndesign. However a key question is how well this technology fulfils the aim, and\nwhether the tooling is mature enough such that software developers using FPGAs\nto accelerate their codes is now a more realistic proposition, or whether it\nsimply increases the convenience for existing experts. To examine this question\nwe use the Himeno benchmark as a vehicle for exploring the Vitis platform for\nbuilding, executing and optimising HPC codes, describing the different steps\nand potential pitfalls of the technology. The outcome of this exploration is a\ndemonstration that, whilst Vitis is an excellent step forwards and\nsignificantly lowers the barrier to entry in developing codes for FPGAs, it is\nnot a silver bullet and an underlying understanding of dataflow style\nalgorithmic design and appreciation of the architecture is still key to\nobtaining good performance on reconfigurable architectures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:36:24 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.00627", "submitter": "Mehdi Ahmadi", "authors": "Mehdi Ahmadi, Shervin Vakili, J.M. Pierre Langlois", "title": "CARLA: A Convolution Accelerator with a Reconfigurable and Low-Energy\n  Architecture", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proven to be extremely accurate for\nimage recognition, even outperforming human recognition capability. When\ndeployed on battery-powered mobile devices, efficient computer architectures\nare required to enable fast and energy-efficient computation of costly\nconvolution operations. Despite recent advances in hardware accelerator design\nfor CNNs, two major problems have not yet been addressed effectively,\nparticularly when the convolution layers have highly diverse structures: (1)\nminimizing energy-hungry off-chip DRAM data movements; (2) maximizing the\nutilization factor of processing resources to perform convolutions. This work\nthus proposes an energy-efficient architecture equipped with several optimized\ndataflows to support the structural diversity of modern CNNs. The proposed\napproach is evaluated by implementing convolutional layers of VGGNet-16 and\nResNet-50. Results show that the architecture achieves a Processing Element\n(PE) utilization factor of 98% for the majority of 3x3 and 1x1 convolutional\nlayers, while limiting latency to 396.9 ms and 92.7 ms when performing\nconvolutional layers of VGGNet-16 and ResNet-50, respectively. In addition, the\nproposed architecture benefits from the structured sparsity in ResNet-50 to\nreduce the latency to 42.5 ms when half of the channels are pruned.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 18:10:40 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ahmadi", "Mehdi", ""], ["Vakili", "Shervin", ""], ["Langlois", "J. M. Pierre", ""]]}, {"id": "2010.02017", "submitter": "Johannes Bund", "authors": "Johannes Bund and Matthias F\\\"ugger and Christoph Lenzen and Moti\n  Medina", "title": "Synchronizer-Free Digital Link Controller", "comments": "12 page journal article", "journal-ref": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I (REGULAR PAPERS),\n  VOLUME 67, NUMBER 10, OCTOBER 2020", "doi": "10.1109/TCSI.2020.2989552", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a producer-consumer link between two independent clock\ndomains. The link allows for metastability-free, low-latency, high-throughput\ncommunication by slight adjustments to the clock frequencies of the producer\nand consumer domains steered by a controller circuit. Any such controller\ncannot deterministically avoid, detect, nor resolve metastability. Typically,\nthis is addressed by synchronizers, incurring a larger dead time in the control\nloop. We follow the approach of Friedrichs et al. (TC 2018) who proposed\nmetastability-containing circuits. The result is a simple control circuit that\nmay become metastable, yet deterministically avoids buffer underrun or\noverflow. More specifically, the controller output may become metastable, but\nthis may only affect oscillator speeds within specific bounds. In contrast,\ncommunication is guaranteed to remain metastability-free. We formally prove\ncorrectness of the producer-consumer link and a possible implementation that\nhas only small overhead. With SPICE simulations of the proposed implementation\nwe further substantiate our claims. The simulation uses 65nm process running at\nroughly 2GHz.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:47:55 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bund", "Johannes", ""], ["F\u00fcgger", "Matthias", ""], ["Lenzen", "Christoph", ""], ["Medina", "Moti", ""]]}, {"id": "2010.02075", "submitter": "Zhan Shi", "authors": "Zhan Shi, Chirag Sakhuja, Milad Hashemi, Kevin Swersky, Calvin Lin", "title": "Learned Hardware/Software Co-Design of Neural Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning has grown at an exponential rate, giving rise to\nnumerous specialized hardware and software systems for deep learning. Because\nthe design space of deep learning software stacks and hardware accelerators is\ndiverse and vast, prior work considers software optimizations separately from\nhardware architectures, effectively reducing the search space. Unfortunately,\nthis bifurcated approach means that many profitable design points are never\nexplored. This paper instead casts the problem as hardware/software co-design,\nwith the goal of automatically identifying desirable points in the joint design\nspace. The key to our solution is a new constrained Bayesian optimization\nframework that avoids invalid solutions by exploiting the highly constrained\nfeatures of this design space, which are semi-continuous/semi-discrete. We\nevaluate our optimization framework by applying it to a variety of neural\nmodels, improving the energy-delay product by 18% (ResNet) and 40% (DQN) over\nhand-tuned state-of-the-art systems, as well as demonstrating strong results on\nother neural network architectures, such as MLPs and Transformers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:12:52 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Shi", "Zhan", ""], ["Sakhuja", "Chirag", ""], ["Hashemi", "Milad", ""], ["Swersky", "Kevin", ""], ["Lin", "Calvin", ""]]}, {"id": "2010.02079", "submitter": "Ivan Fernandez", "authors": "Ivan Fernandez, Ricardo Quislant, Christina Giannoula, Mohammed Alser,\n  Juan G\\'omez-Luna, Eladio Guti\\'errez, Oscar Plata, Onur Mutlu", "title": "NATSA: A Near-Data Processing Accelerator for Time Series Analysis", "comments": "To appear in the 38th IEEE International Conference on Computer\n  Design (ICCD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series analysis is a key technique for extracting and predicting events\nin domains as diverse as epidemiology, genomics, neuroscience, environmental\nsciences, economics, and more. Matrix profile, the state-of-the-art algorithm\nto perform time series analysis, computes the most similar subsequence for a\ngiven query subsequence within a sliced time series. Matrix profile has low\narithmetic intensity, but it typically operates on large amounts of time series\ndata. In current computing systems, this data needs to be moved between the\noff-chip memory units and the on-chip computation units for performing matrix\nprofile. This causes a major performance bottleneck as data movement is\nextremely costly in terms of both execution time and energy.\n  In this work, we present NATSA, the first Near-Data Processing accelerator\nfor time series analysis. The key idea is to exploit modern 3D-stacked High\nBandwidth Memory (HBM) to enable efficient and fast specialized matrix profile\ncomputation near memory, where time series data resides. NATSA provides three\nkey benefits: 1) quickly computing the matrix profile for a wide range of\napplications by building specialized energy-efficient floating-point arithmetic\nprocessing units close to HBM, 2) improving the energy efficiency and execution\ntime by reducing the need for data movement over slow and energy-hungry buses\nbetween the computation units and the memory units, and 3) analyzing time\nseries data at scale by exploiting low-latency, high-bandwidth, and\nenergy-efficient memory access provided by HBM. Our experimental evaluation\nshows that NATSA improves performance by up to 14.2x (9.9x on average) and\nreduces energy by up to 27.2x (19.4x on average), over the state-of-the-art\nmulti-core implementation. NATSA also improves performance by 6.3x and reduces\nenergy by 10.2x over a general-purpose NDP platform with 64 in-order cores.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:16:11 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 21:51:12 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fernandez", "Ivan", ""], ["Quislant", "Ricardo", ""], ["Giannoula", "Christina", ""], ["Alser", "Mohammed", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Guti\u00e9rrez", "Eladio", ""], ["Plata", "Oscar", ""], ["Mutlu", "Onur", ""]]}, {"id": "2010.02825", "submitter": "Lois Orosa", "authors": "Leonid Yavits, Lois Orosa, Suyash Mahar, Jo\\~ao Dinis Ferreira, Mattan\n  Erez, Ran Ginosar, Onur Mutlu", "title": "WoLFRaM: Enhancing Wear-Leveling and Fault Tolerance in Resistive\n  Memories using Programmable Address Decoders", "comments": "To appear in ICCD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive memories have limited lifetime caused by limited write endurance\nand highly non-uniform write access patterns. Two main techniques to mitigate\nendurance-related memory failures are 1) wear-leveling, to evenly distribute\nthe writes across the entire memory, and 2) fault tolerance, to correct memory\ncell failures. However, one of the main open challenges in extending the\nlifetime of existing resistive memories is to make both techniques work\ntogether seamlessly and efficiently. To address this challenge, we propose\nWoLFRaM, a new mechanism that combines both wear-leveling and fault tolerance\ntechniques at low cost by using a programmable resistive address decoder\n(PRAD). The key idea of WoLFRaM is to use PRAD for implementing 1) a new\nefficient wear-leveling mechanism that remaps write accesses to random physical\nlocations on the fly, and 2) a new efficient fault tolerance mechanism that\nrecovers from faults by remapping failed memory blocks to available physical\nlocations. Our evaluations show that, for a Phase Change Memory (PCM) based\nsystem with cell endurance of 108 writes, WoLFRaM increases the memory lifetime\nby 68% compared to a baseline that implements the best state-of-the-art\nwear-leveling and fault correction mechanisms. WoLFRaM's average / worst-case\nperformance and energy overheads are 0.51% / 3.8% and 0.47% / 2.1%\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:38:56 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Yavits", "Leonid", ""], ["Orosa", "Lois", ""], ["Mahar", "Suyash", ""], ["Ferreira", "Jo\u00e3o Dinis", ""], ["Erez", "Mattan", ""], ["Ginosar", "Ran", ""], ["Mutlu", "Onur", ""]]}, {"id": "2010.03032", "submitter": "Mingsheng Ying", "authors": "Mingsheng Ying and Zhengfeng Ji", "title": "Symbolic Verification of Quantum Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note proposes a symbolic approach for representing and reasoning\nabout quantum circuits using complex, vector or matrix-valued Boolean\nexpressions. A major benefit of this approach is that it allows us to directly\nborrow the existing techniques and tools for verification of classical logic\ncircuits in reasoning about quantum circuits.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 00:02:22 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ying", "Mingsheng", ""], ["Ji", "Zhengfeng", ""]]}, {"id": "2010.03397", "submitter": "Siyuan Niu", "authors": "Siyuan Niu (LIRMM), Adrien Suau (LIRMM, CERFACS), Gabriel Staffelbach\n  (CERFACS), Aida Todri-Sanial (LIRMM, CNRS)", "title": "A Hardware-Aware Heuristic for the Qubit Mapping Problem in the NISQ Era", "comments": "IEEE Transactions on Quantum Engineering, 2020", "journal-ref": null, "doi": "10.1109/TQE.2020.3026544", "report-no": null, "categories": "cs.AR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to several physical limitations in the realisation of quantum hardware,\ntoday's quantum computers are qualified as Noisy Intermediate-Scale Quantum\n(NISQ) hardware. NISQ hardware is characterized by a small number of qubits (50\nto a few hundred) and noisy operations. Moreover, current realisations of\nsuperconducting quantum chips do not have the ideal all-to-all connectivity\nbetween qubits but rather at most a nearest-neighbour connectivity. All these\nhardware restrictions add supplementary low-level requirements. They need to be\naddressed before submitting the quantum circuit to an actual chip. Satisfying\nthese requirements is a tedious task for the programmer. Instead, the task of\nadapting the quantum circuit to a given hardware is left to the compiler. In\nthis paper, we propose a Hardware-Aware mapping transition algorithm (HA) that\ntakes the calibration data into account with the aim to improve the overall\nfidelity of the circuit. Evaluation results on IBM quantum hardware show that\nour HA approach can outperform the state of the art both in terms of the number\nof additional gates and circuit fidelity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 07:03:35 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Niu", "Siyuan", "", "LIRMM"], ["Suau", "Adrien", "", "LIRMM, CERFACS"], ["Staffelbach", "Gabriel", "", "CERFACS"], ["Todri-Sanial", "Aida", "", "LIRMM, CNRS"]]}, {"id": "2010.04017", "submitter": "Alex Renda", "authors": "Alex Renda, Yishen Chen, Charith Mendis, Michael Carbin", "title": "DiffTune: Optimizing CPU Simulator Parameters with Learned\n  Differentiable Surrogates", "comments": null, "journal-ref": "MICRO 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPU simulators are useful tools for modeling CPU execution behavior. However,\nthey suffer from inaccuracies due to the cost and complexity of setting their\nfine-grained parameters, such as the latencies of individual instructions. This\ncomplexity arises from the expertise required to design benchmarks and\nmeasurement frameworks that can precisely measure the values of parameters at\nsuch fine granularity. In some cases, these parameters do not necessarily have\na physical realization and are therefore fundamentally approximate, or even\nunmeasurable.\n  In this paper we present DiffTune, a system for learning the parameters of\nx86 basic block CPU simulators from coarse-grained end-to-end measurements.\nGiven a simulator, DiffTune learns its parameters by first replacing the\noriginal simulator with a differentiable surrogate, another function that\napproximates the original function; by making the surrogate differentiable,\nDiffTune is then able to apply gradient-based optimization techniques even when\nthe original function is non-differentiable, such as is the case with CPU\nsimulators. With this differentiable surrogate, DiffTune then applies\ngradient-based optimization to produce values of the simulator's parameters\nthat minimize the simulator's error on a dataset of ground truth end-to-end\nperformance measurements. Finally, the learned parameters are plugged back into\nthe original simulator.\n  DiffTune is able to automatically learn the entire set of\nmicroarchitecture-specific parameters within the Intel x86 simulation model of\nllvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling\nmodel. DiffTune's learned parameters lead llvm-mca to an average error that not\nonly matches but lowers that of its original, expert-provided parameter values.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:28:03 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 00:40:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Renda", "Alex", ""], ["Chen", "Yishen", ""], ["Mendis", "Charith", ""], ["Carbin", "Michael", ""]]}, {"id": "2010.04073", "submitter": "Gianmarco Ottavi", "authors": "Gianmarco Ottavi, Angelo Garofalo, Giuseppe Tagliavini, Francesco\n  Conti, Luca Benini, Davide Rossi", "title": "A Mixed-Precision RISC-V Processor for Extreme-Edge DNN Inference", "comments": "6 pages, 6 figures, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low bit-width Quantized Neural Networks (QNNs) enable deployment of complex\nmachine learning models on constrained devices such as microcontrollers (MCUs)\nby reducing their memory footprint. Fine-grained asymmetric quantization (i.e.,\ndifferent bit-widths assigned to weights and activations on a tensor-by-tensor\nbasis) is a particularly interesting scheme to maximize accuracy under a tight\nmemory constraint. However, the lack of sub-byte instruction set architecture\n(ISA) support in SoA microprocessors makes it hard to fully exploit this\nextreme quantization paradigm in embedded MCUs. Support for sub-byte and\nasymmetric QNNs would require many precision formats and an exorbitant amount\nof opcode space. In this work, we attack this problem with status-based SIMD\ninstructions: rather than encoding precision explicitly, each operand's\nprecision is set dynamically in a core status register. We propose a novel\nRISC-V ISA core MPIC (Mixed Precision Inference Core) based on the open-source\nRI5CY core. Our approach enables full support for mixed-precision QNN inference\nwith different combinations of operands at 16-, 8-, 4- and 2-bit precision,\nwithout adding any extra opcode or increasing the complexity of the decode\nstage. Our results show that MPIC improves both performance and energy\nefficiency by a factor of 1.1-4.9x when compared to software-based\nmixed-precision on RI5CY; with respect to commercially available Cortex-M4 and\nM7 microcontrollers, it delivers 3.6-11.7x better performance and 41-155x\nhigher efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:51:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ottavi", "Gianmarco", ""], ["Garofalo", "Angelo", ""], ["Tagliavini", "Giuseppe", ""], ["Conti", "Francesco", ""], ["Benini", "Luca", ""], ["Rossi", "Davide", ""]]}, {"id": "2010.04212", "submitter": "Gopinath Chennupati", "authors": "Gopinath Chennupati and Nandakishore Santhi and Phill Romero and\n  Stephan Eidenbenz", "title": "Machine Learning Enabled Scalable Performance Prediction of Scientific\n  Codes", "comments": "Under review at ACM TOMACS 2020 for a special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Analytical Memory Model with Pipelines (AMMP) of the\nPerformance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and\nhardware architecture parameters as input, predicts runtime of that code on the\ntarget hardware platform, which is defined in the input parameters. PPT-AMMP\ntransforms the code to an (architecture-independent) intermediate\nrepresentation, then (i) analyzes the basic block structure of the code, (ii)\nprocesses architecture-independent virtual memory access patterns that it uses\nto build memory reuse distance distribution models for each basic block, (iii)\nruns detailed basic-block level simulations to determine hardware pipeline\nusage.\n  PPT-AMMP uses machine learning and regression techniques to build the\nprediction models based on small instances of the input code, then integrates\ninto a higher-order discrete-event simulation model of PPT running on Simian\nPDES engine. We validate PPT-AMMP on four standard computational physics\nbenchmarks, finally present a use case of hardware parameter sensitivity\nanalysis to identify bottleneck hardware resources on different code inputs. We\nfurther extend PPT-AMMP to predict the performance of scientific application\n(radiation transport), SNAP. We analyze the application of multi-variate\nregression models that accurately predict the reuse profiles and the basic\nblock counts. The predicted runtimes of SNAP when compared to that of actual\ntimes are accurate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 18:50:29 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 21:13:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Chennupati", "Gopinath", ""], ["Santhi", "Nandakishore", ""], ["Romero", "Phill", ""], ["Eidenbenz", "Stephan", ""]]}, {"id": "2010.04566", "submitter": "Hayate Okuhara", "authors": "Hayate Okuhara, Ahmed Elnaqib, Davide Rossi, Alfio Di Mauro, Philipp\n  Mayer, Pierpaolo Palestri, Luca Benini", "title": "An Energy-Efficient Low-Voltage Swing Transceiver for mW-Range IoT\n  End-Nodes", "comments": "ISCAS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the Internet-of-Things (IoT) applications become more and more pervasive,\nIoT end nodes are requiring more and more computational power within a few mW\nof power envelope, coupled with high-speed and energy-efficient inter-chip\ncommunication to deal with the growing input/output and memory bandwidth for\nemerging near-sensor analytics applications. While traditional interfaces such\nas SPI cannot cope with these tight requirements, low-voltage swing\ntransceivers can tackle this challenge thanks to their capability to achieve\nseveral Gbps of bandwidth at extremely low power. However, recent research on\nhigh-speed serial links addressed this challenge only partially, proposing only\npartial or stand-alone designs, and not addressing their integration in real\nsystems and the related implications. In this paper, we present for the first\ntime a complete design and system-level architecture of a low-voltage swing\ntransceiver integrated within a low-power (mW range) IoT end-node processors,\nand we compare it with existing microcontroller interfaces. The transceiver,\nimplemented in a commercial 65-nm CMOS technology achieves 10.2x higher energy\nefficiency at 15.7x higher performance than traditional microcontroller\nperipherals (single lane).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:33:11 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Okuhara", "Hayate", ""], ["Elnaqib", "Ahmed", ""], ["Rossi", "Davide", ""], ["Di Mauro", "Alfio", ""], ["Mayer", "Philipp", ""], ["Palestri", "Pierpaolo", ""], ["Benini", "Luca", ""]]}, {"id": "2010.04633", "submitter": "Philipp Krause", "authors": "Philipp Klaus Krause, Nicolas Lesser", "title": "C for a tiny system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We have implemented support for Padauk microcontrollers, tiny 8-Bit devices\nwith 60 B to 256 B of RAM, in the Small Device C Compiler (SDCC), showing that\nthe use of (mostly) standard C to program such minimal devices is feasible. We\nreport on our experience and on the difficulties in supporting the hardware\nmultithreading present on some of these devices. To make the devices a better\ntarget for C, we propose various enhancements of the architecture, and\nempirically evaluated their impact on code size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:25:33 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Krause", "Philipp Klaus", ""], ["Lesser", "Nicolas", ""]]}, {"id": "2010.05037", "submitter": "Samuel Hsia", "authors": "Samuel Hsia, Udit Gupta, Mark Wilkening, Carole-Jean Wu, Gu-Yeon Wei\n  and David Brooks", "title": "Cross-Stack Workload Characterization of Deep Recommendation Systems", "comments": "Published in 2020 IEEE International Symposium on Workload\n  Characterization (IISWC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based recommendation systems form the backbone of most\npersonalized cloud services. Though the computer architecture community has\nrecently started to take notice of deep recommendation inference, the resulting\nsolutions have taken wildly different approaches - ranging from near memory\nprocessing to at-scale optimizations. To better design future hardware systems\nfor deep recommendation inference, we must first systematically examine and\ncharacterize the underlying systems-level impact of design decisions across the\ndifferent levels of the execution stack. In this paper, we characterize eight\nindustry-representative deep recommendation models at three different levels of\nthe execution stack: algorithms and software, systems platforms, and hardware\nmicroarchitectures. Through this cross-stack characterization, we first show\nthat system deployment choices (i.e., CPUs or GPUs, batch size granularity) can\ngive us up to 15x speedup. To better understand the bottlenecks for further\noptimization, we look at both software operator usage breakdown and CPU\nfrontend and backend microarchitectural inefficiencies. Finally, we model the\ncorrelation between key algorithmic model architecture features and hardware\nbottlenecks, revealing the absence of a single dominant algorithmic component\nbehind each hardware bottleneck.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 16:11:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hsia", "Samuel", ""], ["Gupta", "Udit", ""], ["Wilkening", "Mark", ""], ["Wu", "Carole-Jean", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "2010.05197", "submitter": "Kossar Pourahmadi Meibodi", "authors": "Reza Hojabr, Kamyar Givaki, Kossar Pourahmadi, Parsa Nooralinejad,\n  Ahmad Khonsari, Dara Rahmati, M. Hassan Najafi", "title": "TaxoNN: A Light-Weight Accelerator for Deep Neural Network Training", "comments": "Accepted to ISCAS 2020. 5 pages, 5 figures", "journal-ref": "2020 IEEE International Symposium on Circuits and Systems (ISCAS),\n  2020, pp. 1-5", "doi": "10.1109/ISCAS45731.2020.9181001", "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging intelligent embedded devices rely on Deep Neural Networks (DNNs) to\nbe able to interact with the real-world environment. This interaction comes\nwith the ability to retrain DNNs, since environmental conditions change\ncontinuously in time. Stochastic Gradient Descent (SGD) is a widely used\nalgorithm to train DNNs by optimizing the parameters over the training data\niteratively. In this work, first we present a novel approach to add the\ntraining ability to a baseline DNN accelerator (inference only) by splitting\nthe SGD algorithm into simple computational elements. Then, based on this\nheuristic approach we propose TaxoNN, a light-weight accelerator for DNN\ntraining. TaxoNN can easily tune the DNN weights by reusing the hardware\nresources used in the inference process using a time-multiplexing approach and\nlow-bitwidth units. Our experimental results show that TaxoNN delivers, on\naverage, 0.97% higher misclassification rate compared to a full-precision\nimplementation. Moreover, TaxoNN provides 2.1$\\times$ power saving and\n1.65$\\times$ area reduction over the state-of-the-art DNN training accelerator.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:04:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hojabr", "Reza", ""], ["Givaki", "Kamyar", ""], ["Pourahmadi", "Kossar", ""], ["Nooralinejad", "Parsa", ""], ["Khonsari", "Ahmad", ""], ["Rahmati", "Dara", ""], ["Najafi", "M. Hassan", ""]]}, {"id": "2010.05894", "submitter": "Wenqi Jiang", "authors": "Wenqi Jiang, Zhenhao He, Shuai Zhang, Thomas B. Preu{\\ss}er, Kai Zeng,\n  Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, Ce Zhang,\n  Gustavo Alonso", "title": "MicroRec: Efficient Recommendation Inference by Hardware and Data\n  Structure Solutions", "comments": "Accepted by MLSys'21 (the 4th Conference on Machine Learning and\n  Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used in personalized recommendation systems.\nUnlike regular DNN inference workloads, recommendation inference is\nmemory-bound due to the many random memory accesses needed to lookup the\nembedding tables. The inference is also heavily constrained in terms of latency\nbecause producing a recommendation for a user must be done in about tens of\nmilliseconds. In this paper, we propose MicroRec, a high-performance inference\nengine for recommendation systems. MicroRec accelerates recommendation\ninference by (1) redesigning the data structures involved in the embeddings to\nreduce the number of lookups needed and (2) taking advantage of the\navailability of High-Bandwidth Memory (HBM) in FPGA accelerators to tackle the\nlatency by enabling parallel lookups. We have implemented the resulting design\non an FPGA board including the embedding lookup step as well as the complete\ninference process. Compared to the optimized CPU baseline (16 vCPU,\nAVX2-enabled), MicroRec achieves 13.8~14.7x speedup on embedding lookup alone\nand 2.5$~5.4x speedup for the entire recommendation inference in terms of\nthroughput. As for latency, CPU-based engines needs milliseconds for inferring\na recommendation while MicroRec only takes microseconds, a significant\nadvantage in real-time recommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:42:30 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 10:44:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Jiang", "Wenqi", ""], ["He", "Zhenhao", ""], ["Zhang", "Shuai", ""], ["Preu\u00dfer", "Thomas B.", ""], ["Zeng", "Kai", ""], ["Feng", "Liang", ""], ["Zhang", "Jiansong", ""], ["Liu", "Tongxuan", ""], ["Li", "Yong", ""], ["Zhou", "Jingren", ""], ["Zhang", "Ce", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2010.06075", "submitter": "Young-Kyu Choi", "authors": "Young-kyu Choi, Yuze Chi, Jie Wang, Licheng Guo, Jason Cong", "title": "When HLS Meets FPGA HBM: Benchmarking and Bandwidth Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent release of High Bandwidth Memory (HBM) based FPGA boards,\ndevelopers can now exploit unprecedented external memory bandwidth. This allows\nmore memory-bounded applications to benefit from FPGA acceleration. However, we\nfound that it is not easy to fully utilize the available bandwidth when\ndeveloping some applications with high-level synthesis (HLS) tools. This is due\nto the limitation of existing HLS tools when accessing HBM board's large number\nof independent external memory channels. In this paper, we measure the\nperformance of three recent representative HBM FPGA boards (Intel's Stratix 10\nMX and Xilinx's Alveo U50/U280 boards) with microbenchmarks and analyze the HLS\noverhead. Next, we propose HLS-based optimization techniques to improve the\neffective bandwidth when a PE accesses multiple HBM channels or multiple PEs\naccess an HBM channel. Our experiment demonstrates that the effective bandwidth\nimproves by 2.4X-3.8X. We also provide a list of insights for future\nimprovement of the HBM FPGA HLS design flow.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:23:18 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Choi", "Young-kyu", ""], ["Chi", "Yuze", ""], ["Wang", "Jie", ""], ["Guo", "Licheng", ""], ["Cong", "Jason", ""]]}, {"id": "2010.06156", "submitter": "Songming Yu", "authors": "Songming Yu and Yongpan Liu and Lu Zhang and Jingyu Wang and Jinshan\n  Yue and Zhuqing Yuan and Xueqing Li and Huazhong Yang", "title": "High Area/Energy Efficiency RRAM CNN Accelerator with Kernel-Reordering\n  Weight Mapping Scheme Based on Pattern Pruning", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive Random Access Memory (RRAM) is an emerging device for\nprocessing-in-memory (PIM) architecture to accelerate convolutional neural\nnetwork (CNN). However, due to the highly coupled crossbar structure in the\nRRAM array, it is difficult to exploit the sparsity of the network in\nRRAM-based CNN accelerator. To optimize the weight mapping of sparse network in\nthe RRAM array and achieve high area and energy efficiency, we propose a novel\nweight mapping scheme and corresponding RRAM-based CNN accelerator architecture\nbased on pattern pruning and Operation Unit(OU) mechanism. Experimental results\nshow that our work can achieve 4.16x-5.20x crossbar area efficiency,\n1.98x-2.15x energy efficiency, and 1.15x-1.35x performance speedup in\ncomparison with the traditional weight mapping method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 04:08:33 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yu", "Songming", ""], ["Liu", "Yongpan", ""], ["Zhang", "Lu", ""], ["Wang", "Jingyu", ""], ["Yue", "Jinshan", ""], ["Yuan", "Zhuqing", ""], ["Li", "Xueqing", ""], ["Yang", "Huazhong", ""]]}, {"id": "2010.06277", "submitter": "Stijn Eyerman", "authors": "Sriram Aananthakrishnan, Nesreen K. Ahmed, Vincent Cave, Marcelo\n  Cintra, Yigit Demir, Kristof Du Bois, Stijn Eyerman, Joshua B. Fryman, Ivan\n  Ganev, Wim Heirman, Hans-Christian Hoppe, Jason Howard, Ibrahim Hur,\n  MidhunChandra Kodiyath, Samkit Jain, Daniel S. Klowden, Marek M. Landowski,\n  Laurent Montigny, Ankit More, Przemyslaw Ossowski, Robert Pawlowski, Nick\n  Pepperling, Fabrizio Petrini, Mariusz Sikora, Balasubramanian Seshasayee,\n  Shaden Smith, Sebastian Szkoda, Sanjaya Tayal, Jesmin Jahan Tithi, Yves\n  Vandriessche, Izajasz P. Wrosz", "title": "PIUMA: Programmable Integrated Unified Memory Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance large scale graph analytics is essential to timely analyze\nrelationships in big data sets. Conventional processor architectures suffer\nfrom inefficient resource usage and bad scaling on graph workloads. To enable\nefficient and scalable graph analysis, Intel developed the Programmable\nIntegrated Unified Memory Architecture (PIUMA). PIUMA consists of many\nmulti-threaded cores, fine-grained memory and network accesses, a globally\nshared address space and powerful offload engines. This paper presents the\nPIUMA architecture, and provides initial performance estimations, projecting\nthat a PIUMA node will outperform a conventional compute node by one to two\norders of magnitude. Furthermore, PIUMA continues to scale across multiple\nnodes, which is a challenge in conventional multinode setups.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:35:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Aananthakrishnan", "Sriram", ""], ["Ahmed", "Nesreen K.", ""], ["Cave", "Vincent", ""], ["Cintra", "Marcelo", ""], ["Demir", "Yigit", ""], ["Bois", "Kristof Du", ""], ["Eyerman", "Stijn", ""], ["Fryman", "Joshua B.", ""], ["Ganev", "Ivan", ""], ["Heirman", "Wim", ""], ["Hoppe", "Hans-Christian", ""], ["Howard", "Jason", ""], ["Hur", "Ibrahim", ""], ["Kodiyath", "MidhunChandra", ""], ["Jain", "Samkit", ""], ["Klowden", "Daniel S.", ""], ["Landowski", "Marek M.", ""], ["Montigny", "Laurent", ""], ["More", "Ankit", ""], ["Ossowski", "Przemyslaw", ""], ["Pawlowski", "Robert", ""], ["Pepperling", "Nick", ""], ["Petrini", "Fabrizio", ""], ["Sikora", "Mariusz", ""], ["Seshasayee", "Balasubramanian", ""], ["Smith", "Shaden", ""], ["Szkoda", "Sebastian", ""], ["Tayal", "Sanjaya", ""], ["Tithi", "Jesmin Jahan", ""], ["Vandriessche", "Yves", ""], ["Wrosz", "Izajasz P.", ""]]}, {"id": "2010.07185", "submitter": "Cong (Callie) Hao", "authors": "Cong Hao, Yao Chen, Xiaofan Zhang, Yuhong Li, Jinjun Xiong, Wen-mei\n  Hwu and Deming Chen", "title": "Effective Algorithm-Accelerator Co-design for AI Solutions on Edge\n  Devices", "comments": "GLSVLSI, September 7-9, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality AI solutions require joint optimization of AI algorithms, such\nas deep neural networks (DNNs), and their hardware accelerators. To improve the\noverall solution quality as well as to boost the design productivity, efficient\nalgorithm and accelerator co-design methodologies are indispensable. In this\npaper, we first discuss the motivations and challenges for the\nAlgorithm/Accelerator co-design problem and then provide several effective\nsolutions. Especially, we highlight three leading works of effective co-design\nmethodologies: 1) the first simultaneous DNN/FPGA co-design method; 2) a\nbi-directional lightweight DNN and accelerator co-design method; 3) a\ndifferentiable and efficient DNN and accelerator co-search method. We\ndemonstrate the effectiveness of the proposed co-design approaches using\nextensive experiments on both FPGAs and GPUs, with comparisons to existing\nworks. This paper emphasizes the importance and efficacy of\nalgorithm-accelerator co-design and calls for more research breakthroughs in\nthis interesting and demanding area.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 15:59:10 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 13:56:51 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Hao", "Cong", ""], ["Chen", "Yao", ""], ["Zhang", "Xiaofan", ""], ["Li", "Yuhong", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "2010.07746", "submitter": "Christopher Hahne", "authors": "Christopher Hahne, Andrew Lumsdaine, Amar Aggoun, Vladan Velisavljevic", "title": "Real-Time Refocusing using an FPGA-based Standard Plenoptic Camera", "comments": "IEEE Trans. on Industrial Electronics", "journal-ref": "Volume: 65, Issue: 12, Dec. 2018, Pages: 9757 - 9766", "doi": "10.1109/TIE.2018.2818644", "report-no": null, "categories": "eess.IV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenoptic cameras are receiving increasing attention in scientific and\ncommercial applications because they capture the entire structure of light in a\nscene, enabling optical transforms (such as focusing) to be applied\ncomputationally after the fact, rather than once and for all at the time a\npicture is taken. In many settings, real-time interactive performance is also\ndesired, which in turn requires significant computational power due to the\nlarge amount of data required to represent a plenoptic image. Although GPUs\nhave been shown to provide acceptable performance for real-time plenoptic\nrendering, their cost and power requirements make them prohibitive for embedded\nuses (such as in-camera). On the other hand, the computation to accomplish\nplenoptic rendering is well-structured, suggesting the use of specialized\nhardware. Accordingly, this paper presents an array of switch-driven Finite\nImpulse Response (FIR) filters, implemented with FPGA to accomplish\nhigh-throughput spatial-domain rendering. The proposed architecture provides a\npower-efficient rendering hardware design suitable for full-video applications\nas required in broadcasting or cinematography. A benchmark assessment of the\nproposed hardware implementation shows that real-time performance can readily\nbe achieved, with a one order of magnitude performance improvement over a GPU\nimplementation and three orders of magnitude performance improvement over a\ngeneral-purpose CPU implementation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:42:30 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Hahne", "Christopher", ""], ["Lumsdaine", "Andrew", ""], ["Aggoun", "Amar", ""], ["Velisavljevic", "Vladan", ""]]}, {"id": "2010.08065", "submitter": "Omar Mohamed Awad", "authors": "Omar Mohamed Awad, Mostafa Mahmoud, Isak Edo, Ali Hadi Zadeh, Ciaran\n  Bannon, Anand Jayarajan, Gennady Pekhimenko, Andreas Moshovos", "title": "FPRaker: A Processing Element For Accelerating Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present FPRaker, a processing element for composing training accelerators.\nFPRaker processes several floating-point multiply-accumulation operations\nconcurrently and accumulates their result into a higher precision accumulator.\nFPRaker boosts performance and energy efficiency during training by taking\nadvantage of the values that naturally appear during training. Specifically, it\nprocesses the significand of the operands of each multiply-accumulate as a\nseries of signed powers of two. The conversion to this form is done on-the-fly.\nThis exposes ineffectual work that can be skipped: values when encoded have few\nterms and some of them can be discarded as they would fall outside the range of\nthe accumulator given the limited precision of floating-point. We demonstrate\nthat FPRaker can be used to compose an accelerator for training and that it can\nimprove performance and energy efficiency compared to using conventional\nfloating-point units under ISO-compute area constraints. We also demonstrate\nthat FPRaker delivers additional benefits when training incorporates pruning\nand quantization. Finally, we show that FPRaker naturally amplifies performance\nwith training methods that use a different precision per layer.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:24:10 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Awad", "Omar Mohamed", ""], ["Mahmoud", "Mostafa", ""], ["Edo", "Isak", ""], ["Zadeh", "Ali Hadi", ""], ["Bannon", "Ciaran", ""], ["Jayarajan", "Anand", ""], ["Pekhimenko", "Gennady", ""], ["Moshovos", "Andreas", ""]]}, {"id": "2010.08262", "submitter": "Bernd Illing", "authors": "Bernd Illing, Jean Ventura, Guillaume Bellec, Wulfram Gerstner", "title": "Local plasticity rules can learn deep representations using\n  self-supervised contrastive predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in the brain is poorly understood and learning rules that respect\nbiological constraints, yet yield deep hierarchical representations, are still\nunknown. Here, we propose a learning rule that takes inspiration from\nneuroscience and recent advances in self-supervised deep learning. Learning\nminimizes a simple layer-specific loss function and does not need to\nback-propagate error signals within or between layers. Instead, weight updates\nfollow a local, Hebbian, learning rule that only depends on pre- and\npost-synaptic neuronal activity, predictive dendritic input and widely\nbroadcasted modulation factors which are identical for large groups of neurons.\nThe learning rule applies contrastive predictive learning to a causal,\nbiological setting using saccades (i.e. rapid shifts in gaze direction). We\nfind that networks trained with this self-supervised and local rule build deep\nhierarchical representations of images, speech and video.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:32:35 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:13:54 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 10:51:10 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 13:30:39 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Illing", "Bernd", ""], ["Ventura", "Jean", ""], ["Bellec", "Guillaume", ""], ["Gerstner", "Wulfram", ""]]}, {"id": "2010.08412", "submitter": "Rumen Dangovski", "authors": "Matthew Khoury and Rumen Dangovski and Longwu Ou and Preslav Nakov and\n  Yichen Shen and Li Jing", "title": "Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for\n  Low-Latency Inference in NLP Applications", "comments": "To appear at the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP '20), November 16-20, 2020, NMT, AI accelerators,\n  co-design, TPU, OPU, 10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become the standard approach to building reliable\nNatural Language Processing (NLP) applications, ranging from Neural Machine\nTranslation (NMT) to dialogue systems. However, improving accuracy by\nincreasing the model size requires a large number of hardware computations,\nwhich can slow down NLP applications significantly at inference time. To\naddress this issue, we propose a novel vector-vector-matrix architecture\n(VVMA), which greatly reduces the latency at inference time for NMT. This\narchitecture takes advantage of specialized hardware that has low-latency\nvector-vector operations and higher-latency vector-matrix operations. It also\nreduces the number of parameters and FLOPs for virtually all models that rely\non efficient matrix multipliers without significantly impacting accuracy. We\npresent empirical results suggesting that our framework can reduce the latency\nof sequence-to-sequence and Transformer models used for NMT by a factor of\nfour. Finally, we show evidence suggesting that our VVMA extends to other\ndomains, and we discuss novel hardware for its efficient use.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:54:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Khoury", "Matthew", ""], ["Dangovski", "Rumen", ""], ["Ou", "Longwu", ""], ["Nakov", "Preslav", ""], ["Shen", "Yichen", ""], ["Jing", "Li", ""]]}, {"id": "2010.08440", "submitter": "Shweta Shinde", "authors": "Zhijingcheng Yu and Shweta Shinde and Trevor E. Carlson and Prateek\n  Saxena", "title": "Elasticlave: An Efficient Memory Model for Enclaves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trusted-execution environments (TEE), like Intel SGX, isolate user-space\napplications into secure enclaves without trusting the OS. Thus, TEEs reduce\nthe trusted computing base, but add one to two orders of magnitude slow-down.\nThe performance cost stems from a strict memory model, which we call the\nspatial isolation model, where enclaves cannot share memory regions with each\nother. In this work, we present Elasticlave---a new TEE memory model that\nallows enclaves to selectively and temporarily share memory with other enclaves\nand the OS. Elasticlave eliminates the need for expensive data copy operations,\nwhile offering the same level of application-desired security as possible with\nthe spatial model. We prototype Elasticlave design on an RTL-designed\ncycle-level RISC-V core and observe 1 to 2 orders of magnitude performance\nimprovements over the spatial model implemented with the same processor\nconfiguration. Elasticlave has a small TCB. We find that its performance\ncharacteristics and hardware area footprint scale well with the number of\nshared memory regions it is configured to support.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:12:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yu", "Zhijingcheng", ""], ["Shinde", "Shweta", ""], ["Carlson", "Trevor E.", ""], ["Saxena", "Prateek", ""]]}, {"id": "2010.08667", "submitter": "Hao Luan", "authors": "Hao Luan, Alan Gatherer", "title": "Combinatorics and Geometry for the Many-ported, Distributed and Shared\n  Memory Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manycore SoC architectures based on on-chip shared memory are preferred for\nflexible and programmable solutions in many application domains. However, the\ndevelopment of many ported memory is becoming increasingly challenging as we\napproach the end of Moore's Law while systems requirements demand larger shared\nmemory and more access ports. Memory can no longer be designed simply to\nminimize single transaction access time, but must take into account the\nfunctionality on the SoC. In this paper we examine a common large memory usage\nin SoC, where the memory is used as storage for large buffers that are then\nmoved for time scheduled processing. We merge two aspects of many ported memory\ndesign, combinatorial analysis of interconnect, and geometric analysis of\ncritical paths, extending both to show that in this case the SoC performance\nbenefits significantly from a hierarchical, distributed and staged architecture\nwith lower-radix switches and fractal randomization of memory bank addressing,\nalong with judicious and geometry aware application of speed up. The results\npresented show the new architecture supports 20% higher throughput with 20%\nlower latency and 30% less interconnection area at approximately the same power\nconsumption. We demonstrate the flexibility and scalability of this\narchitecture on silicon from a physical design perspective by taking the design\nthrough layout. The architecture enables a much easier implementation flow that\nworks well with physically irregular port access and memory dominant layout,\nwhich is a common issue in real designs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 23:45:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Luan", "Hao", ""], ["Gatherer", "Alan", ""]]}, {"id": "2010.08916", "submitter": "Ruoshi Li", "authors": "Ruoshi Li, Hongjing Huang, Zeke Wang, Zhiyuan Shao, Xiaofei Liao, Hai\n  Jin", "title": "Optimizing Memory Performance of Xilinx FPGAs under Vitis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of research efforts have been devoted to FPGA-based acceleration, due\nto its low latency and high energy efficiency. However, using the original\nlow-level hardware description languages like Verilog to program FPGAs requires\ngenerally good knowledge of hardware design details and hand-on experiences.\nFortunately, the FPGA community intends to address this low programmability\nissues. For example, , with the intention that programming FPGAs is just as\neasy as programming GPUs. Even though Vitis is proven to increase\nprogrammability, we cannot directly obtain high performance without careful\ndesign regarding hardware pipeline and memory subsystem.In this paper, we focus\non the memory subsystem, comprehensively and systematically benchmarking the\neffect of optimization methods on memory performance. Upon benchmarking, we\nquantitatively analyze the typical memory access patterns for a broad range of\napplications, including AI, HPC, and database. Further, we also provide the\ncorresponding optimization direction for each memory access pattern so as to\nimprove overall performance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 04:14:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Ruoshi", ""], ["Huang", "Hongjing", ""], ["Wang", "Zeke", ""], ["Shao", "Zhiyuan", ""], ["Liao", "Xiaofei", ""], ["Jin", "Hai", ""]]}, {"id": "2010.09308", "submitter": "Grzegorz Ficht", "authors": "Grzegorz Ficht, Hafez Farazi, Diego Rodriguez, Dmytro Pavlichenko,\n  Philipp Allgeuer, Andre Brandenburger, Sven Behnke", "title": "NimbRo-OP2X: Affordable Adult-sized 3D-printed Open-Source Humanoid\n  Robot for Research", "comments": null, "journal-ref": null, "doi": "10.1142/S0219843620500218", "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For several years, high development and production costs of humanoid robots\nrestricted researchers interested in working in the field. To overcome this\nproblem, several research groups have opted to work with simulated or smaller\nrobots, whose acquisition costs are significantly lower. However, due to scale\ndifferences and imperfect simulation replicability, results may not be directly\nreproducible on real, adult-sized robots. In this paper, we present the\nNimbRo-OP2X, a capable and affordable adult-sized humanoid platform aiming to\nsignificantly lower the entry barrier for humanoid robot research. With a\nheight of 135 cm and weight of only 19 kg, the robot can interact in an\nunmodified, human environment without special safety equipment. Modularity in\nhardware and software allow this platform enough flexibility to operate in\ndifferent scenarios and applications with minimal effort. The robot is equipped\nwith an on-board computer with GPU, which enables the implementation of\nstate-of-the-art approaches for object detection and human perception demanded\nby areas such as manipulation and human-robot interaction. Finally, the\ncapabilities of the NimbRo-OP2X, especially in terms of locomotion stability\nand visual perception, are evaluated. This includes the performance at RoboCup\n2018, where NimbRo-OP2X won all possible awards in the AdultSize class.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:34:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ficht", "Grzegorz", ""], ["Farazi", "Hafez", ""], ["Rodriguez", "Diego", ""], ["Pavlichenko", "Dmytro", ""], ["Allgeuer", "Philipp", ""], ["Brandenburger", "Andre", ""], ["Behnke", "Sven", ""]]}, {"id": "2010.09330", "submitter": "Mohammad Sadrosadati", "authors": "Mohammad Sadrosadati, Amirhossein Mirhosseini, Ali Hajiabadi, Seyed\n  Borna Ehsani, Hajar Falahati, Hamid Sarbazi-Azad, Mario Drumond, Babak\n  Falsafi, Rachata Ausavarungnirun, Onur Mutlu", "title": "Enabling High-Capacity, Latency-Tolerant, and Highly-Concurrent GPU\n  Register Files via Software/Hardware Cooperation", "comments": "To Appear in ACM Transactions on Computer Systems (TOCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) employ large register files to accommodate\nall active threads and accelerate context switching. Unfortunately, register\nfiles are a scalability bottleneck for future GPUs due to long access latency,\nhigh power consumption, and large silicon area provisioning. Prior work\nproposes hierarchical register file to reduce the register file power\nconsumption by caching registers in a smaller register file cache.\nUnfortunately, this approach does not improve register access latency due to\nthe low hit rate in the register file cache.\n  In this paper, we propose the Latency-Tolerant Register File (LTRF)\narchitecture to achieve low latency in a two-level hierarchical structure while\nkeeping power consumption low. We observe that compile-time interval analysis\nenables us to divide GPU program execution into intervals with an accurate\nestimate of a warp's aggregate register working-set within each interval. The\nkey idea of LTRF is to prefetch the estimated register working-set from the\nmain register file to the register file cache under software control, at the\nbeginning of each interval, and overlap the prefetch latency with the execution\nof other warps. We observe that register bank conflicts while prefetching the\nregisters could greatly reduce the effectiveness of LTRF. Therefore, we devise\na compile-time register renumbering technique to reduce the likelihood of\nregister bank conflicts. Our experimental results show that LTRF enables\nhigh-capacity yet long-latency main GPU register files, paving the way for\nvarious optimizations. As an example optimization, we implement the main\nregister file with emerging high-density high-latency memory technologies,\nenabling 8X larger capacity and improving overall GPU performance by 34%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:08:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sadrosadati", "Mohammad", ""], ["Mirhosseini", "Amirhossein", ""], ["Hajiabadi", "Ali", ""], ["Ehsani", "Seyed Borna", ""], ["Falahati", "Hajar", ""], ["Sarbazi-Azad", "Hamid", ""], ["Drumond", "Mario", ""], ["Falsafi", "Babak", ""], ["Ausavarungnirun", "Rachata", ""], ["Mutlu", "Onur", ""]]}, {"id": "2010.09457", "submitter": "Bingzhao Zhu", "authors": "Bingzhao Zhu, Uisub Shin, Mahsa Shoaran", "title": "Closed-Loop Neural Interfaces with Embedded Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural interfaces capable of multi-site electrical recording, on-site signal\nclassification, and closed-loop therapy are critical for the diagnosis and\ntreatment of neurological disorders. However, deploying machine learning\nalgorithms on low-power neural devices is challenging, given the tight\nconstraints on computational and memory resources for such devices. In this\npaper, we review the recent developments in embedding machine learning in\nneural interfaces, with a focus on design trade-offs and hardware efficiency.\nWe also present our optimized tree-based model for low-power and\nmemory-efficient classification of neural signal in brain implants. Using\nenergy-aware learning and model compression, we show that the proposed oblique\ntrees can outperform conventional machine learning models in applications such\nas seizure or tremor detection and motor decoding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:34:08 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 11:56:43 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhu", "Bingzhao", ""], ["Shin", "Uisub", ""], ["Shoaran", "Mahsa", ""]]}, {"id": "2010.09852", "submitter": "Maciej Besta", "authors": "Hermann Schweizer, Maciej Besta, Torsten Hoefler", "title": "Evaluating the Cost of Atomic Operations on Modern Architectures", "comments": null, "journal-ref": "Proceedings of the 24th International Conference on Parallel\n  Architectures and Compilation (PACT'15), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add\n(FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs\nbetween these operations and various characteristics of such systems, such as\nthe structure of caches, are unclear and have not been thoroughly analyzed. In\nthis paper we establish an evaluation methodology, develop a performance model,\nand present a set of detailed benchmarks for latency and bandwidth of different\natomics. We consider various state-of-the-art x86 architectures: Intel Haswell,\nXeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising\nperformance relationships between the considered atomics and architectural\nproperties such as the coherence state of the accessed cache lines. One key\nfinding is that all the tested atomics have comparable latency and bandwidth\neven if they are characterized by different consensus numbers. Another insight\nis that the hardware implementation of atomics prevents any instruction-level\nparallelism even if there are no dependencies between the issued operations.\nFinally, we discuss solutions to the discovered performance issues in the\nanalyzed architectures. Our analysis enables simpler and more effective\nparallel programming and accelerates data processing on various architectures\ndeployed in both off-the-shelf machines and large compute systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 20:43:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Schweizer", "Hermann", ""], ["Besta", "Maciej", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.10119", "submitter": "M\\`arius Mont\\'on", "authors": "M\\`arius Mont\\'on", "title": "A RISC-V SystemC-TLM simulator", "comments": "4 pages. Presented at CARRV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents a SystemC-TLM based simulator for a RISC-V\nmicrocontroller. This simulator is focused on simplicity and easy expandable of\na RISC-V. It is built around a full RISC-V instruction set simulator that\nsupports full RISC-V ISA and extensions M, A, C, Zicsr and Zifencei. The ISS is\nencapsulated in a TLM-2 wrapper that enables it to communicate with any other\nTLM-2 compatible module. The simulator also includes a very basic set of\nperipherals to enable a complete SoC simulator. The running code can be\ncompiled with standard tools and using standard C libraries without\nmodifications. The simulator is able to correctly execute the riscv-compliance\nsuite. The entire simulator is published as a docker image to ease its\ninstallation and use by developers. A porting of FreeRTOSv10.2.1 for the\nsimulated SoC is also published.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 08:22:24 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mont\u00f3n", "M\u00e0rius", ""]]}, {"id": "2010.10233", "submitter": "Jing Wang", "authors": "Zhiping Jiang, Tom H. Luan, Xincheng Ren, Dongtao Lv, Han Hao, Jing\n  Wang, Kun Zhao, Wei Xi, Yueshen Xu, Rui Li", "title": "Eliminating the Barriers: Demystifying Wi-Fi Baseband Design and\n  Introducing the PicoScenes Wi-Fi Sensing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on Wi-Fi sensing over the past decade has been thriving, but not\nsmooth. Three major barriers severely hamper the research, namely, the unknown\nbaseband design and its influence on CSI, inability to access the low-level\nhardware controls and the lack of a flexible and versatile software toolkit for\nhardware control. This paper tries to break the above three barriers from the\nfollowing aspects. First, an in-depth study on the baseband design of QCA9300,\nthe popular CSI-enabled Wi-Fi NIC, is presented. The lessons learned is of\ngreat guiding significance for understanding what other commercial\noff-the-shelf NICs. Second, several valuable features of QCA9300 are unlocked\nfor research, such as the arbitrary tuning for both the carrier frequency and\nbaseband sampling rate. By leveraging the unlocked features, we identify three\nimportant types of CSI distortion, and pinpoint their origin through extensive\nevaluations. Last, we develop and release PicoScenes, a powerful,\nhardware-unified and extensible Wi-Fi sensing system. PicoScenes allows direct\naccess to the unlocked features of QCA9300 and IWL5300, and therefore greatly\nfacilitate the research on Wi-Fi sensing. It also supports the SDR-based Wi-Fi\nsensing by embedding a 802.11a/g/n/ac/ax software baseband implementation. We\nrelease PicoScenes at https://zpj.io/ps.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 12:53:38 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 05:52:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jiang", "Zhiping", ""], ["Luan", "Tom H.", ""], ["Ren", "Xincheng", ""], ["Lv", "Dongtao", ""], ["Hao", "Han", ""], ["Wang", "Jing", ""], ["Zhao", "Kun", ""], ["Xi", "Wei", ""], ["Xu", "Yueshen", ""], ["Li", "Rui", ""]]}, {"id": "2010.10370", "submitter": "Jean-Fran\\c{c}ois Determe", "authors": "Jean-Fran\\c{c}ois Determe and Sophia Azzagnuni and Utkarsh Singh and\n  Fran\\c{c}ois Horlin and Philippe De Doncker", "title": "Monitoring Large Crowds With WiFi: A Privacy-Preserving Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AR cs.CR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a crowd monitoring system based on the passive detection\nof probe requests. The system meets strict privacy requirements and is suited\nto monitoring events or buildings with a least a few hundreds of attendees. We\npresent our counting process and an associated mathematical model. From this\nmodel, we derive a concentration inequality that highlights the accuracy of our\ncrowd count estimator. Then, we describe our system. We present and discuss our\nsensor hardware, our computing system architecture, and an efficient\nimplementation of our counting algorithm---as well as its space and time\ncomplexity. We also show how our system ensures the privacy of people in the\nmonitored area. Finally, we validate our system using nine weeks of data from a\npublic library endowed with a camera-based counting system, which generates\ncounts against which we compare those of our counting system. This comparison\nempirically quantifies the accuracy of our counting system, thereby showing it\nto be suitable for monitoring public areas. Similarly, the concentration\ninequality provides a theoretical validation of the system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:23:30 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Determe", "Jean-Fran\u00e7ois", ""], ["Azzagnuni", "Sophia", ""], ["Singh", "Utkarsh", ""], ["Horlin", "Fran\u00e7ois", ""], ["De Doncker", "Philippe", ""]]}, {"id": "2010.10416", "submitter": "Moritz Schneider", "authors": "Moritz Schneider, Aritra Dhar, Ivan Puddu, Kari Kostiainen, Srdjan\n  Capkun", "title": "PIE: A Platform-wide TEE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern computing architectures rely on specialized hardware such as\naccelerators to provide performance and functionality, trusted execution\nenvironments (TEEs), one of the most promising recent developments in security,\ncan only protect code confined in the CPU, limiting TEEs potential and\napplicability to a handful of applications. We observe that the TEEs' hardware\ntrusted computing base (TCB) is fixed at design time, forcing users to rely on\n(mostly untrustworthy) software to allow peripherals into the TEE. Based on\nthis observation, we propose PIE, a secure platform design with a configurable\nhardware and software TCB, which allows us to support specialized hardware\nwhile ensuring the least privilege principle. We introduce two new security\nproperties relevant to such systems: platform-wide attestation and platform\nawareness. Platform-wide attestation allows to remotely verify the platform's\ncurrent state, including the state of specialized hardware devices and how they\nare connected with each other, whereas platform awareness defines how the\nenclave reacts upon a change in connected devices. Together, these allow to\nattest to the hardware configuration of a system and check that only the\ntrusted hardware with the right version of its firmware is part of the TCB\n(platform-wide attestation) and will stay part of the TCB for the whole\nexecution (platform awareness). Finally, we present a prototype of PIE based on\nRISC-V's Keystone to show that such systems are feasible with only around 600\nlines added to the software TCB, without compromising performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:27:23 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 10:06:10 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Schneider", "Moritz", ""], ["Dhar", "Aritra", ""], ["Puddu", "Ivan", ""], ["Kostiainen", "Kari", ""], ["Capkun", "Srdjan", ""]]}, {"id": "2010.10562", "submitter": "Khushal Sethi", "authors": "Khushal Sethi, Manan Suri", "title": "NV-Fogstore : Device-aware hybrid caching in fog computing environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge caching via the placement of distributed storages throughout the network\nis a promising solution to reduce latency and network costs of content\ndelivery. With the advent of the upcoming 5G future, billions of F-RAN\n(Fog-Radio Access Network) nodes will created and used for for the purpose of\nEdge Caching. Hence, the total amount of memory deployed at the edge is\nexpected to increase 100 times.\n  Currently, used DRAM-based caches in CDN (Content Delivery Networks) are\nextremely power-hungry and costly. Our purpose is to reduce the cost of\nownership and recurring costs (of power consumption) in an F-RAN node while\nmaintaining Quality of Service.\n  For our purpose, we propose NV-FogStore, a scalable hybrid key-value storage\narchitecture for the utilization of Non-Volatile Memories (such as RRAM, MRAM,\nIntel Optane) in Edge Cache.\n  We further describe in detail a novel, hierarchical, write-damage, size and\nfrequency aware content caching policy H-GREEDY for our architecture.\n  We show that our policy can be tuned as per performance objectives, to lower\nthe power, energy consumption and total cost over an only DRAM-based system for\nonly a relatively smaller trade-off in the average access latency.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:44:48 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Sethi", "Khushal", ""], ["Suri", "Manan", ""]]}, {"id": "2010.10683", "submitter": "Maciej Besta", "authors": "Maciej Besta, Syed Minhaj Hassan, Sudhakar Yalamanchili, Rachata\n  Ausavarungnirun, Onur Mutlu, Torsten Hoefler", "title": "Slim NoC: A Low-Diameter On-Chip Network Topology for High Energy\n  Efficiency and Scalability", "comments": null, "journal-ref": "Proceedings of the 23rd ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems\n  (ASPLOS'18), 2018", "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging chips with hundreds and thousands of cores require networks with\nunprecedented energy/area efficiency and scalability. To address this, we\npropose Slim NoC (SN): a new on-chip network design that delivers significant\nimprovements in efficiency and scalability compared to the state-of-the-art.\nThe key idea is to use two concepts from graph and number theory,\ndegree-diameter graphs combined with non-prime finite fields, to enable the\nsmallest number of ports for a given core count. SN is inspired by\nstate-of-the-art off-chip topologies; it identifies and distills their\nadvantages for NoC settings while solving several key issues that lead to\nsignificant overheads on-chip. SN provides NoC-specific layouts, which further\nenhance area/energy efficiency. We show how to augment SN with state-of-the-art\nrouter microarchitecture schemes such as Elastic Links, to make the network\neven more scalable and efficient. Our extensive experimental evaluations show\nthat SN outperforms both traditional low-radix topologies (e.g., meshes and\ntori) and modern high-radix networks (e.g., various Flattened Butterflies) in\narea, latency, throughput, and static/dynamic power consumption for both\nsynthetic and real workloads. SN provides a promising direction in scalable and\nenergy-efficient NoC topologies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:30:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Besta", "Maciej", ""], ["Hassan", "Syed Minhaj", ""], ["Yalamanchili", "Sudhakar", ""], ["Ausavarungnirun", "Rachata", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.11686", "submitter": "Ren-Song Tsay", "authors": "Tsung-Ying Lu, Hsu-Hsun Chin, Hsin-I Wu, and Ren-Song Tsay", "title": "A Very Compact Embedded CNN Processor Design Based on Logarithmic\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a very compact embedded CNN processor design based\non a modified logarithmic computing method using very low bit-width\nrepresentation. Our high-quality CNN processor can easily fit into edge\ndevices. For Yolov2, our processing circuit takes only 0.15 mm2 using TSMC 40\nnm cell library. The key idea is to constrain the activation and weight values\nof all layers uniformly to be within the range [-1, 1] and produce low\nbit-width logarithmic representation. With the uniform representations, we\ndevise a unified, reusable CNN computing kernel and significantly reduce\ncomputing resources. The proposed approach has been extensively evaluated on\nmany popular image classification CNN models (AlexNet, VGG16, and ResNet-18/34)\nand object detection models (Yolov2). The hardware-implemented results show\nthat our design consumes only minimal computing and storage resources, yet\nattains very high accuracy. The design is thoroughly verified on FPGAs, and the\nSoC integration is underway with promising results. With extremely efficient\nresource and energy usage, our design is excellent for edge computing purposes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 23:48:36 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lu", "Tsung-Ying", ""], ["Chin", "Hsu-Hsun", ""], ["Wu", "Hsin-I", ""], ["Tsay", "Ren-Song", ""]]}, {"id": "2010.11741", "submitter": "Venkata Pavan Kumar Miriyala Mr", "authors": "Venkata Pavan Kumar Miriyala, Masatoshi Ishii", "title": "Ultra-low power on-chip learning of speech commands with phase-change\n  memories", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cond-mat.dis-nn cs.AR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding artificial intelligence at the edge (edge-AI) is an elegant\nsolution to tackle the power and latency issues in the rapidly expanding\nInternet of Things. As edge devices typically spend most of their time in sleep\nmode and only wake-up infrequently to collect and process sensor data,\nnon-volatile in-memory computing (NVIMC) is a promising approach to design the\nnext generation of edge-AI devices. Recently, we proposed an NVIMC-based\nneuromorphic accelerator using the phase change memories (PCMs), which we call\nas Raven. In this work, we demonstrate the ultra-low-power on-chip training and\ninference of speech commands using Raven. We showed that Raven can be trained\non-chip with power consumption as low as 30~uW, which is suitable for edge\napplications. Furthermore, we showed that at iso-accuracies, Raven needs 70.36x\nand 269.23x less number of computations to be performed than a deep neural\nnetwork (DNN) during inference and training, respectively. Owing to such low\npower and computational requirements, Raven provides a promising pathway\ntowards ultra-low-power training and inference at the edge.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:08:46 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Miriyala", "Venkata Pavan Kumar", ""], ["Ishii", "Masatoshi", ""]]}, {"id": "2010.12114", "submitter": "Stephen Ibanez", "authors": "Stephen Ibanez, Alex Mallery, Serhat Arslan, Theo Jepsen, Muhammad\n  Shahbaz, Nick McKeown, Changhoon Kim", "title": "The nanoPU: Redesigning the CPU-Network Interface to Minimize RPC Tail\n  Latency", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nanoPU is a new networking-optimized CPU designed to minimize tail\nlatency for RPCs. By bypassing the cache and memory hierarchy, the nanoPU\ndirectly places arriving messages into the CPU register file. The wire-to-wire\nlatency through the application is just 65ns, about 13x faster than the current\nstate-of-the-art. The nanoPU moves key functions from software to hardware:\nreliable network transport, congestion control, core selection, and thread\nscheduling. It also supports a unique feature to bound the tail latency\nexperienced by high-priority applications. Our prototype nanoPU is based on a\nmodified RISC-V CPU; we evaluate its performance using cycle-accurate\nsimulations of 324 cores on AWS FPGAs, including real applications (MICA and\nchain replication).\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 00:42:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ibanez", "Stephen", ""], ["Mallery", "Alex", ""], ["Arslan", "Serhat", ""], ["Jepsen", "Theo", ""], ["Shahbaz", "Muhammad", ""], ["McKeown", "Nick", ""], ["Kim", "Changhoon", ""]]}, {"id": "2010.12376", "submitter": "Javier Hormigo", "authors": "Javier Hormigo and Sergio D. Mu\\~noz", "title": "Efficient Floating-Point Givens Rotation Unit", "comments": "25 pages, 11 figures, this is a pre-print version of an article that\n  has been accepted for publication in the journal Circuits, Systems, and\n  Signal Processing", "journal-ref": null, "doi": "10.1007/s00034-020-01580-x", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput QR decomposition is a key operation in many advanced signal\nprocessing and communication applications. For some of these applications,\nusing floating-point computation is becoming almost compulsory. However, there\nare scarce works in hardware implementations of floating-point QR decomposition\nfor embedded systems. In this paper, we propose a very efficient\nhigh-throughput floating-point Givens rotation unit for QR decomposition.\nMoreover, the initial proposed design for conventional number formats is\nenhanced by using the new Half-Unit Biased format. The provided error analysis\nshows the effectiveness of our proposals and the trade-off of different\nimplementation parameters. FPGA implementation results are also presented and a\nthorough comparison between both approaches. These implementation results also\nreveal outstanding improvements compared to other previous similar designs in\nterms of area, latency, and throughput.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:08:29 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Hormigo", "Javier", ""], ["Mu\u00f1oz", "Sergio D.", ""]]}, {"id": "2010.12861", "submitter": "Jye Luen Lee", "authors": "Syuan-Hao Sie, Jye-Luen Lee, Yi-Ren Chen, Chih-Cheng Lu, Chih-Cheng\n  Hsieh, Meng-Fan Chang, Kea-Tiong Tang", "title": "MARS: Multi-macro Architecture SRAM CIM-Based Accelerator with\n  Co-designed Compressed Neural Networks", "comments": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and\n  Systems 2021", "journal-ref": null, "doi": "10.1109/TCAD.2021.3082107", "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) play a key role in deep learning\napplications. However, the large storage overheads and the substantial\ncomputation cost of CNNs are problematic in hardware accelerators.\nComputing-in-memory (CIM) architecture has demonstrated great potential to\neffectively compute large-scale matrix-vector multiplication. However, the\nintensive multiply and accumulation (MAC) operations executed at the crossbar\narray and the limited capacity of CIM macros remain bottlenecks for further\nimprovement of energy efficiency and throughput. To reduce computation costs,\nnetwork pruning and quantization are two widely studied compression methods to\nshrink the model size. However, most of the model compression algorithms can\nonly be implemented in digital-based CNN accelerators. For implementation in a\nstatic random access memory (SRAM) CIM-based accelerator, the model compression\nalgorithm must consider the hardware limitations of CIM macros, such as the\nnumber of word lines and bit lines that can be turned on at the same time, as\nwell as how to map the weight to the SRAM CIM macro. In this study, a software\nand hardware co-design approach is proposed to design an SRAM CIM-based CNN\naccelerator and an SRAM CIM-aware model compression algorithm. To lessen the\nhigh-precision MAC required by batch normalization (BN), a quantization\nalgorithm that can fuse BN into the weights is proposed. Furthermore, to reduce\nthe number of network parameters, a sparsity algorithm that considers a CIM\narchitecture is proposed. Last, MARS, a CIM-based CNN accelerator that can\nutilize multiple SRAM CIM macros as processing units and support a sparsity\nneural network, is proposed.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 10:31:49 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 05:38:22 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sie", "Syuan-Hao", ""], ["Lee", "Jye-Luen", ""], ["Chen", "Yi-Ren", ""], ["Lu", "Chih-Cheng", ""], ["Hsieh", "Chih-Cheng", ""], ["Chang", "Meng-Fan", ""], ["Tang", "Kea-Tiong", ""]]}, {"id": "2010.12869", "submitter": "Farhad Merchant", "authors": "Suresh Nambi, Salim Ullah, Aditya Lohana, Siva Satyendra Sahoo, Farhad\n  Merchant, Akash Kumar", "title": "ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network\n  Design in FPGA-based Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.ET cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in machine learning, in general, and Artificial Neural\nNetworks (ANN), in particular, has made smart embedded systems an attractive\noption for a larger number of application areas. However, the high\ncomputational complexity, memory footprints, and energy requirements of machine\nlearning models hinder their deployment on resource-constrained embedded\nsystems. Most state-of-the-art works have considered this problem by proposing\nvarious low bit-width data representation schemes, optimized arithmetic\noperators' implementations, and different complexity reduction techniques such\nas network pruning. To further elevate the implementation gains offered by\nthese individual techniques, there is a need to cross-examine and combine these\ntechniques' unique features. This paper presents ExPAN(N)D, a framework to\nanalyze and ingather the efficacy of the Posit number representation scheme and\nthe efficiency of fixed-point arithmetic implementations for ANNs. The Posit\nscheme offers a better dynamic range and higher precision for various\napplications than IEEE $754$ single-precision floating-point format. However,\ndue to the dynamic nature of the various fields of the Posit scheme, the\ncorresponding arithmetic circuits have higher critical path delay and resource\nrequirements than the single-precision-based arithmetic units. Towards this\nend, we propose a novel Posit to fixed-point converter for enabling\nhigh-performance and energy-efficient hardware implementations for ANNs with\nminimal drop in the output accuracy. We also propose a modified Posit-based\nrepresentation to store the trained parameters of a network. Compared to an\n$8$-bit fixed-point-based inference accelerator, our proposed implementation\noffers $\\approx46\\%$ and $\\approx18\\%$ reductions in the storage requirements\nof the parameters and energy consumption of the MAC units, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 11:02:25 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 05:28:28 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Nambi", "Suresh", ""], ["Ullah", "Salim", ""], ["Lohana", "Aditya", ""], ["Sahoo", "Siva Satyendra", ""], ["Merchant", "Farhad", ""], ["Kumar", "Akash", ""]]}, {"id": "2010.13100", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized\n  Recommendation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are one of the most widely deployed machine\nlearning (ML) workload serviced from cloud datacenters. As such, architectural\nsolutions for high-performance recommendation inference have recently been the\ntarget of several prior literatures. Unfortunately, little have been explored\nand understood regarding the training side of this emerging ML workload. In\nthis paper, we first perform a detailed workload characterization study on\ntraining recommendations, root-causing sparse embedding layer training as one\nof the most significant performance bottlenecks. We then propose our\nalgorithm-architecture co-design called Tensor Casting, which enables the\ndevelopment of a generic accelerator architecture for tensor gather-scatter\nthat encompasses all the key primitives of training embedding layers. When\nprototyped on a real CPU-GPU system, Tensor Casting provides 1.9-21x\nimprovements in training throughput compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:04:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13103", "submitter": "Minsoo Rhu", "authors": "Yujeong Choi, Yunseong Kim, Minsoo Rhu", "title": "LazyBatching: An SLA-aware Batching System for Cloud Machine Learning\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud ML inference systems, batching is an essential technique to increase\nthroughput which helps optimize total-cost-of-ownership. Prior graph batching\ncombines the individual DNN graphs into a single one, allowing multiple inputs\nto be concurrently executed in parallel. We observe that the coarse-grained\ngraph batching becomes suboptimal in effectively handling the dynamic inference\nrequest traffic, leaving significant performance left on the table. This paper\nproposes LazyBatching, an SLA-aware batching system that considers both\nscheduling and batching in the granularity of individual graph nodes, rather\nthan the entire graph for flexible batching. We show that LazyBatching can\nintelligently determine the set of nodes that can be efficiently batched\ntogether, achieving an average 15x, 1.5x, and 5.5x improvement than graph\nbatching in terms of average response time, throughput, and SLA satisfaction,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:13:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Choi", "Yujeong", ""], ["Kim", "Yunseong", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13155", "submitter": "Shayan Mohammed", "authors": "Mohammed Shayan, Kanad Basu, Ramesh Karri", "title": "Security Assessment of Interposer-based Chiplet Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With transistor scaling reaching its limits, interposer-based integration of\ndies (chiplets) is gaining traction. Such an interposer-based integration\nenables finer and tighter interconnect pitch than traditional\nsystem-on-packages and offers two key benefits: 1. It reduces design-to-market\ntime by bypassing the time-consuming process of verification and fabrication.\n2. It reduces the design cost by reusing chiplets. While black-boxing of the\nslow design stages cuts down the design time, it raises significant security\nconcerns. We study the security implications of the emerging interposer-based\nintegration methodology. The black-boxed design stages deploy security measures\nagainst hardware Trojans, reverse engineering, and intellectual property piracy\nin traditional systems-on-chip (SoC) designs and hence are not suitable for\ninterposer-based integration. We propose using functionally diverse chiplets to\ndetect and thwart hardware Trojans and use the inherent logic redundancy to\nshore up anti-piracy measures. Our proposals do not rely on access to the\nblack-box design stages. We evaluate the security, time and cost benefits of\nour plan by implementing a MIPS processor, a DCT core, and an AES core using\nvarious IPs from the Xilinx CORE GENERATOR IP catalog, on an interposer-based\nXilinx FPGA.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 16:29:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Shayan", "Mohammed", ""], ["Basu", "Kanad", ""], ["Karri", "Ramesh", ""]]}, {"id": "2010.13216", "submitter": "Ayaz Akram", "authors": "Ayaz Akram, Anna Giannakou, Venkatesh Akella, Jason Lowe-Power, Sean\n  Peisert", "title": "Performance Analysis of Scientific Computing Workloads on Trusted\n  Execution Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing sometimes involves computation on sensitive data.\nDepending on the data and the execution environment, the HPC (high-performance\ncomputing) user or data provider may require confidentiality and/or integrity\nguarantees. To study the applicability of hardware-based trusted execution\nenvironments (TEEs) to enable secure scientific computing, we deeply analyze\nthe performance impact of AMD SEV and Intel SGX for diverse HPC benchmarks\nincluding traditional scientific computing, machine learning, graph analytics,\nand emerging scientific computing workloads. We observe three main findings: 1)\nSEV requires careful memory placement on large scale NUMA machines\n(1$\\times$$-$3.4$\\times$ slowdown without and 1$\\times$$-$1.15$\\times$ slowdown\nwith NUMA aware placement), 2) virtualization$-$a prerequisite for\nSEV$-$results in performance degradation for workloads with irregular memory\naccesses and large working sets (1$\\times$$-$4$\\times$ slowdown compared to\nnative execution for graph applications) and 3) SGX is inappropriate for HPC\ngiven its limited secure memory size and inflexible programming model\n(1.2$\\times$$-$126$\\times$ slowdown over unsecure execution). Finally, we\ndiscuss forthcoming new TEE designs and their potential impact on scientific\ncomputing.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 20:46:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Akram", "Ayaz", ""], ["Giannakou", "Anna", ""], ["Akella", "Venkatesh", ""], ["Lowe-Power", "Jason", ""], ["Peisert", "Sean", ""]]}, {"id": "2010.13311", "submitter": "Pin-Han Chen", "authors": "Chao-Yang Kao, Huang-Chih Kuo, Jian-Wen Chen, Chiung-Liang Lin,\n  Pin-Han Chen and Youn-Long Lin", "title": "RNNAccel: A Fusion Recurrent Neural Network Accelerator for Edge\n  Intelligence", "comments": "This is a paper summited in vlsicad2020 conference in Taiwan. For\n  more information about RNNAccel, see https://neuchips.ai/rnnaccel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many edge devices employ Recurrent Neural Networks (RNN) to enhance their\nproduct intelligence. However, the increasing computation complexity poses\nchallenges for performance, energy efficiency and product development time. In\nthis paper, we present an RNN deep learning accelerator, called RNNAccel, which\nsupports Long Short-Term Memory (LSTM) network, Gated Recurrent Unit (GRU)\nnetwork, and Fully Connected Layer (FC)/ Multiple-Perceptron Layer (MLP)\nnetworks. This RNN accelerator addresses (1) computing unit utilization\nbottleneck caused by RNN data dependency, (2) inflexible design for specific\napplications, (3) energy consumption dominated by memory access, (4) accuracy\nloss due to coefficient compression, and (5) unpredictable performance\nresulting from processor-accelerator integration. Our proposed RNN accelerator\nconsists of a configurable 32-MAC array and a coefficient decompression engine.\nThe MAC array can be scaled-up to meet throughput requirement and power budget.\nIts sophisticated off-line compression and simple hardware-friendly on-line\ndecompression, called NeuCompression, reduces memory footprint up to 16x and\ndecreases memory access power. Furthermore, for easy SOC integration, we\ndeveloped a tool set for bit-accurate simulation and integration result\nvalidation. Evaluated using a keyword spotting application, the 32-MAC RNN\naccelerator achieves 90% MAC utilization, 1.27 TOPs/W at 40nm process, 8x\ncompression ratio, and 90% inference accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 03:36:36 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kao", "Chao-Yang", ""], ["Kuo", "Huang-Chih", ""], ["Chen", "Jian-Wen", ""], ["Lin", "Chiung-Liang", ""], ["Chen", "Pin-Han", ""], ["Lin", "Youn-Long", ""]]}, {"id": "2010.13619", "submitter": "Jonas Dann", "authors": "Jonas Dann and Daniel Ritter and Holger Fr\\\"oning", "title": "Exploring Memory Access Patterns for Graph Processing Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in business and technology (e.g., machine learning, social\nnetwork analysis) benefit from storing and processing growing amounts of\ngraph-structured data in databases and data science platforms. FPGAs as\naccelerators for graph processing with a customizable memory hierarchy promise\nsolving performance problems caused by inherent irregular memory access\npatterns on traditional hardware (e.g., CPU). However, developing such hardware\naccelerators is yet time-consuming and difficult and benchmarking is\nnon-standardized, hindering comprehension of the impact of memory access\npattern changes and systematic engineering of graph processing accelerators.\n  In this work, we propose a simulation environment for the analysis of graph\nprocessing accelerators based on simulating their memory access patterns.\nFurther, we evaluate our approach on two state-of-the-art FPGA graph processing\naccelerators and show reproducibility, comparablity, as well as the shortened\ndevelopment process by an example. Not implementing the cycle-accurate internal\ndata flow on accelerator hardware like FPGAs significantly reduces the\nimplementation time, increases the benchmark parameter transparency, and allows\ncomparison of graph processing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:34:39 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 11:43:23 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 20:17:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2010.14145", "submitter": "Roberto Bifulco", "authors": "Marco Spaziani Brunella, Giacomo Belocchi, Marco Bonola, Salvatore\n  Pontarelli, Giuseppe Siracusano, Giuseppe Bianchi, Aniello Cammarano,\n  Alessandro Palumbo, Luca Petrucci, Roberto Bifulco", "title": "hXDP: Efficient Software Packet Processing on FPGA NICs", "comments": "Accepted at USENIX OSDI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA accelerators on the NIC enable the offloading of expensive packet\nprocessing tasks from the CPU. However, FPGAs have limited resources that may\nneed to be shared among diverse applications, and programming them is\ndifficult.\n  We present a solution to run Linux's eXpress Data Path programs written in\neBPF on FPGAs, using only a fraction of the available hardware resources while\nmatching the performance of high-end CPUs. The iterative execution model of\neBPF is not a good fit for FPGA accelerators. Nonetheless, we show that many of\nthe instructions of an eBPF program can be compressed, parallelized or\ncompletely removed, when targeting a purpose-built FPGA executor, thereby\nsignificantly improving performance. We leverage that to design hXDP, which\nincludes (i) an optimizing-compiler that parallelizes and translates eBPF\nbytecode to an extended eBPF Instruction-set Architecture defined by us; a (ii)\nsoft-CPU to execute such instructions on FPGA; and (iii) an FPGA-based\ninfrastructure to provide XDP's maps and helper functions as defined within the\nLinux kernel.\n  We implement hXDP on an FPGA NIC and evaluate it running real-world\nunmodified eBPF programs. Our implementation is clocked at 156.25MHz, uses\nabout 15% of the FPGA resources, and can run dynamically loaded programs.\nDespite these modest requirements, it achieves the packet processing throughput\nof a high-end CPU core and provides a 10x lower packet forwarding latency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 09:09:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brunella", "Marco Spaziani", ""], ["Belocchi", "Giacomo", ""], ["Bonola", "Marco", ""], ["Pontarelli", "Salvatore", ""], ["Siracusano", "Giuseppe", ""], ["Bianchi", "Giuseppe", ""], ["Cammarano", "Aniello", ""], ["Palumbo", "Alessandro", ""], ["Petrucci", "Luca", ""], ["Bifulco", "Roberto", ""]]}, {"id": "2010.14246", "submitter": "Edgar Liberis", "authors": "Edgar Liberis, {\\L}ukasz Dudziak, Nicholas D. Lane", "title": "$\\mu$NAS: Constrained Neural Architecture Search for Microcontrollers", "comments": "$\\mu$NAS is available at https://github.com/eliberis/uNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT devices are powered by microcontroller units (MCUs) which are extremely\nresource-scarce: a typical MCU may have an underpowered processor and around 64\nKB of memory and persistent storage, which is orders of magnitude fewer\ncomputational resources than is typically required for deep learning. Designing\nneural networks for such a platform requires an intricate balance between\nkeeping high predictive performance (accuracy) while achieving low memory and\nstorage usage and inference latency. This is extremely challenging to achieve\nmanually, so in this work, we build a neural architecture search (NAS) system,\ncalled $\\mu$NAS, to automate the design of such small-yet-powerful MCU-level\nnetworks. $\\mu$NAS explicitly targets the three primary aspects of resource\nscarcity of MCUs: the size of RAM, persistent storage and processor speed.\n$\\mu$NAS represents a significant advance in resource-efficient models,\nespecially for \"mid-tier\" MCUs with memory requirements ranging from 0.5 KB to\n64 KB. We show that on a variety of image classification datasets $\\mu$NAS is\nable to (a) improve top-1 classification accuracy by up to 4.8%, or (b) reduce\nmemory footprint by 4--13x, or (c) reduce the number of multiply-accumulate\noperations by at least 2x, compared to existing MCU specialist literature and\nresource-efficient models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 12:42:53 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 10:02:32 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 17:02:50 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Liberis", "Edgar", ""], ["Dudziak", "\u0141ukasz", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2010.14684", "submitter": "Maciej Besta", "authors": "Maciej Besta, Marc Fischer, Tal Ben-Nun, Dimitri Stanojevic, Johannes\n  De Fine Licht, Torsten Hoefler", "title": "Substream-Centric Maximum Matchings on FPGA", "comments": "Best Paper finalist at ACM FPGA'19, invited to special issue of ACM\n  TRETS'20", "journal-ref": "Proceedings of the ACM Transactions on Reconfigurable Technology\n  and Systems (TRETS), 2020. Proceedings of the 27th ACM/SIGDA International\n  Symposium on Field-Programmable Gate Arrays (FPGA), 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performance and energy-efficient algorithms for maximum\nmatchings is becoming increasingly important in social network analysis,\ncomputational sciences, scheduling, and others. In this work, we propose the\nfirst maximum matching algorithm designed for FPGAs; it is energy-efficient and\nhas provable guarantees on accuracy, performance, and storage utilization. To\nachieve this, we forego popular graph processing paradigms, such as\nvertex-centric programming, that often entail large communication costs.\nInstead, we propose a substream-centric approach, in which the input stream of\ndata is divided into substreams processed independently to enable more\nparallelism while lowering communication costs. We base our work on the theory\nof streaming graph algorithms and analyze 14 models and 28 algorithms. We use\nthis analysis to provide theoretical underpinning that matches the physical\nconstraints of FPGA platforms. Our algorithm delivers high performance (more\nthan 4x speedup over tuned parallel CPU variants), low memory, high accuracy,\nand effective usage of FPGA resources. The substream-centric approach could\neasily be extended to other algorithms to offer low-power and high-performance\ngraph processing on FPGAs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 00:31:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Besta", "Maciej", ""], ["Fischer", "Marc", ""], ["Ben-Nun", "Tal", ""], ["Stanojevic", "Dimitri", ""], ["Licht", "Johannes De Fine", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.14934", "submitter": "Hugo Hadjur", "authors": "Hugo Hadjur (AVALON), Doreid Ammar, Laurent Lef\\`evre (AVALON)", "title": "Analysis of Energy Consumption in a Precision Beekeeping System", "comments": "IoT '20: 10th International Conference on the Internet of Things, Oct\n  2020, Malm{\\\"o}, Sweden", "journal-ref": null, "doi": "10.1145/3410992.3411010", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey bees have been domesticated by humans for several thousand years and\nmainly provide honey and pollination, which is fundamental for plant\nreproduction. Nowadays, the work of beekeepers is constrained by external\nfactors that stress their production (parasites and pesticides among others).\nTaking care of large numbers of beehives is time-consuming, so integrating\nsensors to track their status can drastically simplify the work of beekeepers.\nPrecision bee-keeping complements beekeepers' work thanks to the In-ternet of\nThings (IoT) technology. If used correctly, data can help to make the right\ndiagnosis for honey bees colony, increase honey production and decrease bee\nmortality. Providing enough energy for on-hive and in-hive sensors is a\nchallenge. Some solutions rely on energy harvesting, others target usage of\nlarge batteries. Either way, it is mandatory to analyze the energy usage of\nembedded equipment in order to design an energy efficient and autonomous bee\nmonitoring system. This paper relies on a fully autonomous IoT framework that\ncollects environmental and image data of a beehive. It consists of a data\ncollecting node (environmental data sensors, camera, Raspberry Pi and Arduino)\nand a solar energy supplying node. Supported services are analyzed task by task\nfrom an energy profiling and efficiency standpoint , in order to identify the\nhighly pressured areas of the framework. This first step will guide our goal of\ndesigning a sustainable precision beekeeping system, both technically and\nenergy-wise.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 12:44:09 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Hadjur", "Hugo", "", "AVALON"], ["Ammar", "Doreid", "", "AVALON"], ["Lef\u00e8vre", "Laurent", "", "AVALON"]]}, {"id": "2010.15964", "submitter": "Shahriar Shahabuddin", "authors": "Shahriar Shahabuddin, Mahmoud A. Albreem, Mohammad Shahanewaz\n  Shahabuddin, Zaheer Khan, Markku Juntti", "title": "FPGA Implementation of Stair Matrix based Massive MIMO Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate matrix inversion based methods is widely used for linear massive\nmultiple-input multiple-output (MIMO) received symbol vector detection. Such\ndetectors typically utilize the diagonally dominant channel matrix of a massive\nMIMO system. Instead of diagonal matrix, a stair matrix can be utilized to\nimprove the error-rate performance of a massive MIMO detector. In this paper,\nwe present very large-scale integration (VLSI) architecture and field\nprogrammable gate array (FPGA) implementation of a stair matrix based iterative\ndetection algorithm. The architecture supports a base station with 128\nantennas, 8 users with single antenna, and 256 quadrature amplitude modulation\n(QAM). The stair matrix based detector can deliver a 142.34 Mbps data rate and\nreach a clock frequency of 258 MHz in a Xilinx Virtex-7 FPGA. The detector\nprovides superior error-rate performance and higher scaled throughput than most\ncontemporary massive MIMO detectors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:00:00 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Shahabuddin", "Shahriar", ""], ["Albreem", "Mahmoud A.", ""], ["Shahabuddin", "Mohammad Shahanewaz", ""], ["Khan", "Zaheer", ""], ["Juntti", "Markku", ""]]}, {"id": "2010.16171", "submitter": "Md Ashraful Islam", "authors": "Md Ashraful Islam and Hiromu Miyazaki and Kenji Kise", "title": "RVCoreP-32IM: An effective architecture to implement mul/div\n  instructions for five stage RISC-V soft processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RISC-V, an open instruction set architecture, is getting the attention of\nsoft processor developers. Implementing only a basic 32-bit integer instruction\nset of RISC-V, which is defined as RV32I, might be satisfactory for embedded\nsystems. However, multiplication and division instructions are not present in\nRV32I, rather than defined as M-extension. Several research projects have\nproposed both RV32I and RV32IM processor. However, there is no indication of\nhow much performance can be improved by adding M-extension to RV32I. In other\nwords, when we should consider adding M-extension into the soft processor and\nhow much hardware resource requirements will increase.\n  In this paper, we propose an extension of the RVCoreP soft processor (which\nimplements RV32I instruction set only) to support RISC-V M-extension\ninstructions. A simple fork-join method is used to expand the execution\ncapability to support M-extension instructions as well as a possible future\nenhancement. We then perform the benchmark using Dhrystone, Coremark, and\nEmbench programs. We found that RV32IM is 1.87 and 3.13 times better in\nperformance for radix-4 and DSP multiplier, respectively. In addition to that,\nour RV32IM implementation is 13\\% better than the equivalent RISC-V processor.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 10:29:12 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Islam", "Md Ashraful", ""], ["Miyazaki", "Hiromu", ""], ["Kise", "Kenji", ""]]}]