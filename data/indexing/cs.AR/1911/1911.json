[{"id": "1911.00267", "submitter": "Johannes Bund", "authors": "Johannes Bund and Christoph Lenzen and Moti Medina", "title": "Optimal Metastability-Containing Sorting via Parallel Prefix Computation", "comments": "This article generalizes and extends work presented at DATE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Friedrichs et al. (TC 2018) showed that metastability can be contained when\nsorting inputs arising from time-to-digital converters, i.e., measurement\nvalues can be correctly sorted without resolving metastability using\nsynchronizers first. However, this work left open whether this can be done by\nsmall circuits. We show that this is indeed possible, by providing a circuit\nthat sorts Gray code inputs (possibly containing a metastable bit) and has\nasymptotically optimal depth and size. Our solution utilizes the parallel\nprefix computation (PPC) framework (JACM 1980). We improve this construction by\nbounding its fan-out by an arbitrary $f \\geq 3$, without affecting depth and\nincreasing circuit size by a small constant factor only. Thus, we obtain the\nfirst PPC circuits with asymptotically optimal size, constant fan-out, and\noptimal depth. To show that applying the PPC framework to the sorting task is\nfeasible, we prove that the latter can, despite potential metastability, be\ndecomposed such that the core operation is associative. We obtain\nasymptotically optimal metastability-containing sorting networks. We complement\nthese results with simulations, independently verifying the correctness as well\nas small size and delay of our circuits.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 09:01:18 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Bund", "Johannes", ""], ["Lenzen", "Christoph", ""], ["Medina", "Moti", ""]]}, {"id": "1911.01258", "submitter": "Reza Yazdani Aminabadi", "authors": "Reza Yazdani, Olatunji Ruwase, Minjia Zhang, Yuxiong He, Jose-Maria\n  Arnau, Antonio Gonzalez", "title": "LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long\n  Short-Term Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of LSTM neural networks for popular tasks such as Automatic\nSpeech Recognition has fostered an increasing interest in LSTM inference\nacceleration. Due to the recurrent nature and data dependencies of LSTM\ncomputations, designing a customized architecture specifically tailored to its\ncomputation pattern is crucial for efficiency. Since LSTMs are used for a\nvariety of tasks, generalizing this efficiency to diverse configurations, i.e.,\nadaptiveness, is another key feature of these accelerators. In this work, we\nfirst show the problem of low resource-utilization and adaptiveness for the\nstate-of-the-art LSTM implementations on GPU, FPGA and ASIC architectures. To\nsolve these issues, we propose an intelligent tiled-based dispatching mechanism\nthat efficiently handles the data dependencies and increases the adaptiveness\nof LSTM computation. To do so, we propose LSTM-Sharp as a hardware accelerator,\nwhich pipelines LSTM computation using an effective scheduling scheme to hide\nmost of the dependent serialization. Furthermore, LSTM-Sharp employs dynamic\nreconfigurable architecture to adapt to the model's characteristics. LSTM-Sharp\nachieves 1.5x, 2.86x, and 82x speedups on average over the state-of-the-art\nASIC, FPGA, and GPU implementations respectively, for different LSTM models and\nresource budgets. Furthermore, we provide significant energy-reduction with\nrespect to the previous solutions, due to the low power dissipation of\nLSTM-Sharp (383 GFLOPs/Watt).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:51:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yazdani", "Reza", ""], ["Ruwase", "Olatunji", ""], ["Zhang", "Minjia", ""], ["He", "Yuxiong", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "1911.02038", "submitter": "Miguel A. Arroyo", "authors": "Mohamed Tarek Ibn Ziad, Miguel A. Arroyo, Evgeny Manzhosov, Vasileios\n  P. Kemerlis, Simha Sethumadhavan", "title": "Using Name Confusion to Enhance Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel concept, called Name Confusion, and demonstrate how it\ncan be employed to thwart multiple classes of code-reuse attacks. By building\nupon Name Confusion, we derive Phantom Name System (PNS): a security protocol\nthat provides multiple names (addresses) to program instructions. Unlike the\nconventional model of virtual memory with a one-to-one mapping between\ninstructions and virtual memory addresses, PNS creates N mappings for the same\ninstruction, and randomly switches between them at runtime. PNS achieves fast\nrandomization, at the granularity of basic blocks, which mitigates a class of\nattacks known as (just-in-time) code-reuse.\n  If an attacker uses a memory safety-related vulnerability to cause any of the\ninstruction addresses to be different from the one chosen during a fetch, the\nexploited program will crash. We quantitatively evaluate how PNS mitigates\nreal-world code-reuse attacks by reducing the success probability of typical\nexploits to approximately $10^{-12}$. We implement PNS and validate it by\nrunning SPEC CPU2017 benchmark suite. We further verify its practicality by\nadding it to a RISC-V core on an FPGA. Lastly, PNS is mainly designed for\nresource constrained (wimpy) devices and has negligible performance overhead,\ncompared to commercially-available, state-of-the-art, hardware-based\nprotections.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:02:53 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 05:39:44 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 18:12:53 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ziad", "Mohamed Tarek Ibn", ""], ["Arroyo", "Miguel A.", ""], ["Manzhosov", "Evgeny", ""], ["Kemerlis", "Vasileios P.", ""], ["Sethumadhavan", "Simha", ""]]}, {"id": "1911.03364", "submitter": "Hui Zhao", "authors": "Xianwei Cheng, Hui Zhao, Mahmut Kandemir, Beilei Jiang, Gayatri Mehta", "title": "AMOEBA: A Coarse Grained Reconfigurable Architecture for Dynamic GPU\n  Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different GPU applications exhibit varying scalability patterns with\nnetwork-on-chip (NoC), coalescing, memory and control divergence, and L1 cache\nbehavior. A GPU consists of several StreamingMulti-processors (SMs) that\ncollectively determine how shared resources are partitioned and accessed.\nRecent years have seen divergent paths in SM scaling towards scale-up (fewer,\nlarger SMs) vs. scale-out (more, smaller SMs). However, neither scaling up nor\nscaling out can meet the scalability requirement of all applications running on\na given GPU system, which inevitably results in performance degradation and\nresource under-utilization for some applications. In this work, we investigate\nmajor design parameters that influence GPU scaling. We then propose AMOEBA, a\nsolution to GPU scaling through reconfigurable SM cores. AMOEBA monitors and\npredicts application scalability at run-time and adjusts the SM configuration\nto meet program requirements. AMOEBA also enables dynamic creation of\nheterogeneous SMs through independent fusing or splitting. AMOEBA is a\nmicroarchitecture-based solution and requires no additional programming effort\nor custom compiler support. Our experimental evaluations with application\nprograms from various benchmark suites indicate that AMOEBA is able to achieve\na maximum performance gain of 4.3x, and generates an average performance\nimprovement of 47% when considering all benchmarks tested.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:43:54 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Cheng", "Xianwei", ""], ["Zhao", "Hui", ""], ["Kandemir", "Mahmut", ""], ["Jiang", "Beilei", ""], ["Mehta", "Gayatri", ""]]}, {"id": "1911.03451", "submitter": "Xingyao Zhang", "authors": "Xingyao Zhang, Shuaiwen Leon Song, Chenhao Xie, Jing Wang, Weigong\n  Zhang and Xin Fu", "title": "Enabling Highly Efficient Capsule Networks Processing Through A\n  PIM-Based Architecture Design", "comments": "To appear in the 2020 26th International Symposium on\n  High-Performance Computer Architecture (HPCA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the CNNs have achieved great successes in the image\nprocessing tasks, e.g., image recognition and object detection. Unfortunately,\ntraditional CNN's classification is found to be easily misled by increasingly\ncomplex image features due to the usage of pooling operations, hence unable to\npreserve accurate position and pose information of the objects. To address this\nchallenge, a novel neural network structure called Capsule Network has been\nproposed, which introduces equivariance through capsules to significantly\nenhance the learning ability for image segmentation and object detection. Due\nto its requirement of performing a high volume of matrix operations, CapsNets\nhave been generally accelerated on modern GPU platforms that provide highly\noptimized software library for common deep learning tasks. However, based on\nour performance characterization on modern GPUs, CapsNets exhibit low\nefficiency due to the special program and execution features of their routing\nprocedure, including massive unshareable intermediate variables and intensive\nsynchronizations, which are very difficult to optimize at software level. To\naddress these challenges, we propose a hybrid computing architecture design\nnamed \\textit{PIM-CapsNet}. It preserves GPU's on-chip computing capability for\naccelerating CNN types of layers in CapsNet, while pipelining with an off-chip\nin-memory acceleration solution that effectively tackles routing procedure's\ninefficiency by leveraging the processing-in-memory capability of today's 3D\nstacked memory. Using routing procedure's inherent parallellization feature,\nour design enables hierarchical improvements on CapsNet inference efficiency\nthrough minimizing data movement and maximizing parallel processing in memory.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 06:03:46 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhang", "Xingyao", ""], ["Song", "Shuaiwen Leon", ""], ["Xie", "Chenhao", ""], ["Wang", "Jing", ""], ["Zhang", "Weigong", ""], ["Fu", "Xin", ""]]}, {"id": "1911.03458", "submitter": "Yu-Sheng Lin", "authors": "Yu-Sheng Lin, Wei-Chao Chen, Shao-Yi Chien", "title": "MERIT: Tensor Transform for Memory-Efficient Vision Processing on\n  Parallel Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally intensive deep neural networks (DNNs) are well-suited to run\non GPUs, but newly developed algorithms usually require the heavily optimized\nDNN routines to work efficiently, and this problem could be even more difficult\nfor specialized DNN architectures. In this paper, we propose a mathematical\nformulation which can be useful for transferring the algorithm optimization\nknowledge across computing platforms. We discover that data movement and\nstorage inside parallel processor architectures can be viewed as tensor\ntransforms across memory hierarchies, making it possible to describe many\nmemory optimization techniques mathematically. Such transform, which we call\nMemory Efficient Ranged Inner-Product Tensor (MERIT) transform, can be applied\nto not only DNN tasks but also many traditional machine learning and computer\nvision computations. Moreover, the tensor transforms can be readily mapped to\nexisting vector processor architectures. In this paper, we demonstrate that\nmany popular applications can be converted to a succinct MERIT notation on\nGPUs, speeding up GPU kernels up to 20 times while using only half as many code\ntokens. We also use the principle of the proposed transform to design a\nspecialized hardware unit called MERIT-z processor. This processor can be\napplied to a variety of DNN tasks as well as other computer vision tasks while\nproviding comparable area and power efficiency to dedicated DNN ASICs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 15:03:43 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Lin", "Yu-Sheng", ""], ["Chen", "Wei-Chao", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "1911.04378", "submitter": "Robert J Walls", "authors": "Jacob T. Grycel, Robert J. Walls", "title": "DRAB-LOCUS: An Area-Efficient AES Architecture for Hardware Accelerator\n  Co-Location on FPGAs", "comments": "16 pages, initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced Encryption Standard (AES) implementations on Field Programmable Gate\nArrays (FPGA) commonly focus on maximizing throughput at the cost of utilizing\nhigh volumes of FPGA slice logic. High resource usage limits systems' abilities\nto implement other functions (such as video processing or machine learning)\nthat may want to share the same FPGA resources. In this paper, we address the\nshared resource challenge by proposing and evaluating a low-area, but\nhigh-throughput, AES architecture. In contrast to existing work, our\nDSP/RAM-Based Low-CLB Usage (DRAB-LOCUS) architecture leverages block RAM tiles\nand Digital Signal Processing (DSP) slices to implement the AES Sub Bytes, Mix\nColumns, and Add Round Key sub-round transformations, reducing resource usage\nby a factor of 3 over traditional approaches. To achieve area-efficiency, we\nbuilt an inner-pipelined architecture using the internal registers of block RAM\ntiles and DSP slices. Our DRAB-LOCUS architecture features a 12-stage pipeline\ncapable of producing 7.055 Gbps of interleaved encrypted or decrypted data, and\nonly uses 909 Look Up tables, 593 Flip Flops, 16 block RAMs, and 18 DSP slices\nin the target device.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 16:41:29 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Grycel", "Jacob T.", ""], ["Walls", "Robert J.", ""]]}, {"id": "1911.05101", "submitter": "Mehrzad Nejat", "authors": "Mehrzad Nejat, Madhavan Manivannan, Miquel Pericas, Per Stenstrom", "title": "Coordinated Management of DVFS and Cache Partitioning under QoS\n  Constraints to Save Energy in Multi-Core Systems", "comments": "Submitted to the Journal of Parallel and Distributed Computing (Nov\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the energy expended to carry out a computational task is important.\nIn this work, we explore the prospects of meeting Quality-of-Service\nrequirements of tasks on a multi-core system while adjusting resources to\nexpend a minimum of energy. This paper considers, for the first time, a\nQoS-driven coordinated resource management algorithm (RMA) that dynamically\nadjusts the size of the per-core last-level cache partitions and the per-core\nvoltage-frequency settings to save energy while respecting QoS requirements of\nevery application in multi-programmed workloads run on multi-core systems. It\ndoes so by doing configuration-space exploration across the spectrum of LLC\npartition sizes and Dynamic Voltage Frequency Scaling (DVFS) settings at\nruntime at negligible overhead. We show that the energy of 4-core and 8-core\nsystems can be reduced by up to 18% and 14%, respectively, compared to a\nbaseline with even distribution of cache resources and a fixed mid-range core\nvoltage-frequency setting. The energy savings can potentially reach 29% if the\nQoS targets are relaxed to 40% longer execution time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:10:18 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Nejat", "Mehrzad", ""], ["Manivannan", "Madhavan", ""], ["Pericas", "Miquel", ""], ["Stenstrom", "Per", ""]]}, {"id": "1911.05114", "submitter": "Mehrzad Nejat", "authors": "Mehrzad Nejat, Madhavan Manivannan, Miquel Pericas, Per Stenstrom", "title": "Coordinated Management of Processor Configuration and Cache Partitioning\n  to Optimize Energy under QoS Constraints", "comments": "Submitted to the 34th IEEE International Parallel & Distributed\n  Processing Symposium (IPDPS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective way to improve energy efficiency is to throttle hardware\nresources to meet a certain performance target, specified as a QoS constraint,\nassociated with all applications running on a multicore system.\n  Prior art has proposed resource management (RM) frameworks in which the share\nof the last-level cache (LLC) assigned to each processor and the\nvoltage-frequency (VF) setting for each processor is managed in a coordinated\nfashion to reduce energy. A drawback of such a scheme is that, while one core\ngives up LLC resources for another core, the performance drop must be\ncompensated by a higher VF setting which leads to a quadratic increase in\nenergy consumption. By allowing each core to be adapted to exploit instruction\nand memory-level parallelism (ILP/MLP), substantially higher energy savings are\nenabled.\n  This paper proposes a coordinated RM for LLC partitioning, processor\nadaptation, and per-core VF scaling. A first contribution is a systematic study\nof the resource trade-offs enabled when trading between the three classes of\nresources in a coordinated fashion. A second contribution is a new RM framework\nthat utilizes these trade-offs to save more energy. Finally, a challenge to\naccurately model the impact of resource throttling on performance is to predict\nthe amount of MLP with high accuracy. To this end, the paper contributes with a\nmechanism that estimates the effect of MLP over different processor\nconfigurations and LLC allocations. Overall, we show that up to 18% of energy,\nand on average 10%, can be saved using the proposed scheme.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:38:35 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Nejat", "Mehrzad", ""], ["Manivannan", "Madhavan", ""], ["Pericas", "Miquel", ""], ["Stenstrom", "Per", ""]]}, {"id": "1911.05289", "submitter": "Jeffrey Dean", "authors": "Jeffrey Dean", "title": "The Deep Learning Revolution and Its Implications for Computer\n  Architecture and Chip Design", "comments": "Companion paper to accompany a keynote talk at ISSCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen a remarkable series of advances in machine learning,\nand in particular deep learning approaches based on artificial neural networks,\nto improve our abilities to build more accurate systems across a broad range of\nareas, including computer vision, speech recognition, language translation, and\nnatural language understanding tasks. This paper is a companion paper to a\nkeynote talk at the 2020 International Solid-State Circuits Conference (ISSCC)\ndiscussing some of the advances in machine learning, and their implications on\nthe kinds of computational devices we need to build, especially in the\npost-Moore's Law-era. It also discusses some of the ways that machine learning\nmay also be able to help with some aspects of the circuit design process.\nFinally, it provides a sketch of at least one interesting direction towards\nmuch larger-scale multi-task models that are sparsely activated and employ much\nmore dynamic, example- and task-based routing than the machine learning models\nof today.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 04:41:31 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Dean", "Jeffrey", ""]]}, {"id": "1911.05662", "submitter": "Xiaoming Chen", "authors": "Xiaoming Chen, Yinhe Han, Yu Wang", "title": "Communication Lower Bound in Convolution Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current convolutional neural network (CNN) accelerators, communication\n(i.e., memory access) dominates the energy consumption. This work provides\ncomprehensive analysis and methodologies to minimize the communication for CNN\naccelerators. For the off-chip communication, we derive the theoretical lower\nbound for any convolutional layer and propose a dataflow to reach the lower\nbound. This fundamental problem has never been solved by prior studies. The\non-chip communication is minimized based on an elaborate workload and storage\nmapping scheme. We in addition design a communication-optimal CNN accelerator\narchitecture. Evaluations based on the 65nm technology demonstrate that the\nproposed architecture nearly reaches the theoretical minimum communication in a\nthree-level memory hierarchy and it is computation dominant. The gap between\nthe energy efficiency of our accelerator and the theoretical best value is only\n37-87%.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 04:54:17 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:08:40 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 03:04:10 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Chen", "Xiaoming", ""], ["Han", "Yinhe", ""], ["Wang", "Yu", ""]]}, {"id": "1911.05664", "submitter": "Ali Jahanshahi", "authors": "Ali Jahanshahi", "title": "A Brief Review on Some Architectures Providing Support for DIFT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dynamic Information Flow Tracking (DIFT) is a technique to track potential\nsecurity vulnerabilities in software and hardware systems at run time. The last\nfifteen years have seen a lot of research work on DIFT, including both\nhardware-based and software-based implementations for different types of\nprocessor architectures. This survey briefly reviews some hardware\narchitectures that provide DIFT support. Starting from introducing different\napproaches for hardware based DIFT, this survey focuses on integrated/in-core\narchitectures. Protection schemes, including tagging system, tag propagation,\nand tag checking for each architecture will be discussed. The survey is\norganized in such a way that it illustrates the evolution of integrated DIFT\narchitectures, each architecture tries to improve the precious proposed\narchitectures generality/versatility weaknesses. However, improving security\nwhile providing generality and versatility is kind of trade-offs. This survey\ncompares the architectures from different aspects to show the trade-offs\nclearer.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 19:54:25 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Jahanshahi", "Ali", ""]]}, {"id": "1911.06859", "submitter": "Minsoo Rhu", "authors": "Bongjoon Hyun, Youngeun Kwon, Yujeong Choi, John Kim, Minsoo Rhu", "title": "NeuMMU: Architectural Support for Efficient Address Translations in\n  Neural Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To satisfy the compute and memory demands of deep neural networks, neural\nprocessing units (NPUs) are widely being utilized for accelerating deep\nlearning algorithms. Similar to how GPUs have evolved from a slave device into\na mainstream processor architecture, it is likely that NPUs will become first\nclass citizens in this fast-evolving heterogeneous architecture space. This\npaper makes a case for enabling address translation in NPUs to decouple the\nvirtual and physical memory address space. Through a careful data-driven\napplication characterization study, we root-cause several limitations of prior\nGPU-centric address translation schemes and propose a memory management unit\n(MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our\nproposal incurs only an average 0.06% performance overhead.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 20:10:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Hyun", "Bongjoon", ""], ["Kwon", "Youngeun", ""], ["Choi", "Yujeong", ""], ["Kim", "John", ""], ["Rhu", "Minsoo", ""]]}, {"id": "1911.07187", "submitter": "Behnam Khaleghi", "authors": "Behnam Khaleghi, Sahand Salamat, Mohsen Imani, Tajana Rosing", "title": "FPGA Energy Efficiency by Leveraging Thermal Margin", "comments": "Accepted in IEEE International Conference on Computer Design (ICCD)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting edge FPGAs are not energy efficient as conventionally presumed to be,\nand therefore, aggressive power-saving techniques have become imperative. The\nclock rate of an FPGA-mapped design is set based on worst-case conditions to\nensure reliable operation under all circumstances. This usually leaves a\nconsiderable timing margin that can be exploited to reduce power consumption by\nscaling voltage without lowering clock frequency. There are hurdles for such\nopportunistic voltage scaling in FPGAs because (a) critical paths change with\ndesigns, making timing evaluation difficult as voltage changes, (b) each FPGA\nresource has particular power-delay trade-off with voltage, (c) data corruption\nof configuration cells and memory blocks further hampers voltage scaling. In\nthis paper, we propose a systematical approach to leverage the available\nthermal headroom of FPGA-mapped designs for power and energy improvement. By\ncomprehensively analyzing the timing and power consumption of FPGA building\nblocks under varying temperatures and voltages, we propose a thermal-aware\nvoltage scaling flow that effectively utilizes the thermal margin to reduce\npower consumption without degrading performance. We show the proposed flow can\nbe employed for energy optimization as well, whereby power consumption and\ndelay are compromised to accomplish the tasks with minimum energy. Lastly, we\npropose a simulation framework to be able to examine the efficiency of the\nproposed method for other applications that are inherently tolerant to a\ncertain amount of error, granting further power saving opportunity.\nExperimental results over a set of industrial benchmarks indicate up to 36%\npower reduction with the same performance, and 66% total energy saving when\nenergy is the optimization target.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:01:11 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Khaleghi", "Behnam", ""], ["Salamat", "Sahand", ""], ["Imani", "Mohsen", ""], ["Rosing", "Tajana", ""]]}, {"id": "1911.08097", "submitter": "Julian Faraone", "authors": "Julian Faraone, Martin Kumm, Martin Hardieck, Peter Zipf, Xueyuan Liu,\n  David Boland, Philip H.W. Leong", "title": "AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TVLSI.2019.2939429", "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision arithmetic operations to accelerate deep-learning applications\non field-programmable gate arrays (FPGAs) have been studied extensively,\nbecause they offer the potential to save silicon area or increase throughput.\nHowever, these benefits come at the cost of a decrease in accuracy. In this\narticle, we demonstrate that reconfigurable constant coefficient multipliers\n(RCCMs) offer a better alternative for saving the silicon area than utilizing\nlow-precision arithmetic. RCCMs multiply input values by a restricted choice of\ncoefficients using only adders, subtractors, bit shifts, and multiplexers\n(MUXes), meaning that they can be heavily optimized for FPGAs. We propose a\nfamily of RCCMs tailored to FPGA logic elements to ensure their efficient\nutilization. To minimize information loss from quantization, we then develop\nnovel training techniques that map the possible coefficient representations of\nthe RCCMs to neural network weight parameter distributions. This enables the\nusage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate\nthe benefits of these techniques using AlexNet, ResNet-18, and ResNet-50\nnetworks. The resulting implementations achieve up to 50% resource savings over\ntraditional 8-bit quantized networks, translating to significant speedups and\npower savings. Our RCCM with the lowest resource requirements exceeds 6-bit\nfixed point accuracy, while all other implementations with RCCMs achieve at\nleast similar accuracy to an 8-bit uniformly quantized design, while achieving\nsignificant resource savings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 05:00:41 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Faraone", "Julian", ""], ["Kumm", "Martin", ""], ["Hardieck", "Martin", ""], ["Zipf", "Peter", ""], ["Liu", "Xueyuan", ""], ["Boland", "David", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "1911.08356", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Florian Zaruba, Torsten Hoefler, Luca Benini", "title": "Stream Semantic Registers: A Lightweight RISC-V ISA Extension Achieving\n  Full Compute Utilization in Single-Issue Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-issue processor cores are very energy efficient but suffer from the\nvon Neumann bottleneck, in that they must explicitly fetch and issue the\nloads/storse necessary to feed their ALU/FPU. Each instruction spent on moving\ndata is a cycle not spent on computation, limiting ALU/FPU utilization to 33%\non reductions. We propose \"Stream Semantic Registers\" to boost utilization and\nincrease energy efficiency. SSR is a lightweight, non-invasive RISC-V ISA\nextension which implicitly encodes memory accesses as register reads/writes,\neliminating a large number of loads/stores. We implement the proposed extension\nin the RTL of an existing multi-core cluster and synthesize the design for a\nmodern 22nm technology. Our extension provides a significant, 2x to 5x,\narchitectural speedup across different kernels at a small 11% increase in core\narea. Sequential code runs 3x faster on a single core, and 3x fewer cores are\nneeded in a cluster to achieve the same performance. The utilization increase\nto almost 100% in leads to a 2x energy efficiency improvement in a multi-core\ncluster. The extension reduces instruction fetches by up to 3.5x and\ninstruction cache power consumption by up to 5.6x. Compilers can automatically\nmap loop nests to SSRs, making the changes transparent to the programmer.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:37:55 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 13:19:00 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 05:11:19 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Hoefler", "Torsten", ""], ["Benini", "Luca", ""]]}, {"id": "1911.08384", "submitter": "Sam Ainsworth", "authors": "Sam Ainsworth and Timothy M. Jones", "title": "MuonTrap: Preventing Cross-Domain Spectre-Like Attacks by Capturing\n  Speculative State", "comments": null, "journal-ref": null, "doi": "10.1109/ISCA45697.2020.00022", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disclosure of the Spectre speculative-execution attacks in January 2018\nhas left a severe vulnerability that systems are still struggling with how to\npatch. The solutions that currently exist tend to have incomplete coverage,\nperform badly, or have highly undesirable edge cases that cause application\ndomains to break.\n  MuonTrap allows processors to continue to speculate, avoiding significant\nreductions in performance, without impacting security. We instead prevent the\npropagation of any state based on speculative execution, by placing the results\nof speculative cache accesses into a small, fast L0 filter cache, that is\nnon-inclusive, non-exclusive with the rest of the cache hierarchy. This\nisolates all parts of the system that can't be quickly cleared on any change in\nthreat domain.\n  MuonTrap uses these speculative filter caches, which are cleared on context\nand protection-domain switches, along with a series of extensions to the cache\ncoherence protocol and prefetcher. This renders systems immune to cross-domain\ninformation leakage via Spectre and a host of similar attacks based on\nspeculative execution, with low performance impact and few changes to the CPU\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 16:32:11 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 12:16:16 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ainsworth", "Sam", ""], ["Jones", "Timothy M.", ""]]}, {"id": "1911.09925", "submitter": "Hasan Genc", "authors": "Hasan Genc, Seah Kim, Alon Amid, Ameer Haj-Ali, Vighnesh Iyer, Pranav\n  Prakash, Jerry Zhao, Daniel Grubb, Harrison Liew, Howard Mao, Albert Ou,\n  Colin Schmidt, Samuel Steffl, John Wright, Ion Stoica, Jonathan Ragan-Kelley,\n  Krste Asanovic, Borivoje Nikolic, Yakun Sophia Shao", "title": "Gemmini: Enabling Systematic Deep-Learning Architecture Evaluation via\n  Full-Stack Integration", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN accelerators are often developed and evaluated in isolation without\nconsidering the cross-stack, system-level effects in real-world environments.\nThis makes it difficult to appreciate the impact of System-on-Chip (SoC)\nresource contention, OS overheads, and programming-stack inefficiencies on\noverall performance/energy-efficiency. To address this challenge, we present\nGemmini, an open-source*, full-stack DNN accelerator generator. Gemmini\ngenerates a wide design-space of efficient ASIC accelerators from a flexible\narchitectural template, together with flexible programming stacks and full SoCs\nwith shared resources that capture system-level effects. Gemmini-generated\naccelerators have also been fabricated, delivering up to three\norders-of-magnitude speedups over high-performance CPUs on various DNN\nbenchmarks.\n  * https://github.com/ucb-bar/gemmini\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:51:28 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 10:33:50 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 06:53:12 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Genc", "Hasan", ""], ["Kim", "Seah", ""], ["Amid", "Alon", ""], ["Haj-Ali", "Ameer", ""], ["Iyer", "Vighnesh", ""], ["Prakash", "Pranav", ""], ["Zhao", "Jerry", ""], ["Grubb", "Daniel", ""], ["Liew", "Harrison", ""], ["Mao", "Howard", ""], ["Ou", "Albert", ""], ["Schmidt", "Colin", ""], ["Steffl", "Samuel", ""], ["Wright", "John", ""], ["Stoica", "Ion", ""], ["Ragan-Kelley", "Jonathan", ""], ["Asanovic", "Krste", ""], ["Nikolic", "Borivoje", ""], ["Shao", "Yakun Sophia", ""]]}, {"id": "1911.10349", "submitter": "Dishank Yadav", "authors": "Dishank Yadav, Chaitanya Paikara", "title": "Arsenal of Hardware Prefetchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware prefetching is one of the latency tolerance optimization techniques\nthat tolerate costly DRAM accesses. Though hardware prefetching is one of the\nfundamental mechanisms prevalent on most of the commercial machines, there is\nno prefetching technique that works well across all the access patterns and\ndifferent types of workloads. Through this paper, we propose Arsenal, a\nprefetching framework which allows the advantages provided by different data\nprefetchers to be combined, by dynamically selecting the best-suited prefetcher\nfor the current workload. Thus effectively improving the versatility of the\nprefetching system. It bases on the classic Sandbox prefetcher that dynamically\nadapts and utilizes multiple offsets for sequential prefetchers. We take it to\nthe next step by switching between prefetchers like Multi look Ahead Offset\nPrefetching and Timing SKID Prefetcher on the run. Arsenal utilizes a\nspace-efficient pooling filter, Bloom filters, that keeps track of useful\nprefetches of each of these component prefetchers and thus helps to maintain a\nscore for each of the component prefetchers. This approach is shown to provide\nbetter speedup than anyone prefetcher alone. Arsenal provides a performance\nimprovement of 44.29% on the single-core mixes and 19.5% for some of the\nselected 25 representative multi-core mixes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 11:06:22 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 03:42:36 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 12:08:14 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Yadav", "Dishank", ""], ["Paikara", "Chaitanya", ""]]}, {"id": "1911.10741", "submitter": "Jun Zhou", "authors": "Bo Wang, Jun Zhou, Weng-Fai Wong, and Li-Shiuan Peh", "title": "Shenjing: A low power reconfigurable neuromorphic accelerator with\n  partial-sum and spike networks-on-chip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next wave of on-device AI will likely require energy-efficient deep\nneural networks. Brain-inspired spiking neural networks (SNN) has been\nidentified to be a promising candidate. Doing away with the need for\nmultipliers significantly reduces energy. For on-device applications, besides\ncomputation, communication also incurs a significant amount of energy and time.\nIn this paper, we propose Shenjing, a configurable SNN architecture which fully\nexposes all on-chip communications to software, enabling software mapping of\nSNN models with high accuracy at low power. Unlike prior SNN architectures like\nTrueNorth, Shenjing does not require any model modification and retraining for\nthe mapping. We show that conventional artificial neural networks (ANN) such as\nmultilayer perceptron, convolutional neural networks, as well as the latest\nresidual neural networks can be mapped successfully onto Shenjing, realizing\nANNs with SNN's energy efficiency. For the MNIST inference problem using a\nmultilayer perceptron, we were able to achieve an accuracy of 96% while\nconsuming just 1.26mW using 10 Shenjing cores.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:32:24 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Wang", "Bo", ""], ["Zhou", "Jun", ""], ["Wong", "Weng-Fai", ""], ["Peh", "Li-Shiuan", ""]]}, {"id": "1911.11642", "submitter": "Ramya Akula", "authors": "Ramya Akula, Kartik Jain, and Deep Jigar Kotecha", "title": "System Performance with varying L1 Instruction and Data Cache Sizes: An\n  Empirical Analysis", "comments": "5 Figures and 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we investigate the fluctuations in performance caused by\nchanging the Instruction (I-cache) size and the Data (D-cache) size in the L1\ncache. We employ the Gem5 framework to simulate a system with varying\nspecifications on a single host machine. We utilize the FreqMine benchmark\navailable under the PARSEC suite as the workload program to benchmark our\nsimulated system. The Out-order CPU (O3) with Ruby memory model was simulated\nin a Full-System X86 environment with Linux OS. The chosen metrics deal with\nHit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the\nsystem. Performance observed by varying L1 size within a certain range of\nvalues was used to compute Confidence Interval based statistics for relevant\nmetrics. Our expectations, corresponding experimental observations, and\ndiscrepancies are also discussed in this report.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:33:56 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Akula", "Ramya", ""], ["Jain", "Kartik", ""], ["Kotecha", "Deep Jigar", ""]]}, {"id": "1911.11768", "submitter": "Katarzyna Grzesiak-Kope\\'c", "authors": "Katarzyna Grzesiak-Kope\\'c and Maciej Ogorza{\\l}ek", "title": "3D IC optimal layout design. A parallel and distributed topological\n  approach", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of 3D ICs layout design involves the assembly of millions of\ncomponents taking into account many different requirements and constraints such\nas topological, wiring or manufacturability ones. It is a NP-hard problem that\nrequires new non-deterministic and heuristic algorithms. Considering the time\ncomplexity, the commonly applied Fiduccia-Mattheyses partitioning algorithm is\nsuperior to any other local search method. Nevertheless, it can often miss to\nreach a quasi-optimal solution in 3D spaces. The presented approach uses an\noriginal 3D layout graph partitioning heuristics implemented with use of the\nextremal optimization method. The goal is to minimize the total wire-length in\nthe chip. In order to improve the time complexity a parallel and distributed\nJava implementation is applied. Inside one Java Virtual Machine separate\noptimization algorithms are executed by independent threads. The work may also\nbe shared among different machines by means of The Java Remote Method\nInvocation system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:29:05 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Grzesiak-Kope\u0107", "Katarzyna", ""], ["Ogorza\u0142ek", "Maciej", ""]]}, {"id": "1911.12815", "submitter": "Weidong Cao", "authors": "Weidong Cao, Liu Ke, Ayan Chakrabarti, Xuan Zhang", "title": "Neural Network-Inspired Analog-to-Digital Conversion to Achieve\n  Super-Resolution with Low-Precision RRAM Devices", "comments": "7 pages, ICCAD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works propose neural network- (NN-) inspired analog-to-digital\nconverters (NNADCs) and demonstrate their great potentials in many emerging\napplications. These NNADCs often rely on resistive random-access memory (RRAM)\ndevices to realize the NN operations and require high-precision RRAM cells\n(6~12-bit) to achieve a moderate quantization resolution (4~8-bit). Such\noptimistic assumption of RRAM resolution, however, is not supported by\nfabrication data of RRAM arrays in large-scale production process. In this\npaper, we propose an NN-inspired super-resolution ADC based on low-precision\nRRAM devices by taking the advantage of a co-design methodology that combines a\npipelined hardware architecture with a custom NN training framework. Results\nobtained from SPICE simulations demonstrate that our method leads to robust\ndesign of a 14-bit super-resolution ADC using 3-bit RRAM devices with improved\npower and speed performance and competitive figure-of-merits (FoMs). In\naddition to the linear uniform quantization, the proposed ADC can also support\nconfigurable high-resolution nonlinear quantization with high conversion speed\nand low conversion energy, enabling future intelligent analog-to-information\ninterfaces for near-sensor analytics and processing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 18:09:07 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cao", "Weidong", ""], ["Ke", "Liu", ""], ["Chakrabarti", "Ayan", ""], ["Zhang", "Xuan", ""]]}]