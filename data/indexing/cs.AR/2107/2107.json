[{"id": "2107.00092", "submitter": "Gaurab Bhattacharya", "authors": "Gaurab Bhattacharya", "title": "From DNNs to GANs: Review of efficient hardware architectures for deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent times, the trend in very large scale integration (VLSI) industry is\nmulti-dimensional, for example, reduction of energy consumption, occupancy of\nless space, precise result, less power dissipation, faster response. To meet\nthese needs, the hardware architecture should be reliable and robust to these\nproblems. Recently, neural network and deep learning has been started to impact\nthe present research paradigm significantly which consists of parameters in the\norder of millions, nonlinear function for activation, convolutional operation\nfor feature extraction, regression for classification, generative adversarial\nnetworks. These operations involve huge calculation and memory overhead.\nPresently available DSP processors are incapable of performing these operations\nand they mostly face the problems, for example, memory overhead, performance\ndrop and compromised accuracy. Moreover, if a huge silicon area is powered to\naccelerate the operation using parallel computation, the ICs will be having\nsignificant chance of burning out due to the considerable generation of heat.\nHence, novel dark silicon constraint is developed to reduce the heat\ndissipation without sacrificing the accuracy. Similarly, different algorithms\nhave been adapted to design a DSP processor compatible for fast performance in\nneural network, activation function, convolutional neural network and\ngenerative adversarial network. In this review, we illustrate the recent\ndevelopments in hardware for accelerating the efficient implementation of deep\nlearning networks with enhanced performance. The techniques investigated in\nthis review are expected to direct future research challenges of hardware\noptimization for high-performance computations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 13:23:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bhattacharya", "Gaurab", ""]]}, {"id": "2107.01725", "submitter": "Pantea Kiaei", "authors": "Pantea Kiaei, Yuan Yao, Patrick Schaumont", "title": "Real-time Detection and Adaptive Mitigation of Power-based Side-Channel\n  Leakage in SoC", "comments": null, "journal-ref": "Boston (and Beyond) Area Architecture Workshop 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Power-based side-channel is a serious security threat to the System on Chip\n(SoC). The secret information is leaked from the power profile of the system\nwhile a cryptographic algorithm is running. The mitigation requires efforts\nfrom both the software level and hardware level. Currently, there is no\ncomprehensive solution that can guarantee the whole complex system is free of\nleakage and can generically protect all cryptographic algorithms. In this\npaper, we propose a real-time leakage detection and mitigation system which\nenables the system to monitor the side-channel leakage effects of the hardware.\nOur proposed system has extensions that provide a real-time monitor of power\nconsumption, detection of side-channel leakage, and real-time adaptive\nmitigation of detected side-channel leakage. Our proposed system is generic and\ncan protect any algorithm running on it.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 20:17:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kiaei", "Pantea", ""], ["Yao", "Yuan", ""], ["Schaumont", "Patrick", ""]]}, {"id": "2107.01807", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Shafique", "title": "Q-SpiNN: A Framework for Quantizing Spiking Neural Networks", "comments": "Accepted for publication at the 2021 International Joint Conference\n  on Neural Networks (IJCNN), July 2021, Virtual Event", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prominent technique for reducing the memory footprint of Spiking Neural\nNetworks (SNNs) without decreasing the accuracy significantly is quantization.\nHowever, the state-of-the-art only focus on employing the weight quantization\ndirectly from a specific quantization scheme, i.e., either the post-training\nquantization (PTQ) or the in-training quantization (ITQ), and do not consider\n(1) quantizing other SNN parameters (e.g., neuron membrane potential), (2)\nexploring different combinations of quantization approaches (i.e., quantization\nschemes, precision levels, and rounding schemes), and (3) selecting the SNN\nmodel with a good memory-accuracy trade-off at the end. Therefore, the memory\nsaving offered by these state-of-the-art to meet the targeted accuracy is\nlimited, thereby hindering processing SNNs on the resource-constrained systems\n(e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel\nquantization framework for memory-efficient SNNs. The key mechanisms of the\nQ-SpiNN are: (1) employing quantization for different SNN parameters based on\ntheir significance to the accuracy, (2) exploring different combinations of\nquantization schemes, precision levels, and rounding schemes to find efficient\nSNN model candidates, and (3) developing an algorithm that quantifies the\nbenefit of the memory-accuracy trade-off obtained by the candidates, and\nselects the Pareto-optimal one. The experimental results show that, for the\nunsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while\nmaintaining the accuracy within 1% from the baseline on the MNIST dataset. For\nthe supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping\nthe accuracy within 2% from the baseline on the DVS-Gesture dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 06:01:15 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2107.01857", "submitter": "Andrea Stanco Ph.D.", "authors": "Andrea Stanco, Francesco B. L. Santagiustina, Luca Calderaro, Marco\n  Avesani, Tommaso Bertapelle, Daniele Dequal, Giuseppe Vallone, Paolo\n  Villoresi", "title": "Versatile and concurrent FPGA-based architecture for practical quantum\n  communication systems", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a hardware and software architecture which can be used in\nthose systems that implement practical Quantum Key Distribution (QKD) and\nQuantum Random Number Generation (QRNG) schemes. This architecture fully\nexploits the capability of a System-on-a-Chip (SoC) which comprehends both a\nField Programmable Gate Array (FPGA) and a dual core CPU unit. By assigning the\ntime-related tasks to the FPGA and the management to the CPU, we built a\nflexible system with optimized resource sharing on a commercial off-the-shelf\n(COTS) evaluation board which includes a SoC. Furthermore, by changing the\ndataflow direction, the versatile system architecture can be exploited as a QKD\ntransmitter, QKD receiver and QRNG control-acquiring unit. Finally, we\nexploited the dual core functionality and realized a concurrent stream device\nto implement a practical QKD transmitter where one core continuously receives\nfresh data at a sustained rate from an external QRNG source while the other\noperates with the FPGA to drive the qubits transmission to the QKD receiver.\nThe system was successfully tested on a long-term run proving its stability and\nsecurity. This demonstration paves the way towards a more secure QKD\nimplementation, with fully unconditional security as the QKD states are\nentirely generated by a true random process and not by deterministic expansion\nalgorithms. Eventually, this enables the realization of a standalone quantum\ntransmitter, including both the random numbers and the qubits generation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:23:08 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Stanco", "Andrea", ""], ["Santagiustina", "Francesco B. L.", ""], ["Calderaro", "Luca", ""], ["Avesani", "Marco", ""], ["Bertapelle", "Tommaso", ""], ["Dequal", "Daniele", ""], ["Vallone", "Giuseppe", ""], ["Villoresi", "Paolo", ""]]}, {"id": "2107.02358", "submitter": "Sumit Mandal", "authors": "Gokul Krishnan, Sumit K. Mandal, Chaitali Chakrabarti, Jae-sun Seo,\n  Umit Y. Ogras, Yu Cao", "title": "Impact of On-Chip Interconnect on In-Memory Acceleration of Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3460233", "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the widespread use of Deep Neural Networks (DNNs), machine learning\nalgorithms have evolved in two diverse directions -- one with ever-increasing\nconnection density for better accuracy and the other with more compact sizing\nfor energy efficiency. The increase in connection density increases on-chip\ndata movement, which makes efficient on-chip communication a critical function\nof the DNN accelerator. The contribution of this work is threefold. First, we\nillustrate that the point-to-point (P2P)-based interconnect is incapable of\nhandling a high volume of on-chip data movement for DNNs. Second, we evaluate\nP2P and network-on-chip (NoC) interconnect (with a regular topology such as a\nmesh) for SRAM- and ReRAM-based in-memory computing (IMC) architectures for a\nrange of DNNs. This analysis shows the necessity for the optimal interconnect\nchoice for an IMC DNN accelerator. Finally, we perform an experimental\nevaluation for different DNNs to empirically obtain the performance of the IMC\narchitecture with both NoC-tree and NoC-mesh. We conclude that, at the tile\nlevel, NoC-tree is appropriate for compact DNNs employed at the edge, and\nNoC-mesh is necessary to accelerate DNNs with high connection density.\nFurthermore, we propose a technique to determine the optimal choice of\ninterconnect for any given DNN. In this technique, we use analytical models of\nNoC to evaluate end-to-end communication latency of any given DNN. We\ndemonstrate that the interconnect optimization in the IMC architecture results\nin up to 6$\\times$ improvement in energy-delay-area product for VGG-19\ninference compared to the state-of-the-art ReRAM-based IMC architectures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:44:00 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Krishnan", "Gokul", ""], ["Mandal", "Sumit K.", ""], ["Chakrabarti", "Chaitali", ""], ["Seo", "Jae-sun", ""], ["Ogras", "Umit Y.", ""], ["Cao", "Yu", ""]]}, {"id": "2107.02388", "submitter": "Kaiyuan Yang", "authors": "Zhiyu Chen, Zhanghao Yu, Qing Jin, Yan He, Jingyu Wang, Sheng Lin, Dai\n  Li, Yanzhi Wang, Kaiyuan Yang", "title": "CAP-RAM: A Charge-Domain In-Memory Computing 6T-SRAM for Accurate and\n  Precision-Programmable CNN Inference", "comments": "This work has been accepted by IEEE Journal of Solid-State Circuits\n  (JSSC 2021)", "journal-ref": "IEEE Journal of Solid-State Circuits, Volume: 56, Issue: 6, Pages:\n  1924 - 1935, June 2021", "doi": "10.1109/JSSC.2021.3056447", "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compact, accurate, and bitwidth-programmable in-memory computing (IMC)\nstatic random-access memory (SRAM) macro, named CAP-RAM, is presented for\nenergy-efficient convolutional neural network (CNN) inference. It leverages a\nnovel charge-domain multiply-and-accumulate (MAC) mechanism and circuitry to\nachieve superior linearity under process variations compared to conventional\nIMC designs. The adopted semi-parallel architecture efficiently stores filters\nfrom multiple CNN layers by sharing eight standard 6T SRAM cells with one\ncharge-domain MAC circuit. Moreover, up to six levels of bit-width of weights\nwith two encoding schemes and eight levels of input activations are supported.\nA 7-bit charge-injection SAR (ciSAR) analog-to-digital converter (ADC) getting\nrid of sample and hold (S&H) and input/reference buffers further improves the\noverall energy efficiency and throughput. A 65-nm prototype validates the\nexcellent linearity and computing accuracy of CAP-RAM. A single 512x128 macro\nstores a complete pruned and quantized CNN model to achieve 98.8% inference\naccuracy on the MNIST data set and 89.0% on the CIFAR-10 data set, with a\n573.4-giga operations per second (GOPS) peak throughput and a 49.4-tera\noperations per second (TOPS)/W energy efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 04:59:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Zhiyu", ""], ["Yu", "Zhanghao", ""], ["Jin", "Qing", ""], ["He", "Yan", ""], ["Wang", "Jingyu", ""], ["Lin", "Sheng", ""], ["Li", "Dai", ""], ["Wang", "Yanzhi", ""], ["Yang", "Kaiyuan", ""]]}, {"id": "2107.02547", "submitter": "Cheng Chu", "authors": "Dawen Xu, Cheng Chu, Cheng Liu, Ying Wang, Huawei Li, Xiaowei Li,\n  Kwang-Ting Cheng", "title": "Energy-Efficient Accelerator Design for Deformable Convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable convolution networks (DCNs) proposed to address the image\nrecognition with geometric or photometric variations typically involve\ndeformable convolution that convolves on arbitrary locations of input features.\nThe locations change with different inputs and induce considerable dynamic and\nirregular memory accesses which cannot be handled by classic neural network\naccelerators (NNAs). Moreover, bilinear interpolation (BLI) operation that is\nrequired to obtain deformed features in DCNs also cannot be deployed on\nexisting NNAs directly. Although a general purposed processor (GPP) seated\nalong with classic NNAs can process the deformable convolution, the processing\non GPP can be extremely slow due to the lack of parallel computing capability.\n  To address the problem, we develop a DCN accelerator on existing NNAs to\nsupport both the standard convolution and deformable convolution. Specifically,\nfor the dynamic and irregular accesses in DCNs, we have both the input and\noutput features divided into tiles and build a tile dependency table (TDT) to\ntrack the irregular tile dependency at runtime. With the TDT, we further\ndevelop an on-chip tile scheduler to handle the dynamic and irregular accesses\nefficiently. In addition, we propose a novel mapping strategy to enable\nparallel BLI processing on NNAs and apply layer fusion techniques for more\nenergy-efficient DCN processing. According to our experiments, the proposed\naccelerator achieves orders of magnitude higher performance and energy\nefficiency compared to the typical computing architectures including ARM,\nARM+TPU, and GPU with 6.6\\% chip area penalty to a classic NNA.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:26:33 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Xu", "Dawen", ""], ["Chu", "Cheng", ""], ["Liu", "Cheng", ""], ["Wang", "Ying", ""], ["Li", "Huawei", ""], ["Li", "Xiaowei", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2107.02762", "submitter": "Saeideh Nabipour", "authors": "Saeideh Nabipour, Masoume Gholizade, Nima Nabipour", "title": "Area-Delay-Efficeint FPGA Design of 32-bit Euclid's GCD based on Sum of\n  Absolute Difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Euclids algorithm is widely used in calculating of GCD (Greatest Common\nDivisor) of two positive numbers. There are various fields where this division\nis used such as channel coding, cryptography, and error correction codes. This\nmakes the GCD a fundamental algorithm in number theory, so a number of methods\nhave been discovered to efficiently compute it. The main contribution of this\npaper is to investigate a method that computes the GCD of two 32-bit numbers\nbased on Euclidean algorithm which targets six different Xilinx chips. The\ncomplexity of this method that we call Optimized_GCDSAD is achieved by\nutilizing Sum of Absolute Difference (SAD) block which is based on a fast\ncarry-out generation function. The efficiency of the proposed architecture is\nevaluated based on criteria such as time (latency), area delay product (ADP)\nand space (slice number) complexity. The VHDL codes of these architectures have\nbeen implemented and synthesized through ISE 14.7. A detailed comparative\nanalysis indicates that the proposed Optimized_GCDSAD method based on SAD block\noutperforms previously known results.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 10:40:42 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nabipour", "Saeideh", ""], ["Gholizade", "Masoume", ""], ["Nabipour", "Nima", ""]]}, {"id": "2107.02992", "submitter": "Kaiyuan Yang", "authors": "Dai Li, Kaiyuan Yang", "title": "A Dual-Port 8-T CAM-Based Network Intrusion Detection Engine for IoT", "comments": "This work has been accepted by 2020 IEEE Solid-State Circuits Letters\n  (SSCL)", "journal-ref": "IEEE Solid-State Circuits Letters (SSCL), Volume: 3, Pages:\n  358-361, Sep. 2020", "doi": "10.1109/LSSC.2020.3022006", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents an energy- and memory-efficient pattern-matching engine\nfor a network intrusion detection system (NIDS) in the Internet of Things.\nTightly coupled architecture and circuit co-designs are proposed to fully\nexploit the statistical behaviors of NIDS pattern matching. The proposed engine\nperforms pattern matching in three phases, where the phase-1 prefix matching\nemploys reconfigurable pipelined automata processing to minimize memory\nfootprint without loss of throughput and efficiency. The processing elements\nutilize 8-T content-addressable memory (CAM) cells for dual-port search by\nleveraging proposed fixed-1s encoding. A 65-nm prototype demonstrates\nbest-in-class 1.54-fJ energy per search per pattern byte and 0.9-byte memory\nusage per pattern byte.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 03:13:48 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Li", "Dai", ""], ["Yang", "Kaiyuan", ""]]}, {"id": "2107.03096", "submitter": "Cheng Liu", "authors": "Dawen Xu, Meng He, Cheng Liu, Ying Wang, Long Cheng, Huawei Li,\n  Xiaowei Li, Kwang-Ting Cheng", "title": "R2F: A Remote Retraining Framework for AIoT Processors with Computing\n  Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AIoT processors fabricated with newer technology nodes suffer rising soft\nerrors due to the shrinking transistor sizes and lower power supply. Soft\nerrors on the AIoT processors particularly the deep learning accelerators\n(DLAs) with massive computing may cause substantial computing errors. These\ncomputing errors are difficult to be captured by the conventional training on\ngeneral purposed processors like CPUs and GPUs in a server. Applying the\noffline trained neural network models to the edge accelerators with errors\ndirectly may lead to considerable prediction accuracy loss.\n  To address the problem, we propose a remote retraining framework (R2F) for\nremote AIoT processors with computing errors. It takes the remote AIoT\nprocessor with soft errors in the training loop such that the on-site computing\nerrors can be learned with the application data on the server and the retrained\nmodels can be resilient to the soft errors. Meanwhile, we propose an optimized\npartial TMR strategy to enhance the retraining. According to our experiments,\nR2F enables elastic design trade-offs between the model accuracy and the\nperformance penalty. The top-5 model accuracy can be improved by 1.93%-13.73%\nwith 0%-200% performance penalty at high fault error rate. In addition, we\nnotice that the retraining requires massive data transmission and even\ndominates the training time, and propose a sparse increment compression\napproach for the data transmission optimization, which reduces the retraining\ntime by 38%-88% on average with negligible accuracy loss over a straightforward\nremote retraining.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:28:30 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Xu", "Dawen", ""], ["He", "Meng", ""], ["Liu", "Cheng", ""], ["Wang", "Ying", ""], ["Cheng", "Long", ""], ["Li", "Huawei", ""], ["Li", "Xiaowei", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2107.03653", "submitter": "Nikhil Pratap Ghanathe", "authors": "Nikhil Pratap Ghanathe, Vivek Seshadri, Rahul Sharma, Steve Wilton,\n  Aayan Kumar", "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications", "comments": "Accepted at The International Conference on Field-Programmable Logic\n  and Applications (FPL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:38:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ghanathe", "Nikhil Pratap", ""], ["Seshadri", "Vivek", ""], ["Sharma", "Rahul", ""], ["Wilton", "Steve", ""], ["Kumar", "Aayan", ""]]}, {"id": "2107.03781", "submitter": "S\\'ergio Pereira", "authors": "S\\'ergio Pereira, David Cerdeira, Cristiano Rodrigues, and Sandro\n  Pinto", "title": "Towards a Trusted Execution Environment via Reconfigurable FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trusted Execution Environments (TEEs) are used to protect sensitive data and\nrun secure execution for security-critical applications, by providing an\nenvironment isolated from the rest of the system. However, over the last few\nyears, TEEs have been proven weak, as either TEEs built upon security-oriented\nhardware extensions (e.g., Arm TrustZone) or resorting to dedicated secure\nelements were exploited multiple times. In this project, we introduce Trusted\nExecution Environments On-Demand (TEEOD), a novel TEE design that leverages the\nprogrammable logic (PL) in the heterogeneous system on chips (SoC) as the\nsecure execution environment. Unlike other TEE designs, TEEOD can provide\nhigh-bandwidth connections and physical on-chip isolation. We implemented a\nproof-of-concept (PoC) implementation targeting an Ultra96-V2 platform. The\nconducted evaluation demonstrated TEEOD can host up to 6 simultaneous enclaves\nwith a resource usage per enclave of 7.0%, 3.8%, and 15.3% of the total LUTs,\nFFs, and BRAMS, respectively. To demonstrate the practicability of TEEOD in\nreal-world applications, we successfully run a legacy open-source Bitcoin\nwallet.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:43:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pereira", "S\u00e9rgio", ""], ["Cerdeira", "David", ""], ["Rodrigues", "Cristiano", ""], ["Pinto", "Sandro", ""]]}, {"id": "2107.04140", "submitter": "Michael Anderson", "authors": "Michael Anderson, Benny Chen, Stephen Chen, Summer Deng, Jordan Fix,\n  Michael Gschwind, Aravind Kalaiah, Changkyu Kim, Jaewon Lee, Jason Liang,\n  Haixin Liu, Yinghai Lu, Jack Montgomery, Arun Moorthy, Satish Nadathur, Sam\n  Naghshineh, Avinash Nayak, Jongsoo Park, Chris Petersen, Martin Schatz,\n  Narayanan Sundaram, Bangsheng Tang, Peter Tang, Amy Yang, Jiecao Yu, Hector\n  Yuen, Ying Zhang, Aravind Anbudurai, Vandana Balan, Harsha Bojja, Joe Boyd,\n  Matthew Breitbach, Claudio Caldato, Anna Calvo, Garret Catron, Sneh\n  Chandwani, Panos Christeas, Brad Cottel, Brian Coutinho, Arun Dalli, Abhishek\n  Dhanotia, Oniel Duncan, Roman Dzhabarov, Simon Elmir, Chunli Fu, Wenyin Fu,\n  Michael Fulthorp, Adi Gangidi, Nick Gibson, Sean Gordon, Beatriz Padilla\n  Hernandez, Daniel Ho, Yu-Cheng Huang, Olof Johansson, Shishir Juluri, Shobhit\n  Kanaujia, Manali Kesarkar, Jonathan Killinger, Ben Kim, Rohan Kulkarni,\n  Meghan Lele, Huayu Li, Huamin Li, Yueming Li, Cynthia Liu, Jerry Liu, Bert\n  Maher, Chandra Mallipedi, Seema Mangla, Kiran Kumar Matam, Jubin Mehta,\n  Shobhit Mehta, Christopher Mitchell, Bharath Muthiah, Nitin Nagarkatte,\n  Ashwin Narasimha, Bernard Nguyen, Thiara Ortiz, Soumya Padmanabha, Deng Pan,\n  Ashwin Poojary, Ye (Charlotte) Qi, Olivier Raginel, Dwarak Rajagopal, Tristan\n  Rice, Craig Ross, Nadav Rotem, Scott Russ, Kushal Shah, Baohua Shan, Hao\n  Shen, Pavan Shetty, Krish Skandakumaran, Kutta Srinivasan, Roshan Sumbaly,\n  Michael Tauberg, Mor Tzur, Sidharth Verma, Hao Wang, Man Wang, Ben Wei, Alex\n  Xia, Chenyu Xu, Martin Yang, Kai Zhang, Ruoxi Zhang, Ming Zhao, Whitney Zhao,\n  Rui Zhu, Lin Qiao, Misha Smelyanskiy, Bill Jia, Vijay Rao", "title": "First-Generation Inference Accelerator Deployment at Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide a deep dive into the deployment of inference\naccelerators at Facebook. Many of our ML workloads have unique characteristics,\nsuch as sparse memory accesses, large model sizes, as well as high compute,\nmemory and network bandwidth requirements. We co-designed a high-performance,\nenergy-efficient inference accelerator platform based on these requirements. We\ndescribe the inference accelerator platform ecosystem we developed and deployed\nat Facebook: both hardware, through Open Compute Platform (OCP), and software\nframework and tooling, through Pytorch/Caffe2/Glow. A characteristic of this\necosystem from the start is its openness to enable a variety of AI accelerators\nfrom different vendors. This platform, with six low-power accelerator cards\nalongside a single-socket host CPU, allows us to serve models of high\ncomplexity that cannot be easily or efficiently run on CPUs. We describe\nvarious performance optimizations, at both platform and accelerator level,\nwhich enables this platform to serve production traffic at Facebook. We also\nshare deployment challenges, lessons learned during performance optimization,\nas well as provide guidance for future inference hardware co-design.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:52:42 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 18:00:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Anderson", "Michael", "", "Charlotte"], ["Chen", "Benny", "", "Charlotte"], ["Chen", "Stephen", "", "Charlotte"], ["Deng", "Summer", "", "Charlotte"], ["Fix", "Jordan", "", "Charlotte"], ["Gschwind", "Michael", "", "Charlotte"], ["Kalaiah", "Aravind", "", "Charlotte"], ["Kim", "Changkyu", "", "Charlotte"], ["Lee", "Jaewon", "", "Charlotte"], ["Liang", "Jason", "", "Charlotte"], ["Liu", "Haixin", "", "Charlotte"], ["Lu", "Yinghai", "", "Charlotte"], ["Montgomery", "Jack", "", "Charlotte"], ["Moorthy", "Arun", "", "Charlotte"], ["Nadathur", "Satish", "", "Charlotte"], ["Naghshineh", "Sam", "", "Charlotte"], ["Nayak", "Avinash", "", "Charlotte"], ["Park", "Jongsoo", "", "Charlotte"], ["Petersen", "Chris", "", "Charlotte"], ["Schatz", "Martin", "", "Charlotte"], ["Sundaram", "Narayanan", "", "Charlotte"], ["Tang", "Bangsheng", "", "Charlotte"], ["Tang", "Peter", "", "Charlotte"], ["Yang", "Amy", "", "Charlotte"], ["Yu", "Jiecao", "", "Charlotte"], ["Yuen", "Hector", "", "Charlotte"], ["Zhang", "Ying", "", "Charlotte"], ["Anbudurai", "Aravind", "", "Charlotte"], ["Balan", "Vandana", "", "Charlotte"], ["Bojja", "Harsha", "", "Charlotte"], ["Boyd", "Joe", "", "Charlotte"], ["Breitbach", "Matthew", "", "Charlotte"], ["Caldato", "Claudio", "", "Charlotte"], ["Calvo", "Anna", "", "Charlotte"], ["Catron", "Garret", "", "Charlotte"], ["Chandwani", "Sneh", "", "Charlotte"], ["Christeas", "Panos", "", "Charlotte"], ["Cottel", "Brad", "", "Charlotte"], ["Coutinho", "Brian", "", "Charlotte"], ["Dalli", "Arun", "", "Charlotte"], ["Dhanotia", "Abhishek", "", "Charlotte"], ["Duncan", "Oniel", "", "Charlotte"], ["Dzhabarov", "Roman", "", "Charlotte"], ["Elmir", "Simon", "", "Charlotte"], ["Fu", "Chunli", "", "Charlotte"], ["Fu", "Wenyin", "", "Charlotte"], ["Fulthorp", "Michael", "", "Charlotte"], ["Gangidi", "Adi", "", "Charlotte"], ["Gibson", "Nick", "", "Charlotte"], ["Gordon", "Sean", "", "Charlotte"], ["Hernandez", "Beatriz Padilla", "", "Charlotte"], ["Ho", "Daniel", "", "Charlotte"], ["Huang", "Yu-Cheng", "", "Charlotte"], ["Johansson", "Olof", "", "Charlotte"], ["Juluri", "Shishir", "", "Charlotte"], ["Kanaujia", "Shobhit", "", "Charlotte"], ["Kesarkar", "Manali", "", "Charlotte"], ["Killinger", "Jonathan", "", "Charlotte"], ["Kim", "Ben", "", "Charlotte"], ["Kulkarni", "Rohan", "", "Charlotte"], ["Lele", "Meghan", "", "Charlotte"], ["Li", "Huayu", "", "Charlotte"], ["Li", "Huamin", "", "Charlotte"], ["Li", "Yueming", "", "Charlotte"], ["Liu", "Cynthia", "", "Charlotte"], ["Liu", "Jerry", "", "Charlotte"], ["Maher", "Bert", "", "Charlotte"], ["Mallipedi", "Chandra", "", "Charlotte"], ["Mangla", "Seema", "", "Charlotte"], ["Matam", "Kiran Kumar", "", "Charlotte"], ["Mehta", "Jubin", "", "Charlotte"], ["Mehta", "Shobhit", "", "Charlotte"], ["Mitchell", "Christopher", "", "Charlotte"], ["Muthiah", "Bharath", "", "Charlotte"], ["Nagarkatte", "Nitin", "", "Charlotte"], ["Narasimha", "Ashwin", "", "Charlotte"], ["Nguyen", "Bernard", "", "Charlotte"], ["Ortiz", "Thiara", "", "Charlotte"], ["Padmanabha", "Soumya", "", "Charlotte"], ["Pan", "Deng", "", "Charlotte"], ["Poojary", "Ashwin", "", "Charlotte"], ["Ye", "", "", "Charlotte"], ["Qi", "", ""], ["Raginel", "Olivier", ""], ["Rajagopal", "Dwarak", ""], ["Rice", "Tristan", ""], ["Ross", "Craig", ""], ["Rotem", "Nadav", ""], ["Russ", "Scott", ""], ["Shah", "Kushal", ""], ["Shan", "Baohua", ""], ["Shen", "Hao", ""], ["Shetty", "Pavan", ""], ["Skandakumaran", "Krish", ""], ["Srinivasan", "Kutta", ""], ["Sumbaly", "Roshan", ""], ["Tauberg", "Michael", ""], ["Tzur", "Mor", ""], ["Verma", "Sidharth", ""], ["Wang", "Hao", ""], ["Wang", "Man", ""], ["Wei", "Ben", ""], ["Xia", "Alex", ""], ["Xu", "Chenyu", ""], ["Yang", "Martin", ""], ["Zhang", "Kai", ""], ["Zhang", "Ruoxi", ""], ["Zhao", "Ming", ""], ["Zhao", "Whitney", ""], ["Zhu", "Rui", ""], ["Qiao", "Lin", ""], ["Smelyanskiy", "Misha", ""], ["Jia", "Bill", ""], ["Rao", "Vijay", ""]]}, {"id": "2107.04175", "submitter": "Tao Lu", "authors": "Tao Lu", "title": "A Survey on RISC-V Security: Hardware and Architecture", "comments": "39 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Internet of Things (IoT) is an ongoing technological revolution. Embedded\nprocessors are the processing engines of smart IoT devices. For decades, these\nprocessors were mainly based on the Arm instruction set architecture (ISA). In\nrecent years, the free and open RISC-V ISA standard has attracted the attention\nof industry and academia and is becoming the mainstream. Data security and user\nprivacy protection are common challenges faced by all IoT devices. In order to\ndeal with foreseeable security threats, the RISC-V community is studying\nsecurity solutions aimed at achieving a root of trust (RoT) and ensuring that\nsensitive information on RISC-V devices is not tampered with or leaked. Many\nRISC-V security research projects are underway, but the academic community has\nnot yet conducted a comprehensive survey of RISC-V security solutions. In order\nto fill this research gap, this paper presents an in-depth survey on RISC-V\nsecurity technologies. This paper summarizes the representative security\nmechanisms of RISC-V hardware and architecture. Based on our survey, we predict\nthe future research and development directions of RISC-V security. We hope that\nour research can inspire RISC-V researchers and developers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 02:04:26 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Lu", "Tao", ""]]}, {"id": "2107.04191", "submitter": "Kongtao Chen", "authors": "Kongtao Chen, Ken Franko, Ruoxin Sang", "title": "Structured Model Pruning of Convolutional Networks on Tensor Processing\n  Units", "comments": "International Conference on Machine Learning 2021 Workshop on\n  Overparameterization: Pitfalls & Opportunities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The deployment of convolutional neural networks is often hindered by high\ncomputational and storage requirements. Structured model pruning is a promising\napproach to alleviate these requirements. Using the VGG-16 model as an example,\nwe measure the accuracy-efficiency trade-off for various structured model\npruning methods and datasets (CIFAR-10 and ImageNet) on Tensor Processing Units\n(TPUs). To measure the actual performance of models, we develop a structured\nmodel pruning library for TensorFlow2 to modify models in place (instead of\nadding mask layers). We show that structured model pruning can significantly\nimprove model memory usage and speed on TPUs without losing accuracy,\nespecially for small datasets (e.g., CIFAR-10).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 03:41:31 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 17:04:28 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chen", "Kongtao", ""], ["Franko", "Ken", ""], ["Sang", "Ruoxin", ""]]}, {"id": "2107.04244", "submitter": "Yao Chen", "authors": "Xinheng Liu, Yao Chen, Cong Hao, Ashutosh Dhar, Deming Chen", "title": "WinoCNN: Kernel Sharing Winograd Systolic Array for Efficient\n  Convolutional Neural Network Acceleration on FPGAs", "comments": "Published in the proceedings of ASAP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The combination of Winograd's algorithm and systolic array architecture has\ndemonstrated the capability of improving DSP efficiency in accelerating\nconvolutional neural networks (CNNs) on FPGA platforms. However, handling\narbitrary convolution kernel sizes in FPGA-based Winograd processing elements\nand supporting efficient data access remain underexplored. In this work, we are\nthe first to propose an optimized Winograd processing element (WinoPE), which\ncan naturally support multiple convolution kernel sizes with the same amount of\ncomputing resources and maintains high runtime DSP efficiency. Using the\nproposed WinoPE, we construct a highly efficient systolic array accelerator,\ntermed WinoCNN. We also propose a dedicated memory subsystem to optimize the\ndata access. Based on the accelerator architecture, we build accurate resource\nand performance modeling to explore optimal accelerator configurations under\ndifferent resource constraints. We implement our proposed accelerator on\nmultiple FPGAs, which outperforms the state-of-the-art designs in terms of both\nthroughput and DSP efficiency. Our implementation achieves DSP efficiency up to\n1.33 GOPS/DSP and throughput up to 3.1 TOPS with the Xilinx ZCU102 FPGA. These\nare 29.1\\% and 20.0\\% better than the best solutions reported previously,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:37:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Xinheng", ""], ["Chen", "Yao", ""], ["Hao", "Cong", ""], ["Dhar", "Ashutosh", ""], ["Chen", "Deming", ""]]}, {"id": "2107.05453", "submitter": "Rui Zhang", "authors": "Rui Zhang, Swarnendu Biswas, Vignesh Balaji, Michael D. Bond, Brandon\n  Lucia", "title": "Neat: Low-Complexity, Efficient On-Chip Cache Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cache coherence protocols such as MESI that use writer-initiated invalidation\nhave high complexity and sometimes have poor performance and energy usage,\nespecially under false sharing. Such protocols require numerous transient\nstates, a shared directory, and support for core-to-core communication, while\nalso suffering under false sharing. An alternative to MESI's writer-initiated\ninvalidation is self-invalidation, which achieves lower complexity than MESI\nbut adds high performance costs or relies on programmer annotations or specific\ndata access patterns.\n  This paper presents Neat, a low-complexity, efficient cache coherence\nprotocol. Neat uses self-invalidation, thus avoiding MESI's transient states,\ndirectory, and core-to-core communication requirements. Neat uses novel\nmechanisms that effectively avoid many unnecessary self-invalidations. An\nevaluation shows that Neat is simple and has lower verification complexity than\nthe MESI protocol. Neat not only outperforms state-of-the-art self-invalidation\nprotocols, but its performance and energy consumption are comparable to MESI's,\nand it outperforms MESI under false sharing.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 14:25:13 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 08:32:05 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Rui", ""], ["Biswas", "Swarnendu", ""], ["Balaji", "Vignesh", ""], ["Bond", "Michael D.", ""], ["Lucia", "Brandon", ""]]}, {"id": "2107.05530", "submitter": "Sudeep Pasricha", "authors": "Febin P. Sunny, Asif Mirza, Mahdi Nikdast, Sudeep Pasricha", "title": "ROBIN: A Robust Optical Binary Neural Network Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Domain specific neural network accelerators have garnered attention because\nof their improved energy efficiency and inference performance compared to CPUs\nand GPUs. Such accelerators are thus well suited for resource-constrained\nembedded systems. However, mapping sophisticated neural network models on these\naccelerators still entails significant energy and memory consumption, along\nwith high inference time overhead. Binarized neural networks (BNNs), which\nutilize single-bit weights, represent an efficient way to implement and deploy\nneural network models on accelerators. In this paper, we present a novel\noptical-domain BNN accelerator, named ROBIN, which intelligently integrates\nheterogeneous microring resonator optical devices with complementary\ncapabilities to efficiently implement the key functionalities in BNNs. We\nperform detailed fabrication-process variation analyses at the optical device\nlevel, explore efficient corrective tuning for these devices, and integrate\ncircuit-level optimization to counter thermal variations. As a result, our\nproposed ROBIN architecture possesses the desirable traits of being robust,\nenergy-efficient, low latency, and high throughput, when executing BNN models.\nOur analysis shows that ROBIN can outperform the best-known optical BNN\naccelerators and also many electronic accelerators. Specifically, our\nenergy-efficient ROBIN design exhibits energy-per-bit values that are ~4x lower\nthan electronic BNN accelerators and ~933x lower than a recently proposed\nphotonic BNN accelerator, while a performance-efficient ROBIN design shows ~3x\nand ~25x better performance than electronic and photonic BNN accelerators,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:00:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sunny", "Febin P.", ""], ["Mirza", "Asif", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2107.06419", "submitter": "Sheng-Chun Kao", "authors": "Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, Tushar Krishna", "title": "ATTACC the Quadratic Bottleneck of Attention Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Attention mechanisms form the backbone of state-of-the-art machine learning\nmodels for a variety of tasks. Deploying them on deep neural network (DNN)\naccelerators, however, is prohibitively challenging especially under long\nsequences. Operators in attention layers exhibit limited reuse and quadratic\ngrowth in memory footprint, leading to severe memory-boundedness. This paper\nintroduces a new attention-tailored dataflow, termed FLAT, which leverages\noperator fusion, loop-nest optimizations, and interleaved execution. It\nincreases the effective memory bandwidth by efficiently utilizing the\nhigh-bandwidth, low-capacity on-chip buffer and thus achieves better run time\nand compute resource utilization. We term FLAT-compatible accelerators ATTACC.\nIn our evaluation, ATTACC achieves 1.94x and 1.76x speedup and 49% and 42% of\nenergy reduction comparing to state-of-the-art edge and cloud accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 22:23:40 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kao", "Sheng-Chun", ""], ["Subramanian", "Suvinay", ""], ["Agrawal", "Gaurav", ""], ["Krishna", "Tushar", ""]]}, {"id": "2107.06433", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi and Fabrizio Petrini", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its\n  Performance on PIUMA and Xeon CPU", "comments": "11 Pages. arXiv admin note: substantial text overlap with\n  arXiv:2005.06727", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an optimization\nproblem that costs $O (V^3 \\log(V)) $ where $V$ is the number of unique words\nin the document. Fortunately, WMD can be framed as an Earth Mover's Distance\n(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding\nan entropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by computing the WMD of a single query document against multiple\ntarget documents at once, for example by finding whether a given tweet is\nsimilar to any other tweets of a given day.\n  In this paper, we first present a shared-memory parallel Sinkhorn-Knopp\nalgorithm to compute the WMD of one document against many other documents by\nadopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the\noriginal $O(V^2)$ dense compute-heavy version into an equivalent sparse one\nwhich is mapped onto the new Intel Programmable Integrated Unified Memory\nArchitecture (PIUMA) system. The WMD parallel implementation achieves 67x\nspeedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We\nalso show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on\nSinkhorn-WMD and also provide better strong scaling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 00:29:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2107.06511", "submitter": "Dingcheng Yang", "authors": "Dingcheng Yang, Wenjian Yu, Yuanbo Guo, Wenjie Liang", "title": "CNN-Cap: Effective Convolutional Neural Network Based Capacitance Models\n  for Full-Chip Parasitic Extraction", "comments": "9 pages, 13 figures. Accepted at 2021 International Conference On\n  Computer Aided Design (ICCAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate capacitance extraction is becoming more important for designing\nintegrated circuits under advanced process technology. The pattern matching\nbased full-chip extraction methodology delivers fast computational speed, but\nsuffers from large error, and tedious efforts on building capacitance models of\nthe increasing structure patterns. In this work, we propose an effective method\nfor building convolutional neural network (CNN) based capacitance models\n(called CNN-Cap) for two-dimensional (2-D) structures in full-chip capacitance\nextraction. With a novel grid-based data representation, the proposed method is\nable to model the pattern with a variable number of conductors, so that largely\nreduce the number of patterns. Based on the ability of ResNet architecture on\ncapturing spatial information and the proposed training skills, the obtained\nCNN-Cap exhibits much better performance over the multilayer perception neural\nnetwork based capacitance model while being more versatile. Extensive\nexperiments on a 55nm and a 15nm process technologies have demonstrated that\nthe error of total capacitance produced with CNN-Cap is always within 1.3% and\nthe error of produced coupling capacitance is less than 10% in over 99.5%\nprobability. CNN-Cap runs more than 4000X faster than 2-D field solver on a GPU\nserver, while it consumes negligible memory compared to the look-up table based\ncapacitance model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 07:14:35 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Yang", "Dingcheng", ""], ["Yu", "Wenjian", ""], ["Guo", "Yuanbo", ""], ["Liang", "Wenjie", ""]]}, {"id": "2107.06814", "submitter": "J\\\"urgen Maier", "authors": "J\\\"urgen Maier", "title": "Gain and Pain of a Reliable Delay Model", "comments": "9 pages, 11 figures, 2 tables, extension of conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art digital circuit design tools almost exclusively rely on pure\nand inertial delay for timing simulations. While these provide reasonable\nestimations at very low execution time in the average case, their ability to\ncover complex signal traces is limited. Research has provided the dynamic\nInvolution Delay Model (IDM) as a promising alternative, which was shown (i) to\ndepict reality more closely and recently (ii) to be compatible with modern\nsimulation suites. In this paper we complement these encouraging results by\nexperimentally exploring the behavioral coverage for more advanced circuits. In\ndetail we apply the IDM to three simple circuits (a combinatorial loop, an SR\nlatch and an adder), interpret the delivered results and evaluate the overhead\nin realistic settings. Comparisons to digital (inertial delay) and analog\n(SPICE) simulations reveal, that the IDM delivers very fine-grained results,\nwhich match analog simulations very closely. Moreover, severe shortcomings of\ninertial delay become apparent in our simulations, as it fails to depict a\nrange of malicious behaviors. Overall the Involution Delay Model hence\nrepresents a viable upgrade to the available delay models in modern digital\ntiming simulation tools.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:32:44 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 06:50:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Maier", "J\u00fcrgen", ""]]}, {"id": "2107.06871", "submitter": "Zheyu Yan", "authors": "Zheyu Yan, Da-Cheng Juan, Xiaobo Sharon Hu, Yiyu Shi", "title": "Uncertainty Modeling of Emerging Device-based Computing-in-Memory Neural\n  Accelerators with Application to Neural Architecture Search", "comments": null, "journal-ref": null, "doi": "10.1145/3394885.3431635", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emerging device-based Computing-in-memory (CiM) has been proved to be a\npromising candidate for high-energy efficiency deep neural network (DNN)\ncomputations. However, most emerging devices suffer uncertainty issues,\nresulting in a difference between actual data stored and the weight value it is\ndesigned to be. This leads to an accuracy drop from trained models to actually\ndeployed platforms. In this work, we offer a thorough analysis of the effect of\nsuch uncertainties-induced changes in DNN models. To reduce the impact of\ndevice uncertainties, we propose UAE, an uncertainty-aware Neural Architecture\nSearch scheme to identify a DNN model that is both accurate and robust against\ndevice uncertainties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 23:29:36 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Yan", "Zheyu", ""], ["Juan", "Da-Cheng", ""], ["Hu", "Xiaobo Sharon", ""], ["Shi", "Yiyu", ""]]}, {"id": "2107.06960", "submitter": "Andreas Gerstlauer", "authors": "Jackson Farley, Andreas Gerstlauer", "title": "Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report UT-CERC-21-01", "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rising research challenge is running costly machine learning (ML) networks\nlocally on resource-constrained edge devices. ML networks with large\nconvolutional layers can easily exceed available memory, increasing latency due\nto excessive swapping. Previous memory reduction techniques such as pruning and\nquantization reduce model accuracy and often require retraining. Alternatively,\ndistributed methods partition the convolutions into equivalent smaller\nsub-computations, but the implementations introduce communication costs and\nrequire a network of devices. However, a distributed partitioning approach can\nalso be used to run in a reduced memory footprint on a single device by\nsubdividing the network into smaller operations.\n  This report extends prior work on distributed partitioning using tiling and\nfusing of convolutional layers into a memory-aware execution on a single\ndevice. Our approach extends prior fusing strategies to allow for two groups of\nconvolutional layers that are fused and tiled independently. This approach\nreduces overhead via data reuse, and reduces the memory footprint further. We\nalso propose a memory usage predictor coupled with a search algorithm to\nprovide fusing and tiling configurations for an arbitrary set of convolutional\nlayers. When applied to the YOLOv2 object detection network, results show that\nour approach can run in less than half the memory, and with a speedup of up to\n2.78 under severe memory constraints. Additionally, our algorithm will return a\nconfiguration with a latency that is within 6% of the best latency measured in\na manual search.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 19:45:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Farley", "Jackson", ""], ["Gerstlauer", "Andreas", ""]]}, {"id": "2107.07169", "submitter": "Mazen Saghir", "authors": "Imad Al Assir, Mohamad El Iskandarani, Hadi Rayan Al Sandid, and Mazen\n  A. R. Saghir", "title": "Arrow: A RISC-V Vector Accelerator for Machine Learning Inference", "comments": "Presented at the Fifth Workshop on Computer Architecture Research\n  with RISC-V (CARRV 2021), co-located with ISCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present Arrow, a configurable hardware accelerator\narchitecture that implements a subset of the RISC-V v0.9 vector ISA extension\naimed at edge machine learning inference. Our experimental results show that an\nArrow co-processor can execute a suite of vector and matrix benchmarks\nfundamental to machine learning inference 2 - 78x faster than a scalar RISC\nprocessor while consuming 20% - 99% less energy when implemented in a Xilinx\nXC7A200T-1SBG484C FPGA.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:36:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Assir", "Imad Al", ""], ["Iskandarani", "Mohamad El", ""], ["Sandid", "Hadi Rayan Al", ""], ["Saghir", "Mazen A. R.", ""]]}, {"id": "2107.07647", "submitter": "Ian Colbert", "authors": "Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das", "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image\n  Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 23:49:37 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 05:34:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "2107.07983", "submitter": "Zhi-Gang Liu", "authors": "Zhi-Gang Liu, Paul N. Whatmough, Yuhao Zhu, Matthew Mattina", "title": "S2TA: Exploiting Structured Sparsity for Energy-Efficient Mobile CNN\n  Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploiting sparsity is a key technique in accelerating quantized\nconvolutional neural network (CNN) inference on mobile devices. Prior sparse\nCNN accelerators largely exploit un-structured sparsity and achieve significant\nspeedups. Due to the unbounded, largely unpredictable sparsity patterns,\nhowever, exploiting unstructured sparsity requires complicated hardware design\nwith significant energy and area overhead, which is particularly detrimental to\nmobile/IoT inference scenarios where energy and area efficiency are crucial. We\npropose to exploit structured sparsity, more specifically, Density Bound Block\n(DBB) sparsity for both weights and activations. DBB block tensors bound the\nmaximum number of non-zeros per block. DBB thus exposes statically predictable\nsparsity patterns that enable lean sparsity-exploiting hardware. We propose new\nhardware primitives to implement DBB sparsity for (static) weights and\n(dynamic) activations, respectively, with very low overheads. Building on top\nof the primitives, we describe S2TA, a systolic array-based CNN accelerator\nthat exploits joint weight and activation DBB sparsity and new dimensions of\ndata reuse unavailable on the traditional systolic array. S2TA in 16nm achieves\nmore than 2x speedup and energy reduction compared to a strong baseline of a\nsystolic array with zero-value clock gating, over five popular CNN benchmarks.\nCompared to two recent non-systolic sparse accelerators, Eyeriss v2 (65nm) and\nSparTen (45nm), S2TA in 65nm uses about 2.2x and 3.1x less energy per\ninference, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:57:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Liu", "Zhi-Gang", ""], ["Whatmough", "Paul N.", ""], ["Zhu", "Yuhao", ""], ["Mattina", "Matthew", ""]]}, {"id": "2107.08367", "submitter": "Bowen Tang", "authors": "Bowen Tang, Chenggang Wu, Zhe Wang, Lichen Jia, Pen-Chung Yew,\n  Yueqiang Cheng, Yinqian Zhang, Chenxi Wang, Guoqing Harry Xu", "title": "SpecBox: A Label-Based Transparent Speculation Scheme Against Transient\n  Execution Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speculative execution techniques have been a cornerstone of modern processors\nto improve instruction-level parallelism. However, recent studies showed that\nthis kind of techniques could be exploited by attackers to leak secret data via\ntransient execution attacks, such as Spectre. Many defenses are proposed to\naddress this problem, but they all face various challenges: (1) Tracking data\nflow in the instruction pipeline could comprehensively address this problem,\nbut it could cause pipeline stalls and incur high performance overhead; (2)\nMaking side effect of speculative execution imperceptible to attackers, but it\noften needs additional storage components and complicated data movement\noperations. In this paper, we propose a label-based transparent speculation\nscheme called SpecBox. It dynamically partitions the cache system to isolate\nspeculative data and non-speculative data, which can prevent transient\nexecution from being observed by subsequent execution. Moreover, it uses thread\nownership semaphores to prevent speculative data from being accessed across\ncores. In addition, SpecBox also enhances the auxiliary components in the cache\nsystem against transient execution attacks, such as hardware prefetcher. Our\nsecurity analysis shows that SpecBox is secure and the performance evaluation\nshows that the performance overhead on SPEC CPU 2006 and PARSEC-3.0 benchmarks\nis small.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 05:24:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tang", "Bowen", ""], ["Wu", "Chenggang", ""], ["Wang", "Zhe", ""], ["Jia", "Lichen", ""], ["Yew", "Pen-Chung", ""], ["Cheng", "Yueqiang", ""], ["Zhang", "Yinqian", ""], ["Wang", "Chenxi", ""], ["Xu", "Guoqing Harry", ""]]}, {"id": "2107.08600", "submitter": "Huazi Zhang", "authors": "Jiajie Tong, Xianbin Wang, Qifan Zhang, Huazi Zhang, Rong Li, Jun\n  Wang, Wen Tong", "title": "Fast polar codes for terabits-per-second throughput communications", "comments": "8 pages, 5 figures. Part of this paper was presented in an invited\n  talk at the 2021 International Symposium on Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Targeting high-throughput and low-power communications, we implement two\nsuccessive cancellation (SC) decoders for polar codes. With $16nm$ ASIC\ntechnology, the area efficiency and energy efficiency are $4Tbps/mm^2$ and\n$0.63pJ/bit$, respectively, for the unrolled decoder, and $561Gbps/mm^2$ and\n$1.21pJ/bit$, respectively, for the recursive decoder. To achieve such a high\nthroughput, a novel code construction, coined as fast polar codes, is proposed\nand jointly optimized with a highly-parallel SC decoding architecture. First,\nwe reuse existing modules to fast decode more outer code blocks, and then\nmodify code construction to facilitate faster decoding for all outer code\nblocks up to a degree of parallelism of $16$. Furthermore, parallel comparison\ncircuits and bit quantization schemes are customized for hardware\nimplementation. Collectively, they contribute to an $2.66\\times$ area\nefficiency improvement and $33\\%$ energy saving over the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 03:32:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tong", "Jiajie", ""], ["Wang", "Xianbin", ""], ["Zhang", "Qifan", ""], ["Zhang", "Huazi", ""], ["Li", "Rong", ""], ["Wang", "Jun", ""], ["Tong", "Wen", ""]]}, {"id": "2107.08607", "submitter": "Huazi Zhang", "authors": "Jiajie Tong, Qifan Zhang, Huazi Zhang, Rong Li, Jun Wang, Wen Tong", "title": "A unified polar decoder platform for low-power and low-cost devices", "comments": "6 pages, 8 figures. Part of this paper was presented in an invited\n  talk at the 2021 International Symposium on Information Theory (ISIT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we design a polar decoding platform for diverse application\nscenarios that require low-cost and low-power communications. Specifically,\nprevalent polar decoders such as successive cancellation (SC), SC-list (SCL)\nand Fano decoders are all supported under the same architecture. Unlike\nhigh-throughput or low-latency decoders that promote parallelism, this\narchitecture promotes serialization by repeatedly calling a ``sub-process''\nthat is executed by a core module. The resulting serial SCL-8 decoder is only 3\ntimes as big as an SC decoder. Cost and power are minimized through resource\nsharing and adaptive decoding techniques, etc. We carried out performance\nsimulation and hardware implementation to evaluate the actual chip area and\nenergy consumption.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 03:43:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tong", "Jiajie", ""], ["Zhang", "Qifan", ""], ["Zhang", "Huazi", ""], ["Li", "Rong", ""], ["Wang", "Jun", ""], ["Tong", "Wen", ""]]}, {"id": "2107.08709", "submitter": "Zhihui Zhang", "authors": "Zhihui Zhang, Jingwen Leng, Shuwen Lu, Youshan Miao, Yijia Diao, Minyi\n  Guo, Chao Li, Yuhao Zhu", "title": "ZIPPER: Exploiting Tile- and Operator-level Parallelism for General and\n  Scalable Graph Neural Network Acceleration", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) start to gain momentum after showing significant\nperformance improvement in a variety of domains including molecular science,\nrecommendation, and transportation. Turning such performance improvement of\nGNNs into practical applications relies on effective and efficient execution,\nespecially for inference. However, neither CPU nor GPU can meet these needs if\nconsidering both performance and energy efficiency. That's because accelerating\nGNNs is challenging due to their excessive memory usage and arbitrary\ninterleaving of diverse operations. Besides, the semantics gap between the\nhigh-level GNN programming model and efficient hardware makes it difficult in\naccelerating general-domain GNNs.\n  To address the challenge, we propose Zipper, an efficient yet general\nacceleration system for GNNs. The keys to Zipper include a graph-native\nintermediate representation (IR) and the associated compiler. By capturing GNN\nprimitive operations and representing with GNN IR, Zipper is able to fit GNN\nsemantics into hardware structure for efficient execution. The IR also enables\nGNN-specific optimizations including sparse graph tiling and redundant\noperation elimination. We further present an hardware architecture design\nconsisting of dedicated blocks for different primitive operations, along with a\nrun-time scheduler to map a IR program to the hardware blocks. Our evaluation\nshows that Zipper achieves 93.6x speedup and 147x energy reduction over Intel\nXeon CPU, and 1.56x speedup and 4.85x energy reduction over NVIDIA V100 GPU on\naverages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:34:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhang", "Zhihui", ""], ["Leng", "Jingwen", ""], ["Lu", "Shuwen", ""], ["Miao", "Youshan", ""], ["Diao", "Yijia", ""], ["Guo", "Minyi", ""], ["Li", "Chao", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2107.08716", "submitter": "Gagandeep Singh", "authors": "Gagandeep Singh, Dionysios Diamantopoulos, Juan G\\'omez-Luna,\n  Christoph Hagleitner, Sander Stuijk, Henk Corporaal, Onur Mutlu", "title": "NERO: Accelerating Weather Prediction using Near-Memory Reconfigurable\n  Fabric", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.08241,\n  arXiv:2106.06433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing climate change calls for fast and accurate weather and climate\nmodeling. However, when solving large-scale weather prediction simulations,\nstate-of-the-art CPU and GPU implementations suffer from limited performance\nand high energy consumption. These implementations are dominated by complex\nirregular memory access patterns and low arithmetic intensity that pose\nfundamental challenges to acceleration. To overcome these challenges, we\npropose and evaluate the use of near-memory acceleration using a reconfigurable\nfabric with high-bandwidth memory (HBM). We focus on compound stencils that are\nfundamental kernels in weather prediction models. By using high-level synthesis\ntechniques, we develop NERO, an FPGA+HBM-based accelerator connected through\nIBM OCAPI (Open Coherent Accelerator Processor Interface) to an IBM POWER9 host\nsystem. Our experimental results show that NERO outperforms a 16-core POWER9\nsystem by 5.3x and 12.7x when running two different compound stencil kernels.\nNERO reduces the energy consumption by 12x and 35x for the same two kernels\nover the POWER9 system with an energy efficiency of 1.61 GFLOPS/Watt and 21.01\nGFLOPS/Watt. We conclude that employing near-memory acceleration solutions for\nweather prediction modeling is promising as a means to achieve both high\nperformance and high energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:41:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Singh", "Gagandeep", ""], ["Diamantopoulos", "Dionysios", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Hagleitner", "Christoph", ""], ["Stuijk", "Sander", ""], ["Corporaal", "Henk", ""], ["Mutlu", "Onur", ""]]}, {"id": "2107.08997", "submitter": "Hans Dermot Doran", "authors": "Hans Dermot Doran, Timo Lang", "title": "Dynamic Lockstep Processors for Applications with Functional Safety\n  Relevance", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lockstep processing is a recognized technique for helping to secure\nfunctional-safety relevant processing against, for instance, single upset\nerrors that might cause faulty execution of code. Lockstepping processors does\nhowever bind processing resources in a fashion not beneficial to architectures\nand applications that would benefit from multi-core/-processors. We propose a\nnovel on-demand synchronizing of cores/processors for lock-step operation\nfeaturing post-processing resource release, a concept that facilitates the\nimplementation of modularly redundant core/processor arrays. We discuss the\nfundamentals of the design and some implementation notes on work achieved to\ndate.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:17:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Doran", "Hans Dermot", ""], ["Lang", "Timo", ""]]}, {"id": "2107.09178", "submitter": "Aman Arora", "authors": "Aman Arora, Bagus Hanindhito, Lizy K. John", "title": "Compute RAMs: Adaptable Compute and Storage Blocks for DL-Optimized\n  FPGAs", "comments": "8 pages, IEEE Signal Processing Society's ASILOMAR Conference on\n  Signals, Systems and Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The configurable building blocks of current FPGAs -- Logic blocks (LBs),\nDigital Signal Processing (DSP) slices, and Block RAMs (BRAMs) -- make them\nefficient hardware accelerators for the rapid-changing world of Deep Learning\n(DL). Communication between these blocks happens through an interconnect fabric\nconsisting of switching elements spread throughout the FPGA. In this paper, a\nnew block, Compute RAM, is proposed. Compute RAMs provide highly-parallel\nprocessing-in-memory (PIM) by combining computation and storage capabilities in\none block. Compute RAMs can be integrated in the FPGA fabric just like the\nexisting FPGA blocks and provide two modes of operation (storage or compute)\nthat can be dynamically chosen. They reduce power consumption by reducing data\nmovement, provide adaptable precision support, and increase the effective\non-chip memory bandwidth. Compute RAMs also help increase the compute density\nof FPGAs. In our evaluation of addition, multiplication and dot-product\noperations across multiple data precisions (int4, int8 and bfloat16), we\nobserve an average savings of 80% in energy consumption, and an improvement in\nexecution time ranging from 20% to 80%. Adding Compute RAMs can benefit non-DL\napplications as well, and make FPGAs more efficient, flexible, and performant\naccelerators.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 22:12:47 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Arora", "Aman", ""], ["Hanindhito", "Bagus", ""], ["John", "Lizy K.", ""]]}, {"id": "2107.09245", "submitter": "Evgeny Manzhosov", "authors": "Evgeny Manzhosov, Adam Hastings, Meghna Pancholi, Ryan Piersma,\n  Mohamed Tarek Ibn Ziad, Simha Sethumadhavan", "title": "MUSE: Multi-Use Error Correcting Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a new set of error correcting codes -- Multi-Use\nError Correcting Codes (MUSE ECC) -- that have the ability to match reliability\nguarantees of all commodity, conventional state-of-the-art ECC with fewer bits\nof storage. MUSE ECC derives its power by building on arithmetic coding methods\n(first used in an experimental system in 1960s). We show that our MUSE\nconstruction can be used as a \"drop in\" replacement within error correction\nframeworks used widely today. Further, we show how MUSE is a promising fit for\nemerging technologies such as a DDR5 memories. Concretely, all instantiations\nof MUSE we show in this paper offer 100% Single Error Correction, and multi-bit\nerror detection between 70% and 95% while using fewer check bits. MUSE ECC\ncorrects failure of a single chip on a DIMM with check bit space savings of\n12.5% compared to conventional techniques. The performance overheads, if any,\nare negligible. Our results open the possibility of reusing ECC storage for\nthings beyond reliability without compromising reliability, thus solving a\n40-year-old puzzle.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 03:35:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Manzhosov", "Evgeny", ""], ["Hastings", "Adam", ""], ["Pancholi", "Meghna", ""], ["Piersma", "Ryan", ""], ["Ziad", "Mohamed Tarek Ibn", ""], ["Sethumadhavan", "Simha", ""]]}, {"id": "2107.09333", "submitter": "Mahyar Emami", "authors": "Endri Bezati, Mahyar Emami, J\\\"orn Janneck, James Larus", "title": "StreamBlocks: A compiler for heterogeneous dataflow computing (technical\n  report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To increase performance and efficiency, systems use FPGAs as reconfigurable\naccelerators. A key challenge in designing these systems is partitioning\ncomputation between processors and an FPGA. An appropriate division of labor\nmay be difficult to predict in advance and require experiments and\nmeasurements. When an investigation requires rewriting part of the system in a\nnew language or with a new programming model, its high cost can retard the\nstudy of different configurations. A single-language system with an appropriate\nprogramming model and compiler that targets both platforms simplifies this\nexploration to a simple recompile with new compiler directives.\n  This work introduces StreamBlocks, an open-source compiler and runtime that\nuses the CAL dataflow programming language to partition computations across\nheterogeneous (CPU/accelerator) platforms. Because of the dataflow model's\nsemantics and the CAL language, StreamBlocks can exploit both thread\nparallelism in multi-core CPUs and the inherent parallelism of FPGAs.\nStreamBlocks supports exploring the design space with a profile-guided tool\nthat helps identify the best hardware-software partitions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 08:46:47 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bezati", "Endri", ""], ["Emami", "Mahyar", ""], ["Janneck", "J\u00f6rn", ""], ["Larus", "James", ""]]}, {"id": "2107.09366", "submitter": "Georgios Zervakis", "authors": "Ourania Spantidi, Georgios Zervakis, Iraklis Anagnostopoulos, Hussam\n  Amrouch, J\\\"org Henkel", "title": "Positive/Negative Approximate Multipliers for DNN Accelerators", "comments": "Accepted for publication at the 40th International Conference On\n  Computer Aided Design (ICCAD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Deep Neural Networks (DNNs) managed to deliver superhuman accuracy\nlevels on many AI tasks. Several applications rely more and more on DNNs to\ndeliver sophisticated services and DNN accelerators are becoming integral\ncomponents of modern systems-on-chips. DNNs perform millions of arithmetic\noperations per inference and DNN accelerators integrate thousands of\nmultiply-accumulate units leading to increased energy requirements. Approximate\ncomputing principles are employed to significantly lower the energy consumption\nof DNN accelerators at the cost of some accuracy loss. Nevertheless, recent\nresearch demonstrated that complex DNNs are increasingly sensitive to\napproximation. Hence, the obtained energy savings are often limited when\ntargeting tight accuracy constraints. In this work, we present a dynamically\nconfigurable approximate multiplier that supports three operation modes, i.e.,\nexact, positive error, and negative error. In addition, we propose a\nfilter-oriented approximation method to map the weights to the appropriate\nmodes of the approximate multiplier. Our mapping algorithm balances the\npositive with the negative errors due to the approximate multiplications,\naiming at maximizing the energy reduction while minimizing the overall\nconvolution error. We evaluate our approach on multiple DNNs and datasets\nagainst state-of-the-art approaches, where our method achieves 18.33% energy\ngains on average across 7 NNs on 4 different datasets for a maximum accuracy\ndrop of only 1%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:36:24 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Spantidi", "Ourania", ""], ["Zervakis", "Georgios", ""], ["Anagnostopoulos", "Iraklis", ""], ["Amrouch", "Hussam", ""], ["Henkel", "J\u00f6rg", ""]]}, {"id": "2107.09408", "submitter": "Marc Riera", "authors": "Marc Riera, Jose-Maria Arnau, Antonio Gonzalez", "title": "CREW: Computation Reuse and Efficient Weight Storage for\n  Hardware-accelerated MLPs and RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved tremendous success for cognitive\napplications. The core operation in a DNN is the dot product between quantized\ninputs and weights. Prior works exploit the weight/input repetition that arises\ndue to quantization to avoid redundant computations in Convolutional Neural\nNetworks (CNNs). However, in this paper we show that their effectiveness is\nseverely limited when applied to Fully-Connected (FC) layers, which are\ncommonly used in state-of-the-art DNNs, as it is the case of modern Recurrent\nNeural Networks (RNNs) and Transformer models.\n  To improve energy-efficiency of FC computation we present CREW, a hardware\naccelerator that implements Computation Reuse and an Efficient Weight Storage\nmechanism to exploit the large number of repeated weights in FC layers. CREW\nfirst performs the multiplications of the unique weights by their respective\ninputs and stores the results in an on-chip buffer. The storage requirements\nare modest due to the small number of unique weights and the relatively small\nsize of the input compared to convolutional layers. Next, CREW computes each\noutput by fetching and adding its required products. To this end, each weight\nis replaced offline by an index in the buffer of unique products. Indices are\ntypically smaller than the quantized weights, since the number of unique\nweights for each input tends to be much lower than the range of quantized\nweights, which reduces storage and memory bandwidth requirements.\n  Overall, CREW greatly reduces the number of multiplications and provides\nsignificant savings in model memory footprint and memory bandwidth usage. We\nevaluate CREW on a diverse set of modern DNNs. On average, CREW provides 2.61x\nspeedup and 2.42x energy savings over a TPU-like accelerator. Compared to UCNN,\na state-of-art computation reuse technique, CREW achieves 2.10x speedup and\n2.08x energy savings on average.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 11:10:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Riera", "Marc", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "2107.09448", "submitter": "Enrico Tabanelli", "authors": "Enrico Tabanelli, Giuseppe Tagliavini, Luca Benini", "title": "DNN is not all you need: Parallelizing Non-Neural ML Algorithms on\n  Ultra-Low-Power IoT Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) functions are becoming ubiquitous in latency- and\nprivacy-sensitive IoT applications, prompting for a shift toward near-sensor\nprocessing at the extreme edge and the consequent increasing adoption of\nParallel Ultra-Low Power (PULP) IoT processors. These compute- and\nmemory-constrained parallel architectures need to run efficiently a wide range\nof algorithms, including key Non-Neural ML kernels that compete favorably with\nDeep Neural Networks (DNNs) in terms of accuracy under severe resource\nconstraints. In this paper, we focus on enabling efficient parallel execution\nof Non-Neural ML algorithms on two RISCV-based PULP platforms, namely GAP8, a\ncommercial chip, and PULP-OPEN, a research platform running on an FPGA\nemulator. We optimized the parallel algorithms through a fine-grained analysis\nand intensive optimization to maximize the speedup, considering two alternative\nFloating-Point (FP) emulation libraries on GAP8 and the native FPU support on\nPULP-OPEN. Experimental results show that a target-optimized emulation library\ncan lead to an average 1.61x runtime improvement compared to a standard\nemulation library, while the native FPU support reaches up to 32.09x. In terms\nof parallel speedup, our design improves the sequential execution by 7.04x on\naverage on the targeted octa-core platforms. Lastly, we present a comparison\nwith the ARM Cortex-M4 microcontroller (MCU), a widely adopted commercial\nsolution for edge deployments, which is 12.87$x slower than PULP-OPEN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:42:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tabanelli", "Enrico", ""], ["Tagliavini", "Giuseppe", ""], ["Benini", "Luca", ""]]}, {"id": "2107.09500", "submitter": "Kaining Zhou", "authors": "Kaining Zhou, Yangshuo He, Rui Xiao and Kejie Huang", "title": "Domino: A Tailored Network-on-Chip Architecture to Enable Highly\n  Localized Inter- and Intra-Memory DNN Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ever-increasing computation complexity of fast-growing Deep Neural\nNetworks (DNNs) has requested new computing paradigms to overcome the memory\nwall in conventional Von Neumann computing architectures. The emerging\nComputing-In-Memory (CIM) architecture has been a promising candidate to\naccelerate neural network computing. However, the data movement between CIM\narrays may still dominate the total power consumption in conventional designs.\nThis paper proposes a flexible CIM processor architecture named Domino to\nenable stream computing and local data access to significantly reduce the data\nmovement energy. Meanwhile, Domino employs tailored distributed instruction\nscheduling within Network-on-Chip (NoC) to implement inter-memory-computing and\nattain mapping flexibility. The evaluation with prevailing CNN models shows\nthat Domino achieves 1.15-to-9.49$\\times$ power efficiency over several\nstate-of-the-art CIM accelerators and improves the throughput by\n1.57-to-12.96$\\times$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 16:19:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhou", "Kaining", ""], ["He", "Yangshuo", ""], ["Xiao", "Rui", ""], ["Huang", "Kejie", ""]]}, {"id": "2107.09804", "submitter": "Mohammad Samavatian", "authors": "Saikat Majumdar, Mohammad Hossein Samavatian, Kristin Barber, Radu\n  Teodorescu", "title": "Using Undervolting as an On-Device Defense Against Adversarial Machine\n  Learning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural network (DNN) classifiers are powerful tools that drive a broad\nspectrum of important applications, from image recognition to autonomous\nvehicles. Unfortunately, DNNs are known to be vulnerable to adversarial attacks\nthat affect virtually all state-of-the-art models. These attacks make small\nimperceptible modifications to inputs that are sufficient to induce the DNNs to\nproduce the wrong classification.\n  In this paper we propose a novel, lightweight adversarial correction and/or\ndetection mechanism for image classifiers that relies on undervolting (running\na chip at a voltage that is slightly below its safe margin). We propose using\ncontrolled undervolting of the chip running the inference process in order to\nintroduce a limited number of compute errors. We show that these errors disrupt\nthe adversarial input in a way that can be used either to correct the\nclassification or detect the input as adversarial. We evaluate the proposed\nsolution in an FPGA design and through software simulation. We evaluate 10\nattacks on two popular DNNs and show an average detection rate of 80% to 95%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 23:21:04 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Majumdar", "Saikat", ""], ["Samavatian", "Mohammad Hossein", ""], ["Barber", "Kristin", ""], ["Teodorescu", "Radu", ""]]}, {"id": "2107.09833", "submitter": "Md Hafizul Islam Chowdhuryy", "authors": "Md Hafizul Islam Chowdhuryy, Fan Yao", "title": "Leaking Secrets through Modern Branch Predictor in the Speculative World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient execution attacks that exploit speculation have raised significant\nconcerns in computer systems. Typically, branch predictors are leveraged to\ntrigger mis-speculation in transient execution attacks. In this work, we\ndemonstrate a new class of speculation-based attack that targets branch\nprediction unit (BPU). We find that speculative resolution of conditional\nbranches (i.e., in nested speculation) alter the states of pattern history\ntable (PHT) in modern processors, which are not restored after the\ncorresponding branches are later squashed. Such characteristic allows attackers\nto exploit BPU as the secret transmitting medium in transient execution\nattacks. To evaluate the discovered vulnerability, we build a novel attack\nframework, BranchSpectre, that enables exfiltration of unintended secrets\nthrough observing speculative PHT updates (in the form of covert and side\nchannels). We further investigate PHT collision mechanism in the history-based\npredictor as well as the branch prediction mode transitions in Intel\nprocessors. Built upon such knowledge, we implement an ultra high-speed covert\nchannel (BranchSpectre-cc) as well as two side channels (i.e., BranchSpectre-v1\nand BranchSpectre-v2) that merely rely on BPU for mis-speculation trigger and\nsecret inference in the speculative domain. Notably, BranchSpectre side\nchannels can take advantage of much simpler code patterns than the ones used in\nSpectre attacks. We present an extensive BranchSpectre code gadget analysis on\na set of popular real-world application code bases followed by a demonstration\nof real-world side channel attack on OpenSSL. The evaluation results show\nsubstantial wider existence and higher exploitability of BranchSpectre code\npatterns in real-world software. Finally, we discuss several secure branch\nprediction mechanisms that can mitigate transient execution attacks exploiting\nmodern branch predictors.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:01:12 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chowdhuryy", "Md Hafizul Islam", ""], ["Yao", "Fan", ""]]}, {"id": "2107.10299", "submitter": "Onel Luis Alcaraz Lopez", "authors": "Onel Luis Alcaraz L\\'opez, Bruno Clerckx, Matti Latva-aho", "title": "Dynamic RF Combining for Multi-Antenna Ambient Energy Harvesting", "comments": "5 pags, 5 figs, submitted to IEEE Wireless Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambient radio frequency (RF) energy harvesting (EH) technology is key to\nrealize self-sustainable, always-on, low-power, massive Internet of Things\nnetworks. Typically, rigid (non-adaptable to channel fluctuations)\nmulti-antenna receive architectures are proposed to support reliable EH\noperation. Herein, we introduce a dynamic RF combining architecture for ambient\nRF EH use cases, and exemplify the attainable performance gains via three\nsimple mechanisms, namely, brute force (BF), sequential testing (ST) and\ncodebook based (CB). Among the proposed mechanisms, BF demands the highest\npower consumption, while CB requires the highest-resolution phase shifters,\nthus tipping the scales in favor of ST. Finally, we show that the performance\ngains of ST over a rigid RF combining scheme increase with the number of\nreceive antennas and energy transmitters' deployment density.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:22:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["L\u00f3pez", "Onel Luis Alcaraz", ""], ["Clerckx", "Bruno", ""], ["Latva-aho", "Matti", ""]]}, {"id": "2107.10308", "submitter": "Orian Leitersdorf", "authors": "Ronny Ronen, Adi Eliahu, Orian Leitersdorf, Natan Peled, Kunal\n  Korgaonkar, Anupam Chattopadhyay, Ben Perach, Shahar Kvatinsky", "title": "The Bitlet Model: A Parameterized Analytical Model to Compare PIM and\n  CPU Systems", "comments": "Accepted to ACM JETC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, data-intensive applications are gaining popularity and, together\nwith this trend, processing-in-memory (PIM)-based systems are being given more\nattention and have become more relevant. This paper describes an analytical\nmodeling tool called Bitlet that can be used, in a parameterized fashion, to\nestimate the performance and the power/energy of a PIM-based system and thereby\nassess the affinity of workloads for PIM as opposed to traditional computing.\nThe tool uncovers interesting tradeoffs between, mainly, the PIM computation\ncomplexity (cycles required to perform a computation through PIM), the amount\nof memory used for PIM, the system memory bandwidth, and the data transfer\nsize. Despite its simplicity, the model reveals new insights when applied to\nreal-life examples. The model is demonstrated for several synthetic examples\nand then applied to explore the influence of different parameters on two\nsystems - IMAGING and FloatPIM. Based on the demonstrations, insights about PIM\nand its combination with CPU are concluded.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 18:53:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ronen", "Ronny", ""], ["Eliahu", "Adi", ""], ["Leitersdorf", "Orian", ""], ["Peled", "Natan", ""], ["Korgaonkar", "Kunal", ""], ["Chattopadhyay", "Anupam", ""], ["Perach", "Ben", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "2107.10845", "submitter": "Hanrui Wang", "authors": "Hanrui Wang and Yongshan Ding and Jiaqi Gu and Yujun Lin and David Z.\n  Pan and Frederic T. Chong and Song Han", "title": "QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits", "comments": "15 pages, 23 figures. Code available at\n  https://github.com/mit-han-lab/pytorch-quantum", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)\ncomputers. Limited research efforts have explored a higher level of\noptimization by making the quantum circuit resilient to noise. We propose and\nexperimentally implement QuantumNAS, the first comprehensive framework for\nnoise-adaptive co-search of variational circuit and qubit mapping. Variational\nquantum circuits are a promising approach for constructing quantum neural\nnetworks for machine learning and variational ansatzes for quantum simulation.\nHowever, finding the best variational circuit and its optimal parameters is\nchallenging in a high-dimensional Hilbert space. We propose to decouple the\nparameter training and circuit search by introducing a novel gate-sharing\nSuperCircuit. The SuperCircuit is trained by sampling and updating the\nSubCircuits in it and provides an accurate estimation of SubCircuit performance\ntrained from scratch. Then we perform an evolutionary co-search of SubCircuit\nand its qubit mapping. The SubCircuit performance is estimated with parameters\ninherited from SuperCircuit and simulated with real device noise models.\nFinally, we perform iterative gate pruning and finetuning to further remove the\nredundant gates in a fine-grained manner.\n  Extensively evaluated with 12 QML and VQE benchmarks on 10 quantum computers,\nQuantumNAS significantly outperforms noise-unaware search, human and random\nbaselines. For QML tasks, QuantumNAS is the first to demonstrate over 95%\n2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum\ncomputers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O,\nLiH, CH4, BeH2 compared with UCCSD baselines. We also open-source QuantumEngine\n(https://github.com/mit-han-lab/pytorch-quantum) for fast training of\nparameterized quantum circuits to facilitate future research.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:58:13 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wang", "Hanrui", ""], ["Ding", "Yongshan", ""], ["Gu", "Jiaqi", ""], ["Lin", "Yujun", ""], ["Pan", "David Z.", ""], ["Chong", "Frederic T.", ""], ["Han", "Song", ""]]}, {"id": "2107.11089", "submitter": "Oscar Casta\\~neda", "authors": "Oscar Casta\\~neda, Zachariah Boynton, Seyed Hadi Mirfarshbafan, Shimin\n  Huang, Jamie C. Ye, Alyosha Molnar, Christoph Studer", "title": "A Resolution-Adaptive 8 mm$^\\text{2}$ 9.98 Gb/s 39.7 pJ/b 32-Antenna\n  All-Digital Spatial Equalizer for mmWave Massive MU-MIMO in 65nm CMOS", "comments": "To be presented at the IEEE European Solid-State Circuits Conference\n  (ESSCIRC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-digital millimeter-wave (mmWave) massive multi-user multiple-input\nmultiple-output (MU-MIMO) receivers enable extreme data rates but require high\npower consumption. In order to reduce power consumption, this paper presents\nthe first resolution-adaptive all-digital receiver ASIC that is able to adjust\nthe resolution of the data-converters and baseband-processing engine to the\ninstantaneous communication scenario. The scalable 32-antenna, 65 nm CMOS\nreceiver occupies a total area of 8 mm$^\\text{2}$ and integrates\nanalog-to-digital converters (ADCs) with programmable gain and resolution,\nbeamspace channel estimation, and a resolution-adaptive processing-in-memory\nspatial equalizer. With 6-bit ADC samples and a 4-bit spatial equalizer, our\nASIC achieves a throughput of 9.98 Gb/s while being at least 2x more\nenergy-efficient than state-of-the-art designs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:03:45 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Casta\u00f1eda", "Oscar", ""], ["Boynton", "Zachariah", ""], ["Mirfarshbafan", "Seyed Hadi", ""], ["Huang", "Shimin", ""], ["Ye", "Jamie C.", ""], ["Molnar", "Alyosha", ""], ["Studer", "Christoph", ""]]}, {"id": "2107.11336", "submitter": "Trevor E. Carlson", "authors": "Yun Chen, Ali Hajiabadi, Romain Poussier, Andreas Diavastos, Shivam\n  Bhasin, Trevor E. Carlson", "title": "Mitigating Power Attacks through Fine-Grained Instruction Reordering", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side-channel attacks are a security exploit that take advantage of\ninformation leakage. They use measurement and analysis of physical parameters\nto reverse engineer and extract secrets from a system. Power analysis attacks\nin particular, collect a set of power traces from a computing device and use\nstatistical techniques to correlate this information with the attacked\napplication data and source code. Counter measures like just-in-time\ncompilation, random code injection and instruction descheduling obfuscate the\nexecution of instructions to reduce the security risk. Unfortunately, due to\nthe randomness and excess instructions executed by these solutions, they\nintroduce large overheads in performance, power and area.\n  In this work we propose a scheduling algorithm that dynamically reorders\ninstructions in an out-of-order processor to provide obfuscated execution and\nmitigate power analysis attacks with little-to-no effect on the performance,\npower or area of the processor. We exploit the time between operand\navailability of critical instructions (slack) to create high-performance random\nschedules without requiring additional instructions or static prescheduling.\nFurther, we perform an extended security analysis using different attacks. We\nhighlight the dangers of using incorrect adversarial assumptions, which can\noften lead to a false sense of security. In that regard, our advanced security\nmetric demonstrates improvements of 34$\\times$, while our basic security\nevaluation shows results up to 261$\\times$. Moreover, our system achieves\nperformance within 96% on average, of the baseline unprotected processor.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:27:01 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Chen", "Yun", ""], ["Hajiabadi", "Ali", ""], ["Poussier", "Romain", ""], ["Diavastos", "Andreas", ""], ["Bhasin", "Shivam", ""], ["Carlson", "Trevor E.", ""]]}, {"id": "2107.11417", "submitter": "Biswadip Maity", "authors": "Tiago M\\\"uck, Bryan Donyanavard, Biswadip Maity, Kasra Moazzemi, Nikil\n  Dutt", "title": "MARS: Middleware for Adaptive Reflective Computer Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-adaptive approaches for runtime resource management of manycore\ncomputing platforms often require a runtime model of the system that represents\nthe software organization or the architecture of the target platform. The\nincreasing heterogeneity in a platform's resource types and the interactions\nbetween resources pose challenges for coordinated model-based decision making\nin the face of dynamic workloads. Self-awareness properties address these\nchallenges for emerging heterogeneous manycore processing (HMP) platforms\nthrough reflective resource managers. However, with HMP computing platform\narchitectures evolving rapidly, porting the self-aware decision logic across\ndifferent hardware platforms is challenging, requiring resource managers to\nupdate their models and platform-specific interfaces. We propose MARS\n(Middleware for Adaptive and Reflective Systems), a cross-layer and\nmulti-platform framework that allows users to easily create resource managers\nby composing system models and resource management policies in a flexible and\ncoordinated manner. MARS consists of a generic user-level sensing/actuation\ninterface that allows for portable policy design, and a reflective system model\nused to coordinate multiple policies. We demonstrate MARS' interaction across\nmultiple layers of the system stack through a dynamic voltage and frequency\nscaling (DVFS) policy example which can run on any Linux-based HMP computing\nplatform.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:58:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["M\u00fcck", "Tiago", ""], ["Donyanavard", "Bryan", ""], ["Maity", "Biswadip", ""], ["Moazzemi", "Kasra", ""], ["Dutt", "Nikil", ""]]}, {"id": "2107.11516", "submitter": "Aditya Narayan", "authors": "Aditya Narayan, Yvain Thonnart, Pascal Vivet, Ayse K. Coskun and Ajay\n  Joshi", "title": "Architecting Optically-Controlled Phase Change Memory", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase Change Memory (PCM) is an attractive candidate for main memory as it\noffers non-volatility and zero leakage power, while providing higher cell\ndensities, longer data retention time, and higher capacity scaling compared to\nDRAM. In PCM, data is stored in the crystalline or amorphous state of the phase\nchange material. The typical electrically-controlled PCM (EPCM), however,\nsuffers from longer write latency and higher write energy compared to DRAM and\nlimited multi-level cell (MLC) capacities. These challenges limit the\nperformance of data-intensive applications running on computing systems with\nEPCMs.\n  Recently, researchers demonstrated optically-controlled PCM (OPCM) cells,\nwith support for 5 bits/cell in contrast to 2 bits/cell in EPCM. These OPCM\ncells can be accessed directly with optical signals that are multiplexed in\nhigh-bandwidth-density silicon-photonic links. The higher MLC capacity in OPCM\nand the direct cell access using optical signals enable an increased read/write\nthroughput and lower energy per access than EPCM. However, due to the direct\ncell access using optical signals, OPCM systems cannot be designed using\nconventional memory architecture. We need a complete redesign of the memory\narchitecture that is tailored to the properties of OPCM technology.\n  This paper presents the design of a unified network and main memory system\ncalled COSMOS that combines OPCM and silicon-photonic links to achieve high\nmemory throughput. COSMOS is composed of a hierarchical multi-banked OPCM array\nwith novel read and write access protocols, and uses an\nElectrical-Optical-Electrical (E-O-E) control unit to interface with the\nprocessor. Our evaluation of a 2.5D-integrated system containing a processor\nand COSMOS demonstrates 2.14x average speedup compared to an EPCM system.\nCOSMOS consumes 3.8x lower read energy-per-bit and 5.97x lower write\nenergy-per-bit compared to EPCM.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 02:56:18 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Narayan", "Aditya", ""], ["Thonnart", "Yvain", ""], ["Vivet", "Pascal", ""], ["Coskun", "Ayse K.", ""], ["Joshi", "Ajay", ""]]}, {"id": "2107.11673", "submitter": "Hanchen Ye", "authors": "Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack Huang, Stephen\n  Neuendorffer, Deming Chen", "title": "ScaleHLS: Scalable High-Level Synthesis through MLIR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-level Synthesis (HLS) has been widely adopted as it significantly\nimproves the hardware design productivity and enables efficient design space\nexploration (DSE). HLS tools can be used to deliver solutions for many\ndifferent kinds of design problems, which are often better solved with\ndifferent levels of abstraction. While existing HLS tools are built using\ncompiler infrastructures largely based on a single-level abstraction (e.g.,\nLLVM), we propose ScaleHLS, a next-generation HLS compilation flow, on top of a\nmulti-level compiler infrastructure called MLIR, for the first time. By using\nan intermediate representation (IR) that can be better tuned to particular\nalgorithms at different representation levels, we are able to build this new\nHLS tool that is more scalable and customizable towards various applications\ncoming with intrinsic structural or functional hierarchies. ScaleHLS is able to\nrepresent and optimize HLS designs at multiple levels of abstraction and\nprovides an HLS-dedicated transform and analysis library to solve the\noptimization problems at the suitable representation levels. On top of the\nlibrary, we also build an automated DSE engine to explore the multi-dimensional\ndesign space efficiently. In addition, we develop an HLS C front-end and a\nC/C++ emission back-end to translate HLS designs into/from MLIR for enabling\nthe end-to-end ScaleHLS flow. Experimental results show that, comparing to the\nbaseline designs only optimized by Xilinx Vivado HLS, ScaleHLS improves the\nperformances with amazing quality-of-results -- up to 768.1x better on\ncomputation kernel level programs and up to 3825.0x better on neural network\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 19:20:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ye", "Hanchen", ""], ["Hao", "Cong", ""], ["Cheng", "Jianyi", ""], ["Jeong", "Hyunmin", ""], ["Huang", "Jack", ""], ["Neuendorffer", "Stephen", ""], ["Chen", "Deming", ""]]}, {"id": "2107.11723", "submitter": "Sumon Bose Mr.", "authors": "Sumon Kumar Bose, Deepak Singla, and Arindam Basu", "title": "A 51.3 TOPS/W, 134.4 GOPS In-memory Binary Image Filtering in 65nm CMOS", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/JSSC.2021.3098539", "report-no": null, "categories": "eess.IV cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic vision sensors (NVS) can enable energy savings due to their\nevent-driven that exploits the temporal redundancy in video streams from a\nstationary camera. However, noise-driven events lead to the false triggering of\nthe object recognition processor. Image denoise operations require\nmemoryintensive processing leading to a bottleneck in energy and latency. In\nthis paper, we present in-memory filtering (IMF), a 6TSRAM in-memory computing\nbased image denoising for eventbased binary image (EBBI) frame from an NVS. We\npropose a non-overlap median filter (NOMF) for image denoising. An inmemory\ncomputing framework enables hardware implementation of NOMF leveraging the\ninherent read disturb phenomenon of 6T-SRAM. To demonstrate the energy-saving\nand effectiveness of the algorithm, we fabricated the proposed architecture in\na 65nm CMOS process. As compared to fully digital implementation, IMF enables >\n70x energy savings and a > 3x improvement of processing time when tested with\nthe video recordings from a DAVIS sensor and achieves a peak throughput of\n134.4 GOPS. Furthermore, the peak energy efficiencies of the NOMF is 51.3\nTOPS/W, comparable with state of the art inmemory processors. We also show that\nthe accuracy of the images obtained by NOMF provide comparable accuracy in\ntracking and classification applications when compared with images obtained by\nconventional median filtering.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 04:33:11 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 05:11:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Bose", "Sumon Kumar", ""], ["Singla", "Deepak", ""], ["Basu", "Arindam", ""]]}, {"id": "2107.11814", "submitter": "Laurent Daudet", "authors": "Charles Brossollet, Alessandro Cappelli, Igor Carron, Charidimos\n  Chaintoutis, Am\\'elie Chatelain, Laurent Daudet, Sylvain Gigan, Daniel\n  Hesslow, Florent Krzakala, Julien Launay, Safa Mokaadi, Fabien Moreau, Kilian\n  M\\\"uller, Ruben Ohana, Gustave Pariente, Iacopo Poli, Giuseppe L. Tommasone", "title": "LightOn Optical Processing Unit: Scaling-up AI and HPC with a Non von\n  Neumann co-processor", "comments": "Proceedings IEEE Hot Chips 33, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LightOn's Optical Processing Unit (OPU), the first photonic AI\naccelerator chip available on the market for at-scale Non von Neumann\ncomputations, reaching 1500 TeraOPS. It relies on a combination of free-space\noptics with off-the-shelf components, together with a software API allowing a\nseamless integration within Python-based processing pipelines. We discuss a\nvariety of use cases and hybrid network architectures, with the OPU used in\ncombination of CPU/GPU, and draw a pathway towards \"optical advantage\".\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:27:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Brossollet", "Charles", ""], ["Cappelli", "Alessandro", ""], ["Carron", "Igor", ""], ["Chaintoutis", "Charidimos", ""], ["Chatelain", "Am\u00e9lie", ""], ["Daudet", "Laurent", ""], ["Gigan", "Sylvain", ""], ["Hesslow", "Daniel", ""], ["Krzakala", "Florent", ""], ["Launay", "Julien", ""], ["Mokaadi", "Safa", ""], ["Moreau", "Fabien", ""], ["M\u00fcller", "Kilian", ""], ["Ohana", "Ruben", ""], ["Pariente", "Gustave", ""], ["Poli", "Iacopo", ""], ["Tommasone", "Giuseppe L.", ""]]}, {"id": "2107.11881", "submitter": "Reza Faghih Mirzaee", "authors": "Fereshteh Karimi, Reza Faghih Mirzaee, Ali Fakeri-Tabrizi, Arman Roohi", "title": "Ultra-Fast, High-Performance 8x8 Approximate Multipliers by a New\n  Multicolumn 3,3:2 Inexact Compressor and its Derivatives", "comments": "25 Pages, 18 Figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiplier, which has a key role in many different applications, is a\ntime-consuming, energy-intensive computation block. Approximate computing is a\npractical design paradigm that attempts to improve hardware efficacy while\nkeeping computation quality satisfactory. A novel multicolumn 3,3:2 inexact\ncompressor is presented in this paper. It takes three partial products from two\nadjacent columns each for rapid partial product reduction. The proposed inexact\ncompressor and its derivates enable us to design a high-speed approximate\nmultiplier. Then, another ultra-fast, high-efficient approximate multiplier is\nachieved by means a systematic truncation strategy. The proposed multipliers\naccumulate partial products in only two stages, one fewer stage than other\napproximate multipliers in the literature. Implementation results by Synopsys\nDesign Compiler and 45 nm technology node demonstrates nearly 11.11% higher\nspeed for the second proposed design over the fastest existing approximate\nmultiplier. Furthermore, the new approximate multipliers are applied to the\nimage processing application of image sharpening. Their performance in this\napplication is highly satisfactory. It is shown in this paper that the error\npattern of an approximate multiplier, in addition to the mean error distance\nand error rate, has a direct effect on the outcomes of the image processing\napplication.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 20:12:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Karimi", "Fereshteh", ""], ["Mirzaee", "Reza Faghih", ""], ["Fakeri-Tabrizi", "Ali", ""], ["Roohi", "Arman", ""]]}, {"id": "2107.12679", "submitter": "Mingbo Zhao", "authors": "Wenlong Cheng and Mingbo Zhao and Zhiling Ye and Shuhang Gu", "title": "MFAGAN: A Compression Framework for Memory-Efficient On-Device\n  Super-Resolution GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generative adversarial networks (GANs) have promoted remarkable advances in\nsingle-image super-resolution (SR) by recovering photo-realistic images.\nHowever, high memory consumption of GAN-based SR (usually generators) causes\nperformance degradation and more energy consumption, hindering the deployment\nof GAN-based SR into resource-constricted mobile devices. In this paper, we\npropose a novel compression framework \\textbf{M}ulti-scale \\textbf{F}eature\n\\textbf{A}ggregation Net based \\textbf{GAN} (MFAGAN) for reducing the memory\naccess cost of the generator. First, to overcome the memory explosion of dense\nconnections, we utilize a memory-efficient multi-scale feature aggregation net\nas the generator. Second, for faster and more stable training, our method\nintroduces the PatchGAN discriminator. Third, to balance the student\ndiscriminator and the compressed generator, we distill both the generator and\nthe discriminator. Finally, we perform a hardware-aware neural architecture\nsearch (NAS) to find a specialized SubGenerator for the target mobile phone.\nBenefiting from these improvements, the proposed MFAGAN achieves up to\n\\textbf{8.3}$\\times$ memory saving and \\textbf{42.9}$\\times$ computation\nreduction, with only minor visual quality degradation, compared with ESRGAN.\nEmpirical studies also show $\\sim$\\textbf{70} milliseconds latency on Qualcomm\nSnapdragon 865 chipset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 09:04:30 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Cheng", "Wenlong", ""], ["Zhao", "Mingbo", ""], ["Ye", "Zhiling", ""], ["Gu", "Shuhang", ""]]}, {"id": "2107.12824", "submitter": "Hiroki Matsutani", "authors": "Hiroki Kawakami, Hirohisa Watanabe, Keisuke Sugiura, Hiroki Matsutani", "title": "A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge\n  Domain Adaptation on FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although high-performance deep neural networks are in high demand in edge\nenvironments, computation resources are strictly limited in edge devices, and\nlight-weight neural network techniques, such as Depthwise Separable Convolution\n(DSC), have been developed. ResNet is one of conventional deep neural network\nmodels that stack a lot of layers and parameters for a higher accuracy. To\nreduce the parameter size of ResNet, by utilizing a similarity to ODE (Ordinary\nDifferential Equation), Neural ODE repeatedly uses most of weight parameters\ninstead of having a lot of different parameters. Thus, Neural ODE becomes\nsignificantly small compared to that of ResNet so that it can be implemented in\nresource-limited edge devices. In this paper, a combination of Neural ODE and\nDSC, called dsODENet, is designed and implemented for FPGAs (Field-Programmable\nGate Arrays). dsODENet is then applied to edge domain adaptation as a practical\nuse case and evaluated with image classification datasets. It is implemented on\nXilinx ZCU104 board and evaluated in terms of domain adaptation accuracy,\ntraining speed, FPGA resource utilization, and speedup rate compared to a\nsoftware execution. The results demonstrate that dsODENet is comparable to or\nslightly better than our baseline Neural ODE implementation in terms of domain\nadaptation accuracy, while the total parameter size without pre- and\npost-processing layers is reduced by 54.2% to 79.8%. The FPGA implementation\naccelerates the prediction tasks by 27.9 times faster than a software\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:44:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kawakami", "Hiroki", ""], ["Watanabe", "Hirohisa", ""], ["Sugiura", "Keisuke", ""], ["Matsutani", "Hiroki", ""]]}, {"id": "2107.12922", "submitter": "Jong Hoon Shin", "authors": "Jong Hoon Shin, Ali Shafiee, Ardavan Pedram, Hamzah Abdel-Aziz, Ling\n  Li, and Joseph Hassoun", "title": "Design Space Exploration of Sparse Accelerators for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel architectures for deep learning exploit both activation and weight\nsparsity to improve the performance of DNN inference. However, this speedup\nusually brings non-negligible overheads which diminish the efficiency of such\ndesigns when running dense models. These overheads specifically are exacerbated\nfor low precision accelerators with optimized SRAM size per core. This paper\nexamines the design space trade-offs of such accelerators aiming to achieve\ncompetitive performance and efficiency metrics for all four combinations of\ndense or sparse activation/weight tensors. To do so, we systematically examine\noverheads of supporting sparsity on top of an optimized dense core. These\noverheads are modeled based on parameters that indicate how a multiplier can\nborrow a nonzero operation from the neighboring multipliers or future cycles.\nAs a result of this exploration, we identify a few promising designs that\nperform better than prior work. Our findings suggest that even a best design\ntargeting dual sparsity yields 20%-30% drop in power efficiency when performing\non single sparse models, i.e., those with only sparse weight or sparse\nactivation tensors. We introduce novel techniques to reuse resources of the\nsame core to maintain high performance and efficiency when running single\nsparsity or dense models. We call this hybrid design Griffin. Griffin is 1.2,\n3.0, 3.1, and 1.4X more power efficient than state-of-the-art sparse\narchitectures, for dense, weight-only sparse, activation-only sparse, and dual\nsparse models, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:26:36 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Shin", "Jong Hoon", ""], ["Shafiee", "Ali", ""], ["Pedram", "Ardavan", ""], ["Abdel-Aziz", "Hamzah", ""], ["Li", "Ling", ""], ["Hassoun", "Joseph", ""]]}, {"id": "2107.13386", "submitter": "Santosh Nagarakatte", "authors": "Mohammadreza Soltaniyeh, Richard P. Martin, Santosh Nagarakatte", "title": "SPOTS: An Accelerator for Sparse CNNs Leveraging General Matrix-Matrix\n  Multiplication", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers Department of Computer Science Technical Report DCS-TR-756", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new hardware accelerator for sparse convolutional\nneural networks (CNNs) by building a hardware unit to perform the Image to\nColumn (IM2COL) transformation of the input feature map coupled with a systolic\narray-based general matrix-matrix multiplication (GEMM) unit. Our design\ncarefully overlaps the IM2COL transformation with the GEMM computation to\nmaximize parallelism. We propose a novel design for the IM2COL unit that uses a\nset of distributed local memories connected by a ring network, which improves\nenergy efficiency and latency by streaming the input feature map only once. We\npropose a tall systolic array for the GEMM unit while also providing the\nability to organize it as multiple small GEMM units, which enables our design\nto handle a wide range of CNNs and their parameters. Further, our design\nimproves performance by effectively mapping the sparse data to the hardware\nunits by utilizing sparsity in both input feature maps and weights. Our\nprototype, SPOTS, is on average 1.74X faster than Eyeriss. It is also 78X, and\n12X more energy-efficient when compared to CPU and GPU implementations,\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:26:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Soltaniyeh", "Mohammadreza", ""], ["Martin", "Richard P.", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "2107.13649", "submitter": "Tejas Shah", "authors": "Tejas Shah, Bobbi Yogatama, Kyle Roarty, Rami Dahman", "title": "Reuse Cache for Heterogeneous CPU-GPU Systems", "comments": "5 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is generally observed that the fraction of live lines in shared last-level\ncaches (SLLC) is very small for chip multiprocessors (CMPs). This can be\ntackled using promotion-based replacement policies like re-reference interval\nprediction (RRIP) instead of LRU, dead-block predictors, or reuse-based cache\nallocation schemes. In GPU systems, similar LLC issues are alleviated using\nvarious cache bypassing techniques. These issues are worsened in heterogeneous\nCPU-GPU systems because the two processors have different data access patterns\nand frequencies. GPUs generally work on streaming data, but have many more\nthreads accessing memory as compared to CPUs. As such, most traditional cache\nreplacement and allocation policies prove ineffective due to the higher number\nof cache accesses in GPU applications, resulting in higher allocation for GPU\ncache lines, despite their minimal reuse. In this work, we implement the Reuse\nCache approach for heterogeneous CPU-GPU systems. The reuse cache is a\ndecoupled tag/data SLLC which is designed to only store the data that is being\naccessed more than once. This design is based on the observation that most of\nthe cache lines in the LLC are stored but do not get reused before being\nreplaced. We find that the reuse cache achieves within 0.5% of the IPC gains of\na statically partitioned LLC, while decreasing the area cost of the LLC by an\naverage of 40%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 21:37:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shah", "Tejas", ""], ["Yogatama", "Bobbi", ""], ["Roarty", "Kyle", ""], ["Dahman", "Rami", ""]]}]