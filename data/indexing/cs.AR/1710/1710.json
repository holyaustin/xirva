[{"id": "1710.04347", "submitter": "Duckhwan Kim", "authors": "Duckhwan Kim, Taesik Na, Sudhakar Yalamanchili, and Saibal\n  Mukhopadhyay", "title": "NeuroTrainer: An Intelligent Memory Module for Deep Learning Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents, NeuroTrainer, an intelligent memory module with\nin-memory accelerators that forms the building block of a scalable architecture\nfor energy efficient training for deep neural networks. The proposed\narchitecture is based on integration of a homogeneous computing substrate\ncomposed of multiple processing engines in the logic layer of a 3D memory\nmodule. NeuroTrainer utilizes a programmable data flow based execution model to\noptimize memory mapping and data re-use during different phases of training\noperation. A programming model and supporting architecture utilizes the\nflexible data flow to efficiently accelerate training of various types of DNNs.\nThe cycle level simulation and synthesized design in 15nm FinFET showspower\nefficiency of 500 GFLOPS/W, and almost similar throughput for a wide range of\nDNNs including convolutional, recurrent, multi-layer-perceptron, and mixed\n(CNN+RNN) networks\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 02:56:37 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kim", "Duckhwan", ""], ["Na", "Taesik", ""], ["Yalamanchili", "Sudhakar", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1710.05027", "submitter": "Shadrokh Samavi", "authors": "Eman Alibeigi, Shadrokh Samavi, Shahram Shirani, Zahra Rahmani", "title": "Real time ridge orientation estimation for fingerprint images", "comments": "8 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint verification is an important bio-metric technique for personal\nidentification. Most of the automatic verification systems are based on\nmatching of fingerprint minutiae. Extraction of minutiae is an essential\nprocess which requires estimation of orientation of the lines in an image. Most\nof the existing methods involve intense mathematical computations and hence are\nperformed through software means. In this paper a hardware scheme to perform\nreal time orientation estimation is presented which is based on pipelined\narchitecture. Synthesized circuits proved the functionality and accuracy of the\nsuggested method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 15:23:18 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Alibeigi", "Eman", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""], ["Rahmani", "Zahra", ""]]}, {"id": "1710.05154", "submitter": "Abdullah Al-Dujaili", "authors": "Abdullah Al-Dujaili, Suhaib A. Fahmy", "title": "High Throughput 2D Spatial Image Filters on FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs are well established in the signal processing domain, where their\nfine-grained programmable nature allows the inherent parallelism in these\napplications to be exploited for enhanced performance. As architectures have\nevolved, FPGA vendors have added more heterogeneous resources to allow\noften-used functions to be implemented with higher performance, at lower power\nand using less area. DSP blocks, for example, have evolved from basic\nmultipliers to support the multiply-accumulate operations that are the core of\nmany signal processing tasks. While more features were added to DSP blocks,\ntheir structure and connectivity has been optimised primarily for\none-dimensional signal processing. Basic operations in image processing are\nsimilar, but performed in a two-dimensional structure, and hence, many of the\noptimisations in newer DSP blocks are not exploited when mapping image\nprocessing algorithms to them. We present a detailed study of two-dimensional\nspatial filter implementation on FPGAs, showing how to maximise performance\nthrough exploitation of DSP block capabilities, while also presenting a lean\nborder pixel management policy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 09:17:04 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 15:08:39 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Al-Dujaili", "Abdullah", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "1710.05470", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, C Dang, D L Maskell, K Prasad", "title": "Asynchronous Early Output Section-Carry Based Carry Lookahead Adder with\n  Alias Carry Logic", "comments": null, "journal-ref": "P. Balasubramanian, C. Dang, D.L. Maskell, K. Prasad,\n  \"Asynchronous Early Output Section-Carry Based Carry Lookahead Adder with\n  Alias Carry Logic,\" Proc. 30th International Conference on Microelectronics,\n  pp. 293-298, 2017, Serbia", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new asynchronous early output section-carry based carry lookahead adder\n(SCBCLA) with alias carry output logic is presented in this paper. To evaluate\nthe proposed SCBCLA with alias carry logic and to make a comparison with other\nCLAs, a 32-bit addition operation is considered. Compared to the\nweak-indication SCBCLA with alias logic, the proposed early output SCBCLA with\nalias logic reports a 13% reduction in area without any increases in latency\nand power dissipation. On the other hand, in comparison with the early output\nrecursive CLA (RCLA), the proposed early output SCBCLA with alias logic reports\na 16% reduction in latency while occupying almost the same area and dissipating\nalmost the same average power. All the asynchronous CLAs are\nquasi-delay-insensitive designs which incorporate the delay-insensitive\ndual-rail data encoding and adhere to the 4-phase return-to-zero handshaking.\nThe adders were realized and the simulations were performed based on a 32/28nm\nCMOS process.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:33:49 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Balasubramanian", "P", ""], ["Dang", "C", ""], ["Maskell", "D L", ""], ["Prasad", "K", ""]]}, {"id": "1710.05474", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, C Dang, D L Maskell, K Prasad", "title": "Approximate Ripple Carry and Carry Lookahead Adders - A Comparative\n  Analysis", "comments": null, "journal-ref": "P. Balasubramanian, C. Dang, D.L. Maskell, K. Prasad, \"Approximate\n  Ripple Carry and Carry Lookahead Adders - A Comparative Analysis,\" Proc. 30th\n  International Conference on Microelectronics, pp. 299-304, 2017, Serbia", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate ripple carry adders (RCAs) and carry lookahead adders (CLAs) are\npresented which are compared with accurate RCAs and CLAs for performing a\n32-bit addition. The accurate and approximate RCAs and CLAs are implemented\nusing a 32/28nm CMOS process. Approximations ranging from 4- to 20-bits are\nconsidered for the less significant adder bit positions. The simulation results\nshow that approximate RCAs report reductions in the power-delay product (PDP)\nranging from 19.5% to 82% than the accurate RCA for approximation sizes varying\nfrom 4- to 20-bits. Also, approximate CLAs report reductions in PDP ranging\nfrom 16.7% to 74.2% than the accurate CLA for approximation sizes varying from\n4- to 20-bits. On average, for the approximation sizes considered, it is\nobserved that approximate CLAs achieve a 46.5% reduction in PDP compared to the\napproximate RCAs. Hence, approximate CLAs are preferable over approximate RCAs\nfor the low power implementation of approximate computer arithmetic.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:42:59 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Balasubramanian", "P", ""], ["Dang", "C", ""], ["Maskell", "D L", ""], ["Prasad", "K", ""]]}, {"id": "1710.05749", "submitter": "Shadrokh Samavi", "authors": "Farshad Kheiri, Shadrokh Samavi, Nader Karimi", "title": "Hardware design for binarization and thinning of fingerprint images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two critical steps in fingerprint recognition are binarization and thinning\nof the image. The need for real time processing motivates us to select local\nadaptive thresholding approach for the binarization step. We introduce a new\nhardware for this purpose based on pipeline architecture. We propose a formula\nfor selecting an optimal block size for the thresholding purpose. To decrease\nminutiae false detection, the binarized image is dilated. We also present in\nthis paper a new pipeline structure for implementing the thinning algorithm\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 19:50:47 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Kheiri", "Farshad", ""], ["Samavi", "Shadrokh", ""], ["Karimi", "Nader", ""]]}, {"id": "1710.07312", "submitter": "Bo Yu", "authors": "Weikang Fang, Yanjun Zhang, Bo Yu, Shaoshan Liu", "title": "FPGA-based ORB Feature Extraction for Real-Time Visual SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous Localization And Mapping (SLAM) is the problem of constructing\nor updating a map of an unknown environment while simultaneously keeping track\nof an agent's location within it. How to enable SLAM robustly and durably on\nmobile, or even IoT grade devices, is the main challenge faced by the industry\ntoday. The main problems we need to address are: 1.) how to accelerate the SLAM\npipeline to meet real-time requirements; and 2.) how to reduce SLAM energy\nconsumption to extend battery life. After delving into the problem, we found\nout that feature extraction is indeed the bottleneck of performance and energy\nconsumption. Hence, in this paper, we design, implement, and evaluate a\nhardware ORB feature extractor and prove that our design is a great balance\nbetween performance and energy consumption compared with ARM Krait and Intel\nCore i5.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:06:14 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Fang", "Weikang", ""], ["Zhang", "Yanjun", ""], ["Yu", "Bo", ""], ["Liu", "Shaoshan", ""]]}, {"id": "1710.08270", "submitter": "Marie Nguyen", "authors": "Marie Nguyen, James C. Hoe", "title": "Amorphous Dynamic Partial Reconfiguration with Flexible Boundaries to\n  Remove Fragmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic partial reconfiguration (DPR) allows one region of an\nfield-programmable gate array (FPGA) fabric to be reconfigured without\naffecting the operations on the rest of the fabric. To use an FPGA as a\ndynamically shared compute resource, one could partition and manage an FPGA\nfabric as multiple DPR partitions that can be independently reconfigured at\nruntime with different application function units (AFUs). Unfortunately,\ndividing a fabric into DPR partitions with fixed boundaries causes the\navailable fabric resources to become fragmented. An AFU of a given size cannot\nbe loaded unless a sufficiently large DPR partition was floorplanned at build\ntime. To overcome this inefficiency, we devised an \"amorphous\" DPR technique\nthat is compatible with current device and tool support but does not require\nthe DPR partition boundaries to be a priori fixed. A collection of AFU\nbitstreams can be simultaneously loaded on the fabric if their footprints (the\nactual area used by an AFU) in the fabric do not overlap. We verified the\nfeasibility of amorphous DPR on Xilinx Zynq System-on-Chip (SoC) FPGAs using\nVivado. We evaluated the benefits of amorphous DPR in the context of a\ndynamically reconfigurable vision processing pipeline framework.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:45:17 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 21:15:48 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Nguyen", "Marie", ""], ["Hoe", "James C.", ""]]}, {"id": "1710.08336", "submitter": "Bahram Rashidi", "authors": "Bahram Rashidi", "title": "A Survey on Hardware Implementations of Elliptic Curve Cryptosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the past two decades, Elliptic Curve Cryptography (ECC) have become\nincreasingly advanced. ECC, with much smaller key sizes, offers equivalent\nsecurity when compared to other asymmetric cryptosystems. In this survey, an\ncomprehensive overview of hardware implementations of ECC is provided. We first\ndiscuss different elliptic curves, point multiplication algorithms and\nunderling finite field operations over binary fields F2m and prime fields Fp\nwhich are used in the literature for hardware implementation. Then methods,\nsteps and considerations of ECC implementation are presented. The\nimplementations of the ECC are categorized in two main groups based on\nimplementation technologies consist of field programmable gate array (FPGA)\nbased implementations and application specific integrated circuit (ASIC)\nimplementations. Therefore, in these categories to have a better presentation\nand comparison, the implementations are presented and distinguished based on\ntype of finite fields. The best and newest structures in the literature are\ndescribed in more details for overall presentation of architectures and\napproaches in each group of implementations. High-speed implementation is an\nimportant factor in the ECC applications such as network servers. Also in smart\ncards, Wireless Sensor Networks (WSN) and Radio Frequency Identification (RFID)\ntags require to low-cost and lightweight implementations. Therefore,\nimplementation methods related to these applications are explored. In addition,\na classification of the previous works in terms of scalability, flexibility,\nperformance and cost effectiveness is provided. Finally, some words and\ntechniques about future works that should be considered are provided.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 17:46:43 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Rashidi", "Bahram", ""]]}, {"id": "1710.09012", "submitter": "Baibhab Chatterjee", "authors": "Baibhab Chatterjee, Priyadarshini Panda, Shovan Maity, Kaushik Roy and\n  Shreyas Sen", "title": "An Energy-Efficient Mixed-Signal Neuron for Inherently Error-Resilient\n  Neuromorphic Systems", "comments": "Accepted in IEEE International Conference on Rebooting Computing\n  (ICRC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the design and analysis of a mixed-signal neuron (MS-N)\nfor convolutional neural networks (CNN) and compares its performance with a\ndigital neuron (Dig-N) in terms of operating frequency, power and noise. The\ncircuit-level implementation of the MS-N in 65 nm CMOS technology exhibits 2-3\norders of magnitude better energy-efficiency over Dig-N for neuromorphic\ncomputing applications - especially at low frequencies due to the high leakage\ncurrents from many transistors in Dig-N. The inherent error-resiliency of CNN\nis exploited to handle the thermal and flicker noise of MS-N. A system-level\nanalysis using a cohesive circuit-algorithmic framework on MNIST and CIFAR-10\ndatasets demonstrate an increase of 3% in worst-case classification error for\nMNIST when the integrated noise power in the bandwidth is ~ 1 {\\mu}V2.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:43:16 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Chatterjee", "Baibhab", ""], ["Panda", "Priyadarshini", ""], ["Maity", "Shovan", ""], ["Roy", "Kaushik", ""], ["Sen", "Shreyas", ""]]}, {"id": "1710.09235", "submitter": "Jaebak Kim", "authors": "J. B. Kim and E. Won", "title": "A software framework for pipelined arithmetic algorithms in field\n  programmable gate arrays", "comments": "8 pages, 8 figures", "journal-ref": "Nucl. Instr. Meth. Phys. Res. A, Volume 883, 1 March 2018, Pages\n  83-89", "doi": "10.1016/j.nima.2017.11.064", "report-no": null, "categories": "cs.OH cs.AR hep-ex physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipelined algorithms implemented in field programmable gate arrays are being\nextensively used for hardware triggers in the modern experimental high energy\nphysics field and the complexity of such algorithms are increases rapidly. For\ndevelopment of such hardware triggers, algorithms are developed in\n$\\texttt{C++}$, ported to hardware description language for synthesizing\nfirmware, and then ported back to $\\texttt{C++}$ for simulating the firmware\nresponse down to the single bit level. We present a $\\texttt{C++}$ software\nframework which automatically simulates and generates hardware description\nlanguage code for pipelined arithmetic algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 05:00:48 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 07:50:26 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 02:39:55 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Kim", "J. B.", ""], ["Won", "E.", ""]]}, {"id": "1710.09517", "submitter": "Hyojong Kim", "authors": "Hyojong Kim, Ramyad Hadidi, Lifeng Nai, Hyesoon Kim, Nuwan Jayasena,\n  Yasuko Eckert, Onur Kayiran, Gabriel H. Loh", "title": "CODA: Enabling Co-location of Computation and Data for Near-Data\n  Processing", "comments": "14 pages, 16 figures", "journal-ref": "ACM Transactions on Architecture and Code Optimization (TACO)\n  Volume 15 Issue 3, October 2018 Article No. 32", "doi": "10.1145/3232521", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated that near-data processing (NDP) is an\neffective technique for improving performance and energy efficiency of\ndata-intensive workloads. However, leveraging NDP in realistic systems with\nmultiple memory modules introduces a new challenge. In today's systems, where\nno computation occurs in memory modules, the physical address space is\ninterleaved at a fine granularity among all memory modules to help improve the\nutilization of processor-memory interfaces by distributing the memory traffic.\nHowever, this is at odds with efficient use of NDP, which requires careful\nplacement of data in memory modules such that near-data computations and their\nexclusively used data can be localized in individual memory modules, while\ndistributing shared data among memory modules to reduce hotspots. In order to\naddress this new challenge, we propose a set of techniques that (1) enable\ncollections of OS pages to either be fine-grain interleaved among memory\nmodules (as is done today) or to be placed contiguously on individual memory\nmodules (as is desirable for NDP private data), and (2) decide whether to\nlocalize or distribute each memory object based on its anticipated access\npattern and steer computations to the memory where the data they access is\nlocated. Our evaluations across a wide range of workloads show that the\nproposed mechanism improves performance by 31% and reduces 38% remote data\naccesses over a baseline system that cannot exploit computate-data affinity\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 02:49:42 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Kim", "Hyojong", ""], ["Hadidi", "Ramyad", ""], ["Nai", "Lifeng", ""], ["Kim", "Hyesoon", ""], ["Jayasena", "Nuwan", ""], ["Eckert", "Yasuko", ""], ["Kayiran", "Onur", ""], ["Loh", "Gabriel H.", ""]]}, {"id": "1710.09975", "submitter": "Renato J Cintra", "authors": "A. Edirisuriya, A. Madanayake, R. J. Cintra, V. S. Dimitrov", "title": "A Single-Channel Architecture for Algebraic Integer Based 8$\\times$8 2-D\n  DCT Computation", "comments": "8 pages, 6 figures, 5 tables", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  volume 23, number 12, pages 2083-2089, Dec. 2013", "doi": "10.1109/TCSVT.2013.2270397", "report-no": null, "categories": "cs.AR cs.MM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An area efficient row-parallel architecture is proposed for the real-time\nimplementation of bivariate algebraic integer (AI) encoded 2-D discrete cosine\ntransform (DCT) for image and video processing. The proposed architecture\ncomputes 8$\\times$8 2-D DCT transform based on the Arai DCT algorithm. An\nimproved fast algorithm for AI based 1-D DCT computation is proposed along with\na single channel 2-D DCT architecture. The design improves on the 4-channel AI\nDCT architecture that was published recently by reducing the number of integer\nchannels to one and the number of 8-point 1-D DCT cores from 5 down to 2. The\narchitecture offers exact computation of 8$\\times$8 blocks of the 2-D DCT\ncoefficients up to the FRS, which converts the coefficients from the AI\nrepresentation to fixed-point format using the method of expansion factors.\nPrototype circuits corresponding to FRS blocks based on two expansion factors\nare realized, tested, and verified on FPGA-chip, using a Xilinx Virtex-6\nXC6VLX240T device. Post place-and-route results show a 20% reduction in terms\nof area compared to the 2-D DCT architecture requiring five 1-D AI cores. The\narea-time and area-time${}^2$ complexity metrics are also reduced by 23% and\n22% respectively for designs with 8-bit input word length. The digital\nrealizations are simulated up to place and route for ASICs using 45 nm CMOS\nstandard cells. The maximum estimated clock rate is 951 MHz for the CMOS\nrealizations indicating 7.608$\\cdot$10$^9$ pixels/seconds and a 8$\\times$8\nblock rate of 118.875 MHz.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 03:32:48 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Edirisuriya", "A.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1710.10290", "submitter": "Zhipeng Zhao", "authors": "Zhipeng Zhao and James C. Hoe", "title": "Using Vivado-HLS for Structural Design: a NoC Case Study", "comments": "A poster with the same title was presented at the 2017 International\n  Symposium on Field Programmable Gate Arrays", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been ample successful examples of applying Xilinx Vivado's\n\"function-to-module\" high-level synthesis (HLS) where the subject is\nalgorithmic in nature. In this work, we carried out a design study to assess\nthe effectiveness of applying Vivado-HLS in structural design. We employed\nVivado-HLS to synthesize C functions corresponding to standalone\nnetwork-on-chip (NoC) routers as well as complete multi-endpoint NoCs.\nInterestingly, we find that describing a complete NoC comprising router\nsubmodules faces fundamental difficulties not present in describing the routers\nas standalone modules. Ultimately, we succeeded in using Vivado-HLS to produce\nrouter and NoC modules that are exact cycle- and bit-accurate replacements of\nour reference RTL-based router and NoC modules. Furthermore, the routers and\nNoCs resulting from HLS and RTL are comparable in resource utilization and\ncritical path delay. Our experience subjectively suggests that HLS is able to\nsimplify the design effort even though much of the structural details had to be\nprovided in the HLS description through a combination of coding discipline and\nexplicit pragmas. The C++ source code can be found at\nhttps://github.com/zhipengzhaocmu/HLS_NoC.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 18:29:32 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 19:24:23 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhao", "Zhipeng", ""], ["Hoe", "James C.", ""]]}, {"id": "1710.10296", "submitter": "Yufeng Hao", "authors": "Yufeng Hao, Steven Quigley", "title": "The implementation of a Deep Recurrent Neural Network Language Model on\n  a Xilinx FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, FPGA has been increasingly applied to problems such as speech\nrecognition, machine learning, and cloud computation such as the Bing search\nengine used by Microsoft. This is due to FPGAs great parallel computation\ncapacity as well as low power consumption compared to general purpose\nprocessors. However, these applications mainly focus on large scale FPGA\nclusters which have an extreme processing power for executing massive matrix or\nconvolution operations but are unsuitable for portable or mobile applications.\nThis paper describes research on single-FPGA platform to explore the\napplications of FPGAs in these fields. In this project, we design a Deep\nRecurrent Neural Network (DRNN) Language Model (LM) and implement a hardware\naccelerator with AXI Stream interface on a PYNQ board which is equipped with a\nXILINX ZYNQ SOC XC7Z020 1CLG400C. The PYNQ has not only abundant programmable\nlogic resources but also a flexible embedded operation system, which makes it\nsuitable to be applied in the natural language processing field. We design the\nDRNN language model with Python and Theano, train the model on a CPU platform,\nand deploy the model on a PYNQ board to validate the model with Jupyter\nnotebook. Meanwhile, we design the hardware accelerator with Overlay, which is\na kind of hardware library on PYNQ, and verify the acceleration effect on the\nPYNQ board. Finally, we have found that the DRNN language model can be deployed\non the embedded system smoothly and the Overlay accelerator with AXI Stream\ninterface performs at 20 GOPS processing throughput, which constitutes a 70.5X\nand 2.75X speed up compared to the work in Ref.30 and Ref.31 respectively.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 07:34:48 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 22:14:09 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 13:51:15 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Hao", "Yufeng", ""], ["Quigley", "Steven", ""]]}, {"id": "1710.10746", "submitter": "Pranith Kumar", "authors": "Pranith Kumar, Prasun Gera, Hyojong Kim, Hyesoon Kim", "title": "Louvre: Lightweight Ordering Using Versioning for Release Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fence instructions are fundamental primitives that ensure consistency in a\nweakly consistent shared memory multi-core processor. The execution cost of\nthese instructions is significant and adds a non-trivial overhead to parallel\nprograms. In a naive architecture implementation, we track the ordering\nconstraints imposed by a fence by its entry in the reorder buffer and its\nexecution overhead entails stalling the processor's pipeline until the store\nbuffer is drained and also conservatively invalidating speculative loads. These\nactions create a cascading effect of increased overhead on the execution of the\nfollowing instructions in the program. We find these actions to be overly\nrestrictive and that they can be further relaxed thereby allowing aggressive\noptimizations.\n  The current work proposes a lightweight mechanism in which we assign ordering\ntags, called versions, to load and store instructions when they reside in the\nload/store queues and the write buffer. The version assigned to a memory access\nallows us to fully exploit the relaxation allowed by the weak consistency model\nand restricts its execution in such a way that the ordering constraints by the\nmodel are satisfied. We utilize the information captured through the assigned\nversions to reduce stalls caused by waiting for the store buffer to drain and\nto avoid unnecessary squashing of speculative loads, thereby minimizing the\nre-execution penalty. This method is particularly effective for the release\nconsistency model that employs uni-directional fence instructions. We show that\nthis mechanism reduces the ordering instruction latency by 39.6% and improves\nprogram performance by 11% on average over the baseline implementation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:37:51 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 16:04:49 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 20:12:55 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Kumar", "Pranith", ""], ["Gera", "Prasun", ""], ["Kim", "Hyojong", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1710.11200", "submitter": "Renato J Cintra", "authors": "N. Rajapaksha, A. Madanayake, R. J. Cintra, J. Adikari, V. S. Dimitrov", "title": "VLSI Computational Architectures for the Arithmetic Cosine Transform", "comments": "8 pages, 2 figures, 6 tables", "journal-ref": "IEEE Transactions on Computers, vol. 64, no. 9, Sep 2015", "doi": "10.1109/TC.2014.2366732", "report-no": null, "categories": "cs.AR cs.DS cs.MM math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete cosine transform (DCT) is a widely-used and important signal\nprocessing tool employed in a plethora of applications. Typical fast algorithms\nfor nearly-exact computation of DCT require floating point arithmetic, are\nmultiplier intensive, and accumulate round-off errors. Recently proposed fast\nalgorithm arithmetic cosine transform (ACT) calculates the DCT exactly using\nonly additions and integer constant multiplications, with very low area\ncomplexity, for null mean input sequences. The ACT can also be computed\nnon-exactly for any input sequence, with low area complexity and low power\nconsumption, utilizing the novel architecture described. However, as a\ntrade-off, the ACT algorithm requires 10 non-uniformly sampled data points to\ncalculate the 8-point DCT. This requirement can easily be satisfied for\napplications dealing with spatial signals such as image sensors and biomedical\nsensor arrays, by placing sensor elements in a non-uniform grid. In this work,\na hardware architecture for the computation of the null mean ACT is proposed,\nfollowed by a novel architectures that extend the ACT for non-null mean\nsignals. All circuits are physically implemented and tested using the Xilinx\nXC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for\nperformance assessment.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:06:19 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rajapaksha", "N.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Adikari", "J.", ""], ["Dimitrov", "V. S.", ""]]}]