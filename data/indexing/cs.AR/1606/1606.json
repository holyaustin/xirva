[{"id": "1606.00251", "submitter": "Daniel Sorin", "authors": "Ralph Nathan, Helia Naeimi, Daniel J. Sorin, Xiaobai Sun", "title": "Profile-Driven Automated Mixed Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme to automatically set the precision of floating point\nvariables in an application. We design a framework that profiles applications\nto measure undesirable numerical behavior at the floating point operation\nlevel. We use this framework to perform mixed precision analysis to\nheuristically set the precision of all variables in an application based on\ntheir numerical profiles. We experimentally evaluate the mixed precision\nanalysis to show that it can generate a range of results with different\naccuracy and performance characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:27:54 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Nathan", "Ralph", ""], ["Naeimi", "Helia", ""], ["Sorin", "Daniel J.", ""], ["Sun", "Xiaobai", ""]]}, {"id": "1606.01037", "submitter": "Jan Gray", "authors": "Jan Gray", "title": "GRVI Phalanx: A Massively Parallel RISC-V FPGA Accelerator Accelerator", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/05", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GRVI is an FPGA-efficient RISC-V RV32I soft processor. Phalanx is a parallel\nprocessor and accelerator array framework. Groups of processors and\naccelerators form shared memory clusters. Clusters are interconnected with each\nother and with extreme bandwidth I/O and memory devices by a 300- bit-wide\nHoplite NOC. An example Kintex UltraScale KU040 system has 400 RISC-V cores,\npeak throughput of 100,000 MIPS, peak shared memory bandwidth of 600 GB/s, NOC\nbisection bandwidth of 700 Gbps, and uses 13 W.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 10:37:41 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Gray", "Jan", ""]]}, {"id": "1606.01607", "submitter": "Milad Mohammadi", "authors": "Milad Mohammadi, Tor M. Aamodt, William J. Dally", "title": "CG-OoO: Energy-Efficient Coarse-Grain Out-of-Order Execution", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Coarse-Grain Out-of-Order (CG- OoO) general purpose\nprocessor designed to achieve close to In-Order processor energy while\nmaintaining Out-of-Order (OoO) performance. CG-OoO is an energy-performance\nproportional general purpose architecture that scales according to the program\nload. Block-level code processing is at the heart of the this architecture;\nCG-OoO speculates, fetches, schedules, and commits code at block-level\ngranularity. It eliminates unnecessary accesses to energy consuming tables, and\nturns large tables into smaller and distributed tables that are cheaper to\naccess. CG-OoO leverages compiler-level code optimizations to deliver efficient\nstatic code, and exploits dynamic instruction-level parallelism and block-level\nparallelism. CG-OoO introduces Skipahead issue, a complexity effective, limited\nout-of-order instruction scheduling model. Through the energy efficiency\ntechniques applied to the compiler and processor pipeline stages, CG-OoO closes\n64% of the average energy gap between the In-Order and Out-of-Order baseline\nprocessors at the performance of the OoO baseline. This makes CG-OoO 1.9x more\nefficient than the OoO on the energy-delay product inverse metric.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 03:44:52 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mohammadi", "Milad", ""], ["Aamodt", "Tor M.", ""], ["Dally", "William J.", ""]]}, {"id": "1606.01980", "submitter": "Gagan Gupta", "authors": "Gagan Gupta, Tony Nowatzki, Vinay Gangadhar, and Karthikeyan\n  Sankaralingam", "title": "Open-source Hardware: Opportunities and Challenges", "comments": "To appear in IEEE Computer. IEEE copyright notice, and DOI to be\n  added. 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation in hardware is slowing due to rising costs of chip design and\ndiminishing benefits from Moore's law and Dennard scaling. Software innovation,\non the other hand, is flourishing, helped in good measure by a thriving\nopen-source ecosystem. We believe that open source can similarly help hardware\ninnovation, but has not yet due to several reasons. We identify these reasons\nand how the industry, academia, and the hardware community at large can come\ntogether to address them.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 00:15:16 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 20:53:13 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Gupta", "Gagan", ""], ["Nowatzki", "Tony", ""], ["Gangadhar", "Vinay", ""], ["Sankaralingam", "Karthikeyan", ""]]}, {"id": "1606.03248", "submitter": "ShenChen Ruan", "authors": "Shenchen Ruan, Haixia Wang and Dongsheng Wang", "title": "MAC: a novel systematically multilevel cache replacement policy for PCM\n  memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of multi-core system and increase of data-intensive\napplication in recent years call for larger main memory. Traditional DRAM\nmemory can increase its capacity by reducing the feature size of storage cell.\nNow further scaling of DRAM faces great challenge, and the frequent refresh\noperations of DRAM can bring a lot of energy consumption. As an emerging\ntechnology, Phase Change Memory (PCM) is promising to be used as main memory.\nIt draws wide attention due to the advantages of low power consumption, high\ndensity and nonvolatility, while it incurs finite endurance and relatively long\nwrite latency. To handle the problem of write, optimizing the cache replacement\npolicy to protect dirty cache block is an efficient way. In this paper, we\nconstruct a systematically multilevel structure, and based on it propose a\nnovel cache replacement policy called MAC. MAC can effectively reduce write\ntraffic to PCM memory with low hardware overhead. We conduct simulation\nexperiments on GEM5 to evaluate the performances of MAC and other related\nworks. The results show that MAC performs best in reducing the amount of writes\n(averagely 25.12%) without increasing the program execution time.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 09:47:14 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Ruan", "Shenchen", ""], ["Wang", "Haixia", ""], ["Wang", "Dongsheng", ""]]}, {"id": "1606.03717", "submitter": "Hossein Omidian", "authors": "Hossein Omidian and Guy G.F. Lemieux", "title": "Automated Space/Time Scaling of Streaming Task Graph", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/01", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a high-level synthesis (HLS) tool that\nautomatically allows area/throughput trade-offs for implementing streaming task\ngraphs (STG). Our tool targets a massively parallel processor array (MPPA)\narchitecture, very similar to the Ambric MPPA chip architecture, which is to be\nimplemented as an FPGA overlay. Similar to Ambric tools, our HLS tool accepts a\nSTG as input written in a subset of Java and a structural language in the style\nof a Kahn Processing Network (KPN). Unlike the Ambric tools, our HLS tool\nanalyzes the parallelism internal to each Java \"node\" and evaluates the\nthroughput and area of several possible implementations. It then analyzes the\nfull graph for bottlenecks or excess compute capacity, selects an\nimplementation for each node, and even considers replicating or splitting nodes\nwhile either minimizing area (for a fixed throughput target), or maximizing\nthroughput (for a fixed area target). In addition to traditional node selection\nand replication methods used in prior work, we have uniquely implemented node\ncombining and splitting to find a better area/throughput trade-off. We present\ntwo optimization approaches, a formal ILP formulation and a heuristic solution.\nResults show that the heuristic is more flexible and can find design points not\navailable to the ILP, thereby achieving superior results.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 14:26:01 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Omidian", "Hossein", ""], ["Lemieux", "Guy G. F.", ""]]}, {"id": "1606.03742", "submitter": "Vincent T. Lee", "authors": "Vincent T. Lee, Amrita Mazumdar, Carlo C. del Mundo, Armin Alaghi,\n  Luis Ceze, Mark Oskin", "title": "Application-Driven Near-Data Processing for Similarity Search", "comments": "15 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a key to a variety of applications including\ncontent-based search for images and video, recommendation systems, data\ndeduplication, natural language processing, computer vision, databases,\ncomputational biology, and computer graphics. At its core, similarity search\nmanifests as k-nearest neighbors (kNN), a computationally simple primitive\nconsisting of highly parallel distance calculations and a global top-k sort.\nHowever, kNN is poorly supported by today's architectures because of its high\nmemory bandwidth requirements.\n  This paper proposes an application-driven near-data processing accelerator\nfor similarity search: the Similarity Search Associative Memory (SSAM). By\ninstantiating compute units close to memory, SSAM benefits from the higher\nmemory bandwidth and density exposed by emerging memory technologies. We\nevaluate the SSAM design down to layout on top of the Micron hybrid memory cube\n(HMC), and show that SSAM can achieve up to two orders of magnitude\narea-normalized throughput and energy efficiency improvement over multicore\nCPUs; we also show SSAM is faster and more energy efficient than competing GPUs\nand FPGAs. Finally, we show that SSAM is also useful for other data intensive\ntasks like kNN index construction, and can be generalized to semantically\nfunction as a high capacity content addressable memory.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 17:08:43 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 16:56:51 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lee", "Vincent T.", ""], ["Mazumdar", "Amrita", ""], ["del Mundo", "Carlo C.", ""], ["Alaghi", "Armin", ""], ["Ceze", "Luis", ""], ["Oskin", "Mark", ""]]}, {"id": "1606.04074", "submitter": "Pedro Lopez-Garcia", "authors": "Kerstin Eder and John P. Gallagher and Pedro Lopez-Garcia and Henk\n  Muller and Zorana Bankovic and Kyriakos Georgiou and Remy Haemmerle and\n  Manuel V. Hermenegildo and Bishoksan Kafle and Steve Kerrison and Maja\n  Kirkeby and Maximiliano Klemen and Xueliang Li and Umer Liqat and Jeremy\n  Morse and Morten Rhiger and Mads Rosendahl", "title": "ENTRA: Whole-Systems Energy Transparency", "comments": "Revised preprint submitted to MICPRO on 27 May 2016, 23 pages, 3\n  figures", "journal-ref": null, "doi": "10.1016/j.micpro.2016.07.003", "report-no": null, "categories": "cs.AR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoting energy efficiency to a first class system design goal is an\nimportant research challenge. Although more energy-efficient hardware can be\ndesigned, it is software that controls the hardware; for a given system the\npotential for energy savings is likely to be much greater at the higher levels\nof abstraction in the system stack. Thus the greatest savings are expected from\nenergy-aware software development, which is the vision of the EU ENTRA project.\nThis article presents the concept of energy transparency as a foundation for\nenergy-aware software development. We show how energy modelling of hardware is\ncombined with static analysis to allow the programmer to understand the energy\nconsumption of a program without executing it, thus enabling exploration of the\ndesign space taking energy into consideration. The paper concludes by\nsummarising the current and future challenges identified in the ENTRA project.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:16:52 GMT"}, {"version": "v2", "created": "Sat, 18 Jun 2016 05:56:38 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Eder", "Kerstin", ""], ["Gallagher", "John P.", ""], ["Lopez-Garcia", "Pedro", ""], ["Muller", "Henk", ""], ["Bankovic", "Zorana", ""], ["Georgiou", "Kyriakos", ""], ["Haemmerle", "Remy", ""], ["Hermenegildo", "Manuel V.", ""], ["Kafle", "Bishoksan", ""], ["Kerrison", "Steve", ""], ["Kirkeby", "Maja", ""], ["Klemen", "Maximiliano", ""], ["Li", "Xueliang", ""], ["Liqat", "Umer", ""], ["Morse", "Jeremy", ""], ["Rhiger", "Morten", ""], ["Rosendahl", "Mads", ""]]}, {"id": "1606.04609", "submitter": "Raqibul Hasan", "authors": "Raqibul Hasan, Tarek M. Taha, Chris Yakopcic, and David J. Mountain", "title": "High Throughput Neural Network based Embedded Streaming Multicore\n  Processors", "comments": "8 pages. arXiv admin note: text overlap with arXiv:1603.07400", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With power consumption becoming a critical processor design issue,\nspecialized architectures for low power processing are becoming popular.\nSeveral studies have shown that neural networks can be used for signal\nprocessing and pattern recognition applications. This study examines the design\nof memristor based multicore neural processors that would be used primarily to\nprocess data directly from sensors. Additionally, we have examined the design\nof SRAM based neural processors for the same task. Full system evaluation of\nthe multicore processors based on these specialized cores were performed taking\nI/O and routing circuits into consideration. The area and power benefits were\ncompared with traditional multicore RISC processors. Our results show that the\nmemristor based architectures can provide an energy efficiency between three\nand five orders of magnitude greater than that of RISC processors for the\nbenchmarks examined.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 01:26:24 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Hasan", "Raqibul", ""], ["Taha", "Tarek M.", ""], ["Yakopcic", "Chris", ""], ["Mountain", "David J.", ""]]}, {"id": "1606.05094", "submitter": "Bert Moons", "authors": "Bert Moons and Marian Verhelst", "title": "A 0.3-2.6 TOPS/W Precision-Scalable Processor for Real-Time Large-Scale\n  ConvNets", "comments": "Published at the Symposium on VLSI Circuits, 2016, Honolulu, HI, US", "journal-ref": null, "doi": null, "report-no": "paper C17p1", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-power precision-scalable processor for ConvNets or convolutional neural\nnetworks (CNN) is implemented in a 40nm technology. Its 256 parallel processing\nunits achieve a peak 102GOPS running at 204MHz. To minimize energy consumption\nwhile maintaining throughput, this works is the first to both exploit the\nsparsity of convolutions and to implement dynamic precision-scalability\nenabling supply- and energy scaling. The processor is fully C-programmable,\nconsumes 25-288mW at 204 MHz and scales efficiency from 0.3-2.6 real TOPS/W.\nThis system hereby outperforms the state-of-the-art up to 3.9x in energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2016 08:40:57 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Moons", "Bert", ""], ["Verhelst", "Marian", ""]]}, {"id": "1606.05416", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Arvind, Muralidaran Vijayaraghavan", "title": "Taming Weak Memory Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speculative techniques in microarchitectures relax various dependencies in\nprograms, which contributes to the complexity of (weak) memory models. We show\nusing WMM, a new weak memory model, that the model becomes simpler if it\nincludes load-value speculation and thus, does not enforce any dependency!\nHowever, in the absence of good value-prediction techniques, a programmer may\nend up paying a price for the extra fences. Thus, we also present WMM-D, which\nenforces the dependencies captured by the current microarchitectures. WMM-D is\nstill much simpler than other existing models. We also show that non-atomic\nmulti-copy stores arise as a result of sharing write-through caches. We think\nrestricting microarchitectures to write-back caches (and thus simpler weak\nmemory models) will not incur any performance penalty. Nevertheless, we present\nWMM-S, another extension to WMM, which could model the effects of non-atomic\nmulti-copy stores. WMM, WMM-D, and WMM-S are all defined using Instantaneous\nInstruction Execution (I^2E), a new way of describing memory models without\nexplicit reordering or speculative execution.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 04:34:34 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Arvind", "", ""], ["Vijayaraghavan", "Muralidaran", ""]]}, {"id": "1606.05487", "submitter": "Renzo Andri", "authors": "Renzo Andri and Lukas Cavigelli and Davide Rossi and Luca Benini", "title": "YodaNN: An Architecture for Ultra-Low Power Binary-Weight CNN\n  Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have revolutionized the world of\ncomputer vision over the last few years, pushing image classification beyond\nhuman accuracy. The computational effort of today's CNNs requires power-hungry\nparallel processors or GP-GPUs. Recent developments in CNN accelerators for\nsystem-on-chip integration have reduced energy consumption significantly.\nUnfortunately, even these highly optimized devices are above the power envelope\nimposed by mobile and deeply embedded applications and face hard limitations\ncaused by CNN weight I/O and storage. This prevents the adoption of CNNs in\nfuture ultra-low power Internet of Things end-nodes for near-sensor analytics.\nRecent algorithmic and theoretical advancements enable competitive\nclassification accuracy even when limiting CNNs to binary (+1/-1) weights\nduring training. These new findings bring major optimization opportunities in\nthe arithmetic core by removing the need for expensive multiplications, as well\nas reducing I/O bandwidth and storage. In this work, we present an accelerator\noptimized for binary-weight CNNs that achieves 1510 GOp/s at 1.2 V on a core\narea of only 1.33 MGE (Million Gate Equivalent) or 0.19 mm$^2$ and with a power\ndissipation of 895 {\\mu}W in UMC 65 nm technology at 0.6 V. Our accelerator\nsignificantly outperforms the state-of-the-art in terms of energy and area\nefficiency achieving 61.2 TOp/s/W@0.6 V and 1135 GOp/s/MGE@1.2 V, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 11:48:29 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2017 11:00:58 GMT"}, {"version": "v3", "created": "Thu, 16 Feb 2017 10:54:26 GMT"}, {"version": "v4", "created": "Fri, 24 Feb 2017 08:46:12 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1606.05562", "submitter": "Renato J Cintra", "authors": "T. L. T. da Silveira, F. M. Bayer, R. J. Cintra, S. Kulasekera, A.\n  Madanayake, A. J. Kozakevicius", "title": "An Orthogonal 16-point Approximate DCT for Image and Video Compression", "comments": "18 pages, 7 figures, 6 tables", "journal-ref": "Multidimensional Systems and Signal Processing, vol. 27, no. 1,\n  pp. 87-104, 2016", "doi": "10.1007/s11045-014-0291-6", "report-no": null, "categories": "cs.IT cs.AR cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity orthogonal multiplierless approximation for the 16-point\ndiscrete cosine transform (DCT) was introduced. The proposed method was\ndesigned to possess a very low computational cost. A fast algorithm based on\nmatrix factorization was proposed requiring only 60~additions. The proposed\narchitecture outperforms classical and state-of-the-art algorithms when\nassessed as a tool for image and video compression. Digital VLSI hardware\nimplementations were also proposed being physically realized in FPGA technology\nand implemented in 45 nm up to synthesis and place-route levels. Additionally,\nthe proposed method was embedded into a high efficiency video coding (HEVC)\nreference software for actual proof-of-concept. Obtained results show\nnegligible video degradation when compared to Chen DCT algorithm in HEVC.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 01:19:46 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Kozakevicius", "A. J.", ""]]}, {"id": "1606.05621", "submitter": "P Balasubramanian", "authors": "P Balasubramanian", "title": "Design of Synchronous Section-Carry Based Carry Lookahead Adders with\n  Improved Figure of Merit", "comments": "arXiv admin note: text overlap with arXiv:1603.07961", "journal-ref": "WSEAS Transactions on Circuits and Systems, vol. 15, Article #18,\n  pp. 155-164, 2016", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The section-carry based carry lookahead adder (SCBCLA) architecture was\nproposed as an efficient alternative to the conventional carry lookahead adder\n(CCLA) architecture for the physical implementation of computer arithmetic. In\nprevious related works, self-timed SCBCLA architectures and synchronous SCBCLA\narchitectures were realized using standard cells and FPGAs. In this work, we\ndeal with improved realizations of synchronous SCBCLA architectures designed in\na semi-custom fashion using standard cells. The improvement is quantified in\nterms of a figure of merit (FOM), where the FOM is defined as the inverse\nproduct of power, delay and area. Since power, delay and area of digital\ndesigns are desirable to be minimized, the FOM is desirable to be maximized.\nStarting from an efficient conventional carry lookahead generator, we show how\nan optimized section-carry based carry lookahead generator is realized. In\ncomparison with our recent work dealing with standard cells based\nimplementation of SCBCLAs to perform 32-bit addition of two binary operands, we\nshow in this work that with improved section-carry based carry lookahead\ngenerators, the resulting SCBCLAs exhibit significant improvements in FOM.\nCompared to the earlier optimized hybrid SCBCLA, the proposed optimized hybrid\nSCBCLA improves the FOM by 88.3%. Even the optimized hybrid CCLA features\nimprovement in FOM by 77.3% over the earlier optimized hybrid CCLA. However,\nthe proposed optimized hybrid SCBCLA is still the winner and has a better FOM\nthan the currently optimized hybrid CCLA by 15.3%. All the CCLAs and SCBCLAs\nare implemented to realize 32-bit dual-operand binary addition using a 32/28nm\nCMOS process.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 18:54:48 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 06:44:29 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Balasubramanian", "P", ""]]}, {"id": "1606.05933", "submitter": "Sandeep Navada Sandeep Navada", "authors": "Sandeep Navada and Anil Krishna", "title": "Criticality Aware Multiprocessors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, a memory request from a processor may need to go through many\nintermediate interconnect routers, directory node, owner node, etc before it is\nfinally serviced. Current multiprocessors do not give preference to any\nparticular memory request. But certain memory requests are more critical to\nmultiprocessor's performance than other requests. Example: memory requests from\ncritical sections, load request feeding into multiple dependent instructions,\netc. This knowledge can be used to improve the performance of current\nmultiprocessors by letting the ordering point and the interconnect routers\nprioritize critical requests over non-critical ones. In this paper, we evaluate\nusing SIMICS/GEMS infrastructure. For lock-intensive microbenchmarks,\ncriticality-aware multiprocessors showed 5-15% performance improvement over\nbaseline multiprocessor. Criticality aware multiprocessor provides a new\ndirection for tapping performance in a shared memory multiprocessor and can\nprovide substantial speedup in lock intensive benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 00:57:30 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Navada", "Sandeep", ""], ["Krishna", "Anil", ""]]}, {"id": "1606.06451", "submitter": "Shaoyi Cheng", "authors": "Shaoyi Cheng and John Wawrzynek", "title": "High Level Synthesis with a Dataflow Architectural Template", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/03", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new approach to high level synthesis (HLS), where\nhigh level functions are first mapped to an architectural template, before\nhardware synthesis is performed. As FPGA platforms are especially suitable for\nimplementing streaming processing pipelines, we perform transformations on\nconventional high level programs where they are turned into multi-stage\ndataflow engines [1]. This target template naturally overlaps slow memory data\naccesses with computations and therefore has much better tolerance towards\nmemory subsystem latency. Using a state-of-the-art HLS tool for the actual\ncircuit generation, we observe up to 9x improvement in overall performance when\nthe dataflow architectural template is used as an intermediate compilation\ntarget.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:22:12 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Cheng", "Shaoyi", ""], ["Wawrzynek", "John", ""]]}, {"id": "1606.06452", "submitter": "Mihalis Psarakis", "authors": "Mihalis Psarakis", "title": "Reliability-Aware Overlay Architectures for FPGAs: Features and Design\n  Challenges", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/04", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FPGA overlay architectures have been mainly proposed to improve design\nproductivity, circuit portability and system debugging. In this paper, we\naddress the use of overlay architectures for building fault tolerant SRAM-based\nFPGA systems and discuss the main features and design challenges of a\nreliability-aware overlay architecture.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:25:05 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Psarakis", "Mihalis", ""]]}, {"id": "1606.06454", "submitter": "Russell Tessier", "authors": "Kevin Andryc, Tedy Thomas and Russell Tessier", "title": "Soft GPGPUs for Embedded FPGAs: An Architectural Evaluation", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/06", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a customizable soft architecture which allows for the execution of\nGPGPU code on an FPGA without the need to recompile the design. Issues related\nto scaling the overlay architecture to multiple GPGPU multiprocessors are\nconsidered along with application-class architectural optimizations. The\noverlay architecture is optimized for FPGA implementation to support efficient\nuse of embedded block memories and DSP blocks. This architecture supports\ndirect CUDA compilation of integer computations to a binary which is executable\non the FPGA-based GPGPU. The benefits of our architecture are evaluated for a\ncollection of five standard CUDA benchmarks which are compiled using standard\nGPGPU compilation tools. Speedups of 44x, on average, versus a MicroBlaze\nmicroprocessor are achieved. We show dynamic energy savings versus a soft-core\nprocessor of 80% on average. Application-customized versions of the soft GPGPU\ncan be used to further reduce dynamic energy consumption by an average of 14%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:33:43 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Andryc", "Kevin", ""], ["Thomas", "Tedy", ""], ["Tessier", "Russell", ""]]}, {"id": "1606.06457", "submitter": "Fatemeh Eslami", "authors": "Fatemeh Eslami, Eddie Hung and Steven J.E. Wilton", "title": "Enabling Effective FPGA Debug using Overlays: Opportunities and\n  Challenges", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/08", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs are going mainstream. Major companies that were not traditionally\nFPGA-focused are now seeking ways to exploit the benefits of reconfigurable\ntechnology and provide it to their customers. In order to do so, a debug\necosystem that provides for effective visibility into a working design and\nquick debug turn-around times is essential. Overlays have the opportunity to\nplay a key role in this ecosystem. In this overview paper, we discuss how an\noverlay fabric that allows the user to rapidly add debug instrumentation to a\ndesign can be created and exploited. We discuss the requirements of such an\noverlay and some of the research challenges and opportunities that need to be\naddressed. To make our exposition concrete, we use two previously-published\nexamples of overlays that have been developed to implement debug\ninstrumentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:43:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Eslami", "Fatemeh", ""], ["Hung", "Eddie", ""], ["Wilton", "Steven J. E.", ""]]}, {"id": "1606.06460", "submitter": "Xiangwei Li", "authors": "Xiangwei Li, Abhishek Jain, Douglas Maskell and Suhaib A. Fahmy", "title": "An Area-Efficient FPGA Overlay using DSP Block based Time-multiplexed\n  Functional Units", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/02", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse grained overlay architectures improve FPGA design productivity by\nproviding fast compilation and software-like programmability. Throughput\noriented spatially configurable overlays typically suffer from area overheads\ndue to the requirement of one functional unit for each compute kernel\noperation. Hence, these overlays have often been of limited size, supporting\nonly relatively small compute kernels while consuming considerable FPGA\nresources. This paper examines the possibility of sharing the functional units\namong kernel operations for reducing area overheads. We propose a linear\ninterconnected array of time-multiplexed FUs as an overlay architecture with\nreduced instruction storage and interconnect resource requirements, which uses\na fully-pipelined, architecture-aware FU design supporting a fast context\nswitching time. The results presented show a reduction of up to 85% in FPGA\nresource requirements compared to existing throughput oriented overlay\narchitectures, with an operating frequency which approaches the theoretical\nlimit for the FPGA device.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 07:52:00 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Li", "Xiangwei", ""], ["Jain", "Abhishek", ""], ["Maskell", "Douglas", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "1606.06483", "submitter": "Ho-Cheung Ng", "authors": "Ho-Cheung Ng, Cheng Liu, Hayden Kwok-Hay So", "title": "A Soft Processor Overlay with Tightly-coupled FPGA Accelerator", "comments": "Presented at 2nd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2016) arXiv:1605.08149", "journal-ref": null, "doi": null, "report-no": "OLAF/2016/07", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA overlays are commonly implemented as coarse-grained reconfigurable\narchitectures with a goal to improve designers' productivity through balancing\nflexibility and ease of configuration of the underlying fabric. To truly\nfacilitate full application acceleration, it is often necessary to also include\na highly efficient processor that integrates and collaborates with the\naccelerators while maintaining the benefits of being implemented within the\nsame overlay framework. This paper presents an open-source soft processor that\nis designed to tightly-couple with FPGA accelerators as part of an overlay\nframework. RISC-V is chosen as the instruction set for its openness and\nportability, and the soft processor is designed as a 4-stage pipeline to\nbalance resource consumption and performance when implemented on FPGAs. The\nprocessor is generically implemented so as to promote design portability and\ncompatibility across different FPGA platforms. Experimental results show that\nintegrated software-hardware applications using the proposed tightly-coupled\narchitecture achieve comparable performance as hardware-only accelerators while\nthe proposed architecture provides additional run-time flexibility. The\nprocessor has been synthesized to both low-end and high-performance FPGA\nfamilies from different vendors, achieving the highest frequency of 268.67MHz\nand resource consumption comparable to existing RISC-V designs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 09:00:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Ng", "Ho-Cheung", ""], ["Liu", "Cheng", ""], ["So", "Hayden Kwok-Hay", ""]]}, {"id": "1606.07852", "submitter": "Jing Pu", "authors": "Jing Pu, Sameh Galal, Xuan Yang, Ofer Shacham, Mark Horowitz", "title": "FPMax: a 106GFLOPS/W at 217GFLOPS/mm2 Single-Precision FPU, and a\n  43.7GFLOPS/W at 74.6GFLOPS/mm2 Double-Precision FPU, in 28nm UTBB FDSOI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPMax implements four FPUs optimized for latency or throughput workloads in\ntwo precisions, fabricated in 28nm UTBB FDSOI. Each unit's parameters, e.g\npipeline stages, booth encoding etc., were optimized to yield 1.42ns latency at\n110GLOPS/W (SP) and 1.39ns latency at 36GFLOPS/W (DP). At 100% activity,\nbody-bias control improves the energy efficiency by about 20%; at 10% activity\nthis saving is almost 2x.\n  Keywords: FPU, energy efficiency, hardware generator, SOI\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 23:20:50 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Pu", "Jing", ""], ["Galal", "Sameh", ""], ["Yang", "Xuan", ""], ["Shacham", "Ofer", ""], ["Horowitz", "Mark", ""]]}, {"id": "1606.08686", "submitter": "Steve Kerrison MEng PhD", "authors": "Steve Kerrison, David May and Kerstin Eder", "title": "A Benes Based NoC Switching Architecture for Mixed Criticality Embedded\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-core, Mixed Criticality Embedded (MCE) real-time systems require high\ntiming precision and predictability to guarantee there will be no interference\nbetween tasks. These guarantees are necessary in application areas such as\navionics and automotive, where task interference or missed deadlines could be\ncatastrophic, and safety requirements are strict. In modern multi-core systems,\nthe interconnect becomes a potential point of uncertainty, introducing major\nchallenges in proving behaviour is always within specified constraints,\nlimiting the means of growing system performance to add more tasks, or provide\nmore computational resources to existing tasks.\n  We present MCENoC, a Network-on-Chip (NoC) switching architecture that\nprovides innovations to overcome this with predictable, formally verifiable\ntiming behaviour that is consistent across the whole NoC. We show how the\nfundamental properties of Benes networks benefit MCE applications and meet our\narchitecture requirements. Using SystemVerilog Assertions (SVA), formal\nproperties are defined that aid the refinement of the specification of the\ndesign as well as enabling the implementation to be exhaustively formally\nverified. We demonstrate the performance of the design in terms of size,\nthroughput and predictability, and discuss the application level considerations\nneeded to exploit this architecture.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:19:10 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Kerrison", "Steve", ""], ["May", "David", ""], ["Eder", "Kerstin", ""]]}]