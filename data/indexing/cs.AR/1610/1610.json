[{"id": "1610.00751", "submitter": "Marcelo Berejuck", "authors": "Marcelo Daniel Berejuck", "title": "An overview about Networks-on-Chip with multicast suppor", "comments": "06 pages, 02 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern System-on-Chip (SoC) platforms typically consist of multiple\nprocessors and a communication interconnect between them. Network-on-Chip (NoC)\narises as a solution to interconnect these systems, which provides a scalable,\nreusable, and an efficient interconnect. For these SoC platforms, multicast\ncommunication is significantly used for parallel applications. Cache coherency\nin distributed sharedmemory,clock synchronization, replication, or barrier\nsynchronization are examples of these requests. This paper presents an overview\nof research on NoC with support for multicast communication and delineates the\nmajor issues addressed so far by the scientific community in this investigation\narea.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:59:17 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Berejuck", "Marcelo Daniel", ""]]}, {"id": "1610.01832", "submitter": "Andreas Olofsson", "authors": "Andreas Olofsson", "title": "Epiphany-V: A 1024 processor 64-bit RISC System-On-Chip", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design of a 1024-core processor chip in 16nm FinFet\ntechnology. The chip (\"Epiphany-V\") contains an array of 1024 64-bit RISC\nprocessors, 64MB of on-chip SRAM, three 136-bit wide mesh Networks-On-Chip, and\n1024 programmable IO pins. The chip has taped out and is being manufactured by\nTSMC.\n  This research was developed with funding from the Defense Advanced Research\nProjects Agency (DARPA). The views, opinions and/or findings expressed are\nthose of the author and should not be interpreted as representing the official\nviews or policies of the Department of Defense or the U.S. Government.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 12:05:14 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Olofsson", "Andreas", ""]]}, {"id": "1610.02094", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Andrew Wright, Daniel Sanchez and Arvind", "title": "Validating Simplified Processor Models in Architectural Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cycle-accurate software simulation of multicores with complex\nmicroarchitectures is often excruciatingly slow. People use simplified core\nmodels to gain simulation speed. However, a persistent question is to what\nextent the results derived from a simplified core model can be used to\ncharacterize the behavior of a real machine.\n  We propose a new methodology of validating simplified simulation models,\nwhich focuses on the trends of metric values across benchmarks and\narchitectures, instead of errors of absolute metric values. To illustrate this\nmethodology, we conduct a case study using an FPGA-accelerated cycle-accurate\nfull system simulator. We evaluated three cache replacement polices on a\n10-stage in-order core model, and then re-conducted all the experiments by\nsubstituting a 1-IPC core model for the 10-stage core model. We found that the\n1-IPC core model generally produces qualitatively the same results as the\naccurate core model except for a few mismatches. We argue that most observed\nmismatches were either indistinguishable from experimental noise or\ncorresponded to the cases where the policy differences even in the accurate\nmodel showed inconclusive results. We think it is fair to use simplified core\nmodels to study a feature once the influence of the simplification is\nunderstood. Additional studies on branch predictors and scaling properties of\nmultithread benchmarks reinforce our argument. However, the validation of a\nsimplified model requires a detailed cycle-accurate model!\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 23:21:05 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Wright", "Andrew", ""], ["Sanchez", "Daniel", ""], ["Arvind", "", ""]]}, {"id": "1610.02273", "submitter": "Hyeokjun Choe", "authors": "Hyeokjun Choe, Seil Lee, Hyunha Nam, Seongsik Park, Seijoon Kim,\n  Eui-Young Chung, Sungroh Yoon", "title": "Near-Data Processing for Differentiable Machine Learning Models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-data processing (NDP) refers to augmenting memory or storage with\nprocessing power. Despite its potential for acceleration computing and reducing\npower requirements, only limited progress has been made in popularizing NDP for\nvarious reasons. Recently, two major changes have occurred that have ignited\nrenewed interest and caused a resurgence of NDP. The first is the success of\nmachine learning (ML), which often demands a great deal of computation for\ntraining, requiring frequent transfers of big data. The second is the\npopularity of NAND flash-based solid-state drives (SSDs) containing multicore\nprocessors that can accommodate extra computation for data processing. In this\npaper, we evaluate the potential of NDP for ML using a new SSD platform that\nallows us to simulate instorage processing (ISP) of ML workloads. Our platform\n(named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD\nthat can execute various ML algorithms using data stored in the SSD. To conduct\na thorough performance analysis and an in-depth comparison with alternative\ntechniques, we focus on a specific algorithm: stochastic gradient descent\n(SGD), which is the de facto standard for training differentiable models such\nas logistic regression and neural networks. We implement and compare three SGD\nvariants (synchronous, Downpour, and elastic averaging) using ISP-ML,\nexploiting the multiple NAND channels to parallelize SGD. In addition, we\ncompare the performance of ISP and that of conventional in-host processing,\nrevealing the advantages of ISP. Based on the advantages and limitations\nidentified through our experiments, we further discuss directions for future\nresearch on ISP for accelerating ML.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 05:28:33 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 01:58:47 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 03:57:56 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Choe", "Hyeokjun", ""], ["Lee", "Seil", ""], ["Nam", "Hyunha", ""], ["Park", "Seongsik", ""], ["Kim", "Seijoon", ""], ["Chung", "Eui-Young", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1610.03360", "submitter": "Philipp F\\\"odisch", "authors": "Philipp F\\\"odisch, Artsiom Bryksa, Bert Lange, Wolfgang Enghardt,\n  Peter Kaever", "title": "Implementing High-Order FIR Filters in FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary field-programmable gate arrays (FPGAs) are predestined for the\napplication of finite impulse response (FIR) filters. Their embedded digital\nsignal processing (DSP) blocks for multiply-accumulate operations enable\nefficient fixed-point computations, in cases where the filter structure is\naccurately mapped to the dedicated hardware architecture. This brief presents a\ngeneric systolic structure for high-order FIR filters, efficiently exploiting\nthe hardware resources of an FPGA in terms of routability and timing. Although\nthis seems to be an easily implementable task, the synthesizing tools require\nan adaptation of the straightforward digital filter implementation for an\noptimal mapping. Using the example of a symmetric FIR filter with 90 taps, we\ndemonstrate the performance of the proposed structure with FPGAs from Xilinx\nand Altera. The implementation utilizes less than 1% of slice logic and runs at\nclock frequencies up to 526 MHz. Moreover, an enhancement of the structure\nultimately provides an extended dynamic range for the quantized coefficients\nwithout the costs of additional slice logic.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:24:52 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 11:23:27 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["F\u00f6disch", "Philipp", ""], ["Bryksa", "Artsiom", ""], ["Lange", "Bert", ""], ["Enghardt", "Wolfgang", ""], ["Kaever", "Peter", ""]]}, {"id": "1610.05601", "submitter": "Zhehao Li", "authors": "Zhehao Li, Jifang Jin, Lingli Wang", "title": "High-performance K-means Implementation based on a Simplified Map-Reduce\n  Architecture", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-means algorithm is one of the most common clustering algorithms and\nwidely used in data mining and pattern recognition. The increasing\ncomputational requirement of big data applications makes hardware acceleration\nfor the k-means algorithm necessary. In this paper, a simplified Map-Reduce\narchitecture is proposed to implement the k-means algorithm on an FPGA.\nAlgorithmic segmentation, data path elaboration and automatic control are\napplied to optimize the architecture for high performance. In addition, high\nlevel synthesis technique is utilized to reduce development cycles and\ncomplexity. For a single iteration in the k-means algorithm, a throughput of\n28.74 Gbps is achieved. The performance shows at least 3.93x speedup compared\nwith four representative existing FPGA-based implementations and can satisfy\nthe demand of big data applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 13:46:43 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 07:47:29 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 16:20:05 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Li", "Zhehao", ""], ["Jin", "Jifang", ""], ["Wang", "Lingli", ""]]}, {"id": "1610.06050", "submitter": "Fran\\c{c}ois Leduc-Primeau", "authors": "Carlo Condo, Pascal Giard, Fran\\c{c}ois Leduc-Primeau, Gabi Sarkis and\n  Warren J. Gross", "title": "A 9.52 dB NCG FEC scheme and 164 bits/cycle low-complexity product\n  decoder architecture", "comments": null, "journal-ref": null, "doi": "10.1109/TCSI.2017.2745902", "report-no": null, "categories": "cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful Forward Error Correction (FEC) schemes are used in optical\ncommunications to achieve bit-error rates below $10^{-15}$. These FECs follow\none of two approaches: concatenation of simpler hard-decision codes or usage of\ninherently powerful soft-decision codes. The first approach yields lower Net\nCoding Gains (NCGs), but can usually work at higher code rates and have lower\ncomplexity decoders. In this work, we propose a novel FEC scheme based on a\nproduct code and a post-processing technique. It can achieve an NCG of 9.52~dB\nat a BER of $10^{-15}$ and 9.96~dB at a BER of $10^{-18}$, an error-correction\nperformance that sits between that of current hard-decision and soft-decision\nFECs. A decoder architecture is designed, tested on FPGA and synthesized in 65\nnm CMOS technology: its 164 bits/cycle worst-case information throughput can\nreach 100 Gb/s at the achieved frequency of 609~MHz. Its complexity is shown to\nbe lower than that of hard-decision decoders in literature, and an order of\nmagnitude lower than the estimated complexity of soft-decision decoders.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 11:46:19 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 16:56:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Condo", "Carlo", ""], ["Giard", "Pascal", ""], ["Leduc-Primeau", "Fran\u00e7ois", ""], ["Sarkis", "Gabi", ""], ["Gross", "Warren J.", ""]]}, {"id": "1610.06088", "submitter": "Lafifa Jamal", "authors": "Sadia Nowrin, Papiya Nazneen and Lafifa Jamal", "title": "Design of a Compact Reversible Read-Only-Memory with MOS Transistors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy conservative devices are the need of the modern technology which leads\nto the development of reversible logic. The synthesis of reversible logic has\nbecome an intensely studied area as it overcomes the problem of power\ndissipation associated with irreversibility. Storage device such as\nRead-Only-Memory (ROM) can be realized in a reversible way with low power\ndissipation. The reversibility of ROM has not been yet realized in literature\nand hence, this paper presents a novel reversible ROM with its Complementary\nMetal Oxide Semiconductor (CMOS) realization. On the way to present the\narchitecture of reversible ROM, we propose a new reversible gate named as\nNowrin Papiya (NP) gate. All the proposed circuits and gates are realized with\nCMOS based pass transistor logic. Finally, an algorithm as well as several\ntheorems on the numbers of gates, transistors and garbage outputs have been\npresented to show the optimality of the reversible ROM. Simulations using\nMicrowind DSCH software has been shown to verify the correctness of the\nproposed design. The comparative results prove that the proposed designs are\nefficient and optimized in terms of numbers of gates, transistors, garbage\noutputs, quantum cost and delay.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 16:44:56 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Nowrin", "Sadia", ""], ["Nazneen", "Papiya", ""], ["Jamal", "Lafifa", ""]]}, {"id": "1610.06385", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, and Ranjani Narayan", "title": "Accelerating BLAS on Custom Architecture through Algorithm-Architecture\n  Co-design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) play key role in high performance and\nscientific computing applications. Experimentally, yesteryear multicore and\nGeneral Purpose Graphics Processing Units (GPGPUs) are capable of achieving up\nto 15 to 57% of the theoretical peak performance at 65W to 240W respectively\nfor compute bound operations like Double/Single Precision General Matrix\nMultiplication (XGEMM). For bandwidth bound operations like Single/Double\nprecision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to\n7% of the theoretical peak performance in multicores and GPGPUs respectively.\nAchieving performance in BLAS requires moving away from conventional wisdom and\nevolving towards customized accelerator tailored for BLAS through\nalgorithm-architecture co-design. In this paper, we present acceleration of\nLevel-1 (vector operations), Level-2 (matrix-vector operations), and Level-3\n(matrix-matrix operations) BLAS through algorithm architecture co-design on a\nCoarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a\nplatform for our experiments since REDEFINE can be adapted to support domain of\ninterest through tailor-made Custom Function Units (CFUs). For efficient\nsequential realization of BLAS, we present design of a Processing Element (PE)\nand perform micro-architectural enhancements in the PE to achieve up-to 74% of\nthe theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double\nprecision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and\nshow the scalability of our solution. Finally, we show performance improvement\nof 3-140x in PE over commercially available Intel micro-architectures,\nClearSpeed CSX700, FPGA, and Nvidia GPGPUs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 12:46:07 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:37:28 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 04:26:29 GMT"}, {"version": "v4", "created": "Wed, 23 Nov 2016 14:23:17 GMT"}, {"version": "v5", "created": "Sun, 27 Nov 2016 14:11:36 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1610.06920", "submitter": "Jorge Albericio", "authors": "J. Albericio, P. Judd, A. Delm\\'as, S. Sharify, A. Moshovos", "title": "Bit-pragmatic Deep Neural Network Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify a source of ineffectual computations when processing the\nmultiplications of the convolutional layers in Deep Neural Networks (DNNs) and\npropose Pragmatic (PRA), an architecture that exploits it improving performance\nand energy efficiency. The source of these ineffectual computations is best\nunderstood in the context of conventional multipliers which generate internally\nmultiple terms, that is, products of the multiplicand and powers of two, which\nadded together produce the final product [1]. At runtime, many of these terms\nare zero as they are generated when the multiplicand is combined with the\nzero-bits of the multiplicator. While conventional bit-parallel multipliers\ncalculate all terms in parallel to reduce individual product latency, PRA\ncalculates only the non-zero terms using a) on-the-fly conversion of the\nmultiplicator representation into an explicit list of powers of two, and b)\nhybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA\nexploits two sources of ineffectual computations: 1) the aforementioned zero\nproduct terms which are the result of the lack of explicitness in the\nmultiplicator representation, and 2) the excess in the representation precision\nused for both multiplicants and multiplicators, e.g., [2]. Measurements\ndemonstrate that for the convolutional layers, a straightforward variant of PRA\nimproves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by\n1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on\naverage compared to DaDN and STR. An improved cross lane synchronication scheme\nboosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits\npersist even with an 8-bit quantized representation [5].\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 22:16:05 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Albericio", "J.", ""], ["Judd", "P.", ""], ["Delm\u00e1s", "A.", ""], ["Sharify", "S.", ""], ["Moshovos", "A.", ""]]}, {"id": "1610.07501", "submitter": "Mingu Kang", "authors": "Mingu Kang, Sujan Gonugondla, Ameya Patil, Naresh Shanbhag", "title": "A 481pJ/decision 3.4M decision/s Multifunctional Deep In-memory\n  Inference Processor using Standard 6T SRAM Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a multi-functional deep in-memory processor for\ninference applications. Deep in-memory processing is achieved by embedding\npitch-matched low-SNR analog processing into a standard 6T 16KB SRAM array in\n65 nm CMOS. Four applications are demonstrated. The prototype achieves up to\n5.6X (9.7X estimated for multi-bank scenario) energy savings with negligible\n(<1%) accuracy degradation in all four applications as compared to the\nconventional architecture.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 17:24:56 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Kang", "Mingu", ""], ["Gonugondla", "Sujan", ""], ["Patil", "Ameya", ""], ["Shanbhag", "Naresh", ""]]}, {"id": "1610.08705", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Anupam Chattopadhyay, Soumyendu Raha, S K Nandy,\n  Ranjani Narayan", "title": "Accelerating BLAS and LAPACK via Efficient Floating Point Architecture\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK)\nform basic building blocks for several High Performance Computing (HPC)\napplications and hence dictate performance of the HPC applications. Performance\nin such tuned packages is attained through tuning of several algorithmic and\narchitectural parameters such as number of parallel operations in the Directed\nAcyclic Graph of the BLAS/LAPACK routines, sizes of the memories in the memory\nhierarchy of the underlying platform, bandwidth of the memory, and structure of\nthe compute resources in the underlying platform. In this paper, we closely\ninvestigate the impact of the Floating Point Unit (FPU) micro-architecture for\nperformance tuning of BLAS and LAPACK. We present theoretical analysis for\npipeline depth of different floating point operations like multiplier, adder,\nsquare root, and divider followed by characterization of BLAS and LAPACK to\ndetermine several parameters required in the theoretical framework for deciding\noptimum pipeline depth of the floating operations. A simple design of a\nProcessing Element (PE) is presented and shown that the PE outperforms the most\nrecent custom realizations of BLAS and LAPACK by 1.1X to 1.5X in Gflops/W, and\n1.9X to 2.1X in Gflops/mm^2.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 11:02:38 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:04:32 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 17:30:38 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Merchant", "Farhad", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1610.09603", "submitter": "Vivek Seshadri", "authors": "Vivek Seshadri, Onur Mutlu", "title": "The Processing Using Memory Paradigm:In-DRAM Bulk Copy, Initialization,\n  Bitwise AND and OR", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.06483", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing systems, the off-chip memory interface allows the memory\ncontroller to perform only read or write operations. Therefore, to perform any\noperation, the processor must first read the source data and then write the\nresult back to memory after performing the operation. This approach consumes\nhigh latency, bandwidth, and energy for operations that work on a large amount\nof data. Several works have proposed techniques to process data near memory by\nadding a small amount of compute logic closer to the main memory chips. In this\narticle, we describe two techniques proposed by recent works that take this\napproach of processing in memory further by exploiting the underlying operation\nof the main memory technology to perform more complex tasks. First, we describe\nRowClone, a mechanism that exploits DRAM technology to perform bulk copy and\ninitialization operations completely inside main memory. We then describe a\ncomplementary work that uses DRAM to perform bulk bitwise AND and OR operations\ninside main memory. These two techniques significantly improve the performance\nand energy efficiency of the respective operations.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 05:01:54 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Seshadri", "Vivek", ""], ["Mutlu", "Onur", ""]]}, {"id": "1610.09604", "submitter": "Donghyuk Lee", "authors": "Donghyuk Lee, Samira Khan, Lavanya Subramanian, Saugata Ghose, Rachata\n  Ausavarungnirun, Gennady Pekhimenko, Vivek Seshadri, Onur Mutlu", "title": "Understanding and Exploiting Design-Induced Latency Variation in Modern\n  DRAM Chips", "comments": "This paper is a two column version of the paper, D. Lee et al.,\n  \"Design-Induced Latency Variation in Modern DRAM Chips: Characterization,\n  Analysis, and Latency Reduction Mechanisms\", SIGMETRICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variation has been shown to exist across the cells within a modern DRAM chip.\nWe empirically demonstrate a new form of variation that exists within a real\nDRAM chip, induced by the design and placement of different components in the\nDRAM chip. Our goals are to understand design-induced variation that exists in\nreal, state-of-the-art DRAM chips, exploit it to develop low-cost mechanisms\nthat can dynamically find and use the lowest latency at which to operate a DRAM\nchip reliably, and, thus, improve overall system performance while ensuring\nreliable system operation.\n  To this end, we first experimentally demonstrate and analyze designed-induced\nvariation in modern DRAM devices by testing and characterizing 96 DIMMs (768\nDRAM chips). Our characterization identifies DRAM regions that are vulnerable\nto errors, if operated at lower latency, and finds consistency in their\nlocations across a given DRAM chip generation, due to design-induced variation.\nBased on our extensive experimental analysis, we develop two mechanisms that\nreliably reduce DRAM latency. First, DIVA Profiling uses runtime profiling to\ndynamically identify the lowest DRAM latency that does not introduce failures.\nDIVA Profiling exploits design-induced variation and periodically profiles only\nthe vulnerable regions to determine the lowest DRAM latency at low cost. Our\nsecond mechanism, DIVA Shuffling, shuffles data such that values stored in\nvulnerable regions are mapped to multiple error-correcting code (ECC)\ncodewords. Combined together, our two mechanisms reduce read/write latency by\n40.0%/60.5%, which translates to an overall system performance improvement of\n14.7%/13.7%/13.8% (in 2-/4-/8-core systems) across a variety of workloads,\nwhile ensuring reliable operation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 05:21:07 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 20:37:29 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Lee", "Donghyuk", ""], ["Khan", "Samira", ""], ["Subramanian", "Lavanya", ""], ["Ghose", "Saugata", ""], ["Ausavarungnirun", "Rachata", ""], ["Pekhimenko", "Gennady", ""], ["Seshadri", "Vivek", ""], ["Mutlu", "Onur", ""]]}, {"id": "1610.09761", "submitter": "Zhenman Fang", "authors": "Yu-Ting Chen, Jason Cong, Zhenman Fang, Bingjun Xiao, Peipei Zhou", "title": "ARAPrototyper: Enabling Rapid Prototyping and Evaluation for\n  Accelerator-Rich Architectures", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to conventional general-purpose processors, accelerator-rich\narchitectures (ARAs) can provide orders-of-magnitude performance and energy\ngains and are emerging as one of the most promising solutions in the age of\ndark silicon. However, many design issues related to the complex interaction\nbetween general-purpose cores, accelerators, customized on-chip interconnects,\nand memory systems remain unclear and difficult to evaluate.\n  In this paper we design and implement the ARAPrototyper to enable rapid\ndesign space explorations for ARAs in real silicons and reduce the tedious\nprototyping efforts far down to manageable efforts. First, ARAPrototyper\nprovides a reusable baseline prototype with a highly customizable memory\nsystem, including interconnect between accelerators and buffers, interconnect\nbetween buffers and last-level cache (LLC) or DRAM, coherency choice at LLC or\nDRAM, and address translation support. Second, ARAPrototyper provides a clean\ninterface to quickly integrate users' own accelerators written in high-level\nsynthesis (HLS) code. The whole design flow is highly automated to generate a\nprototype of ARA on an FPGA system-on-chip (SoC). Third, to quickly develop\napplications that run seamlessly on the ARA prototype, ARAPrototyper provides a\nsystem software stack, abstracts the accelerators as software libraries, and\nprovides APIs for software developers. Our experimental results demonstrate\nthat ARAPrototyper enables a wide range of design space explorations for ARAs\nat manageable prototyping efforts, which has 4,000X to 10,000X faster\nevaluation time than full-system simulations. We believe that ARAPrototyper can\nbe an attractive alternative for ARA design and evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 02:29:23 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Chen", "Yu-Ting", ""], ["Cong", "Jason", ""], ["Fang", "Zhenman", ""], ["Xiao", "Bingjun", ""], ["Zhou", "Peipei", ""]]}]