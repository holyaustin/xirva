[{"id": "2101.00055", "submitter": "Abhijit Das", "authors": "Abhijit Das, John Jose and Prabhat Mishra", "title": "Data Criticality in Multi-Threaded Applications: An Insight for\n  Many-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-threaded applications are capable of exploiting the full potential of\nmany-core systems. However, Network-on-Chip (NoC) based inter-core\ncommunication in many-core systems is responsible for 60-75% of the miss\nlatency experienced by multi-threaded applications. Delay in the arrival of\ncritical data at the requesting core severely hampers performance. This brief\npresents some interesting insights about how critical data is requested from\nthe memory by multi-threaded applications. Then it investigates the cause of\ndelay in NoC and how it affects the performance. Finally, this brief shows how\nNoC-aware memory access optimisations can significantly improve performance.\nOur experimental evaluation considers early restart memory access optimisation\nand demonstrates that by exploiting NoC resources, critical data can be\nprioritised to reduce miss penalty by 10-12% and improve system performance by\n7-11%.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 20:13:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Das", "Abhijit", ""], ["Jose", "John", ""], ["Mishra", "Prabhat", ""]]}, {"id": "2101.00557", "submitter": "Sairam Sri Vatsavai", "authors": "Sairam Sri Vatsavai, Ishan Thakkar", "title": "Silicon Photonic Microring Based Chip-Scale Accelerator for Delayed\n  Feedback Reservoir Computing", "comments": "Paper accepted at VLSID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform temporal and sequential machine learning tasks, the use of\nconventional Recurrent Neural Networks (RNNs) has been dwindling due to the\ntraining complexities of RNNs. To this end, accelerators for delayed feedback\nreservoir computing (DFRC) have attracted attention in lieu of RNNs, due to\ntheir simple hardware implementations. A typical implementation of a DFRC\naccelerator consists of a delay loop and a single nonlinear neuron, together\nacting as multiple virtual nodes for computing. In prior work, photonic DFRC\naccelerators have shown an undisputed advantage of fast computation over their\nelectronic counterparts. In this paper, we propose a more energy-efficient\nchip-scale DFRC accelerator that employs a silicon photonic microring (MR)\nbased nonlinear neuron along with on-chip photonic waveguides-based delayed\nfeedback loop. Our evaluations show that, compared to a well-known photonic\nDFRC accelerator from prior work, our proposed MR-based DFRC accelerator\nachieves 35% and 98.7% lower normalized root mean square error (NRMSE),\nrespectively, for the prediction tasks of NARMA10 and Santa Fe time series. In\naddition, our MR-based DFRC accelerator achieves 58.8% lower symbol error rate\n(SER) for the Non-Linear Channel Equalization task. Moreover, our MR-based DFRC\naccelerator has 98% and 93% faster training time, respectively, compared to an\nelectronic and a photonic DFRC accelerators from prior work.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 04:13:30 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Vatsavai", "Sairam Sri", ""], ["Thakkar", "Ishan", ""]]}, {"id": "2101.00587", "submitter": "Lorenzo Ferretti", "authors": "Lorenzo Ferretti, Jihye Kwon, Giovanni Ansaloni, Giuseppe Di\n  Guglielmo, Luca Carloni, Laura Pozzi", "title": "DB4HLS: A Database of High-Level Synthesis Design Space Explorations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  High-Level Synthesis (HLS) frameworks allow to easily specify a large number\nof variants of the same hardware design by only acting on optimization\ndirectives. Nonetheless, the hardware synthesis of implementations for all\npossible combinations of directive values is impractical even for simple\ndesigns. Addressing this shortcoming, many HLS Design Space Exploration (DSE)\nstrategies have been proposed to devise directive settings leading to\nhigh-quality implementations while limiting the number of synthesis runs. All\nthese works require considerable efforts to validate the proposed strategies\nand/or to build the knowledge base employed to tune abstract models, as both\ntasks mandate the syntheses of large collections of implementations. Currently,\nsuch data gathering is performed ad-hoc, a) leading to a lack of\nstandardization, hampering comparisons between DSE alternatives, and b) posing\na very high burden to researchers willing to develop novel DSE strategies.\nAgainst this backdrop, we here introduce DB4HLS, a database of exhaustive HLS\nexplorations comprising more than 100000 design points collected over 4 years\nof synthesis time. The open structure of DB4HLS allows the incremental\nintegration of new DSEs, which can be easily defined with a dedicated\ndomain-specific language. We think that of our database, available at\nhttps://www.db4hls.inf.usi.ch/, will be a valuable tool for the research\ncommunity investigating automated strategies for the optimization of HLS-based\nhardware designs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:58:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ferretti", "Lorenzo", ""], ["Kwon", "Jihye", ""], ["Ansaloni", "Giovanni", ""], ["Di Guglielmo", "Giuseppe", ""], ["Carloni", "Luca", ""], ["Pozzi", "Laura", ""]]}, {"id": "2101.00793", "submitter": "Karthik E", "authors": "Karthik E", "title": "A Framework for Fast Scalable BNN Inference using Googlenet and Transfer\n  Learning", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate object detection in video and image analysis is one of\nthe major beneficiaries of the advancement in computer vision systems with the\nhelp of deep learning. With the aid of deep learning, more powerful tools\nevolved, which are capable to learn high-level and deeper features and thus can\novercome the existing problems in traditional architectures of object detection\nalgorithms. The work in this thesis aims to achieve high accuracy in object\ndetection with good real-time performance.\n  In the area of computer vision, a lot of research is going into the area of\ndetection and processing of visual information, by improving the existing\nalgorithms. The binarized neural network has shown high performance in various\nvision tasks such as image classification, object detection, and semantic\nsegmentation. The Modified National Institute of Standards and Technology\ndatabase (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street\nView House Numbers (SVHN) datasets are used which is implemented using a\npre-trained convolutional neural network (CNN) that is 22 layers deep.\nSupervised learning is used in the work, which classifies the particular\ndataset with the proper structure of the model. In still images, to improve\naccuracy, Googlenet is used. The final layer of the Googlenet is replaced with\nthe transfer learning to improve the accuracy of the Googlenet. At the same\ntime, the accuracy in moving images can be maintained by transfer learning\ntechniques. Hardware is the main backbone for any model to obtain faster\nresults with a large number of datasets. Here, Nvidia Jetson Nano is used which\nis a graphics processing unit (GPU), that can handle a large number of\ncomputations in the process of object detection. Results show that the accuracy\nof objects detected by the transfer learning method is more when compared to\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:16:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 07:28:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["E", "Karthik", ""]]}, {"id": "2101.00969", "submitter": "Behzad Salami", "authors": "Seyed Saber Nabavi Larimi, Behzad Salami, Osman S. Unsal, Adrian\n  Cristal Kestelman, Hamid Sarbazi-Azad, Onur Mutlu", "title": "Understanding Power Consumption and Reliability of High-Bandwidth Memory\n  with Voltage Underscaling", "comments": "To appear at DATE 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern computing devices employ High-Bandwidth Memory (HBM) to meet their\nmemory bandwidth requirements. An HBM-enabled device consists of multiple DRAM\nlayers stacked on top of one another next to a compute chip (e.g. CPU, GPU, and\nFPGA) in the same package. Although such HBM structures provide high bandwidth\nat a small form factor, the stacked memory layers consume a substantial portion\nof the package's power budget. Therefore, power-saving techniques that preserve\nthe performance of HBM are desirable. Undervolting is one such technique: it\nreduces the supply voltage to decrease power consumption without reducing the\ndevice's operating frequency to avoid performance loss. Undervolting takes\nadvantage of voltage guardbands put in place by manufacturers to ensure correct\noperation under all environmental conditions. However, reducing voltage without\nchanging frequency can lead to reliability issues manifested as unwanted bit\nflips. In this paper, we provide the first experimental study of real HBM chips\nunder reduced-voltage conditions. We show that the guardband regions for our\nHBM chips constitute 19% of the nominal voltage. Pushing the supply voltage\ndown within the guardband region reduces power consumption by a factor of 1.5X\nfor all bandwidth utilization rates. Pushing the voltage down further by 11%\nleads to a total of2.3X power savings at the cost of unwanted bit flips. We\nexplore and characterize the rate and types of these reduced-voltage-induced\nbit flips and present a fault map that enables the possibility of a\nthree-factor trade-off among power, memory capacity, and fault rate.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:08:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Larimi", "Seyed Saber Nabavi", ""], ["Salami", "Behzad", ""], ["Unsal", "Osman S.", ""], ["Kestelman", "Adrian Cristal", ""], ["Sarbazi-Azad", "Hamid", ""], ["Mutlu", "Onur", ""]]}, {"id": "2101.01173", "submitter": "Partha Sarathi Paul", "authors": "Partha Sarathi Paul, Maisha Sadia, Md Sakib Hasan", "title": "Design of a Dynamic Parameter-Controlled Chaotic-PRNG in a 65nm CMOS\n  process", "comments": "Accepted and presented in '14TH IEEE DALLAS CIRCUITS AND SYSTEMS\n  CONFERENCE'. The name is appeared in the schedule of the conference:\n  https://engineering.utdallas.edu/DCAS/schedule.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design of a new chaotic map circuit with a 65nm\nCMOS process. This chaotic map circuit uses a dynamic parameter-control\ntopology and generates a wide chaotic range. We propose two designs of dynamic\nparameter-controlled chaotic map (DPCCM)-based pseudo-random number generators\n(PRNG). The randomness of the generated sequence is verified using three\ndifferent statistical tests, namely, NIST SP 800-22 test, FIPS PUB 140-2 test,\nand Diehard test. Our first design offers a throughput of 200 MS/s with an\non-chip area of 0.024mm2 and a power consumption of 2.33mW. The throughput of\nour second design is 300 MS/s with an area consumption of 0.132mm2 and power\nconsumption of 2.14mW. The wider chaotic range and lower-overhead, offered by\nour designs, can be highly suitable for various applications such as, logic\nobfuscation, chaos-based cryptography, re-configurable random number\ngeneration,and hard-ware security for resource-constrained edge devices like\nIoT.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 20:49:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Paul", "Partha Sarathi", ""], ["Sadia", "Maisha", ""], ["Hasan", "Md Sakib", ""]]}, {"id": "2101.01177", "submitter": "Gihan  Ravideva Mudalige", "authors": "Kamalavasan Kamalakkannan, Gihan R. Mudalige, Istvan Z. Reguly, Suhaib\n  A. Fahmy", "title": "High-Level FPGA Accelerator Design for Structured-Mesh-Based Explicit\n  Numerical Solvers", "comments": "Preprint - Accepted to the 35th IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS 2021), May 2021, Portland, Oregon USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a workflow for synthesizing near-optimal FPGA\nimplementations for structured-mesh based stencil applications for explicit\nsolvers. It leverages key characteristics of the application class, its\ncomputation-communication pattern, and the architectural capabilities of the\nFPGA to accelerate solvers from the high-performance computing domain. Key new\nfeatures of the workflow are (1) the unification of standard state-of-the-art\ntechniques with a number of high-gain optimizations such as batching and\nspatial blocking/tiling, motivated by increasing throughput for real-world work\nloads and (2) the development and use of a predictive analytic model for\nexploring the design space, resource estimates and performance. Three\nrepresentative applications are implemented using the design workflow on a\nXilinx Alveo U280 FPGA, demonstrating near-optimal performance and over 85%\npredictive model accuracy. These are compared with equivalent highly-optimized\nimplementations of the same applications on modern HPC-grade GPUs (Nvidia V100)\nanalyzing time to solution, bandwidth and energy consumption. Performance\nresults indicate equivalent runtime performance of the FPGA implementations to\nthe V100 GPU, with over 2x energy savings, for the largest non-trivial\napplication synthesized on the FPGA compared to the best performing GPU-based\nsolution. Our investigation shows the considerable challenges in gaining high\nperformance on current generation FPGAs compared to traditional architectures.\nWe discuss determinants for a given stencil code to be amenable to FPGA\nimplementation, providing insights into the feasibility and profitability of a\ndesign and its resulting performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:27:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:57:16 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Kamalakkannan", "Kamalavasan", ""], ["Mudalige", "Gihan R.", ""], ["Reguly", "Istvan Z.", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "2101.01284", "submitter": "Thomas M. Conte", "authors": "Thomas M. Conte, Ian T. Foster, William Gropp, and Mark D. Hill", "title": "Advancing Computing's Foundation of US Industry & Society", "comments": "A Computing Community Consortium (CCC) white paper, 4 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020whitepaper_17", "categories": "cs.CY cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While past information technology (IT) advances have transformed society,\nfuture advances hold even greater promise. For example, we have only just begun\nto reap the changes from artificial intelligence (AI), especially machine\nlearning (ML). Underlying IT's impact are the dramatic improvements in computer\nhardware, which deliver performance that unlock new capabilities. For example,\nrecent successes in AI/ML required the synergy of improved algorithms and\nhardware architectures (e.g., general-purpose graphics processing units).\nHowever, unlike in the 20th Century and early 2000s, tomorrow's performance\naspirations must be achieved without continued semiconductor scaling formerly\nprovided by Moore's Law and Dennard Scaling. How will one deliver the next 100x\nimprovement in capability at similar or less cost to enable great value? Can we\nmake the next AI leap without 100x better hardware?\n  This whitepaper argues for a multipronged effort to develop new computing\napproaches beyond Moore's Law to advance the foundation that computing provides\nto US industry, education, medicine, science, and government. This impact\nextends far beyond the IT industry itself, as IT is now central for providing\nvalue across society, for example in semi-autonomous vehicles, tele-education,\nhealth wearables, viral analysis, and efficient administration. Herein we draw\nupon considerable visioning work by CRA's Computing Community Consortium (CCC)\nand the IEEE Rebooting Computing Initiative (IEEE RCI), enabled by thought\nleader input from industry, academia, and the US government.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:40:45 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Conte", "Thomas M.", ""], ["Foster", "Ian T.", ""], ["Gropp", "William", ""], ["Hill", "Mark D.", ""]]}, {"id": "2101.01416", "submitter": "Farhad Merchant", "authors": "Ihsen Alouani, Anouar Ben Khalifa, Farhad Merchant, Rainer Leupers", "title": "An Investigation on Inherent Robustness of Posit Data Representation", "comments": "To appear in VLSID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As the dimensions and operating voltages of computer electronics shrink to\ncope with consumers' demand for higher performance and lower power consumption,\ncircuit sensitivity to soft errors increases dramatically. Recently, a new\ndata-type is proposed in the literature called posit data type. Posit\narithmetic has absolute advantages such as higher numerical accuracy, speed,\nand simpler hardware design than IEEE 754-2008 technical standard-compliant\narithmetic. In this paper, we propose a comparative robustness study between\n32-bit posit and 32-bit IEEE 754-2008 compliant representations. At first, we\npropose a theoretical analysis for IEEE 754 compliant numbers and posit numbers\nfor single bit flip and double bit flips. Then, we conduct exhaustive fault\ninjection experiments that show a considerable inherent resilience in posit\nformat compared to classical IEEE 754 compliant representation. To show a\nrelevant use-case of fault-tolerant applications, we perform experiments on a\nset of machine-learning applications. In more than 95% of the exhaustive fault\ninjection exploration, posit representation is less impacted by faults than the\nIEEE 754 compliant floating-point representation. Moreover, in 100% of the\ntested machine-learning applications, the accuracy of posit-implemented systems\nis higher than the classical floating-point-based ones.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:54:17 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Alouani", "Ihsen", ""], ["Khalifa", "Anouar Ben", ""], ["Merchant", "Farhad", ""], ["Leupers", "Rainer", ""]]}, {"id": "2101.01516", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Best CNTFET Ternary Adders?", "comments": "5 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MUX implementation of ternary half adders and full adders using\npredecessor and successor functions lead to the most efficient efficient\nimplementation using the smallest transistor count. These designs are compared\nwith the binary implementation of the corresponding half adders and full adders\nusing the MUX technique or the typical complementary CMOS circuit style. The\ntransistor count ratio between ternary and binary implementations is always\ngreater than the information ratio ($log_2(3)/log_2(2)$ = 1.585) between\nternary and binary wires.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:06:43 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "2101.01745", "submitter": "Razvan Nane", "authors": "Tom Hogervorst, Tong Dong Qiu, Giacomo Marchiori, Alf Birger, Markus\n  Blatt, Razvan Nane", "title": "Hardware Acceleration of HPC Computational Flow Dynamics using\n  HBM-enabled FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing is at the core of many High-Performance Computing\napplications, including computational flow dynamics. Because of the uttermost\nimportance to simulate increasingly larger computational models, hardware\nacceleration is receiving increased attention due to its potential to maximize\nthe performance of scientific computing. A Field-Programmable Gate Array is a\nreconfigurable hardware accelerator that is fully customizable in terms of\ncomputational resources and memory storage requirements of an application\nduring its lifetime. Therefore, it is an ideal candidate to accelerate\nscientific computing applications because of the possibility to fully customize\nthe memory hierarchy important in irregular applications such as iterative\nlinear solvers found in scientific libraries. In this paper, we study the\npotential of using FPGA in HPC because of the rapid advances in reconfigurable\nhardware, such as the increase in on-chip memory size, increasing number of\nlogic cells, and the integration of High-Bandwidth Memories on board. To\nperform this study, we first propose a novel ILU0 preconditioner tightly\nintegrated with a BiCGStab solver kernel designed using a mixture of High-Level\nSynthesis and Register-Transfer Level hand-coded design. Second, we integrate\nthe developed preconditioned iterative solver in Flow from the Open Porous\nMedia (OPM) project, a state-of-the-art open-source reservoir simulator.\nFinally, we perform a thorough evaluation of the FPGA solver kernel in both\nstandalone mode and integrated into the reservoir simulator that includes all\nthe on-chip URAM and BRAM, on-board High-Bandwidth Memory, and off-chip CPU\nmemory data transfers required in a complex simulator software such as OPM's\nFlow. We evaluate the performance on the Norne field, a real-world case\nreservoir model using a grid with more than 10^5 cells and using 3 unknowns per\ncell.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:17:47 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hogervorst", "Tom", ""], ["Qiu", "Tong Dong", ""], ["Marchiori", "Giacomo", ""], ["Birger", "Alf", ""], ["Blatt", "Markus", ""], ["Nane", "Razvan", ""]]}, {"id": "2101.01751", "submitter": "Sudeep Pasricha", "authors": "Febin P Sunny, Ebadollah Taheri, Mahdi Nikdast, Sudeep Pasricha", "title": "A Survey on Silicon Photonics for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning has led to unprecedented successes in solving some very\ndifficult problems in domains such as computer vision, natural language\nprocessing, and general pattern recognition. These achievements are the\nculmination of decades-long research into better training techniques and deeper\nneural network models, as well as improvements in hardware platforms that are\nused to train and execute the deep neural network models. Many\napplication-specific integrated circuit (ASIC) hardware accelerators for deep\nlearning have garnered interest in recent years due to their improved\nperformance and energy-efficiency over conventional CPU and GPU architectures.\nHowever, these accelerators are constrained by fundamental bottlenecks due to\n1) the slowdown in CMOS scaling, which has limited computational and\nperformance-per-watt capabilities of emerging electronic processors, and 2) the\nuse of metallic interconnects for data movement, which do not scale well and\nare a major cause of bandwidth, latency, and energy inefficiencies in almost\nevery contemporary processor. Silicon photonics has emerged as a promising\nCMOS-compatible alternative to realize a new generation of deep learning\naccelerators that can use light for both communication and computation. This\narticle surveys the landscape of silicon photonics to accelerate deep learning,\nwith a coverage of developments across design abstractions in a bottom-up\nmanner, to convey both the capabilities and limitations of the silicon\nphotonics paradigm in the context of deep learning acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:35:29 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 23:00:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sunny", "Febin P", ""], ["Taheri", "Ebadollah", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2101.02419", "submitter": "Rui Xiso", "authors": "Rui Xiao, Kejie Huang, Yewei Zhang, and Haibin Shen", "title": "A Low Power In-Memory Multiplication andAccumulation Array with Modified\n  Radix-4 Inputand Canonical Signed Digit Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A mass of data transfer between the processing and storage units has been the\nleading bottleneck in modern Von-Neuman computing systems, especially when used\nfor Artificial Intelligence (AI) tasks. Computing-in-Memory (CIM) has shown\ngreat potential to reduce both latency and power consumption. However, the\nconventional analog CIM schemes are suffering from reliability issues, which\nmay significantly degenerate the accuracy of the computation. Recently, CIM\nschemes with digitized input data and weights have been proposed for high\nreliable computing. However, the properties of the digital memory and input\ndata are not fully utilized. This paper presents a novel low power CIM scheme\nto further reduce the power consumption by using a Modified Radix-4 (M-RD4)\nbooth algorithm at the input and a Modified Canonical Signed Digit (M-CSD) for\nthe network weights. The simulation results show that M-Rd4 and M-CSD reduce\nthe ratio of $1\\times1$ by 78.5\\% on LeNet and 80.2\\% on AlexNet, and improve\nthe computing efficiency by 41.6\\% in average. The computing-power rate at the\nfixed-point 8-bit is 60.68 TOPS/s/W.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:50:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Xiao", "Rui", ""], ["Huang", "Kejie", ""], ["Zhang", "Yewei", ""], ["Shen", "Haibin", ""]]}, {"id": "2101.02559", "submitter": "Lois Orosa", "authors": "Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos\n  Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi", "title": "Robust Machine Learning Systems: Challenges, Current Trends,\n  Perspectives, and the Road Ahead", "comments": "Final version appears in https://ieeexplore.ieee.org/document/8979377", "journal-ref": "IEEE Design and Test (Volume: 37, Issue: 2, April 2020): 30-57", "doi": "10.1109/MDAT.2020.2971217", "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) techniques have been rapidly adopted by smart\nCyber-Physical Systems (CPS) and Internet-of-Things (IoT) due to their powerful\ndecision-making capabilities. However, they are vulnerable to various security\nand reliability threats, at both hardware and software levels, that compromise\ntheir accuracy. These threats get aggravated in emerging edge ML devices that\nhave stringent constraints in terms of resources (e.g., compute, memory,\npower/energy), and that therefore cannot employ costly security and reliability\nmeasures. Security, reliability, and vulnerability mitigation techniques span\nfrom network security measures to hardware protection, with an increased\ninterest towards formal verification of trained ML models.\n  This paper summarizes the prominent vulnerabilities of modern ML systems,\nhighlights successful defenses and mitigation techniques against these\nvulnerabilities, both at the cloud (i.e., during the ML training phase) and\nedge (i.e., during the ML inference stage), discusses the implications of a\nresource-constrained design on the reliability and security of the system,\nidentifies verification methodologies to ensure correct system behavior, and\ndescribes open research challenges for building secure and reliable ML systems\nat both the edge and the cloud.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:06:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shafique", "Muhammad", ""], ["Naseer", "Mahum", ""], ["Theocharides", "Theocharis", ""], ["Kyrkou", "Christos", ""], ["Mutlu", "Onur", ""], ["Orosa", "Lois", ""], ["Choi", "Jungwook", ""]]}, {"id": "2101.02577", "submitter": "Yuntao Liu", "authors": "Yuntao Liu, Michael Zuzak, Yang Xie, Abhishek Chakraborty, Ankur\n  Srivastava", "title": "Robust and Attack Resilient Logic Locking with a High Application-Level\n  Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logic locking is a hardware security technique to intellectual property (IP)\nagainst security threats in the IC supply chain, especially untrusted fabs.\nSuch techniques incorporate additional locking circuitry within an IC that\ninduces incorrect functionality when an incorrect key is provided. The amount\nof error induced is known as the effectiveness of the locking technique. \"SAT\nattacks\" provide a strong mathematical formulation to find the correct key of\nlocked circuits. In order to achieve high SAT resilience(i.e. complexity of SAT\nattacks), many conventional logic locking schemes fail to inject sufficient\nerror into the circuit. For example, in the case of SARLock and Anti-SAT, there\nare usually very few (or only one) input minterms that cause any error at the\ncircuit output. The state-of-the-art stripped functionality logic locking\n(SFLL) technique introduced a trade-off between SAT resilience and\neffectiveness. In this work, we prove that such a trade-off is universal in\nlogic locking. In order to attain high effectiveness of locking without\ncompromising SAT resilience, we propose a novel logic locking scheme, called\nStrong Anti-SAT (SAS). In addition to SAT attacks, removal-based attacks are\nalso popular against logic locking. Based on SAS, we propose Robust SAS (RSAS)\nwhich is resilient to removal attacks and maintains the same SAT resilience and\nas effectiveness as SAS. SAS and RSAS have the following significant\nimprovements over existing techniques. (1) SAT resilience of SAS and RSAS\nagainst SAT attack is not compromised by increase in effectiveness. (2) In\ncontrast to prior work focusing solely on the circuit-level locking impact, we\nintegrate SAS-locked modules into a processor and show that SAS has a high\napplication-level impact. (3) Our experiments show that SAS and RSAS exhibit\nbetter SAT resilience than SFLL and have similar effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 15:01:31 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Liu", "Yuntao", ""], ["Zuzak", "Michael", ""], ["Xie", "Yang", ""], ["Chakraborty", "Abhishek", ""], ["Srivastava", "Ankur", ""]]}, {"id": "2101.02667", "submitter": "Seyed Abolfazl Ghasemzadeh", "authors": "Seyed Abolfazl Ghasemzadeh, Erfan Bank Tavakoli, Mehdi Kamal, Ali\n  Afzali-Kusha, Massoud Pedram", "title": "BRDS: An FPGA-based LSTM Accelerator with Row-Balanced Dual-Ratio\n  Sparsification", "comments": "8 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, first, a hardware-friendly pruning algorithm for reducing\nenergy consumption and improving the speed of Long Short-Term Memory (LSTM)\nneural network accelerators is presented. Next, an FPGA-based platform for\nefficient execution of the pruned networks based on the proposed algorithm is\nintroduced. By considering the sensitivity of two weight matrices of the LSTM\nmodels in pruning, different sparsity ratios (i.e., dual-ratio sparsity) are\napplied to these weight matrices. To reduce memory accesses, a row-wise\nsparsity pattern is adopted. The proposed hardware architecture makes use of\ncomputation overlapping and pipelining to achieve low-power and high-speed. The\neffectiveness of the proposed pruning algorithm and accelerator is assessed\nunder some benchmarks for natural language processing, binary sentiment\nclassification, and speech recognition. Results show that, e.g., compared to a\nrecently published work in this field, the proposed accelerator could provide\nup to 272% higher effective GOPS/W and the perplexity error is reduced by up to\n1.4% for the PTB dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:23:48 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ghasemzadeh", "Seyed Abolfazl", ""], ["Tavakoli", "Erfan Bank", ""], ["Kamal", "Mehdi", ""], ["Afzali-Kusha", "Ali", ""], ["Pedram", "Massoud", ""]]}, {"id": "2101.02729", "submitter": "Prabuddha Chakraborty", "authors": "Prabuddha Chakraborty and Swarup Bhunia", "title": "Neural Storage: A New Paradigm of Elastic Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage and retrieval of data in a computer memory plays a major role in\nsystem performance. Traditionally, computer memory organization is static -\ni.e., they do not change based on the application-specific characteristics in\nmemory access behaviour during system operation. Specifically, the association\nof a data block with a search pattern (or cues) as well as the granularity of a\nstored data do not evolve. Such a static nature of computer memory, we observe,\nnot only limits the amount of data we can store in a given physical storage,\nbut it also misses the opportunity for dramatic performance improvement in\nvarious applications. On the contrary, human memory is characterized by\nseemingly infinite plasticity in storing and retrieving data - as well as\ndynamically creating/updating the associations between data and corresponding\ncues. In this paper, we introduce Neural Storage (NS), a brain-inspired\nlearning memory paradigm that organizes the memory as a flexible neural memory\nnetwork. In NS, the network structure, strength of associations, and\ngranularity of the data adjust continuously during system operation, providing\nunprecedented plasticity and performance benefits. We present the associated\nstorage/retrieval/retention algorithms in NS, which integrate a formalized\nlearning process. Using a full-blown operational model, we demonstrate that NS\nachieves an order of magnitude improvement in memory access performance for two\nrepresentative applications when compared to traditional content-based memory.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:19:25 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Chakraborty", "Prabuddha", ""], ["Bhunia", "Swarup", ""]]}, {"id": "2101.02860", "submitter": "Khaza Anuarul Hoque", "authors": "Ayesha Siddique, Kanad Basu, Khaza Anuarul Hoque", "title": "Exploring Fault-Energy Trade-offs in Approximate DNN Hardware\n  Accelerators", "comments": "Accepted for publication in the The 22nd International Symposium on\n  Quality Electronic Design (ISQED'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systolic array-based deep neural network (DNN) accelerators have recently\ngained prominence for their low computational cost. However, their high energy\nconsumption poses a bottleneck to their deployment in energy-constrained\ndevices. To address this problem, approximate computing can be employed at the\ncost of some tolerable accuracy loss. However, such small accuracy variations\nmay increase the sensitivity of DNNs towards undesired subtle disturbances,\nsuch as permanent faults. The impact of permanent faults in accurate DNNs has\nbeen thoroughly investigated in the literature. Conversely, the impact of\npermanent faults in approximate DNN accelerators (AxDNNs) is yet\nunder-explored. The impact of such faults may vary with the fault bit\npositions, activation functions and approximation errors in AxDNN layers. Such\ndynamacity poses a considerable challenge to exploring the trade-off between\ntheir energy efficiency and fault resilience in AxDNNs. Towards this, we\npresent an extensive layer-wise and bit-wise fault resilience and energy\nanalysis of different AxDNNs, using the state-of-the-art Evoapprox8b signed\nmultipliers. In particular, we vary the stuck-at-0, stuck-at-1 fault-bit\npositions, and activation functions to study their impact using the most widely\nused MNIST and Fashion-MNIST datasets. Our quantitative analysis shows that the\npermanent faults exacerbate the accuracy loss in AxDNNs when compared to the\naccurate DNN accelerators. For instance, a permanent fault in AxDNNs can lead\nup to 66\\% accuracy loss, whereas the same faulty bit can lead to only 9\\%\naccuracy loss in an accurate DNN accelerator. Our results demonstrate that the\nfault resilience in AxDNNs is orthogonal to the energy efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 05:52:12 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Siddique", "Ayesha", ""], ["Basu", "Kanad", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "2101.04395", "submitter": "Michael Witterauf", "authors": "Michael Witterauf and Dominik Walter and Frank Hannig and J\\\"urgen\n  Teich", "title": "Symbolic Loop Compilation for Tightly Coupled Processor Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop compilation for Tightly Coupled Processor Arrays (TCPAs), a class of\nmassively parallel loop accelerators, entails solving NP-hard problems, yet\ndepends on the loop bounds and number of available processing elements (PEs),\nparameters known only at runtime because of dynamic resource management and\ninput sizes. Therefore, this article proposes a two-phase approach called\nsymbolic loop compilation: At compile time, the necessary NP-complete problems\nare solved and the solutions compiled into a space-efficient symbolic\nconfiguration. At runtime, a concrete configuration is generated from the\nsymbolic configuration according to the parameters values. We show that the\nlatter phase, called instantiation, runs in polynomial time with its most\ncomplex step, program instantiation, not depending on the number of PEs. As\nvalidation, we performed symbolic loop compilation on real-world loops and\nmeasured time and space requirements. Our experiments confirm that a symbolic\nconfiguration is space-efficient and suited for systems with little memory --\noften, a symbolic configuration is smaller than a single concrete configuration\n-- and that program instantiation scales well with the number of PEs -- for\nexample, when instantiating a symbolic configuration of a matrix-matrix\nmultiplication, the execution time is similar for $4\\times 4$ and $32\\times 32$\nPEs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:38:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Witterauf", "Michael", ""], ["Walter", "Dominik", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "2101.04799", "submitter": "Ananda Samajdar", "authors": "Ananda Samajdar, Michael Pellauer, Tushar Krishna", "title": "Self-Adaptive Reconfigurable Arrays (SARA): Using ML to Assist Scaling\n  GEMM Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing diversity in Deep Neural Network(DNN) models in terms of\nlayer shapes and sizes, the research community has been investigating\nflexible/reconfigurable accelerator substrates. This line of research has\nopened up two challenges. The first is to determine the appropriate amount of\nflexibility within an accelerator array that that can trade-off the performance\nbenefits versus the area overheads of the reconfigurability. The second is\nbeing able to determine the right configuration of the array for the current\nDNN model and/or layer and reconfigure the accelerator at runtime. This work\nintroduces a new class of accelerators that we call Self Adaptive\nReconfigurable Array (SARA). SARA architectures comprise of both a\nreconfigurable array and a hardware unit capable of determining an optimized\nconfiguration for the array at runtime. We demonstrate an instance of SARA with\nan accelerator we call SAGAR, which introduces a novel reconfigurable systolic\narray that can be configured to work as a distributed collection of smaller\narrays of various sizes or as a single array with flexible aspect ratios. We\nalso develop a novel recommendation neural network called ADAPTNET which\nrecommends an array configuration and dataflow for the current layer\nparameters. ADAPTNET runs on an integrated custom hardware ADAPTNETX that runs\nADAPTNET at runtime and reconfigures the array, making the entire accelerator\nself-sufficient. SAGAR is capable of providing the same mapping flexibility as\na collection of 10244x4 arrays working as a distributed system while achieving\n3.5x more power efficiency and 3.2x higher compute density Furthermore, the\nruntime achieved on the recommended parameters from ADAPTNET is 99.93% of the\nbest achievable runtime.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:20:23 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Samajdar", "Ananda", ""], ["Pellauer", "Michael", ""], ["Krishna", "Tushar", ""]]}, {"id": "2101.05314", "submitter": "Lei Jiang", "authors": "Lei Jiang, Farzaneh Zokaee", "title": "EXMA: A Genomics Accelerator for Exact-Matching", "comments": "IEEE International Symposium on High-Performance Computer\n  Architecture, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genomics is the foundation of precision medicine, global food security and\nvirus surveillance. Exact-match is one of the most essential operations widely\nused in almost every step of genomics such as alignment, assembly, annotation,\nand compression. Modern genomics adopts Ferragina-Manzini Index (FM-Index)\naugmenting space-efficient Burrows-Wheeler transform (BWT) with additional data\nstructures to permit ultra-fast exact-match operations. However, FM-Index is\nnotorious for its poor spatial locality and random memory access pattern. Prior\nworks create GPU-, FPGA-, ASIC- and even process-in-memory (PIM)-based\naccelerators to boost FM-Index search throughput. Though they achieve the\nstate-of-the-art FM-Index search throughput, the same as all prior conventional\naccelerators, FM-Index PIMs process only one DNA symbol after each DRAM row\nactivation, thereby suffering from poor memory bandwidth utilization.\n  In this paper, we propose a hardware accelerator, EXMA, to enhance FM-Index\nsearch throughput. We first create a novel EXMA table with a\nmulti-task-learning (MTL)-based index to process multiple DNA symbols with each\nDRAM row activation. We then build an accelerator to search over an EXMA table.\nWe propose 2-stage scheduling to increase the cache hit rate of our\naccelerator. We introduce dynamic page policy to improve the row buffer hit\nrate of DRAM main memory. We also present CHAIN compression to reduce the data\nstructure size of EXMA tables. Compared to state-of-the-art FM-Index PIMs, EXMA\nimproves search throughput by $4.9\\times$, and enhances search throughput per\nWatt by $4.8\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:35:12 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jiang", "Lei", ""], ["Zokaee", "Farzaneh", ""]]}, {"id": "2101.05357", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Mo Han, Deniz Erdogmus, and Gunar Schirner", "title": "Towards Creating a Deployable Grasp Type Probability Estimator for a\n  Prosthetic Hand", "comments": null, "journal-ref": "CyPhy 2019, WESE 2019. Lecture Notes in Computer Science, vol\n  11971. Springer, Cham", "doi": "10.1007/978-3-030-41131-2_3", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lower arm amputees, prosthetic hands promise to restore most of physical\ninteraction capabilities. This requires to accurately predict hand gestures\ncapable of grabbing varying objects and execute them timely as intended by the\nuser. Current approaches often rely on physiological signal inputs such as\nElectromyography (EMG) signal from residual limb muscles to infer the intended\nmotion. However, limited signal quality, user diversity and high variability\nadversely affect the system robustness. Instead of solely relying on EMG\nsignals, our work enables augmenting EMG intent inference with physical state\nprobability through machine learning and computer vision method. To this end,\nwe: (1) study state-of-the-art deep neural network architectures to select a\nperformant source of knowledge transfer for the prosthetic hand, (2) use a\ndataset containing object images and probability distribution of grasp types as\na new form of labeling where instead of using absolute values of zero and one\nas the conventional classification labels, our labels are a set of\nprobabilities whose sum is 1. The proposed method generates probabilistic\npredictions which could be fused with EMG prediction of probabilities over\ngrasps by using the visual information from the palm camera of a prosthetic\nhand. Our results demonstrate that InceptionV3 achieves highest accuracy with\n0.95 angular similarity followed by 1.4 MobileNetV2 with 0.93 at ~20% the\namount of operations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:39:41 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Han", "Mo", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2101.05591", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Dominik Sisejkovic, Lennart M. Reimann, Kirthihan\n  Yasotharan, Thomas Grass, Rainer Leupers", "title": "ANDROMEDA: An FPGA Based RISC-V MPSoC Exploration Framework", "comments": "Accepted in VLSI Design 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the growing demands of consumer electronic products, the computational\nrequirements are increasing exponentially. Due to the applications'\ncomputational needs, the computer architects are trying to pack as many cores\nas possible on a single die for accelerated execution of the application\nprogram codes. In a multiprocessor system-on-chip (MPSoC), striking a balance\namong the number of cores, memory subsystems, and network-on-chip parameters is\nessential to attain the desired performance. In this paper, we present\nANDROMEDA, a RISC-V based framework that allows us to explore the different\nconfigurations of an MPSoC and observe the performance penalties and gains. We\nemulate the various configurations of MPSoC on the Synopsys HAPS-80D Dual FPGA\nplatform. Using STREAM, matrix multiply, and N-body simulations as benchmarks,\nwe demonstrate our framework's efficacy in quickly identifying the right\nparameters for efficient execution of these benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 13:52:34 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Merchant", "Farhad", ""], ["Sisejkovic", "Dominik", ""], ["Reimann", "Lennart M.", ""], ["Yasotharan", "Kirthihan", ""], ["Grass", "Thomas", ""], ["Leupers", "Rainer", ""]]}, {"id": "2101.06665", "submitter": "Farhad Merchant", "authors": "Vinay Saxena, Ankitha Reddy, Jonathan Neudorfer, John Gustafson,\n  Sangeeth Nambiar, Rainer Leupers, Farhad Merchant", "title": "Brightening the Optical Flow through Posit Arithmetic", "comments": "To appear in ISQED 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.MS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As new technologies are invented, their commercial viability needs to be\ncarefully examined along with their technical merits and demerits. The posit\ndata format, proposed as a drop-in replacement for IEEE 754 float format, is\none such invention that requires extensive theoretical and experimental study\nto identify products that can benefit from the advantages of posits for\nspecific market segments. In this paper, we present an extensive empirical\nstudy of posit-based arithmetic vis-\\`a-vis IEEE 754 compliant arithmetic for\nthe optical flow estimation method called Lucas-Kanade (LuKa). First, we use\nSoftPosit and SoftFloat format emulators to perform an empirical error analysis\nof the LuKa method. Our study shows that the average error in LuKa with\nSoftPosit is an order of magnitude lower than LuKa with SoftFloat. We then\npresent the integration of the hardware implementation of a posit adder and\nmultiplier in a RISC-V open-source platform. We make several recommendations,\nalong with the analysis of LuKa in the RISC-V context, for future generation\nplatforms incorporating posit arithmetic units.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:19:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Saxena", "Vinay", ""], ["Reddy", "Ankitha", ""], ["Neudorfer", "Jonathan", ""], ["Gustafson", "John", ""], ["Nambiar", "Sangeeth", ""], ["Leupers", "Rainer", ""], ["Merchant", "Farhad", ""]]}, {"id": "2101.07232", "submitter": "Felix Klein", "authors": "Gideon Geier, Philippe Heim, Felix Klein, Bernd Finkbeiner", "title": "Syntroids: Synthesizing a Game for FPGAs using Temporal Logic\n  Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Syntroids, a case study for the automatic synthesis of hardware\nfrom a temporal logic specification. Syntroids is a space shooter arcade game\nrealized on an FPGA, where the control flow architecture has been completely\nspecified in Temporal Stream Logic (TSL) and implemented using reactive\nsynthesis. TSL is a recently introduced temporal logic that separates control\nand data. This leads to scalable synthesis, because the cost of the synthesis\nprocess is independent of the complexity of the handled data.\n  In this case study, we report on our experience with the TSL-based\ndevelopment of the Syntroids game and on the implementation quality obtained\nwith synthesis in comparison to manual programming. We also discuss solved and\nopen challenges with respect to currently available synthesis tools.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:35:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Geier", "Gideon", ""], ["Heim", "Philippe", ""], ["Klein", "Felix", ""], ["Finkbeiner", "Bernd", ""]]}, {"id": "2101.07557", "submitter": "Christina Giannoula", "authors": "Christina Giannoula, Nandita Vijaykumar, Nikela Papadopoulou,\n  Vasileios Karakostas, Ivan Fernandez, Juan G\\'omez-Luna, Lois Orosa,\n  Nectarios Koziris, Georgios Goumas, Onur Mutlu", "title": "SynCron: Efficient Synchronization Support for Near-Data-Processing\n  Architectures", "comments": "To appear in the 27th IEEE International Symposium on\n  High-Performance Computer Architecture (HPCA-27)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-Data-Processing (NDP) architectures present a promising way to alleviate\ndata movement costs and can provide significant performance and energy benefits\nto parallel applications. Typically, NDP architectures support several NDP\nunits, each including multiple simple cores placed close to memory. To fully\nleverage the benefits of NDP and achieve high performance for parallel\nworkloads, efficient synchronization among the NDP cores of a system is\nnecessary. However, supporting synchronization in many NDP systems is\nchallenging because they lack shared caches and hardware cache coherence\nsupport, which are commonly used for synchronization in multicore systems, and\ncommunication across different NDP units can be expensive.\n  This paper comprehensively examines the synchronization problem in NDP\nsystems, and proposes SynCron, an end-to-end synchronization solution for NDP\nsystems. SynCron adds low-cost hardware support near memory for synchronization\nacceleration, and avoids the need for hardware cache coherence support. SynCron\nhas three components: 1) a specialized cache memory structure to avoid memory\naccesses for synchronization and minimize latency overheads, 2) a hierarchical\nmessage-passing communication protocol to minimize expensive communication\nacross NDP units of the system, and 3) a hardware-only overflow management\nscheme to avoid performance degradation when hardware resources for\nsynchronization tracking are exceeded.\n  We evaluate SynCron using a variety of parallel workloads, covering various\ncontention scenarios. SynCron improves performance by 1.27$\\times$ on average\n(up to 1.78$\\times$) under high-contention scenarios, and by 1.35$\\times$ on\naverage (up to 2.29$\\times$) under low-contention real applications, compared\nto state-of-the-art approaches. SynCron reduces system energy consumption by\n2.08$\\times$ on average (up to 4.25$\\times$).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:48:58 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 00:09:20 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 11:47:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Giannoula", "Christina", ""], ["Vijaykumar", "Nandita", ""], ["Papadopoulou", "Nikela", ""], ["Karakostas", "Vasileios", ""], ["Fernandez", "Ivan", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Orosa", "Lois", ""], ["Koziris", "Nectarios", ""], ["Goumas", "Georgios", ""], ["Mutlu", "Onur", ""]]}, {"id": "2101.08458", "submitter": "Jian Weng", "authors": "Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony\n  Nowatzki", "title": "UNIT: Unifying Tensorized Instruction Compilation", "comments": "13 pages, 13 figures, and 1 table", "journal-ref": "2021 IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO), Seoul, Korea (South), 2021, pp. 77-89", "doi": "10.1109/CGO51591.2021.9370330", "report-no": null, "categories": "cs.PL cs.AR cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the increasing demand for computation in DNN, researchers develope\nboth hardware and software mechanisms to reduce the compute and memory burden.\nA widely adopted approach is to use mixed precision data types. However, it is\nhard to leverage mixed precision without hardware support because of the\noverhead of data casting. Hardware vendors offer tensorized instructions for\nmixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT.\nThese instructions involve a computing idiom that reduces multiple low\nprecision elements into one high precision element. The lack of compilation\ntechniques for this makes it hard to utilize these instructions: Using\nvendor-provided libraries for computationally-intensive kernels is inflexible\nand prevents further optimizations, and manually writing hardware intrinsics is\nerror-prone and difficult for programmers. Some prior works address this\nproblem by creating compilers for each instruction. This requires excessive\neffort when it comes to many tensorized instructions. In this work, we develop\na compiler framework to unify the compilation for these instructions -- a\nunified semantics abstraction eases the integration of new instructions, and\nreuses the analysis and transformations. Tensorized instructions from different\nplatforms can be compiled via UNIT with moderate effort for favorable\nperformance. Given a tensorized instruction and a tensor operation, UNIT\nautomatically detects the applicability, transforms the loop organization of\nthe operation,and rewrites the loop body to leverage the tensorized\ninstruction. According to our evaluation, UNIT can target various mainstream\nhardware platforms. The generated end-to-end inference model achieves 1.3x\nspeedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an\nNvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on\nan ARM CPU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:22:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 03:36:45 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 04:11:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weng", "Jian", ""], ["Jain", "Animesh", ""], ["Wang", "Jie", ""], ["Wang", "Leyuan", ""], ["Wang", "Yida", ""], ["Nowatzki", "Tony", ""]]}, {"id": "2101.08685", "submitter": "Thomas Pfeil", "authors": "Thomas Pfeil", "title": "ItNet: iterative neural networks with small graphs for accurate and\n  efficient anytime prediction", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs both during training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:56:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:25:35 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Pfeil", "Thomas", ""]]}, {"id": "2101.08715", "submitter": "Edward Stow", "authors": "Edward Stow, Riku Murai, Sajad Saeedi, Paul H. J. Kelly", "title": "Cain: Automatic Code Generation for Simultaneous Convolutional Kernels\n  on Focal-plane Sensor-processors", "comments": "17 pages, 4 figures, Accepted at LCPC 2020 to be published by\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Focal-plane Sensor-processors (FPSPs) are a camera technology that enable low\npower, high frame rate computation, making them suitable for edge computation.\nUnfortunately, these devices' limited instruction sets and registers make\ndeveloping complex algorithms difficult. In this work, we present Cain - a\ncompiler that targets SCAMP-5, a general-purpose FPSP - which generates code\nfrom multiple convolutional kernels. As an example, given the convolutional\nkernels for an MNIST digit recognition neural network, Cain produces code that\nis half as long, when compared to the other available compilers for SCAMP-5.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:48:28 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Stow", "Edward", ""], ["Murai", "Riku", ""], ["Saeedi", "Sajad", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2101.08744", "submitter": "Hongyu Miao", "authors": "Hongyu Miao, Felix Xiaozhu Lin", "title": "Enabling Large Neural Networks on Tiny Microcontrollers with Swapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running neural networks (NNs) on microcontroller units (MCUs) is becoming\nincreasingly important, but is very difficult due to the tiny SRAM size of MCU.\nPrior work proposes many algorithm-level techniques to reduce NN memory\nfootprints, but all at the cost of sacrificing accuracy and generality, which\ndisqualifies MCUs for many important use cases. We investigate a system\nsolution for MCUs to execute NNs out of core: dynamically swapping NN data\nchunks between an MCU's tiny SRAM and its large, low-cost external flash.\nOut-of-core NNs on MCUs raise multiple concerns: execution slowdown, storage\nwear out, energy consumption, and data security. We present a study showing\nthat none is a showstopper; the key benefit -- MCUs being able to run large NNs\nwith full accuracy and generality -- triumphs the overheads. Our findings\nsuggest that MCUs can play a much greater role in edge intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 21:38:57 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:58:50 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Miao", "Hongyu", ""], ["Lin", "Felix Xiaozhu", ""]]}, {"id": "2101.08754", "submitter": "Behnam Ghavami", "authors": "Farzane Khajuyi, Behnam Ghavami, Human Nikmehr", "title": "An Efficient Communication Protocol for FPGA IP Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a protection-based IP security scheme to protect soft and firm\nIP cores which are used on FPGA devices. The scheme is based on Finite State\nMachin (FSM) obfuscation and exploits Physical Unclonable Function (PUF) for\nFPGA unique identification (ID) generation which help pay-per-device licensing.\nWe introduce a communication protocol to protect the rights of parties in this\nmarket. On standard benchmark circuits, the experimental results show that our\nscheme is secure, attack-resilient and can be implemented with low area, power\nand delay overheads.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:09:51 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Khajuyi", "Farzane", ""], ["Ghavami", "Behnam", ""], ["Nikmehr", "Human", ""]]}, {"id": "2101.08877", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Changwoo Min, and Young Ik Eom", "title": "Virtual Memory Partitioning for Enhancing Application Performance in\n  Mobile Platforms", "comments": null, "journal-ref": null, "doi": "10.1109/TCE.2013.6689690", "report-no": null, "categories": "cs.AR cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the amount of running software on smart mobile devices is gradually\nincreasing due to the introduction of application stores. The application store\nis a type of digital distribution platform for application software, which is\nprovided as a component of an operating system on a smartphone or tablet.\nMobile devices have limited memory capacity and, unlike server and desktop\nsystems, due to their mobility they do not have a memory slot that can expand\nthe memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK)\nare widely used memory management solutions in mobile systems. They forcibly\nterminate applications when the available physical memory becomes insufficient.\nIn addition, before the forced termination, the memory shortage incurs\nthrashing and fragmentation, thus slowing down application performance.\nAlthough the existing page reclamation mechanism is designed to secure\navailable memory, it could seriously degrade user responsiveness due to the\nthrashing. Memory management is therefore still important especially in mobile\ndevices with small memory capacity. This paper presents a new memory\npartitioning technique that resolves the deterioration of the existing\napplication life cycle induced by LMK and OOMK. It provides a completely\nisolated virtual memory node at the operating system level. Evaluation results\ndemonstrate that the proposed method improves application execution time under\nmemory shortage, compared with methods in previous studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:54:21 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Min", "Changwoo", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.08884", "submitter": "Matthew Denton", "authors": "Matthew Denton and Herman Schmit", "title": "Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing systems rely on the recurrent multiplication of a very\nlarge, sparse, fixed matrix. We argue that direct spatial implementation of\nthese fixed matrices minimizes the work performed in the computation, and\nallows for significant reduction in latency and power through constant\npropagation and logic minimization. Bit-serial arithmetic enables massive\nstatic matrices to be implemented. We present the structure of our bit-serial\nmatrix multiplier, and evaluate using canonical signed digit representation to\nfurther reduce logic utilization. We have implemented these matrices on a large\nFPGA and provide a cost model that is simple and extensible. These FPGA\nimplementations, on average, reduce latency by 50x up to 86x versus GPU\nlibraries. Comparing against a recent sparse DNN accelerator, we measure a 4.1x\nto 47x reduction in latency depending on matrix dimension and sparsity.\nThroughput of the FPGA solution is also competitive for a wide range of matrix\ndimensions and batch sizes. Finally, we discuss ways these techniques could be\ndeployed in ASICs, making them applicable for dynamic sparse matrix\ncomputations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:16:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Denton", "Matthew", ""], ["Schmit", "Herman", ""]]}, {"id": "2101.08885", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Changwoo Min, Dong Hyun Kang, and Young Ik Eom", "title": "User-Aware Power Management for Mobile Devices", "comments": null, "journal-ref": null, "doi": "10.1109/GCCE.2013.6664780", "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power management techniques to extend battery lifespan is becoming\nincreasingly important due to longer user applications' running time in mobile\ndevices. Even when users do not use any applications, battery lifespan\ndecreases continually. It occurs because of service daemons of mobile platform\nand network-based data synchronization operations. In this paper, we propose a\nnew power management system that recognizes the idle time of the device to\nreduce the battery consumption of mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:17:42 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Min", "Changwoo", ""], ["Kang", "Dong Hyun", ""], ["Eom", "Young Ik", ""]]}, {"id": "2101.09821", "submitter": "Pranjal Singh Rajput", "authors": "Pranjal Singh Rajput, Sonnya Dellarosa, Kanya Satis", "title": "A Survey of Novel Cache Hierarchy Designs for High Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional on-die, three-level cache hierarchy design is very commonly used\nbut is also prone to latency, especially at the Level 2 (L2) cache. We discuss\nthree distinct ways of improving this design in order to have better\nperformance. Performance is especially important for systems with high\nworkloads. The first method proposes to eliminate L2 altogether while proposing\na new prefetching technique, the second method suggests increasing the size of\nL2, while the last method advocates the implementation of optical caches. After\ncarefully contemplating the results in performance gains and the advantages and\ndisadvantages of each method, we found the last method to be the best of the\nthree.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 23:01:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rajput", "Pranjal Singh", ""], ["Dellarosa", "Sonnya", ""], ["Satis", "Kanya", ""]]}, {"id": "2101.09968", "submitter": "Olivier Sentieys", "authors": "Davide Pala and Ivan Miro-Panades and Olivier Sentieys", "title": "Freezer: A Specialized NVM Backup Controller for Intermittently-Powered\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2020.3025063", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of IoT and wearable devices determined a rising attention\ntowards energy harvesting as source for powering these systems. In this\ncontext, many applications cannot afford the presence of a battery because of\nsize, weight and cost issues. Therefore, due to the intermittent nature of\nambient energy sources, these systems must be able to save and restore their\nstate, in order to guarantee progress across power interruptions. In this work,\nwe propose a specialized backup/restore controller that dynamically tracks the\nmemory accesses during the execution of the program. The controller then\ncommits the changes to a snapshot in a Non-Volatile Memory (NVM) when a power\nfailure is detected. Our approach does not require complex hybrid memories and\ncan be implemented with standard components. % and integrated in any MCU with\nResults on a set of benchmarks show an average $8\\times$ reduction in backup\nsize. Thanks to our dedicated controller, the backup time is further reduced by\nmore than $100\\times$, with an area and power overhead of only 0.4\\% and 0.8\\%,\nrespectively, w.r.t. a low-end IoT node.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:17:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pala", "Davide", ""], ["Miro-Panades", "Ivan", ""], ["Sentieys", "Olivier", ""]]}, {"id": "2101.10015", "submitter": "Kai Han", "authors": "Yunhe Wang, Mingqiang Huang, Kai Han, Hanting Chen, Wei Zhang,\n  Chunjing Xu, Dacheng Tao", "title": "AdderNet and its Minimalist Hardware Design for Energy-Efficient\n  Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been widely used for boosting the\nperformance of many machine intelligence tasks. However, the CNN models are\nusually computationally intensive and energy consuming, since they are often\ndesigned with numerous multiply-operations and considerable parameters for the\naccuracy reason. Thus, it is difficult to directly apply them in the\nresource-constrained environments such as 'Internet of Things' (IoT) devices\nand smart phones. To reduce the computational complexity and energy burden,\nhere we present a novel minimalist hardware architecture using adder\nconvolutional neural network (AdderNet), in which the original convolution is\nreplaced by adder kernel using only additions. To maximally excavate the\npotential energy consumption, we explore the low-bit quantization algorithm for\nAdderNet with shared-scaling-factor method, and we design both specific and\ngeneral-purpose hardware accelerators for AdderNet. Experimental results show\nthat the adder kernel with int8/int16 quantization also exhibits high\nperformance, meanwhile consuming much less resources (theoretically ~81% off).\nIn addition, we deploy the quantized AdderNet on FPGA (Field Programmable Gate\nArray) platform. The whole AdderNet can practically achieve 16% enhancement in\nspeed, 67.6%-71.4% decrease in logic resource utilization and 47.85%-77.9%\ndecrease in power consumption compared to CNN under the same circuit\narchitecture. With a comprehensive comparison on the performance, power\nconsumption, hardware resource consumption and network generalization\ncapability, we conclude the AdderNet is able to surpass all the other\ncompetitors including the classical CNN, novel memristor-network, XNOR-Net and\nthe shift-kernel based network, indicating its great potential in future high\nperformance and energy-efficient artificial intelligence applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:31:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:48:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wang", "Yunhe", ""], ["Huang", "Mingqiang", ""], ["Han", "Kai", ""], ["Chen", "Hanting", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.10463", "submitter": "An Zou", "authors": "An Zou, Jing Li, Christopher D. Gill, and Xuan Zhang", "title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with\n  Fine-Grain Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many emerging cyber-physical systems, such as autonomous vehicles and robots,\nrely heavily on artificial intelligence and machine learning algorithms to\nperform important system operations. Since these highly parallel applications\nare computationally intensive, they need to be accelerated by graphics\nprocessing units (GPUs) to meet stringent timing constraints. However, despite\nthe wide adoption of GPUs, efficiently scheduling multiple GPU applications\nwhile providing rigorous real-time guarantees remains a challenge. In this\npaper, we propose RTGPU, which can schedule the execution of multiple GPU\napplications in real-time to meet hard deadlines. Each GPU application can have\nmultiple CPU execution and memory copy segments, as well as GPU kernels. We\nstart with a model to explicitly account for the CPU and memory copy segments\nof these applications. We then consider the GPU architecture in the development\nof a precise timing model for the GPU kernels and leverage a technique known as\npersistent threads to implement fine-grained kernel scheduling with improved\nperformance through interleaved execution. Next, we propose a general method\nfor scheduling parallel GPU applications in real time. Finally, to schedule\nmultiple parallel GPU applications, we propose a practical real-time scheduling\nalgorithm based on federated scheduling and grid search (for GPU kernel\nsegments) with uniprocessor fixed priority scheduling (for multiple CPU and\nmemory copy segments). Our approach provides superior schedulability compared\nwith previous work, and gives real-time guarantees to meet hard deadlines for\nmultiple GPU applications according to comprehensive validation and evaluation\non a real NVIDIA GTX1080Ti GPU system.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:22:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zou", "An", ""], ["Li", "Jing", ""], ["Gill", "Christopher D.", ""], ["Zhang", "Xuan", ""]]}, {"id": "2101.10605", "submitter": "Soramichi Akiyama", "authors": "Soramichi Akiyama, Ryota Shioya", "title": "The Granularity Gap Problem: A Hurdle for Applying Approximate Memory to\n  Complex Data Layout", "comments": "Extended version of a conference paper published in the 12th ACM/SPEC\n  International Conference on Performance Engineering (ICPE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main memory access latency has not much improved for more than two\ndecades while the CPU performance had been exponentially increasing until\nrecently. Approximate memory is a technique to reduce the DRAM access latency\nin return of losing data integrity. It is beneficial for applications that are\nrobust to noisy input and intermediate data such as artificial intelligence,\nmultimedia processing, and graph processing. To obtain reasonable outputs from\napplications on approximate memory, it is crucial to protect critical data\nwhile accelerating accesses to non-critical data. We refer the minimum size of\na continuous memory region that the same error rate is applied in approximate\nmemory to as the approximation granularity. A fundamental limitation of\napproximate memory is that the approximation granularity is as large as a few\nkilo bytes. However, applications may have critical and non-critical data\ninterleaved with smaller granularity. For example, a data structure for graph\nnodes can have pointers (critical) to neighboring nodes and its score\n(non-critical, depending on the use-case). This data structure cannot be\ndirectly mapped to approximate memory due to the gap between the approximation\ngranularity and the granularity of data criticality. We refer to this issue as\nthe granularity gap problem. In this paper, we first show that many\napplications potentially suffer from this problem. Then we propose a framework\nto quantitatively evaluate the performance overhead of a possible method to\navoid this problem using known techniques. The evaluation results show that the\nperformance overhead is non-negligible compared to expected benefit from\napproximate memory, suggesting that the granularity gap problem is a\nsignificant concern.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:34:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Akiyama", "Soramichi", ""], ["Shioya", "Ryota", ""]]}, {"id": "2101.11404", "submitter": "Zain Ul Abideen", "authors": "Malik Imran and Zain Ul Abideen and Samuel Pagliarini", "title": "An Open-source Library of Large Integer Polynomial Multipliers", "comments": "This paper has been accepted for conference proceeding in DDECS 2021\n  - April 7-9 2021 Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polynomial multiplication is a bottleneck in most of the public-key\ncryptography protocols, including Elliptic-curve cryptography and several of\nthe post-quantum cryptography algorithms presently being studied. In this\npaper, we present a library of various large integer polynomial multipliers to\nbe used in hardware cryptocores. Our library contains both digitized and\nnon-digitized multiplier flavours for circuit designers to choose from. The\nlibrary is supported by a C++ generator that automatically produces the\nmultipliers' logic in Verilog HDL that is amenable for FPGA and ASIC designs.\nMoreover, for ASICs, it also generates configurable and parameterizable\nsynthesis scripts. The features of the generator allow for a quick generation\nand assessment of several architectures at the same time, thus allowing a\ndesigner to easily explore the (complex) optimization search space of\npolynomial multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:57:09 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 09:40:05 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 09:10:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Imran", "Malik", ""], ["Abideen", "Zain Ul", ""], ["Pagliarini", "Samuel", ""]]}, {"id": "2101.11748", "submitter": "Hamzah Abdel-Aziz", "authors": "Hamzah Abdel-Aziz, Ali Shafiee, Jong Hoon Shin, Ardavan Pedram and\n  Joseph H. Hassoun", "title": "Rethinking Floating Point Overheads for Mixed Precision DNN Accelerators", "comments": "Accepted to appear in 4th Conference on Machine Learning and Systems\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mixed-precision convolution unit architecture\nwhich supports different integer and floating point (FP) precisions. The\nproposed architecture is based on low-bit inner product units and realizes\nhigher precision based on temporal decomposition. We illustrate how to\nintegrate FP computations on integer-based architecture and evaluate overheads\nincurred by FP arithmetic support. We argue that alignment and addition\noverhead for FP inner product can be significant since the maximum exponent\ndifference could be up to 58 bits, which results into a large alignment logic.\nTo address this issue, we illustrate empirically that no more than\n26-bitproduct bits are required and up to 8-bit of alignment is sufficient in\nmost inference cases. We present novel optimizations based on the above\nobservations to reduce the FP arithmetic hardware overheads. Our empirical\nresults, based on simulation and hardware implementation, show significant\nreduction in FP16 overhead. Over typical mixed precision implementation, the\nproposed architecture achieves area improvements of up to 25% in TFLOPS/mm2and\nup to 46% in TOPS/mm2with power efficiency improvements of up to 40% in\nTFLOPS/Wand up to 63% in TOPS/W.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:57:43 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Abdel-Aziz", "Hamzah", ""], ["Shafiee", "Ali", ""], ["Shin", "Jong Hoon", ""], ["Pedram", "Ardavan", ""], ["Hassoun", "Joseph H.", ""]]}, {"id": "2101.12351", "submitter": "Muhammad Abdullah Hanif", "authors": "Muhammad Abdullah Hanif, Muhammad Shafique", "title": "DNN-Life: An Energy-Efficient Aging Mitigation Framework for Improving\n  the Lifetime of On-Chip Weight Memories in Deep Neural Network Hardware\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negative Biased Temperature Instability (NBTI)-induced aging is one of the\ncritical reliability threats in nano-scale devices. This paper makes the first\nattempt to study the NBTI aging in the on-chip weight memories of deep neural\nnetwork (DNN) hardware accelerators, subjected to complex DNN workloads. We\npropose DNN-Life, a specialized aging analysis and mitigation framework for\nDNNs, which jointly exploits hardware- and software-level knowledge to improve\nthe lifetime of a DNN weight memory with reduced energy overhead. At the\nsoftware-level, we analyze the effects of different DNN quantization methods on\nthe distribution of the bits of weight values. Based on the insights gained\nfrom this analysis, we propose a micro-architecture that employs low-cost\nmemory-write (and read) transducers to achieve an optimal duty-cycle at run\ntime in the weight memory cells, thereby balancing their aging. As a result,\nour DNN-Life framework enables efficient aging mitigation of weight memory of\nthe given DNN hardware at minimal energy overhead during the inference process.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:43:40 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2101.12691", "submitter": "Tao Wang", "authors": "Tao Wang, Xiangrui Yang, Gianni Antichi, Anirudh Sivaraman, Aurojit\n  Panda", "title": "Isolation mechanisms for high-speed packet-processing pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-plane programmability is now mainstream, both in the form of\nprogrammable switches and smart network-interface cards (SmartNICs). As the\nnumber of use cases for programmable network devices grows, each device will\nneed to support multiple packet-processing modules simultaneously. These\nmodules are likely to be independently developed, e.g., measurement and\nsecurity modules developed by different teams, or cloud tenants offloading\npacket processing to a NIC. Hence, we need isolation mechanisms to ensure that\nmodules on the same device do not interfere with each other. This paper\npresents a system, Menshen, for inter-module isolation on programmable\npacket-processing pipelines similar to the RMT/PISA architecture. Menshen\nconsists of a set of lightweight hardware primitives that can be added to an\nRMT pipeline and a compiler to take advantage of these primitives. We prototype\nthe Menshen hardware using the NetFPGA switch and Corundum FPGA NIC platforms\nand the Menshen software using the open-source P4-16 reference compiler. We\nshow that Menshen supports multiple modules simultaneously, allows one module\nto be quickly updated without disrupting other modules, and consumes a modest\namount of additional hardware resources relative to an RMT pipeline. We have\nopen sourced the code for Menshen's hardware and software at\nhttps://github.com/anonymous-submission-855. Although we do not have an ASIC\nimplementation of Menshen, we believe its primitives are simple enough that\nthey can be added to an ASIC realization of RMT as well.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:21:27 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:48:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Tao", ""], ["Yang", "Xiangrui", ""], ["Antichi", "Gianni", ""], ["Sivaraman", "Anirudh", ""], ["Panda", "Aurojit", ""]]}, {"id": "2101.12700", "submitter": "Matthew Dale", "authors": "Matthew Dale, Richard F. L. Evans, Sarah Jenkins, Simon O'Keefe,\n  Angelika Sebald, Susan Stepney, Fernando Torre, Martin Trefzer", "title": "Reservoir Computing with Thin-film Ferromagnetic Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.mtrl-sci cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence are driven by technologies inspired by\nthe brain, but these technologies are orders of magnitude less powerful and\nenergy efficient than biological systems. Inspired by the nonlinear dynamics of\nneural networks, new unconventional computing hardware has emerged with the\npotential for extreme parallelism and ultra-low power consumption. Physical\nreservoir computing demonstrates this with a variety of unconventional systems\nfrom optical-based to spintronic. Reservoir computers provide a nonlinear\nprojection of the task input into a high-dimensional feature space by\nexploiting the system's internal dynamics. A trained readout layer then\ncombines features to perform tasks, such as pattern recognition and time-series\nanalysis. Despite progress, achieving state-of-the-art performance without\nexternal signal processing to the reservoir remains challenging. Here we show,\nthrough simulation, that magnetic materials in thin-film geometries can realise\nreservoir computers with greater than or similar accuracy to digital recurrent\nneural networks. Our results reveal that basic spin properties of magnetic\nfilms generate the required nonlinear dynamics and memory to solve machine\nlearning tasks. Furthermore, we show that neuromorphic hardware can be reduced\nin size by removing the need for discrete neural components and external\nprocessing. The natural dynamics and nanoscale size of magnetic thin-films\npresent a new path towards fast energy-efficient computing with the potential\nto innovate portable smart devices, self driving vehicles, and robotics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Dale", "Matthew", ""], ["Evans", "Richard F. L.", ""], ["Jenkins", "Sarah", ""], ["O'Keefe", "Simon", ""], ["Sebald", "Angelika", ""], ["Stepney", "Susan", ""], ["Torre", "Fernando", ""], ["Trefzer", "Martin", ""]]}]