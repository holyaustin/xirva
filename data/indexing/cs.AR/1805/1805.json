[{"id": "1805.00359", "submitter": "Duc-Phuc Nguyen", "authors": "Duc-Phuc Nguyen, Dinh-Dung Le, Thi-Hong Tran, Huu-Thuan Huynh,\n  Yasuhiko Nakashima", "title": "Hardware Implementation of A Non-RLL Soft-decoding Beacon-based Visible\n  Light Communication Receiver", "comments": "In review process of ATC'18, HCMC, Vietnam", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible light communication (VLC)-based beacon systems, which usually\ntransmit identification (ID) information in small-size data frames are applied\nwidely in indoor localization applications. There is one fact that flicker of\nLED light should be avoid in any VLC systems. Current flicker mitigation\nsolutions based on run-length limited (RLL) codes suffer from reduced code\nrates, or are limited to hard-decoding forward error correction (FEC) decoders.\nRecently, soft-decoding techniques of RLL-codes are proposed to support\nsoft-decoding FEC algorithms, but they contain potentials of high-complexity\nand time-consuming computations. Fortunately, non-RLL direct current\n(DC)-balance solutions can overcome the drawbacks of RLL-based algorithms,\nhowever, they meet some difficulties in system latency or inferior\nerror-correction performances. Recently, non-RLL flicker mitigation solution\nbased on Polar code has proved to be an optimal approach due to its natural\nequal probabilities of short runs of 1's and 0's with high error-correction\nperformance. However, we found that this solution can only maintain the DC\nbalance only when the data frame length is sufficiently long. Accordingly,\nshort beacon-based data frames might still be a big challenge for flicker\nmitigation in such non-RLL cases. In this paper, we introduce a flicker\nmitigation solution designed for VLC-based beacon systems that combines a\nsimple pre-scrambler with a Polar encoder which has a codeword smaller than the\nprevious work 8 times. We also propose a hardware architecture for the proposed\ncompact non-RLL VLC receiver for the first time. Also, a 3-bit soft-decision\nfilter is introduce to enable soft-decoding of Polar decoder to improve the\nperformance of the receiver.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:04:49 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 01:25:56 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Nguyen", "Duc-Phuc", ""], ["Le", "Dinh-Dung", ""], ["Tran", "Thi-Hong", ""], ["Huynh", "Huu-Thuan", ""], ["Nakashima", "Yasuhiko", ""]]}, {"id": "1805.00585", "submitter": "Tanishq Dubey", "authors": "Adam Auten, Tanishq Dubey, Rohan Mathur", "title": "Dynamically Improving Branch Prediction Accuracy Between Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Branch prediction is a standard feature in most processors, significantly\nimproving the run time of programs by allowing a processor to predict the\ndirection of a branch before it has been evaluated. Current branch prediction\nmethods can achieve excellent prediction accuracy through global tables,\nvarious hashing methods, and even machine learning techniques such as SVMs or\nneural networks. Such designs, however, may lose effectiveness when attempting\nto predict across context switches in the operating system. Such a scenario may\nlead to destructive interference between contexts, therefore reducing overall\npredictor accuracy. To solve this problem, we propose a novel scheme for\ndeciding whether a context switch produces destructive or constructive\ninterference. First, we present evidence that shows that destructive\ninterference can have a significant negative impact on prediction accuracy.\nSecond, we present an extensible framework that keeps track of context switches\nand prediction accuracy to improve overall accuracy. Experimental results show\nthat this framework effectively reduces the effect of destructive interference\non branch prediction.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 00:24:36 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Auten", "Adam", ""], ["Dubey", "Tanishq", ""], ["Mathur", "Rohan", ""]]}, {"id": "1805.00976", "submitter": "Saba Ahmadian", "authors": "Saba Ahmadian, Onur Mutlu, and Hossein Asadi", "title": "ECI-Cache: A High-Endurance and Cost-Efficient I/O Caching Scheme for\n  Virtualized Platforms", "comments": null, "journal-ref": "Proceedings of the ACM on Measurement and Analysis of Computing\n  Systems 2.1 (2018): 9", "doi": "10.1145/3179412", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, high interest in using Virtual Machines (VMs) in data\ncenters and Cloud computing has significantly increased the demand for\nhigh-performance data storage systems. Recent studies suggest using SSDs as a\ncaching layer for HDD-based storage subsystems in virtualization platforms.\nSuch studies neglect to address the endurance and cost of SSDs, which can\nsignificantly affect the efficiency of I/O caching. Moreover, previous studies\nonly configure the cache size to provide the required performance level for\neach VM, while neglecting other important parameters such as cache write policy\nand request type, which can adversely affect both performance-per-cost and\nendurance.\n  In this paper, we present a new high-Endurance and Cost-efficient I/O Caching\n(ECI-Cache) scheme for virtualized platforms, which can significantly improve\nboth the performance-per-cost and endurance of storage subsystems as opposed to\npreviously proposed I/O caching schemes. Unlike traditional I/O caching schemes\nwhich allocate cache size only based on reuse distance of accesses, we propose\na new metric, Useful Reuse Distance (URD), which considers the request type in\nreuse distance calculation, resulting in improved performance-per-cost and\nendurance for the SSD cache. Via online characterization of workloads and using\nURD, ECI-Cache partitions the SSD cache across VMs and is able to dynamically\nadjust the cache size and write policy for each VM. To evaluate the proposed\nscheme, we have implemented ECI-Cache in an open source hypervisor, QEMU\n(version 2.8.0), on a server running the CentOS 7 operating system (kernel\nversion 3.10.0-327). Experimental results show that our proposed scheme\nimproves the performance, performance-per-cost, and endurance of the SSD cache\nby 17%, 30% and 65%, respectively, compared to the state-of-the-art dynamic\ncache partitioning scheme.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 18:41:58 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Ahmadian", "Saba", ""], ["Mutlu", "Onur", ""], ["Asadi", "Hossein", ""]]}, {"id": "1805.01289", "submitter": "Kevin Chang", "authors": "K. K. Chang, D. Lee, Z. Chishti, A. R. Alameldeen, C. Wilkerson, Y.\n  Kim, O. Mutlu", "title": "Reducing DRAM Refresh Overheads with Refresh-Access Parallelism", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1712.07754,\n  arXiv:1601.06352", "journal-ref": "IPSI BgD Transactions on Advanced Research, July 2018, Volume 14,\n  Number 2, ISSN 1820 - 4511", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article summarizes the idea of \"refresh-access parallelism,\" which was\npublished in HPCA 2014, and examines the work's significance and future\npotential. The overarching objective of our HPCA 2014 paper is to reduce the\nsignificant negative performance impact of DRAM refresh with intelligent memory\ncontroller mechanisms.\n  To mitigate the negative performance impact of DRAM refresh, our HPCA 2014\npaper proposes two complementary mechanisms, DARP (Dynamic Access Refresh\nParallelization) and SARP (Subarray Access Refresh Parallelization). The goal\nis to address the drawbacks of state-of-the-art per-bank refresh mechanism by\nbuilding more efficient techniques to parallelize refreshes and accesses within\nDRAM. First, instead of issuing per-bank refreshes in a round-robin order, as\nit is done today, DARP issues per-bank refreshes to idle banks in an\nout-of-order manner. Furthermore, DARP proactively schedules refreshes during\nintervals when a batch of writes are draining to DRAM. Second, SARP exploits\nthe existence of mostly-independent subarrays within a bank. With minor\nmodifications to DRAM organization, it allows a bank to serve memory accesses\nto an idle subarray while another subarray is being refreshed. Our extensive\nevaluations on a wide variety of workloads and systems show that our mechanisms\nimprove system performance (and energy efficiency) compared to three\nstate-of-the-art refresh policies, and their performance bene ts increase as\nDRAM density increases.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 05:48:56 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Chang", "K. K.", ""], ["Lee", "D.", ""], ["Chishti", "Z.", ""], ["Alameldeen", "A. R.", ""], ["Wilkerson", "C.", ""], ["Kim", "Y.", ""], ["Mutlu", "O.", ""]]}, {"id": "1805.01966", "submitter": "Donghyuk Lee", "authors": "Yoongu Kim, Vivek Seshadri, Donghyuk Lee, Jamie Liu, Onur Mutlu", "title": "Exploiting the DRAM Microarchitecture to Increase Memory-Level\n  Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the idea of Subarray-Level Parallelism (SALP) in DRAM,\nwhich was published in ISCA 2012, and examines the work's significance and\nfuture potential. Modern DRAMs have multiple banks to serve multiple memory\nrequests in parallel. However, when two requests go to the same bank, they have\nto be served serially, exacerbating the high latency of on-chip memory. Adding\nmore banks to the system to mitigate this problem incurs high system cost. Our\ngoal in this work is to achieve the benefits of increasing the number of banks\nwith a low-cost approach. To this end, we propose three new mechanisms, SALP-1,\nSALP-2, and MASA (Multitude of Activated Subarrays), to reduce the\nserialization of different requests that go to the same bank. The key\nobservation exploited by our mechanisms is that a modern DRAM bank is\nimplemented as a collection of subarrays that operate largely independently\nwhile sharing few global peripheral structures.\n  Our three proposed mechanisms mitigate the negative impact of bank\nserialization by overlapping different components of the bank access latencies\nof multiple requests that go to different subarrays within the same bank.\nSALP-1 requires no changes to the existing DRAM structure, and needs to only\nreinterpret some of the existing DRAM timing parameters. SALP-2 and MASA\nrequire only modest changes (< 0.15% area overhead) to the DRAM peripheral\nstructures, which are much less design constrained than the DRAM core. Our\nevaluations show that SALP-1, SALP-2 and MASA significantly improve performance\nfor both single-core systems (7%/13%/17%) and multi-core systems (15%/16%/20%),\naveraged across a wide range of workloads. We also demonstrate that our\nmechanisms can be combined with application-aware memory request scheduling in\nmulticore systems to further improve performance and fairness.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 23:50:13 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Kim", "Yoongu", ""], ["Seshadri", "Vivek", ""], ["Lee", "Donghyuk", ""], ["Liu", "Jamie", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.02807", "submitter": "Myoungsoo Jung", "authors": "Jie Zhang and Myoungsoo Jung", "title": "FlashAbacus: A Self-Governing Flash-Based Accelerator for Low-Power\n  Systems", "comments": "This paper is published at the 13th edition of EuroSys", "journal-ref": null, "doi": "10.1145/3190508.3190544", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency and computing flexibility are some of the primary design\nconstraints of heterogeneous computing. In this paper, we present FlashAbacus,\na data-processing accelerator that self-governs heterogeneous kernel executions\nand data storage accesses by integrating many flash modules in lightweight\nmultiprocessors. The proposed accelerator can simultaneously process data from\ndifferent applications with diverse types of operational functions, and it\nallows multiple kernels to directly access flash without the assistance of a\nhost-level file system or an I/O runtime library. We prototype FlashAbacus on a\nmulticore-based PCIe platform that connects to FPGA-based flash controllers\nwith a 20 nm node process. The evaluation results show that FlashAbacus can\nimprove the bandwidth of data processing by 127%, while reducing energy\nconsumption by 78.4%, as compared to a conventional method of heterogeneous\ncomputing. \\blfootnote{This paper is accepted by and will be published at 2018\nEuroSys. This document is presented to ensure timely dissemination of scholarly\nand technical work.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 02:36:04 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhang", "Jie", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1805.02819", "submitter": "Yixin Luo", "authors": "Yu Cai, Yixin Luo, Erich F. Haratsch, Ken Mai, Saugata Ghose, Onur\n  Mutlu", "title": "Experimental Characterization, Optimization, and Recovery of Data\n  Retention Errors in MLC NAND Flash Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our work on experimentally characterizing, mitigating,\nand recovering data retention errors in multi-level cell (MLC) NAND flash\nmemory, which was published in HPCA 2015, and examines the work's significance\nand future potential. Retention errors, caused by charge leakage over time, are\nthe dominant source of flash memory errors. Understanding, characterizing, and\nreducing retention errors can significantly improve NAND flash memory\nreliability and endurance. In this work, we first characterize, with real 2Y-nm\nMLC NAND flash chips, how the threshold voltage distribution of flash memory\nchanges with different retention ages -- the length of time since a flash cell\nwas programmed. We observe from our characterization results that 1) the\noptimal read reference voltage of a flash cell, using which the data can be\nread with the lowest raw bit error rate (RBER), systematically changes with its\nretention age, and 2) different regions of flash memory can have different\nretention ages, and hence different optimal read reference voltages.\n  Based on our findings, we propose two new techniques. First, Retention\nOptimized Reading (ROR) adaptively learns and applies the optimal read\nreference voltage for each flash memory block online. The key idea of ROR is to\nperiodically learn a tight upper bound of the optimal read reference voltage,\nand from there approach the optimal read reference voltage. Our evaluations\nshow that ROR can extend flash memory lifetime by 64% and reduce average error\ncorrection latency by 10.1%. Second, Retention Failure Recovery (RFR) recovers\ndata with uncorrectable errors offline by identifying and probabilistically\ncorrecting flash cells with retention errors. Our evaluation shows that RFR\nessentially doubles the error correction capability.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 03:41:23 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Cai", "Yu", ""], ["Luo", "Yixin", ""], ["Haratsch", "Erich F.", ""], ["Mai", "Ken", ""], ["Ghose", "Saugata", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.02916", "submitter": "Chenyang Xia", "authors": "ChenYang Xia, Ji Chen, YouZhe Fan, Chi-ying Tsui, Jie Jin, Hui Shen\n  and Bin Li", "title": "A High-Throughput Architecture of List Successive Cancellation Polar\n  Codes Decoder with Large List Size", "comments": "16 pages, 13 figures, 8 tables, accepted by IEEE Transactions on\n  Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 66, Issue: 14,\n  2018 )", "doi": "10.1109/TSP.2018.2838554", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first kind of forward error correction (FEC) codes that achieve\nchannel capacity, polar codes have attracted much research interest recently.\nCompared with other popular FEC codes, polar codes decoded by list successive\ncancellation decoding (LSCD) with a large list size have better error\ncorrection performance. However, due to the serial decoding nature of LSCD and\nthe high complexity of list management (LM), the decoding latency is high,\nwhich limits the usage of polar codes in practical applications that require\nlow latency and high throughput. In this work, we study the high-throughput\nimplementation of LSCD with a large list size. Specifically, at the algorithmic\nlevel, to achieve a low decoding latency with moderate hardware complexity, two\ndecoding schemes, a multi-bit double thresholding scheme and a partial G-node\nlook-ahead scheme, are proposed. Then, a high-throughput VLSI architecture\nimplementing the proposed algorithms is developed with optimizations on\ndifferent computation modules. From the implementation results on UMC 90 nm\nCMOS technology, the proposed architecture achieves decoding throughputs of\n1.103 Gbps, 977 Mbps and 827 Mbps when the list sizes are 8, 16 and 32,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 09:25:54 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Xia", "ChenYang", ""], ["Chen", "Ji", ""], ["Fan", "YouZhe", ""], ["Tsui", "Chi-ying", ""], ["Jin", "Jie", ""], ["Shen", "Hui", ""], ["Li", "Bin", ""]]}, {"id": "1805.02921", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya and Irina Dolzhikova and Alex Pappachen James", "title": "Hierarchical Temporal Memory using Memristor Networks: A Survey", "comments": null, "journal-ref": "IEEE Transactions on Emerging Topics in Computational\n  Intelligence, 2018", "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a survey of the currently available hardware designs for\nimplementation of the human cortex inspired algorithm, Hierarchical Temporal\nMemory (HTM). In this review, we focus on the state of the art advances of\nmemristive HTM implementation and related HTM applications. With the advent of\nedge computing, HTM can be a potential algorithm to implement on-chip near\nsensor data processing. The comparison of analog memristive circuit\nimplementations with the digital and mixed-signal solutions are provided. The\nadvantages of memristive HTM over digital implementations against performance\nmetrics such as processing speed, reduced on-chip area and power dissipation\nare discussed. The limitations and open problems concerning the memristive HTM,\nsuch as the design scalability, sneak currents, leakage, parasitic effects,\nlack of the analog learning circuits implementations and unreliability of the\nmemristive devices integrated with CMOS circuits are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 09:39:29 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Krestinskaya", "Olga", ""], ["Dolzhikova", "Irina", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1805.03047", "submitter": "Donghyuk Lee", "authors": "Donghyuk Lee, Yoongu Kim, Gennady Pekhimenko, Samira Khan, Vivek\n  Seshadri, Kevin Chang, Onur Mutlu", "title": "Adaptive-Latency DRAM: Reducing DRAM Latency by Exploiting Timing\n  Margins", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.08454", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the idea of Adaptive-Latency DRAM (AL-DRAM), which was\npublished in HPCA 2015, and examines the work's significance and future\npotential. AL-DRAM is a mechanism that optimizes DRAM latency based on the DRAM\nmodule and the operating temperature, by exploiting the extra margin that is\nbuilt into the DRAM timing parameters. DRAM manufacturers provide a large\nmargin for the timing parameters as a provision against two worst-case\nscenarios. First, due to process variation, some outlier DRAM chips are much\nslower than others. Second, chips become slower at higher temperatures. The\ntiming parameter margin ensures that the slow outlier chips operate reliably at\nthe worst-case temperature, and hence leads to a high access latency.\n  Using an FPGA-based DRAM testing platform, our work first characterizes the\nextra margin for 115 DRAM modules from three major manufacturers. The\nexperimental results demonstrate that it is possible to reduce four of the most\ncritical timing parameters by a minimum/maximum of 17.3%/54.8% at 55C while\nmaintaining reliable operation. AL-DRAM uses these observations to adaptively\nselect reliable DRAM timing parameters for each DRAM module based on the\nmodule's current operating conditions. AL-DRAM does not require any changes to\nthe DRAM chip or its interface; it only requires multiple different timing\nparameters to be specified and supported by the memory controller. Our real\nsystem evaluations show that AL-DRAM improves the performance of\nmemory-intensive workloads by an average of 14% without introducing any errors.\nOur characterization and proposed techniques have inspired several other works\non analyzing and/or exploiting different sources of latency and performance\nvariation within DRAM chips.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 19:24:32 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lee", "Donghyuk", ""], ["Kim", "Yoongu", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Seshadri", "Vivek", ""], ["Chang", "Kevin", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03048", "submitter": "Donghyuk Lee", "authors": "Donghyuk Lee, Yoongu Kim, Vivek Seshadri, Jamie Liu, Lavanya\n  Subramanian, Onur Mutlu", "title": "Tiered-Latency DRAM: Enabling Low-Latency Main Memory at Low Cost", "comments": "arXiv admin note: substantial text overlap with arXiv:1601.06903", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the idea of Tiered-Latency DRAM (TL-DRAM), which was\npublished in HPCA 2013, and examines the work's significance and future\npotential. The capacity and cost-per-bit of DRAM have historically scaled to\nsatisfy the needs of increasingly large and complex computer systems. However,\nDRAM latency has remained almost constant, making memory latency the\nperformance bottleneck in today's systems. We observe that the high access\nlatency is not intrinsic to DRAM, but a trade-off is made to decrease the cost\nper bit. To mitigate the high area overhead of DRAM sensing structures,\ncommodity DRAMs connect many DRAM cells to each sense amplifier through a wire\ncalled a bitline. These bit-lines have a high parasitic capacitance due to\ntheir long length, and this bitline capacitance is the dominant source of DRAM\nlatency. Specialized low-latency DRAMs use shorter bitlines with fewer cells,\nbut have a higher cost-per-bit due to greater sense amplifier area overhead. To\nachieve both low latency and low cost per bit, we introduce Tiered-Latency DRAM\n(TL-DRAM). In TL-DRAM, each long bitline is split into two shorter segments by\nan isolation transistor, allowing one of the two segments to be accessed with\nthe latency of a short-bitline DRAM without incurring a high cost per bit. We\npropose mechanisms that use the low-latency segment as a hardware-managed or\nsoftware-managed cache. Our evaluations show that our proposed mechanisms\nimprove both performance and energy efficiency for both single-core and\nmultiprogrammed workloads. Tiered-Latency DRAM has inspired several other works\non reducing DRAM latency with little to no architectural modification.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 19:31:43 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Lee", "Donghyuk", ""], ["Kim", "Yoongu", ""], ["Seshadri", "Vivek", ""], ["Liu", "Jamie", ""], ["Subramanian", "Lavanya", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03154", "submitter": "Saugata Ghose", "authors": "Kevin K. Chang, Abhijith Kashyap, Hasan Hassan, Saugata Ghose, Kevin\n  Hsieh, Donghyuk Lee, Tianshi Li, Gennady Pekhimenko, Samira Khan, Onur Mutlu", "title": "Flexible-Latency DRAM: Understanding and Exploiting Latency Variation in\n  Modern DRAM Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article summarizes key results of our work on experimental\ncharacterization and analysis of latency variation and latency-reliability\ntrade-offs in modern DRAM chips, which was published in SIGMETRICS 2016, and\nexamines the work's significance and future potential.\n  The goal of this work is to (i) experimentally characterize and understand\nthe latency variation across cells within a DRAM chip for these three\nfundamental DRAM operations, and (ii) develop new mechanisms that exploit our\nunderstanding of the latency variation to reliably improve performance. To this\nend, we comprehensively characterize 240 DRAM chips from three major vendors,\nand make six major new observations about latency variation within DRAM.\nNotably, we find that (i) there is large latency variation across the cells for\neach of the three operations; (ii) variation characteristics exhibit\nsignificant spatial locality: slower cells are clustered in certain regions of\na DRAM chip; and (iii) the three fundamental operations exhibit different\nreliability characteristics when the latency of each operation is reduced.\n  Based on our observations, we propose Flexible-LatencY DRAM (FLY-DRAM), a\nmechanism that exploits latency variation across DRAM cells within a DRAM chip\nto improve system performance. The key idea of FLY-DRAM is to exploit the\nspatial locality of slower cells within DRAM, and access the faster DRAM\nregions with reduced latencies for the fundamental operations. Our evaluations\nshow that FLY-DRAM improves the performance of a wide range of applications by\n13.3%, 17.6%, and 19.5%, on average, for each of the three different vendors'\nreal DRAM chips, in a simulated 8-core system.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 16:45:01 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Chang", "Kevin K.", ""], ["Kashyap", "Abhijith", ""], ["Hassan", "Hasan", ""], ["Ghose", "Saugata", ""], ["Hsieh", "Kevin", ""], ["Lee", "Donghyuk", ""], ["Li", "Tianshi", ""], ["Pekhimenko", "Gennady", ""], ["Khan", "Samira", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03175", "submitter": "Saugata Ghose", "authors": "Kevin K. Chang, Abdullah Giray Yagl{\\i}k\\c{c}{\\i}, Saugata Ghose,\n  Aditya Agrawal, Niladrish Chatterjee, Abhijith Kashyap, Donghyuk Lee, Mike\n  O'Connor, Hasan Hassan, Onur Mutlu", "title": "Voltron: Understanding and Exploiting the Voltage-Latency-Reliability\n  Trade-Offs in Modern DRAM Chips to Improve Energy Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our work on experimental characterization and analysis\nof reduced-voltage operation in modern DRAM chips, which was published in\nSIGMETRICS 2017, and examines the work's significance and future potential.\n  We take a comprehensive approach to understanding and exploiting the latency\nand reliability characteristics of modern DRAM when the DRAM supply voltage is\nlowered below the nominal voltage level specified by DRAM standards. We perform\nan experimental study of 124 real DDR3L (low-voltage) DRAM chips manufactured\nrecently by three major DRAM vendors. We find that reducing the supply voltage\nbelow a certain point introduces bit errors in the data, and we comprehensively\ncharacterize the behavior of these errors. We discover that these errors can be\navoided by increasing the latency of three major DRAM operations (activation,\nrestoration, and precharge). We perform detailed DRAM circuit simulations to\nvalidate and explain our experimental findings. We also characterize the\nvarious relationships between reduced supply voltage and error locations,\nstored data patterns, DRAM temperature, and data retention.\n  Based on our observations, we propose a new DRAM energy reduction mechanism,\ncalled Voltron. The key idea of Voltron is to use a performance model to\ndetermine by how much we can reduce the supply voltage without introducing\nerrors and without exceeding a user-specified threshold for performance loss.\nOur evaluations show that Voltron reduces the average DRAM and system energy\nconsumption by 10.5% and 7.3%, respectively, while limiting the average system\nperformance loss to only 1.8%, for a variety of memory-intensive quad-core\nworkloads. We also show that Voltron significantly outperforms prior dynamic\nvoltage and frequency scaling mechanisms for DRAM.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:22:26 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Chang", "Kevin K.", ""], ["Yagl\u0131k\u00e7\u0131", "Abdullah Giray", ""], ["Ghose", "Saugata", ""], ["Agrawal", "Aditya", ""], ["Chatterjee", "Niladrish", ""], ["Kashyap", "Abhijith", ""], ["Lee", "Donghyuk", ""], ["O'Connor", "Mike", ""], ["Hassan", "Hasan", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03184", "submitter": "Saugata Ghose", "authors": "Kevin K. Chang, Prashant J. Nair, Saugata Ghose, Donghyuk Lee,\n  Moinuddin K. Qureshi, Onur Mutlu", "title": "LISA: Increasing Internal Connectivity in DRAM for Fast Data Movement\n  and Low Latency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the idea of Low-Cost Interlinked Subarrays (LISA),\nwhich was published in HPCA 2016, and examines the work's significance and\nfuture potential. Contemporary systems perform bulk data movement movement\ninefficiently, by transferring data from DRAM to the processor, and then back\nto DRAM, across a narrow off-chip channel. The use of this narrow channel\nresults in high latency and energy consumption. Prior work proposes to avoid\nthese high costs by exploiting the existing wide internal DRAM bandwidth for\nbulk data movement, but the limited connectivity of wires within DRAM allows\nfast data movement within only a single DRAM subarray. Each subarray is only a\nfew megabytes in size, greatly restricting the range over which fast bulk data\nmovement can happen within DRAM.\n  Our HPCA 2016 paper proposes a new DRAM substrate, Low-Cost Inter-Linked\nSubarrays (LISA), whose goal is to enable fast and efficient data movement\nacross a large range of memory at low cost. LISA adds low-cost connections\nbetween adjacent subarrays. By using these connections to interconnect the\nexisting internal wires (bitlines) of adjacent subarrays, LISA enables\nwide-bandwidth data transfer across multiple subarrays with little (only 0.8%)\nDRAM area overhead. As a DRAM substrate, LISA is versatile, enabling a variety\nof new applications. We describe and evaluate three such applications in\ndetail: (1) fast inter-subarray bulk data copy, (2) in-DRAM caching using a\nDRAM architecture whose rows have heterogeneous access latencies, and (3)\naccelerated bitline precharging by linking multiple precharge units together.\nOur extensive evaluations show that each of LISA's three applications\nsignificantly improves performance and memory energy efficiency on a variety of\nworkloads and system configurations.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:35:28 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Chang", "Kevin K.", ""], ["Nair", "Prashant J.", ""], ["Ghose", "Saugata", ""], ["Lee", "Donghyuk", ""], ["Qureshi", "Moinuddin K.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03195", "submitter": "Saugata Ghose", "authors": "Hasan Hassan, Nandita Vijaykumar, Samira Khan, Saugata Ghose, Kevin\n  Chang, Gennady Pekhimenko, Donghyuk Lee, Oguz Ergin, Onur Mutlu", "title": "SoftMC: Practical DRAM Characterization Using an FPGA-Based\n  Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the SoftMC DRAM characterization infrastructure, which\nwas published in HPCA 2017, and examines the work's significance and future\npotential.\n  SoftMC (Soft Memory Controller) is the first publicly-available DRAM testing\ninfrastructure that can flexibly and efficiently test DRAM chips in a manner\naccessible to both software and hardware developers. SoftMC is an FPGA-based\ntesting platform that can control and test memory modules designed for the\ncommonly-used DDR (Double Data Rate) interface. SoftMC has two key properties:\n(i) it provides flexibility to thoroughly control memory behavior or to\nimplement a wide range of mechanisms using DDR commands; and (ii) it is easy to\nuse as it provides a simple and intuitive high-level programming interface for\nusers, completely hiding the low-level details of the FPGA.\n  We demonstrate the capability, flexibility, and programming ease of SoftMC\nwith two example use cases. First, we implement a test that characterizes the\nretention time of DRAM cells. Second, we show that the expected latency\nreduction of two recently-proposed mechanisms, which rely on accessing\nrecently-refreshed or recently-accessed DRAM cells faster than other DRAM\ncells, is not observable in existing DRAM chips.\n  Various versions of the SoftMC platform have enabled many of our other DRAM\ncharacterization studies. We discuss several other use cases of SoftMC,\nincluding the ability to characterize emerging non-volatile memory modules that\nobey the DDR standard. We hope that our open-source release of SoftMC fills a\ngap in the space of publicly-available experimental memory testing\ninfrastructures and inspires new studies, ideas, and methodologies in memory\nsystem design.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 17:54:39 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hassan", "Hasan", ""], ["Vijaykumar", "Nandita", ""], ["Khan", "Samira", ""], ["Ghose", "Saugata", ""], ["Chang", "Kevin", ""], ["Pekhimenko", "Gennady", ""], ["Lee", "Donghyuk", ""], ["Ergin", "Oguz", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03283", "submitter": "Yixin Luo", "authors": "Yu Cai, Yixin Luo, Saugata Ghose, Erich F. Haratsch, Ken Mai, Onur\n  Mutlu", "title": "Read Disturb Errors in MLC NAND Flash Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our work on experimentally characterizing, mitigating,\nand recovering read disturb errors in multi-level cell (MLC) NAND flash memory,\nwhich was published in DSN 2015, and examines the work's significance and\nfuture potential. NAND flash memory reliability continues to degrade as the\nmemory is scaled down and more bits are programmed per cell. A key contributor\nto this reduced reliability is read disturb, where a read to one row of cells\nimpacts the threshold voltages of unread flash cells in different rows of the\nsame block.\n  For the first time in open literature, this work experimentally characterizes\nread disturb errors on state-of-the-art 2Y-nm (i.e., 20-24 nm) MLC NAND flash\nmemory chips. Our findings (1) correlate the magnitude of threshold voltage\nshifts with read operation counts, (2) demonstrate how program/erase cycle\ncount and retention age affect the read-disturb-induced error rate, and (3)\nidentify that lowering pass-through voltage levels reduces the impact of read\ndisturb and extend flash lifetime. Particularly, we find that the probability\nof read disturb errors increases with both higher wear-out and higher\npass-through voltage levels.\n  We leverage these findings to develop two new techniques. The first technique\nmitigates read disturb errors by dynamically tuning the pass-through voltage on\na per-block basis. Using real workload traces, our evaluations show that this\ntechnique increases flash memory endurance by an average of 21%. The second\ntechnique recovers from previously-uncorrectable flash errors by identifying\nand probabilistically correcting cells susceptible to read disturb errors. Our\nevaluations show that this recovery technique reduces the raw bit error rate by\n36%.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 21:00:24 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Cai", "Yu", ""], ["Luo", "Yixin", ""], ["Ghose", "Saugata", ""], ["Haratsch", "Erich F.", ""], ["Mai", "Ken", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.03291", "submitter": "Yixin Luo", "authors": "Yu Cai, Saugata Ghose, Yixin Luo, Ken Mai, Onur Mutlu, Erich F.\n  Haratsch", "title": "Characterizing, Exploiting, and Mitigating Vulnerabilities in MLC NAND\n  Flash Memory Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our work on experimentally analyzing, exploiting, and\naddressing vulnerabilities in multi-level cell NAND flash memory programming,\nwhich was published in the industrial session of HPCA 2017, and examines the\nwork's significance and future potential. Modern NAND flash memory chips use\nmulti-level cells (MLC), which store two bits of data in each cell, to improve\nchip density. As MLC NAND flash memory scaled down to smaller manufacturing\nprocess technologies, manufacturers adopted a two-step programming method to\nimprove reliability. In two-step programming, the two bits of a multi-level\ncell are programmed using two separate steps, in order to minimize the amount\nof cell-to-cell program interference induced on neighboring flash cells.\n  In this work, we demonstrate that two-step programming exposes new\nreliability and security vulnerabilities in state-of-the-art MLC NAND flash\nmemory. We experimentally characterize contemporary 1X-nm (i.e., 15--19nm)\nflash memory chips, and find that a partially-programmed flash cell (i.e., a\ncell where the second programming step has not yet been performed) is much more\nvulnerable to cell-to-cell interference and read disturb than a\nfully-programmed cell. We show that it is possible to exploit these\nvulnerabilities on solid-state drives (SSDs) to alter the partially-programmed\ndata, causing (potentially malicious) data corruption. Based on our\nobservations, we propose several new mechanisms that eliminate or mitigate\nthese vulnerabilities in partially-programmed cells, and at the same time\nincrease flash memory lifetime by 16%.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 21:10:30 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Cai", "Yu", ""], ["Ghose", "Saugata", ""], ["Luo", "Yixin", ""], ["Mai", "Ken", ""], ["Mutlu", "Onur", ""], ["Haratsch", "Erich F.", ""]]}, {"id": "1805.03502", "submitter": "Rachata Ausavarungnirun", "authors": "Vivek Seshadri, Yoongu Kim, Chris Fallin, Donghyuk Lee, Rachata\n  Ausavarungnirun, Gennady Pekhimenko, Yixin Luo, Onur Mutlu, Phillip B.\n  Gibbons, Michael A. Kozuch, Todd C. Mowry", "title": "RowClone: Accelerating Data Movement and Initialization Using DRAM", "comments": "arXiv admin note: text overlap with arXiv:1605.06483", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing systems, to perform any bulk data movement operation (copy or\ninitialization), the data has to first be read into the on-chip processor, all\nthe way into the L1 cache, and the result of the operation must be written back\nto main memory. This is despite the fact that these operations do not involve\nany actual computation. RowClone exploits the organization and operation of\ncommodity DRAM to perform these operations completely inside DRAM using two\nmechanisms. The first mechanism, Fast Parallel Mode, copies data between two\nrows inside the same DRAM subarray by issuing back-to-back activate commands to\nthe source and the destination row. The second mechanism, Pipelined Serial\nMode, transfers cache lines between two banks using the shared internal bus.\nRowClone significantly reduces the raw latency and energy consumption of bulk\ndata copy and initialization. This reduction directly translates to improvement\nin performance and energy efficiency of systems running copy or\ninitialization-intensive workloads\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 18:54:58 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Seshadri", "Vivek", ""], ["Kim", "Yoongu", ""], ["Fallin", "Chris", ""], ["Lee", "Donghyuk", ""], ["Ausavarungnirun", "Rachata", ""], ["Pekhimenko", "Gennady", ""], ["Luo", "Yixin", ""], ["Mutlu", "Onur", ""], ["Gibbons", "Phillip B.", ""], ["Kozuch", "Michael A.", ""], ["Mowry", "Todd C.", ""]]}, {"id": "1805.03648", "submitter": "Ryan Kastner", "authors": "Ryan Kastner, Janarbek Matai, and Stephen Neuendorffer", "title": "Parallel Programming for FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This book focuses on the use of algorithmic high-level synthesis (HLS) to\nbuild application-specific FPGA systems. Our goal is to give the reader an\nappreciation of the process of creating an optimized hardware design using HLS.\nAlthough the details are, of necessity, different from parallel programming for\nmulticore processors or GPUs, many of the fundamental concepts are similar. For\nexample, designers must understand memory hierarchy and bandwidth, spatial and\ntemporal locality of reference, parallelism, and tradeoffs between computation\nand storage. This book is a practical guide for anyone interested in building\nFPGA systems. In a university environment, it is appropriate for advanced\nundergraduate and graduate courses. At the same time, it is also useful for\npracticing system designers and embedded programmers. The book assumes the\nreader has a working knowledge of C/C++ and includes a significant amount of\nsample code. In addition, we assume familiarity with basic computer\narchitecture concepts (pipelining, speedup, Amdahl's Law, etc.). A knowledge of\nthe RTL-based FPGA design flow is helpful, although not required.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 17:26:28 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Kastner", "Ryan", ""], ["Matai", "Janarbek", ""], ["Neuendorffer", "Stephen", ""]]}, {"id": "1805.03718", "submitter": "Xiaowei Wang", "authors": "Charles Eckert, Xiaowei Wang, Jingcheng Wang, Arun Subramaniyan, Ravi\n  Iyer, Dennis Sylvester, David Blaauw, Reetuparna Das", "title": "Neural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks", "comments": "To appear in the 45th ACM/IEEE International Symposium on Computer\n  Architecture (ISCA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Neural Cache architecture, which re-purposes cache\nstructures to transform them into massively parallel compute units capable of\nrunning inferences for Deep Neural Networks. Techniques to do in-situ\narithmetic in SRAM arrays, create efficient data mapping and reducing data\nmovement are proposed. The Neural Cache architecture is capable of fully\nexecuting convolutional, fully connected, and pooling layers in-cache. The\nproposed architecture also supports quantization in-cache. Our experimental\nresults show that the proposed architecture can improve inference latency by\n18.3x over state-of-art multi-core CPU (Xeon E5), 7.7x over server class GPU\n(Titan Xp), for Inception v3 model. Neural Cache improves inference throughput\nby 12.4x over CPU (2.2x over GPU), while reducing power consumption by 50% over\nCPU (53% over GPU).\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:16:37 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Eckert", "Charles", ""], ["Wang", "Xiaowei", ""], ["Wang", "Jingcheng", ""], ["Subramaniyan", "Arun", ""], ["Iyer", "Ravi", ""], ["Sylvester", "Dennis", ""], ["Blaauw", "David", ""], ["Das", "Reetuparna", ""]]}, {"id": "1805.03969", "submitter": "Hasan Ibrahim Hasan", "authors": "Hasan Hassan, Gennady Pekhimenko, Nandita Vijaykumar, Vivek Seshadri,\n  Donghyuk Lee, Oguz Ergin, and Onur Mutlu", "title": "Exploiting Row-Level Temporal Locality in DRAM to Reduce the Memory\n  Access Latency", "comments": "arXiv admin note: substantial text overlap with arXiv:1609.07234", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the idea of ChargeCache, which was published in HPCA\n2016 [51], and examines the work's significance and future potential. DRAM\nlatency continues to be a critical bottleneck for system performance. In this\nwork, we develop a low-cost mechanism, called ChargeCache, that enables faster\naccess to recently-accessed rows in DRAM, with no modifications to DRAM chips.\nOur mechanism is based on the key observation that a recently-accessed row has\nmore charge and thus the following access to the same row can be performed\nfaster. To exploit this observation, we propose to track the addresses of\nrecently-accessed rows in a table in the memory controller. If a later DRAM\nrequest hits in that table, the memory controller uses lower timing parameters,\nleading to reduced DRAM latency. Row addresses are removed from the table after\na specified duration to ensure rows that have leaked too much charge are not\naccessed with lower latency. We evaluate ChargeCache on a wide variety of\nworkloads and show that it provides significant performance and energy benefits\nfor both single-core and multi-core systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 18:08:23 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Hassan", "Hasan", ""], ["Pekhimenko", "Gennady", ""], ["Vijaykumar", "Nandita", ""], ["Seshadri", "Vivek", ""], ["Lee", "Donghyuk", ""], ["Ergin", "Oguz", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.04074", "submitter": "Sirisha Chimata", "authors": "A. Nagalakshmi, Ch. Sirisha, Dr. D.N. Madhusudana Rao", "title": "Hybrid CMOS-CNFET based NP dynamic Carry Look Ahead Adder", "comments": "6 pages, 1 figure, 6 tables, Based on Master's thesis project\n  (2014-16) carried by A. Nagalakshmi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced electronic device technologies require a faster operation and\nsmaller average power consumption, which are the most important parameters in\nvery large scale integrated circuit design. The conventional Complementary\nMetal-Oxide Semiconductor (CMOS) technology is limited by the threshold voltage\nand subthreshold leakage problems in scaling of devices. This leads to failure\nin adapting it to sub-micron and nanotechnologies. The carbon nanotube (CNT)\ntechnology overcomes the threshold voltage and subthreshold leakage problems\ndespite reduction in size. The CNT based technology develops the most promising\ndevices among emerging technologies because it has most of the desired\nfeatures. Carbon Nanotube Field Effect Transistors (CNFETs) are the novel\ndevices that are expected to sustain the transistor scalability while\nincreasing its performance. Recently, there have been tremendous advances in\nCNT technology for nanoelectronics applications. CNFETs avoid most of the\nfundamental limitations and offer several advantages compared to silicon-based\ntechnology. Though CNT evolves as a better option to overcome some of the bulk\nCMOS problems, the CNT itself still immersed with setbacks. The fabrication of\ncarbon nanotube at very large digital circuits on a single substrate is\ndifficult to achieve. Therefore, a hybrid NP dynamic Carry Look Ahead Adder\n(CLA) is designed using p-CNFET and n-MOS transistors. Here, the performance of\nCLA is evaluated in 8-bit, 16-bit, 32-bit and 64-bit stages with the following\nfour different implementations: silicon MOSFET (Si-MOSFET) domino logic,\nSi-MOSFET NP dynamic CMOS, carbon nanotube MOSFET (CN-MOSFET) domino logic, and\nCN-MOSFET NP dynamic CMOS. Finally, a Hybrid CMOS-CNFET based 64-bit NP dynamic\nCLA is evaluated based on HSPICE simulation in 32nm technology, which\neffectively suppresses power dissipation without an increase in propagation\ndelay.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 17:27:16 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Nagalakshmi", "A.", ""], ["Sirisha", "Ch.", ""], ["Rao", "Dr. D. N. Madhusudana", ""]]}, {"id": "1805.04513", "submitter": "Sayeh Sharify", "authors": "Sayeh Sharify, Mostafa Mahmoud, Alberto Delmas Lascorz, Milos Nikolic,\n  Andreas Moshovos", "title": "Laconic Deep Learning Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate a method for transparently identifying ineffectual computations\nin unmodified Deep Learning models and without affecting accuracy.\nSpecifically, we show that if we decompose multiplications down to the bit\nlevel the amount of work performed during inference for image classification\nmodels can be consistently reduced by two orders of magnitude. In the best case\nstudied of a sparse variant of AlexNet, this approach can ideally reduce\ncomputation work by more than 500x. We present Laconic a hardware accelerator\nthat implements this approach to improve execution time, and energy efficiency\nfor inference with Deep Learning Networks. Laconic judiciously gives up some of\nthe work reduction potential to yield a low-cost, simple, and energy efficient\ndesign that outperforms other state-of-the-art accelerators. For example, a\nLaconic configuration that uses a weight memory interface with just 128 wires\noutperforms a conventional accelerator with a 2K-wire weight memory interface\nby 2.3x on average while being 2.13x more energy efficient on average. A\nLaconic configuration that uses a 1K-wire weight memory interface, outperforms\nthe 2K-wire conventional accelerator by 15.4x and is 1.95x more energy\nefficient. Laconic does not require but rewards advances in model design such\nas a reduction in precision, the use of alternate numeric representations that\nreduce the number of bits that are \"1\", or an increase in weight or activation\nsparsity.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 18:14:08 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sharify", "Sayeh", ""], ["Mahmoud", "Mostafa", ""], ["Lascorz", "Alberto Delmas", ""], ["Nikolic", "Milos", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1805.05926", "submitter": "Lavanya Subramanian", "authors": "Lavanya Subramanian, Vivek Seshadri, Yoongu Kim, Ben Jaiyen, Onur\n  Mutlu", "title": "Predictable Performance and Fairness Through Accurate Slowdown\n  Estimation in Shared Main Memory Systems", "comments": null, "journal-ref": "IPSI BgD Transactions on Advanced Research (TAR), July 2018,\n  Volume 14, Number 2, ISSN 1820 - 4511", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the ideas and key concepts in MISE (Memory\nInterference-induced Slowdown Estimation), which was published in HPCA 2013\n[97], and examines the work's significance and future potential. Applications\nrunning concurrently on a multicore system interfere with each other at the\nmain memory. This interference can slow down different applications\ndifferently. Accurately estimating the slowdown of each application in such a\nsystem can enable mechanisms that can enforce quality-of-service. While much\nprior work has focused on mitigating the performance degradation due to\ninter-application interference, there is little work on accurately estimating\nslowdown of individual applications in a multi-programmed environment. Our goal\nis to accurately estimate application slowdowns, towards providing predictable\nperformance.\n  To this end, we first build a simple Memory Interference-induced Slowdown\nEstimation (MISE) model, which accurately estimates slowdowns caused by memory\ninterference. We then leverage our MISE model to develop two new memory\nscheduling schemes: 1) one that provides soft quality-of-service guarantees,\nand 2) another that explicitly attempts to minimize maximum slowdown (i.e.,\nunfairness) in the system. Evaluations show that our techniques perform\nsignificantly better than state-of-the-art memory scheduling approaches to\naddress the same problems.\n  Our proposed model and techniques have enabled significant research in the\ndevelopment of accurate performance models [35, 59, 98, 110] and interference\nmanagement mechanisms [66, 99, 100, 108, 119, 120].\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 17:42:10 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Subramanian", "Lavanya", ""], ["Seshadri", "Vivek", ""], ["Kim", "Yoongu", ""], ["Jaiyen", "Ben", ""], ["Mutlu", "Onur", ""]]}, {"id": "1805.06050", "submitter": "Soheil Hashemi", "authors": "Soheil Hashemi, Hokchhay Tann, Sherief Reda", "title": "BLASYS: Approximate Logic Synthesis Using Boolean Matrix Factorization", "comments": "To Appear in DAC'18", "journal-ref": null, "doi": "10.1145/3195970.3196001", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing is an emerging paradigm where design accuracy can be\ntraded off for benefits in design metrics such as design area, power\nconsumption or circuit complexity. In this work, we present a novel paradigm to\nsynthesize approximate circuits using Boolean matrix factorization (BMF). In\nour methodology the truth table of a sub-circuit of the design is approximated\nusing BMF to a controllable approximation degree, and the results of the\nfactorization are used to synthesize a less complex subcircuit. To scale our\ntechnique to large circuits, we devise a circuit decomposition method and a\nsubcircuit design-space exploration technique to identify the best order for\nsubcircuit approximations. Our method leads to a smooth trade-off between\naccuracy and full circuit complexity as measured by design area and power\nconsumption. Using an industrial strength design flow, we extensively evaluate\nour methodology on a number of testcases, where we demonstrate that the\nproposed methodology can achieve up to 63% in power savings, while introducing\nan average relative error of 5%. We also compare our work to previous works in\nBoolean circuit synthesis and demonstrate significant improvements in design\nmetrics for same accuracy targets.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 22:18:10 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Hashemi", "Soheil", ""], ["Tann", "Hokchhay", ""], ["Reda", "Sherief", ""]]}, {"id": "1805.06407", "submitter": "Rachata Ausavarungnirun", "authors": "Onur Mutlu, Saugata Ghose, Rachata Ausavarungnirun", "title": "Recent Advances in Overcoming Bottlenecks in Memory Systems and Managing\n  Memory Resources in GPU Systems", "comments": "arXiv admin note: text overlap with arXiv:1805.09127", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article features extended summaries and retrospectives of some of the\nrecent research done by our research group, SAFARI, on (1) various critical\nproblems in memory systems and (2) how memory system bottlenecks affect\ngraphics processing unit (GPU) systems. As more applications share a single\nsystem, operations from each application can contend with each other at various\nshared components. Such contention can slow down each application or thread of\nexecution. The compound effect of contention, high memory latency and access\noverheads, as well as inefficient management of resources, greatly degrades\nperformance, quality-of-service, and energy efficiency. The ten works featured\nin this issue study several aspects of (1) inter-application interference in\nmulticore systems, heterogeneous systems, and GPUs; (2) the growing overheads\nand expenses associated with growing memory densities and latencies; and (3)\nperformance, programmability, and portability issues in modern GPUs, especially\nthose related to memory system resources.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:27:42 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 17:14:33 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Mutlu", "Onur", ""], ["Ghose", "Saugata", ""], ["Ausavarungnirun", "Rachata", ""]]}, {"id": "1805.06747", "submitter": "Reza Salkhordeh", "authors": "Reza Salkhordeh and Shahriar Ebrahimi and Hossein Asadi", "title": "ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems\n  with Online Workload Characterization", "comments": null, "journal-ref": "IEEE TPDS 2018", "doi": "10.1109/TPDS.2018.2796100", "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, SSDs have gained tremendous attention in computing and\nstorage systems due to significant performance improvement over HDDs. The cost\nper capacity of SSDs, however, prevents them from entirely replacing HDDs in\nsuch systems. One approach to effectively take advantage of SSDs is to use them\nas a caching layer to store performance critical data blocks to reduce the\nnumber of accesses to disk subsystem. Due to characteristics of Flash-based\nSSDs such as limited write endurance and long latency on write operations,\nemploying caching algorithms at the Operating System (OS) level necessitates to\ntake such characteristics into consideration. Previous caching techniques are\noptimized towards only one type of application, which affects both generality\nand applicability. In addition, they are not adaptive when the workload pattern\nchanges over time. This paper presents an efficient Reconfigurable Cache\nArchitecture (ReCA) for storage systems using a comprehensive workload\ncharacterization to find an optimal cache configuration for I/O intensive\napplications. For this purpose, we first investigate various types of I/O\nworkloads and classify them into five major classes. Based on this\ncharacterization, an optimal cache configuration is presented for each class of\nworkloads. Then, using the main features of each class, we continuously monitor\nthe characteristics of an application during system runtime and the cache\norganization is reconfigured if the application changes from one class to\nanother class of workloads. The cache reconfiguration is done online and\nworkload classes can be extended to emerging I/O workloads in order to maintain\nits efficiency with the characteristics of I/O requests. Experimental results\nobtained by implementing ReCA in a server running Linux show that the proposed\narchitecture improves performance and lifetime up to 24\\% and 33\\%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 19:24:20 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Salkhordeh", "Reza", ""], ["Ebrahimi", "Shahriar", ""], ["Asadi", "Hossein", ""]]}, {"id": "1805.07409", "submitter": "Pritam Bhattacharjee", "authors": "Pritam Bhattacharjee, Bipasha Nath, Alak Majumder", "title": "LECTOR Based Clock Gating for Low Power Multi-Stage Flip Flop\n  Applications", "comments": null, "journal-ref": "ICEIC 2017 International Conference on Electronics, Information,\n  and Communication, 2017.1, 106-109 (4 pages)", "doi": null, "report-no": null, "categories": "cs.AR cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Power dissipation in integrated circuits is one of the major concerns to the\nresearch community, at the verge when more number of transistors are integrated\non a single chip. The substantial source of power dissipation in sequential\nelements of the integrated circuit is due to the fast switching of high\nfrequency clock signals. These signals do not carry any information and are\nmainly intended to synchronize the operation of sequential components. This\nunnecessary switching of Clock, during the HOLD phase of either logic 1 or\nlogic 0, may be eliminated using a technique, called Clock Gating. In this\npaper, we have incorporated a recent clock gating style called LECTOR based\nclock gating LB CG to drive multi stage architecture and simulated its\nperformance using 90nm CMOS Predictive Technology Model PTM with a power supply\nof 1.1V at 18GHz clock frequency. A substantial savings in terms of average\npower in comparison to its non gated correspondent have been observed.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 07:28:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Bhattacharjee", "Pritam", ""], ["Nath", "Bipasha", ""], ["Majumder", "Alak", ""]]}, {"id": "1805.07718", "submitter": "Myoungsoo Jung", "authors": "Jie Zhang, Shuwen Gao, Nam Sung Kim, Myoungsoo Jung", "title": "CIAO: Cache Interference-Aware Throughput-Oriented Architecture and\n  Scheduling for GPUs", "comments": "IPDPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modern GPU aims to simultaneously execute more warps for higher\nThread-Level Parallelism (TLP) and performance. When generating many memory\nrequests, however, warps contend for limited cache space and thrash cache,\nwhich in turn severely degrades performance. To reduce such cache thrashing, we\nmay adopt cache locality-aware warp scheduling which gives higher execution\npriority to warps with higher potential of data locality. However, we observe\nthat warps with high potential of data locality often incurs far more cache\nthrashing or interference than warps with low potential of data locality.\nConsequently, cache locality-aware warp scheduling may undesirably increase\ncache interference and/or unnecessarily decrease TLP. In this paper, we propose\nCache Interference-Aware throughput-Oriented (CIAO) on-chip memory architecture\nand warp scheduling which exploit unused shared memory space and take insight\nopposite to cache locality-aware warp scheduling. Specifically, CIAO on-chip\nmemory architecture can adaptively redirect memory requests of severely\ninterfering warps to unused shared memory space to isolate memory requests of\nthese interfering warps from those of interfered warps. If these interfering\nwarps still incur severe cache interference, CIAO warp scheduling then begins\nto selectively throttle execution of these interfering warps. Our experiment\nshows that CIAO can offer 54% higher performance than prior cache\nlocality-aware scheduling at a small chip cost.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 07:09:47 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Zhang", "Jie", ""], ["Gao", "Shuwen", ""], ["Kim", "Nam Sung", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1805.07886", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Muralidaran Vijayaraghavan, Andrew Wright, Mehdi\n  Alipour, Arvind", "title": "Constructing a Weak Memory Model", "comments": null, "journal-ref": "Computer Architecture (ISCA), 2018 ACM/IEEE 45th Annual\n  International Symposium on, pp. 124-137. IEEE, 2018", "doi": "10.1109/ISCA.2018.00021", "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak memory models are a consequence of the desire on part of architects to\npreserve all the uniprocessor optimizations while building a shared memory\nmultiprocessor. The efforts to formalize weak memory models of ARM and POWER\nover the last decades are mostly empirical -- they try to capture empirically\nobserved behaviors -- and end up providing no insight into the inherent nature\nof weak memory models. This paper takes a constructive approach to find a\ncommon base for weak memory models: we explore what a weak memory would look\nlike if we constructed it with the explicit goal of preserving all the\nuniprocessor optimizations. We will disallow some optimizations which break a\nprogrammer's intuition in highly unexpected ways. The constructed model, which\nwe call General Atomic Memory Model (GAM), allows all four load/store\nreorderings. We give the construction procedure of GAM, and provide insights\nwhich are used to define its operational and axiomatic semantics. Though no\nattempt is made to match GAM to any existing weak memory model, we show by\nsimulation that GAM has comparable performance with other models. No deep\nknowledge of memory models is needed to read this paper.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:13:51 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 05:57:21 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 18:00:34 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Wright", "Andrew", ""], ["Alipour", "Mehdi", ""], ["Arvind", "", ""]]}, {"id": "1805.08695", "submitter": "Panagiotis Mousouliotis", "authors": "Panagiotis G. Mousouliotis, Loukas P. Petrou", "title": "SqueezeJet: High-level Synthesis Accelerator Design for Deep\n  Convolutional Neural Networks", "comments": "The final publication is available at Springer via\n  https://doi.org/10.1007/978-3-319-78890-6_5", "journal-ref": null, "doi": "10.1007/978-3-319-78890-6_5", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have dominated the pattern recognition\nscene by providing much more accurate solutions in computer vision problems\nsuch as object recognition and object detection. Most of these solutions come\nat a huge computational cost, requiring billions of multiply-accumulate\noperations and, thus, making their use quite challenging in real-time\napplications that run on embedded mobile (resource-power constrained) hardware.\nThis work presents the architecture, the high-level synthesis design, and the\nimplementation of SqueezeJet, an FPGA accelerator for the inference phase of\nthe SqueezeNet DCNN architecture, which is designed specifically for use in\nembedded systems. Results show that SqueezeJet can achieve 15.16 times speed-up\ncompared to the software implementation of SqueezeNet running on an embedded\nmobile processor with less than 1% drop in top-5 accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 21:56:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mousouliotis", "Panagiotis G.", ""], ["Petrou", "Loukas P.", ""]]}, {"id": "1805.09127", "submitter": "Rachata Ausavarungnirun", "authors": "Onur Mutlu, Saugata Ghose, Rachata Ausavarungnirun", "title": "Recent Advances in DRAM and Flash Memory Architectures", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.06407", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article features extended summaries and retrospectives of some of the\nrecent research done by our group, SAFARI, on (1) understanding,\ncharacterizing, and modeling various critical properties of modern DRAM and\nNAND flash memory, the dominant memory and storage technologies, respectively;\nand (2) several new mechanisms we have proposed based on our observations from\nthese analyses, characterization, and modeling, to tackle various key\nchallenges in memory and storage scaling. In order to understand the sources of\nvarious bottlenecks of the dominant memory and storage technologies, these\nworks perform rigorous studies of device-level and application-level behavior,\nusing a combination of detailed simulation and experimental characterization of\nreal memory and storage devices.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:49:36 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 17:13:02 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 05:43:41 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Mutlu", "Onur", ""], ["Ghose", "Saugata", ""], ["Ausavarungnirun", "Rachata", ""]]}, {"id": "1805.09612", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits, Roman Kaplan and Ran Ginosar", "title": "PRINS: Resistive CAM Processing in Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-data in-storage processing research has been gaining momentum in recent\nyears. Typical processing-in-storage architecture places a single or several\nprocessing cores inside the storage and allows data processing without\ntransferring it to the host CPU. Since this approach replicates von Neumann\narchitecture inside storage, it is exposed to the problems faced by von Neumann\narchitecture, especially the bandwidth wall. We present PRINS, a novel in-data\nprocessing-in-storage architecture based on Resistive Content Addressable\nMemory (RCAM). PRINS functions simultaneously as a storage and a massively\nparallel associative processor. PRINS alleviates the bandwidth wall faced by\nconventional processing-in-storage architectures by keeping the computing\ninside the storage arrays, thus implementing in-data, rather than near-data,\nprocessing. We show that PRINS may outperform a reference computer architecture\nwith a bandwidth-limited external storage. The performance of PRINS Euclidean\ndistance, dot product and histogram implementation exceeds the attainable\nperformance of a reference architecture by up to four orders of magnitude,\ndepending on the dataset size. The performance of PRINS SpMV may exceed the\nattainable performance of such reference architecture by more than two orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 11:27:59 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 08:48:50 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 10:28:12 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yavits", "Leonid", ""], ["Kaplan", "Roman", ""], ["Ginosar", "Ran", ""]]}, {"id": "1805.10041", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Michele Weiland, Mark Parsons, Bernhard Homoelle", "title": "Architectures for High Performance Computing and Data Systems using\n  Byte-Addressable Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile, byte addressable, memory technology with performance close to\nmain memory promises to revolutionise computing systems in the near future.\nSuch memory technology provides the potential for extremely large memory\nregions (i.e. > 3TB per server), very high performance I/O, and new ways of\nstoring and sharing data for applications and workflows. This paper outlines an\narchitecture that has been designed to exploit such memory for High Performance\nComputing and High Performance Data Analytics systems, along with descriptions\nof how applications could benefit from such hardware.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:53:05 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Jackson", "Adrian", ""], ["Weiland", "Michele", ""], ["Parsons", "Mark", ""], ["Homoelle", "Bernhard", ""]]}, {"id": "1805.10174", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris and Christos-Savvas Bouganis", "title": "f-CNN$^{\\text{x}}$: A Toolflow for Mapping Multi-CNN Applications on\n  FPGAs", "comments": "Accepted at the 28th International Conference on Field Programmable\n  Logic & Applications (FPL) 2018", "journal-ref": null, "doi": "10.1109/FPL.2018.00072", "report-no": null, "categories": "cs.CV cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive power of Convolutional Neural Networks (CNNs) has been an\nintegral factor for emerging latency-sensitive applications, such as autonomous\ndrones and vehicles. Such systems employ multiple CNNs, each one trained for a\nparticular task. The efficient mapping of multiple CNNs on a single FPGA device\nis a challenging task as the allocation of compute resources and external\nmemory bandwidth needs to be optimised at design time. This paper proposes\nf-CNN$^{\\text{x}}$, an automated toolflow for the optimised mapping of multiple\nCNNs on FPGAs, comprising a novel multi-CNN hardware architecture together with\nan automated design space exploration method that considers the user-specified\nperformance requirements for each model to allocate compute resources and\ngenerate a synthesisable accelerator. Moreover, f-CNN$^{\\text{x}}$ employs a\nnovel scheduling algorithm that alleviates the limitations of the memory\nbandwidth contention between CNNs and sustains the high utilisation of the\narchitecture. Experimental evaluation shows that f-CNN$^{\\text{x}}$'s designs\noutperform contention-unaware FPGA mappings by up to 50% and deliver up to 6.8x\nhigher performance-per-Watt over highly optimised GPU designs for multi-CNN\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:25:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:16:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1805.10431", "submitter": "Marie Nguyen", "authors": "Marie Nguyen and James C. Hoe", "title": "Time-Shared Execution of Realtime Computer Vision Pipelines by Dynamic\n  Partial Reconfiguration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an FPGA runtime framework that demonstrates the\nfeasibility of using dynamic partial reconfiguration (DPR) for time-sharing an\nFPGA by multiple realtime computer vision pipelines. The presented time-sharing\nruntime framework manages an FPGA fabric that can be round-robin time-shared by\ndifferent pipelines at the time scale of individual frames. In this new\nuse-case, the challenge is to achieve useful performance despite high\nreconfiguration time. The paper describes the basic runtime support as well as\nfour optimizations necessary to achieve realtime performance given the\nlimitations of DPR on today's FPGAs. The paper provides a characterization of a\nworking runtime framework prototype on a Xilinx ZC706 development board. The\npaper also reports the performance of realtime computer vision pipelines when\ntime-shared.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 05:48:53 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 02:53:36 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 03:38:40 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Nguyen", "Marie", ""], ["Hoe", "James C.", ""]]}, {"id": "1805.11344", "submitter": "Yuta Tokusashi Mr", "authors": "Yuta Tokusashi, Hiroki Matsutani, Noa Zilberman", "title": "LaKe: An Energy Efficient, Low Latency, Accelerated Key-Value Store", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key-value store is a popular type of cloud computing applications. The\nperformance of key-value store applications have been shown to be very\nsensitive to load within the data center, and in particular to latency. As load\nwithin data center increases, it is becoming hard to maintain key-value store\napplications' performance, without exceeding both the processing capacity of\nhosts and the power budgets of racks. In this paper, we present LaKe: a low\nlatency, power efficient key-value store design for cloud applications. LaKe is\na modular design, combining multiple cores and cache layering, both in hardware\nand software. LaKe achieves full line rate throughput, while maintaining a\nlatency of 1.1us and better power efficiency than existing hardware based\nmemcached designs. Using the modularity of our design, we study trade-offs in\nthe use of on-chip memory, SRAM and DRAM in accelerated designs and provide\ninsights for future architectures.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 10:16:34 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Tokusashi", "Yuta", ""], ["Matsutani", "Hiroki", ""], ["Zilberman", "Noa", ""]]}]