[{"id": "1709.01965", "submitter": "Naveen Kumar Macha", "authors": "Naveen Kumar Macha, Mostafizur Rahman", "title": "Cost Modeling and Projection for Stacked Nanowire Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To continue scaling beyond 2-D CMOS with 3-D integration, any new 3-D IC\ntechnology has to be comparable or better than 2-D CMOS in terms of\nscalability, enhanced functionality, density, power, performance, cost, and\nreliability. Transistor-level 3-D integration carries the most potential in\nthis regard. Recently, we proposed a stacked horizontal nanowire based\ntransistor-level 3-D integration approach, called SN3D [1][2] that solves\nscaling challenges and achieves tremendous benefits with respect to 2-D CMOS\nwhile keeping manageable thermal profile. In this paper, we present the cost\nanalysis of SN3D and show comparison with 2-D CMOS (2D), conventional TSV based\n3-D (T3D) and Monolithic 3-D integrations (M3D). In our cost model, we capture\nthe implications of manufacturing, circuit density, interconnects, bonding and\nheat in determining die cost, and evaluate how cost scales as transistor count\nincreases. Since SN3D is a new 3-D IC fabric, based on our proposed\nmanufacturing pathway[1] we assumed complexity of fabrication steps as\nproportionality constants in our cost estimation model. Our analysis revealed\n86%, 72% and 74% reduction in area; 55%, 43% and 43% reduction in interconnects\ndistribution and total interconnect length for SN3D, which largely contributed\nto 70%, 67% and 68% reduction in cost in comparison to 2D, T3D and M3D\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 19:04:46 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 20:24:23 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Macha", "Naveen Kumar", ""], ["Rahman", "Mostafizur", ""]]}, {"id": "1709.02381", "submitter": "S. Karen Khatamifard", "authors": "S. Karen Khatamifard, Zamshed Chowdhury, Nakul Pande, Meisam\n  Razaviyayn, Chris Kim, Ulya R. Karpuzcu", "title": "Read Mapping Near Non-Volatile Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA sequencing is the physical/biochemical process of identifying the\nlocation of the four bases (Adenine, Guanine, Cytosine, Thymine) in a DNA\nstrand. As semiconductor technology revolutionized computing, modern DNA\nsequencing technology (termed Next Generation Sequencing, NGS)revolutionized\ngenomic research. As a result, modern NGS platforms can sequence hundreds of\nmillions of short DNA fragments in parallel. The sequenced DNA fragments,\nrepresenting the output of NGS platforms, are termed reads. Besides genomic\nvariations, NGS imperfections induce noise in reads. Mapping each read to (the\nmost similar portion of) a reference genome of the same species, i.e., read\nmapping, is a common critical first step in a diverse set of emerging\nbioinformatics applications. Mapping represents a search-heavy memory-intensive\nsimilarity matching problem, therefore, can greatly benefit from near-memory\nprocessing. Intuition suggests using fast associative search enabled by Ternary\nContent Addressable Memory (TCAM) by construction. However, the excessive\nenergy consumption and lack of support for similarity matching (under NGS and\ngenomic variation induced noise) renders direct application of TCAM infeasible,\nirrespective of volatility, where only non-volatile TCAM can accommodate the\nlarge memory footprint in an area-efficient way. This paper introduces GeNVoM,\na scalable, energy-efficient and high-throughput solution. Instead of\noptimizing an algorithm developed for general-purpose computers or GPUs, GeNVoM\nrethinks the algorithm and non-volatile TCAM-based accelerator design together\nfrom the ground up. Thereby GeNVoM can improve the throughput by up to 113.5\ntimes (3.6); the energy consumption, by up to 210.9 times (1.36), when compared\nto a GPU (accelerator) baseline, which represents one of the highest-throughput\nimplementations known.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 03:51:51 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 23:54:43 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 15:20:47 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Khatamifard", "S. Karen", ""], ["Chowdhury", "Zamshed", ""], ["Pande", "Nakul", ""], ["Razaviyayn", "Meisam", ""], ["Kim", "Chris", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "1709.04184", "submitter": "Themistoklis Prodromakis", "authors": "Alexantrou Serb, Ali Khiat, Themis Prodromakis", "title": "Charge-based computing with analogue reconfigurable gates", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world enters the age of ubiquitous computing, the need for\nreconfigurable hardware operating close to the fundamental limits of energy\nconsumption becomes increasingly pressing. Simultaneously, scaling-driven\nperformance improvements within the framework of traditional analogue and\ndigital design become progressively more restricted by fundamental physical\nconstraints. Thus, a true paradigm shift in electronics design is required for\nfuelling the next big burst in technology. Here we lay the foundations of a new\ndesign paradigm that fuses analogue and digital thinking by combining digital\nelectronics with memristive devices for achieving charge-based computation;\ninformation processing where every dissipated charge counts. This is realised\nby introducing memristive devices into standard logic gates, thus rendering\nthem reconfigurable and able to perform analogue computation at a power cost\nclose to digital. The power of this concept is then showcased by experimentally\ndemonstrating a hardware data clusterer and a fuzzy NAND gate using this\nprinciple.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 08:36:39 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Serb", "Alexantrou", ""], ["Khiat", "Ali", ""], ["Prodromakis", "Themis", ""]]}, {"id": "1709.04697", "submitter": "Vishwesh Jatala", "authors": "Vishwesh Jatala, Jayvant Anantpur, Amey Karkare", "title": "GREENER: A Tool for Improving Energy Efficiency of Register Files", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) maintain a large register file to increase\nthe thread level parallelism (TLP). To increase the TLP further, recent GPUs\nhave increased the number of on-chip registers in every generation. However,\nwith the increase in the register file size, the leakage power increases. Also,\nwith the technology advances, the leakage power component has increased and has\nbecome an important consideration for the manufacturing process. The leakage\npower of a register file can be reduced by turning infrequently used registers\ninto low power (drowsy or off) state after accessing them. A major challenge in\ndoing so is the lack of runtime register access information.\n  This paper proposes GREENER (GPU REgister file ENErgy Reducer): a system to\nminimize leakage energy of the register file of GPUs. GREENER employs a\ncompile-time analysis to estimate the run-time register access information. The\nresult of the analysis is used to determine the power state of the registers\n(ON, SLEEP, or OFF) after each instruction. We propose a power optimized\nassembly instruction set that allows GREENER to encode the power state of the\nregisters in the executable itself. The modified assembly, along with a\nrun-time optimization to update the power state of a register during execution,\nresults in significant power reduction.\n  We implemented GREENER in GPGPU-Sim simulator, and used GPUWattch framework\nto measure the register file's leakage power. Evaluation of GREENER on 21\nkernels from CUDASDK, GPGPU-SIM, Parboil, and Rodinia benchmarks suites shows\nan average reduction of register leakage energy by 69.04% and maximum reduction\nof 87.95% with a negligible number of simulation cycles overhead (0.53% on\naverage).\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 10:47:17 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:37:44 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Jatala", "Vishwesh", ""], ["Anantpur", "Jayvant", ""], ["Karkare", "Amey", ""]]}, {"id": "1709.04806", "submitter": "Myoungsoo Jung", "authors": "Miryeong Kwon, Jie Zhang, Gyuyoung Park, Wonil Choi, David Donofrio,\n  John Shalf, Mahmut Kandemir, and Myoungsoo Jung", "title": "TraceTracker: Hardware/Software Co-Evaluation for Large-Scale I/O\n  Workload Reconstruction", "comments": "This paper is accepted by and will be published at 2017 IEEE\n  International Symposium on Workload Characterization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block traces are widely used for system studies, model verifications, and\ndesign analyses in both industry and academia. While such traces include\ndetailed block access patterns, existing trace-driven research unfortunately\noften fails to find true-north due to a lack of runtime contexts such as user\nidle periods and system delays, which are fundamentally linked to the\ncharacteristics of target storage hardware. In this work, we propose\nTraceTracker, a novel hardware/software co-evaluation method that allows users\nto reuse a broad range of the existing block traces by keeping most their\nexecution contexts and user scenarios while adjusting them with new system\ninformation. Specifically, our TraceTracker's software evaluation model can\ninfer CPU burst times and user idle periods from old storage traces, whereas\nits hardware evaluation method remasters the storage traces by interoperating\nthe inferred time information, and updates all inter-arrival times by making\nthem aware of the target storage system. We apply the proposed co-evaluation\nmodel to 577 traces, which were collected by servers from different\ninstitutions and locations a decade ago, and revive the traces on a\nhigh-performance flash-based storage array. The evaluation results reveal that\nthe accuracy of the execution contexts reconstructed by TraceTracker is on\naverage 99% and 96% with regard to the frequency of idle operations and the\ntotal idle periods, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:14:03 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kwon", "Miryeong", ""], ["Zhang", "Jie", ""], ["Park", "Gyuyoung", ""], ["Choi", "Wonil", ""], ["Donofrio", "David", ""], ["Shalf", "John", ""], ["Kandemir", "Mahmut", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1709.05116", "submitter": "Yuan Du", "authors": "Yuan Du, Li Du, Yilei Li, Junjie Su, Mau-Chung Frank Chang", "title": "A Streaming Accelerator for Deep Convolutional Neural Networks with\n  Image and Feature Decomposition for Resource-limited System Applications", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) are widely used in modern artificial\nintelligence (AI) and smart vision systems but also limited by computation\nlatency, throughput, and energy efficiency on a resource-limited scenario, such\nas mobile devices, internet of things (IoT), unmanned aerial vehicles (UAV),\nand so on. A hardware streaming architecture is proposed to accelerate\nconvolution and pooling computations for state-of-the-art deep CNNs. It is\noptimized for energy efficiency by maximizing local data reuse to reduce\noff-chip DRAM data access. In addition, image and feature decomposition\ntechniques are introduced to optimize memory access pattern for an arbitrary\nsize of image and number of features within limited on-chip SRAM capacity. A\nprototype accelerator was implemented in TSMC 65 nm CMOS technology with 2.3 mm\nx 0.8 mm core area, which achieves 144 GOPS peak throughput and 0.8 TOPS/W peak\nenergy efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 09:03:42 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Du", "Yuan", ""], ["Du", "Li", ""], ["Li", "Yilei", ""], ["Su", "Junjie", ""], ["Chang", "Mau-Chung Frank", ""]]}, {"id": "1709.05365", "submitter": "Myoungsoo Jung", "authors": "Sungjoon Koh, Jie Zhang, Miryeong Kwon, Jungyeon Yoon, David Donofrio,\n  Namsung Kim and Myoungsoo Jung", "title": "Understanding System Characteristics of Online Erasure Coding on\n  Scalable, Distributed and Large-Scale SSD Array Systems", "comments": "This paper is accepted by and will be published at 2017 IEEE\n  International Symposium on Workload Characterization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale systems with arrays of solid state disks (SSDs) have become\nincreasingly common in many computing segments. To make such systems resilient,\nwe can adopt erasure coding such as Reed-Solomon (RS) code as an alternative to\nreplication because erasure coding can offer a significantly lower storage cost\nthan replication. To understand the impact of using erasure coding on system\nperformance and other system aspects such as CPU utilization and network\ntraffic, we build a storage cluster consisting of approximately one hundred\nprocessor cores with more than fifty high-performance SSDs, and evaluate the\ncluster with a popular open-source distributed parallel file system, Ceph. Then\nwe analyze behaviors of systems adopting erasure coding from the following five\nviewpoints, compared with those of systems using replication: (1) storage\nsystem I/O performance; (2) computing and software overheads; (3) I/O\namplification; (4) network traffic among storage nodes; (5) the impact of\nphysical data layout on performance of RS-coded SSD arrays. For all these\nanalyses, we examine two representative RS configurations, which are used by\nGoogle and Facebook file systems, and compare them with triple replication that\na typical parallel file system employs as a default fault tolerance mechanism.\nLastly, we collect 54 block-level traces from the cluster and make them\navailable for other researchers.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 14:14:10 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 04:14:56 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Koh", "Sungjoon", ""], ["Zhang", "Jie", ""], ["Kwon", "Miryeong", ""], ["Yoon", "Jungyeon", ""], ["Donofrio", "David", ""], ["Kim", "Namsung", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1709.06614", "submitter": "Yuan Du", "authors": "Yuan Du, Li Du, Xuefeng Gu, Jieqiong Du, X. Shawn Wang, Boyu Hu,\n  Mingzhe Jiang, Xiaoliang Chen, Junjie Su, Subramanian S. Iyer, Mau-Chung\n  Frank Chang", "title": "An Analog Neural Network Computing Engine using CMOS-Compatible\n  Charge-Trap-Transistor (CTT)", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analog neural network computing engine based on CMOS-compatible\ncharge-trap transistor (CTT) is proposed in this paper. CTT devices are used as\nanalog multipliers. Compared to digital multipliers, CTT-based analog\nmultiplier shows significant area and power reduction. The proposed computing\nengine is composed of a scalable CTT multiplier array and energy efficient\nanalog-digital interfaces. Through implementing the sequential analog fabric\n(SAF), the engine mixed-signal interfaces are simplified and hardware overhead\nremains constant regardless of the size of the array. A proof-of-concept 784 by\n784 CTT computing engine is implemented using TSMC 28nm CMOS technology and\noccupied 0.68mm2. The simulated performance achieves 76.8 TOPS (8-bit) with 500\nMHz clock frequency and consumes 14.8 mW. As an example, we utilize this\ncomputing engine to address a classic pattern recognition problem --\nclassifying handwritten digits on MNIST database and obtained a performance\ncomparable to state-of-the-art fully connected neural networks using 8-bit\nfixed-point resolution.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 19:09:16 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 06:38:37 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 10:15:01 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2018 07:36:18 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Du", "Yuan", ""], ["Du", "Li", ""], ["Gu", "Xuefeng", ""], ["Du", "Jieqiong", ""], ["Wang", "X. Shawn", ""], ["Hu", "Boyu", ""], ["Jiang", "Mingzhe", ""], ["Chen", "Xiaoliang", ""], ["Su", "Junjie", ""], ["Iyer", "Subramanian S.", ""], ["Chang", "Mau-Chung Frank", ""]]}, {"id": "1709.07241", "submitter": "Suchandra Banerjee", "authors": "Suchandra Banerjee, Anand Ratna, Suchismita Roy", "title": "Satisfiability Modulo Theory based Methodology for Floorplanning in VLSI\n  Circuits", "comments": "8 pages,5 figures", "journal-ref": null, "doi": null, "report-no": "floor12", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Satisfiability Modulo Theory based formulation for\nfloorplanning in VLSI circuits. The proposed approach allows a number of fixed\nblocks to be placed within a layout region without overlapping and at the same\ntime minimizing the area of the layout region. The proposed approach is\nextended to allow a number of fixed blocks with ability to rotate and flexible\nblocks (with variable width and height) to be placed within a layout without\noverlap. Our target in all cases is reduction in area occupied on a chip which\nis of vital importance in obtaining a good circuit design. Satisfiability\nModulo Theory combines the problem of Boolean satisfiability with domains such\nas convex optimization. Satisfiability Modulo Theory provides a richer modeling\nlanguage than is possible with pure Boolean SAT formulas. We have conducted our\nexperiments on MCNC and GSRC benchmark circuits to calculate the total area\noccupied, amount of deadspace and the total CPU time consumed while placing the\nblocks without overlapping. The results obtained shows clearly that the amount\nof dead space or wasted space is reduced if rotation is applied to the blocks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 10:03:07 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Banerjee", "Suchandra", ""], ["Ratna", "Anand", ""], ["Roy", "Suchismita", ""]]}, {"id": "1709.07529", "submitter": "Md Shahriar Shamim", "authors": "Md Shahriar Shamim, M Meraj Ahmed, Naseef Mansoor, Amlan Ganguly", "title": "Energy-Efficient Wireless Interconnection Framework for Multichip\n  Systems with In-package Memory Stacks", "comments": "To appear in proceedings of the 30th IEEE International\n  System-on-Chip (SoC) Conference, pp. 272-277, Munich, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multichip systems with memory stacks and various processing chips are at the\nheart of platform based designs such as servers and embedded systems. Full\nutilization of the benefits of these integrated multichip systems need a\nseamless, and scalable in-package interconnection framework. However,\nstate-of-the-art inter-chip communication requires long wireline channels which\nincreases energy consumption and latency while decreasing data bandwidth. Here,\nwe propose the design of an energy-efficient, seamless wireless interconnection\nnetwork for multichip systems. We demonstrate with cycle-accurate simulations\nthat such a design reduces the energy consumption and latency while increasing\nthe bandwidth in comparison to modern multichip integration systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 22:14:01 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Shamim", "Md Shahriar", ""], ["Ahmed", "M Meraj", ""], ["Mansoor", "Naseef", ""], ["Ganguly", "Amlan", ""]]}, {"id": "1709.09491", "submitter": "Vignesh Balaji", "authors": "Vignesh Balaji, Dhruva Tirumala, Brandon Lucia", "title": "Flexible Support for Fast Parallel Commutative Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privatizing data is a useful strategy for increasing parallelism in a shared\nmemory multithreaded program. Independent cores can compute independently on\nduplicates of shared data, combining their results at the end of their\ncomputations. Conventional approaches to privatization, however, rely on\nexplicit static or dynamic memory allocation for duplicated state, increasing\nmemory footprint and contention for cache resources, especially in shared\ncaches. In this work, we describe CCache, a system for on-demand privatization\nof data manipulated by commutative operations. CCache garners the benefits of\nprivatization, without the increase in memory footprint or cache occupancy.\nEach core in CCache dynamically privatizes commutatively manipulated data,\noperating on a copy. Periodically or at the end of its computation, the core\nmerges its value with the value resident in memory, and when all cores have\nmerged, the in-memory copy contains the up-to-date value. We describe a\nlow-complexity architectural implementation of CCache that extends a\nconventional multicore to support on-demand privatization without using\nadditional memory for private copies. We evaluate CCache on several high-value\napplications, including random access key-value store, clustering, breadth\nfirst search and graph ranking, showing speedups upto 3.2X.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 15:29:57 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Balaji", "Vignesh", ""], ["Tirumala", "Dhruva", ""], ["Lucia", "Brandon", ""]]}, {"id": "1709.09921", "submitter": "Eric Cheng", "authors": "Eric Cheng, Shahrzad Mirkhani, Lukasz G. Szafaryn, Chen-Yong Cher,\n  Hyungmin Cho, Kevin Skadron, Mircea R. Stan, Klas Lilja, Jacob A. Abraham,\n  Pradip Bose, and Subhasish Mitra", "title": "Tolerating Soft Errors in Processor Cores Using CLEAR (Cross-Layer\n  Exploration for Architecting Resilience)", "comments": "Unedited version of paper published in Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems", "journal-ref": null, "doi": "10.1109/TCAD.2017.2752705", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CLEAR (Cross-Layer Exploration for Architecting Resilience), a\nfirst of its kind framework which overcomes a major challenge in the design of\ndigital systems that are resilient to reliability failures: achieve desired\nresilience targets at minimal costs (energy, power, execution time, area) by\ncombining resilience techniques across various layers of the system stack\n(circuit, logic, architecture, software, algorithm). This is also referred to\nas cross-layer resilience. In this paper, we focus on radiation-induced soft\nerrors in processor cores. We address both single-event upsets (SEUs) and\nsingle-event multiple upsets (SEMUs) in terrestrial environments. Our framework\nautomatically and systematically explores the large space of comprehensive\nresilience techniques and their combinations across various layers of the\nsystem stack (586 cross-layer combinations in this paper), derives\ncost-effective solutions that achieve resilience targets at minimal costs, and\nprovides guidelines for the design of new resilience techniques. Our results\ndemonstrate that a carefully optimized combination of circuit-level hardening,\nlogic-level parity checking, and micro-architectural recovery provides a highly\ncost-effective soft error resilience solution for general-purpose processor\ncores. For example, a 50x improvement in silent data corruption rate is\nachieved at only 2.1% energy cost for an out-of-order core (6.1% for an\nin-order core) with no speed impact. However, (application-aware) selective\ncircuit-level hardening alone, guided by a thorough analysis of the effects of\nsoft errors on application benchmarks, provides a cost-effective soft error\nresilience solution as well (with ~1% additional energy cost for a 50x\nimprovement in silent data corruption rate).\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 12:38:45 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Cheng", "Eric", ""], ["Mirkhani", "Shahrzad", ""], ["Szafaryn", "Lukasz G.", ""], ["Cher", "Chen-Yong", ""], ["Cho", "Hyungmin", ""], ["Skadron", "Kevin", ""], ["Stan", "Mircea R.", ""], ["Lilja", "Klas", ""], ["Abraham", "Jacob A.", ""], ["Bose", "Pradip", ""], ["Mitra", "Subhasish", ""]]}, {"id": "1709.10008", "submitter": "Valentin Touzeau", "authors": "Claire Maiza (VERIMAG - IMAG), Valentin Touzeau (VERIMAG - IMAG),\n  Claire Ma\u00a8{\\i}za, David Monniaux (VERIMAG - IMAG), Jan Reineke", "title": "Ascertaining Uncertainty for Efficient Exact Cache Analysis", "comments": null, "journal-ref": "Rupak Majumdar; Viktor Kuncak. Computer Aided Verification - 29th\n  International Conference, Jul 2017, Heidelberg, France. Springer, 10427 (2),\n  pp.20 - 40, 2017, Lecture notes in computer science.\n  http://cavconference.org/2017/", "doi": "10.1007/978-3-319-63390-9_2", "report-no": null, "categories": "cs.PL cs.AR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static cache analysis characterizes a program's cache behavior by determining\nin a sound but approximate manner which memory accesses result in cache hits\nand which result in cache misses. Such information is valuable in optimizing\ncompilers, worst-case execution time analysis, and side-channel attack\nquantification and mitigation.Cache analysis is usually performed as a\ncombination of `must' and `may' abstract interpretations, classifying\ninstructions as either `always hit', `always miss', or `unknown'. Instructions\nclassified as `unknown' might result in a hit or a miss depending on program\ninputs or the initial cache state. It is equally possible that they do in fact\nalways hit or always miss, but the cache analysis is too coarse to see it.Our\napproach to eliminate this uncertainty consists in (i) a novel abstract\ninterpretation able to ascertain that a particular instruction may definitely\ncause a hit and a miss on different paths, and (ii) an exact analysis, removing\nall remaining uncertainty, based on model checking, using\nabstract-interpretation results to prune down the model for scalability.We\nevaluated our approach on a variety of examples; it notably improves precision\nupon classical abstract interpretation at reasonable cost.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 15:05:54 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 08:38:34 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Maiza", "Claire", "", "VERIMAG - IMAG"], ["Touzeau", "Valentin", "", "VERIMAG - IMAG"], ["Ma\u00a8\u0131za", "Claire", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"], ["Reineke", "Jan", ""]]}, {"id": "1709.10396", "submitter": "Valentin Savin", "authors": "Thien Truong Nguyen-Ly, Valentin Savin, Khoa Le, David Declercq,\n  Fakhreddine Ghaffari, Oana Boncalo", "title": "Analysis and Design of Cost-Effective, High-Throughput LDPC Decoders", "comments": "Submitted to IEEE Transactions on VLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to cost-effective, high-throughput\nhardware designs for Low Density Parity Check (LDPC) decoders. The proposed\napproach, called Non-Surjective Finite Alphabet Iterative Decoders (NS-FAIDs),\nexploits the robustness of message-passing LDPC decoders to inaccuracies in the\ncalculation of exchanged messages, and it is shown to provide a unified\nframework for several designs previously proposed in the literature. NS-FAIDs\nare optimized by density evolution for regular and irregular LDPC codes, and\nare shown to provide different trade-offs between hardware complexity and\ndecoding performance. Two hardware architectures targeting high-throughput\napplications are also proposed, integrating both Min-Sum (MS) and NS-FAID\ndecoding kernels. ASIC post synthesis implementation results on 65nm CMOS\ntechnology show that NS-FAIDs yield significant improvements in the throughput\nto area ratio, by up to 58.75% with respect to the MS decoder, with even better\nor only slightly degraded error correction performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 09:42:17 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Nguyen-Ly", "Thien Truong", ""], ["Savin", "Valentin", ""], ["Le", "Khoa", ""], ["Declercq", "David", ""], ["Ghaffari", "Fakhreddine", ""], ["Boncalo", "Oana", ""]]}]