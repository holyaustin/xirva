[{"id": "1804.00180", "submitter": "Chuan Zhang", "authors": "Chuan Zhang (1 and 2 and 3), Chao Yang (1 and 2 and 3), Wei Xu (3),\n  Shunqing Zhang (4), Zaichen Zhang (2 and 3), Xiaohu You (3) ((1) Lab of\n  Efficient Architectures for Digital-communication and Signal-processing\n  (LEADS), (2) Quantum Information Center, Southeast University, China, (3)\n  National Mobile Communications Research Laboratory, (4) Shanghai Institute\n  for Advanced Communications and Data Science, Shanghai University, Shanghai,\n  China)", "title": "Efficient Sparse Code Multiple Access Decoder Based on Deterministic\n  Message Passing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being an effective non-orthogonal multiple access (NOMA) technique, sparse\ncode multiple access (SCMA) is promising for future wireless communication.\nCompared with orthogonal techniques, SCMA enjoys higher overloading tolerance\nand lower complexity because of its sparsity. In this paper, based on\ndeterministic message passing algorithm (DMPA), algorithmic simplifications\nsuch as domain changing and probability approximation are applied for SCMA\ndecoding. Early termination, adaptive decoding, and initial noise reduction are\nalso employed for faster convergence and better performance. Numerical results\nshow that the proposed optimizations benefit both decoding complexity and\nspeed. Furthermore, efficient hardware architectures based on folding and\nretiming are proposed. VLSI implementation is also given in this paper.\nComparison with the state-of-the-art have shown the proposed decoder's\nadvantages in both latency and throughput (multi-Gbps).\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 14:51:08 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhang", "Chuan", "", "1 and 2 and 3"], ["Yang", "Chao", "", "1 and 2 and 3"], ["Xu", "Wei", "", "2 and 3"], ["Zhang", "Shunqing", "", "2 and 3"], ["Zhang", "Zaichen", "", "2 and 3"], ["You", "Xiaohu", ""]]}, {"id": "1804.00261", "submitter": "Sparsh Mittal", "authors": "Sparsh Mittal", "title": "A Survey of Techniques for Dynamic Branch Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Branch predictor (BP) is an essential component in modern processors since\nhigh BP accuracy can improve performance and reduce energy by decreasing the\nnumber of instructions executed on wrong-path. However, reducing latency and\nstorage overhead of BP while maintaining high accuracy presents significant\nchallenges. In this paper, we present a survey of dynamic branch prediction\ntechniques. We classify the works based on key features to underscore their\ndifferences and similarities. We believe this paper will spark further research\nin this area and will be useful for computer architects, processor designers\nand researchers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 06:08:29 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Mittal", "Sparsh", ""]]}, {"id": "1804.00623", "submitter": "Renzo Andri", "authors": "Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini", "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN\n  Inference Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive results in computer vision and\nmachine learning. Unfortunately, state-of-the-art networks are extremely\ncompute and memory intensive which makes them unsuitable for mW-devices such as\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\nfollow this trend, pushing weight quantization to the limit. Hardware\naccelerators for BWNs presented up to now have focused on core efficiency,\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\nbinary-weight streaming approach, which can be used for arbitrarily sized\nconvolutional neural network architecture and input resolution by exploiting\nthe natural scalability of the compute units both at chip-level and\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\nFP16 arithmetic for increased robustness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:35:42 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 13:25:50 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 11:15:37 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1804.00706", "submitter": "Cheng Tan", "authors": "Guanwen Zhong, Akshat Dubey, Tan Cheng, Tulika Mitra", "title": "Synergy: A HW/SW Framework for High Throughput CNNs on Embedded\n  Heterogeneous SoC", "comments": "34 pages, submitted to ACM Transactions on Embedded Computing Systems\n  (TECS)", "journal-ref": "TECS, 18 (2019) 13-39", "doi": "10.1145/3301278", "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been widely deployed in diverse\napplication domains. There has been significant progress in accelerating both\ntheir training and inference using high-performance GPUs, FPGAs, and custom\nASICs for datacenter-scale environments. The recent proliferation of mobile and\nIoT devices have necessitated real-time, energy-efficient deep neural network\ninference on embedded-class, resource-constrained platforms. In this context,\nwe present {\\em Synergy}, an automated, hardware-software co-designed,\npipelined, high-throughput CNN inference framework on embedded heterogeneous\nsystem-on-chip (SoC) architectures (Xilinx Zynq). {\\em Synergy} leverages,\nthrough multi-threading, all the available on-chip resources, which includes\nthe dual-core ARM processor along with the FPGA and the NEON SIMD engines as\naccelerators. Moreover, {\\em Synergy} provides a unified abstraction of the\nheterogeneous accelerators (FPGA and NEON) and can adapt to different network\nconfigurations at runtime without changing the underlying hardware accelerator\narchitecture by balancing workload across accelerators through work-stealing.\n{\\em Synergy} achieves 7.3X speedup, averaged across seven CNN models, over a\nwell-optimized software-only solution. {\\em Synergy} demonstrates substantially\nbetter throughput and energy-efficiency compared to the contemporary CNN\nimplementations on the same SoC architecture.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 16:02:45 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhong", "Guanwen", ""], ["Dubey", "Akshat", ""], ["Cheng", "Tan", ""], ["Mitra", "Tulika", ""]]}, {"id": "1804.02068", "submitter": "Yang Song", "authors": "Yang Song and Olivier Alavoine and Bill Lin", "title": "SARA: Self-Aware Resource Allocation for Heterogeneous MPSoCs", "comments": "Accepted by the 55th annual Design Automation Conference 2018\n  (DAC'18)", "journal-ref": null, "doi": "10.1145/3195970.3196110", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern heterogeneous MPSoCs, the management of shared memory resources is\ncrucial in delivering end-to-end QoS. Previous frameworks have either focused\non singular QoS targets or the allocation of partitionable resources among CPU\napplications at relatively slow timescales. However, heterogeneous MPSoCs\ntypically require instant response from the memory system where most resources\ncannot be partitioned. Moreover, the health of different cores in a\nheterogeneous MPSoC is often measured by diverse performance objectives. In\nthis work, we propose a Self-Aware Resource Allocation (SARA) framework for\nheterogeneous MPSoCs. Priority-based adaptation allows cores to use different\ntarget performance and self-monitor their own intrinsic health. In response,\nthe system allocates non-partitionable resources based on priorities. The\nproposed framework meets a diverse range of QoS demands from heterogeneous\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 21:49:49 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Song", "Yang", ""], ["Alavoine", "Olivier", ""], ["Lin", "Bill", ""]]}, {"id": "1804.02317", "submitter": "Phillip Stanley-Marbell", "authors": "Phillip Stanley-Marbell and Paul Hurley", "title": "Probabilistic Value-Deviation-Bounded Integer Codes for Approximate\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR cs.ET math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When computing systems can tolerate the effects of errors or erasures in\ntheir communicated data values, they can trade this tolerance for improved\nresource efficiency. One method for enabling this tradeoff in the I/O\nsubsystems of computing systems, is to use channel codes that reduce the power\nneeded to send bits on a channel in exchange for bounded errors and erasures on\nnumeric program values---value-deviation-bounded (VDB) codes. Unlike rate\ndistortion codes, which guarantee a bound on the expected value of channel\ndistortion, the probabilistic VDB codes we present guarantee any desired tail\ndistribution on integer distances of words transmitted over a channel. We\nextend prior work to present tighter upper bounds on the efficiency for VDB\ncodes. We present a new probabilistic VDB encoder that lowers power dissipation\nin exchange for bounded channel integer distortions. The code we present takes\nthe peculiar approach of changing the channel bit error rate across the ordinal\nbit positions in a word to reduce power dissipation. We implement the code\ntable generator in a software tool built on the dReal SMT solver and we\nvalidate the generated codes using Monte Carlo simulation. We present one\nrealization of hardware to implement the technique, requiring 2 mm$^2$ of\ncircuit board area and dissipating less than 0.5 $\\mu$W.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 15:10:29 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Stanley-Marbell", "Phillip", ""], ["Hurley", "Paul", ""]]}, {"id": "1804.02330", "submitter": "Xuan-Thuan Nguyen Dr", "authors": "Xuan-Thuan Nguyen, Trong-Thuc Hoang, Hong-Thu Nguyen, Katsumi Inoue,\n  and Cong-Kha Pham", "title": "An Efficient I/O Architecture for RAM-based Content-Addressable Memory\n  on FPGA", "comments": "Accepted to publish in IEEE Transactions on Circuits and Systems II:\n  Express Brief", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the impressive search rate of one key per clock cycle, the update\nstage of a random-access-memory-based content-addressable-memory (RAM-based\nCAM) always suffers high latency. Two primary causes of such latency include:\n(1) the compulsory erasing stage along with the writing stage and (2) the major\ndifference in data width between the RAM-based CAM (e.g., 8-bit width) and the\nmodern systems (e.g., 256-bit width). This brief, therefore, aims for an\nefficient input/output (I/O) architecture of RAM-based binary CAM (RCAM) for\nlow-latency update. To achieve this goal, three techniques, namely centralized\nerase RAM, bit-sliced, and hierarchical-partitioning, are proposed to eliminate\nthe latency of erasing stage, as well as to allow RCAM to exploit the bandwidth\nof modern systems effectively. Several RCAMs, whose data width ranges from 8\nbits to 64 bits, were integrated into a 256-bit system for the evaluation. The\nexperimental results in an Intel Arria V 5ASTFD5 FPGA prove that at 100 MHz,\nthe proposed designs achieve at least 9.6 times higher I/O efficiency as\ncompared to the traditional RCAM.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 02:14:06 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:41:19 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 23:53:44 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Nguyen", "Xuan-Thuan", ""], ["Hoang", "Trong-Thuc", ""], ["Nguyen", "Hong-Thu", ""], ["Inoue", "Katsumi", ""], ["Pham", "Cong-Kha", ""]]}, {"id": "1804.03893", "submitter": "Andrea Biagioni", "authors": "Roberto Ammendola, Andrea Biagioni, Fabrizio Capuani, Paolo Cretaro,\n  Giulia De Bonis, Francesca Lo Cicero, Alessandro Lonardo, Michele Martinelli,\n  Pier Stanislao Paolucci, Elena Pastorelli, Luca Pontisso, Francesco Simula,\n  Piero Vicini", "title": "Large Scale Low Power Computing System - Status of Network Design in\n  ExaNeSt and EuroExa Projects", "comments": null, "journal-ref": "(2018) Advances in Parallel Computing, 32, pp. 750-759", "doi": "10.3233/978-1-61499-843-3-750", "report-no": null, "categories": "cs.DC cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of the next generation computing platform at ExaFlops scale\nrequires to solve new technological challenges mainly related to the impressive\nnumber (up to 10^6) of compute elements required. This impacts on system power\nconsumption, in terms of feasibility and costs, and on system scalability and\ncomputing efficiency. In this perspective analysis, exploration and evaluation\nof technologies characterized by low power, high efficiency and high degree of\ncustomization is strongly needed. Among the various European initiative\ntargeting the design of ExaFlops system, ExaNeSt and EuroExa are EU-H2020\nfunded initiatives leveraging on high end MPSoC FPGAs. Last generation MPSoC\nFPGAs can be seen as non-mainstream but powerful HPC Exascale enabling\ncomponents thanks to the integration of embedded multi-core, ARM-based low\npower CPUs and a huge number of hardware resources usable to co-design\napplication oriented accelerators and to develop a low latency high bandwidth\nnetwork architecture. In this paper we introduce ExaNet the FPGA-based,\nscalable, direct network architecture of ExaNeSt system. ExaNet allow us to\nexplore different interconnection topologies, to evaluate advanced routing\nfunctions for congestion control and fault tolerance and to design specific\nhardware components for acceleration of collective operations. After a brief\nintroduction of the motivations and goals of ExaNeSt and EuroExa projects, we\nwill report on the status of network architecture design and its\nhardware/software testbed adding preliminary bandwidth and latency\nachievements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:32:02 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Ammendola", "Roberto", ""], ["Biagioni", "Andrea", ""], ["Capuani", "Fabrizio", ""], ["Cretaro", "Paolo", ""], ["De Bonis", "Giulia", ""], ["Cicero", "Francesca Lo", ""], ["Lonardo", "Alessandro", ""], ["Martinelli", "Michele", ""], ["Paolucci", "Pier Stanislao", ""], ["Pastorelli", "Elena", ""], ["Pontisso", "Luca", ""], ["Simula", "Francesco", ""], ["Vicini", "Piero", ""]]}, {"id": "1804.09241", "submitter": "Phillip Stanley-Marbell", "authors": "Phillip Stanley-Marbell and Martin Rinard", "title": "A Hardware Platform for Efficient Multi-Modal Sensing with Adaptive\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.app-ph cs.AR cs.ET cs.RO physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Warp, a hardware platform to support research in approximate\ncomputing, sensor energy optimization, and energy-scavenged systems. Warp\nincorporates 11 state-of-the-art sensor integrated circuits, computation, and\nan energy-scavenged power supply, all within a miniature system that is just\n3.6 cm x 3.3 cm x 0.5 cm. Warp's sensor integrated circuits together contain a\ntotal of 21 sensors with a range of precisions and accuracies for measuring\neight sensing modalities of acceleration, angular rate, magnetic flux density\n(compass heading), humidity, atmospheric pressure (elevation), infrared\nradiation, ambient temperature, and color. Warp uses a combination of analog\ncircuits and digital control to facilitate further tradeoffs between sensor and\ncommunication accuracy, energy efficiency, and performance. This article\npresents the design of Warp and presents an evaluation of our hardware\nimplementation. The results show how Warp's design enables performance and\nenergy efficiency versus ac- curacy tradeoffs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 15:18:32 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Stanley-Marbell", "Phillip", ""], ["Rinard", "Martin", ""]]}, {"id": "1804.10998", "submitter": "Marc Dahlem", "authors": "Marc Dahlem, Anoop Bhagyanath, and Klaus Schneider", "title": "Optimal Scheduling for Exposed Datapath Architectures with Buffered\n  Processing Units by ASP", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018 18 pages,\n  LaTeX, 3 PDF figures (arXiv:YYMM.NNNNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional processor architectures are restricted in exploiting instruction\nlevel parallelism (ILP) due to the relatively low number of programmer-visible\nregisters. Therefore, more recent processor architectures expose their\ndatapaths so that the compiler (1) can schedule parallel instructions to\ndifferent processing units and (2) can make effective use of local storage of\nthe processing units. Among these architectures, the Synchronous Control\nAsynchronous Dataflow (SCAD) architecture is a new exposed datapath\narchitecture whose processing units are equipped with first-in first-out (FIFO)\nbuffers at their input and output ports.\n  In contrast to register-based machines, the optimal code generation for SCAD\nis still a matter of research. In particular, SAT and SMT solvers were used to\ngenerate optimal resource constrained and optimal time constrained schedules\nfor SCAD, respectively. As Answer Set Programming (ASP) offers better\nflexibility in handling such scheduling problems, we focus in this paper on\nusing an answer set solver for both resource and time constrained optimal SCAD\ncode generation. As a major benefit of using ASP, we are able to generate\n\\emph{all} optimal schedules for a given program which allows one to study\ntheir properties. Furthermore, the experimental results of this paper\ndemonstrate that the answer set solver can compete with SAT solvers and\noutperforms SMT solvers. \\emph{This paper is under consideration for acceptance\nin TPLP.}\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 22:28:31 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dahlem", "Marc", ""], ["Bhagyanath", "Anoop", ""], ["Schneider", "Klaus", ""]]}, {"id": "1804.11038", "submitter": "Rachata Ausavarungnirun", "authors": "Rachata Ausavarungnirun, Saugata Ghose, Onur Kay{\\i}ran, Gabriel H.\n  Loh, Chita R. Das, Mahmut T. Kandemir, Onur Mutlu", "title": "Holistic Management of the GPGPU Memory Hierarchy to Manage Warp-level\n  Latency Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a modern GPU architecture, all threads within a warp execute the same\ninstruction in lockstep. For a memory instruction, this can lead to memory\ndivergence: the memory requests for some threads are serviced early, while the\nremaining requests incur long latencies. This divergence stalls the warp, as it\ncannot execute the next instruction until all requests from the current\ninstruction complete. In this work, we make three new observations. First,\nGPGPU warps exhibit heterogeneous memory divergence behavior at the shared\ncache: some warps have most of their requests hit in the cache, while other\nwarps see most of their request miss. Second, a warp retains the same\ndivergence behavior for long periods of execution. Third, requests going to the\nshared cache can incur queuing delays as large as hundreds of cycles,\nexacerbating the effects of memory divergence. We propose a set of techniques,\ncollectively called Memory Divergence Correction (MeDiC), that reduce the\nnegative performance impact of memory divergence and cache queuing. MeDiC\ndelivers an average speedup of 21.8%, and 20.1% higher energy efficiency, over\na state-of-the-art GPU cache management mechanism across 15 different GPGPU\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 03:37:09 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ausavarungnirun", "Rachata", ""], ["Ghose", "Saugata", ""], ["Kay\u0131ran", "Onur", ""], ["Loh", "Gabriel H.", ""], ["Das", "Chita R.", ""], ["Kandemir", "Mahmut T.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1804.11040", "submitter": "Rachata Ausavarungnirun", "authors": "HanBin Yoon, Justin Meza, Rachata Ausavarungnirun, Rachael A. Harding,\n  Onur Mutlu", "title": "A Memory Controller with Row Buffer Locality Awareness for Hybrid Memory\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) is a class of promising scalable memory\ntechnologies that can potentially offer higher capacity than DRAM at the same\ncost point. Unfortunately, the access latency and energy of NVM is often higher\nthan those of DRAM, while the endurance of NVM is lower. Many DRAM-NVM hybrid\nmemory systems use DRAM as a cache to NVM, to achieve the low access latency,\nlow energy, and high endurance of DRAM, while taking advantage of the large\ncapacity of NVM. A key question for a hybrid memory system is what data to\ncache in DRAM to best exploit the advantages of each technology while avoiding\nthe disadvantages of each technology as much as possible.\n  We propose a new memory controller design that improves hybrid memory\nperformance and energy efficiency. We observe that both DRAM and NVM banks\nemploy row buffers that act as a cache for the most recently accessed memory\nrow. Accesses that are row buffer hits incur similar latencies (and energy\nconsumption) in both DRAM and NVM, whereas accesses that are row buffer misses\nincur longer latencies (and higher energy consumption) in NVM than in DRAM. To\nexploit this, we devise a policy that caches heavily-reused data that\nfrequently misses in the NVM row buffers into DRAM. Our policy tracks the row\nbuffer miss counts of recently-used rows in NVM, and caches in DRAM the rows\nthat are predicted to incur frequent row buffer misses. Our proposed policy\nalso takes into account the high write latencies of NVM, in addition to row\nbuffer locality and more likely places the write-intensive pages in DRAM\ninstead of NVM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 03:58:05 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Yoon", "HanBin", ""], ["Meza", "Justin", ""], ["Ausavarungnirun", "Rachata", ""], ["Harding", "Rachael A.", ""], ["Mutlu", "Onur", ""]]}, {"id": "1804.11043", "submitter": "Rachata Ausavarungnirun", "authors": "Rachata Ausavarungnirun, Gabriel H. Loh, Lavanya Subramanian, Kevin\n  Chang, Onur Mutlu", "title": "High-Performance and Energy-Effcient Memory Scheduler Design for\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When multiple processor cores (CPUs) and a GPU integrated together on the\nsame chip share the off-chip DRAM, requests from the GPU can heavily interfere\nwith requests from the CPUs, leading to low system performance and starvation\nof cores. Unfortunately, state-of-the-art memory scheduling algorithms are\nineffective at solving this problem due to the very large amount of GPU memory\ntraffic, unless a very large and costly request buffer is employed to provide\nthese algorithms with enough visibility across the global request stream.\n  Previously-proposed memory controller (MC) designs use a single monolithic\nstructure to perform three main tasks. First, the MC attempts to schedule\ntogether requests to the same DRAM row to increase row buffer hit rates.\nSecond, the MC arbitrates among the requesters (CPUs and GPU) to optimize for\noverall system throughput, average response time, fairness and quality of\nservice. Third, the MC manages the low-level DRAM command scheduling to\ncomplete requests while ensuring compliance with all DRAM timing and power\nconstraints. This paper proposes a fundamentally new approach, called the\nStaged Memory Scheduler (SMS), which decouples the three primary MC tasks into\nthree significantly simpler structures that together improve system performance\nand fairness. Our evaluation shows that SMS provides 41.2% performance\nimprovement and fairness improvement compared to the best previous\nstate-of-the-art technique, while enabling a design that is significantly less\ncomplex and more power-efficient to implement.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 04:17:55 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ausavarungnirun", "Rachata", ""], ["Loh", "Gabriel H.", ""], ["Subramanian", "Lavanya", ""], ["Chang", "Kevin", ""], ["Mutlu", "Onur", ""]]}, {"id": "1804.11239", "submitter": "Caiwen Ding", "authors": "Caiwen Ding, Ao Ren, Geng Yuan, Xiaolong Ma, Jiayu Li, Ning Liu, Bo\n  Yuan, Yanzhi Wang", "title": "Structured Weight Matrices-Based Hardware Accelerators in Deep Neural\n  Networks: FPGAs and ASICs", "comments": "6 pages, 7 figures, GLSVLSI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both industry and academia have extensively investigated hardware\naccelerations. In this work, to address the increasing demands in computational\ncapability and memory requirement, we propose structured weight matrices\n(SWM)-based compression techniques for both \\emph{field programmable gate\narray} (FPGA) and \\emph{application-specific integrated circuit} (ASIC)\nimplementations. In algorithm part, SWM-based framework adopts block-circulant\nmatrices to achieve a fine-grained tradeoff between accuracy and compression\nratio. The SWM-based technique can reduce computational complexity from\nO($n^2$) to O($n\\log n$) and storage complexity from O($n^2$) to O($n$) for\neach layer and both training and inference phases. For FPGA implementations on\ndeep convolutional neural networks (DCNNs), we achieve at least 152X and 72X\nimprovement in performance and energy efficiency, respectively using the\nSWM-based framework, compared with the baseline of IBM TrueNorth processor\nunder same accuracy constraints using the data set of MNIST, SVHN, and\nCIFAR-10. For FPGA implementations on long short term memory (LSTM) networks,\nthe proposed SWM-based LSTM can achieve up to 21X enhancement in performance\nand 33.5X gains in energy efficiency compared with the baseline accelerator.\nFor ASIC implementations, the SWM-based ASIC design exhibits impressive\nadvantages in terms of power, throughput, and energy efficiency. Experimental\nresults indicate that this method is greatly suitable for applying DNNs onto\nboth FPGAs and mobile/IoT devices.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:57:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ding", "Caiwen", ""], ["Ren", "Ao", ""], ["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Li", "Jiayu", ""], ["Liu", "Ning", ""], ["Yuan", "Bo", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1804.11265", "submitter": "Rachata Ausavarungnirun", "authors": "Rachata Ausavarungnirun, Joshua Landgraf, Vance Miller, Saugata Ghose,\n  Jayneel Gandhi, Christopher J. Rossbach, Onur Mutlu", "title": "Mosaic: An Application-Transparent Hardware-Software Cooperative Memory\n  Manager for GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern GPUs face a trade-off on how the page size used for memory management\naffects address translation and demand paging. Support for multiple page sizes\ncan help relax the page size trade-off so that address translation and demand\npaging optimizations work together synergistically. However, existing page\ncoalescing and splintering policies require costly base page migrations that\nundermine the benefits multiple page sizes provide. In this paper, we observe\nthat GPGPU applications present an opportunity to support multiple page sizes\nwithout costly data migration, as the applications perform most of their memory\nallocation en masse (i.e., they allocate a large number of base pages at once).\nWe show that this en masse allocation allows us to create intelligent memory\nallocation policies which ensure that base pages that are contiguous in virtual\nmemory are allocated to contiguous physical memory pages. As a result,\ncoalescing and splintering operations no longer need to migrate base pages.\n  We introduce Mosaic, a GPU memory manager that provides\napplication-transparent support for multiple page sizes. Mosaic uses base pages\nto transfer data over the system I/O bus, and allocates physical memory in a\nway that (1) preserves base page contiguity and (2) ensures that a large page\nframe contains pages from only a single memory protection domain. This\nmechanism allows the TLB to use large pages, reducing address translation\noverhead. During data transfer, this mechanism enables the GPU to transfer only\nthe base pages that are needed by the application over the system I/O bus,\nkeeping demand paging overhead low.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:08:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ausavarungnirun", "Rachata", ""], ["Landgraf", "Joshua", ""], ["Miller", "Vance", ""], ["Ghose", "Saugata", ""], ["Gandhi", "Jayneel", ""], ["Rossbach", "Christopher J.", ""], ["Mutlu", "Onur", ""]]}]