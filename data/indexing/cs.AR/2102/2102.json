[{"id": "2102.00035", "submitter": "Shamma Nasrin", "authors": "Shamma Nasrin, Diaa Badawi, Ahmet Enis Cetin, Wilfred Gomes, and Amit\n  Ranjan Trivedi", "title": "MF-Net: Compute-In-Memory SRAM for Multibit Precision Inference using\n  Memory-immersed Data Conversion and Multiplication-free Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a co-design approach for compute-in-memory inference for deep\nneural networks (DNN). We use multiplication-free function approximators based\non ell_1 norm along with a co-adapted processing array and compute flow. Using\nthe approach, we overcame many deficiencies in the current art of in-SRAM DNN\nprocessing such as the need for digital-to-analog converters (DACs) at each\noperating SRAM row/column, the need for high precision analog-to-digital\nconverters (ADCs), limited support for multi-bit precision weights, and limited\nvector-scale parallelism. Our co-adapted implementation seamlessly extends to\nmulti-bit precision weights, it doesn't require DACs, and it easily extends to\nhigher vector-scale parallelism. We also propose an SRAM-immersed successive\napproximation ADC (SA-ADC), where we exploit the parasitic capacitance of bit\nlines of SRAM array as a capacitive DAC. Since the dominant area overhead in\nSA-ADC comes due to its capacitive DAC, by exploiting the intrinsic parasitic\nof SRAM array, our approach allows low area implementation of within-SRAM\nSA-ADC. Our 8$\\times$62 SRAM macro, which requires a 5-bit ADC, achieves\n$\\sim$105 tera operations per second per Watt (TOPS/W) with 8-bit input/weight\nprocessing at 45 nm CMOS.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 19:35:19 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Nasrin", "Shamma", ""], ["Badawi", "Diaa", ""], ["Cetin", "Ahmet Enis", ""], ["Gomes", "Wilfred", ""], ["Trivedi", "Amit Ranjan", ""]]}, {"id": "2102.00075", "submitter": "Mark Wilkening", "authors": "Mark Wilkening, Udit Gupta, Samuel Hsia, Caroline Trippel, Carole-Jean\n  Wu, David Brooks, Gu-Yeon Wei", "title": "RecSSD: Near Data Processing for Solid State Drive Based Recommendation\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural personalized recommendation models are used across a wide variety of\ndatacenter applications including search, social media, and entertainment.\nState-of-the-art models comprise large embedding tables that have billions of\nparameters requiring large memory capacities. Unfortunately, large and fast\nDRAM-based memories levy high infrastructure costs. Conventional SSD-based\nstorage solutions offer an order of magnitude larger capacity, but have worse\nread latency and bandwidth, degrading inference performance. RecSSD is a near\ndata processing based SSD memory system customized for neural recommendation\ninference that reduces end-to-end model inference latency by 2X compared to\nusing COTS SSDs across eight industry-representative models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 21:25:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wilkening", "Mark", ""], ["Gupta", "Udit", ""], ["Hsia", "Samuel", ""], ["Trippel", "Caroline", ""], ["Wu", "Carole-Jean", ""], ["Brooks", "David", ""], ["Wei", "Gu-Yeon", ""]]}, {"id": "2102.00294", "submitter": "Ian Colbert", "authors": "Ian Colbert, Jake Daly, Ken Kreutz-Delgado, and Srinjoy Das", "title": "A Competitive Edge: Can FPGAs Beat GPUs at DCNN Inference Acceleration\n  in Resource-Limited Edge Computing Applications?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trained as generative models, Deep Learning algorithms have shown\nexceptional performance on tasks involving high dimensional data such as image\ndenoising and super-resolution. In an increasingly connected world dominated by\nmobile and edge devices, there is surging demand for these algorithms to run\nlocally on embedded platforms. FPGAs, by virtue of their reprogrammability and\nlow-power characteristics, are ideal candidates for these edge computing\napplications. As such, we design a spatio-temporally parallelized hardware\narchitecture capable of accelerating a deconvolution algorithm optimized for\npower-efficient inference on a resource-limited FPGA. We propose this\nFPGA-based accelerator to be used for Deconvolutional Neural Network (DCNN)\ninference in low-power edge computing applications. To this end, we develop\nmethods that systematically exploit micro-architectural innovations, design\nspace exploration, and statistical analysis. Using a Xilinx PYNQ-Z2 FPGA, we\nleverage our architecture to accelerate inference for two DCNNs trained on the\nMNIST and CelebA datasets using the Wasserstein GAN framework. On these\nnetworks, our FPGA design achieves a higher throughput to power ratio with\nlower run-to-run variation when compared to the NVIDIA Jetson TX1 edge\ncomputing GPU.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 19:20:14 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 05:19:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Colbert", "Ian", ""], ["Daly", "Jake", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "2102.00554", "submitter": "Torsten Hoefler", "authors": "Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra\n  Peste", "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference\n  and training in neural networks", "comments": "90 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing energy and performance costs of deep learning have driven the\ncommunity to reduce the size of neural networks by selectively pruning\ncomponents. Similarly to their biological counterparts, sparse networks\ngeneralize just as well, if not better than, the original dense networks.\nSparsity can reduce the memory footprint of regular networks to fit mobile\ndevices, as well as shorten training time for ever growing networks. In this\npaper, we survey prior work on sparsity in deep learning and provide an\nextensive tutorial of sparsification for both inference and training. We\ndescribe approaches to remove and add elements of neural networks, different\ntraining strategies to achieve model sparsity, and mechanisms to exploit\nsparsity in practice. Our work distills ideas from more than 300 research\npapers and provides guidance to practitioners who wish to utilize sparsity\ntoday, as well as to researchers whose goal is to push the frontier forward. We\ninclude the necessary background on mathematical methods in sparsification,\ndescribe phenomena such as early structure adaptation, the intricate relations\nbetween sparsity and the training process, and show techniques for achieving\nacceleration on real hardware. We also define a metric of pruned parameter\nefficiency that could serve as a baseline for comparison of different sparse\nnetworks. We close by speculating on how sparsity can improve future workloads\nand outline major open problems in the field.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 22:48:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hoefler", "Torsten", ""], ["Alistarh", "Dan", ""], ["Ben-Nun", "Tal", ""], ["Dryden", "Nikoli", ""], ["Peste", "Alexandra", ""]]}, {"id": "2102.00818", "submitter": "Frank Hannig", "authors": "Frank Hannig, Paolo Meloni, Matteo Spallanzani, Matthias Ziegler", "title": "Proceedings of the DATE Friday Workshop on System-level Design Methods\n  for Deep Learning on Heterogeneous Architectures (SLOHA 2021)", "comments": "Website of the workshop: https://www12.cs.fau.de/ws/sloha2021/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This volume contains the papers accepted at the first DATE Friday Workshop on\nSystem-level Design Methods for Deep Learning on Heterogeneous Architectures\n(SLOHA 2021), held virtually on February 5, 2021. SLOHA 2021 was co-located\nwith the Conference on Design, Automation and Test in Europe (DATE).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:14:02 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hannig", "Frank", ""], ["Meloni", "Paolo", ""], ["Spallanzani", "Matteo", ""], ["Ziegler", "Matthias", ""]]}, {"id": "2102.00932", "submitter": "Bernhard Klein", "authors": "Bernhard Klein and Christoph Gratl and Manfred M\\\"ucke and Holger\n  Fr\\\"oning", "title": "Understanding Cache Boundness of ML Operators on ARM Processors", "comments": "published at the HiPEAC 2021 Conference, at the 3rd Workshop on\n  Accelerated Machine Learning (AccML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning compilers like TVM allow a fast and flexible deployment on\nembedded CPUs. This enables the use of non-standard operators, which are common\nin ML compression techniques. However, it is necessary to understand the\nlimitations of typical compute-intense operators in ML workloads to design a\nproper solution. This is the first in-detail analysis of dense and convolution\noperators, generated with TVM, that compares to the fundamental hardware limits\nof embedded ARM processors. Thereby it explains the gap between computational\npeak performance, theoretical and measured, and real-world state-of-the-art\nresults, created with TVM and openBLAS. Instead, one can see that\nsingle-precision general matrix multiply (GEMM) and convolutions are bound by\nL1-cache-read bandwidth. Explorations of 8-bit and bit-serial quantized\noperators show that quantization can be used to achieve relevant speedups\ncompared to cache-bound floating-point operators. However, the performance of\nquantized operators highly depends on the interaction between data layout and\nbit packing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:05:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Klein", "Bernhard", ""], ["Gratl", "Christoph", ""], ["M\u00fccke", "Manfred", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2102.01341", "submitter": "Quentin Ducasse", "authors": "Quentin Ducasse, Pascal Cotret, Lo\\\"ic Lagadec, Robert Stewart", "title": "Benchmarking Quantized Neural Networks on FPGAs with FINN", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/03", "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing cost of both training and inference for state-of-the-art\nneural networks has brought literature to look upon ways to cut off resources\nused with a minimal impact on accuracy. Using lower precision comes at the cost\nof negligible loss in accuracy. While training neural networks may require a\npowerful setup, deploying a network must be possible on low-power and\nlow-resource hardware architectures. Reconfigurable architectures have proven\nto be more powerful and flexible than GPUs when looking at a specific\napplication. This article aims to assess the impact of mixed-precision when\napplied to neural networks deployed on FPGAs. While several frameworks exist\nthat create tools to deploy neural networks using reduced-precision, few of\nthem assess the importance of quantization and the framework quality. FINN and\nBrevitas, two frameworks from Xilinx labs, are used to assess the impact of\nquantization on neural networks using 2 to 8 bit precisions and weights with\nseveral parallelization configurations. Equivalent accuracy can be obtained\nusing lower-precision representation and enough training. However, the\ncompressed network can be better parallelized allowing the deployed network\nthroughput to be 62 times faster. The benchmark set up in this work is\navailable in a public repository (https://github.com/QDucasse/nn benchmark).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:42:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ducasse", "Quentin", ""], ["Cotret", "Pascal", ""], ["Lagadec", "Lo\u00efc", ""], ["Stewart", "Robert", ""]]}, {"id": "2102.01343", "submitter": "Walther Carballo-Hern\\'andez", "authors": "Walther Carballo-Hern\\'andez, Maxime Pelcat, Fran\\c{c}ois Berry", "title": "Why is FPGA-GPU Heterogeneity the Best Option for Embedded Deep Neural\n  Networks?", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/04", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Processing Units (GPUs) are currently the dominating programmable\narchitecture for Deep Learning (DL) accelerators. The adoption of Field\nProgrammable Gate Arrays (FPGAs) in DL accelerators is however getting\nmomentum. In this paper, we demonstrate that Direct Hardware Mapping (DHM) of a\nConvolutional Neural Network (CNN) on an embedded FPGA substantially\noutperforms a GPU implementation in terms of energy efficiency and execution\ntime. However, DHM is highly resource intensive and cannot fully substitute the\nGPU when implementing a state-of-the-art CNN. We thus propose a hybrid FPGA-GPU\nDL acceleration method and demonstrate that heterogeneous acceleration\noutperforms GPU acceleration even including communication overheads.\n  Experimental results are conducted on a heterogeneous multi-platform setup\nembedding an Nvidia(R) Jetson TX2 CPU-GPU board and an Intel(R) Cyclone10GX\nFPGA board. The SqueezeNet, MobileNetv2, and ShuffleNetv2 mobile-oriented CNNs\nare experimented. We show that heterogeneous FPG-AGPU acceleration outperforms\nGPU acceleration for classification inference task over MobileNetv2 (12%-30%\nenergy reduction, 4% to 26% latency reduction), SqueezeNet (21%-28% energy\nreduction, same latency), and ShuffleNetv2 (25% energy reduction, 21% latency\nreduction).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:43:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Carballo-Hern\u00e1ndez", "Walther", ""], ["Pelcat", "Maxime", ""], ["Berry", "Fran\u00e7ois", ""]]}, {"id": "2102.01349", "submitter": "Cristina Chesta", "authors": "Cristina Chesta, Luca Rinelli", "title": "Modular approach to data preprocessing in ALOHA and application to a\n  smart industry use case", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/08", "categories": "cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in the smart industry domain, such as interaction with\ncollaborative robots using vocal commands or machine vision systems often\nrequires the deployment of deep learning algorithms on heterogeneous low power\ncomputing platforms. The availability of software tools and frameworks to\nautomatize different design steps can support the effective implementation of\nDL algorithms on embedded systems, reducing related effort and costs. One very\nimportant aspect for the acceptance of the framework, is its extensibility,\ni.e. the capability to accommodate different datasets and define customized\npreprocessing, without requiring advanced skills. The paper addresses a modular\napproach, integrated into the ALOHA tool flow, to support the data\npreprocessing and transformation pipeline. This is realized through\ncustomizable plugins and allows the easy extension of the tool flow to\nencompass new use cases. To demonstrate the effectiveness of the approach, we\npresent some experimental results related to a keyword spotting use case and we\noutline possible extensions to different use cases.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:48:51 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chesta", "Cristina", ""], ["Rinelli", "Luca", ""]]}, {"id": "2102.01351", "submitter": "Olivia Weng", "authors": "Olivia Weng, Alireza Khodamoradi, Ryan Kastner", "title": "Hardware-efficient Residual Networks for FPGAs", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/10", "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (ResNets) employ skip connections in their networks --\nreusing activations from previous layers -- to improve training convergence,\nbut these skip connections create challenges for hardware implementations of\nResNets. The hardware must either wait for skip connections to be processed\nbefore processing more incoming data or buffer them elsewhere. Without skip\nconnections, ResNets would be more hardware-efficient. Thus, we present the\nteacher-student learning method to gradually prune away all of a ResNet's skip\nconnections, constructing a network we call NonResNet. We show that when\nimplemented for FPGAs, NonResNet decreases ResNet's BRAM utilization by 9% and\nLUT utilization by 3% and increases throughput by 5%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:50:22 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Weng", "Olivia", ""], ["Khodamoradi", "Alireza", ""], ["Kastner", "Ryan", ""]]}, {"id": "2102.01723", "submitter": "Amir Yazdanbakhsh", "authors": "Amir Yazdanbakhsh, Christof Angermueller, Berkin Akin, Yanqi Zhou,\n  Albin Jones, Milad Hashemi, Kevin Swersky, Satrajit Chatterjee, Ravi\n  Narayanaswami, James Laudon", "title": "Apollo: Transferable Architecture Exploration", "comments": "10 pages, 5 figures, Accepted to Workshop on ML for Systems at the\n  34th Conference on Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The looming end of Moore's Law and ascending use of deep learning drives the\ndesign of custom accelerators that are optimized for specific neural\narchitectures. Architecture exploration for such accelerators forms a\nchallenging constrained optimization problem over a complex, high-dimensional,\nand structured input space with a costly to evaluate objective function.\nExisting approaches for accelerator design are sample-inefficient and do not\ntransfer knowledge between related optimizations tasks with different design\nconstraints, such as area and/or latency budget, or neural architecture\nconfigurations. In this work, we propose a transferable architecture\nexploration framework, dubbed Apollo, that leverages recent advances in\nblack-box function optimization for sample-efficient accelerator design. We use\nthis framework to optimize accelerator configurations of a diverse set of\nneural architectures with alternative design constraints. We show that our\nframework finds high reward design configurations (up to 24.6% speedup) more\nsample-efficiently than a baseline black-box optimization approach. We further\nshow that by transferring knowledge between target architectures with different\ndesign constraints, Apollo is able to find optimal configurations faster and\noften with better objective value (up to 25% improvements). This encouraging\noutcome portrays a promising path forward to facilitate generating higher\nquality accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 19:36:02 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Yazdanbakhsh", "Amir", ""], ["Angermueller", "Christof", ""], ["Akin", "Berkin", ""], ["Zhou", "Yanqi", ""], ["Jones", "Albin", ""], ["Hashemi", "Milad", ""], ["Swersky", "Kevin", ""], ["Chatterjee", "Satrajit", ""], ["Narayanaswami", "Ravi", ""], ["Laudon", "James", ""]]}, {"id": "2102.01764", "submitter": "Fatemeh Golshan", "authors": "Ali Ansari and Fatemeh Golshan and Pejman Lotfi-Kamran and Hamid\n  Sarbazi-Azad", "title": "MANA: Microarchitecting an Instruction Prefetcher", "comments": "24 pages with 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  L1 instruction (L1-I) cache misses are a source of performance bottleneck.\nSequential prefetchers are simple solutions to mitigate this problem; however,\nprior work has shown that these prefetchers leave considerable potentials\nuncovered. This observation has motivated many researchers to come up with more\nadvanced instruction prefetchers. In 2011, Proactive Instruction Fetch (PIF)\nshowed that a hardware prefetcher could effectively eliminate all of the\ninstruction-cache misses. However, its enormous storage cost makes it an\nimpractical solution. Consequently, reducing the storage cost was the main\nresearch focus in the instruction prefetching in the past decade. Several\ninstruction prefetchers, including RDIP and Shotgun, were proposed to offer\nPIF-level performance with significantly lower storage overhead. However, our\nfindings show that there is a considerable performance gap between these\nproposals and PIF. While these proposals use different mechanisms for\ninstruction prefetching, the performance gap is largely not because of the\nmechanism, and instead, is due to not having sufficient storage. Prior\nproposals suffer from one or both of the following shortcomings: (1) a large\nnumber of metadata records to cover the potential, and (2) a high storage cost\nof each record. The first problem causes metadata miss, and the second problem\nprohibits the prefetcher from storing enough records within reasonably-sized\nstorage.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 21:32:52 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Ansari", "Ali", ""], ["Golshan", "Fatemeh", ""], ["Lotfi-Kamran", "Pejman", ""], ["Sarbazi-Azad", "Hamid", ""]]}, {"id": "2102.02035", "submitter": "Koen Bertels", "authors": "K. Bertels, A. Sarkar, A. Krol, R. Budhrani, J. Samadi, E. Geoffroy,\n  J. Matos, R. Abreu, G. Gielen, I. Ashraf", "title": "Quantum Accelerator Stack: A Research Roadmap", "comments": "39 pages. arXiv admin note: text overlap with arXiv:1903.09575", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the definition and implementation of a quantum computer\narchitecture to enable creating a new computational device - a quantum computer\nas an accelerator In this paper, we present explicitly the idea of a quantum\naccelerator which contains the full stack of the layers of an accelerator. Such\na stack starts at the highest level describing the target application of the\naccelerator. Important to realise is that qubits are defined as perfect qubits,\nimplying they do not decohere and perform good quantum gate operations. The\nnext layer abstracts the quantum logic outlining the algorithm that is to be\nexecuted on the quantum accelerator. In our case, the logic is expressed in the\nuniversal quantum-classical hybrid computation language developed in the group,\ncalled OpenQL. We also have to start thinking about how to verify, validate and\ntest the quantum software such that the compiler generates a correct version of\nthe quantum circuit. The OpenQL compiler translates the program to a common\nassembly language, called cQASM. We need to develop a quantum operating system\nthat manages all the hardware of the micro-architecture. The layer below the\nmicro-architecture is responsible of the mapping and routing of the qubits on\nthe topology such that the nearest-neighbour-constraint can be be respected. At\nany moment in the future when we are capable of generating multiple good\nqubits, the compiler can convert the cQASM to generate the eQASM, which is\nexecutable on a particular experimental device incorporating the\nplatform-specific parameters. This way, we are able to distinguish clearly the\nexperimental research towards better qubits, and the industrial and societal\napplications that need to be developed and executed on a quantum device.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 12:33:12 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 08:26:53 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 20:11:57 GMT"}, {"version": "v4", "created": "Mon, 17 May 2021 12:17:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bertels", "K.", ""], ["Sarkar", "A.", ""], ["Krol", "A.", ""], ["Budhrani", "R.", ""], ["Samadi", "J.", ""], ["Geoffroy", "E.", ""], ["Matos", "J.", ""], ["Abreu", "R.", ""], ["Gielen", "G.", ""], ["Ashraf", "I.", ""]]}, {"id": "2102.02308", "submitter": "Timothy Trippel", "authors": "Timothy Trippel, Kang G. Shin, Alex Chernyakhovsky, Garret Kelly,\n  Dominic Rizzo, Matthew Hicks", "title": "Fuzzing Hardware Like Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware flaws are permanent and potent: hardware cannot be patched once\nfabricated, and any flaws may undermine any software executing on top.\nConsequently, verification time dominates implementation time. The gold\nstandard in hardware Design Verification (DV) is concentrated at two extremes:\nrandom dynamic verification and formal verification. Both struggle to root out\nthe subtle flaws in complex hardware that often manifest as security\nvulnerabilities. The root problem with random verification is its undirected\nnature, making it inefficient, while formal verification is constrained by the\nstate-space explosion problem, making it infeasible against complex designs.\nWhat is needed is a solution that is directed, yet under-constrained.\n  Instead of making incremental improvements to existing DV approaches, we\nleverage the observation that existing software fuzzers already provide such a\nsolution, and adapt them for hardware DV. Specifically, we translate RTL\nhardware to a software model and fuzz that model. The central challenge we\naddress is how best to mitigate the differences between the hardware execution\nmodel and software execution model. This includes: 1) how to represent test\ncases, 2) what is the hardware equivalent of a crash, 3) what is an appropriate\ncoverage metric, and 4) how to create a general-purpose fuzzing harness for\nhardware.\n  To evaluate our approach, we fuzz four IP blocks from Google's OpenTitan SoC.\nOur experiments reveal a two orders-of-magnitude reduction in run time to\nachieve Finite State Machine (FSM) coverage over traditional dynamic\nverification schemes. Moreover, with our design-agnostic harness, we achieve\nover 88% HDL line coverage in three out of four of our designs -- even without\nany initial seeds.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 22:03:30 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Trippel", "Timothy", ""], ["Shin", "Kang G.", ""], ["Chernyakhovsky", "Alex", ""], ["Kelly", "Garret", ""], ["Rizzo", "Dominic", ""], ["Hicks", "Matthew", ""]]}, {"id": "2102.02645", "submitter": "Christopher A. Metz", "authors": "Christopher A. Metz, Mehran Goli, Rolf Drechsler", "title": "Pick the Right Edge Device: Towards Power and Performance Estimation of\n  CUDA-based CNNs on GPGPUs", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/06", "categories": "cs.LG cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Machine Learning (ML) as a powerful technique has been\nhelping nearly all fields of business to increase operational efficiency or to\ndevelop new value propositions. Besides the challenges of deploying and\nmaintaining ML models, picking the right edge device (e.g., GPGPUs) to run\nthese models (e.g., CNN with the massive computational process) is one of the\nmost pressing challenges faced by organizations today. As the cost of renting\n(on Cloud) or purchasing an edge device is directly connected to the cost of\nfinal products or services, choosing the most efficient device is essential.\nHowever, this decision making requires deep knowledge about performance and\npower consumption of the ML models running on edge devices that must be\nidentified at the early stage of ML workflow.\n  In this paper, we present a novel ML-based approach that provides ML\nengineers with the early estimation of both power consumption and performance\nof CUDA-based CNNs on GPGPUs. The proposed approach empowers ML engineers to\npick the most efficient GPGPU for a given CNN model at the early stage of\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:46:53 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Metz", "Christopher A.", ""], ["Goli", "Mehran", ""], ["Drechsler", "Rolf", ""]]}, {"id": "2102.02988", "submitter": "Srivatsan Krishnan", "authors": "Srivatsan Krishnan, Zishen Wan, Kshitij Bharadwaj, Paul Whatmough,\n  Aleksandra Faust, Sabrina Neuman, Gu-Yeon Wei, David Brooks, Vijay Janapa\n  Reddi", "title": "Machine Learning-Based Automated Design Space Exploration for Autonomous\n  Aerial Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building domain-specific architectures for autonomous aerial robots is\nchallenging due to a lack of systematic methodology for designing onboard\ncompute. We introduce a novel performance model called the F-1 roofline to help\narchitects understand how to build a balanced computing system for autonomous\naerial robots considering both its cyber (sensor rate, compute performance) and\nphysical components (body-dynamics) that affect the performance of the machine.\nWe use F-1 to characterize commonly used learning-based autonomy algorithms\nwith onboard platforms to demonstrate the need for cyber-physical co-design. To\nnavigate the cyber-physical design space automatically, we subsequently\nintroduce AutoPilot. This push-button framework automates the co-design of\ncyber-physical components for aerial robots from a high-level specification\nguided by the F-1 model. AutoPilot uses Bayesian optimization to automatically\nco-design the autonomy algorithm and hardware accelerator while considering\nvarious cyber-physical parameters to generate an optimal design under different\ntask level complexities for different robots and sensor framerates. As a\nresult, designs generated by AutoPilot, on average, lower mission time up to 2x\nover baseline approaches, conserving battery energy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 03:50:54 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Krishnan", "Srivatsan", ""], ["Wan", "Zishen", ""], ["Bharadwaj", "Kshitij", ""], ["Whatmough", "Paul", ""], ["Faust", "Aleksandra", ""], ["Neuman", "Sabrina", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "2102.03045", "submitter": "Nae-Chyun Chen", "authors": "Nae-Chyun Chen, Yu-Cheng Li and Yi-Chang Lu", "title": "A Memory-Efficient FM-Index Constructor for Next-Generation Sequencing\n  Applications on FPGAs", "comments": null, "journal-ref": "2018 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FM-index is an efficient data structure for string search and is widely used\nin next-generation sequencing (NGS) applications such as sequence alignment and\nde novo assembly. Recently, FM-indexing is even performed down to the read\nlevel, raising a demand of an efficient algorithm for FM-index construction. In\nthis work, we propose a hardware-compatible Self-Aided Incremental Indexing\n(SAII) algorithm and its hardware architecture. This novel algorithm builds\nFM-index with no memory overhead, and the hardware system for realizing the\nalgorithm can be very compact. Parallel architecture and a special prefetch\ncontroller is designed to enhance computational efficiency. An SAII-based\nFM-index constructor is implemented on an Altera Stratix V FPGA board. The\npresented constructor can support DNA sequences of sizes up to 131,072-bp,\nwhich is enough for small-scale references and reads obtained from current\nmajor platforms. Because the proposed constructor needs very few hardware\nresource, it can be easily integrated into different hardware accelerators\ndesigned for FM-index-based applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:04:01 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Chen", "Nae-Chyun", ""], ["Li", "Yu-Cheng", ""], ["Lu", "Yi-Chang", ""]]}, {"id": "2102.04010", "submitter": "Aojun Zhou", "authors": "Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan,\n  Wenxiu Sun, Hongsheng Li", "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch", "comments": "ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress\nand accelerate the models on resource-constrained environments. It can be\ngenerally categorized into unstructured fine-grained sparsity that zeroes out\nmultiple individual weights distributed across the neural network, and\nstructured coarse-grained sparsity which prunes blocks of sub-networks of a\nneural network. Fine-grained sparsity can achieve a high compression ratio but\nis not hardware friendly and hence receives limited speed gains. On the other\nhand, coarse-grained sparsity cannot concurrently achieve both apparent\nacceleration on modern GPUs and decent performance. In this paper, we are the\nfirst to study training from scratch an N:M fine-grained structured sparse\nnetwork, which can maintain the advantages of both unstructured fine-grained\nsparsity and structured coarse-grained sparsity simultaneously on specifically\ndesigned GPUs. Specifically, a 2:4 sparse network could achieve 2x speed-up\nwithout performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel\nand effective ingredient, sparse-refined straight-through estimator (SR-STE),\nto alleviate the negative influence of the approximated gradients computed by\nvanilla STE during optimization. We also define a metric, Sparse Architecture\nDivergence (SAD), to measure the sparse network's topology change during the\ntraining process. Finally, We justify SR-STE's advantages with SAD and\ndemonstrate the effectiveness of SR-STE by performing comprehensive experiments\non various tasks. Source codes and models are available at\nhttps://github.com/NM-sparsity/NM-sparsity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 05:55:47 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 10:18:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Aojun", ""], ["Ma", "Yukun", ""], ["Zhu", "Junnan", ""], ["Liu", "Jianbo", ""], ["Zhang", "Zhijie", ""], ["Yuan", "Kun", ""], ["Sun", "Wenxiu", ""], ["Li", "Hongsheng", ""]]}, {"id": "2102.04270", "submitter": "Erwei Wang", "authors": "Erwei Wang, James J. Davis, Daniele Moro, Piotr Zielinski, Claudionor\n  Coelho, Satrajit Chatterjee, Peter Y. K. Cheung, George A. Constantinides", "title": "Enabling Binary Neural Network Training on the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing computational demands of increasingly complex machine\nlearning models frequently necessitate the use of powerful cloud-based\ninfrastructure for their training. Binary neural networks are known to be\npromising candidates for on-device inference due to their extreme compute and\nmemory savings over higher-precision alternatives. However, their existing\ntraining methods require the concurrent storage of high-precision activations\nfor all layers, generally making learning on memory-constrained devices\ninfeasible. In this paper, we demonstrate that the backward propagation\noperations needed for binary neural network training are strongly robust to\nquantization, thereby making on-the-edge learning with modern models a\npractical proposition. We introduce a low-cost binary neural network training\nstrategy exhibiting sizable memory footprint and energy reductions while\ninducing little to no accuracy loss vs Courbariaux & Bengio's standard\napproach. These resource decreases are primarily enabled through the retention\nof activations exclusively in binary format. Against the latter algorithm, our\ndrop-in replacement sees coincident memory requirement and energy consumption\ndrops of 2--6$\\times$, while reaching similar test accuracy in comparable time,\nacross a range of small-scale models trained to classify popular datasets. We\nalso demonstrate from-scratch ImageNet training of binarized ResNet-18,\nachieving a 3.12$\\times$ memory reduction. Such savings will allow for\nunnecessary cloud offloading to be avoided, reducing latency, increasing energy\nefficiency and safeguarding privacy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 15:06:41 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 21:57:45 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 13:24:13 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 17:39:07 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Erwei", ""], ["Davis", "James J.", ""], ["Moro", "Daniele", ""], ["Zielinski", "Piotr", ""], ["Coelho", "Claudionor", ""], ["Chatterjee", "Satrajit", ""], ["Cheung", "Peter Y. K.", ""], ["Constantinides", "George A.", ""]]}, {"id": "2102.04503", "submitter": "Steve Dai", "authors": "Steve Dai, Rangharajan Venkatesan, Haoxing Ren, Brian Zimmer, William\n  J. Dally, Brucek Khailany", "title": "VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision\n  Neural Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization enables efficient acceleration of deep neural networks by\nreducing model memory footprint and exploiting low-cost integer math hardware\nunits. Quantization maps floating-point weights and activations in a trained\nmodel to low-bitwidth integer values using scale factors. Excessive\nquantization, reducing precision too aggressively, results in accuracy\ndegradation. When scale factors are shared at a coarse granularity across many\ndimensions of each tensor, effective precision of individual elements within\nthe tensor are limited. To reduce quantization-related accuracy loss, we\npropose using a separate scale factor for each small vector of ($\\approx$16-64)\nelements within a single dimension of a tensor. To achieve an efficient\nhardware implementation, the per-vector scale factors can be implemented with\nlow-bitwidth integers when calibrated using a two-level quantization scheme. We\nfind that per-vector scaling consistently achieves better inference accuracy at\nlow precision compared to conventional scaling techniques for popular neural\nnetworks without requiring retraining. We also modify a deep learning\naccelerator hardware design to study the area and energy overheads of\nper-vector scaling support. Our evaluation demonstrates that per-vector scaled\nquantization with 4-bit weights and activations achieves 37% area saving and\n24% energy saving while maintaining over 75% accuracy for ResNet50 on ImageNet.\n4-bit weights and 8-bit activations achieve near-full-precision accuracy for\nboth BERT-base and BERT-large on SQuAD while reducing area by 26% compared to\nan 8-bit baseline.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 19:56:04 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Dai", "Steve", ""], ["Venkatesan", "Rangharajan", ""], ["Ren", "Haoxing", ""], ["Zimmer", "Brian", ""], ["Dally", "William J.", ""], ["Khailany", "Brucek", ""]]}, {"id": "2102.04554", "submitter": "Debjit Pal", "authors": "Debjit Pal, Shobha Vasudevan", "title": "Feature Engineering for Scalable Application-Level Post-Silicon\n  Debugging", "comments": "15 pages, 13 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present systematic and efficient solutions for both observability\nenhancement and root-cause diagnosis of post-silicon System-on-Chips (SoCs)\nvalidation with diverse usage scenarios. We model specification of interacting\nflows in typical applications for message selection. Our method for message\nselection optimizes flow specification coverage and trace buffer utilization.\nWe define the diagnosis problem as identifying buggy traces as outliers and\nbug-free traces as inliers/normal behaviors, for which we use unsupervised\nlearning algorithms for outlier detection. Instead of direct application of\nmachine learning algorithms over trace data using the signals as raw features,\nwe use feature engineering to transform raw features into more sophisticated\nfeatures using domain specific operations. The engineered features are highly\nrelevant to the diagnosis task and are generic to be applied across any\nhardware designs. We present debugging and root cause analysis of subtle\npost-silicon bugs in industry-scale OpenSPARC T2 SoC. We achieve a trace buffer\nutilization of 98.96\\% with a flow specification coverage of 94.3\\% (average).\nOur diagnosis method was able to diagnose up to 66.7\\% more bugs and took up to\n847$\\times$ less diagnosis time as compared to the manual debugging with a\ndiagnosis precision of 0.769.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 22:11:59 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Pal", "Debjit", ""], ["Vasudevan", "Shobha", ""]]}, {"id": "2102.05271", "submitter": "Vinay Joshi", "authors": "Vinay Joshi, Wangxin He, Jae-sun Seo and Bipin Rajendran", "title": "Hybrid In-memory Computing Architecture for the Training of Deep Neural\n  Networks", "comments": "Accepted at ISCAS 2021 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost involved in training deep neural networks (DNNs) on von-Neumann\narchitectures has motivated the development of novel solutions for efficient\nDNN training accelerators. We propose a hybrid in-memory computing (HIC)\narchitecture for the training of DNNs on hardware accelerators that results in\nmemory-efficient inference and outperforms baseline software accuracy in\nbenchmark tasks. We introduce a weight representation technique that exploits\nboth binary and multi-level phase-change memory (PCM) devices, and this leads\nto a memory-efficient inference accelerator. Unlike previous in-memory\ncomputing-based implementations, we use a low precision weight update\naccumulator that results in more memory savings. We trained the ResNet-32\nnetwork to classify CIFAR-10 images using HIC. For a comparable model size,\nHIC-based training outperforms baseline network, trained in floating-point\n32-bit (FP32) precision, by leveraging appropriate network width multiplier.\nFurthermore, we observe that HIC-based training results in about 50% less\ninference model size to achieve baseline comparable accuracy. We also show that\nthe temporal drift in PCM devices has a negligible effect on post-training\ninference accuracy for extended periods (year). Finally, our simulations\nindicate HIC-based training naturally ensures that the number of write-erase\ncycles seen by the devices is a small fraction of the endurance limit of PCM,\ndemonstrating the feasibility of this architecture for achieving hardware\nplatforms that can learn in the field.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 05:26:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Joshi", "Vinay", ""], ["He", "Wangxin", ""], ["Seo", "Jae-sun", ""], ["Rajendran", "Bipin", ""]]}, {"id": "2102.05321", "submitter": "Siyuan Niu", "authors": "Siyuan Niu (LIRMM), Aida Todri-Sanial (LIRMM)", "title": "Enabling multi-programming mechanism for quantum computing in the NISQ\n  era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As NISQ devices have several physical limitations and unavoidable noisy\nquantum operations, only small circuits can be executed on a quantum machine to\nget reliable results. This leads to the quantum hardware under-utilization\nissue. Here, we address this problem and improve the quantum hardware\nthroughput by proposing a multiprogramming approach to execute multiple quantum\ncircuits on quantum hardware simultaneously. We first introduce a parallelism\nmanager to select an appropriate number of circuits to be executed at the same\ntime. Second, we present two different qubit partitioning algorithms to\nallocate reliable partitions to multiple circuits-a greedy and a heuristic.\nThird, we use the Simultaneous Randomized Benchmarking protocol to characterize\nthe crosstalk properties and consider them in the qubit partition process to\navoid crosstalk effect during simultaneous executions. Finally, we enhance the\nmapping transition algorithm to make circuits executable on hardware using\ndecreased number of inserted gates. We demonstrate the performance of our\nmulti-programming approach by executing circuits of different size on IBM\nquantum hardware simultaneously. We also investigate this method on VQE\nalgorithm to reduce its overhead.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 08:46:16 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Niu", "Siyuan", "", "LIRMM"], ["Todri-Sanial", "Aida", "", "LIRMM"]]}, {"id": "2102.05981", "submitter": "Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}", "authors": "Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}, Minesh Patel, Jeremie S. Kim,\n  Roknoddin Azizi, Ataberk Olgun, Lois Orosa, Hasan Hassan, Jisung Park,\n  Konstantinos Kanellopoulos, Taha Shahroodi, Saugata Ghose, Onur Mutlu", "title": "BlockHammer: Preventing RowHammer at Low Cost by Blacklisting\n  Rapidly-Accessed DRAM Rows", "comments": "A shorter version of this work is to appear at the 27th IEEE\n  International Symposium on High-Performance Computer Architecture (HPCA-27),\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aggressive memory density scaling causes modern DRAM devices to suffer from\nRowHammer, a phenomenon where rapidly activating a DRAM row can cause bit-flips\nin physically-nearby rows. Recent studies demonstrate that modern DRAM chips,\nincluding chips previously marketed as RowHammer-safe, are even more vulnerable\nto RowHammer than older chips. Many works show that attackers can exploit\nRowHammer bit-flips to reliably mount system-level attacks to escalate\nprivilege and leak private data. Therefore, it is critical to ensure\nRowHammer-safe operation on all DRAM-based systems. Unfortunately,\nstate-of-the-art RowHammer mitigation mechanisms face two major challenges.\nFirst, they incur increasingly higher performance and/or area overheads when\napplied to more vulnerable DRAM chips. Second, they require either proprietary\ninformation about or modifications to the DRAM chip design. In this paper, we\nshow that it is possible to efficiently and scalably prevent RowHammer\nbit-flips without knowledge of or modification to DRAM internals. We introduce\nBlockHammer, a low-cost, effective, and easy-to-adopt RowHammer mitigation\nmechanism that overcomes the two key challenges by selectively throttling\nmemory accesses that could otherwise cause RowHammer bit-flips. The key idea of\nBlockHammer is to (1) track row activation rates using area-efficient Bloom\nfilters and (2) use the tracking data to ensure that no row is ever activated\nrapidly enough to induce RowHammer bit-flips. By doing so, BlockHammer (1)\nmakes it impossible for a RowHammer bit-flip to occur and (2) greatly reduces a\nRowHammer attack's impact on the performance of co-running benign applications.\nCompared to state-of-the-art RowHammer mitigation mechanisms, BlockHammer\nprovides competitive performance and energy when the system is not under a\nRowHammer attack and significantly better performance and energy when the\nsystem is under attack.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:56:45 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ya\u011fl\u0131k\u00e7\u0131", "Abdullah Giray", ""], ["Patel", "Minesh", ""], ["Kim", "Jeremie S.", ""], ["Azizi", "Roknoddin", ""], ["Olgun", "Ataberk", ""], ["Orosa", "Lois", ""], ["Hassan", "Hasan", ""], ["Park", "Jisung", ""], ["Kanellopoulos", "Konstantinos", ""], ["Shahroodi", "Taha", ""], ["Ghose", "Saugata", ""], ["Mutlu", "Onur", ""]]}, {"id": "2102.06018", "submitter": "Simon Pfenning", "authors": "Simon Pfenning, Philipp Holzinger, Marc Reichenbach", "title": "Transparent FPGA Acceleration with TensorFlow", "comments": "Presented at DATE Friday Workshop on System-level Design Methods for\n  Deep Learning on Heterogeneous Architectures (SLOHA 2021) (arXiv:2102.00818)", "journal-ref": null, "doi": null, "report-no": "SLOHA/2021/09", "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, artificial neural networks are one of the major innovators pushing the\nprogress of machine learning. This has particularly affected the development of\nneural network accelerating hardware. However, since most of these\narchitectures require specialized toolchains, there is a certain amount of\nadditional effort for developers each time they want to make use of a new deep\nlearning accelerator. Furthermore the flexibility of the device is bound to the\narchitecture itself, as well as to the functionality of the runtime\nenvironment.\n  In this paper we propose a toolflow using TensorFlow as frontend, thus\noffering developers the opportunity of using a familiar environment. On the\nbackend we use an FPGA, which is addressable via an HSA runtime environment. In\nthis way we are able to hide the complexity of controlling new hardware from\nthe user, while at the same time maintaining a high amount of flexibility. This\ncan be achieved by our HSA toolflow, since the hardware is not statically\nconfigured with the structure of the network. Instead, it can be dynamically\nreconfigured during runtime with the respective kernels executed by the network\nand simultaneously from other sources e.g. OpenCL/OpenMP.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:49:33 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Pfenning", "Simon", ""], ["Holzinger", "Philipp", ""], ["Reichenbach", "Marc", ""]]}, {"id": "2102.06326", "submitter": "Steve Dai", "authors": "Steve Dai, Alicia Klinefelter, Haoxing Ren, Rangharajan Venkatesan,\n  Ben Keller, Nathaniel Pinckney, Brucek Khailany", "title": "Verifying High-Level Latency-Insensitive Designs with Formal Model\n  Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AR cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency-insensitive design mitigates increasing interconnect delay and\nenables productive component reuse in complex digital systems. This design\nstyle has been adopted in high-level design flows because untimed functional\nblocks connected through latency-insensitive interfaces provide a natural\ncommunication abstraction. However, latency-insensitive design with high-level\nlanguages also introduces a unique set of verification challenges that\njeopardize functional correctness. In particular, bugs due to invalid\nconsumption of inputs and deadlocks can be difficult to detect and debug with\ndynamic simulation methods. To tackle these two classes of bugs, we propose\nformal model checking methods to guarantee that a high-level\nlatency-insensitive design is unaffected by invalid input data and is free of\ndeadlock. We develop a well-structured verification wrapper for each property\nto automatically construct the corresponding formal model for checking. Our\nexperiments demonstrate that the formal checks are effective in realistic bug\nscenarios from high-level designs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 01:56:23 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Dai", "Steve", ""], ["Klinefelter", "Alicia", ""], ["Ren", "Haoxing", ""], ["Venkatesan", "Rangharajan", ""], ["Keller", "Ben", ""], ["Pinckney", "Nathaniel", ""], ["Khailany", "Brucek", ""]]}, {"id": "2102.06536", "submitter": "Jason Kamran Jr Eshraghian", "authors": "Jason K. Eshraghian, Kyoungrok Cho, Sung Mo Kang", "title": "CrossStack: A 3-D Reconfigurable RRAM Crossbar Inference Engine", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural network inference accelerators are rapidly growing in importance\nas we turn to massively parallelized processing beyond GPUs and ASICs. The\ndominant operation in feedforward inference is the multiply-and-accumlate\nprocess, where each column in a crossbar generates the current response of a\nsingle neuron. As a result, memristor crossbar arrays parallelize inference and\nimage processing tasks very efficiently. In this brief, we present a 3-D active\nmemristor crossbar array `CrossStack', which adopts stacked pairs of\nAl/TiO2/TiO2-x/Al devices with common middle electrodes. By designing\nCMOS-memristor hybrid cells used in the layout of the array, CrossStack can\noperate in one of two user-configurable modes as a reconfigurable inference\nengine: 1) expansion mode and 2) deep-net mode. In expansion mode, the\nresolution of the network is doubled by increasing the number of inputs for a\ngiven chip area, reducing IR drop by 22%. In deep-net mode, inference speed\nper-10-bit convolution is improved by 29\\% by simultaneously using one\nTiO2/TiO2-x layer for read processes, and the other for write processes. We\nexperimentally verify both modes on our $10\\times10\\times2$ array.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 22:59:01 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Eshraghian", "Jason K.", ""], ["Cho", "Kyoungrok", ""], ["Kang", "Sung Mo", ""]]}, {"id": "2102.06876", "submitter": "Yu Luo", "authors": "Yu Luo and Lina Pu", "title": "Hardware Architecture of Wireless Power Transfer, RFID, and WIPT Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we provide an overview of the hardware architecture of wireless\npower transfer (WPT), RFID, and wireless information and power transfer (WIPT)\nsystems. The historical milestones and structure differences among WPT, RFID,\nand WIPT are introduced.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 07:05:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Luo", "Yu", ""], ["Pu", "Lina", ""]]}, {"id": "2102.06888", "submitter": "Rourab Paul", "authors": "Rourab Paul, Sreetama Sarkar, Suman Sau, Koushik Chakraborty,\n  Sanghamitra Roy, Amlan Chakrabarti", "title": "Voltage Scaling for Partitioned Systolic Array in A Reconfigurable\n  Platform", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The exponential emergence of Field Programmable Gate Array (FPGA) has\naccelerated the research of hardware implementation of Deep Neural Network\n(DNN). Among all DNN processors, domain specific architectures, such as,\nGoogle's Tensor Processor Unit (TPU) have outperformed conventional GPUs.\nHowever, implementation of TPUs in reconfigurable hardware should emphasize\nenergy savings to serve the green computing requirement. Voltage scaling, a\npopular approach towards energy savings, can be a bit critical in FPGA as it\nmay cause timing failure if not done in an appropriate way. In this work, we\npresent an ultra low power FPGA implementation of a TPU for edge applications.\nWe divide the systolic-array of a TPU into different FPGA partitions, where\neach partition uses different near threshold (NTC) biasing voltages to run its\nFPGA cores. The biasing voltage for each partition is roughly calculated by the\nproposed offline schemes. However, further calibration of biasing voltage is\ndone by the proposed online scheme. Four clustering algorithms based on the\nslack value of different design paths study the partitioning of FPGA. To\novercome the timing failure caused by NTC, the higher slack paths are placed in\nlower voltage partitions and lower slack paths are placed in higher voltage\npartitions. The proposed architecture is simulated in Artix-7 FPGA using the\nVivado design suite and Python tool. The simulation results substantiate the\nimplementation of voltage scaled TPU in FPGAs and also justifies its power\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 09:51:40 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Rourab", ""], ["Sarkar", "Sreetama", ""], ["Sau", "Suman", ""], ["Chakraborty", "Koushik", ""], ["Roy", "Sanghamitra", ""], ["Chakrabarti", "Amlan", ""]]}, {"id": "2102.06892", "submitter": "Mirza Beg", "authors": "Asim Ikram, Muhammad Awais Ali, Mirza Omer Beg", "title": "Cache Bypassing for Machine Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphics Processing Units (GPUs) were once used solely for graphical\ncomputation tasks but with the increase in the use of machine learning\napplications, the use of GPUs to perform general-purpose computing has\nincreased in the last few years. GPUs employ a massive amount of threads, that\nin turn achieve a high amount of parallelism, to perform tasks. Though GPUs\nhave a high amount of computation power, they face the problem of cache\ncontention due to the SIMT model that they use. A solution to this problem is\ncalled \"cache bypassing\". This paper presents a predictive model that analyzes\nthe access patterns of various machine learning algorithms and determines\nwhether certain data should be stored in the cache or not. It presents insights\non how well each model performs on different datasets and also shows how\nminimizing the size of each model will affect its performance The performance\nof most of the models were found to be around 90% with KNN performing the best\nbut not with the smallest size. We further increase the features by splitting\nthe addresses into chunks of 4 bytes. We observe that this increased the\nperformance of the neural network substantially and increased the accuracy to\n99.9% with three neurons.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 10:21:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Ikram", "Asim", ""], ["Ali", "Muhammad Awais", ""], ["Beg", "Mirza Omer", ""]]}, {"id": "2102.06960", "submitter": "Sudeep Pasricha", "authors": "Febin Sunny, Asif Mirza, Mahdi Nikdast, and Sudeep Pasricha", "title": "CrossLight: A Cross-Layer Optimized Silicon Photonic Neural Network\n  Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain-specific neural network accelerators have seen growing interest in\nrecent years due to their improved energy efficiency and inference performance\ncompared to CPUs and GPUs. In this paper, we propose a novel cross-layer\noptimized neural network accelerator called CrossLight that leverages silicon\nphotonics. CrossLight includes device-level engineering for resilience to\nprocess variations and thermal crosstalk, circuit-level tuning enhancements for\ninference latency reduction, and architecture-level optimization to enable\nhigher resolution, better energy-efficiency, and improved throughput. On\naverage, CrossLight offers 9.5x lower energy-per-bit and 15.9x higher\nperformance-per-watt at 16-bit resolution than state-of-the-art photonic deep\nlearning accelerators.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 17:08:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sunny", "Febin", ""], ["Mirza", "Asif", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2102.07511", "submitter": "Jinho Lee", "authors": "Heesu Kim, Hanmin Park, Taehyun Kim, Kwanheum Cho, Eojin Lee, Soojung\n  Ryu, Hyuk-Jae Lee, Kiyoung Choi, Jinho Lee", "title": "GradPIM: A Practical Processing-in-DRAM Architecture for Gradient\n  Descent", "comments": "Accepted to HPCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GradPIM, a processing-in-memory architecture which\naccelerates parameter updates of deep neural networks training. As one of\nprocessing-in-memory techniques that could be realized in the near future, we\npropose an incremental, simple architectural design that does not invade the\nexisting memory protocol. Extending DDR4 SDRAM to utilize bank-group\nparallelism makes our operation designs in processing-in-memory (PIM) module\nefficient in terms of hardware cost and performance. Our experimental results\nshow that the proposed architecture can improve the performance of DNN training\nand greatly reduce memory bandwidth requirement while posing only a minimal\namount of overhead to the protocol and DRAM area.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:25:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kim", "Heesu", ""], ["Park", "Hanmin", ""], ["Kim", "Taehyun", ""], ["Cho", "Kwanheum", ""], ["Lee", "Eojin", ""], ["Ryu", "Soojung", ""], ["Lee", "Hyuk-Jae", ""], ["Choi", "Kiyoung", ""], ["Lee", "Jinho", ""]]}, {"id": "2102.07952", "submitter": "Nan Wu", "authors": "Nan Wu, Yuan Xie", "title": "A Survey of Machine Learning for Computer Architecture and Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has been a long time that computer architecture and systems are optimized\nto enable efficient execution of machine learning (ML) algorithms or models.\nNow, it is time to reconsider the relationship between ML and systems, and let\nML transform the way that computer architecture and systems are designed. This\nembraces a twofold meaning: the improvement of designers' productivity, and the\ncompletion of the virtuous cycle. In this paper, we present a comprehensive\nreview of work that applies ML for system design, which can be grouped into two\nmajor categories, ML-based modelling that involves predictions of performance\nmetrics or some other criteria of interest, and ML-based design methodology\nthat directly leverages ML as the design tool. For ML-based modelling, we\ndiscuss existing studies based on their target level of system, ranging from\nthe circuit level to the architecture/system level. For ML-based design\nmethodology, we follow a bottom-up path to review current work, with a scope of\n(micro-)architecture design (memory, branch prediction, NoC), coordination\nbetween architecture/system and workload (resource allocation and management,\ndata center management, and security), compiler, and design automation. We\nfurther provide a future vision of opportunities and potential directions, and\nenvision that applying ML for computer architecture and systems would thrive in\nthe community.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:09:57 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wu", "Nan", ""], ["Xie", "Yuan", ""]]}, {"id": "2102.07959", "submitter": "Aqeeb Iqbal Arka", "authors": "Aqeeb Iqbal Arka, Biresh Kumar Joardar, Janardhan Rao Doppa, Partha\n  Pratim Pande, Krishnendu Chakrabarty", "title": "ReGraphX: NoC-enabled 3D Heterogeneous ReRAM Architecture for Training\n  Graph Neural Networks", "comments": "This paper has been accepted and presented at Design Automation and\n  Test in Europe (DATE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Network (GNN) is a variant of Deep Neural Networks (DNNs)\noperating on graphs. However, GNNs are more complex compared to traditional\nDNNs as they simultaneously exhibit features of both DNN and graph\napplications. As a result, architectures specifically optimized for either DNNs\nor graph applications are not suited for GNN training. In this work, we propose\na 3D heterogeneous manycore architecture for on-chip GNN training to address\nthis problem. The proposed architecture, ReGraphX, involves heterogeneous ReRAM\ncrossbars to fulfill the disparate requirements of both DNN and graph\ncomputations simultaneously. The ReRAM-based architecture is complemented with\na multicast-enabled 3D NoC to improve the overall achievable performance. We\ndemonstrate that ReGraphX outperforms conventional GPUs by up to 3.5X (on an\naverage 3X) in terms of execution time, while reducing energy consumption by as\nmuch as 11X.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:59:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Arka", "Aqeeb Iqbal", ""], ["Joardar", "Biresh Kumar", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Chakrabarty", "Krishnendu", ""]]}, {"id": "2102.08138", "submitter": "Callie Hao", "authors": "Nan Wu, Yuan Xie, Cong Hao", "title": "IronMan: GNN-assisted Design Space Exploration in High-Level Synthesis\n  via Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of High-Level Synthesis (HLS) tools, we observe\nseveral unresolved challenges: 1) the high-level abstraction of programming\nstyles in HLS sometimes conceals optimization opportunities; 2) existing HLS\ntools do not provide flexible trade-off (Pareto) solutions among different\nobjectives and constraints; 3) the actual quality of the resulting RTL designs\nis hard to predict. To address these challenges, we propose an end-to-end\nframework, namelyIronMan. The primary goal is to enable a flexible and\nautomated design space exploration (DSE), to provide either optimal solutions\nunder user-specified constraints, or various trade-offs among different\nobjectives (such as different types of resources, area, and latency). Such DSE\neither requires tedious manual efforts or is not achievable to attain these\ngoals through existing HLS tools. There are three components in IronMan: 1)\nGPP, a highly accurate graph-neural-network-based performance and resource\npredictor; 2) RLMD, a reinforcement-learning-based multi-objective DSE engine\nthat explores the optimal resource allocation strategy, to provide Pareto\nsolutions between different objectives; 3) CT, a code transformer to assist\nRLMD and GPP, which extracts the data flow graph from original HLS C/C++ and\nautomatically generates synthesizable code with HLS directives. The\nexperimental results show that: 1) GPP achieves high prediction accuracy,\nreducing prediction errors of HLS tools by 10.9x in resource utilization and\n5.7x in timing; 2) RLMD obtains optimal or Pareto solutions that outperform the\ngenetic algorithm and simulated annealing by 12.7% and 12.9%, respectively; 3)\nIronMan is able to find optimized solutions perfectly matching various DSP\nconstraints, with 2.54x fewer DSPs and up to 6x shorter latency than those of\nHLS tools while being up to 400x faster than the heuristic algorithms and HLS\ntools.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 13:22:00 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Wu", "Nan", ""], ["Xie", "Yuan", ""], ["Hao", "Cong", ""]]}, {"id": "2102.08247", "submitter": "Priyesh Shukla", "authors": "Priyesh Shukla, Ankith Muralidhar, Nick Iliev, Theja Tulabandhula,\n  Sawyer B. Fuller and Amit Ranjan Trivedi", "title": "Probabilistic Localization of Insect-Scale Drones on Floating-Gate\n  Inverter Arrays", "comments": "Will submit the revised article.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel compute-in-memory (CIM)-based ultra-low-power framework\nfor probabilistic localization of insect-scale drones. The conventional\nprobabilistic localization approaches rely on the three-dimensional (3D)\nGaussian Mixture Model (GMM)-based representation of a 3D map. A GMM model with\nhundreds of mixture functions is typically needed to adequately learn and\nrepresent the intricacies of the map. Meanwhile, localization using complex GMM\nmap models is computationally intensive. Since insect-scale drones operate\nunder extremely limited area/power budget, continuous localization using GMM\nmodels entails much higher operating energy -- thereby, limiting flying\nduration and/or size of the drone due to a larger battery. Addressing the\ncomputational challenges of localization in an insect-scale drone using a CIM\napproach, we propose a novel framework of 3D map representation using a\nharmonic mean of \"Gaussian-like\" mixture (HMGM) model. The likelihood function\nuseful for drone localization can be efficiently implemented by connecting many\nmulti-input inverters in parallel, each programmed with the parameters of the\n3D map model represented as HMGM. When the depth measurements are projected to\nthe input of the implementation, the summed current of the inverters emulates\nthe likelihood of the measurement. We have characterized our approach on an\nRGB-D indoor localization dataset. The average localization error in our\napproach is $\\sim$0.1125 m which is only slightly degraded than software-based\nevaluation ($\\sim$0.08 m). Meanwhile, our localization framework is\nultra-low-power, consuming as little as $\\sim$17 $\\mu$W power while processing\na depth frame in 1.33 ms over hundred pose hypotheses in the particle-filtering\n(PF) algorithm used to localize the drone.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 16:06:28 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 13:22:38 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shukla", "Priyesh", ""], ["Muralidhar", "Ankith", ""], ["Iliev", "Nick", ""], ["Tulabandhula", "Theja", ""], ["Fuller", "Sawyer B.", ""], ["Trivedi", "Amit Ranjan", ""]]}, {"id": "2102.08323", "submitter": "Mahdi Nikdast", "authors": "Ebadollah Taheri and Ryan G. Kim and Mahdi Nikdast", "title": "AdEle: An Adaptive Congestion-and-Energy-Aware Elevator Selection for\n  Partially Connected 3D NoCs", "comments": "This paper will be published in Proc. IEEE/ACM Design Automation\n  Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By lowering the number of vertical connections in fully connected 3D\nnetworks-on-chip (NoCs), partially connected 3D NoCs (PC-3DNoCs) help alleviate\nreliability and fabrication issues. This paper proposes a novel, adaptive\ncongestion- and energy-aware elevator-selection scheme called AdEle to improve\nthe traffic distribution in PC-3DNoCs. AdEle employs an offline multi-objective\nsimulated-annealing-based algorithm to find good elevator subsets and an online\nelevator selection policy to enhance elevator selection during routing.\nCompared to the state-of-the-art techniques under different real-application\ntraffics and configuration scenarios, AdEle improves the network latency by\n14.9% on average (up to 21.4%) with less than 10.5% energy consumption\noverhead.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:10:26 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Taheri", "Ebadollah", ""], ["Kim", "Ryan G.", ""], ["Nikdast", "Mahdi", ""]]}, {"id": "2102.08619", "submitter": "Yanqi Zhou", "authors": "Yanqi Zhou, Xuanyi Dong, Berkin Akin, Mingxing Tan, Daiyi Peng,\n  Tianjian Meng, Amir Yazdanbakhsh, Da Huang, Ravi Narayanaswami, James Laudon", "title": "Rethinking Co-design of Neural Architectures and Hardware Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural architectures and hardware accelerators have been two driving forces\nfor the progress in deep learning. Previous works typically attempt to optimize\nhardware given a fixed model architecture or model architecture given fixed\nhardware. And the dominant hardware architecture explored in this prior work is\nFPGAs. In our work, we target the optimization of hardware and software\nconfigurations on an industry-standard edge accelerator. We systematically\nstudy the importance and strategies of co-designing neural architectures and\nhardware accelerators. We make three observations: 1) the software search space\nhas to be customized to fully leverage the targeted hardware architecture, 2)\nthe search for the model architecture and hardware architecture should be done\njointly to achieve the best of both worlds, and 3) different use cases lead to\nvery different search outcomes. Our experiments show that the joint search\nmethod consistently outperforms previous platform-aware neural architecture\nsearch, manually crafted models, and the state-of-the-art EfficientNet on all\nlatency targets by around 1% on ImageNet top-1 accuracy. Our method can reduce\nenergy consumption of an edge accelerator by up to 2x under the same accuracy\nconstraint, when co-adapting the model architecture and hardware accelerator\nconfigurations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 07:55:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhou", "Yanqi", ""], ["Dong", "Xuanyi", ""], ["Akin", "Berkin", ""], ["Tan", "Mingxing", ""], ["Peng", "Daiyi", ""], ["Meng", "Tianjian", ""], ["Yazdanbakhsh", "Amir", ""], ["Huang", "Da", ""], ["Narayanaswami", "Ravi", ""], ["Laudon", "James", ""]]}, {"id": "2102.08739", "submitter": "Qingqing Wu", "authors": "Qingqing Wu, Xiaobo Zhou, Robert Schober", "title": "IRS-Assisted Wireless Powered NOMA: Do We Really Need Different Phase\n  Shifts in DL and UL?", "comments": "We unveiled in the paper that dynamic IRS beamforming is not needed\n  for wireless powered communication networks (WPCNs) employing NOMA. The TDMA\n  based IRS-aided WPCN is ready soon as a sister work. My online tutorials for\n  more information on IRS. 1)\n  https://www.bilibili.com/video/BV1wZ4y1N7J4?from=search&seid=17353402214239402263\n  2) https://www.youtube.com/watch?v=J9CpUIdpfeo&t=306s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR cs.ET cs.NI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent reflecting surface (IRS) is a promising technology to improve the\nperformance of wireless powered communication networks (WPCNs) due to its\ncapability to reconfigure signal propagation environments via smart reflection.\nIn particular, the high passive beamforming gain promised by IRS can\nsignificantly enhance the efficiency of both downlink wireless power transfer\n(DL WPT) and uplink wireless information transmission (UL WIT) in WPCNs.\nAlthough adopting different IRS phase shifts for DL WPT and UL WIT, i.e.,\ndynamic IRS beamforming, is in principle possible but incurs additional\nsignaling overhead and computational complexity, it is an open problem whether\nit is actually beneficial. To answer this question, we consider an IRS-assisted\nWPCN where multiple devices employ a hybrid access point (HAP) to first harvest\nenergy and then transmit information using non-orthogonal multiple access\n(NOMA). Specifically, we aim to maximize the sum throughput of all devices by\njointly optimizing the IRS phase shifts and the resource allocation. To this\nend, we first prove that dynamic IRS beamforming is not needed for the\nconsidered system, which helps reduce the number of IRS phase shifts to be\noptimized. Then, we propose both joint and alternating optimization based\nalgorithms to solve the resulting problem. Simulation results demonstrate the\neffectiveness of our proposed designs over benchmark schemes and also provide\nuseful insights into the importance of IRS for realizing spectrally and energy\nefficient WPCNs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:06:08 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 02:28:47 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 04:09:37 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 11:21:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wu", "Qingqing", ""], ["Zhou", "Xiaobo", ""], ["Schober", "Robert", ""]]}, {"id": "2102.09642", "submitter": "Georgios Zervakis", "authors": "Georgios Zervakis, Ourania Spantidi, Iraklis Anagnostopoulos, Hussam\n  Amrouch, J\\\"org Henkel", "title": "Control Variate Approximation for DNN Accelerators", "comments": "Accepted for publication at the 58th Design Automation Conference\n  (DAC'21), December 5-9, 2021, San Francisco, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a control variate approximation technique for low\nerror approximate Deep Neural Network (DNN) accelerators. The control variate\ntechnique is used in Monte Carlo methods to achieve variance reduction. Our\napproach significantly decreases the induced error due to approximate\nmultiplications in DNN inference, without requiring time-exhaustive retraining\ncompared to state-of-the-art. Leveraging our control variate method, we use\nhighly approximated multipliers to generate power-optimized DNN accelerators.\nOur experimental evaluation on six DNNs, for Cifar-10 and Cifar-100 datasets,\ndemonstrates that, compared to the accurate design, our control variate\napproximation achieves same performance and 24% power reduction for a merely\n0.16% accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 22:11:58 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zervakis", "Georgios", ""], ["Spantidi", "Ourania", ""], ["Anagnostopoulos", "Iraklis", ""], ["Amrouch", "Hussam", ""], ["Henkel", "J\u00f6rg", ""]]}, {"id": "2102.09673", "submitter": "Sharjeel Khan", "authors": "Bodhisatwa Chatterjee, Sharjeel Khan, Santosh Pande", "title": "Effective Cache Apportioning for Performance Isolation Under Compiler\n  Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With a growing number of cores per socket in modern data-centers where\nmulti-tenancy of a diverse set of applications must be efficiently supported,\neffective sharing of the last level cache is a very important problem. This is\nchallenging because modern workloads exhibit dynamic phase behavior - their\ncache requirements & sensitivity vary across different execution points. To\ntackle this problem, we propose Com-CAS, a compiler-guided cache apportioning\nsystem that provides smart cache allocation to co-executing applications in a\nsystem. The front-end of Com-CAS is primarily a compiler-framework equipped\nwith learning mechanisms to predict cache requirements, while the backend\nconsists of an allocation framework with a pro-active scheduler that apportions\ncache dynamically to co-executing applications. Our system improved average\nthroughput by 21%, with a maximum of 54% while maintaining the worst individual\napplication execution time degradation within 15% to meet SLA requirements.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:44:42 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:21:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chatterjee", "Bodhisatwa", ""], ["Khan", "Sharjeel", ""], ["Pande", "Santosh", ""]]}, {"id": "2102.09713", "submitter": "Rachit Nigam", "authors": "Rachit Nigam, Samuel Thomas, Zhijing Li, Adrian Sampson", "title": "A Compiler Infrastructure for Accelerator Generators", "comments": "To appear at ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Calyx, a new intermediate language (IL) for compiling high-level\nprograms into hardware designs. Calyx combines a hardware-like structural\nlanguage with a software-like control flow representation with loops and\nconditionals. This split representation enables a new class of hardware-focused\noptimizations that require both structural and control flow information which\nare crucial for high-level programming models for hardware design. The Calyx\ncompiler lowers control flow constructs using finite-state machines and\ngenerates synthesizable hardware descriptions.\n  We have implemented Calyx in an optimizing compiler that translates\nhigh-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL\ncompilers, a systolic array generator and one for a recent imperative\naccelerator language, and compare them to equivalent designs generated using\nhigh-level synthesis (HLS). The systolic arrays are $4.6\\times$ faster and\n$1.1\\times$ larger on average than HLS implementations, and the HLS-like\nimperative language compiler is within a few factors of a highly optimized\ncommercial HLS toolchain. We also describe three optimizations implemented in\nthe Calyx compiler.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 02:29:04 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nigam", "Rachit", ""], ["Thomas", "Samuel", ""], ["Li", "Zhijing", ""], ["Sampson", "Adrian", ""]]}, {"id": "2102.10031", "submitter": "Lang Feng", "authors": "Lang Feng, Jiayi Huang, Jeff Huang, Jiang Hu", "title": "Toward Taming the Overhead Monster for Data-Flow Integrity", "comments": "16 pages, 15 figures, submitting to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-Flow Integrity (DFI) is a well-known approach to effectively detecting a\nwide range of software attacks. However, its real-world application has been\nquite limited so far because of the prohibitive performance overhead it incurs.\nMoreover, the overhead is enormously difficult to overcome without\nsubstantially lowering the DFI criterion. In this work, an analysis is\nperformed to understand the main factors contributing to the overhead.\nAccordingly, a hardware-assisted parallel approach is proposed to tackle the\noverhead challenge. Simulations on SPEC CPU 2006 benchmark show that the\nproposed approach can completely verify the DFI defined in the original seminal\nwork while reducing performance overhead by 4x on average.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:03:16 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Feng", "Lang", ""], ["Huang", "Jiayi", ""], ["Huang", "Jeff", ""], ["Hu", "Jiang", ""]]}, {"id": "2102.10140", "submitter": "Sudeep Pasricha", "authors": "D. Dang, S. V. R. Chittamuru, S. Pasricha, R. Mahapatra, D. Sahoo", "title": "BPLight-CNN: A Photonics-based Backpropagation Accelerator for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "epic-21-01", "categories": "cs.LG cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Training deep learning networks involves continuous weight updates across the\nvarious layers of the deep network while using a backpropagation algorithm\n(BP). This results in expensive computation overheads during training.\nConsequently, most deep learning accelerators today employ pre-trained weights\nand focus only on improving the design of the inference phase. The recent trend\nis to build a complete deep learning accelerator by incorporating the training\nmodule. Such efforts require an ultra-fast chip architecture for executing the\nBP algorithm. In this article, we propose a novel photonics-based\nbackpropagation accelerator for high performance deep learning training. We\npresent the design for a convolutional neural network, BPLight-CNN, which\nincorporates the silicon photonics-based backpropagation accelerator.\nBPLight-CNN is a first-of-its-kind photonic and memristor-based CNN\narchitecture for end-to-end training and prediction. We evaluate BPLight-CNN\nusing a photonic CAD framework (IPKISS) on deep learning benchmark models\nincluding LeNet and VGG-Net. The proposed design achieves (i) at least 34x\nspeedup, 34x improvement in computational efficiency, and 38.5x energy savings,\nduring training; and (ii) 29x speedup, 31x improvement in computational\nefficiency, and 38.7x improvement in energy savings, during inference compared\nto the state-of-the-art designs. All these comparisons are done at a 16-bit\nresolution; and BPLight-CNN achieves these improvements at a cost of\napproximately 6% lower accuracy compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 20:00:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dang", "D.", ""], ["Chittamuru", "S. V. R.", ""], ["Pasricha", "S.", ""], ["Mahapatra", "R.", ""], ["Sahoo", "D.", ""]]}, {"id": "2102.10195", "submitter": "Satyabrata Sarangi", "authors": "Satyabrata Sarangi and Bevan Baas", "title": "DeepScaleTool : A Tool for the Accurate Estimation of Technology Scaling\n  in the Deep-Submicron Era", "comments": "This paper has been accepted for the 2021 IEEE International\n  Symposium on Circuits and Systems. \\copyright 2021 IEEE. Copyright statements\n  are posted on the first page. 5 Pages, 5 Figures", "journal-ref": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": "10.1109/ISCAS51556.2021.9401196", "report-no": null, "categories": "eess.SY cs.AR cs.PF cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The estimation of classical CMOS \"constant-field\" or \"Dennard\" scaling\nmethods that define scaling factors for various dimensional and electrical\nparameters have become less accurate in the deep-submicron regime, which drives\nthe need for better estimation approaches especially in the educational and\nresearch domains. We present DeepScaleTool, a tool for the accurate estimation\nof deep-submicron technology scaling by modeling and curve fitting published\ndata by a leading commercial fabrication company for silicon fabrication\ntechnology generations from 130~nm to 7~nm for the key parameters of area,\ndelay, and energy. Compared to 10~nm--7~nm scaling data published by a leading\nfoundry, the DeepScaleTool achieves an error of 1.7% in area, 2.5% in delay,\nand 5% in power. This compares favorably with another leading academic\nestimation method that achieves an error of 24% in area, 9.1% in delay, and\n24.9% in power.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 22:53:44 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sarangi", "Satyabrata", ""], ["Baas", "Bevan", ""]]}, {"id": "2102.10423", "submitter": "Amir Yazdanbakhsh", "authors": "Amir Yazdanbakhsh, Kiran Seshadri, Berkin Akin, James Laudon, Ravi\n  Narayanaswami", "title": "An Evaluation of Edge TPU Accelerators for Convolutional Neural Networks", "comments": "11 pages, 15 figures, submitted to ISCA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge TPUs are a domain of accelerators for low-power, edge devices and are\nwidely used in various Google products such as Coral and Pixel devices. In this\npaper, we first discuss the major microarchitectural details of Edge TPUs.\nThen, we extensively evaluate three classes of Edge TPUs, covering different\ncomputing ecosystems, that are either currently deployed in Google products or\nare the product pipeline, across 423K unique convolutional neural networks.\nBuilding upon this extensive study, we discuss critical and interpretable\nmicroarchitectural insights about the studied classes of Edge TPUs. Mainly, we\ndiscuss how Edge TPU accelerators perform across convolutional neural networks\nwith different structures. Finally, we present our ongoing efforts in\ndeveloping high-accuracy learned machine learning models to estimate the major\nperformance metrics of accelerators such as latency and energy consumption.\nThese learned models enable significantly faster (in the order of milliseconds)\nevaluations of accelerators as an alternative to time-consuming cycle-accurate\nsimulators and establish an exciting opportunity for rapid hard-ware/software\nco-design.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 19:25:09 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yazdanbakhsh", "Amir", ""], ["Seshadri", "Kiran", ""], ["Akin", "Berkin", ""], ["Laudon", "James", ""], ["Narayanaswami", "Ravi", ""]]}, {"id": "2102.10732", "submitter": "Chai Wah Wu", "authors": "Chai Wah Wu", "title": "Dither computing: a hybrid deterministic-stochastic computing framework", "comments": "8 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic computing has a long history as an alternative method of\nperforming arithmetic on a computer. While it can be considered an unbiased\nestimator of real numbers, it has a variance and MSE on the order of\n$\\Omega(\\frac{1}{N})$. On the other hand, deterministic variants of stochastic\ncomputing remove the stochastic aspect, but cannot approximate arbitrary real\nnumbers with arbitrary precision and are biased estimators. However, they have\nan asymptotically superior MSE on the order of $O(\\frac{1}{N^2})$. Recent\nresults in deep learning with stochastic rounding suggest that the bias in the\nrounding can degrade performance. We proposed an alternative framework, called\ndither computing, that combines aspects of stochastic computing and its\ndeterministic variants and that can perform computing with similar efficiency,\nis unbiased, and with a variance and MSE also on the optimal order of\n$\\Theta(\\frac{1}{N^2})$. We also show that it can be beneficial in stochastic\nrounding applications as well. We provide implementation details and give\nexperimental results to comparatively show the benefits of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 01:51:09 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 20:50:07 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wu", "Chai Wah", ""]]}, {"id": "2102.10837", "submitter": "Subho Sankar Banerjee", "authors": "Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk, Ravishankar K.\n  Iyer", "title": "BayesPerf: Minimizing Performance Monitoring Errors Using Bayesian\n  Statistics", "comments": null, "journal-ref": "Proceedings of the Twenty-Sixth International Conference on\n  Architectural Support for Programming Languages and Operating Systems (ASPLOS\n  21), 2021", "doi": "10.1145/3445814.3446739", "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware performance counters (HPCs) that measure low-level architectural and\nmicroarchitectural events provide dynamic contextual information about the\nstate of the system. However, HPC measurements are error-prone due to non\ndeterminism (e.g., undercounting due to event multiplexing, or OS\ninterrupt-handling behaviors). In this paper, we present BayesPerf, a system\nfor quantifying uncertainty in HPC measurements by using a domain-driven\nBayesian model that captures microarchitectural relationships between HPCs to\njointly infer their values as probability distributions. We provide the design\nand implementation of an accelerator that allows for low-latency and low-power\ninference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the\naverage error in HPC measurements from 40.1% to 7.6% when events are being\nmultiplexed. The value of BayesPerf in real-time decision-making is illustrated\nwith a simple example of scheduling of PCIe transfers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 09:00:14 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Banerjee", "Subho S.", ""], ["Jha", "Saurabh", ""], ["Kalbarczyk", "Zbigniew T.", ""], ["Iyer", "Ravishankar K.", ""]]}, {"id": "2102.10932", "submitter": "Ismail Akturk", "authors": "Christos Sakalis, Zamshed I. Chowdhury, Shayne Wadle, Ismail Akturk,\n  Alberto Ros, Magnus Sj\\\"alander, Stefanos Kaxiras, Ulya R. Karpuzcu", "title": "On Value Recomputation to Accelerate Invisible Speculation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent architectural approaches that address speculative side-channel attacks\naim to prevent software from exposing the microarchitectural state changes of\ntransient execution. The Delay-on-Miss technique is one such approach, which\nsimply delays loads that miss in the L1 cache until they become\nnon-speculative, resulting in no transient changes in the memory hierarchy.\nHowever, this costs performance, prompting the use of value prediction (VP) to\nregain some of the delay. However, the problem cannot be solved by simply\nintroducing a new kind of speculation (value prediction). Value-predicted loads\nhave to be validated, which cannot be commenced until the load becomes\nnon-speculative. Thus, value-predicted loads occupy the same amount of precious\ncore resources (e.g., reorder buffer entries) as Delay-on-Miss. The end result\nis that VP only yields marginal benefits over Delay-on-Miss. In this paper, our\ninsight is that we can achieve the same goal as VP (increasing performance by\nproviding the value of loads that miss) without incurring its negative\nside-effect (delaying the release of precious resources), if we can safely,\nnon-speculatively, recompute a value in isolation (without being seen from the\noutside), so that we do not expose any information by transferring such a value\nvia the memory hierarchy. Value Recomputation, which trades computation for\ndata transfer was previously proposed in an entirely different context: to\nreduce energy-expensive data transfers in the memory hierarchy. In this paper,\nwe demonstrate the potential of value recomputation in relation to the\nDelay-on-Miss approach of hiding speculation, discuss the trade-offs, and show\nthat we can achieve the same level of security, reaching 93% of the unsecured\nbaseline performance (5% higher than Delay-on-miss), and exceeding (by 3%) what\neven an oracular (100% accuracy and coverage) value predictor could do.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:04:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sakalis", "Christos", ""], ["Chowdhury", "Zamshed I.", ""], ["Wadle", "Shayne", ""], ["Akturk", "Ismail", ""], ["Ros", "Alberto", ""], ["Sj\u00e4lander", "Magnus", ""], ["Kaxiras", "Stefanos", ""], ["Karpuzcu", "Ulya R.", ""]]}, {"id": "2102.11245", "submitter": "Harish Dattatraya Dixit", "authors": "Harish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason,\n  Tejasvi Chakravarthy, Bharath Muthiah, Sriram Sankar", "title": "Silent Data Corruptions at Scale", "comments": "8 pages, 3 figures, 33 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Silent Data Corruption (SDC) can have negative impact on large-scale\ninfrastructure services. SDCs are not captured by error reporting mechanisms\nwithin a Central Processing Unit (CPU) and hence are not traceable at the\nhardware level. However, the data corruptions propagate across the stack and\nmanifest as application-level problems. These types of errors can result in\ndata loss and can require months of debug engineering time. In this paper, we\ndescribe common defect types observed in silicon manufacturing that leads to\nSDCs. We discuss a real-world example of silent data corruption within a\ndatacenter application. We provide the debug flow followed to root-cause and\ntriage faulty instructions within a CPU using a case study, as an illustration\non how to debug this class of errors. We provide a high-level overview of the\nmitigations to reduce the risk of silent data corruptions within a large\nproduction fleet. In our large-scale infrastructure, we have run a vast library\nof silent error test scenarios across hundreds of thousands of machines in our\nfleet. This has resulted in hundreds of CPUs detected for these errors, showing\nthat SDCs are a systemic issue across generations. We have monitored SDCs for a\nperiod longer than 18 months. Based on this experience, we determine that\nreducing silent data corruptions requires not only hardware resiliency and\nproduction detection mechanisms, but also robust fault-tolerant software\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:30:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dixit", "Harish Dattatraya", ""], ["Pendharkar", "Sneha", ""], ["Beadon", "Matt", ""], ["Mason", "Chris", ""], ["Chakravarthy", "Tejasvi", ""], ["Muthiah", "Bharath", ""], ["Sankar", "Sriram", ""]]}, {"id": "2102.11528", "submitter": "Nadja Ramh\\\"oj Holtryd", "authors": "Nadja Ramh\\\"oj Holtryd, Madhavan Manivannan, Per Stenstr\\\"om, Miquel\n  Peric\\`as", "title": "CBP: Coordinated management of cache partitioning, bandwidth\n  partitioning and prefetch throttling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reducing the average memory access time is crucial for improving the\nperformance of applications running on multi-core architectures. With workload\nconsolidation this becomes increasingly challenging due to shared resource\ncontention. Techniques for partitioning of shared resources - cache and\nbandwidth - and prefetching throttling have been proposed to mitigate\ncontention and reduce the average memory access time. However, existing\nproposals only employ a single or a subset of these techniques and are\ntherefore not able to exploit the full potential of coordinated management of\ncache, bandwidth and prefetching. Our characterization results show that\napplication performance, in several cases, is sensitive to prefetching, cache\nand bandwidth allocation. Furthermore, the results show that managing these\ntogether provides higher performance potential during workload consolidation as\nit enables more resource trade-offs. In this paper, we propose CBP a\ncoordination mechanism for dynamically managing prefetching throttling, cache\nand bandwidth partitioning, in order to reduce average memory access time and\nimprove performance. CBP works by employing individual resource managers to\ndetermine the appropriate setting for each resource and a coordinating\nmechanism in order to enable inter-resource trade-offs. Our evaluation on a\n16-core CMP shows that CBP, on average, improves performance by 11% compared to\nthe state-of-the-art technique that manages cache partitioning and prefetching\nand by 50% compared to the baseline without cache partitioning, bandwidth\npartitioning and prefetch throttling.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 07:29:34 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Holtryd", "Nadja Ramh\u00f6j", ""], ["Manivannan", "Madhavan", ""], ["Stenstr\u00f6m", "Per", ""], ["Peric\u00e0s", "Miquel", ""]]}, {"id": "2102.12103", "submitter": "Je Yang", "authors": "Je Yang, Seongmin Hong, Joo-Young Kim", "title": "FIXAR: A Fixed-Point Deep Reinforcement Learning Platform with\n  Quantization-Aware Training and Adaptive Parallelism", "comments": "This paper will be published in Proc. IEEE/ACM Design Automation\n  Conference (DAC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present a deep reinforcement learning platform named FIXAR\nwhich employs fixed-point data types and arithmetic units for the first time\nusing a SW/HW co-design approach. Starting from 32-bit fixed-point data,\nQuantization-Aware Training (QAT) reduces its data precision based on the range\nof activations and performs retraining to minimize the reward degradation.\nFIXAR proposes the adaptive array processing core composed of configurable\nprocessing elements to support both intra-layer parallelism and intra-batch\nparallelism for high-throughput inference and training. Finally, FIXAR was\nimplemented on Xilinx U50 and achieves 25293.3 inferences per second (IPS)\ntraining throughput and 2638.0 IPS/W accelerator efficiency, which is 2.7 times\nfaster and 15.4 times more energy efficient than those of the CPU-GPU platform\nwithout any accuracy degradation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:22:38 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Yang", "Je", ""], ["Hong", "Seongmin", ""], ["Kim", "Joo-Young", ""]]}, {"id": "2102.13191", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Xie Li, Yang Hu, Huwan Peng, Michael Taylor, Shuaiwen\n  Leon Song", "title": "Q-VR: System-Level Design for Future Mobile Collaborative Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": "10.1145/3445814.3446715", "report-no": null, "categories": "cs.AR cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High Quality Mobile Virtual Reality (VR) is what the incoming graphics\ntechnology era demands: users around the world, regardless of their hardware\nand network conditions, can all enjoy the immersive virtual experience.\nHowever, the state-of-the-art software-based mobile VR designs cannot fully\nsatisfy the realtime performance requirements due to the highly interactive\nnature of user's actions and complex environmental constraints during VR\nexecution. Inspired by the unique human visual system effects and the strong\ncorrelation between VR motion features and realtime hardware-level information,\nwe propose Q-VR, a novel dynamic collaborative rendering solution via\nsoftware-hardware co-design for enabling future low-latency high-quality mobile\nVR. At software-level, Q-VR provides flexible high-level tuning interface to\nreduce network latency while maintaining user perception. At hardware-level,\nQ-VR accommodates a wide spectrum of hardware and network conditions across\nusers by effectively leveraging the computing capability of the increasingly\npowerful VR hardware. Extensive evaluation on real-world games demonstrates\nthat Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to\n6.7x) over the traditional local rendering design in commercial VR devices, and\na 4.1x frame rate improvement over the state-of-the-art static collaborative\nrendering.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:56:05 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xie", "Chenhao", ""], ["Li", "Xie", ""], ["Hu", "Yang", ""], ["Peng", "Huwan", ""], ["Taylor", "Michael", ""], ["Song", "Shuaiwen Leon", ""]]}, {"id": "2102.13301", "submitter": "Ashish Shrivastava", "authors": "Ashish Shrivastava and Alan Gatherer and Tong Sun and Sushma Wokhlu\n  and Alex Chandra", "title": "SLAP: A Split Latency Adaptive VLIW pipeline architecture which enables\n  on-the-fly variable SIMD vector-length", "comments": "Selected in ICASSP 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Over the last decade the relative latency of access to shared memory by\nmulticore increased as wire resistance dominated latency and low wire density\nlayout pushed multiport memories farther away from their ports. Various\ntechniques were deployed to improve average memory access latencies, such as\nspeculative pre-fetching and branch-prediction, often leading to high variance\nin execution time which is unacceptable in real time systems. Smart DMAs can be\nused to directly copy data into a layer1 SRAM, but with overhead. The VLIW\narchitecture, the de facto signal processing engine, suffers badly from a\nbreakdown in lockstep execution of scalar and vector instructions. We describe\nthe Split Latency Adaptive Pipeline (SLAP) VLIW architecture, a cache\nperformance improvement technology that requires zero change to object code,\nwhile removing smart DMAs and their overhead. SLAP builds on the Decoupled\nAccess and Execute concept by 1) breaking lockstep execution of functional\nunits, 2) enabling variable vector length for variable data level parallelism,\nand 3) adding a novel triangular load mechanism. We discuss the SLAP\narchitecture and demonstrate the performance benefits on real traces from a\nwireless baseband system (where even the most compute intensive functions\nsuffer from an Amdahls law limitation due to a mixture of scalar and vector\nprocessing).\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 04:42:40 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shrivastava", "Ashish", ""], ["Gatherer", "Alan", ""], ["Sun", "Tong", ""], ["Wokhlu", "Sushma", ""], ["Chandra", "Alex", ""]]}, {"id": "2102.13410", "submitter": "Rakesh Kumar", "authors": "Rakesh Kumar, Alejandro Martinez and Antonio Gonzalez", "title": "A Variable Vector Length SIMD Architecture for HW/SW Co-designed\n  Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hardware/Software (HW/SW) co-designed processors provide a promising solution\nto the power and complexity problems of the modern microprocessors by keeping\ntheir hardware simple. Moreover, they employ several runtime optimizations to\nimprove the performance. One of the most potent optimizations, vectorization,\nhas been utilized by modern microprocessors, to exploit the data level\nparallelism through SIMD accelerators. Due to their hardware simplicity, these\naccelerators have evolved in terms of width from 64-bit vectors in Intel MMX to\n512-bit wide vector units in Intel Xeon Phi and AVX-512. Although SIMD\naccelerators are simple in terms of hardware design, code generation for them\nhas always been a challenge. Moreover, increasing vector lengths with each new\ngeneration add to this complexity.\n  This paper explores the scalability of SIMD accelerators from the code\ngeneration point of view. We discover that the SIMD accelerators remain\nunderutilized at higher vector lengths mainly due to: a) reduced dynamic\ninstruction stream coverage for vectorization and b) increase in permutations.\nBoth of these factors can be attributed to the rigidness of the SIMD\narchitecture. We propose a novel SIMD architecture that possesses the\nflexibility needed to support higher vector lengths. Furthermore, we propose\nVariable Length Vectorization and Selective Writing in a HW/SW co-designed\nenvironment to transparently target the flexibility of the proposed\narchitecture. We evaluate our proposals using a set of SPECFP2006 and\nPhysicsbench applications. Our experimental results show an average dynamic\ninstruction reduction of 31% and 40% and an average speed up of 13% and 10% for\nSPECFP2006 and Physicsbench respectively, for 512-bit vector length, over the\nscalar baseline code.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:38:53 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kumar", "Rakesh", ""], ["Martinez", "Alejandro", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "2102.13460", "submitter": "Martin Schoeberl", "authors": "Andrew Dobis and Tjark Petersen and Kasper Juul Hesse Rasmussen and\n  Enrico Tolotto and Hans Jakob Damsgaard and Simon Thye Andersen and Richard\n  Lin and Martin Schoeberl", "title": "Open-Source Verification with Chisel and Scala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Performance increase with general-purpose processors has come to a halt. We\ncan no longer depend on Moore's Law to increase computing performance. The only\nway to achieve higher performance or lower energy consumption is by building\ndomain-specific hardware accelerators. To efficiently design and verify those\ndomain-specific accelerators, we need agile hardware development. One of the\nmain obstacles when proposing such a modern method is the lack of modern tools\nto attack it. To be able to verify a design in such a time-constrained\ndevelopment method, one needs to have efficient tools both for design and\nverification.\n  This paper thus proposes ChiselVerify, an open-source tool for verifying\ncircuits described in any Hardware Description Language. It builds on top of\nthe Chisel hardware construction language and uses Scala to drive the\nverification using a testing strategy inspired by the Universal Verification\nMethodology (UVM) and adapted for designs described in Chisel. ChiselVerify is\ncreated based on three key ideas. First, our solution highly increases the\nproductivity of the verification engineer, by allowing hardware testing to be\ndone in a modern high-level programming environment. Second, the framework\nfunctions with any hardware description language thanks to the flexibility of\nChisel blackboxes. Finally, the solution is well integrated into the existing\nChisel universe, making it an extension of currently existing testing\nlibraries.\n  We implement ChiselVerify in a way inspired by the functionalities found in\nSystemVerilog. This allows one to use functional coverage, constrained-random\nverification, bus functional models, transaction-level modeling and much more\nduring the verification process of a design in a contemporary high-level\nprogramming ecosystem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 13:35:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Dobis", "Andrew", ""], ["Petersen", "Tjark", ""], ["Rasmussen", "Kasper Juul Hesse", ""], ["Tolotto", "Enrico", ""], ["Damsgaard", "Hans Jakob", ""], ["Andersen", "Simon Thye", ""], ["Lin", "Richard", ""], ["Schoeberl", "Martin", ""]]}]