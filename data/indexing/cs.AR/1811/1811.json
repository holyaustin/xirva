[{"id": "1811.01544", "submitter": "Myoungsoo Jung", "authors": "Donghyun Gouk, Miryeong Kwon, Jie Zhang, Sungjoon Koh, Wonil Choi, Nam\n  Sung Kim, Mahmut Kandemir and Myoungsoo Jung", "title": "Amber: Enabling Precise Full-System Simulation with Detailed Modeling of\n  All SSD Resources", "comments": "This paper has been accepted at the 51st Annual IEEE/ACM\n  International Symposium on Microarchitecture (MICRO '51), 2018. This material\n  is presented to ensure timely dissemination of scholarly and technical work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SSDs become a major storage component in modern memory hierarchies, and SSD\nresearch demands exploring future simulation-based studies by integrating SSD\nsubsystems into a full-system environment. However, several challenges exist to\nmodel SSDs under a full-system simulations; SSDs are composed upon their own\ncomplete system and architecture, which employ all necessary hardware, such as\nCPUs, DRAM and interconnect network. Employing the hardware components, SSDs\nalso require to have multiple device controllers, internal caches and software\nmodules that respect a wide spectrum of storage interfaces and protocols. These\nSSD hardware and software are all necessary to incarnate storage subsystems\nunder full-system environment, which can operate in parallel with the host\nsystem. In this work, we introduce a new SSD simulation framework, SimpleSSD\n2.0, namely Amber, that models embedded CPU cores, DRAMs, and various flash\ntechnologies (within an SSD), and operate under the full system simulation\nenvironment by enabling a data transfer emulation. Amber also includes full\nfirmware stack, including DRAM cache logic, flash firmware, such as FTL and\nHIL, and obey diverse standard protocols by revising the host DMA engines and\nsystem buses of a popular full system simulator's all functional and timing CPU\nmodels (gem5). The proposed simulator can capture the details of dynamic\nperformance and power of embedded cores, DRAMs, firmware and flash under the\nexecutions of various OS systems and hardware platforms. Using Amber, we\ncharacterize several system-level challenges by simulating different types of\nfullsystems, such as mobile devices and general-purpose computers, and offer\ncomprehensive analyses by comparing passive storage and active storage\narchitectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 07:59:34 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Gouk", "Donghyun", ""], ["Kwon", "Miryeong", ""], ["Zhang", "Jie", ""], ["Koh", "Sungjoon", ""], ["Choi", "Wonil", ""], ["Kim", "Nam Sung", ""], ["Kandemir", "Mahmut", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1811.01740", "submitter": "David Monniaux", "authors": "David Monniaux (VERIMAG - IMAG), Valentin Touzeau (VERIMAG - IMAG)", "title": "On the complexity of cache analysis for different replacement policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors use cache memory: a memory access that \"hits\" the cache\nreturns early, while a \"miss\" takes more time. Given a memory access in a\nprogram, cache analysis consists in deciding whether this access is always a\nhit, always a miss, or is a hit or a miss depending on execution. Such an\nanalysis is of high importance for bounding the worst-case execution time of\nsafety-critical real-time programs.There exist multiple possible policies for\nevicting old data from the cache when new data are brought in, and different\npolicies, though apparently similar in goals and performance, may be very\ndifferent from the analysis point of view. In this paper, we explore these\ndifferences from a complexity-theoretical point of view. Specifically, we show\nthat, among the common replacement policies, LRU (Least Recently Used) is the\nonly one whose analysis is NP-complete, whereas the analysis problems for the\nother policies are PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:38:14 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 09:45:45 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Monniaux", "David", "", "VERIMAG - IMAG"], ["Touzeau", "Valentin", "", "VERIMAG - IMAG"]]}, {"id": "1811.01780", "submitter": "Steve Hoover", "authors": "Steven Hoover, Ahmed Salman", "title": "Top-Down Transaction-Level Design with TL-Verilog", "comments": "5 pages. 9 figures. Presented by Ahmed Salman at VSDOpen 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction-Level Verilog (TL-Verilog) is an emerging extension to\nSystemVerilog that supports a new design methodology, called transaction-level\ndesign. A transaction, in this methodology, is an entity that moves through\nstructures like pipelines, arbiters, and queues, A transaction might be a\nmachine instruction, a flit of a packet, or a memory read/write. Transaction\nlogic, like packet header decode or instruction execution, that operates on the\ntransaction can be placed anywhere along the transaction's flow. Tools produce\nthe logic to carry signals through their flows to stitch the transaction logic.\n  We implemented a small library of TL-Verilog flow components, and we\nillustrate the use of these components in a top-down design methodology. We\nconstruct a hypothetical microarchitecture simply by instantiating components.\nWithin the flows created by these components, we add combinational transaction\nlogic, enabling verification activities and performance evaluation to begin. We\nthen refine the model by positioning the transaction logic within its flow to\nproduce a high-quality register-transfer-level (RTL) implementation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:14:35 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hoover", "Steven", ""], ["Salman", "Ahmed", ""]]}, {"id": "1811.02883", "submitter": "Ananda Samajdar", "authors": "Ananda Samajdar, Yuhao Zhu, Paul Whatmough, Matthew Mattina and Tushar\n  Krishna", "title": "SCALE-Sim: Systolic CNN Accelerator Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systolic Arrays are one of the most popular compute substrates within Deep\nLearning accelerators today, as they provide extremely high efficiency for\nrunning dense matrix multiplications. However, the research community lacks\ntools to insights on both the design trade-offs and efficient mapping\nstrategies for systolic-array based accelerators. We introduce Systolic CNN\nAccelerator Simulator (SCALE-Sim), which is a configurable systolic array based\ncycle accurate DNN accelerator simulator. SCALE-Sim exposes various\nmicro-architectural features as well as system integration parameters to the\ndesigner to enable comprehensive design space exploration. This is the first\nsystolic-array simulator tuned for running DNNs to the best of our knowledge.\nUsing SCALE-Sim, we conduct a suite of case studies and demonstrate the effect\nof bandwidth, data flow and aspect ratio on the overall runtime and energy of\nDeep Learning kernels across vision, speech, text, and games. We believe that\nthese insights will be highly beneficial to architects and ML practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 22:33:46 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 04:01:45 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Samajdar", "Ananda", ""], ["Zhu", "Yuhao", ""], ["Whatmough", "Paul", ""], ["Mattina", "Matthew", ""], ["Krishna", "Tushar", ""]]}, {"id": "1811.02884", "submitter": "Yifan Sun", "authors": "Yifan Sun, Trinayan Baruah, Saiful A. Mojumder, Shi Dong, Rafael Ubal,\n  Xiang Gong, Shane Treadway, Yuhui Bao, Vincent Zhao, Jos\\'e L. Abell\\'an,\n  John Kim, Ajay Joshi, David Kaeli", "title": "MGSim + MGMark: A Framework for Multi-GPU System Research", "comments": "Updated typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing popularity and scale of data-parallel workloads demand a\ncorresponding increase in raw computational power of GPUs (Graphics Processing\nUnits). As single-GPU systems struggle to satisfy the performance demands,\nmulti-GPU systems have begun to dominate the high-performance computing world.\nThe advent of such systems raises a number of design challenges, including the\nGPU microarchitecture, multi-GPU interconnect fabrics, runtime libraries and\nassociated programming models. The research community currently lacks a\npublically available and comprehensive multi-GPU simulation framework and\nbenchmark suite to evaluate multi-GPU system design solutions.\n  In this work, we present MGSim, a cycle-accurate, extensively validated,\nmulti-GPU simulator, based on AMD's Graphics Core Next 3 (GCN3) instruction set\narchitecture. We complement MGSim with MGMark, a suite of multi-GPU workloads\nthat explores multi-GPU collaborative execution patterns. Our simulator is\nscalable and comes with in-built support for multi-threaded execution to enable\nfast and efficient simulations. In terms of performance accuracy, MGSim differs\n$5.5\\%$ on average when compared against actual GPU hardware. We also achieve a\n$3.5\\times$ and a $2.5\\times$ average speedup in function emulation and\narchitectural simulation with 4 CPU cores, while delivering the same accuracy\nas the serial simulation.\n  We illustrate the novel simulation capabilities provided by our simulator\nthrough a case study exploring programming models based on a unified multi-GPU\nsystem (U-MGPU) and a discrete multi-GPU system (D-MGPU) that both utilize\nunified memory space and cross-GPU memory access. We evaluate the design\nimplications from our case study, suggesting that D-MGPU is an attractive\nprogramming model for future multi-GPU systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:38:57 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 02:17:30 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 20:20:00 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Sun", "Yifan", ""], ["Baruah", "Trinayan", ""], ["Mojumder", "Saiful A.", ""], ["Dong", "Shi", ""], ["Ubal", "Rafael", ""], ["Gong", "Xiang", ""], ["Treadway", "Shane", ""], ["Bao", "Yuhui", ""], ["Zhao", "Vincent", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Kim", "John", ""], ["Joshi", "Ajay", ""], ["Kaeli", "David", ""]]}, {"id": "1811.03458", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow, Galina Cariowa", "title": "Hardware-Efficient Structure of the Accelerating Module for\n  Implementation of Convolutional Neural Network Basic Operation", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a structural design of the hardware-efficient module for\nimplementation of convolution neural network (CNN) basic operation with reduced\nimplementation complexity. For this purpose we utilize some modification of the\nWinograd minimal filtering method as well as computation vectorization\nprinciples. This module calculate inner products of two consecutive segments of\nthe original data sequence, formed by a sliding window of length 3, with the\nelements of a filter impulse response. The fully parallel structure of the\nmodule for calculating these two inner products, based on the implementation of\na naive method of calculation, requires 6 binary multipliers and 4 binary\nadders. The use of the Winograd minimal filtering method allows to construct a\nmodule structure that requires only 4 binary multipliers and 8 binary adders.\nSince a high-performance convolutional neural network can contain tens or even\nhundreds of such modules, such a reduction can have a significant effect.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:19:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""]]}, {"id": "1811.04047", "submitter": "Hongyang Jia", "authors": "Hongyang Jia, Yinqi Tang, Hossein Valavi, Jintao Zhang and Naveen\n  Verma", "title": "A Microprocessor implemented in 65nm CMOS with Configurable and\n  Bit-scalable Accelerator for Programmable In-memory Computing", "comments": null, "journal-ref": "IEEE Journal of Solid-State Circuits, vol. 55, no. 9, pp.\n  2609-2621, Sept. 2020", "doi": "10.1109/JSSC.2020.2987714", "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a programmable in-memory-computing processor,\ndemonstrated in a 65nm CMOS technology. For data-centric workloads, such as\ndeep neural networks, data movement often dominates when implemented with\ntoday's computing architectures. This has motivated spatial architectures,\nwhere the arrangement of data-storage and compute hardware is distributed and\nexplicitly aligned to the computation dataflow, most notably for matrix-vector\nmultiplication. In-memory computing is a spatial architecture where processing\nelements correspond to dense bit cells, providing local storage and compute,\ntypically employing analog operation. Though this raises the potential for high\nenergy efficiency and throughput, analog operation has significantly limited\nrobustness, scale, and programmability. This paper describes a 590kb\nin-memory-computing accelerator integrated in a programmable processor\narchitecture, by exploiting recent approaches to charge-domain in-memory\ncomputing. The architecture takes the approach of tight coupling with an\nembedded CPU, through accelerator interfaces enabling integration in the\nstandard processor memory space. Additionally, a near-memory-computing datapath\nboth enables diverse computations locally, to address operations required\nacross applications, and enables bit-precision scalability for\nmatrix/input-vector elements, through a bit-parallel/bit-serial (BP/BS) scheme.\nChip measurements show an energy efficiency of 152/297 1b-TOPS/W and throughput\nof 4.7/1.9 1b-TOPS (scaling linearly with the matrix/input-vector element\nprecisions) at VDD of 1.2/0.85V. Neural network demonstrations with 1-b/4-b\nweights and activations for CIFAR-10 classification consume 5.3/105.2\n$\\mu$J/image at 176/23 fps, with accuracy at the level of digital/software\nimplementation (89.3/92.4 $\\%$ accuracy).\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:03:14 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Jia", "Hongyang", ""], ["Tang", "Yinqi", ""], ["Valavi", "Hossein", ""], ["Zhang", "Jintao", ""], ["Verma", "Naveen", ""]]}, {"id": "1811.04770", "submitter": "Bradley McDanel", "authors": "H. T. Kung and Bradley McDanel and Sai Qian Zhang", "title": "Packing Sparse Convolutional Neural Networks for Efficient Systolic\n  Array Implementations: Column Combining Under Joint Optimization", "comments": "To appear in ASPLOS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach of packing sparse convolutional neural\nnetworks for their efficient systolic array implementations. By combining\nsubsets of columns in the original filter matrix associated with a\nconvolutional layer, we increase the utilization efficiency of the systolic\narray substantially (e.g., ~4x) due to the increased density of nonzeros in the\nresulting packed filter matrix. In combining columns, for each row, all filter\nweights but one with the largest magnitude are pruned. We retrain the remaining\nweights to preserve high accuracy. We demonstrate that in mitigating data\nprivacy concerns the retraining can be accomplished with only fractions of the\noriginal dataset (e.g., 10\\% for CIFAR-10). We study the effectiveness of this\njoint optimization for both high utilization and classification accuracy with\nASIC and FPGA designs based on efficient bit-serial implementations of\nmultiplier-accumulators. We present analysis and empirical evidence on the\nsuperior performance of our column combining approach against prior arts under\nmetrics such as energy efficiency (3x) and inference latency (12x).\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:09:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kung", "H. T.", ""], ["McDanel", "Bradley", ""], ["Zhang", "Sai Qian", ""]]}, {"id": "1811.06841", "submitter": "Lu Hang", "authors": "Hang Lu and Xin Wei and Ning Lin and Guihai Yan and and Xiaowei Li", "title": "Tetris: Re-architecting Convolutional Neural Network Computation for\n  Machine Learning Accelerators", "comments": null, "journal-ref": "ICCAD 2018 paper", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference efficiency is the predominant consideration in designing deep\nlearning accelerators. Previous work mainly focuses on skipping zero values to\ndeal with remarkable ineffectual computation, while zero bits in non-zero\nvalues, as another major source of ineffectual computation, is often ignored.\nThe reason lies on the difficulty of extracting essential bits during operating\nmultiply-and-accumulate (MAC) in the processing element. Based on the fact that\nzero bits occupy as high as 68.9% fraction in the overall weights of modern\ndeep convolutional neural network models, this paper firstly proposes a weight\nkneading technique that could eliminate ineffectual computation caused by\neither zero value weights or zero bits in non-zero weights, simultaneously.\nBesides, a split-and-accumulate (SAC) computing pattern in replacement of\nconventional MAC, as well as the corresponding hardware accelerator design\ncalled Tetris are proposed to support weight kneading at the hardware level.\nExperimental results prove that Tetris could speed up inference up to 1.50x,\nand improve power efficiency up to 5.33x compared with the state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 07:46:58 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Lu", "Hang", ""], ["Wei", "Xin", ""], ["Lin", "Ning", ""], ["Yan", "Guihai", ""], ["Li", "and Xiaowei", ""]]}, {"id": "1811.07612", "submitter": "Bharath Srinivas Prabakaran", "authors": "Bharath Srinivas Prabakaran, Mihika Dave, Florian Kriebel, Semeen\n  Rehman, Muhammad Shafique", "title": "Architectural-Space Exploration of Heterogeneous Reliability and\n  Checkpointing Modes for Out-of-Order Superscalar Processors", "comments": "16 Pages, 19 Figures, 6 Tables", "journal-ref": "IEEE Access, vol. 7, pp. 145324-145339, 2019", "doi": "10.1109/ACCESS.2019.2945622", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability has emerged as a key topic of interest for researchers around the\nworld to detect and/or mitigate the side effects of decreasing transistor\nsizes, such as soft errors. Traditional solutions, like DMR and TMR, incur\nsignificant area and power overheads, which might not always be applicable due\nto power restrictions. Therefore, we investigate alternative heterogeneous\nreliability modes that can be activated at run-time based on the system\nrequirements, while reducing the power and area overheads of the processor. Our\nheterogeneous reliability modes are successful in reducing the processor\nvulnerability by 87% on average, with area and power overheads of 10% and 43%,\nrespectively. To further enhance the design space of heterogeneous reliability,\nwe investigate combinations of efficient compression techniques like\nDistributed Multi-threaded Checkpointing, Hash-based Incremental Checkpointing,\nand GNU zip, to reduce the storage requirements of data that are backed-up at\nan application checkpoint. We have successfully reduced checkpoint sizes by a\nfactor ~6x by combining various state compression techniques. We use gem5 to\nimplement and simulate the state compression techniques and the heterogeneous\nreliability modes discussed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 11:08:30 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 10:48:18 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 22:42:01 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Prabakaran", "Bharath Srinivas", ""], ["Dave", "Mihika", ""], ["Kriebel", "Florian", ""], ["Rehman", "Semeen", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.08091", "submitter": "Katherine Lim", "authors": "Katie Lim, Jonathan Balkind, David Wentzlaff", "title": "JuxtaPiton: Enabling Heterogeneous-ISA Research with RISC-V and SPARC\n  FPGA Soft-cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency has become an increasingly important concern in computer\narchitecture due to the end of Dennard scaling. Heterogeneity has been explored\nas a way to achieve better energy efficiency and heterogeneous\nmicroarchitecture chips have become common in the mobile setting.\n  Recent research has explored using heterogeneous-ISA, heterogeneous\nmicroarchitecture, general-purpose cores to achieve further energy efficiency\ngains. However, there is no open-source hardware implementation of a\nheterogeneous-ISA processor available for research, and effective research on\nheterogeneous-ISA processors necessitates the emulation speed provided by FPGA\nprototyping. This work describes our experiences creating JuxtaPiton by\nintegrating a small RISC-V core into the OpenPiton framework, which uses a\nmodified OpenSPARC T1 core. This is the first time a new core has been\nintegrated with the OpenPiton framework, and JuxtaPiton is the first\nopen-source, general-purpose, heterogeneous-ISA processor. JuxtaPiton inherits\nall the capabilities of OpenPiton, including vital FPGA emulation\ninfrastructure which can boot full-stack Debian Linux. Using this\ninfrastructure, we investigate area and timing effects of using the new RISC-V\ncore on FPGA and the performance of the new core running microbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 06:51:42 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Lim", "Katie", ""], ["Balkind", "Jonathan", ""], ["Wentzlaff", "David", ""]]}, {"id": "1811.08309", "submitter": "Md Aamir Raihan", "authors": "Md Aamir Raihan, Negar Goli, Tor Aamodt", "title": "Modeling Deep Learning Accelerator Enabled GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of deep learning has resulted in its use in a growing number of\napplications. The Volta graphics processor unit (GPU) architecture from NVIDIA\nintroduced a specialized functional unit, the \"tensor core\", that helps meet\nthe growing demand for higher performance for deep learning. In this paper we\nstudy the design of the tensor cores in NVIDIA's Volta and Turing\narchitectures. We further propose an architectural model for the tensor cores\nin Volta. When implemented a GPU simulator, GPGPU-Sim, our tensor core model\nachieves 99.6\\% correlation versus an NVIDIA Titan~V GPU in terms of average\ninstructions per cycle when running tensor core enabled GEMM workloads. We also\ndescribe support added to enable GPGPU-Sim to run CUTLASS, an open-source CUDA\nC++ template library providing customizable GEMM templates that utilize tensor\ncores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:07:34 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 02:11:33 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Raihan", "Md Aamir", ""], ["Goli", "Negar", ""], ["Aamodt", "Tor", ""]]}, {"id": "1811.08426", "submitter": "Kyriakos Deliparaschos M", "authors": "K. M. Deliparaschos and S. G. Tzafestas", "title": "Design paradigms of intelligent control systems on a chip", "comments": null, "journal-ref": "Panhellenic Conf. on Electronics and Telecommunications (PACET\n  2009), Patras, Greece, 20-22 Mar. 2009", "doi": null, "report-no": null, "categories": "cs.OH cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the Field Programmable Gate Array (FPGA) design and\nimplementation of intelligent control system applications on a chip,\nspecifically fuzzy logic and genetic algorithm processing units. Initially, an\noverview of the FPGA technology is presented, followed by design methodologies,\ndevelopment tools and the use of hardware description languages (HDL). Two FPGA\ndesign examples with the use of Hardware Description Languages (HDLs) of\nparameterized fuzzy logic controller cores are discussed. Thereinafter, a\nSystem-on-a-Chip (SoC) designed by the authors in previous work and realized on\nFPGA featuring a Digital Fuzzy Logic Controller (DFLC) and a soft processor\ncore for the path tracking problem of mobile robots is discussed. Finally a\nGenetic Algorithm implementation (previously published by the authors) in FPGA\nchip for the Traveling Salesman Problem (TSP) is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 12:16:26 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Deliparaschos", "K. M.", ""], ["Tzafestas", "S. G.", ""]]}, {"id": "1811.08507", "submitter": "Sachin Taneja", "authors": "Sachin Taneja, Massimo Alioto", "title": "Ultra-Low Power Crypto-Engine Based on Simon 32/64 for Energy- and\n  Area-Constrained Integrated Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an ultra-low power crypto-engine achieving sub-pJ/bit\nenergy and sub-1K$\\mu$$m^2$ in 40nm CMOS, based on the Simon cryptographic\nalgorithm. Energy and area efficiency are pursued via microarchitectural\nexploration, ultra-low voltage operation with high resiliency via latch-based\npipelines, and power reduction techniques via multi-bit sequential elements.\nOverall, the comparison with the state of the art shows best-in-class energy\nefficiency and area. This makes it well suited for ubiquitous security in\ntightly-constrained platforms, e.g. RFIDs, low-end sensor nodes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 17:00:06 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Taneja", "Sachin", ""], ["Alioto", "Massimo", ""]]}, {"id": "1811.08634", "submitter": "Yifan Yang", "authors": "Yifan Yang, Qijing Huang, Bichen Wu, Tianjun Zhang, Liang Ma, Giulio\n  Gambardella, Michaela Blott, Luciano Lavagno, Kees Vissers, John Wawrzynek,\n  Kurt Keutzer", "title": "Synetgy: Algorithm-hardware Co-design for ConvNet Accelerators on\n  Embedded FPGAs", "comments": "Update to the latest results", "journal-ref": null, "doi": "10.1145/3289602.3293902", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using FPGAs to accelerate ConvNets has attracted significant attention in\nrecent years. However, FPGA accelerator design has not leveraged the latest\nprogress of ConvNets. As a result, the key application characteristics such as\nframes-per-second (FPS) are ignored in favor of simply counting GOPs, and\nresults on accuracy, which is critical to application success, are often not\neven reported. In this work, we adopt an algorithm-hardware co-design approach\nto develop a ConvNet accelerator called Synetgy and a novel ConvNet model\ncalled DiracDeltaNet$^{\\dagger}$. Both the accelerator and ConvNet are tailored\nto FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with\nonly $1\\times 1$ convolutions while spatial convolutions are replaced by more\nefficient shift operations. DiracDeltaNet achieves competitive accuracy on\nImageNet (88.7\\% top-5), but with 42$\\times$ fewer parameters and 48$\\times$\nfewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 4-bit and\nactivations to 4-bits, with less than 1\\% accuracy loss. These quantizations\nexploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model\nsize, low computational OP count, low precision and simplified operators allow\nus to co-design a highly customized computing unit for an FPGA. We implement\nthe computing units for DiracDeltaNet on an Ultra96 SoC system through\nhigh-level synthesis. Our accelerator's final top-5 accuracy of 88.1\\% on\nImageNet, is higher than all the previously reported embedded FPGA\naccelerators. In addition, the accelerator reaches an inference speed of 66.3\nFPS on the ImageNet classification task, surpassing prior works with similar\naccuracy by at least 11.6$\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 08:42:30 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 13:00:11 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 03:06:40 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 01:45:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yang", "Yifan", ""], ["Huang", "Qijing", ""], ["Wu", "Bichen", ""], ["Zhang", "Tianjun", ""], ["Ma", "Liang", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Lavagno", "Luciano", ""], ["Vissers", "Kees", ""], ["Wawrzynek", "John", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1811.08932", "submitter": "Alberto Marchisio", "authors": "Alberto Marchisio, Muhammad Abdullah Hanif, and Muhammad Shafique", "title": "CapsAcc: An Efficient Hardware Accelerator for CapsuleNets with Data\n  Reuse", "comments": "Accepted for publication at Design, Automation and Test in Europe\n  (DATE 2019). Florence, Italy", "journal-ref": null, "doi": "10.23919/DATE.2019.8714922", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have been widely deployed for many Machine\nLearning applications. Recently, CapsuleNets have overtaken traditional DNNs,\nbecause of their improved generalization ability due to the multi-dimensional\ncapsules, in contrast to the single-dimensional neurons. Consequently,\nCapsuleNets also require extremely intense matrix computations, making it a\ngigantic challenge to achieve high performance. In this paper, we propose\nCapsAcc, the first specialized CMOS-based hardware architecture to perform\nCapsuleNets inference with high performance and energy efficiency.\nState-of-the-art convolutional DNN accelerators would not work efficiently for\nCapsuleNets, as their designs do not account for key operations involved in\nCapsuleNets, like squashing and dynamic routing, as well as multi-dimensional\nmatrix processing. Our CapsAcc architecture targets this problem and achieves\nsignificant improvements, when compared to an optimized GPU implementation. Our\narchitecture exploits the massive parallelism by flexibly feeding the data to a\nspecialized systolic array according to the operations required in different\nlayers. It also avoids extensive load and store operations on the on-chip\nmemory, by reusing the data when possible. We further optimize the routing\nalgorithm to reduce the computations needed at this stage. We synthesized the\ncomplete CapsAcc architecture in a 32nm CMOS technology using Synopsys design\ntools, and evaluated it for the MNIST benchmark (as also done by the original\nCapsuleNet paper) to ensure consistent and fair comparisons. This work enables\nhighly-efficient CapsuleNets inference on embedded platforms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:56:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Marchisio", "Alberto", ""], ["Hanif", "Muhammad Abdullah", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1811.09285", "submitter": "Gerard Memmi P", "authors": "Kameswar Vaddina, Florian Brandner, Gerard Memmi, Pierre Jouvelot", "title": "Building the Case for Temperature Awareness in Energy Consumption\n  Models: an Application of the Energy-Frequency Convexity Rule", "comments": "A new campaign of mesearements showed that we have wrong data in a\n  couple of figures of this paper. This will be fixed with an updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing computing and communication systems that host energy-critical\napplications is becoming a key issue for software developers. In previous work,\nwe introduced and validated the Energy/Frequency Convexity Rule for CPU-bound\nbenchmarks on recent ARM platforms. This rule states that there exists an\noptimal clock frequency that minimizes the CPU's energy consumption for\nnon-performance-critical programs. We showed that the Energy/Frequency\nConvexity Rule is related to the non-linearity of power with respect to\nfrequency and is not dependent on the supply voltage.\n  Here, we discuss the application of an analytical energy consumption model\nproposed previously to our target board, a TI AM572x EVM. We show that this\nnon-linear analytical model can, for our experimental settings, be approximated\nby a frequency-linear variant, as our voltage is maintained constant. This,\nhowever, does not fit the measurements on the board, suggesting that a\nparameter is currently missing in the analytical model. We conjecture that\naccounting for temperature in the model would yield more accurate results that\nare in-line with our measurements. This builds the case for the inclusion of\nthis important parameter in our energy models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 19:05:20 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 09:35:38 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Vaddina", "Kameswar", ""], ["Brandner", "Florian", ""], ["Memmi", "Gerard", ""], ["Jouvelot", "Pierre", ""]]}, {"id": "1811.09557", "submitter": "Abbas Rahimi", "authors": "Abbas Rahimi, Tony F. Wu, Haitong Li, Jan M. Rabaey, H.-S. Philip\n  Wong, Max M. Shulaker, Subhasish Mitra", "title": "Hyperdimensional Computing Nanosystem", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One viable solution for continuous reduction in energy-per-operation is to\nrethink functionality to cope with uncertainty by adopting computational\napproaches that are inherently robust to uncertainty. It requires a novel look\nat data representations, associated operations, and circuits, and at materials\nand substrates that enable them. 3D integrated nanotechnologies combined with\nnovel brain-inspired computational paradigms that support fast learning and\nfault tolerance could lead the way. Recognizing the very size of the brain's\ncircuits, hyperdimensional (HD) computing can model neural activity patterns\nwith points in a HD space, that is, with hypervectors as large randomly\ngenerated patterns. At its very core, HD computing is about manipulating and\ncomparing these patterns inside memory. Emerging nanotechnologies such as\ncarbon nanotube field effect transistors (CNFETs) and resistive RAM (RRAM), and\ntheir monolithic 3D integration offer opportunities for hardware\nimplementations of HD computing through tight integration of logic and memory,\nenergy-efficient computation, and unique device characteristics. We\nexperimentally demonstrate and characterize an end-to-end HD computing\nnanosystem built using monolithic 3D integration of CNFETs and RRAM. With our\nnanosystem, we experimentally demonstrate classification of 21 languages with\nmeasured accuracy of up to 98% on >20,000 sentences (6.4 million characters),\ntraining using one text sample (~100,000 characters) per language, and\nresilient operation (98% accuracy) despite 78% hardware errors in HD\nrepresentation (outputs stuck at 0 or 1). By exploiting the unique properties\nof the underlying nanotechnologies, we show that HD computing, when implemented\nwith monolithic 3D integration, can be up to 420X more energy-efficient while\nusing 25X less area compared to traditional silicon CMOS implementations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:52:16 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Rahimi", "Abbas", ""], ["Wu", "Tony F.", ""], ["Li", "Haitong", ""], ["Rabaey", "Jan M.", ""], ["Wong", "H. -S. Philip", ""], ["Shulaker", "Max M.", ""], ["Mitra", "Subhasish", ""]]}, {"id": "1811.12474", "submitter": "Steve Hoover", "authors": "Steven Hoover, \\'Akos Hadnagy", "title": "Formally Verifying WARP-V, an Open-Source TL-Verilog RISC-V Core\n  Generator", "comments": "4-pages. Presented by \\'Akos Hadnagy at open-source hardware\n  conferences: ORConf 2018 and VSDOpen 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing-abstract and transaction-level design using TL-Verilog have shown\nsignificant productivity gains for logic design. In this work, we explored the\nnatural extension of transaction-level design methodology into formal\nverification.\n  WARP-V is a CPU core generator written in TL-Verilog. Our primary\nverification vehicle for WARP-V was a formal verification framework for RISC-V,\ncalled riscv-formal. The timing-abstract and transaction-level logic modeling\ntechniques of TL-Verilog greatly simplified the task of creating a harness\nconnecting the WARP-V model to the verification interface of riscv-formal.\nFurthermore, the same harness works across all RISC-V configurations of WARP-V.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:59:12 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hoover", "Steven", ""], ["Hadnagy", "\u00c1kos", ""]]}]