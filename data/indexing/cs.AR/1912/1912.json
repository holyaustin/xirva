[{"id": "1912.00154", "submitter": "Muhammet Abdullah Soyturk", "authors": "Muhammet Abdullah Soyturk, Konstantinos Parasyris, Behzad Salami,\n  Osman Unsal, Gulay Yalcin, Leonardo Bautista Gomez", "title": "Hardware Versus Software Fault Injection of Modern Undervolted SRAMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve power efficiency, researchers are experimenting with dynamically\nadjusting the supply voltage of systems below the nominal operating points.\nHowever, production systems are typically not allowed to function on voltage\nsettings that is below the reliable limit. Consequently, existing software\nfault tolerance studies are based on fault models, which inject faults on\nrandom fault locations using fault injection techniques. In this work we study\nwhether random fault injection is accurate to simulate the behavior of\nundervolted SRAMs.\n  Our study extends the Gem5 simulator to support fault injection on the caches\nof the simulated system. The fault injection framework uses fault maps, which\ndescribe the faulty bits of SRAMs, as inputs. To compare random fault injection\nand hardware guided fault injection, we use two types of fault maps. The first\ntype of maps are created through undervolting real SRAMs and observing the\nlocation of the erroneous bits, whereas the second type of maps are created by\ncorrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache\nof the simulated system and we monitor the behavior of the two types of fault\nmaps on the resiliency of six benchmarks. The difference among the resiliency\nof a benchmark when tested with the different fault maps can be up to 24%.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 07:57:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Soyturk", "Muhammet Abdullah", ""], ["Parasyris", "Konstantinos", ""], ["Salami", "Behzad", ""], ["Unsal", "Osman", ""], ["Yalcin", "Gulay", ""], ["Gomez", "Leonardo Bautista", ""]]}, {"id": "1912.01347", "submitter": "Farshad Moradi", "authors": "Farshad Moradi, Hooman Farkhani, Behzad Zeinali, Hamdam Ghanatian,\n  Johan Michel Alain Pelloux-Prayer, Tim Boehnert, Mohammad Zahedinejad, Hadi\n  Heidari, Vahid Nabaei, Ricardo Ferreira, Johan Akerman, Jens Kargaard Madsen", "title": "Spin-Orbit-Torque-based Devices, Circuits and Architectures", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spintronics, the use of spin of an electron instead of its charge, has\nreceived huge attention from research communities for different applications\nincluding memory, interconnects, logic implementation, neuromorphic computing,\nand many other applications. Here, in this paper, we review the works within\nspintronics, more specifically on spin-orbit torque (SOT) within different\nresearch groups. We also provide researchers an insight into the future\npotentials of the SOT-based designs. This comprehensive review paper covers\ndifferent aspects of SOT-based design from device and circuit to architecture\nlevel as well as more ambitious and futuristic applications of such technology.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 12:55:50 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Moradi", "Farshad", ""], ["Farkhani", "Hooman", ""], ["Zeinali", "Behzad", ""], ["Ghanatian", "Hamdam", ""], ["Pelloux-Prayer", "Johan Michel Alain", ""], ["Boehnert", "Tim", ""], ["Zahedinejad", "Mohammad", ""], ["Heidari", "Hadi", ""], ["Nabaei", "Vahid", ""], ["Ferreira", "Ricardo", ""], ["Akerman", "Johan", ""], ["Madsen", "Jens Kargaard", ""]]}, {"id": "1912.01664", "submitter": "Robert Guirado", "authors": "Robert Guirado, Hyoukjun Kwon, Eduard Alarc\\'on, Sergi Abadal and\n  Tushar Krishna", "title": "Understanding the Impact of On-chip Communication on DNN Accelerator\n  Performance", "comments": "ICECS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have flourished at an unprecedented pace in recent\nyears. They have achieved outstanding accuracy in fields such as computer\nvision, natural language processing, medicine or economics. Specifically,\nConvolutional Neural Networks (CNN) are particularly suited to object\nrecognition or identification tasks. This, however, comes at a high\ncomputational cost, prompting the use of specialized GPU architectures or even\nASICs to achieve high speeds and energy efficiency. ASIC accelerators\nstreamline the execution of certain dataflows amenable to CNN computation that\nimply the constant movement of large amounts of data, thereby turning on-chip\ncommunication into a critical function within the accelerator. This paper\nstudies the communication flows within CNN inference accelerators of edge\ndevices, with the aim to justify current and future decisions in the design of\nthe on-chip networks that interconnect their processing elements. Leveraging\nthis analysis, we then qualitatively discuss the potential impact of\nintroducing the novel paradigm of wireless on-chip network in this context.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 20:02:41 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Guirado", "Robert", ""], ["Kwon", "Hyoukjun", ""], ["Alarc\u00f3n", "Eduard", ""], ["Abadal", "Sergi", ""], ["Krishna", "Tushar", ""]]}, {"id": "1912.01710", "submitter": "Miriam Leeser", "authors": "Xin Fang, Stratis Ioannidis and Miriam Leeser", "title": "SIFO: Secure Computational Infrastructure using FPGA Overlays", "comments": "International Journal of Reconfigurable Computing, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure Function Evaluation (SFE) has received recent attention due to the\nmassive collection and mining of personal data, but remains impractical due to\nits large computational cost. Garbled Circuits (GC) is a protocol for\nimplementing SFE which can evaluate any function that can be expressed as a\nBoolean circuit and obtain the result while keeping each party's input private.\nRecent advances have led to a surge of garbled circuit implementations in\nsoftware for a variety of different tasks. However, these implementations are\ninefficient and therefore GC is not widely used, especially for large problems.\nThis research investigates, implements and evaluates secure computation\ngeneration using a heterogeneous computing platform featuring FPGAs. We have\ndesigned and implemented SIFO: Secure computational Infrastructure using FPGA\nOverlays. Unlike traditional FPGA design, a coarse grained overlay architecture\nis adopted which supports mapping SFE problems that are too large to map to a\nsingle FPGA. Host tools provided include SFE problem generator, parser and\nautomatic host code generation. Our design allows re-purposing an FPGA to\nevaluate different SFE tasks without the need for reprogramming, and fully\nexplores the parallelism for any GC problem. Our system demonstrates an order\nof magnitude speedup compared with an existing software platform.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 14:46:50 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Fang", "Xin", ""], ["Ioannidis", "Stratis", ""], ["Leeser", "Miriam", ""]]}, {"id": "1912.03413", "submitter": "Daniele Scarpazza", "authors": "Zhe Jia and Blake Tillman and Marco Maggioni and Daniele Paolo\n  Scarpazza", "title": "Dissecting the Graphcore IPU Architecture via Microbenchmarking", "comments": "91 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report focuses on the architecture and performance of the Intelligence\nProcessing Unit (IPU), a novel, massively parallel platform recently introduced\nby Graphcore and aimed at Artificial Intelligence/Machine Learning (AI/ML)\nworkloads. We dissect the IPU's performance behavior using microbenchmarks that\nwe crafted for the purpose. We study the IPU's memory organization and\nperformance. We study the latency and bandwidth that the on-chip and off-chip\ninterconnects offer, both in point-to-point transfers and in a spectrum of\ncollective operations, under diverse loads. We evaluate the IPU's compute power\nover matrix multiplication, convolution, and AI/ML primitives. We discuss\nactual performance in comparison with its theoretical limits. Our findings\nreveal how the IPU's architectural design affects its performance. Moreover,\nthey offer simple mental models to predict an application's performance on the\nIPU, on the basis of the computation and communication steps it involves. This\nreport is the natural extension to a novel architecture of a continuing effort\nof ours that focuses on the microbenchmark-based discovery of massively\nparallel architectures.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:10:19 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Jia", "Zhe", ""], ["Tillman", "Blake", ""], ["Maggioni", "Marco", ""], ["Scarpazza", "Daniele Paolo", ""]]}, {"id": "1912.05670", "submitter": "Jan Moritz Joseph", "authors": "Jan Moritz Joseph, Lennart Bamberg, Imad Hajjar, Anna Drewes, Behnam\n  Razi Perjikolaei, Alberto Garc\\'ia-Ortiz, Thilo Pionteck", "title": "Ratatoskr: An open-source framework for in-depth power, performance and\n  area analysis in 3D NoCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ratatoskr, an open-source framework for in-depth power,\nperformance and area (PPA) analysis in NoCs for 3D-integrated and heterogeneous\nSystem-on-Chips (SoCs). It covers all layers of abstraction by providing a NoC\nhardware implementation on RT level, a NoC simulator on cycle-accurate level\nand an application model on transaction level. By this comprehensive approach,\nratatoskr can provide the following specific PPA analyses: Dynamic power of\nlinks can be measured within 2.4% accuracy of bit-level simulations while\nmaintaining cycle-accurate simulation speed. Router power is determined from RT\nlevel synthesis combined with cycle-accurate simulations. The performance of\nthe whole NoC can be measured both via cycle-accurate and RT level simulations.\nThe performance of individual routers is obtained from RT level including\ngate-level verification. The NoC area is calculated from RT level. Despite\nthese manifold features, ratatoskr offers easy two-step user interaction:\nFirst, a single point-of-entry that allows to set design parameters and second,\nPPA reports are generated automatically. For both the input and the output,\ndifferent levels of abstraction can be chosen for high-level rapid network\nanalysis or low-level improvement of architectural details. The synthesize NoC\nmodel reduces up to 32% total router power and 3% router area in comparison to\na conventional standard router. As a forward-thinking and unique feature not\nfound in other NoC PPA-measurement tools, ratatoskr supports heterogeneous 3D\nintegration that is one of the most promising integration paradigms for\nupcoming SoCs. Thereby, ratatoskr lies the groundwork to design their\ncommunication architectures.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 22:16:38 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 15:40:36 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Joseph", "Jan Moritz", ""], ["Bamberg", "Lennart", ""], ["Hajjar", "Imad", ""], ["Drewes", "Anna", ""], ["Perjikolaei", "Behnam Razi", ""], ["Garc\u00eda-Ortiz", "Alberto", ""], ["Pionteck", "Thilo", ""]]}, {"id": "1912.06576", "submitter": "Salman Onsori", "authors": "Salman Onsori, Arghavan Asad, Kaamran Raahemifar, and Mahmood Fathy", "title": "An Energy-Efficient Heterogeneous Memory Architecture for Future Dark\n  Silicon Embedded Chip-Multiprocessors", "comments": "10 pages, 13 figures", "journal-ref": "IEEE Transactions on Emerging Topics in Computing 1 (2016): 1-1", "doi": "10.1109/TETC.2016.2563323", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main memories play an important role in overall energy consumption of\nembedded systems. Using conventional memory technologies in future designs in\nnanoscale era causes a drastic increase in leakage power consumption and\ntemperature-related problems. Emerging non-volatile memory (NVM) technologies\noffer many desirable characteristics such as near-zero leakage power, high\ndensity and non-volatility. They can significantly mitigate the issue of memory\nleakage power in future embedded chip-multiprocessor (eCMP) systems. However,\nthey suffer from challenges such as limited write endurance and high write\nenergy consumption which restrict them for adoption in modern memory systems.\nIn this article, we present a convex optimization model to design a 3D stacked\nhybrid memory architecture in order to minimize the future embedded systems\nenergy consumption in the dark silicon era. This proposed approach satisfies\nendurance constraint in order to design a reliable memory system. Our convex\nmodel optimizes numbers and placement of eDRAM and STT-RAM memory banks on the\nmemory layer to exploit the advantages of both technologies in future eCMPs.\nEnergy consumption, the main challenge in the dark silicon era, is represented\nas a major target in this work and it is minimized by the detailed optimization\nmodel in order to design a dark silicon aware 3D Chip-Multiprocessor.\nExperimental results show that in comparison with the Baseline memory design,\nthe proposed architecture improves the energy consumption and performance of\nthe 3D CMP on average about 61.33 and 9 percent respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 16:00:46 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Onsori", "Salman", ""], ["Asad", "Arghavan", ""], ["Raahemifar", "Kaamran", ""], ["Fathy", "Mahmood", ""]]}, {"id": "1912.06901", "submitter": "Sudarshan Sharma", "authors": "Sudarshan Sharma, Dhruv Thapar, Nikhil Bhelave and Mrigank Sharad", "title": "Adaptive Multi-bit SRAM Topology Based Analog PUF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physically Unclonable Functions (PUFs) are lightweight cryptographic\nprimitives for generating unique signatures from minuscule manufacturing\nvariations. In this work, we present lightweight, area efficient and low power\nadaptive multi-bit SRAM topology based Current Mirror Array (CMA) analog PUF\ndesign for securing the sensor nodes, authentication and key generation. The\nproposed Strong PUF increases the complexity of the machine learning attacks\nthus making it difficult for the adversary. The design is based on scl180\nlibrary.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 18:28:53 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sharma", "Sudarshan", ""], ["Thapar", "Dhruv", ""], ["Bhelave", "Nikhil", ""], ["Sharad", "Mrigank", ""]]}, {"id": "1912.10663", "submitter": "Su Dongchu", "authors": "Dongchu Su, Yong Li, Bo Yuan", "title": "SSR: A Stall Scheme Reducing Bubbles in Load-Use Hazard of RISC-V\n  Pipeline", "comments": "The International Conference on Advanced Information Networking and\n  Applications (AINA-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors usually adopt pipeline structure and often load data from\nmemory. At that point, the load-use hazard will inevitably occur, which usually\nstall the pipeline and reduce performance. This paper introduces and compares\ntwo schemes to solve load-use hazard. One is the traditional scheme that detect\nhazard between ID stage and EXE stage, which stalls the pipeline and insert\nbubbles between the two instructions. In the scheme we proposed, we add a\nsimple bypass unit between EXE and MEM stage that disables the stall of\nload-use hazard caused by the traditional scheme, which can reduce the bubble\nbetween the two instructions. It's quite a considerable benefit in eliminating\nbubbles especially in the long pipeline or programs of plenty load\ninstructions. The scheme was implemented in the open source RISC-V SoC\ngenerator Rocket-chip and synthesized in SMIC 130-nm technology. The results\nshow that the performance of the latter scheme is increased by 6.9% in the\nDhrystone benchmark with the reasonable cost of area and power.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 07:52:37 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Su", "Dongchu", ""], ["Li", "Yong", ""], ["Yuan", "Bo", ""]]}, {"id": "1912.11516", "submitter": "Aayush Ankit", "authors": "Aayush Ankit, Izzat El Hajj, Sai Rahul Chalamalasetti, Sapan Agarwal,\n  Matthew Marinella, Martin Foltin, John Paul Strachan, Dejan Milojicic,\n  Wen-mei Hwu, Kaushik Roy", "title": "PANTHER: A Programmable Architecture for Neural Network Training\n  Harnessing Energy-efficient ReRAM", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.ET eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of deep neural networks has been accompanied by\never-increasing energy and performance demands due to the expensive nature of\ntraining them. Numerous special-purpose architectures have been proposed to\naccelerate training: both digital and hybrid digital-analog using resistive RAM\n(ReRAM) crossbars. ReRAM-based accelerators have demonstrated the effectiveness\nof ReRAM crossbars at performing matrix-vector multiplication operations that\nare prevalent in training. However, they still suffer from inefficiency due to\nthe use of serial reads and writes for performing the weight gradient and\nupdate step. A few works have demonstrated the possibility of performing outer\nproducts in crossbars, which can be used to realize the weight gradient and\nupdate step without the use of serial reads and writes. However, these works\nhave been limited to low precision operations which are not sufficient for\ntypical training workloads. Moreover, they have been confined to a limited set\nof training algorithms for fully-connected layers only. To address these\nlimitations, we propose a bit-slicing technique for enhancing the precision of\nReRAM-based outer products, which is substantially different from bit-slicing\nfor matrix-vector multiplication only. We incorporate this technique into a\ncrossbar architecture with three variants catered to different training\nalgorithms. To evaluate our design on different types of layers in neural\nnetworks (fully-connected, convolutional, etc.) and training algorithms, we\ndevelop PANTHER, an ISA-programmable training accelerator with compiler\nsupport. Our evaluation shows that PANTHER achieves up to $8.02\\times$,\n$54.21\\times$, and $103\\times$ energy reductions as well as $7.16\\times$,\n$4.02\\times$, and $16\\times$ execution time reductions compared to digital\naccelerators, ReRAM-based accelerators, and GPUs, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:24:32 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ankit", "Aayush", ""], ["Hajj", "Izzat El", ""], ["Chalamalasetti", "Sai Rahul", ""], ["Agarwal", "Sapan", ""], ["Marinella", "Matthew", ""], ["Foltin", "Martin", ""], ["Strachan", "John Paul", ""], ["Milojicic", "Dejan", ""], ["Hwu", "Wen-mei", ""], ["Roy", "Kaushik", ""]]}, {"id": "1912.11523", "submitter": "Zane Weissman", "authors": "Zane Weissman, Thore Tiemann, Daniel Moghimi, Evan Custodio, Thomas\n  Eisenbarth and Berk Sunar", "title": "JackHammer: Efficient Rowhammer on Heterogeneous FPGA-CPU Platforms", "comments": "Accepted to IACR Transactions on Cryptographic Hardware and Embedded\n  Systems (TCHES), Volume 2020, Issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After years of development, FPGAs are finally making an appearance on\nmulti-tenant cloud servers. These heterogeneous FPGA-CPU architectures break\ncommon assumptions about isolation and security boundaries. Since the FPGA and\nCPU architectures share hardware resources, a new class of vulnerabilities\nrequires us to reassess the security and dependability of these platforms.\n  In this work, we analyze the memory and cache subsystem and study Rowhammer\nand cache attacks enabled on two proposed heterogeneous FPGA-CPU platforms by\nIntel: the Arria 10 GX with an integrated FPGA-CPU platform, and the Arria 10\nGX PAC expansion card which connects the FPGA to the CPU via the PCIe\ninterface. We show that while Intel PACs currently are immune to cache attacks\nfrom FPGA to CPU, the integrated platform is indeed vulnerable to Prime and\nProbe style attacks from the FPGA to the CPU's last level cache. Further, we\ndemonstrate JackHammer, a novel and efficient Rowhammer from the FPGA to the\nhost's main memory. Our results indicate that a malicious FPGA can perform\ntwice as fast as a typical Rowhammer attack from the CPU on the same system and\ncauses around four times as many bit flips as the CPU attack. We demonstrate\nthe efficacy of JackHammer from the FPGA through a realistic fault attack on\nthe WolfSSL RSA signing implementation that reliably causes a fault after an\naverage of fifty-eight RSA signatures, 25% faster than a CPU rowhammer attack.\nIn some scenarios our JackHammer attack produces faulty signatures more than\nthree times more often and almost three times faster than a conventional CPU\nrowhammer attack.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 20:37:36 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 00:26:57 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 21:09:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Weissman", "Zane", ""], ["Tiemann", "Thore", ""], ["Moghimi", "Daniel", ""], ["Custodio", "Evan", ""], ["Eisenbarth", "Thomas", ""], ["Sunar", "Berk", ""]]}, {"id": "1912.12636", "submitter": "Tzofnat Greenberg-Toledo", "authors": "Tzofnat Greenberg Toledo, Ben Perach, Daniel Soudry and Shahar\n  Kvatinsky", "title": "MTJ-Based Hardware Synapse Design for Quantized Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized neural networks (QNNs) are being actively researched as a solution\nfor the computational complexity and memory intensity of deep neural networks.\nThis has sparked efforts to develop algorithms that support both inference and\ntraining with quantized weight and activation values without sacrificing\naccuracy. A recent example is the GXNOR framework for stochastic training of\nternary and binary neural networks. In this paper, we introduce a novel\nhardware synapse circuit that uses magnetic tunnel junction (MTJ) devices to\nsupport the GXNOR training. Our solution enables processing near memory (PNM)\nof QNNs, therefore can further reduce the data movements from and into the\nmemory. We simulated MTJ-based stochastic training of a TNN over the MNIST and\nSVHN datasets and achieved an accuracy of 98.61% and 93.99%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 11:36:32 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Toledo", "Tzofnat Greenberg", ""], ["Perach", "Ben", ""], ["Soudry", "Daniel", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "1912.12953", "submitter": "Liu Ke", "authors": "Liu Ke, Udit Gupta, Carole-Jean Wu, Benjamin Youngjae Cho, Mark\n  Hempstead, Brandon Reagen, Xuan Zhang, David Brooks, Vikas Chandra, Utku\n  Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng\n  Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail\n  Smelyanskiy, Xiaodong Wang", "title": "RecNMP: Accelerating Personalized Recommendation with Near-Memory\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation systems leverage deep learning models and account\nfor the majority of data center AI cycles. Their performance is dominated by\nmemory-bound sparse embedding operations with unique irregular memory access\npatterns that pose a fundamental challenge to accelerate. This paper proposes a\nlightweight, commodity DRAM compliant, near-memory processing solution to\naccelerate personalized recommendation inference. The in-depth characterization\nof production-grade recommendation models shows that embedding operations with\nhigh model-, operator- and data-level parallelism lead to memory bandwidth\nsaturation, limiting recommendation inference performance. We propose RecNMP\nwhich provides a scalable solution to improve system throughput, supporting a\nbroad range of sparse embedding models. RecNMP is specifically tailored to\nproduction environments with heavy co-location of operators on a single server.\nSeveral hardware/software co-optimization techniques such as memory-side\ncaching, table-aware packet scheduling, and hot entry profiling are studied,\nresulting in up to 9.8x memory latency speedup over a highly-optimized\nbaseline. Overall, RecNMP offers 4.2x throughput improvement and 45.8% memory\nenergy savings.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 15:08:49 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Ke", "Liu", ""], ["Gupta", "Udit", ""], ["Wu", "Carole-Jean", ""], ["Cho", "Benjamin Youngjae", ""], ["Hempstead", "Mark", ""], ["Reagen", "Brandon", ""], ["Zhang", "Xuan", ""], ["Brooks", "David", ""], ["Chandra", "Vikas", ""], ["Diril", "Utku", ""], ["Firoozshahian", "Amin", ""], ["Hazelwood", "Kim", ""], ["Jia", "Bill", ""], ["Lee", "Hsien-Hsin S.", ""], ["Li", "Meng", ""], ["Maher", "Bert", ""], ["Mudigere", "Dheevatsa", ""], ["Naumov", "Maxim", ""], ["Schatz", "Martin", ""], ["Smelyanskiy", "Mikhail", ""], ["Wang", "Xiaodong", ""]]}]