[{"id": "1809.00419", "submitter": "Alex James Dr", "authors": "Olga Krestinskaya, Akshay Kumar Maan, Alex Pappachen James", "title": "Programmable Memristive Threshold Logic Gate Array", "comments": "IEEE Asia Pacific Conference on Circuits and Systems (IEEE APCCAS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the implementation of programmable threshold logic gate\n(TLG) crossbar array based on modified TLG cells for high speed processing and\ncomputation. The proposed TLG array operation does not depend on input signal\nand time pulses, comparing to the existing architectures. The circuit is\nimplemented using TSMC $180nm$ CMOS technology. The on-chip area and power\ndissipation of the simulated $3\\times 4$ TLG array is $1463 \\mu m^2$ and $425\n\\mu W$, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 01:10:49 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Krestinskaya", "Olga", ""], ["Maan", "Akshay Kumar", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1809.01221", "submitter": "Thinh Pham", "authors": "Thinh Hung Pham, Alexander Fell, Arnab Kumar Biswas, Siew-Kei Lam, and\n  Nandeesha Veeranna", "title": "CIDPro: Custom Instructions for Dynamic Program Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing side-channel attacks pose a major threat to embedded systems due to\ntheir ease of accessibility. We propose CIDPro, a framework that relies on\ndynamic program diversification to mitigate timing side-channel leakage. The\nproposed framework integrates the widely used LLVM compiler infrastructure and\nthe increasingly popular RISC-V FPGA soft-processor. The compiler automatically\ngenerates custom instructions in the security critical segments of the program,\nand the instructions execute on the RISC-V custom co-processor to produce\ndiversified timing characteristics on each execution instance. CIDPro has been\nimplemented on the Zynq7000 XC7Z020 FPGA device to study the performance\noverhead and security tradeoffs. Experimental results show that our solution\ncan achieve 80% and 86% timing side-channel capacity reduction for two\nbenchmarks with an acceptable performance overhead compared to existing\nsolutions. In addition, the proposed method incurs only a negligible hardware\narea overhead of 1% slices of the entire RISC-V system.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 19:46:59 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Pham", "Thinh Hung", ""], ["Fell", "Alexander", ""], ["Biswas", "Arnab Kumar", ""], ["Lam", "Siew-Kei", ""], ["Veeranna", "Nandeesha", ""]]}, {"id": "1809.01536", "submitter": "Lin Bai", "authors": "Lin Bai, Yiming Zhao and Xinming Huang", "title": "A CNN Accelerator on FPGA Using Depthwise Separable Convolution", "comments": "Accepted by IEEE Transaction on Circuits and Systems II: Express\n  Briefs, Volume 65, Issue 10, Oct 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely deployed in the fields\nof computer vision and pattern recognition because of their high accuracy.\nHowever, large convolution operations are computing-intensive that often\nrequires a powerful computing platform such as Graphics Processing Unit (GPU).\nThis makes it difficult to apply CNNs to portable devices. The state-of-the-art\nCNNs, such as MobileNetV2 and Xception, adopt depthwise separable convolution\nto replace the standard convolution for embedded platforms. That significantly\nreduces operations and parameters with only limited loss in accuracy. This\nhighly structured model is very suitable for Field-Programmable Gate Array\n(FPGA) implementation. In this paper, a scalable high performance depthwise\nseparable convolution optimized CNN accelerator is proposed. The accelerator\ncan be fit into an FPGA of different sizes, provided the balancing between\nhardware resources and processing speed. As an example, MobileNetV2 is\nimplemented on Arria 10 SoC FPGA, and the results show this accelerator can\nclassify each picture from ImageNet in 3.75ms, which is about 266.6 frames per\nsecond. This achieves 20x speedup if compared to CPU.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 21:14:54 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 18:25:41 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Bai", "Lin", ""], ["Zhao", "Yiming", ""], ["Huang", "Xinming", ""]]}, {"id": "1809.02887", "submitter": "Waqar  Ahmed", "authors": "Waqar Ahmad, Imran Hafeez Abbassi, Usman Sanwal and Hasan Mahmood", "title": "Accelerating Viterbi Algorithm using Custom Instruction Approach", "comments": "7 Pages, 4 Figures, 2018 14th IEEE/ASME International Conference on\n  Mechatronic and Embedded Systems and Applications (MESA)", "journal-ref": null, "doi": "10.1109/MESA.2018.8449144", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the decoding algorithms in communication networks are\nbecoming increasingly complex aiming to achieve high reliability in correctly\ndecoding received messages. These decoding algorithms involve computationally\ncomplex operations requiring high performance computing hardware, which are\ngenerally expensive. A cost-effective solution is to enhance the Instruction\nSet Architecture (ISA) of the processors by creating new custom instructions\nfor the computational parts of the decoding algorithms. In this paper, we\npropose to utilize the custom instruction approach to efficiently implement the\nwidely used Viterbi decoding algorithm by adding the assembly language\ninstructions to the ISA of DLX, PicoJava II and NIOS II processors, which\nrepresent RISC, stack and FPGA-based soft-core processor architectures,\nrespectively. By using the custom instruction approach, the execution time of\nthe Viterbi algorithm is significantly improved by approximately 3 times for\nDLX and PicoJava II, and by 2 times for NIOS II.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 22:21:14 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ahmad", "Waqar", ""], ["Abbassi", "Imran Hafeez", ""], ["Sanwal", "Usman", ""], ["Mahmood", "Hasan", ""]]}, {"id": "1809.03147", "submitter": "Hameedah Sultan", "authors": "Hameedah Sultan, Shashank Varshney, Smruti R Sarangi", "title": "Is Leakage Power a Linear Function of Temperature?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a study of the leakage power modeling techniques\ncommonly used in the architecture community. We further provide an analysis of\nthe error in leakage power estimation using the various modeling techniques. We\nstrongly believe that this study will help researchers determine an appropriate\nleakage model to use in their work, based on the desired modeling accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 06:06:16 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Sultan", "Hameedah", ""], ["Varshney", "Shashank", ""], ["Sarangi", "Smruti R", ""]]}, {"id": "1809.04570", "submitter": "Michaela Blott", "authors": "Michaela Blott, Thomas Preusser, Nicholas Fraser, Giulio Gambardella,\n  Kenneth O'Brien, Yaman Umuroglu", "title": "FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of\n  Quantized Neural Networks", "comments": "to be published in ACM TRETS Special Edition on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have rapidly become the most successful machine\nlearning algorithm, enabling ubiquitous machine vision and intelligent\ndecisions on even embedded computing-systems. While the underlying arithmetic\nis structurally simple, compute and memory requirements are challenging. One of\nthe promising opportunities is leveraging reduced-precision representations for\ninputs, activations and model parameters. The resulting scalability in\nperformance, power efficiency and storage footprint provides interesting design\ncompromises in exchange for a small reduction in accuracy. FPGAs are ideal for\nexploiting low-precision inference engines leveraging custom precisions to\nachieve the required numerical accuracy for a given application. In this\narticle, we describe the second generation of the FINN framework, an end-to-end\ntool which enables design space exploration and automates the creation of fully\ncustomized inference engines on FPGAs. Given a neural network description, the\ntool optimizes for given platforms, design targets and a specific precision. We\nintroduce formalizations of resource cost functions and performance\npredictions, and elaborate on the optimization algorithms. Finally, we evaluate\na selection of reduced precision neural networks ranging from CIFAR-10\nclassifiers to YOLO-based object detection on a range of platforms including\nPYNQ and AWS\\,F1, demonstrating new unprecedented measured throughput at\n50TOp/s on AWS-F1 and 5TOp/s on embedded devices.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:24:49 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Blott", "Michaela", ""], ["Preusser", "Thomas", ""], ["Fraser", "Nicholas", ""], ["Gambardella", "Giulio", ""], ["O'Brien", "Kenneth", ""], ["Umuroglu", "Yaman", ""]]}, {"id": "1809.05859", "submitter": "Phillip Stanley-Marbell", "authors": "Phillip Stanley-Marbell and Armin Alaghi and Michael Carbin and Eva\n  Darulova and Lara Dolecek and Andreas Gerstlauer and Ghayoor Gillani and\n  Djordje Jevdjic and Thierry Moreau and Mattia Cacciotti and Alexandros Daglis\n  and Natalie Enright Jerger and Babak Falsafi and Sasa Misailovic and Adrian\n  Sampson and Damien Zufferey", "title": "Exploiting Errors for Efficiency: A Survey from Circuits to Algorithms", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a computational task tolerates a relaxation of its specification or when\nan algorithm tolerates the effects of noise in its execution, hardware,\nprogramming languages, and system software can trade deviations from correct\nbehavior for lower resource usage. We present, for the first time, a synthesis\nof research results on computing systems that only make as many errors as their\nusers can tolerate, from across the disciplines of computer aided design of\ncircuits, digital system design, computer architecture, programming languages,\noperating systems, and information theory.\n  Rather than over-provisioning resources at each layer to avoid errors, it can\nbe more efficient to exploit the masking of errors occurring at one layer which\ncan prevent them from propagating to a higher layer. We survey tradeoffs for\nindividual layers of computing systems from the circuit level to the operating\nsystem level and illustrate the potential benefits of end-to-end approaches\nusing two illustrative examples. To tie together the survey, we present a\nconsistent formalization of terminology, across the layers, which does not\nsignificantly deviate from the terminology traditionally used by research\ncommunities in their layer of focus.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 11:55:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Stanley-Marbell", "Phillip", ""], ["Alaghi", "Armin", ""], ["Carbin", "Michael", ""], ["Darulova", "Eva", ""], ["Dolecek", "Lara", ""], ["Gerstlauer", "Andreas", ""], ["Gillani", "Ghayoor", ""], ["Jevdjic", "Djordje", ""], ["Moreau", "Thierry", ""], ["Cacciotti", "Mattia", ""], ["Daglis", "Alexandros", ""], ["Jerger", "Natalie Enright", ""], ["Falsafi", "Babak", ""], ["Misailovic", "Sasa", ""], ["Sampson", "Adrian", ""], ["Zufferey", "Damien", ""]]}, {"id": "1809.06016", "submitter": "Saber Moradi", "authors": "Saber Moradi, Rajit Manohar", "title": "The Impact of On-chip Communication on Memory Technologies for\n  Neuromorphic Systems", "comments": "26 pages, 6 figures, Journal of Physics D: Applied Physics 2018", "journal-ref": null, "doi": "10.1088/1361-6463/aae641", "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent nanoscale non-volatile memory technologies with high integration\ndensity offer a promising solution to overcome the scalability limitations of\nCMOS-based neural networks architectures, by efficiently exhibiting the key\nprinciple of neural computation. Despite the potential improvements in\ncomputational costs, designing high-performance on-chip communication networks\nthat support flexible, large-fanout connectivity remains as daunting task. In\nthis paper, we elaborate on the communication requirements of large-scale\nneuromorphic designs, and point out the differences with the conventional\nnetwork-on-chip architectures. We present existing approaches for on-chip\nneuromorphic routing networks, and discuss how new memory and integration\ntechnologies may help to alleviate the communication issues in constructing\nnext-generation intelligent computing machines.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 04:40:52 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 05:00:47 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Moradi", "Saber", ""], ["Manohar", "Rajit", ""]]}, {"id": "1809.07683", "submitter": "Peng Wei", "authors": "Jason Cong, Peng Wei, Cody Hao Yu, Peng Zhang", "title": "AutoAccel: Automated Accelerator Generation and Optimization with\n  Composable, Parallel and Pipeline Architecture", "comments": null, "journal-ref": null, "doi": "10.1145/3195970.3195999", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPU-FPGA heterogeneous architectures are attracting ever-increasing attention\nin an attempt to advance computational capabilities and energy efficiency in\ntoday's datacenters. These architectures provide programmers with the ability\nto reprogram the FPGAs for flexible acceleration of many workloads.\nNonetheless, this advantage is often overshadowed by the poor programmability\nof FPGAs whose programming is conventionally a RTL design practice. Although\nrecent advances in high-level synthesis (HLS) significantly improve the FPGA\nprogrammability, it still leaves programmers facing the challenge of\nidentifying the optimal design configuration in a tremendous design space.\n  This paper aims to address this challenge and pave the path from software\nprograms towards high-quality FPGA accelerators. Specifically, we first propose\nthe composable, parallel and pipeline (CPP) microarchitecture as a template of\naccelerator designs. Such a well-defined template is able to support efficient\naccelerator designs for a broad class of computation kernels, and more\nimportantly, drastically reduce the design space. Also, we introduce an\nanalytical model to capture the performance and resource trade-offs among\ndifferent design configurations of the CPP microarchitecture, which lays the\nfoundation for fast design space exploration. On top of the CPP\nmicroarchitecture and its analytical model, we develop the AutoAccel framework\nto make the entire accelerator generation automated. AutoAccel accepts a\nsoftware program as an input and performs a series of code transformations\nbased on the result of the analytical-model-based design space exploration to\nconstruct the desired CPP microarchitecture. Our experiments show that the\nAutoAccel-generated accelerators outperform their corresponding software\nimplementations by an average of 72x for a broad class of computation kernels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 06:35:27 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Cong", "Jason", ""], ["Wei", "Peng", ""], ["Yu", "Cody Hao", ""], ["Zhang", "Peng", ""]]}, {"id": "1809.07696", "submitter": "Jeffrey Young", "authors": "Jeffrey S. Young, Eric Hein, Srinivas Eswar, Patrick Lavin, Jiajia Li,\n  Jason Riedy, Richard Vuduc, Thomas M. Conte", "title": "A Microbenchmark Characterization of the Emu Chick", "comments": null, "journal-ref": "Parallel Computing, 2019, ISSN 0167-8191", "doi": "10.1016/j.parco.2019.04.012", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Emu Chick is a prototype system designed around the concept of migratory\nmemory-side processing. Rather than transferring large amounts of data across\npower-hungry, high-latency interconnects, the Emu Chick moves lightweight\nthread contexts to near-memory cores before the beginning of each memory read.\nThe current prototype hardware uses FPGAs to implement cache-less \"Gossamer\ncores for doing computational work and a stationary core to run basic operating\nsystem functions and migrate threads between nodes. In this multi-node\ncharacterization of the Emu Chick, we extend an earlier single-node\ninvestigation (Hein, et al. AsHES 2018) of the the memory bandwidth\ncharacteristics of the system through benchmarks like STREAM, pointer chasing,\nand sparse matrix-vector multiplication. We compare the Emu Chick hardware to\narchitectural simulation and an Intel Xeon-based platform. Our results\ndemonstrate that for many basic operations the Emu Chick can use available\nmemory bandwidth more efficiently than a more traditional, cache-based\narchitecture although bandwidth usage suffers for computationally intensive\nworkloads like SpMV. Moreover, the Emu Chick provides stable, predictable\nperformance with up to 65% of the peak bandwidth utilization on a random-access\npointer chasing benchmark with weak locality.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 15:52:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:37:16 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 14:17:25 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Young", "Jeffrey S.", ""], ["Hein", "Eric", ""], ["Eswar", "Srinivas", ""], ["Lavin", "Patrick", ""], ["Li", "Jiajia", ""], ["Riedy", "Jason", ""], ["Vuduc", "Richard", ""], ["Conte", "Thomas M.", ""]]}, {"id": "1809.07862", "submitter": "Naseef Mansoor", "authors": "Naseef Mansoor, Abhishek Vashist, M Meraj Ahmed, Md Shahriar Shamim,\n  Syed Ashraf Mamun, Amlan Ganguly", "title": "A Traffic-Aware Medium Access Control Mechanism for Energy-Efficient\n  Wireless Network-on-Chip Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless interconnection has emerged as an energy efficient solution to the\nchallenges of multi-hop communication over the wireline paths in conventional\nNetworks-on-Chips (NoCs). However, to ensure the full benefits of this novel\ninterconnect technology, design of simple, fair and efficient Medium Access\nControl (MAC) mechanism to grant access to the on-chip wireless communication\nchannel is needed. Moreover, to adapt to the varying traffic demands from the\napplications running on a multicore environment, MAC mechanisms should\ndynamically adjust the transmission slots of the wireless interfaces (WIs).\nSuch dynamic adjustment in transmission slots will result in improving the\nutilization of the wireless medium in a Wireless NoC (WiNoC). In this paper we\npresent the design of two dynamic MAC mechanisms that adjust the transmission\nslots of the WIs based on predicted traffic demands and allow partial packet\ntransfer. Through system level simulations, we demonstrate that the traffic\naware MAC mechanisms are more energy efficient as well as capable of sustaining\nhigher data bandwidth in WiNoCs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 20:02:10 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Mansoor", "Naseef", ""], ["Vashist", "Abhishek", ""], ["Ahmed", "M Meraj", ""], ["Shamim", "Md Shahriar", ""], ["Mamun", "Syed Ashraf", ""], ["Ganguly", "Amlan", ""]]}, {"id": "1809.08358", "submitter": "Xin Ma", "authors": "Xin Ma, Liang Chang, Shuangchen Li, Lei Deng, Yufei Ding, Yuan Xie", "title": "In-memory multiplication engine with SOT-MRAM based stochastic computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing-in-memory (PIM) turns out to be a promising solution to\nbreakthrough the memory wall and the power wall. While prior PIM designs yield\nsuccessful implementation of bitwise Boolean logic operations locally in\nmemory, it is difficult to accomplish the multiplication (MUL) instruction in a\nfast and efficient manner. In this paper, we propose a new stochastic computing\n(SC) design to perform MUL with in-memory operations. Instead of using the\nstochastic number generators (SNGs), we harness the inherent stochasticity in\nthe memory write behavior of the magnetic random access memory (MRAM). Each\nmemory bit serves as an SC engine, performs MUL on operands in the form of\nwrite voltage pulses, and stores the MUL outcome in-situ. The proposed design\nprovides up to 4x improvement in performance compared with conversational SC\napproaches, and achieves 18x speedup over implementing MUL with only in-memory\nbitwise Boolean logic operations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 00:57:09 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Ma", "Xin", ""], ["Chang", "Liang", ""], ["Li", "Shuangchen", ""], ["Deng", "Lei", ""], ["Ding", "Yufei", ""], ["Xie", "Yuan", ""]]}, {"id": "1809.08828", "submitter": "Mohammad Bakhshalipour", "authors": "Mohammad Bakhshalipour, HamidReza Zare, Pejman Lotfi-Kamran, Hamid\n  Sarbazi-Azad", "title": "Die-Stacked DRAM: Memory, Cache, or MemCache?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Die-stacked DRAM is a promising solution for satisfying the ever-increasing\nmemory bandwidth requirements of multi-core processors. Manufacturing\ntechnology has enabled stacking several gigabytes of DRAM modules on the active\ndie, thereby providing orders of magnitude higher bandwidth as compared to the\nconventional DIMM-based DDR memories. Nevertheless, die-stacked DRAM, due to\nits limited capacity, cannot accommodate entire datasets of modern big-data\napplications. Therefore, prior proposals use it either as a sizable memory-side\ncache or as a part of the software-visible main memory. Cache designs can adapt\nthemselves to the dynamic variations of applications but suffer from the tag\nstorage/latency/bandwidth overhead. On the other hand, memory designs eliminate\nthe need for tags, and hence, provide efficient access to data, but are unable\nto capture the dynamic behaviors of applications due to their static nature.\n  In this work, we make a case for using the die-stacked DRAM partly as main\nmemory and partly as a cache. We observe that in modern big-data applications\nthere are many hot pages with a large number of accesses. Based on this\nobservation, we propose to use a portion of the die-stacked DRAM as main memory\nto host hot pages, enabling serving a significant number of the accesses from\nthe high-bandwidth DRAM without the overhead of tag-checking, and manage the\nrest of the DRAM as a cache, for capturing the dynamic behavior of\napplications. In this proposal, a software procedure pre-processes the\napplication and determines hot pages, then asks the OS to map them to the\nmemory portion of the die-stacked DRAM. The cache portion of the die-stacked\nDRAM is managed by hardware, caching data allocated in the off-chip memory.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 10:18:48 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Bakhshalipour", "Mohammad", ""], ["Zare", "HamidReza", ""], ["Lotfi-Kamran", "Pejman", ""], ["Sarbazi-Azad", "Hamid", ""]]}, {"id": "1809.09249", "submitter": "Himanshu Thapliyal", "authors": "Edgard Mu\\~noz-Coreas, Himanshu Thapliyal", "title": "T-count Optimized Quantum Circuits for Bilinear Interpolation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum circuits for basic image processing functions such as bilinear\ninterpolation are required to implement image processing algorithms on quantum\ncomputers. In this work, we propose quantum circuits for the bilinear\ninterpolation of NEQR encoded images based on Clifford+T gates. Quantum\ncircuits for the scale up operation and scale down operation are illustrated.\nThe proposed quantum circuits are based on quantum Clifford+T gates and are\noptimized for T-count. Quantum circuits based on Clifford+T gates can be made\nfault tolerant but the T gate is very costly to implement. As a result,\nreducing T-count is an important optimization goal. The proposed quantum\nbilinear interpolation circuits are based on (i) a quantum adder, (ii) a\nproposed quantum subtractor, and (iii) a quantum multiplication circuit.\nFurther, both designs are compared and shown to be superior to existing work in\nterms of T-count. The proposed quantum bilinear interpolation circuits for the\nscale down operation and for the scale up operation each have a $92.52\\%$\nimprovement in terms of T-count compared to the existing work.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 22:45:25 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 19:41:59 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 22:11:55 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Mu\u00f1oz-Coreas", "Edgard", ""], ["Thapliyal", "Himanshu", ""]]}, {"id": "1809.09732", "submitter": "Himanshu Thapliyal", "authors": "Himanshu Thapliyal, Edgard Mu\\~noz-Coreas, T. S. S. Varun, Travis S.\n  Humble", "title": "Quantum Circuit Designs of Integer Division Optimizing T-count and\n  T-depth", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum circuits for mathematical functions such as division are necessary to\nuse quantum computers for scientific computing. Quantum circuits based on\nClifford+T gates can easily be made fault-tolerant but the T gate is very\ncostly to implement. The small number of qubits available in existing quantum\ncomputers adds another constraint on quantum circuits. As a result, reducing\nT-count and qubit cost have become important optimization goals. The design of\nquantum circuits for integer division has caught the attention of researchers\nand designs have been proposed in the literature. However, these designs suffer\nfrom excessive T gate and qubit costs. Many of these designs also produce\nsignificant garbage output resulting in additional qubit and T gate costs to\neliminate these outputs. In this work, we propose two quantum integer division\ncircuits. The first proposed quantum integer division circuit is based on the\nrestoring division algorithm and the second proposed design implements the\nnon-restoring division algorithm. Both proposed designs are optimized in terms\nof T-count, T-depth and qubits. Both proposed quantum circuit designs are based\non (i) a quantum subtractor, (ii) a quantum adder-subtractor circuit, and (iii)\na novel quantum conditional addition circuit. Our proposed restoring division\ncircuit achieves average T-count savings from $79.03 \\%$ to $91.69 \\%$ compared\nto the existing works. Our proposed non-restoring division circuit achieves\naverage T-count savings from $49.75 \\%$ to $90.37 \\%$ compared to the existing\nworks. Further, both our proposed designs have linear T-depth.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 21:25:54 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Thapliyal", "Himanshu", ""], ["Mu\u00f1oz-Coreas", "Edgard", ""], ["Varun", "T. S. S.", ""], ["Humble", "Travis S.", ""]]}, {"id": "1809.11156", "submitter": "Daniel Ziener", "authors": "Daniel Ziener", "title": "Improving Reliability, Security, and Efficiency of Reconfigurable\n  Hardware Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "urn:nbn:de:bvb:29-opus4-92715", "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this treatise, my research on methods to improve efficiency, reliability,\nand security of reconfigurable hardware systems, i.e., FPGAs, through partial\ndynamic reconfiguration is outlined. The efficiency of reconfigurable systems\ncan be improved by loading optimized data paths on-the-fly on an FPGA fabric.\nThis technique was applied to the acceleration of SQL queries for large\ndatabase applications as well as for image and signal processing applications.\nThe focus was not only on performance improvements and resource efficiency, but\nalso the energy efficiency has been significantly improved. In the area of\nreliability, countermeasures against radiation-induced faults and aging effects\nfor long mission times were investigated and applied to SRAM-FPGA-based\nsatellite systems. Finally, to increase the security of cryptographic\nFPGA-based implementations against physical attacks, i.e., side-channel and\nfault injection analysis as well as reverse engineering, it is proposed to\ntransform static circuit structures into dynamic ones by applying dynamic\npartial reconfiguration.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 17:40:05 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Ziener", "Daniel", ""]]}]