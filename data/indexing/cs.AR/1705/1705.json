[{"id": "1705.00218", "submitter": "Riyansh Ketan Karani", "authors": "Riyansh K. Karani, Akash K. Rana, Dhruv H. Reshamwala, Kishore\n  Saldanha", "title": "A floating point division unit based on Taylor-Series expansion\n  algorithm and Iterative Logarithmic Multiplier", "comments": "NeCoM, CSITEC - 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating point division, even though being an infrequent operation in the\ntraditional sense, is indis- pensable when it comes to a range of\nnon-traditional applications such as K-Means Clustering and QR Decomposition\njust to name a few. In such applications, hardware support for floating point\ndivision would boost the performance of the entire system. In this paper, we\npresent a novel architecture for a floating point division unit based on the\nTaylor-series expansion algorithm. We show that the Iterative Logarithmic\nMultiplier is very well suited to be used as a part of this architecture. We\npropose an implementation of the powering unit that can calculate an odd power\nand an even power of a number simultaneously, meanwhile having little hardware\noverhead when compared to the Iterative Logarithmic Multiplier.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 17:58:28 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Karani", "Riyansh K.", ""], ["Rana", "Akash K.", ""], ["Reshamwala", "Dhruv H.", ""], ["Saldanha", "Kishore", ""]]}, {"id": "1705.01626", "submitter": "Minsoo Rhu", "authors": "Minsoo Rhu, Mike O'Connor, Niladrish Chatterjee, Jeff Pool, Stephen W.\n  Keckler", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular deep learning frameworks require users to fine-tune their memory\nusage so that the training data of a deep neural network (DNN) fits within the\nGPU physical memory. Prior work tries to address this restriction by\nvirtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be\nutilized for memory allocations. Despite its merits, virtualizing memory can\nincur significant performance overheads when the time needed to copy data back\nand forth from CPU memory is higher than the latency to perform the\ncomputations required for DNN forward and backward propagation. We introduce a\nhigh-performance virtualization strategy based on a \"compressing DMA engine\"\n(cDMA) that drastically reduces the size of the data structures that are\ntargeted for CPU-side allocations. The cDMA engine offers an average 2.6x\n(maximum 13.8x) compression ratio by exploiting the sparsity inherent in\noffloaded data, improving the performance of virtualized DNNs by an average 32%\n(maximum 61%).\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 21:07:47 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Rhu", "Minsoo", ""], ["O'Connor", "Mike", ""], ["Chatterjee", "Niladrish", ""], ["Pool", "Jeff", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "1705.01738", "submitter": "Amit Kulkarni", "authors": "Amit Kulkarni and Dirk Stroobandt and Andre Werner and Florian Fricke\n  and Michael Huebner", "title": "Pixie: A heterogeneous Virtual Coarse-Grained Reconfigurable Array for\n  high performance image processing applications", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/01", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse-Grained Reconfigurable Arrays (CGRAs) enable ease of programmability\nand result in low development costs. They enable the ease of use specifically\nin reconfigurable computing applications. The smaller cost of compilation and\nreduced reconfiguration overhead enables them to become attractive platforms\nfor accelerating high-performance computing applications such as image\nprocessing. The CGRAs are ASICs and therefore, expensive to produce. However,\nField Programmable Gate Arrays (FPGAs) are relatively cheaper for low volume\nproducts but they are not so easily programmable. We combine best of both\nworlds by implementing a Virtual Coarse-Grained Reconfigurable Array (VCGRA) on\nFPGA. VCGRAs are a trade off between FPGA with large routing overheads and\nASICs. In this perspective we present a novel heterogeneous Virtual\nCoarse-Grained Reconfigurable Array (VCGRA) called \"Pixie\" which is suitable\nfor implementing high performance image processing applications. The proposed\nVCGRA contains generic processing elements and virtual channels that are\ndescribed using the Hardware Description Language VHDL. Both elements have been\noptimized by using the parameterized configuration tool flow and result in a\nresource reduction of 24% for each processing elements and 82% for each virtual\nchannels respectively.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:35:02 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Kulkarni", "Amit", ""], ["Stroobandt", "Dirk", ""], ["Werner", "Andre", ""], ["Fricke", "Florian", ""], ["Huebner", "Michael", ""]]}, {"id": "1705.02610", "submitter": "Bing Li", "authors": "Bing Li, Christoph Knoth, Walter Schneider, Manuel Schmidt, Ulf\n  Schlichtmann", "title": "Static Timing Model Extraction for Combinational Circuits", "comments": "International Workshop on Power and Timing Modeling, Optimization and\n  Simulation (PATMOS), 2008", "journal-ref": null, "doi": "10.1007/978-3-540-95948-9_16", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large circuits, static timing analysis (STA) needs to be performed in a\nhierarchical manner to achieve higher performance in arrival time propagation.\nIn hierarchical STA, efficient and accurate timing models of sub-modules need\nto be created. We propose a timing model extraction method that significantly\nreduces the size of timing models without losing any accuracy by removing\nredundant timing information. Circuit components which do not contribute to the\ndelay of any input to output pair are removed. The proposed method is\ndeterministic. Compared to the original models, the numbers of edges and\nvertices of the resulting timing models are reduced by 84% and 85% on average,\nrespectively, which are significantly more than the results achieved by other\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 12:49:19 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Li", "Bing", ""], ["Knoth", "Christoph", ""], ["Schneider", "Walter", ""], ["Schmidt", "Manuel", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.02730", "submitter": "Abhishek Jain", "authors": "Abhishek Kumar Jain and Douglas L. Maskell and Suhaib A. Fahmy", "title": "Resource-Aware Just-in-Time OpenCL Compiler for Coarse-Grained FPGA\n  Overlays", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/02", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA vendors have recently started focusing on OpenCL for FPGAs because of\nits ability to leverage the parallelism inherent to heterogeneous computing\nplatforms. OpenCL allows programs running on a host computer to launch\naccelerator kernels which can be compiled at run-time for a specific\narchitecture, thus enabling portability. However, the prohibitive compilation\ntimes (specifically the FPGA place and route times) are a major stumbling block\nwhen using OpenCL tools from FPGA vendors. The long compilation times mean that\nthe tools cannot effectively use just-in-time (JIT) compilation or runtime\nperformance scaling. Coarse-grained overlays represent a possible solution by\nvirtue of their coarse granularity and fast compilation. In this paper, we\npresent a methodology for run-time compilation of OpenCL kernels to a DSP block\nbased coarse-grained overlay, rather than directly to the fine-grained FPGA\nfabric. The proposed methodology allows JIT compilation and on-demand\nresource-aware kernel replication to better utilize available overlay\nresources, raising the abstraction level while reducing compile times\nsignificantly. We further demonstrate that this approach can even be used for\nrun-time compilation of OpenCL kernels on the ARM processor of the embedded\nheterogeneous Zynq device.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 02:59:45 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Jain", "Abhishek Kumar", ""], ["Maskell", "Douglas L.", ""], ["Fahmy", "Suhaib A.", ""]]}, {"id": "1705.02732", "submitter": "David Wilson", "authors": "David Wilson and Greg Stitt", "title": "A Scalable, Low-Overhead Finite-State Machine Overlay for Rapid FPGA\n  Application Development", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/03", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Productivity issues such as lengthy compilation and limited code reuse have\nrestricted usage of field-programmable gate arrays (FPGAs), despite significant\ntechnical advantages. Recent work into overlays -- virtual coarse-grained\narchitectures implemented atop FPGAs -- has aimed to address these concerns\nthrough abstraction, but have mostly focused on pipelined applications with\nminimal control requirements. Although research has introduced overlays for\nfinite-state machines, those architectures suffer from limited scalability and\nflexibility, which we address with a new overlay architecture using memory\ndecomposition on transitional logic. Although our overlay provides modest\naverage improvements of 15% to 29% fewer lookup tables for individual\nfinite-state machines, for the more common usage of an overlay supporting\ndifferent finite-state machines, our overlay achieves a 77% to 99% reduction in\nlookup tables. In addition, our overlay reduces compilation time to tenths of a\nsecond to enable rapid iterative-development methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 03:08:00 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wilson", "David", ""], ["Stitt", "Greg", ""]]}, {"id": "1705.02734", "submitter": "Siddhartha", "authors": "Siddhartha and Nachiket Kapre", "title": "Out-of-Order Dataflow Scheduling for FPGA Overlays", "comments": "Presented at 3rd International Workshop on Overlay Architectures for\n  FPGAs (OLAF 2017) arXiv:1704.08802", "journal-ref": null, "doi": null, "report-no": "OLAF/2017/04", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit floating-point DSPs in the Arria10 FPGA and multi-pumping feature\nof the M20K RAMs to build a dataflow-driven soft processor fabric for large\ngraph workloads. In this paper, we introduce the idea of out-of-order node\nscheduling across a large number of local nodes (thousands) per processor by\ncombining an efficient node tagging scheme along with leading-one detector\ncircuits. We use a static one-time node labeling algorithm to sort nodes based\non criticality to organize local memory inside each soft processor. This\ntranslates to a small ~6% memory overhead. When compared to a memory-expensive\nFIFO-based first-come-first-serve approach used in previous studies, we deliver\nup to 50% performance improvement while eliminating the cost of the FIFOs. On\nthe Arria10 10AX115S board, we can create an overlay design of up to 300\nprocessors connected by high bandwidth Hoplite NoC at frequencies up to 250MHz.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 03:18:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Siddhartha", "", ""], ["Kapre", "Nachiket", ""]]}, {"id": "1705.03623", "submitter": "Youyou Lu", "authors": "Youyou Lu, Jiwu Shu, Long Sun, Onur Mutlu", "title": "Improving the Performance and Endurance of Persistent Memory with\n  Loose-Ordering Consistency", "comments": "This paper has been accepted by IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2017.2701364", "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent memory provides high-performance data persistence at main memory.\nMemory writes need to be performed in strict order to satisfy storage\nconsistency requirements and enable correct recovery from system crashes.\nUnfortunately, adhering to such a strict order significantly degrades system\nperformance and persistent memory endurance. This paper introduces a new\nmechanism, Loose-Ordering Consistency (LOC), that satisfies the ordering\nrequirements at significantly lower performance and endurance loss. LOC\nconsists of two key techniques. First, Eager Commit eliminates the need to\nperform a persistent commit record write within a transaction. We do so by\nensuring that we can determine the status of all committed transactions during\nrecovery by storing necessary metadata information statically with blocks of\ndata written to memory. Second, Speculative Persistence relaxes the write\nordering between transactions by allowing writes to be speculatively written to\npersistent memory. A speculative write is made visible to software only after\nits associated transaction commits. To enable this, our mechanism supports the\ntracking of committed transaction ID and multi-versioning in the CPU cache. Our\nevaluations show that LOC reduces the average performance overhead of memory\npersistence from 66.9% to 34.9% and the memory write traffic overhead from\n17.1% to 3.4% on a variety of workloads.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 06:47:40 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lu", "Youyou", ""], ["Shu", "Jiwu", ""], ["Sun", "Long", ""], ["Mutlu", "Onur", ""]]}, {"id": "1705.04627", "submitter": "Myoungsoo Jung", "authors": "Myoungsoo Jung and Mahmut T. Kandemir", "title": "Sprinkler: Maximizing Resource Utilization in Many-Chip Solid State\n  Disks", "comments": "This paper is published at 20th IEEE International Symposium On High\n  Performance Computer Architecture", "journal-ref": null, "doi": "10.1109/HPCA.2014.6835961", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource utilization is one of the emerging problems in many-chip SSDs. In\nthis paper, we propose Sprinkler, a novel device-level SSD controller, which\ntargets maximizing resource utilization and achieving high performance without\nadditional NAND flash chips. Specifically, Sprinkler relaxes parallelism\ndependency by scheduling I/O requests based on internal resource layout rather\nthan the order imposed by the device-level queue. In addition, Sprinkler\nimproves flash-level parallelism and reduces the number of transactions (i.e.,\nimproves transactional-locality) by over-committing flash memory requests to\nspecific resources. Our extensive experimental evaluation using a\ncycle-accurate large-scale SSD simulation framework shows that a many-chip SSD\nequipped with our Sprinkler provides at least 56.6% shorter latency and 1.8 ~\n2.2 times better throughput than the state-of-the-art SSD controllers. Further,\nit improves overall resource utilization by 68.8% under different I/O request\npatterns and provides, on average, 80.2% more flash-level parallelism by\nreducing half of the flash memory requests at runtime.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 15:34:10 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Jung", "Myoungsoo", ""], ["Kandemir", "Mahmut T.", ""]]}, {"id": "1705.04975", "submitter": "Bing Li", "authors": "Bing Li, Ning Chen, Manuel Schmidt, Walter Schneider, Ulf Schlichtmann", "title": "On Hierarchical Statistical Static Timing Analysis", "comments": "Design, Automation and Test in Europe (DATE) 2009", "journal-ref": null, "doi": "10.1109/DATE.2009.5090869", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical static timing analysis deals with the increasing variations in\nmanufacturing processes to reduce the pessimism in the worst case timing\nanalysis. Because of the correlation between delays of circuit components,\ntiming model generation and hierarchical timing analysis face more challenges\nthan in static timing analysis. In this paper, a novel method to generate\ntiming models for combinational circuits considering variations is proposed.\nThe resulting timing models have accurate input-output delays and are about 80%\nsmaller than the original circuits. Additionally, an accurate hierarchical\ntiming analysis method at design level using pre-characterized timing models is\nproposed. This method incorporates the correlation between modules by replacing\nindependent random variables to improve timing accuracy. Experimental results\nshow that the correlation between modules strongly affects the delay\ndistribution of the hierarchical design and the proposed method has good\naccuracy compared with Monte Carlo simulation, but is faster by three orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:29:29 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Bing", ""], ["Chen", "Ning", ""], ["Schmidt", "Manuel", ""], ["Schneider", "Walter", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04976", "submitter": "Bing Li", "authors": "Bing Li, Ning Chen, Ulf Schlichtmann", "title": "Timing Model Extraction for Sequential Circuits Considering Process\n  Variations", "comments": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD),\n  2009", "journal-ref": null, "doi": "10.1145/1687399.1687463", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As semiconductor devices continue to scale down, process vari- ations become\nmore relevant for circuit design. Facing such variations, statistical static\ntiming analysis is introduced to model variations more accurately so that the\npessimism in tra- ditional worst case timing analysis is reduced. Because all\nde- lays are modeled using correlated random variables, most statis- tical\ntiming methods are much slower than corner based timing analysis. To speed up\nstatistical timing analysis, we propose a method to extract timing models for\nflip-flop and latch based sequential circuits respectively. When such a circuit\nis used as a module in a hierarchical design, the timing model instead of the\noriginal circuit is used for timing analysis. The extracted timing models are\nmuch smaller than the original circuits. Ex- periments show that using\nextracted timing models accelerates timing verification by orders of magnitude\ncompared to previ- ous approaches using flat netlists directly. Accuracy is\nmain- tained, however, with the mean and standard deviation of the clock period\nboth showing usually less than 1% error compared to Monte Carlo simulation on a\nnumber of benchmark circuits.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:32:57 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Bing", ""], ["Chen", "Ning", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04979", "submitter": "Bing Li", "authors": "Bing Li, Ning Chen, Ulf Schlichtmann", "title": "Fast Statistical Timing Analysis for Circuits with Post-Silicon Tunable\n  Clock Buffers", "comments": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD),\n  2011", "journal-ref": null, "doi": "10.1109/ICCAD.2011.6105314", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-Silicon Tunable (PST) clock buffers are widely used in high performance\ndesigns to counter process variations. By allowing delay compensation between\nconsecutive register stages, PST buffers can effectively improve the yield of\ndigital circuits. To date, the evaluation of manufacturing yield in the\npresence of PST buffers is only possible using Monte Carlo simulation. In this\npaper, we propose an alternative method based on graph transformations, which\nis much faster, more than 1000 times, and computes a parametric minimum clock\nperiod. It also identifies the gates which are most critical to the circuit\nperformance, therefore enabling a fast analysis-optimization flow.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:39:59 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Bing", ""], ["Chen", "Ning", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04981", "submitter": "Bing Li", "authors": "Bing Li, Ning Chen, Yang Xu, Ulf Schlichtmann", "title": "On Timing Model Extraction and Hierarchical Statistical Timing Analysis", "comments": null, "journal-ref": "IEEE Transactions on Computer-Aided Design of Integrated Circuits\n  and Systems 32(3), 367-380, March 2013", "doi": "10.1109/TCAD.2012.2228305", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the challenges to apply Statistical Static\nTiming Analysis (SSTA) in hierarchical design flow, where modules supplied by\nIP vendors are used to hide design details for IP protection and to reduce the\ncomplexity of design and verification. For the three basic circuit types,\ncombinational, flip-flop-based and latch-controlled, we propose methods to\nextract timing models which contain interfacing as well as compressed internal\nconstraints. Using these compact timing models the runtime of full-chip timing\nanalysis can be reduced, while circuit details from IP vendors are not exposed.\nWe also propose a method to reconstruct the correlation between modules during\nfull-chip timing analysis. This correlation can not be incorporated into timing\nmodels because it depends on the layout of the corresponding modules in the\nchip. In addition, we investigate how to apply the extracted timing models with\nthe reconstructed correlation to evaluate the performance of the complete\ndesign. Experiments demonstrate that using the extracted timing models and\nreconstructed correlation full-chip timing analysis can be several times faster\nthan applying the flattened circuit directly, while the accuracy of statistical\ntiming analysis is still well maintained.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:49:14 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Bing", ""], ["Chen", "Ning", ""], ["Xu", "Yang", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04982", "submitter": "Bing Li", "authors": "Tsun-Ming, Tseng Bing Li, Tsung-Yi Ho, Ulf Schlichtmann", "title": "Post-Route Refinement for High-Frequency PCBs Considering Meander\n  Segment Alleviation", "comments": "ACM Great Lake Symposium on VLSI (GLSVLSI), 2013", "journal-ref": null, "doi": "10.1145/2483028.2483123", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a post-processing framework which iteratively\nrefines the routing results from an existing PCB router by removing dense\nmeander segments. By swapping and detouring dense meander segments the proposed\nmethod can effectively alleviate accumulating crosstalk noise, while respecting\npre-defined area constraints. Experimental results show more than 85% reduction\nof the meander segments and hence the noise cost.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:53:07 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Tsun-Ming", "", ""], ["Li", "Tseng Bing", ""], ["Ho", "Tsung-Yi", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04983", "submitter": "Bing Li", "authors": "Tsun-Ming Tseng, Bing Li, Tsung-Yi Ho, Ulf Schlichtmann", "title": "Post-Route Alleviation of Dense Meander Segments in High-Performance\n  Printed Circuit Boards", "comments": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD),\n  2013", "journal-ref": null, "doi": "10.1109/ICCAD.2013.6691193", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length-matching is an important technique to balance delays of bus signals in\nhigh-performance PCB routing. Existing routers, however, may generate dense\nmeander segments with small distance. Signals propagating across these meander\nsegments exhibit a speedup effect due to crosstalks between the segments of the\nsame wire, thus leading to mismatch of arrival times even with the same\nphysical wire length. In this paper, we propose a post-processing method to\nenlarge the width and the distance of meander segments and distribute them more\nevenly on the board so that the crosstalks can be reduced. In the proposed\nframework, we model the sharing combinations of available routing areas after\nremoving dense meander segments from the initial routing, as well as the\ngeneration of relaxed meander segments and their groups in subareas.\nThereafter, this model is transformed into an ILP problem and solved\nefficiently. Experimental results show that the proposed method can extend the\nwidth and the distance of meander segments about two times even under very\ntight area constraints, so that the crosstalks and thus the speedup effect can\nbe alleviated effectively in high-performance PCB designs.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:56:30 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Tseng", "Tsun-Ming", ""], ["Li", "Bing", ""], ["Ho", "Tsung-Yi", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04984", "submitter": "Bing Li", "authors": "Tsun-Ming Tseng, Bing Li, Tsung-Yi Ho and Ulf Schlichtmann", "title": "ILP-based Alleviation of Dense Meander Segments with Prioritized\n  Shifting and Progressive Fixing in PCB Routing", "comments": null, "journal-ref": "IEEE Transactions on Computer-Aided Design of Integrated Circuits\n  and Systems 34(6), 1000-1013, June 2015", "doi": "10.1109/TCAD.2015.2402657", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Length-matching is an important technique to bal- ance delays of bus signals\nin high-performance PCB routing. Existing routers, however, may generate very\ndense meander segments. Signals propagating along these meander segments\nexhibit a speedup effect due to crosstalk between the segments of the same\nwire, thus leading to mismatch of arrival times even under the same physical\nwire length. In this paper, we present a post-processing method to enlarge the\nwidth and the distance of meander segments and hence distribute them more\nevenly on the board so that crosstalk can be reduced. In the proposed\nframework, we model the sharing of available routing areas after removing dense\nmeander segments from the initial routing, as well as the generation of relaxed\nmeander segments and their groups for wire length compensation. This model is\ntransformed into an ILP problem and solved for a balanced distribution of wire\npatterns. In addition, we adjust the locations of long wire segments according\nto wire priorities to swap free spaces toward critical wires that need much\nlength compensation. To reduce the problem space of the ILP model, we also\nintroduce a progressive fixing technique so that wire patterns are grown\ngradually from the edge of the routing toward the center area. Experimental\nresults show that the proposed method can expand meander segments significantly\neven under very tight area constraints, so that the speedup effect can be\nalleviated effectively in high- performance PCB designs.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:59:45 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Tseng", "Tsun-Ming", ""], ["Li", "Bing", ""], ["Ho", "Tsung-Yi", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04986", "submitter": "Bing Li", "authors": "Bing Li and Ulf Schlichtmann", "title": "Statistical Timing Analysis and Criticality Computation for Circuits\n  with Post-Silicon Clock Tuning Elements", "comments": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and\n  Systems, November 2015", "journal-ref": null, "doi": "10.1109/TCAD.2015.2432143", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-silicon clock tuning elements are widely used in high-performance\ndesigns to mitigate the effects of process variations and aging. Located on\nclock paths to flip-flops, these tuning elements can be configured through the\nscan chain so that clock skews to these flip-flops can be adjusted after man-\nufacturing. Owing to the delay compensation across consecutive register stages\nenabled by the clock tuning elements, higher yield and enhanced robustness can\nbe achieved. These benefits are, nonetheless, attained by increasing die area\ndue to the inserted clock tuning elements. For balancing performance\nimprovement and area cost, an efficient timing analysis algorithm is needed to\nevaluate the performance of such a circuit. So far this evaluation is only\npossible by Monte Carlo simulation which is very timing- consuming. In this\npaper, we propose an alternative method using graph transformation, which\ncomputes a parametric minimum clock period and is more than 10 4 times faster\nthan Monte Carlo simulation while maintaining a good accuracy. This method also\nidentifies the gates that are critical to circuit performance, so that a fast\nanalysis-optimization flow becomes possible.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 16:03:30 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Bing", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04990", "submitter": "Bing Li", "authors": "Grace Li Zhang, Bing Li and Ulf Schlichtmann", "title": "Sampling-based Buffer Insertion for Post-Silicon Yield Improvement under\n  Process Variability", "comments": "Design, Automation and Test in Europe (DATE), 2016", "journal-ref": null, "doi": "10.3850/9783981537079_0250", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At submicron manufacturing technology nodes process variations affect circuit\nperformance significantly. This trend leads to a large timing margin and thus\noverdesign to maintain yield. To combat this pessimism, post-silicon clock\ntuning buffers can be inserted into circuits to balance timing budgets of\ncritical paths with their neighbors. After manufacturing, these clock buffers\ncan be configured for each chip individually so that chips with timing failures\nmay be rescued to improve yield. In this paper, we propose a sampling-based\nmethod to determine the proper locations of these buffers. The goal of this\nbuffer insertion is to reduce the number of buffers and their ranges, while\nstill maintaining a good yield improvement. Experimental results demonstrate\nthat our algorithm can achieve a significant yield improvement (up to 35%) with\nonly a small number of buffers.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 16:09:10 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Grace Li", ""], ["Li", "Bing", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04992", "submitter": "Bing Li", "authors": "Grace Li Zhang, Bing Li and Ulf Schlichtmann", "title": "EffiTest: Efficient Delay Test and Statistical Prediction for\n  Configuring Post-silicon Tunable Buffers", "comments": "ACM/IEEE Design Automation Conference (DAC), June 2016", "journal-ref": null, "doi": "10.1145/2897937.2898017", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At nanometer manufacturing technology nodes, process variations significantly\naffect circuit performance. To combat them, post- silicon clock tuning buffers\ncan be deployed to balance timing bud- gets of critical paths for each\nindividual chip after manufacturing. The challenge of this method is that path\ndelays should be mea- sured for each chip to configure the tuning buffers\nproperly. Current methods for this delay measurement rely on path-wise\nfrequency stepping. This strategy, however, requires too much time from ex-\npensive testers. In this paper, we propose an efficient delay test framework\n(EffiTest) to solve the post-silicon testing problem by aligning path delays\nusing the already-existing tuning buffers in the circuit. In addition, we only\ntest representative paths and the delays of other paths are estimated by\nstatistical delay prediction. Exper- imental results demonstrate that the\nproposed method can reduce the number of frequency stepping iterations by more\nthan 94% with only a slight yield loss.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 16:17:58 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Grace Li", ""], ["Li", "Bing", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04993", "submitter": "Bing Li", "authors": "Grace Li Zhang, Bing Li, Ulf Schlichtmann", "title": "PieceTimer: A Holistic Timing Analysis Framework Considering Setup/Hold\n  Time Interdependency Using A Piecewise Model", "comments": "IEEE/ACM International Conference on Computer-Aided Design (ICCAD),\n  November 2016", "journal-ref": null, "doi": "10.1145/2966986.2967064", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In static timing analysis, clock-to-q delays of flip-flops are considered as\nconstants. Setup times and hold times are characterized separately and also\nused as constants. The characterized delays, setup times and hold times, are\nap- plied in timing analysis independently to verify the perfor- mance of\ncircuits. In reality, however, clock-to-q delays of flip-flops depend on both\nsetup and hold times. Instead of being constants, these delays change with\nrespect to different setup/hold time combinations. Consequently, the simple ab-\nstraction of setup/hold times and constant clock-to-q delays introduces\ninaccuracy in timing analysis. In this paper, we propose a holistic method to\nconsider the relation between clock-to-q delays and setup/hold time\ncombinations with a piecewise linear model. The result is more accurate than\nthat of traditional timing analysis, and the incorporation of the\ninterdependency between clock-to-q delays, setup times and hold times may also\nimprove circuit performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 16:21:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Grace Li", ""], ["Li", "Bing", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.04995", "submitter": "Bing Li", "authors": "Li Zhang, Bing Li, Jinglan Liu, Yiyu Shi, Ulf Schlichtmann", "title": "Design-Phase Buffer Allocation for Post-Silicon Clock Binning by\n  Iterative Learning", "comments": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and\n  Systems, 2017", "journal-ref": null, "doi": "10.1109/TCAD.2017.2702632", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At submicron manufacturing technology nodes, pro- cess variations affect\ncircuit performance significantly. To counter these variations, engineers are\nreserving more timing margin to maintain yield, leading to an unaffordable\noverdesign. Most of these margins, however, are wasted after manufacturing,\nbecause process variations cause only some chips to be really slow, while other\nchips can easily meet given timing specifications. To reduce this pessimism, we\ncan reserve less timing margin and tune failed chips after manufacturing with\nclock buffers to make them meet timing specifications. With this post-silicon\nclock tuning, critical paths can be balanced with neighboring paths in each\nchip specifically to counter the effect of process variations. Consequently,\nchips with timing failures can be rescued and the yield can thus be improved.\nThis is specially useful in high- performance designs, e.g., high-end CPUs,\nwhere clock binning makes chips with higher performance much more profitable.\nIn this paper, we propose a method to determine where to insert post-silicon\ntuning buffers during the design phase to improve the overall profit with clock\nbinning. This method learns the buffer locations with a Sobol sequence\niteratively and reduces the buffer ranges afterwards with tuning concentration\nand buffer grouping. Experimental results demonstrate that the proposed method\ncan achieve a profit improvement of about 14% on average and up to 26%, with\nonly a small number of tuning buffers inserted into the circuit.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 16:26:08 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Li", ""], ["Li", "Bing", ""], ["Liu", "Jinglan", ""], ["Shi", "Yiyu", ""], ["Schlichtmann", "Ulf", ""]]}, {"id": "1705.06419", "submitter": "Myoungsoo Jung", "authors": "Myoungsoo Jung, Jie Zhang, Ahmed Abulila, Miryeong Kwon, Narges\n  Shahidi, John Shalf, Nam Sung Kim and Mahmut Kandemir", "title": "SimpleSSD: Modeling Solid State Drives for Holistic System Simulation", "comments": "This paper has been accepted at IEEE Computer Architecture Letters\n  (CAL)", "journal-ref": null, "doi": "10.1109/LCA.2017.2750658", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing solid state drive (SSD) simulators unfortunately lack hardware\nand/or software architecture models. Consequently, they are far from capturing\nthe critical features of contemporary SSD devices. More importantly, while the\nperformance of modern systems that adopt SSDs can vary based on their numerous\ninternal design parameters and storage-level configurations, a full system\nsimulation with traditional SSD models often requires unreasonably long\nruntimes and excessive computational resources. In this work, we propose\nSimpleSSD, a highfidelity simulator that models all detailed characteristics of\nhardware and software, while simplifying the nondescript features of storage\ninternals. In contrast to existing SSD simulators, SimpleSSD can easily be\nintegrated into publicly-available full system simulators. In addition, it can\naccommodate a complete storage stack and evaluate the performance of SSDs along\nwith diverse memory technologies and microarchitectures. Thus, it facilitates\nsimulations that explore the full design space at different levels of system\nabstraction.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 05:08:34 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 14:28:11 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Jung", "Myoungsoo", ""], ["Zhang", "Jie", ""], ["Abulila", "Ahmed", ""], ["Kwon", "Miryeong", ""], ["Shahidi", "Narges", ""], ["Shalf", "John", ""], ["Kim", "Nam Sung", ""], ["Kandemir", "Mahmut", ""]]}, {"id": "1705.06923", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits, Amir Morad, Uri Weiser, Ran Ginosar", "title": "MultiAmdahl: Optimal Resource Allocation in Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future multiprocessor chips will integrate many different units, each\ntailored to a specific computation. When designing such a system, the chip\narchitect must decide how to distribute limited system resources such as area,\npower, and energy among the computational units. We extend MultiAmdahl, an\nanalytical optimization technique for resource allocation in heterogeneous\narchitectures, for energy optimality under a variety of constant system power\nscenarios. We conclude that reduction in constant system power should be met by\nreallocating resources from general-purpose computing to heterogeneous\naccelerator-dominated computing, to keep the overall energy consumption at a\nminimum. We extend this conclusion to offer an intuition regarding\nenergy-optimal resource allocation in data center computing.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:36:49 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Weiser", "Uri", ""], ["Ginosar", "Ran", ""]]}, {"id": "1705.07280", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits, Amir Morad, Ran Ginosar", "title": "The Effect of Temperature on Amdahl Law in 3D Multicore Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the influence of temperature on performance and scalability\nof 3D Chip Multiprocessors (CMP) from Amdahl law perspective. We find that 3D\nCMP may reach its thermal limit before reaching its maximum power. We show that\na high level of parallelism may lead to high peak temperatures even in small\nscale 3D CMPs, thus limiting 3D CMP scalability and calling for different,\nin-memory computing architectures.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:51:40 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Ginosar", "Ran", ""]]}, {"id": "1705.07281", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits, Amir Morad, Ran Ginosar", "title": "Cache Hierarchy Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power consumption, off-chip memory bandwidth, chip area and Network on Chip\n(NoC) capacity are among main chip resources limiting the scalability of Chip\nMultiprocessors (CMP). A closed form analytical solution for optimizing the CMP\ncache hierarchy and optimally allocating area among hierarchy levels under such\nconstrained resources is developed. The optimization framework is extended by\nincorporating the impact of data sharing on cache miss rate. An analytical\nmodel for cache access time as a function of cache size is proposed and\nverified using CACTI simulation.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 08:54:52 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Ginosar", "Ran", ""]]}, {"id": "1705.07465", "submitter": "Aleksandr Cariow", "authors": "Aleksandr Cariow and Galina Cariowa", "title": "Some Schemes for Implementation of Arithmetic Operations with Complex\n  Numbers Using Squaring Units", "comments": "3 pages. 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, new schemes for a squarer, multiplier and divider of complex\nnumbers are proposed. Traditional structural solutions for each of these\noperations require the presence some number of general-purpose binary\nmultipliers. The advantage of our solutions is a removing of multiplications\nthrough replacing them by less costly squarers. We use Logan's trick and\nquarter square technique, which propose to replace the calculation of the\nproduct of two real numbers by summing the squares. Replacing usual multipliers\non digital squares implies reducing power consumption as well as decreases\nhardware circuit complexity. The squarer requiring less area and power as\ncompared to general-purpose multiplier, it is interesting to assess the use of\nsquarers to implementation of complex arithmetic.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:56:04 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Cariow", "Aleksandr", ""], ["Cariowa", "Galina", ""]]}, {"id": "1705.08009", "submitter": "Yuxiang Huan", "authors": "Yuxiang Huan, Yifan Qin, Yantian You, Lirong Zheng, and Zhuo Zou", "title": "A Low-Power Accelerator for Deep Neural Networks with Enlarged Near-Zero\n  Sparsity", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains a challenge to run Deep Learning in devices with stringent power\nbudget in the Internet-of-Things. This paper presents a low-power accelerator\nfor processing Deep Neural Networks in the embedded devices. The power\nreduction is realized by avoiding multiplications of near-zero valued data. The\nnear-zero approximation and a dedicated Near-Zero Approximation Unit (NZAU) are\nproposed to predict and skip the near-zero multiplications under certain\nthresholds. Compared with skipping zero-valued computations, our design\nachieves 1.92X and 1.51X further reduction of the total multiplications in\nLeNet-5 and Alexnet respectively, with negligible lose of accuracy. In the\nproposed accelerator, 256 multipliers are grouped into 16 independent\nProcessing Lanes (PL) to support up to 16 neuron activations simultaneously.\nWith the help of data pre-processing and buffering in each PL, multipliers can\nbe clock-gated in most of the time even the data is excessively streaming in.\nDesigned and simulated in UMC 65 nm process, the accelerator operating at 500\nMHz is $>$ 4X faster than the mobile GPU Tegra K1 in processing the\nfully-connected layer FC8 of Alexnet, while consuming 717X less energy.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:28:50 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Huan", "Yuxiang", ""], ["Qin", "Yifan", ""], ["You", "Yantian", ""], ["Zheng", "Lirong", ""], ["Zou", "Zhuo", ""]]}, {"id": "1705.09937", "submitter": "Leonid Yavits PhD", "authors": "Leonid Yavits and Ran Ginosar", "title": "Sparse Matrix Multiplication on CAM Based Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix multiplication is an important component of linear algebra\ncomputations. In this paper, an architecture based on Content Addressable\nMemory (CAM) and Resistive Content Addressable Memory (ReCAM) is proposed for\naccelerating sparse matrix by sparse vector and matrix multiplication in CSR\nformat. Using functional simulation, we show that the proposed ReCAM-based\naccelerator exhibits two orders of magnitude higher power efficiency as\ncompared to existing sparse matrix-vector multiplication implementations.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 13:00:11 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yavits", "Leonid", ""], ["Ginosar", "Ran", ""]]}, {"id": "1705.10292", "submitter": "Kevin Chang", "authors": "Kevin K. Chang, Abdullah Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}, Saugata Ghose,\n  Aditya Agrawal, Niladrish Chatterjee, Abhijith Kashyap, Donghyuk Lee, Mike\n  O'Connor, Hasan Hassan, Onur Mutlu", "title": "Understanding Reduced-Voltage Operation in Modern DRAM Chips:\n  Characterization, Analysis, and Mechanisms", "comments": "25 pages, 25 figures, 7 tables, Proceedings of the ACM on Measurement\n  and Analysis of Computing Systems (POMACS)", "journal-ref": null, "doi": "10.1145/3084447", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy consumption of DRAM is a critical concern in modern computing\nsystems. Improvements in manufacturing process technology have allowed DRAM\nvendors to lower the DRAM supply voltage conservatively, which reduces some of\nthe DRAM energy consumption. We would like to reduce the DRAM supply voltage\nmore aggressively, to further reduce energy. Aggressive supply voltage\nreduction requires a thorough understanding of the effect voltage scaling has\non DRAM access latency and DRAM reliability.\n  In this paper, we take a comprehensive approach to understanding and\nexploiting the latency and reliability characteristics of modern DRAM when the\nsupply voltage is lowered below the nominal voltage level specified by DRAM\nstandards. Using an FPGA-based testing platform, we perform an experimental\nstudy of 124 real DDR3L (low-voltage) DRAM chips manufactured recently by three\nmajor DRAM vendors. We find that reducing the supply voltage below a certain\npoint introduces bit errors in the data, and we comprehensively characterize\nthe behavior of these errors. We discover that these errors can be avoided by\nincreasing the latency of three major DRAM operations (activation, restoration,\nand precharge). We perform detailed DRAM circuit simulations to validate and\nexplain our experimental findings. We also characterize the various\nrelationships between reduced supply voltage and error locations, stored data\npatterns, DRAM temperature, and data retention.\n  Based on our observations, we propose a new DRAM energy reduction mechanism,\ncalled Voltron. The key idea of Voltron is to use a performance model to\ndetermine by how much we can reduce the supply voltage without introducing\nerrors and without exceeding a user-specified threshold for performance loss.\nVoltron reduces the average system energy by 7.3% while limiting the average\nsystem performance loss to only 1.8%, for a variety of workloads.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:12:33 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Chang", "Kevin K.", ""], ["Ya\u011fl\u0131k\u00e7\u0131", "Abdullah Giray", ""], ["Ghose", "Saugata", ""], ["Agrawal", "Aditya", ""], ["Chatterjee", "Niladrish", ""], ["Kashyap", "Abhijith", ""], ["Lee", "Donghyuk", ""], ["O'Connor", "Mike", ""], ["Hassan", "Hasan", ""], ["Mutlu", "Onur", ""]]}]