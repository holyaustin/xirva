[{"id": "2011.00448", "submitter": "Hyokeun Lee", "authors": "Hyokeun Lee, Seungyong Lee, Byeongki Song, Moonsoo Kim, Seokbo Shim,\n  Hyun Kim, Hyuk-Jae Lee", "title": "Mitigating Write Disturbance Errors of Phase-Change Memory as In-Module\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing demand for technology scaling and storage capacity in server\nsystems to support high-performance computing, phase-change memory (PCM) has\ngarnered attention as the next-generation non-volatile memory to satisfy these\nrequirements. However, write disturbance error (WDE) appears as a serious\nreliability problem preventing PCM from general commercialization. WDE occurs\non the neighboring cells of a written cell due to heat dissipation. Previous\nstudies for the prevention of WDEs are based on the write cache or\nverify-n-correction while they often suffer from significant area overhead and\nperformance degradation, making it unsuitable for high-performance computing.\nTherefore, an on-demand correction is required to minimize the performance\noverhead. In this paper, an in-module disturbance barrier (IMDB) mitigating\nWDEs is proposed. IMDB includes two sets of SRAMs into two levels and evicts\nentries with a policy that leverages the characteristics of WDE. In this work,\nthe comparator dedicated to the replacement policy requires significant\nhardware resources and latency. Thus, an approximate comparator is designed to\nreduce the area and latency considerably. Furthermore, the exploration of\narchitecture parameters is conducted to obtain cost-effective design. The\nproposed work significantly reduces WDEs without a noticeable speed degradation\nand additional energy consumption compared to previous methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 08:32:37 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 06:33:36 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Lee", "Hyokeun", ""], ["Lee", "Seungyong", ""], ["Song", "Byeongki", ""], ["Kim", "Moonsoo", ""], ["Shim", "Seokbo", ""], ["Kim", "Hyun", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "2011.00624", "submitter": "Joshua Mack", "authors": "Joshua Mack, Ruben Purdy, Kris Rockowitz, Michael Inouye, Edward\n  Richter, Spencer Valancius, Nirmal Kumbhare, Md Sahil Hassan, Kaitlin Fair,\n  John Mixter, Ali Akoglu", "title": "RANC: Reconfigurable Architecture for Neuromorphic Computing", "comments": "18 pages, 12 figures, accepted for publication in IEEE Transactions\n  on Computer-Aided Design of Integrated Circuits and Systems. For associated\n  source files see https://github.com/UA-RCL/RANC", "journal-ref": null, "doi": "10.1109/TCAD.2020.3038151", "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic architectures have been introduced as platforms for energy\nefficient spiking neural network execution. The massive parallelism offered by\nthese architectures has also triggered interest from non-machine learning\napplication domains. In order to lift the barriers to entry for hardware\ndesigners and application developers we present RANC: a Reconfigurable\nArchitecture for Neuromorphic Computing, an open-source highly flexible\necosystem that enables rapid experimentation with neuromorphic architectures in\nboth software via C++ simulation and hardware via FPGA emulation. We present\nthe utility of the RANC ecosystem by showing its ability to recreate behavior\nof the IBM's TrueNorth and validate with direct comparison to IBM's Compass\nsimulation environment and published literature. RANC allows optimizing\narchitectures based on application insights as well as prototyping future\nneuromorphic architectures that can support new classes of applications\nentirely. We demonstrate the highly parameterized and configurable nature of\nRANC by studying the impact of architectural changes on improving application\nmapping efficiency with quantitative analysis based on Alveo U250 FPGA. We\npresent post routing resource usage and throughput analysis across\nimplementations of Synthetic Aperture Radar classification and Vector Matrix\nMultiplication applications, and demonstrate a neuromorphic architecture that\nscales to emulating 259K distinct neurons and 73.3M distinct synapses.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 20:29:52 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mack", "Joshua", ""], ["Purdy", "Ruben", ""], ["Rockowitz", "Kris", ""], ["Inouye", "Michael", ""], ["Richter", "Edward", ""], ["Valancius", "Spencer", ""], ["Kumbhare", "Nirmal", ""], ["Hassan", "Md Sahil", ""], ["Fair", "Kaitlin", ""], ["Mixter", "John", ""], ["Akoglu", "Ali", ""]]}, {"id": "2011.00648", "submitter": "Sina Sayyah Ensan", "authors": "Sina Sayyah Ensan, Swaroop Ghosh, Seyedhamidreza Motaman, and Derek\n  Weast", "title": "Addressing Resiliency of In-Memory Floating Point Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-memory computing (IMC) can eliminate the data movement between processor\nand memory which is a barrier to the energy-efficiency and performance in\nVon-Neumann computing. Resistive RAM (RRAM) is one of the promising devices for\nIMC applications (e.g. integer and Floating Point (FP) operations and random\nlogic implementation) due to low power consumption, fast operation, and small\nfootprint in crossbar architecture. In this paper, we propose FAME, a pipelined\nFP arithmetic (adder/subtractor) using RRAM crossbar based IMC. A novel shift\ncircuitry is proposed to lower the shift overhead during FP operations. Since\n96% of the RRAMs used in our architecture are in High Resistance State (HRS),\nwe propose two approaches namely Shift-At-The-Output (SATO) and Force To VDD\n(FTV) (ground (FTG)) to mitigate Stuck-at-1 (SA1) failures. In both techniques,\nthe fault-free RRAMs are exploited to perform the computation by using an extra\nclock cycle. Although performance degrades by 50%, SATO can handle 50% of the\nfaults whereas FTV can handle 99% of the faults in the RRAM-based compute array\nat low power and area overhead. Simulation results show that the proposed\nsingle precision FP adder consumes 335 pJ and 322 pJ for NAND-NAND and NOR-NOR\nbased implementations, respectively. The area overheads of SATO and FTV are\n28.5% and 9.5%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:44:04 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ensan", "Sina Sayyah", ""], ["Ghosh", "Swaroop", ""], ["Motaman", "Seyedhamidreza", ""], ["Weast", "Derek", ""]]}, {"id": "2011.00850", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "On the Impact of Partial Sums on Interconnect Bandwidth and Memory\n  Accesses in a DNN Accelerator", "comments": null, "journal-ref": "2020 IEEE 15th International Conference on Industrial and\n  Information Systems (ICIIS)", "doi": "10.1109/ICIIS51140.2020.9342717", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dedicated accelerators are being designed to address the huge resource\nrequirement of the deep neural network (DNN) applications. The power,\nperformance and area (PPA) constraints limit the number of MACs available in\nthese accelerators. The convolution layers which require huge number of MACs\nare often partitioned into multiple iterative sub-tasks. This puts huge\npressure on the available system resources such as interconnect and memory\nbandwidth. The optimal partitioning of the feature maps for these sub-tasks can\nreduce the bandwidth requirement substantially. Some accelerators avoid\noff-chip or interconnect transfers by implementing local memories; however, the\nmemory accesses are still performed and a reduced bandwidth can help in saving\npower in such architectures. In this paper, we propose a first order analytical\nmethod to partition the feature maps for optimal bandwidth and evaluate the\nimpact of such partitioning on the bandwidth. This bandwidth can be saved by\ndesigning an active memory controller which can perform basic arithmetic\noperations. It is shown that the optimal partitioning and active memory\ncontroller can achieve up to 40% bandwidth reduction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:44:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2011.01148", "submitter": "Zahra Ebrahimi", "authors": "Zahra Ebrahimi and Salim Ullah and Akash Kumar", "title": "SIMDive: Approximate SIMD Soft Multiplier-Divider for FPGAs with Tunable\n  Accuracy", "comments": null, "journal-ref": "ACM Great Lakes Symposium on VLSI (GLSVLSI) 2020", "doi": "10.1145/3386263.3406907", "report-no": null, "categories": "cs.AR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing quest for data-level parallelism and variable precision\nin ubiquitous multimedia and Deep Neural Network (DNN) applications has\nmotivated the use of Single Instruction, Multiple Data (SIMD) architectures. To\nalleviate energy as their main resource constraint, approximate computing has\nre-emerged,albeit mainly specialized for their Application-Specific Integrated\nCircuit (ASIC) implementations. This paper, presents for the first time, an\nSIMD architecture based on novel multiplier and divider with tunable accuracy,\ntargeted for Field-Programmable Gate Arrays (FPGAs). The proposed hybrid\narchitecture implements Mitchell's algorithms and supports precision\nvariability from 8 to 32 bits. Experimental results obtained from Vivado,\nmultimedia and DNN applications indicate superiority of proposed architecture\n(both SISD and SIMD) over accurate and state-of-the-art approximate\ncounterparts. In particular, the proposed SISD divider outperforms the accurate\nIntellectual Property (IP) divider provided by Xilinx with 4x higher speed and\n4.6x less energy and tolerating only < 0.8% error. Moreover, the proposed SIMD\nmultiplier-divider supersede accurate SIMD multiplier by achieving up to 26%,\n45%, 36%, and 56% improvement in area, throughput, power, and energy,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 17:40:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ebrahimi", "Zahra", ""], ["Ullah", "Salim", ""], ["Kumar", "Akash", ""]]}, {"id": "2011.01713", "submitter": "Moritz Scherer", "authors": "Moritz Scherer, Georg Rutishauser, Lukas Cavigelli, Luca Benini", "title": "CUTIE: Beyond PetaOp/s/W Ternary DNN Inference Acceleration with\n  Better-than-Binary Energy Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3.1 POp/s/W fully digital hardware accelerator for ternary\nneural networks. CUTIE, the Completely Unrolled Ternary Inference Engine,\nfocuses on minimizing non-computational energy and switching activity so that\ndynamic power spent on storing (locally or globally) intermediate results is\nminimized. This is achieved by 1) a data path architecture completely unrolled\nin the feature map and filter dimensions to reduce switching activity by\nfavoring silencing over iterative computation and maximizing data re-use, 2)\ntargeting ternary neural networks which, in contrast to binary NNs, allow for\nsparse weights which reduce switching activity, and 3) introducing an optimized\ntraining method for higher sparsity of the filter weights, resulting in a\nfurther reduction of the switching activity. Compared with state-of-the-art\naccelerators, CUTIE achieves greater or equal accuracy while decreasing the\noverall core inference energy cost by a factor of 4.8x-21x.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:00:55 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 09:30:51 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Scherer", "Moritz", ""], ["Rutishauser", "Georg", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "2011.02022", "submitter": "T. N. Vijaykumar", "authors": "Mingxuan He, T. N. Vijaykumar, and Mithuna Thottethodi", "title": "Booster: An Accelerator for Gradient Boosting Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Booster, a novel accelerator for gradient boosting trees based on\nthe unique characteristics of gradient boosting models. We observe that the\ndominant steps of gradient boosting training (accounting for 90-98% of training\ntime) involve simple, fine-grained, independent operations on small-footprint\ndata structures (e.g., accumulate and compare values in the structures).\nUnfortunately, existing multicores and GPUs are unable to harness this\nparallelism because they do not support massively-parallel data structure\naccesses that are irregular and data-dependent. By employing a scalable\nsea-of-small-SRAMs approach and an SRAM bandwidth-preserving mapping of data\nrecord fields to the SRAMs, Booster achieves significantly more parallelism\n(e.g., 3200-way parallelism) than multicores and GPU. In addition, Booster\nemploys a redundant data representation that significantly lowers the memory\nbandwidth demand. Our simulations reveal that Booster achieves 11.4x speedup\nand 6.4x speedup over an ideal 32-core multicore and an ideal GPU,\nrespectively. Based on ASIC synthesis of FPGA-validated RTL using 45 nm\ntechnology, we estimate a Booster chip to occupy 60 mm^2 of area and dissipate\n23 W when operating at 1-GHz clock speed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 21:59:48 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 18:23:12 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["He", "Mingxuan", ""], ["Vijaykumar", "T. N.", ""], ["Thottethodi", "Mithuna", ""]]}, {"id": "2011.02368", "submitter": "Nilanjan Goswami", "authors": "Nilanjan Goswami, Amer Qouneh, Chao Li, Tao Li", "title": "An Empirical-cum-Statistical Approach to Power-Performance\n  Characterization of Concurrent GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing deployment of power and energy efficient throughput accelerators\n(GPU) in data centers demands enhancement of power-performance co-optimization\ncapabilities of GPUs. Realization of exascale computing using accelerators\nrequires further improvements in power efficiency. With hardwired kernel\nconcurrency enablement in accelerators, inter- and intra-workload simultaneous\nkernels computation predicts increased throughput at lower energy budget. To\nimprove Performance-per-Watt metric of the architectures, a systematic\nempirical study of real-world throughput workloads (with concurrent kernel\nexecution) is required. To this end, we propose a multi-kernel throughput\nworkload generation framework that will facilitate aggressive energy and\nperformance management of exascale data centers and will stimulate synergistic\npower-performance co-optimization of throughput architectures. Also, we\ndemonstrate a multi-kernel throughput benchmark suite based on the framework\nthat encapsulates symmetric, asymmetric and co-existing (often appears\ntogether) kernel based workloads. On average, our analysis reveals that spatial\nand temporal concurrency within kernel execution in throughput architectures\nsaves energy consumption by 32%, 26% and 33% in GTX470, Tesla M2050 and Tesla\nK20 across 12 benchmarks. Concurrency and enhanced utilization are often\ncorrelated but do not imply significant deviation in power dissipation.\nDiversity analysis of proposed multi-kernels confirms characteristic variation\nand power-profile diversity within the suite. Besides, we explain several\nfindings regarding power-performance co-optimization of concurrent throughput\nworkloads.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:58:54 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 03:25:55 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Goswami", "Nilanjan", ""], ["Qouneh", "Amer", ""], ["Li", "Chao", ""], ["Li", "Tao", ""]]}, {"id": "2011.02556", "submitter": "Saeed Kargar", "authors": "Saeed Kargar, Heiner Litz, Faisal Nawab", "title": "Predict and Write: Using K-Means Clustering to Extend the Lifetime of\n  NVM Storage", "comments": "ICDE2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies suffer from limited write endurance.\nTo address this challenge, we propose Predict and Write (PNW), a K/V-store that\nuses a clustering-based machine learning approach to extend the lifetime of\nNVMs. PNW decreases the number of bit flips for PUT/UPDATE operations by\ndetermining the best memory location an updated value should be written to. PNW\nleverages the indirection level of K/V-stores to freely choose the target\nmemory location for any given write based on its value. PNW organizes NVM\naddresses in a dynamic address pool clustered by the similarity of the data\nvalues they refer to. We show that, by choosing the right target memory\nlocation for a given PUT/UPDATE operation, the number of total bit flips and\ncache lines can be reduced by up to 85% and 56% over the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 22:03:09 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Kargar", "Saeed", ""], ["Litz", "Heiner", ""], ["Nawab", "Faisal", ""]]}, {"id": "2011.02839", "submitter": "Udit Gupta", "authors": "Udit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse, Hsien-Hsin S. Lee,\n  Gu-Yeon Wei, David Brooks, Carole-Jean Wu", "title": "Chasing Carbon: The Elusive Environmental Footprint of Computing", "comments": "To appear in IEEE International Symposium on High-Performance\n  Computer Architecture (HPCA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given recent algorithm, software, and hardware innovation, computing has\nenabled a plethora of new applications. As computing becomes increasingly\nubiquitous, however, so does its environmental impact. This paper brings the\nissue to the attention of computer-systems researchers. Our analysis, built on\nindustry-reported characterization, quantifies the environmental effects of\ncomputing in terms of carbon emissions. Broadly, carbon emissions have two\nsources: operational energy consumption, and hardware manufacturing and\ninfrastructure. Although carbon emissions from the former are decreasing thanks\nto algorithmic, software, and hardware innovations that boost performance and\npower efficiency, the overall carbon footprint of computer systems continues to\ngrow. This work quantifies the carbon output of computer systems to show that\nmost emissions related to modern mobile and data-center equipment come from\nhardware manufacturing and infrastructure. We therefore outline future\ndirections for minimizing the environmental impact of computing systems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:15:22 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Gupta", "Udit", ""], ["Kim", "Young Geun", ""], ["Lee", "Sylvia", ""], ["Tse", "Jordan", ""], ["Lee", "Hsien-Hsin S.", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2011.03190", "submitter": "Linghao Song", "authors": "Linghao Song, Fan Chen, Xuehai Qian, Hai Li, Yiran Chen", "title": "Low-Cost Floating-Point Processing in ReRAM for Scientific Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ReFloat, a data format and an accelerator architecture, for\nlow-cost and high-performance floating-point processing in ReRAM for scientific\ncomputing. ReFloat reduces bit number for floating-point representation and\nprocessing to achieve a smaller number ofReRAM crossbars and processing cycles\nfor floating-point matrix-vector multiplication. In the ReFloat data format,\nfor scalars of a matrix block, an exponent offset is derived from an optimized\nbase, and then an exponent offset is re-served for each scalar and fraction bit\nnumbers are reduced. The exponent base optimization is enabled by taking\nadvantage of the existence of value locality in real-world matrices. After\ndefining the ReFloat data format, we develop the conversion scheme from default\ndouble-precision floating-point format to ReFloat format, the computation\nprocedure, and the low-cost high-performance floating-point processing\narchitecture in ReRAM. With ReFloat, we find that for all 12 matrices only 3\nbits for matrix exponent, matrix fraction and vector exponent, and 8 or 16 bits\nfor vector fraction are sufficient to ensure convergence on solvers CG and\nBiCGSTAB. It translates to an average speedup of20.10x/24.59x onCG / BiCGSTAB\ncompared with a GPU baseline and an average speedup of18.86x/54.00x on CG /\nBiCGSTAB compared with a state-of-the-art ReRAM-based accelerator for\nscientific computing.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:59:25 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 19:35:25 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 22:16:28 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 00:33:28 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Song", "Linghao", ""], ["Chen", "Fan", ""], ["Qian", "Xuehai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.03651", "submitter": "Pedro Machado", "authors": "Samuel Brandenburg, Pedro Machado, Nikesh Lama, T.M. McGinnity", "title": "Strawberry Detection Using a Heterogeneous Multi-Processor Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the number of precision farming projects has\nincreased specifically in harvesting robots and many of which have made\ncontinued progress from identifying crops to grasping the desired fruit or\nvegetable. One of the most common issues found in precision farming projects is\nthat successful application is heavily dependent not just on identifying the\nfruit but also on ensuring that localisation allows for accurate navigation.\nThese issues become significant factors when the robot is not operating in a\nprearranged environment, or when vegetation becomes too thick, thus covering\ncrop. Moreover, running a state-of-the-art deep learning algorithm on an\nembedded platform is also very challenging, resulting most of the times in low\nframe rates. This paper proposes using the You Only Look Once version 3\n(YOLOv3) Convolutional Neural Network (CNN) in combination with utilising image\nprocessing techniques for the application of precision farming robots targeting\nstrawberry detection, accelerated on a heterogeneous multiprocessor platform.\nThe results show a performance acceleration by five times when implemented on a\nField-Programmable Gate Array (FPGA) when compared with the same algorithm\nrunning on the processor side with an accuracy of 78.3\\% over the test set\ncomprised of 146 images.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 01:08:21 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Brandenburg", "Samuel", ""], ["Machado", "Pedro", ""], ["Lama", "Nikesh", ""], ["McGinnity", "T. M.", ""]]}, {"id": "2011.03669", "submitter": "Gang Liu", "authors": "Gang Liu, Kenli Li, Zheng Xiao and Rujia Wang", "title": "EHAP-ORAM: Efficient Hardware-Assisted Persistent ORAM System for\n  Non-volatile Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oblivious RAM (ORAM) protected access pattern is essential for secure NVM. In\nthe ORAM system, data and PosMap metadata are maps in pairs to perform secure\naccess. Therefore, we focus on the problem of crash consistency in the ORAM\nsystem. Unfortunately, using traditional software-based support for ORAM system\ncrash consistency is not only expensive, it can also lead to information leaks.\nAt present, there is no relevant research on the specific crash consistency\nmechanism supporting the ORAM system. To support crash consistency without\ndamaging ORAM system security and compromising the performance, we propose\nEHAP-ORAM. Firstly, we analyze the access steps of basic ORAM to obtain the\nbasic requirements to support the ORAM system crash consistency. Secondly,\nimprove the ORAM controller. Thirdly, for the improved hardware system, we\npropose several persistence protocols supporting the ORAM system crash\nconsistency. Finally, we compared our persistent ORAM with the system without\ncrash consistency support, non-recursive and recursive EHAP-ORAM only incurs\n3.36% and 3.65% performance overhead. The results show that EHAP-ORAM not only\nsupports effective crash consistency with minimal performance and hardware\noverhead but also is friendly to NVM lifetime.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 03:15:50 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 22:02:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Liu", "Gang", ""], ["Li", "Kenli", ""], ["Xiao", "Zheng", ""], ["Wang", "Rujia", ""]]}, {"id": "2011.03897", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Zirui Xu, Tong Shen, Dimitrios Stamoulis, Longfei Shangguan,\n  Di Wang, Rishi Madhok, Chunshui Zhao, Xin Li, Nikolaos Karianakis, Dimitrios\n  Lymberopoulos, Ang Li, ChenChen Liu, Yiran Chen, Xiang Chen", "title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and\n  Tail Effect Elimination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the\nincreasing computational cost makes them very challenging to meet real-time\nlatency and accuracy requirements. Although DNN runtime latency is dictated by\nmodel property (e.g., architecture, operations), hardware property (e.g.,\nutilization, throughput), and more importantly, the effective mapping between\nthese two, many existing approaches focus only on optimizing model property\nsuch as FLOPS reduction and overlook the mismatch between DNN model and\nhardware properties. In this work, we show that the mismatch between the varied\nDNN computation workloads and GPU capacity can cause the idle GPU tail effect,\nleading to GPU under-utilization and low throughput. As a result, the FLOPs\nreduction cannot bring effective latency reduction, which causes sub-optimal\naccuracy versus latency trade-offs. Motivated by this, we propose a GPU\nruntime-aware DNN optimization methodology to eliminate such GPU tail effect\nadaptively on GPU platforms. Our methodology can be applied on top of existing\nSOTA DNN optimization approaches to achieve better latency and accuracy\ntrade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy\nimprovement over several SOTA DNN pruning and NAS methods, respectively\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 03:19:46 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 21:57:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Yu", "Fuxun", ""], ["Xu", "Zirui", ""], ["Shen", "Tong", ""], ["Stamoulis", "Dimitrios", ""], ["Shangguan", "Longfei", ""], ["Wang", "Di", ""], ["Madhok", "Rishi", ""], ["Zhao", "Chunshui", ""], ["Li", "Xin", ""], ["Karianakis", "Nikolaos", ""], ["Lymberopoulos", "Dimitrios", ""], ["Li", "Ang", ""], ["Liu", "ChenChen", ""], ["Chen", "Yiran", ""], ["Chen", "Xiang", ""]]}, {"id": "2011.04107", "submitter": "Sergi Abadal", "authors": "Sergi Abadal, Robert Guirado, Hamidreza Taghvaee, Akshay Jain, Elana\n  Pereira de Santana, Peter Haring Bol\\'ivar, Mohamed Saeed, Renato Negra,\n  Zhenxing Wang, Kun-Ta Wang, Max C. Lemme, Joshua Klein, Marina Zapater,\n  Alexandre Levisse, David Atienza, Davide Rossi, Francesco Conti, Martino\n  Dazzi, Geethan Karunaratne, Irem Boybat and Abu Sebastian", "title": "Graphene-based Wireless Agile Interconnects for Massive Heterogeneous\n  Multi-chip Processors", "comments": "7 pages, 4 figures, 1 table - Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main design principles in computer architecture have recently shifted\nfrom a monolithic scaling-driven approach to the development of heterogeneous\narchitectures that tightly co-integrate multiple specialized processor and\nmemory chiplets. In such data-hungry multi-chip architectures, current\nNetworks-in-Package (NiPs) may not be enough to cater to their heterogeneous\nand fast-changing communication demands. This position paper makes the case for\nwireless in-package nanonetworking as the enabler of efficient and versatile\nwired-wireless interconnect fabrics for massive heterogeneous processors. To\nthat end, the use of graphene-based antennas and transceivers with unique\nfrequency-beam reconfigurability in the terahertz band is proposed. The\nfeasibility of such a nanonetworking vision and the main research challenges\ntowards its realization are analyzed from the technological, communications,\nand computer architecture perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 23:39:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Abadal", "Sergi", ""], ["Guirado", "Robert", ""], ["Taghvaee", "Hamidreza", ""], ["Jain", "Akshay", ""], ["de Santana", "Elana Pereira", ""], ["Bol\u00edvar", "Peter Haring", ""], ["Saeed", "Mohamed", ""], ["Negra", "Renato", ""], ["Wang", "Zhenxing", ""], ["Wang", "Kun-Ta", ""], ["Lemme", "Max C.", ""], ["Klein", "Joshua", ""], ["Zapater", "Marina", ""], ["Levisse", "Alexandre", ""], ["Atienza", "David", ""], ["Rossi", "Davide", ""], ["Conti", "Francesco", ""], ["Dazzi", "Martino", ""], ["Karunaratne", "Geethan", ""], ["Boybat", "Irem", ""], ["Sebastian", "Abu", ""]]}, {"id": "2011.04275", "submitter": "Angelica Sofia Valeriani", "authors": "Angelica Sofia Valeriani", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "comments": "arXiv admin note: text overlap with arXiv:1903.11406,\n  arXiv:2002.00819 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper wants to focus on providing a characterization of the runtime\nperformances of state-of-the-art implementations of KGE alghoritms, in terms of\nmemory footprint and execution time. Despite the rapidly growing interest in\nKGE methods, so far little attention has been devoted to their comparison and\nevaluation; in particular, previous work mainly focused on performance in terms\nof accuracy in specific tasks, such as link prediction. To this extent, a\nframework is proposed for evaluating available KGE implementations against\ngraphs with different properties, with a particular focus on the effectiveness\nof the adopted optimization strategies. Graphs and models have been trained\nleveraging different architectures, in order to enlighten features and\nproperties of both models and the architectures they have been trained on. Some\nresults enlightened with experiments in this document are the fact that\nmultithreading is efficient, but benefit deacreases as the number of threads\ngrows in case of CPU. GPU proves to be the best architecture for the given\ntask, even if CPU with some vectorized instructions still behaves well.\nFinally, RAM utilization for the loading of the graph never changes between\ndifferent architectures and depends only on the type of graph, not on the\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 21:58:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Valeriani", "Angelica Sofia", ""]]}, {"id": "2011.04567", "submitter": "Fei Wen", "authors": "Fei Wen, Mian Qin, Paul V. Gratz, A.L.Narasimha Reddy", "title": "FPGA-based Hyrbid Memory Emulation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid memory systems, comprised of emerging non-volatile memory (NVM) and\nDRAM, have been proposed to address the growing memory demand of applications.\nEmerging NVM technologies, such as phase-change memories (PCM), memristor, and\n3D XPoint, have higher capacity density, minimal static power consumption and\nlower cost per GB. However, NVM has longer access latency and limited write\nendurance as opposed to DRAM. The different characteristics of two memory\nclasses point towards the design of hybrid memory systems containing multiple\nclasses of main memory.\n  In the iterative and incremental development of new architectures, the\ntimeliness of simulation completion is critical to project progression. Hence,\na highly efficient simulation method is needed to evaluate the performance of\ndifferent hybrid memory system designs. Design exploration for hybrid memory\nsystems is challenging, because it requires emulation of the full system stack,\nincluding the OS, memory controller, and interconnect. Moreover, benchmark\napplications for memory performance test typically have much larger working\nsets, thus taking even longer simulation warm-up period.\n  In this paper, we propose a FPGA-based hybrid memory system emulation\nplatform. We target at the mobile computing system, which is sensitive to\nenergy consumption and is likely to adopt NVM for its power efficiency. Here,\nbecause the focus of our platform is on the design of the hybrid memory system,\nwe leverage the on-board hard IP ARM processors to both improve simulation\nperformance while improving accuracy of the results. Thus, users can implement\ntheir data placement/migration policies with the FPGA logic elements and\nevaluate new designs quickly and effectively. Results show that our emulation\nplatform provides a speedup of 9280x in simulation time compared to the\nsoftware counterpart Gem5.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:12:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wen", "Fei", ""], ["Qin", "Mian", ""], ["Gratz", "Paul V.", ""], ["Reddy", "A. L. Narasimha", ""]]}, {"id": "2011.04727", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "von Neumann's missing \"Second Draft\": what it should contain", "comments": "5 pages, 4 figures. Accepted to 2020 International Conference on\n  Computational Science and Computational Intelligence, Las Vegas, US, as paper\n  CSCI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing science is based on a computing paradigm that is not valid anymore\nfor today's technological conditions. The reason is that the transmission time\neven inside the processor chip, but especially between the components of the\nsystem, is not negligible anymore. The paper introduces a quantitative measure\nfor dispersion, which is vital for both computing performance and energy\nconsumption, and demonstrates how its value increased with the changing\ntechnology. The temporal behavior (including the dispersion of the commonly\nused synchronization clock time) of computing components has a critical impact\non the system's performance at all levels, as demonstrated from gate-level\noperation to supercomputing. The same effect limits the utility of the\nresearched new materials/effects if the related transfer time cannot be\nproportionally mitigated. von Neumann's model is perfect, but now it is used\noutside of its range of validity. The correct procedure to consider the\ntransfer time for the present technological background is also derived.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:07:53 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2011.04931", "submitter": "Ang Li", "authors": "Cheng Tan, Chenhao Xie, Tong Geng, Andres Marquez, Antonino Tumeo,\n  Kevin Barker, Ang Li", "title": "ARENA: Asynchronous Reconfigurable Accelerator Ring to Enable\n  Data-Centric Parallel Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation HPC and data centers are likely to be reconfigurable and\ndata-centric due to the trend of hardware specialization and the emergence of\ndata-driven applications. In this paper, we propose ARENA -- an asynchronous\nreconfigurable accelerator ring architecture as a potential scenario on how the\nfuture HPC and data centers will be like. Despite using the coarse-grained\nreconfigurable arrays (CGRAs) as the substrate platform, our key contribution\nis not only the CGRA-cluster design itself, but also the ensemble of a new\narchitecture and programming model that enables asynchronous tasking across a\ncluster of reconfigurable nodes, so as to bring specialized computation to the\ndata rather than the reverse. We presume distributed data storage without\nasserting any prior knowledge on the data distribution. Hardware specialization\noccurs at runtime when a task finds the majority of data it requires are\navailable at the present node. In other words, we dynamically generate\nspecialized CGRA accelerators where the data reside. The asynchronous tasking\nfor bringing computation to data is achieved by circulating the task token,\nwhich describes the data-flow graphs to be executed for a task, among the CGRA\ncluster connected by a fast ring network. Evaluations on a set of HPC and\ndata-driven applications across different domains show that ARENA can provide\nbetter parallel scalability with reduced data movement (53.9%). Compared with\ncontemporary compute-centric parallel models, ARENA can bring on average 4.37x\nspeedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm^2\nchip area under 45nm process technology and can run at 800MHz with on average\n759.8mW power consumption. ARENA also supports the concurrent execution of\nmulti-applications, offering ideal architectural support for future\nhigh-performance parallel computing and data analytics systems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:25:28 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:48:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tan", "Cheng", ""], ["Xie", "Chenhao", ""], ["Geng", "Tong", ""], ["Marquez", "Andres", ""], ["Tumeo", "Antonino", ""], ["Barker", "Kevin", ""], ["Li", "Ang", ""]]}, {"id": "2011.05160", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Fabrizio Petrini, Hongbo Rong, Andrei Valentin,\n  Carl Ebeling", "title": "Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencils represent a class of computational patterns where an output grid\npoint depends on a fixed shape of neighboring points in an input grid. Stencil\ncomputations are prevalent in scientific applications engaging a significant\nportion of supercomputing resources. Therefore, it has been always important to\noptimize stencil programs for the best performance. A rich body of research has\nfocused on optimizing stencil computations on almost all parallel\narchitectures. Stencil applications have regular dependency patterns, inherent\npipeline-parallelism, and plenty of data reuse. This makes these applications a\nperfect match for a coarse-grained reconfigurable spatial architecture (CGRA).\nA CGRA consists of many simple, small processing elements (PEs) connected with\nan on-chip network. Each PE can be configured to execute part of a stencil\ncomputation and all PEs run in parallel; the network can also be configured so\nthat data loaded can be passed from a PE to a neighbor PE directly and thus\nreused by many PEs without register spilling and memory traffic. How to\nefficiently map a stencil computation to a CGRA is the key to performance. In\nthis paper, we show a few unique and generalizable ways of mapping one- and\nmultidimensional stencil computations to a CGRA, fully exploiting the data\nreuse opportunities and parallelism. Our simulation experiments demonstrate\nthat these mappings are efficient and enable the CGRA to outperform\nstate-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:51:20 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:31:46 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""], ["Rong", "Hongbo", ""], ["Valentin", "Andrei", ""], ["Ebeling", "Carl", ""]]}, {"id": "2011.05422", "submitter": "Steven Kommrusch", "authors": "Steve Kommrusch, Marcos Horro, Louis-No\\\"el Pouchet, Gabriel\n  Rodr\\'iguez, Juan Touri\\~no", "title": "Coherence Traffic in Manycore Processors with Opaque Distributed\n  Directories", "comments": "17 pages, 13 figures, submitted to IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manycore processors feature a high number of general-purpose cores designed\nto work in a multithreaded fashion. Recent manycore processors are kept\ncoherent using scalable distributed directories. A paramount example is the\nIntel Mesh interconnect, which consists of a network-on-chip interconnecting\n\"tiles\", each of which contains computation cores, local caches, and coherence\nmasters. The distributed coherence subsystem must be queried for every\nout-of-tile access, imposing an overhead on memory latency. This paper studies\nthe physical layout of an Intel Knights Landing processor, with a particular\nfocus on the coherence subsystem, and uncovers the pseudo-random mapping\nfunction of physical memory blocks across the pieces of the distributed\ndirectory. Leveraging this knowledge, candidate optimizations to improve memory\nlatency through the minimization of coherence traffic are studied. Although\nthese optimizations do improve memory throughput, ultimately this does not\ntranslate into performance gains due to inherent overheads stemming from the\ncomputational complexity of the mapping functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:10:10 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kommrusch", "Steve", ""], ["Horro", "Marcos", ""], ["Pouchet", "Louis-No\u00ebl", ""], ["Rodr\u00edguez", "Gabriel", ""], ["Touri\u00f1o", "Juan", ""]]}, {"id": "2011.05497", "submitter": "Bilge Acun", "authors": "Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean Wu,\n  Kim Hazelwood", "title": "Understanding Training Efficiency of Deep Learning Recommendation Models\n  at Scale", "comments": "To appear in IEEE International Symposium on High-Performance\n  Computer Architecture (HPCA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of GPUs has proliferated for machine learning workflows and is now\nconsidered mainstream for many deep learning models. Meanwhile, when training\nstate-of-the-art personal recommendation models, which consume the highest\nnumber of compute cycles at our large-scale datacenters, the use of GPUs came\nwith various challenges due to having both compute-intensive and\nmemory-intensive components. GPU performance and efficiency of these\nrecommendation models are largely affected by model architecture configurations\nsuch as dense and sparse features, MLP dimensions. Furthermore, these models\noften contain large embedding tables that do not fit into limited GPU memory.\nThe goal of this paper is to explain the intricacies of using GPUs for training\nrecommendation models, factors affecting hardware efficiency at scale, and\nlearnings from a new scale-up GPU server design, Zion.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 01:21:43 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Acun", "Bilge", ""], ["Murphy", "Matthew", ""], ["Wang", "Xiaodong", ""], ["Nie", "Jade", ""], ["Wu", "Carole-Jean", ""], ["Hazelwood", "Kim", ""]]}, {"id": "2011.06376", "submitter": "Peichen Xie", "authors": "Peichen Xie, Xuanle Ren, Guangyu Sun", "title": "Customizing Trusted AI Accelerators for Efficient Privacy-Preserving\n  Machine Learning", "comments": "This work was carried out in 2019 and was accepted to DAC 2020 WIP\n  session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The use of trusted hardware has become a promising solution to enable\nprivacy-preserving machine learning. In particular, users can upload their\nprivate data and models to a hardware-enforced trusted execution environment\n(e.g. an enclave in Intel SGX-enabled CPUs) and run machine learning tasks in\nit with confidentiality and integrity guaranteed. To improve performance, AI\naccelerators have been widely employed for modern machine learning tasks.\nHowever, how to protect privacy on an AI accelerator remains an open question.\nTo address this question, we propose a solution for efficient\nprivacy-preserving machine learning based on an unmodified trusted CPU and a\ncustomized trusted AI accelerator. We carefully leverage cryptographic\nprimitives to establish trust and protect the channel between the CPU and the\naccelerator. As a case study, we demonstrate our solution based on the\nopen-source versatile tensor accelerator. The result of evaluation shows that\nthe proposed solution provides efficient privacy-preserving machine learning at\na small design cost and moderate performance overhead.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 13:41:18 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Xie", "Peichen", ""], ["Ren", "Xuanle", ""], ["Sun", "Guangyu", ""]]}, {"id": "2011.07317", "submitter": "Lucian Petrica", "authors": "Lucian Petrica, Tobias Alonso, Mairin Kroes, Nicholas Fraser, Sorin\n  Cotofana, Michaela Blott", "title": "Memory-Efficient Dataflow Inference for Deep CNNs on FPGA", "comments": "To appear in FPT 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom dataflow Convolutional Neural Network (CNN) inference accelerators on\nFPGA are tailored to a specific CNN topology and store parameters in On-Chip\nMemory (OCM), resulting in high energy efficiency and low inference latency.\nHowever, in these accelerators the shapes of parameter memories are dictated by\nthroughput constraints and do not map well to the underlying OCM, which becomes\nan implementation bottleneck. In this work, we propose an accelerator design\nmethodology - Frequency Compensated Memory Packing (FCMP) - which improves the\nOCM utilization efficiency of dataflow accelerators with minimal reduction in\nthroughput and no modifications to the physical structure of FPGA OCM. To\nvalidate our methodology, we apply it to several realizations of medium-sized\nCIFAR-10 inference accelerators and demonstrate up to 30% reduction in OCM\nutilization without loss of inference throughput, allowing us to port the\naccelerators from Xilinx Zynq 7020 to 7012S, reducing application cost. We also\nimplement a custom dataflow FPGA inference accelerator for a quantized\nResNet-50 CNN, utilizing on-chip weights, the largest topology ever implemented\nwith this accelerator architecture. We demonstrate that by applying FCMP to the\nResNet accelerator, the OCM bottleneck is alleviated which enables the\naccelerator to be ported from Alveo U250 to the smaller Alveo U280 board with\nless throughput loss compared to alternative techniques. By providing a\nfiner-grained trade off between throughput and OCM requirements, FCMP increases\nthe flexibility of custom dataflow CNN inference designs on FPGA.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 15:14:03 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Petrica", "Lucian", ""], ["Alonso", "Tobias", ""], ["Kroes", "Mairin", ""], ["Fraser", "Nicholas", ""], ["Cotofana", "Sorin", ""], ["Blott", "Michaela", ""]]}, {"id": "2011.07391", "submitter": "Shurui Li", "authors": "Shurui Li, Mario Miscuglio, Volker J. Sorger, Puneet Gupta", "title": "Channel Tiling for Improved Performance and Accuracy of Optical Neural\n  Network Accelerators", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low latency, high throughput inference on Convolution Neural Networks (CNNs)\nremains a challenge, especially for applications requiring large input or large\nkernel sizes. 4F optics provides a solution to accelerate CNNs by converting\nconvolutions into Fourier-domain point-wise multiplications that are\ncomputationally 'free' in optical domain. However, existing 4F CNN systems\nsuffer from the all-positive sensor readout issue which makes the\nimplementation of a multi-channel, multi-layer CNN not scalable or even\nimpractical. In this paper we propose a simple channel tiling scheme for 4F CNN\nsystems that utilizes the high resolution of 4F system to perform channel\nsummation inherently in optical domain before sensor detection, so the outputs\nof different channels can be correctly accumulated. Compared to state of the\nart, channel tiling gives similar accuracy, significantly better robustness to\nsensing quantization (33\\% improvement in required sensing precision) error and\nnoise (10dB reduction in tolerable sensing noise), 0.5X total filters required,\n10-50X+ throughput improvement and as much as 3X reduction in required output\ncamera resolution/bandwidth. Not requiring any additional optical hardware, the\nproposed channel tiling approach addresses an important throughput and\nprecision bottleneck of high-speed, massively-parallel optical 4F computing\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:22:54 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 04:36:22 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Li", "Shurui", ""], ["Miscuglio", "Mario", ""], ["Sorger", "Volker J.", ""], ["Gupta", "Puneet", ""]]}, {"id": "2011.07400", "submitter": "Ivan De Oliveira Nunes", "authors": "Ivan De Oliveira Nunes, Sashidhar Jakkamsetti, Gene Tsudik", "title": "Tiny-CFA: A Minimalistic Approach for Control-Flow Attestation Using\n  Verified Proofs of Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of tiny trust anchors has received significant attention over the\npast decade, to secure low-end MCU-s that cannot afford expensive security\nmechanisms. In particular, hardware/software (hybrid) co-designs offer low\nhardware cost, while retaining similar security guarantees as (more expensive)\nhardware-based techniques. Hybrid trust anchors support security services, such\nas remote attestation, proofs of software update/erasure/reset, proofs of\nremote software execution, in resource-constrained MCU-s, e.g., MSP430 and AVR\nAtMega32. Despite these advances, detection of control-flow attacks in low-end\nMCU-s remains a challenge, since hardware requirements of the cheapest related\narchitectures are often more expensive than the MCU-s themselves. In this work,\nwe tackle this challenge by designing Tiny-CFA - a control-flow attestation\n(CFA) technique with a single hardware requirement - the ability to generate\nproofs of remote software execution (PoX). In turn, PoX can be implemented very\nefficiently and securely in low-end MCU-s. Consequently, our design achieves\nthe lowest hardware overhead of any CFA architecture (i.e., two orders of\nmagnitude cheaper), while relying on a formally verified PoX architecture as\nits sole hardware requirement. With respect to runtime overhead, Tiny-CFA also\nachieves better performance than prior CFA techniques based on code\ninstrumentation. We implement and evaluate Tiny-CFA, analyze its security, and\ndemonstrate its practicality using real-world publicly available applications.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 22:09:15 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 20:20:06 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Nunes", "Ivan De Oliveira", ""], ["Jakkamsetti", "Sashidhar", ""], ["Tsudik", "Gene", ""]]}, {"id": "2011.08070", "submitter": "Paul Scheffler", "authors": "Paul Scheffler, Florian Zaruba, Fabian Schuiki, Torsten Hoefler, Luca\n  Benini", "title": "Indirection Stream Semantic Register Architecture for Efficient\n  Sparse-Dense Linear Algebra", "comments": "6 pages, 4 figures. Submitted to DATE 2021. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse-dense linear algebra is crucial in many domains, but challenging to\nhandle efficiently on CPUs, GPUs, and accelerators alike; multiplications with\nsparse formats like CSR and CSF require indirect memory lookups. In this work,\nwe enhance a memory-streaming RISC-V ISA extension to accelerate sparse-dense\nproducts through streaming indirection. We present efficient dot,\nmatrix-vector, and matrix-matrix product kernels using our hardware, enabling\nsingle-core FPU utilizations of up to 80% and speedups of up to 7.2x over an\noptimized baseline without extensions. A matrix-vector implementation on a\nmulti-core cluster is up to 5.8x faster and 2.7x more energy-efficient with our\nkernels than an optimized baseline. We propose further uses for our indirection\nhardware, such as scatter-gather operations and codebook decoding, and compare\nour work to state-of-the-art CPU, GPU, and accelerator approaches, measuring a\n2.8x higher peak FP64 utilization in CSR matrix-vector multiplication than a\nGTX 1080 Ti GPU running a cuSPARSE kernel.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 16:20:54 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:05:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Scheffler", "Paul", ""], ["Zaruba", "Florian", ""], ["Schuiki", "Fabian", ""], ["Hoefler", "Torsten", ""], ["Benini", "Luca", ""]]}, {"id": "2011.08353", "submitter": "Biswadip Maity", "authors": "Biswadip Maity, Bryan Donyanavard, Anmol Surhonne, Amir Rahmani,\n  Andreas Herkersdorf and Nikil Dutt", "title": "AXES: Approximation Manager for Emerging Memory Architectures", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory approximation techniques are commonly limited in scope, targeting\nindividual levels of the memory hierarchy. Existing approximation techniques\nfor a full memory hierarchy determine optimal configurations at design-time\nprovided a goal and application. Such policies are rigid: they cannot adapt to\nunknown workloads and must be redesigned for different memory configurations\nand technologies. We propose AXES: the first self-optimizing runtime manager\nfor coordinating configurable approximation knobs across all levels of the\nmemory hierarchy. AXES continuously updates and optimizes its approximation\nmanagement policy throughout runtime for diverse workloads. AXES optimizes the\napproximate memory configuration to minimize power consumption without\ncompromising the quality threshold specified by application developers. AXES\ncan (1) learn a policy at runtime to manage variable application quality of\nservice (QoS) constraints, (2) automatically optimize for a target metric\nwithin those constraints, and (3) coordinate runtime decisions for\ninterdependent knobs and subsystems. We demonstrate AXES' ability to\nefficiently provide functions 1-3 on a RISC-V Linux platform with approximate\nmemory segments in the on-chip cache and main memory. We demonstrate AXES'\nability to save up to 37% energy in the memory subsystem without any\ndesign-time overhead. We show AXES' ability to reduce QoS violations by 75%\nwith $<5\\%$ additional energy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 00:55:09 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Maity", "Biswadip", ""], ["Donyanavard", "Bryan", ""], ["Surhonne", "Anmol", ""], ["Rahmani", "Amir", ""], ["Herkersdorf", "Andreas", ""], ["Dutt", "Nikil", ""]]}, {"id": "2011.08451", "submitter": "Vignesh Balaji", "authors": "Vignesh Balaji, Brandon Lucia", "title": "Optimizing Graph Processing and Preprocessing with Hardware Assisted\n  Propagation Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extensive prior research has focused on alleviating the characteristic poor\ncache locality of graph analytics workloads. However, graph pre-processing\ntasks remain relatively unexplored. In many important scenarios, graph\npre-processing tasks can be as expensive as the downstream graph analytics\nkernel. We observe that Propagation Blocking (PB), a software optimization\ndesigned for SpMV kernels, generalizes to many graph analytics kernels as well\nas common pre-processing tasks. In this work, we identify the lingering\ninefficiencies of a PB execution on conventional multicores and propose\narchitecture support to eliminate PB's bottlenecks, further improving the\nperformance gains from PB. Our proposed architecture -- COBRA -- optimizes the\nPB execution of both graph processing and pre-processing alike to provide\nend-to-end speedups of up to 4.6x (3.5x on average).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:13:52 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Balaji", "Vignesh", ""], ["Lucia", "Brandon", ""]]}, {"id": "2011.08455", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Revising the classic computing paradigm and its technological\n  implementations", "comments": "12 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2006.01128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.ET cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's computing is told to be based on the classic paradigm, proposed by\nvon Neumann, a three-quarter century ago. However, that paradigm was justified\n(for the timing relations of) vacuum tubes only. The technological development\ninvalidated the classic paradigm (but not the model!) and led to catastrophic\nperformance losses in computing systems, from operating gate level to large\nnetworks, including the neuromorphic ones. The paper reviews the critical\npoints of the classic paradigm and scrutinizes the confusion made around it. It\ndiscusses some of the consequences of improper technological implementation,\nfrom the shared media to the parallelized operation. The model is perfect, but\nit is applied outside of its range of validity. The paradigm is extended by\nproviding the \"procedure\" that enables computing science to work with cases\nwhere the transfer time is not negligible apart from processing time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:37:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2011.08781", "submitter": "Erick Carvajal Barboza", "authors": "Erick Carvajal Barboza and Sara Jacob and Mahesh Ketkar and Michael\n  Kishinevsky and Paul Gratz and Jiang Hu", "title": "Automatic Microprocessor Performance Bug Detection", "comments": "14 pages, 13 figures, to appear in the 27th International Symposium\n  on High-Performance Computer Architecture (HPCA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processor design validation and debug is a difficult and complex task, which\nconsumes the lion's share of the design process. Design bugs that affect\nprocessor performance rather than its functionality are especially difficult to\ncatch, particularly in new microarchitectures. This is because, unlike\nfunctional bugs, the correct processor performance of new microarchitectures on\ncomplex, long-running benchmarks is typically not deterministically known.\nThus, when performance benchmarking new microarchitectures, performance teams\nmay assume that the design is correct when the performance of the new\nmicroarchitecture exceeds that of the previous generation, despite significant\nperformance regressions existing in the design. In this work, we present a\ntwo-stage, machine learning-based methodology that is able to detect the\nexistence of performance bugs in microprocessors. Our results show that our\nbest technique detects 91.5% of microprocessor core performance bugs whose\naverage IPC impact across the studied applications is greater than 1% versus a\nbug-free design with zero false positives. When evaluated on memory system\nbugs, our technique achieves 100% detection with zero false positives.\nMoreover, the detection is automatic, requiring very little performance\nengineer time.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 17:18:45 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 15:39:21 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Barboza", "Erick Carvajal", ""], ["Jacob", "Sara", ""], ["Ketkar", "Mahesh", ""], ["Kishinevsky", "Michael", ""], ["Gratz", "Paul", ""], ["Hu", "Jiang", ""]]}, {"id": "2011.09067", "submitter": "M. Ali Vosoughi", "authors": "M. Ali Vosoughi", "title": "Distributed Injection-Locking in Analog Ising Machines to Solve\n  Combinatorial Optimizations", "comments": "5 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The oscillator-based Ising machine (OIM) is a network of coupled CMOS\noscillators that solves combinatorial optimization problems. In this paper, the\ndistribution of the injection-locking oscillations throughout the circuit is\nproposed to accelerate the phase-locking of the OIM. The implications of the\nproposed technique theoretically investigated and verified by extensive\nsimulations in EDA tools with a $130~nm$ PTM model. By distributing the\ninjective signal of the super-harmonic oscillator, the speed is increased by\n$219.8\\%$ with negligible increase in the power dissipation and phase-locking\nerror of the device due to the distributed technique.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:34:34 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 05:46:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Vosoughi", "M. Ali", ""]]}, {"id": "2011.09073", "submitter": "Masudul Quraishi", "authors": "Masudul Hassan Quraishi, Erfan Bank Tavakoli, Fengbo Ren", "title": "A Survey of System Architectures and Techniques for FPGA Virtualization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  FPGA accelerators are gaining increasing attention in both cloud and edge\ncomputing because of their hardware flexibility, high computational throughput,\nand low power consumption. However, the design flow of FPGAs often requires\nspecific knowledge of the underlying hardware, which hinders the wide adoption\nof FPGAs by application developers. Therefore, the virtualization of FPGAs\nbecomes extremely important to create a useful abstraction of the hardware\nsuitable for application developers. Such abstraction also enables the sharing\nof FPGA resources among multiple users and accelerator applications, which is\nimportant because, traditionally, FPGAs have been mostly used in single-user,\nsingle-embedded-application scenarios. There are many works in the field of\nFPGA virtualization covering different aspects and targeting different\napplication areas. In this survey, we review the system architectures used in\nthe literature for FPGA virtualization. In addition, we identify the primary\nobjectives of FPGA virtualization, based on which we summarize the techniques\nfor realizing FPGA virtualization. This survey helps researchers to efficiently\nlearn about FPGA virtualization research by providing a comprehensive review of\nthe existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:48:42 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 05:17:42 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 04:38:31 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Quraishi", "Masudul Hassan", ""], ["Tavakoli", "Erfan Bank", ""], ["Ren", "Fengbo", ""]]}, {"id": "2011.09261", "submitter": "Hui Chen", "authors": "Hui Chen, Peng Chen, Jun Zhou, Duong H. K. Luan, and Weichen Liu", "title": "ArSMART: An Improved SMART NoC Design Supporting Arbitrary-Turn\n  Transmission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SMART NoC, which transmits unconflicted flits to distant processing elements\n(PEs) in one cycle through the express bypass, is a high-performance NoC design\nproposed recently. However, if contention occurs, flits with low priority would\nnot only be buffered but also could not fully utilize bypass. Although there\nexist several routing algorithms that decrease contentions by rounding busy\nrouters and links, they cannot be directly applicable to SMART since it lacks\nthe support for arbitrary-turn (i.e., the number and direction of turns are\nfree of constraints) routing. Thus, in this article, to minimize contentions\nand further utilize bypass, we propose an improved SMART NoC, called ArSMART,\nin which arbitrary-turn transmission is enabled. Specifically, ArSMART divides\nthe whole NoC into multiple clusters where the route computation is conducted\nby the cluster controller and the data forwarding is performed by the\nbufferless reconfigurable router. Since the long-range transmission in SMART\nNoC needs to bypass the intermediate arbitration, to enable this feature, we\ndirectly configure the input and output ports connection rather than apply\nhop-by-hop table-based arbitration. To further explore the higher communication\ncapabilities, effective adaptive routing algorithms that are compatible with\nArSMART are proposed. The route computation overhead, one of the main concerns\nfor adaptive routing algorithms, is hidden by our carefully designed control\nmechanism. Compared with the state-of-the-art SMART NoC, the experimental\nresults demonstrate an average reduction of 40.7% in application schedule\nlength and 29.7% in energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 13:18:09 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Chen", "Hui", ""], ["Chen", "Peng", ""], ["Zhou", "Jun", ""], ["Luan", "Duong H. K.", ""], ["Liu", "Weichen", ""]]}, {"id": "2011.09819", "submitter": "Amir Mozammel", "authors": "Amir Mozammel", "title": "Hardware Implementation of Fano Decoder for Polarization-adjusted\n  Convolutional (PAC) Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This brief proposes a hardware implementation architecture for Fano decoding\nof polarization-adjusted convolutional (PAC) codes. This architecture uses a\nnovel branch metric unit specific to PAC codes. The proposed decoder is tested\non FPGA, and its performance is evaluated on ASIC using TSMC 28 nm 0.72 V\nlibrary. The decoder can be clocked at 500 MHz and reach an average information\nthroughput of 38 Mb/s at 3.5 dB signal-to-noise ratio for a block length of 128\nand a code rate of 1/2.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 13:46:09 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 09:39:09 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 13:52:52 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Mozammel", "Amir", ""]]}, {"id": "2011.10249", "submitter": "Tuo Li", "authors": "Tuo Li, Bradley Hopkins, Sri Parameswaran", "title": "SIMF: Single-Instruction Multiple-Flush Mechanism for Processor Temporal\n  Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarchitectural timing attacks are a type of information leakage attack,\nwhich exploit the time-shared microarchitectural components, such as caches,\ntranslation look-aside buffers (TLBs), branch prediction unit (BPU), and\nspeculative execution, in modern processors to leak critical information from a\nvictim process or thread. To mitigate such attacks, the mechanism for flushing\nthe on-core state is extensively used by operating-system-level solutions,\nsince on-core state is too expensive to partition. In these systems, the\nflushing operations are implemented in software (using cache maintenance\ninstructions), which severely limit the efficiency of timing attack protection.\n  To bridge this gap, we propose specialized hardware support, a\nsingle-instruction multiple-flush (SIMF) mechanism to flush the core-level\nstate, which consists of L1 caches, BPU, TLBs, and register file. We\ndemonstrate SIMF by implementing it as an ISA extension, i.e., flushx\ninstruction, in scalar in-order RISC-V processor. The resultant processor is\nprototyped on Xilinx ZCU102 FPGA and validated with state-of-art seL4\nmicrokernel, Linux kernel in multi-core scenarios, and a cache timing attack.\nOur evaluation shows that SIMF significantly alleviates the overhead of\nflushing by more than a factor of two in execution time and reduces dynamic\ninstruction count by orders-of-magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:48:27 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Li", "Tuo", ""], ["Hopkins", "Bradley", ""], ["Parameswaran", "Sri", ""]]}, {"id": "2011.10351", "submitter": "Jonas Fritzsch", "authors": "Jonas Fritzsch, Tobias Schmid, Stefan Wagner", "title": "Experiences from Large-Scale Model Checking: Verification of a Vehicle\n  Control System", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the age of autonomously driving vehicles, functionality and complexity of\nembedded systems are increasing tremendously. Safety aspects become more\nimportant and require such systems to operate with the highest possible level\nof fault tolerance. Simulation and systematic testing techniques have reached\ntheir limits in this regard. Here, formal verification as a long established\ntechnique can be an appropriate complement. However, the necessary preparatory\nwork like adequately modeling a system and specifying properties in temporal\nlogic are anything but trivial. In this paper, we report on our experiences\napplying model checking to verify the arbitration logic of a Vehicle Control\nSystem. We balance pros and cons of different model checking techniques and\ntools, and reason about our choice of the symbolic model checker NuSMV. We\ndescribe the process of modeling the architecture, resulting in ~1500 LOC, 69\nstate variables and 38 LTL constraints. To handle this large-scale model, we\nautomate and optimize the model checking procedure for use on multi-core CPUs\nand employ Bounded Model Checking to avoid the state explosion problem. We\nshare our lessons learned and provide valuable insights for architects,\ndevelopers, and test engineers involved in this highly present topic.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:29:58 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Fritzsch", "Jonas", ""], ["Schmid", "Tobias", ""], ["Wagner", "Stefan", ""]]}, {"id": "2011.10550", "submitter": "Mark Stephenson", "authors": "Mark W. Stephenson and Ram Rangan", "title": "AZP: Automatic Specialization for Zero Values in Gaming Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that dynamic zeros in shader programs of gaming\napplications can be effectively leveraged with a profile-guided,\ncode-versioning transform. This transform duplicates code, specializes one path\nassuming certain key program operands, called versioning variables, are zero,\nand leaves the other path unspecialized. Dynamically, depending on the\nversioning variable's value, either the specialized fast path or the default\nslow path will execute. Prior work applied this transform manually and showed\npromising gains on gaming applications. In this paper, we present AZP, an\nautomatic compiler approach to perform the above code-versioning transform. Our\nframework automatically determines which versioning variables or combinations\nof them are profitable, and determines the code region to duplicate and\nspecialize (called the versioning scope). AZP takes operand zero value\nprobabilities as input and it then uses classical techniques such as constant\nfolding and dead-code elimination to determine the most profitable versioning\nvariables and their versioning scopes. This information is then used to affect\nthe final transform in a straightforward manner. We demonstrate that AZP is\nable to achieve an average speedup of 16.4% for targeted shader programs,\namounting to an average frame-rate speedup of 3.5% across a collection of\nmodern gaming applications on an NVIDIA GeForce RTX 2080 GPU GPU.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 18:44:01 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Stephenson", "Mark W.", ""], ["Rangan", "Ram", ""]]}, {"id": "2011.10912", "submitter": "Fuxun Yu", "authors": "Fuxun Yu, Dimitrios Stamoulis, Di Wang, Dimitrios Lymberopoulos, Xiang\n  Chen", "title": "Third ArchEdge Workshop: Exploring the Design Space of Efficient Deep\n  Neural Networks", "comments": "Presented in Third ArchEdge Workshop, Co-located with SEC'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper gives an overview of our ongoing work on the design space\nexploration of efficient deep neural networks (DNNs). Specifically, we cover\ntwo aspects: (1) static architecture design efficiency and (2) dynamic model\nexecution efficiency. For static architecture design, different from existing\nend-to-end hardware modeling assumptions, we conduct full-stack profiling at\nthe GPU core level to identify better accuracy-latency trade-offs for DNN\ndesigns. For dynamic model execution, different from prior work that tackles\nmodel redundancy at the DNN-channels level, we explore a new dimension of DNN\nfeature map redundancy to be dynamically traversed at runtime. Last, we\nhighlight several open questions that are poised to draw research attention in\nthe next few years.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 01:56:46 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yu", "Fuxun", ""], ["Stamoulis", "Dimitrios", ""], ["Wang", "Di", ""], ["Lymberopoulos", "Dimitrios", ""], ["Chen", "Xiang", ""]]}, {"id": "2011.10932", "submitter": "Bahar Asgari", "authors": "Bahar Asgari, Ramyad Hadidi, Joshua Dierberger, Charlotte Steinichen,\n  Hyesoon Kim", "title": "Copernicus: Characterizing the Performance Implications of Compression\n  Formats Used in Sparse Workloads", "comments": "11 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrices are the key ingredients of several application domains, from\nscientific computation to machine learning. The primary challenge with sparse\nmatrices has been efficiently storing and transferring data, for which many\nsparse formats have been proposed to significantly eliminate zero entries. Such\nformats, essentially designed to optimize memory footprint, may not be as\nsuccessful in performing faster processing. In other words, although they allow\nfaster data transfer and improve memory bandwidth utilization -- the classic\nchallenge of sparse problems -- their decompression mechanism can potentially\ncreate a computation bottleneck. Not only is this challenge not resolved, but\nalso it becomes more serious with the advent of domain-specific architectures\n(DSAs), as they intend to more aggressively improve performance. The\nperformance implications of using various formats along with DSAs, however, has\nnot been extensively studied by prior work. To fill this gap of knowledge, we\ncharacterize the impact of using seven frequently used sparse formats on\nperformance, based on a DSA for sparse matrix-vector multiplication (SpMV),\nimplemented on an FPGA using high-level synthesis (HLS) tools, a growing and\npopular method for developing DSAs. Seeking a fair comparison, we tailor and\noptimize the HLS implementation of decompression for each format. We thoroughly\nexplore diverse metrics, including decompression overhead, latency, balance\nratio, throughput, memory bandwidth utilization, resource utilization, and\npower consumption, on a variety of real-world and synthetic sparse workloads.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 04:42:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Asgari", "Bahar", ""], ["Hadidi", "Ramyad", ""], ["Dierberger", "Joshua", ""], ["Steinichen", "Charlotte", ""], ["Kim", "Hyesoon", ""]]}, {"id": "2011.11246", "submitter": "Takuto Kanamori", "authors": "Takuto Kanamori, Hiromu Miyazaki, Kenji Kise", "title": "RVCoreP-32IC: A high-performance RISC-V soft processor with an efficient\n  fetch unit supporting the compressed instructions", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a high-performance RISC-V soft processor with an\nefficient fetch unit supporting the compressed instructions targeting on FPGA.\nThe compressed instruction extension in RISC-V can reduce the program size by\nabout 25%. But it needs a complicated logic for the instruction fetch unit and\nhas a significant impact on performance. We propose an instruction fetch unit\nthat supports the compressed instructions while exhibiting high performance.\nFurthermore, we propose a RISC-V soft processor using this unit. We implement\nthis proposed processor in Verilog HDL and verify the behavior using Verilog\nsimulation and an actual Xilinx Atrix-7 FPGA board. We compare the results of\nsome benchmarks and the amount of hardware with related works. DMIPS, CoreMark\nvalue, and Embench value of the proposed processor achieved 42.5%, 41.1% and\n21.3% higher performance than the related work, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 07:13:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kanamori", "Takuto", ""], ["Miyazaki", "Hiromu", ""], ["Kise", "Kenji", ""]]}, {"id": "2011.11695", "submitter": "Shankar Balachandran", "authors": "Anant V. Nori, Rahul Bera, Shankar Balachandran, Joydeep Rakshit, Om\n  J. Omer, Avishaii Abuhatzera, Belliappa Kuttanna and Sreenivas Subramoney", "title": "Proximu$: Efficiently Scaling DNN Inference in Multi-core CPUs through\n  Near-Cache Compute", "comments": "18 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep Neural Network (DNN) inference is emerging as the fundamental bedrock\nfor a multitude of utilities and services. CPUs continue to scale up their raw\ncompute capabilities for DNN inference along with mature high performance\nlibraries to extract optimal performance. While general purpose CPUs offer\nunique attractive advantages for DNN inference at both datacenter and edge,\nthey have primarily evolved to optimize single thread performance. For highly\nparallel, throughput-oriented DNN inference, this results in inefficiencies in\nboth power and performance, impacting both raw performance scaling and overall\nperformance/watt.\n  We present Proximu$\\$$, where we systematically tackle the root\ninefficiencies in power and performance scaling for CPU DNN inference.\nPerformance scales efficiently by distributing light-weight tensor compute near\nall caches in a multi-level cache hierarchy. This maximizes the cumulative\nutilization of the existing bandwidth resources in the system and minimizes\nmovement of data. Power is drastically reduced through simple ISA extensions\nthat encode the structured, loop-y workload behavior. This enables a bulk\noffload of pre-decoded work, with loop unrolling in the light-weight near-cache\nunits, effectively bypassing the power-hungry stages of the wide Out-of-Order\n(OOO) CPU pipeline.\n  Across a number of DNN models, Proximu$\\$$ achieves a 2.3x increase in\nconvolution performance/watt with a 2x to 3.94x scaling in raw performance.\nSimilarly, Proximu$\\$$ achieves a 1.8x increase in inner-product\nperformance/watt with 2.8x scaling in performance. With no changes to the\nprogramming model, no increase in cache capacity or bandwidth and minimal\nadditional hardware, Proximu$\\$$ enables unprecedented CPU efficiency gains\nwhile achieving similar performance to state-of-the-art Domain Specific\nAccelerators (DSA) for DNN inference in this AI era.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 19:46:47 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 04:37:37 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Nori", "Anant V.", ""], ["Bera", "Rahul", ""], ["Balachandran", "Shankar", ""], ["Rakshit", "Joydeep", ""], ["Omer", "Om J.", ""], ["Abuhatzera", "Avishaii", ""], ["Kuttanna", "Belliappa", ""], ["Subramoney", "Sreenivas", ""]]}, {"id": "2011.11716", "submitter": "Pingakshya Goswami", "authors": "Pingakshya Goswami and Dinesh Bhatia", "title": "Automated Floorplanning for Partially Reconfigurable Designs on\n  Heterogenrous FPGAs", "comments": "This paper was accepted as a poster in ACM SIGDA International\n  Symposium on FPGA, 2016", "journal-ref": null, "doi": "10.1145/2847263.2847323", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Floorplanning problem has been extensively explored for homogeneous FPGAs.\nMost modern FPGAs consist of heterogeneous resources in the form of\nconfigurable logic blocks, DSP blocks, BRAMs and more. Very little work has\nbeen done for heterogeneous FPGAs. In addition, features like partial\nreconfigurability allow on-the-fly changes to the executable design that can\nresult in enhanced performance and very efficient utilization of resources. In\nthis paper, we have designed a floorplanner for Partially Reconfigurable (PR)\ndesigns in FPGA that smartly decides one of the three proposed resource\nallocation schemes to floorplan a particular type of reconfigurable region. We\nalso propose a White Space Detection algorithm for efficient management of\nwhite space inside an FPGA in order to reduce the area and the wire length. The\nfloorplanner is demonstrated on Xilinx Virtex 5 and Artix 7 FPGA architectures\nand can be easily integrated with existing vendor-supplied Place and Route\ntools. The main objective of the floorplanner is to reduce the wire length,\nminimize wasted resources and the area. The performance of our floorplanner is\nevaluated using MCNC benchmarks. We have compared our proposed floorplanner\nwith other previously published results reported in the literature. We observe\na substantial improvement in the overall wire length as well as the execution\ntime. Also, the floorplanner was integrated with vendor supplied place and\nroute tools (Xilinx Vivado) to automate the floorplanning flow. The automation\nprocess was tested on a partially reconfigurable median filter used in image\nprocessing applications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:41:03 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Goswami", "Pingakshya", ""], ["Bhatia", "Dinesh", ""]]}, {"id": "2011.11840", "submitter": "Omobayode Fagbohungbe", "authors": "Omobayode Fagbohungbe, Lijun Qian", "title": "Benchmarking Inference Performance of Deep Learning Models on Analog\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analog hardware implemented deep learning models are promising for\ncomputation and energy constrained systems such as edge computing devices.\nHowever, the analog nature of the device and the associated many noise sources\nwill cause changes to the value of the weights in the trained deep learning\nmodels deployed on such devices. In this study, systematic evaluation of the\ninference performance of trained popular deep learning models for image\nclassification deployed on analog devices has been carried out, where additive\nwhite Gaussian noise has been added to the weights of the trained models during\ninference. It is observed that deeper models and models with more redundancy in\ndesign such as VGG are more robust to the noise in general. However, the\nperformance is also affected by the design philosophy of the model, the\ndetailed structure of the model, the exact machine learning task, as well as\nthe datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 02:14:39 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:04:52 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Fagbohungbe", "Omobayode", ""], ["Qian", "Lijun", ""]]}, {"id": "2011.12092", "submitter": "Ashish Panwar", "authors": "Venkat Sri Sai Ram, Ashish Panwar, Arkaprava Basu", "title": "Leveraging Architectural Support of Three Page Sizes with Trident", "comments": "13 pages, 16 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large pages are commonly deployed to reduce address translation overheads for\nbig-memory workloads. Modern x86-64 processors from Intel and AMD support two\nlarge page sizes -- 1GB and 2MB. However, previous works on large pages have\nprimarily focused on 2MB pages, partly due to lack of substantial evidence on\nthe profitability of 1GB pages to real-world applications. We argue that in\nfact, inadequate system software support is responsible for a decade of\nunderutilized hardware support for 1GB pages.\n  Through extensive experimentation on a real system, we demonstrate that 1GB\npages can improve performance over 2MB pages, and when used in tandem with 2MB\npages for an important set of applications; the support for the latter is\ncrucial but missing in current systems. Our design and implementation of\n\\trident{} in Linux fully exploit hardware supported large pages by dynamically\nand transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.\n\\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on\naverage, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an\nextension to \\trident{} that effectively virtualizes 1GB pages via copy-less\npromotion and compaction in the guest OS. Overall, this paper shows that even\nGB-sized pages have considerable practical significance with adequate software\nenablement, in turn motivating architects to continue investing/innovating in\nlarge pages.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:55 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ram", "Venkat Sri Sai", ""], ["Panwar", "Ashish", ""], ["Basu", "Arkaprava", ""]]}, {"id": "2011.12669", "submitter": "Prashant Laddha", "authors": "Om Ji Omer, Prashant Laddha, Gurpreet S Kalsi, Anirud Thyagharajan,\n  Kamlesh R Pillai, Abhimanyu Kulkarni, Anbang Yao, Yurong Chen, Sreenivas\n  Subramoney", "title": "AccSS3D: Accelerator for Spatially Sparse 3D DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semantic understanding and completion of real world scenes is a foundational\nprimitive of 3D Visual perception widely used in high-level applications such\nas robotics, medical imaging, autonomous driving and navigation. Due to the\ncurse of dimensionality, compute and memory requirements for 3D scene\nunderstanding grow in cubic complexity with voxel resolution, posing a huge\nimpediment to realizing real-time energy efficient deployments. The inherent\nspatial sparsity present in the 3D world due to free space is fundamentally\ndifferent from the channel-wise sparsity that has been extensively studied. We\npresent ACCELERATOR FOR SPATIALLY SPARSE 3D DNNs (AccSS3D), the first\nend-to-end solution for accelerating 3D scene understanding by exploiting the\nample spatial sparsity. As an algorithm-dataflow-architecture co-designed\nsystem specialized for spatially-sparse 3D scene understanding, AccSS3D\nincludes novel spatial locality-aware metadata structures, a near-zero latency\nand spatial sparsity-aware dataflow optimizer, a surface orientation aware\npointcloud reordering algorithm and a codesigned hardware accelerator for\nspatial sparsity that exploits data reuse through systolic and multicast\ninterconnects. The SSpNNA accelerator core together with the 64 KB of L1 memory\nrequires 0.92 mm2 of area in 16nm process at 1 GHz. Overall, AccSS3D achieves\n16.8x speedup and a 2232x energy efficiency improvement for 3D sparse\nconvolution compared to an Intel-i7-8700K 4-core CPU, which translates to a\n11.8x end-to-end 3D semantic segmentation speedup and a 24.8x energy efficiency\nimprovement (iso technology node)\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 11:57:30 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Omer", "Om Ji", ""], ["Laddha", "Prashant", ""], ["Kalsi", "Gurpreet S", ""], ["Thyagharajan", "Anirud", ""], ["Pillai", "Kamlesh R", ""], ["Kulkarni", "Abhimanyu", ""], ["Yao", "Anbang", ""], ["Chen", "Yurong", ""], ["Subramoney", "Sreenivas", ""]]}, {"id": "2011.12839", "submitter": "Nick Iliev", "authors": "Nick Iliev and Amit Ranjan Trivedi", "title": "Low Latency CMOS Hardware Acceleration for Fully Connected Layers in\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel low latency CMOS hardware accelerator for fully connected\n(FC) layers in deep neural networks (DNNs). The FC accelerator, FC-ACCL, is\nbased on 128 8x8 or 16x16 processing elements (PEs) for matrix-vector\nmultiplication, and 128 multiply-accumulate (MAC) units integrated with 128\nHigh Bandwidth Memory (HBM) units for storing the pretrained weights.\nMicro-architectural details for CMOS ASIC implementations are presented and\nsimulated performance is compared to recent hardware accelerators for DNNs for\nAlexNet and VGG 16. When comparing simulated processing latency for a 4096-1000\nFC8 layer, our FC-ACCL is able to achieve 48.4 GOPS (with a 100 MHz clock)\nwhich improves on a recent FC8 layer accelerator quoted at 28.8 GOPS with a 150\nMHz clock. We have achieved this considerable improvement by fully utilizing\nthe HBM units for storing and reading out column-specific FClayer weights in 1\ncycle with a novel colum-row-column schedule, and implementing a maximally\nparallel datapath for processing these weights with the corresponding MAC and\nPE units. When up-scaled to 128 16x16 PEs, for 16x16 tiles of weights, the\ndesign can reduce latency for the large FC6 layer by 60 % in AlexNet and by 3 %\nin VGG16 when compared to an alternative EIE solution which uses compression.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:49:38 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Iliev", "Nick", ""], ["Trivedi", "Amit Ranjan", ""]]}, {"id": "2011.13000", "submitter": "Reena Elangovan", "authors": "Reena Elangovan, Shubham Jain, Anand Raghunathan", "title": "Ax-BxP: Approximate Blocked Computation for Precision-Reconfigurable\n  Deep Neural Network Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision scaling has emerged as a popular technique to optimize the compute\nand storage requirements of Deep Neural Networks (DNNs). Efforts toward\ncreating ultra-low-precision (sub-8-bit) DNNs suggest that the minimum\nprecision required to achieve a given network-level accuracy varies\nconsiderably across networks, and even across layers within a network,\nrequiring support for variable precision in DNN hardware. Previous proposals\nsuch as bit-serial hardware incur high overheads, significantly diminishing the\nbenefits of lower precision. To efficiently support precision\nre-configurability in DNN accelerators, we introduce an approximate computing\nmethod wherein DNN computations are performed block-wise (a block is a group of\nbits) and re-configurability is supported at the granularity of blocks. Results\nof block-wise computations are composed in an approximate manner to enable\nefficient re-configurability. We design a DNN accelerator that embodies\napproximate blocked computation and propose a method to determine a suitable\napproximation configuration for a given DNN. By varying the approximation\nconfigurations across DNNs, we achieve 1.17x-1.73x and 1.02x-2.04x improvement\nin system energy and performance respectively, over an 8-bit fixed-point (FxP8)\nbaseline, with negligible loss in classification accuracy. Further, by varying\nthe approximation configurations across layers and data-structures within DNNs,\nwe achieve 1.25x-2.42x and 1.07x-2.95x improvement in system energy and\nperformance respectively, with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 20:00:38 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 15:35:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Elangovan", "Reena", ""], ["Jain", "Shubham", ""], ["Raghunathan", "Anand", ""]]}, {"id": "2011.13152", "submitter": "Shengheng Liu", "authors": "Yongming Huang, Shengheng Liu, Cheng Zhang, Xiaohu You, Hequan Wu", "title": "True-data Testbed for 5G/B5G Intelligent Network", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile\ncommunications will shift from facilitating interpersonal communications to\nsupporting Internet of Everything (IoE), where intelligent communications with\nfull integration of big data and artificial intelligence (AI) will play an\nimportant role in improving network efficiency and providing high-quality\nservice. As a rapid evolving paradigm, the AI-empowered mobile communications\ndemand large amounts of data acquired from real network environment for\nsystematic test and verification. Hence, we build the world's first true-data\ntestbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site\nexperimental networks, data acquisition & data warehouse, and AI engine &\nnetwork optimization. In the TTIN, true network data acquisition, storage,\nstandardization, and analysis are available, which enable system-level online\nverification of B5G/6G-orientated key technologies and support data-driven\nnetwork optimization through the closed-loop control mechanism. This paper\nelaborates on the system architecture and module design of TTIN. Detailed\ntechnical specifications and some of the established use cases are also\nshowcased.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 06:42:36 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 08:51:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Huang", "Yongming", ""], ["Liu", "Shengheng", ""], ["Zhang", "Cheng", ""], ["You", "Xiaohu", ""], ["Wu", "Hequan", ""]]}, {"id": "2011.13493", "submitter": "Zhiyao Xie", "authors": "Zhiyao Xie, Guan-Qi Fang, Yu-Hung Huang, Haoxing Ren, Yanqing Zhang,\n  Brucek Khailany, Shao-Yun Fang, Jiang Hu, Yiran Chen, Erick Carvajal Barboza", "title": "FIST: A Feature-Importance Sampling and Tree-Based Method for Automatic\n  Design Flow Parameter Tuning", "comments": null, "journal-ref": "2020 Asia and South Pacific Design Automation Conference (ASP-DAC\n  2020)", "doi": "10.1109/ASP-DAC47756.2020.9045201", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Design flow parameters are of utmost importance to chip design quality and\nrequire a painfully long time to evaluate their effects. In reality, flow\nparameter tuning is usually performed manually based on designers' experience\nin an ad hoc manner. In this work, we introduce a machine learning-based\nautomatic parameter tuning methodology that aims to find the best design\nquality with a limited number of trials. Instead of merely plugging in machine\nlearning engines, we develop clustering and approximate sampling techniques for\nimproving tuning efficiency. The feature extraction in this method can reuse\nknowledge from prior designs. Furthermore, we leverage a state-of-the-art\nXGBoost model and propose a novel dynamic tree technique to overcome\noverfitting. Experimental results on benchmark circuits show that our approach\nachieves 25% improvement in design quality or 37% reduction in sampling cost\ncompared to random forest method, which is the kernel of a highly cited\nprevious work. Our approach is further validated on two industrial designs. By\nsampling less than 0.02% of possible parameter sets, it reduces area by 1.83%\nand 1.43% compared to the best solutions hand-tuned by experienced designers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:13:42 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xie", "Zhiyao", ""], ["Fang", "Guan-Qi", ""], ["Huang", "Yu-Hung", ""], ["Ren", "Haoxing", ""], ["Zhang", "Yanqing", ""], ["Khailany", "Brucek", ""], ["Fang", "Shao-Yun", ""], ["Hu", "Jiang", ""], ["Chen", "Yiran", ""], ["Barboza", "Erick Carvajal", ""]]}, {"id": "2011.13494", "submitter": "Zhiyao Xie", "authors": "Zhiyao Xie, Haoxing Ren, Brucek Khailany, Ye Sheng, Santosh Santosh,\n  Jiang Hu, Yiran Chen", "title": "PowerNet: Transferable Dynamic IR Drop Estimation via Maximum\n  Convolutional Neural Network", "comments": null, "journal-ref": "2020 Asia and South Pacific Design Automation Conference (ASP-DAC\n  2020)", "doi": "10.1109/ASP-DAC47756.2020.9045574", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  IR drop is a fundamental constraint required by almost all chip designs.\nHowever, its evaluation usually takes a long time that hinders mitigation\ntechniques for fixing its violations. In this work, we develop a fast dynamic\nIR drop estimation technique, named PowerNet, based on a convolutional neural\nnetwork (CNN). It can handle both vector-based and vectorless IR analyses.\nMoreover, the proposed CNN model is general and transferable to different\ndesigns. This is in contrast to most existing machine learning (ML) approaches,\nwhere a model is applicable only to a specific design. Experimental results\nshow that PowerNet outperforms the latest ML method by 9% in accuracy for the\nchallenging case of vectorless IR drop and achieves a 30 times speedup compared\nto an accurate IR drop commercial tool. Further, a mitigation tool guided by\nPowerNet reduces IR drop hotspots by 26% and 31% on two industrial designs,\nrespectively, with very limited modification on their power grids.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 23:14:17 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xie", "Zhiyao", ""], ["Ren", "Haoxing", ""], ["Khailany", "Brucek", ""], ["Sheng", "Ye", ""], ["Santosh", "Santosh", ""], ["Hu", "Jiang", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.13522", "submitter": "Zhiyao Xie", "authors": "Zhiyao Xie, Rongjian Liang, Xiaoqing Xu, Jiang Hu, Yixiao Duan, Yiran\n  Chen", "title": "Net2: A Graph Attention Network Method Customized for Pre-Placement Net\n  Length Estimation", "comments": null, "journal-ref": "2021 Asia and South Pacific Design Automation Conference (ASP-DAC\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Net length is a key proxy metric for optimizing timing and power across\nvarious stages of a standard digital design flow. However, the bulk of net\nlength information is not available until cell placement, and hence it is a\nsignificant challenge to explicitly consider net length optimization in design\nstages prior to placement, such as logic synthesis. This work addresses this\nchallenge by proposing a graph attention network method with customization,\ncalled Net2, to estimate individual net length before cell placement. Its\naccuracy-oriented version Net2a achieves about 15% better accuracy than several\nprevious works in identifying both long nets and long critical paths. Its fast\nversion Net2f is more than 1000 times faster than placement while still\noutperforms previous works and other neural network techniques in terms of\nvarious accuracy metrics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 01:47:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xie", "Zhiyao", ""], ["Liang", "Rongjian", ""], ["Xu", "Xiaoqing", ""], ["Hu", "Jiang", ""], ["Duan", "Yixiao", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.13959", "submitter": "Shihao Song", "authors": "Shihao Song and Anup Das", "title": "Design Methodologies for Reliable and Energy-efficient PCM Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phase-change memory (PCM) is a scalable and low latency non-volatile memory\n(NVM) technology that has been proposed to serve as storage class memory (SCM),\nproviding low access latency similar to DRAM and often approaching or exceeding\nthe capacity of SSD. The multilevel property of PCM also enables its adoption\nin neuromorphic systems to build high-density synaptic storage. We investigate\nand describe two significant bottlenecks of a PCM system. First, writing to PCM\ncells incurs significantly higher latency and energy penalties than reading its\ncontent. Second, high operating voltages of PCM impacts its reliable\noperations. In this work, we propose methodologies to tackle the bottlenecks,\nimproving performance, reliability, energy consumption, and sustainability for\na PCM system.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:05:49 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""]]}, {"id": "2011.13965", "submitter": "Adarsha Balaji", "authors": "Adarsha Balaji and Anup Das", "title": "Compiling Spiking Neural Networks to Mitigate Neuromorphic Hardware\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking Neural Networks (SNNs) are efficient computation models to perform\nspatio-temporal pattern recognition on {resource}- and {power}-constrained\nplatforms. SNNs executed on neuromorphic hardware can further reduce energy\nconsumption of these platforms. With increasing model size and complexity,\nmapping SNN-based applications to tile-based neuromorphic hardware is becoming\nincreasingly challenging. This is attributed to the limitations of\nneuro-synaptic cores, viz. a crossbar, to accommodate only a fixed number of\npre-synaptic connections per post-synaptic neuron. For complex SNN-based models\nthat have many neurons and pre-synaptic connections per neuron, (1) connections\nmay need to be pruned after training to fit onto the crossbar resources,\nleading to a loss in model quality, e.g., accuracy, and (2) the neurons and\nsynapses need to be partitioned and placed on the neuro-sypatic cores of the\nhardware, which could lead to increased latency and energy consumption. In this\nwork, we propose (1) a novel unrolling technique that decomposes a neuron\nfunction with many pre-synaptic connections into a sequence of homogeneous\nneural units to significantly improve the crossbar utilization and retain all\npre-synaptic connections, and (2) SpiNeMap, a novel methodology to map SNNs on\nneuromorphic hardware with an aim to minimize energy consumption and spike\nlatency.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:10:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Balaji", "Adarsha", ""], ["Das", "Anup", ""]]}, {"id": "2011.14203", "submitter": "Thierry Tambe", "authors": "Thierry Tambe, Coleman Hooper, Lillian Pentecost, Tianyu Jia, En-Yu\n  Yang, Marco Donato, Victor Sanh, Paul Whatmough, Alexander M. Rush, David\n  Brooks and Gu-Yeon Wei", "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware\n  Multi-Task NLP Inference", "comments": "12 pages plus references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language models such as BERT provide significant accuracy\nimprovement for a multitude of natural language processing (NLP) tasks.\nHowever, their hefty computational and memory demands make them challenging to\ndeploy to resource-constrained edge platforms with strict latency requirements.\nWe present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware\nenergy optimization for multi-task NLP. EdgeBERT employs entropy-based early\nexit predication in order to perform dynamic voltage-frequency scaling (DVFS),\nat a sentence granularity, for minimal energy consumption while adhering to a\nprescribed target latency. Computation and memory footprint overheads are\nfurther alleviated by employing a calibrated combination of adaptive attention\nspan, selective network pruning, and floating-point quantization. Furthermore,\nin order to maximize the synergistic benefits of these algorithms in always-on\nand intermediate edge computing settings, we specialize a 12nm scalable\nhardware accelerator system, integrating a fast-switching low-dropout voltage\nregulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,\nhigh-density embedded non-volatile memories (eNVMs) wherein the sparse\nfloating-point bit encodings of the shared multi-task parameters are carefully\nstored. Altogether, latency-aware multi-task NLP inference acceleration on the\nEdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy\ncompared to the conventional inference without early stopping, the\nlatency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson\nTegra X2 mobile GPU, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 19:21:47 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 03:03:49 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 04:02:59 GMT"}, {"version": "v4", "created": "Sat, 17 Apr 2021 22:11:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tambe", "Thierry", ""], ["Hooper", "Coleman", ""], ["Pentecost", "Lillian", ""], ["Jia", "Tianyu", ""], ["Yang", "En-Yu", ""], ["Donato", "Marco", ""], ["Sanh", "Victor", ""], ["Whatmough", "Paul", ""], ["Rush", "Alexander M.", ""], ["Brooks", "David", ""], ["Wei", "Gu-Yeon", ""]]}, {"id": "2011.14325", "submitter": "Angelo Garofalo", "authors": "Angelo Garofalo, Giuseppe Tagliavini, Francesco Conti, Luca Benini and\n  Davide Rossi", "title": "XpulpNN: Enabling Energy Efficient and Flexible Inference of Quantized\n  Neural Network on RISC-V based IoT End Nodes", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work introduces lightweight extensions to the RISC-V ISA to boost the\nefficiency of heavily Quantized Neural Network (QNN) inference on\nmicrocontroller-class cores. By extending the ISA with nibble (4-bit) and crumb\n(2-bit) SIMD instructions, we are able to show near-linear speedup with respect\nto higher precision integer computation on the key kernels for QNN computation.\nAlso, we propose a custom execution paradigm for SIMD sum-of-dot-product\noperations, which consists of fusing a dot product with a load operation, with\nan up to 1.64x peak MAC/cycle improvement compared to a standard execution\nscenario. To further push the efficiency, we integrate the RISC-V extended core\nin a parallel cluster of 8 processors, with near-linear improvement with\nrespect to a single core architecture. To evaluate the proposed extensions, we\nfully implement the cluster of processors in GF22FDX technology. QNN\nconvolution kernels on a parallel cluster implementing the proposed extension\nrun 6 x and 8 x faster when considering 4- and 2-bit data operands,\nrespectively, compared to a baseline processing cluster only supporting 8-bit\nSIMD instructions. With a peak of 2.22 TOPs/s/W, the proposed solution achieves\nefficiency levels comparable with dedicated DNN inference accelerators, and up\nto three orders of magnitude better than state-of-the-art ARM Cortex-M based\nmicrocontroller systems such as the low-end STM32L4 MCU and the high-end\nSTM32H7 MCU.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 09:57:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Garofalo", "Angelo", ""], ["Tagliavini", "Giuseppe", ""], ["Conti", "Francesco", ""], ["Benini", "Luca", ""], ["Rossi", "Davide", ""]]}, {"id": "2011.14755", "submitter": "Robert Guirado", "authors": "Robert Guirado, Hyoukjun Kwon, Sergi Abadal, Eduard Alarc\\'on, Tushar\n  Krishna", "title": "Dataflow-Architecture Co-Design for 2.5D DNN Accelerators using Wireless\n  Network-on-Package", "comments": "ASPDAC '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) models continue to grow in size and complexity,\ndemanding higher computational power to enable real-time inference. To\nefficiently deliver such computational demands, hardware accelerators are being\ndeveloped and deployed across scales. This naturally requires an efficient\nscale-out mechanism for increasing compute density as required by the\napplication. 2.5D integration over interposer has emerged as a promising\nsolution, but as we show in this work, the limited interposer bandwidth and\nmultiple hops in the Network-on-Package (NoP) can diminish the benefits of the\napproach. To cope with this challenge, we propose WIENNA, a wireless NoP-based\n2.5D DNN accelerator. In WIENNA, the wireless NoP connects an array of DNN\naccelerator chiplets to the global buffer chiplet, providing high-bandwidth\nmulticasting capabilities. Here, we also identify the dataflow style that most\nefficienty exploits the wireless NoP's high-bandwidth multicasting capability\non each layer. With modest area and power overheads, WIENNA achieves 2.2X--5.1X\nhigher throughput and 38.2% lower energy than an interposer-based NoP design.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:10:30 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Guirado", "Robert", ""], ["Kwon", "Hyoukjun", ""], ["Abadal", "Sergi", ""], ["Alarc\u00f3n", "Eduard", ""], ["Krishna", "Tushar", ""]]}]