[{"id": "2007.00060", "submitter": "Kanishkan Vadivel", "authors": "Kanishkan Vadivel, Lorenzo Chelini, Ali BanaGozar, Gagandeep Singh,\n  Stefano Corda, Roel Jordans, Henk Corporaal", "title": "TDO-CIM: Transparent Detection and Offloading for Computation In-memory", "comments": "Full version of DATE2020 publication", "journal-ref": null, "doi": "10.23919/DATE48585.2020.9116464", "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation in-memory is a promising non-von Neumann approach aiming at\ncompletely diminishing the data transfer to and from the memory subsystem.\nAlthough a lot of architectures have been proposed, compiler support for such\narchitectures is still lagging behind. In this paper, we close this gap by\nproposing an end-to-end compilation flow for in-memory computing based on the\nLLVM compiler infrastructure. Starting from sequential code, our approach\nautomatically detects, optimizes, and offloads kernels suitable for in-memory\nacceleration. We demonstrate our compiler tool-flow on the PolyBench/C\nbenchmark suite and evaluate the benefits of our proposed in-memory\narchitecture simulated in Gem5 by comparing it with a state-of-the-art von\nNeumann architecture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:49:33 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Vadivel", "Kanishkan", ""], ["Chelini", "Lorenzo", ""], ["BanaGozar", "Ali", ""], ["Singh", "Gagandeep", ""], ["Corda", "Stefano", ""], ["Jordans", "Roel", ""], ["Corporaal", "Henk", ""]]}, {"id": "2007.00156", "submitter": "Saeed Rashidi", "authors": "Saeed Rashidi, Srinivas Sridharan, Sudarshan Srinivasan, Matthew\n  Denton, Tushar Krishna", "title": "Efficient Communication Acceleration for Next-Gen Scale-up Deep Learning\n  Training Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) training platforms are built by interconnecting multiple\nDL accelerators (e.g., GPU/TPU) via fast, customized interconnects. As the size\nof DL models and the compute efficiency of the accelerators has continued to\nincrease, there has also been a corresponding steady increase in the bandwidth\nof these interconnects.Systems today provide 100s of gigabytes (GBs) of\ninter-connect bandwidth via a mix of solutions such as Multi-Chip packaging\nmodules (MCM) and proprietary interconnects(e.g., NVlink) that together from\nthe scale-up network of accelerators. However, as we identify in this work, a\nsignificant portion of this bandwidth goes under-utilized. This is because(i)\nusing compute cores for executing collective operations such as all-reduce\ndecreases overall compute efficiency, and(ii) there is memory bandwidth\ncontention between the accesses for arithmetic operations vs those for\ncollectives, and(iii) there are significant internal bus congestions that\nincrease the latency of communication operations. To address this challenge, we\npropose a novel microarchitecture, calledAccelerator Collectives Engine(ACE),\nforDL collective communication offload. ACE is a smart net-work interface (NIC)\ntuned to cope with the high-bandwidth and low latency requirements of scale-up\nnetworks and is able to efficiently drive the various scale-up network\nsystems(e.g. switch-based or point-to-point topologies). We evaluate the\nbenefits of the ACE with micro-benchmarks (e.g. single collective performance)\nand popular DL models using an end-to-end DL training simulator. For modern DL\nworkloads, ACE on average increases the net-work bandwidth utilization by\n1.97X, resulting in 2.71X and 1.44X speedup in iteration time for ResNet-50 and\nGNMT, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:56:41 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:31:50 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:04:50 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Rashidi", "Saeed", ""], ["Sridharan", "Srinivas", ""], ["Srinivasan", "Sudarshan", ""], ["Denton", "Matthew", ""], ["Krishna", "Tushar", ""]]}, {"id": "2007.00864", "submitter": "Shail Dave", "authors": "Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral\n  Shrivastava, Baoxin Li", "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML\n  Models: A Survey and Insights", "comments": "Accepted for publication in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:08:40 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:41:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dave", "Shail", ""], ["Baghdadi", "Riyadh", ""], ["Nowatzki", "Tony", ""], ["Avancha", "Sasikanth", ""], ["Shrivastava", "Aviral", ""], ["Li", "Baoxin", ""]]}, {"id": "2007.01348", "submitter": "Hasan Unlu", "authors": "Hasan Unlu", "title": "Efficient Neural Network Deployment for Microcontroller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing for neural networks is getting important especially for low\npower applications and offline devices. TensorFlow Lite and PyTorch Mobile were\nreleased for this purpose. But they mainly support mobile devices instead of\nmicrocontroller level yet. Microcontroller support is an emerging area now.\nThere are many approaches to reduce network size and compute load like pruning,\nbinarization and layer manipulation i.e. operator reordering. This paper is\ngoing to explore and generalize convolution neural network deployment for\nmicrocontrollers with two novel optimization proposals offering memory saving\nand compute efficiency in 2D convolutions as well as fully connected layers.\nThe first one is in-place max-pooling, if the stride is greater than or equal\nto pooling kernel size. The second optimization is to use ping-pong buffers\nbetween layers to reduce memory consumption significantly. The memory savings\nand performance will be compared with CMSIS-NN framework developed for ARM\nCortex-M CPUs. The final purpose is to develop a tool consuming PyTorch model\nwith trained network weights, and it turns into an optimized inference\nengine(forward pass) in C/C++ for low memory(kilobyte level) and limited\ncomputing capable microcontrollers.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 19:21:05 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Unlu", "Hasan", ""]]}, {"id": "2007.01377", "submitter": "Somdip Dey Mr.", "authors": "Somdip Dey, Amit Kumar Singh, Xiaohang Wang, and Klaus Dieter\n  McDonald-Maier", "title": "DATE: Defense Against TEmperature Side-Channel Attacks in DVFS Enabled\n  MPSoCs", "comments": "13 pages, 18 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the constant rise in utilizing embedded devices in daily life, side\nchannels remain a challenge to information flow control and security in such\nsystems. One such important security flaw could be exploited through\ntemperature side-channel attacks, where heat dissipation and propagation from\nthe processing elements are observed over time in order to deduce security\nflaws. In our proposed methodology, DATE: Defense Against TEmperature\nside-channel attacks, we propose a novel approach of reducing spatial and\ntemporal thermal gradient, which makes the system more secure against\ntemperature side-channel attacks, and at the same time increases the\nreliability of the device in terms of lifespan. In this paper, we have also\nintroduced a new metric, Thermal-Security-in-Multi-Processors (TSMP), which is\ncapable of quantifying the security against temperature side-channel attacks on\ncomputing systems, and DATE is evaluated to be 139.24% more secure at the most\nfor certain applications than the state-of-the-art, while reducing thermal\ncycle by 67.42% at the most.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 20:41:23 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dey", "Somdip", ""], ["Singh", "Amit Kumar", ""], ["Wang", "Xiaohang", ""], ["McDonald-Maier", "Klaus Dieter", ""]]}, {"id": "2007.01465", "submitter": "Ghasem Pasandi", "authors": "Ghasem Pasandi and Mackenzie Peterson and Moises Herrera and Shahin\n  Nazarian and Massoud Pedram", "title": "Deep-PowerX: A Deep Learning-Based Framework for Low-Power Approximate\n  Logic Synthesis", "comments": null, "journal-ref": null, "doi": "10.1145/3370748.3406555", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at integrating three powerful techniques namely Deep\nLearning, Approximate Computing, and Low Power Design into a strategy to\noptimize logic at the synthesis level. We utilize advances in deep learning to\nguide an approximate logic synthesis engine to minimize the dynamic power\nconsumption of a given digital CMOS circuit, subject to a predetermined error\nrate at the primary outputs. Our framework, Deep-PowerX, focuses on replacing\nor removing gates on a technology-mapped network and uses a Deep Neural Network\n(DNN) to predict error rates at primary outputs of the circuit when a specific\npart of the netlist is approximated. The primary goal of Deep-PowerX is to\nreduce the dynamic power whereas area reduction serves as a secondary\nobjective. Using the said DNN, Deep-PowerX is able to reduce the exponential\ntime complexity of standard approximate logic synthesis to linear time.\nExperiments are done on numerous open source benchmark circuits. Results show\nsignificant reduction in power and area by up to 1.47 times and 1.43 times\ncompared to exact solutions and by up to 22% and 27% compared to\nstate-of-the-art approximate logic synthesis tools while having orders of\nmagnitudes lower run-time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:47:58 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Pasandi", "Ghasem", ""], ["Peterson", "Mackenzie", ""], ["Herrera", "Moises", ""], ["Nazarian", "Shahin", ""], ["Pedram", "Massoud", ""]]}, {"id": "2007.01530", "submitter": "Stefan Mach", "authors": "Stefan Mach, Fabian Schuiki, Florian Zaruba, Luca Benini", "title": "FPnew: An Open-Source Multi-Format Floating-Point Unit Architecture for\n  Energy-Proportional Transprecision Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The slowdown of Moore's law and the power wall necessitates a shift towards\nfinely tunable precision (a.k.a. transprecision) computing to reduce energy\nfootprint. Hence, we need circuits capable of performing floating-point\noperations on a wide range of precisions with high energy-proportionality. We\npresent FPnew, a highly configurable open-source transprecision floating-point\nunit (TP-FPU) capable of supporting a wide range of standard and custom FP\nformats. To demonstrate the flexibility and efficiency of FPnew in\ngeneral-purpose processor architectures, we extend the RISC-V ISA with\noperations on half-precision, bfloat16, and an 8bit FP format, as well as SIMD\nvectors and multi-format operations. Integrated into a 32-bit RISC-V core, our\nTP-FPU can speed up execution of mixed-precision applications by 1.67x w.r.t.\nan FP32 baseline, while maintaining end-to-end precision and reducing system\nenergy by 37%. We also integrate FPnew into a 64-bit RISC-V core, supporting\nfive FP formats on scalars or 2, 4, or 8-way SIMD vectors. For this core, we\nmeasured the silicon manufactured in Globalfoundries 22FDX technology across a\nwide voltage range from 0.45V to 1.2V. The unit achieves leading-edge measured\nenergy efficiencies between 178 Gflop/sW (on FP64) and 2.95 Tflop/sW (on 8-bit\nmini-floats), and a performance between 3.2 Gflop/s and 25.3 Gflop/s.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 07:38:00 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Mach", "Stefan", ""], ["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Benini", "Luca", ""]]}, {"id": "2007.01820", "submitter": "Arash Fouman", "authors": "Arash Fouman Ajirlou, Inna Partin-Vaisband", "title": "A Machine Learning Pipeline Stage for Adaptive Frequency Adjustment", "comments": "12 pages, 8 figures, 5 tables, IEEE transaction on computers. arXiv\n  admin note: substantial text overlap with arXiv:2006.07450", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A machine learning (ML) design framework is proposed for adaptively adjusting\nclock frequency based on propagation delay of individual instructions. A random\nforest model is trained to classify propagation delays in real time, utilizing\ncurrent operation type, current operands, and computation history as ML\nfeatures. The trained model is implemented in Verilog as an additional pipeline\nstage within a baseline processor. The modified system is experimentally tested\nat the gate level in 45 nm CMOS technology, exhibiting a speedup of 70% and\nenergy reduction of 30% with coarse-grained ML classification. A speedup of 89%\nis demonstrated with finer granularities with 15.5% reduction in energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:55:08 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Ajirlou", "Arash Fouman", ""], ["Partin-Vaisband", "Inna", ""]]}, {"id": "2007.01948", "submitter": "George Floros Ph.D.", "authors": "Chrysostomos Chatzigeorgiou, Dimitrios Garyfallou, George Floros,\n  Nestor Evmorfopoulos, and George Stamoulis", "title": "Exploiting Extended Krylov Subspace for the Reduction of Regular and\n  Singular Circuit Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, Model Order Reduction (MOR) has become key enabler\nfor the efficient simulation of large circuit models. MOR techniques based on\nmoment-matching are well established due to their simplicity and computational\nperformance in the reduction process. However, moment-matching methods based on\nthe ordinary Krylov subspace are usually inadequate to accurately approximate\nthe original circuit behavior. In this paper, we present a moment-matching\nmethod which is based on the extended Krylov subspace and exploits the\nsuperposition property in order to deal with many terminals. The proposed\nmethod can handle large-scale regular and singular circuits and generate\naccurate and efficient reduced-order models for circuit simulation.\nExperimental results on industrial IBM power grids demonstrate that our method\nachieves an error reduction up to 83.69% over a standard Krylov subspace\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 21:55:20 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 17:36:29 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Chatzigeorgiou", "Chrysostomos", ""], ["Garyfallou", "Dimitrios", ""], ["Floros", "George", ""], ["Evmorfopoulos", "Nestor", ""], ["Stamoulis", "George", ""]]}, {"id": "2007.02210", "submitter": "Anup Das", "authors": "Shihao Song and Anup Das", "title": "A Case for Lifetime Reliability-Aware Neuromorphic Computing", "comments": "4 pages, 6 figures, accepted at MWCAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Neuromorphic computing with non-volatile memory (NVM) can significantly\nimprove performance and lower energy consumption of machine learning tasks\nimplemented using spike-based computations and bio-inspired learning\nalgorithms. High voltages required to operate certain NVMs such as phase-change\nmemory (PCM) can accelerate aging in a neuron's CMOS circuit, thereby reducing\nthe lifetime of neuromorphic hardware. In this work, we evaluate the long-term,\ni.e., lifetime reliability impact of executing state-of-the-art machine\nlearning tasks on a neuromorphic hardware, considering failure models such as\nnegative bias temperature instability (NBTI) and time-dependent dielectric\nbreakdown (TDDB). Based on such formulation, we show the\nreliability-performance trade-off obtained due to periodic relaxation of\nneuromorphic circuits, i.e., a stop-and-go style of neuromorphic computing.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 23:53:13 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""]]}, {"id": "2007.02242", "submitter": "Wo-Tak Wu", "authors": "Wo-Tak Wu", "title": "A Ring Router Microarchitecture for NoCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-on-Chip (NoC) has become a popular choice for connecting a large\nnumber of processing cores in chip multiprocessor design. In a conventional NoC\ndesign, most of the area in the router is occupied by the buffers and the\ncrossbar switch. These two components also consume the majority of the router's\npower. Much of the research in NoC has been based on the conventional router\nmicroarchitecture. We propose a novel router microarchitecture that treats the\nrouter itself as a small network of the ring topology. It eliminates the large\ncrossbar switch in the conventional design. In addition, network latency is\nmuch reduced. Simulation and circuit synthesis show that the proposed\nmicroarchitecture can reduce the latency, area and power by 53%, 34% and 27%,\nrespectively, compared to the conventional design.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:51:00 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wu", "Wo-Tak", ""]]}, {"id": "2007.03152", "submitter": "Jason Lowe-Power", "authors": "Jason Lowe-Power, Abdul Mutaal Ahmad, Ayaz Akram, Mohammad Alian, Rico\n  Amslinger, Matteo Andreozzi, Adri\\`a Armejach, Nils Asmussen, Brad Beckmann,\n  Srikant Bharadwaj, Gabe Black, Gedare Bloom, Bobby R. Bruce, Daniel Rodrigues\n  Carvalho, Jeronimo Castrillon, Lizhong Chen, Nicolas Derumigny, Stephan\n  Diestelhorst, Wendy Elsasser, Carlos Escuin, Marjan Fariborz, Amin\n  Farmahini-Farahani, Pouya Fotouhi, Ryan Gambord, Jayneel Gandhi, Dibakar\n  Gope, Thomas Grass, Anthony Gutierrez, Bagus Hanindhito, Andreas Hansson,\n  Swapnil Haria, Austin Harris, Timothy Hayes, Adrian Herrera, Matthew\n  Horsnell, Syed Ali Raza Jafri, Radhika Jagtap, Hanhwi Jang, Reiley Jeyapaul,\n  Timothy M. Jones, Matthias Jung, Subash Kannoth, Hamidreza Khaleghzadeh,\n  Yuetsu Kodama, Tushar Krishna, Tommaso Marinelli, Christian Menard, Andrea\n  Mondelli, Miquel Moreto, Tiago M\\\"uck, Omar Naji, Krishnendra Nathella, Hoa\n  Nguyen, Nikos Nikoleris, Lena E. Olson, Marc Orr, Binh Pham, Pablo Prieto,\n  Trivikram Reddy, Alec Roelke, Mahyar Samani, Andreas Sandberg, Javier\n  Setoain, Boris Shingarov, Matthew D. Sinclair, Tuan Ta, Rahul Thakur, Giacomo\n  Travaglini, Michael Upton, Nilay Vaish, Ilias Vougioukas, William Wang,\n  Zhengrong Wang, Norbert Wehn, Christian Weis, David A. Wood, Hongil Yoon,\n  \\'Eder F. Zulian", "title": "The gem5 Simulator: Version 20.0+", "comments": "Source, comments, and feedback:\n  https://github.com/darchr/gem5-20-paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The open-source and community-supported gem5 simulator is one of the most\npopular tools for computer architecture research. This simulation\ninfrastructure allows researchers to model modern computer hardware at the\ncycle level, and it has enough fidelity to boot unmodified Linux-based\noperating systems and run full applications for multiple architectures\nincluding x86, Arm, and RISC-V. The gem5 simulator has been under active\ndevelopment over the last nine years since the original gem5 release. In this\ntime, there have been over 7500 commits to the codebase from over 250 unique\ncontributors which have improved the simulator by adding new features, fixing\nbugs, and increasing the code quality. In this paper, we give and overview of\ngem5's usage and features, describe the current state of the gem5 simulator,\nand enumerate the major changes since the initial release of gem5. We also\ndiscuss how the gem5 simulator has transitioned to a formal governance model to\nenable continued improvement and community support for the next 20 years of\ncomputer architecture research.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 01:15:13 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 02:35:48 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Lowe-Power", "Jason", ""], ["Ahmad", "Abdul Mutaal", ""], ["Akram", "Ayaz", ""], ["Alian", "Mohammad", ""], ["Amslinger", "Rico", ""], ["Andreozzi", "Matteo", ""], ["Armejach", "Adri\u00e0", ""], ["Asmussen", "Nils", ""], ["Beckmann", "Brad", ""], ["Bharadwaj", "Srikant", ""], ["Black", "Gabe", ""], ["Bloom", "Gedare", ""], ["Bruce", "Bobby R.", ""], ["Carvalho", "Daniel Rodrigues", ""], ["Castrillon", "Jeronimo", ""], ["Chen", "Lizhong", ""], ["Derumigny", "Nicolas", ""], ["Diestelhorst", "Stephan", ""], ["Elsasser", "Wendy", ""], ["Escuin", "Carlos", ""], ["Fariborz", "Marjan", ""], ["Farmahini-Farahani", "Amin", ""], ["Fotouhi", "Pouya", ""], ["Gambord", "Ryan", ""], ["Gandhi", "Jayneel", ""], ["Gope", "Dibakar", ""], ["Grass", "Thomas", ""], ["Gutierrez", "Anthony", ""], ["Hanindhito", "Bagus", ""], ["Hansson", "Andreas", ""], ["Haria", "Swapnil", ""], ["Harris", "Austin", ""], ["Hayes", "Timothy", ""], ["Herrera", "Adrian", ""], ["Horsnell", "Matthew", ""], ["Jafri", "Syed Ali Raza", ""], ["Jagtap", "Radhika", ""], ["Jang", "Hanhwi", ""], ["Jeyapaul", "Reiley", ""], ["Jones", "Timothy M.", ""], ["Jung", "Matthias", ""], ["Kannoth", "Subash", ""], ["Khaleghzadeh", "Hamidreza", ""], ["Kodama", "Yuetsu", ""], ["Krishna", "Tushar", ""], ["Marinelli", "Tommaso", ""], ["Menard", "Christian", ""], ["Mondelli", "Andrea", ""], ["Moreto", "Miquel", ""], ["M\u00fcck", "Tiago", ""], ["Naji", "Omar", ""], ["Nathella", "Krishnendra", ""], ["Nguyen", "Hoa", ""], ["Nikoleris", "Nikos", ""], ["Olson", "Lena E.", ""], ["Orr", "Marc", ""], ["Pham", "Binh", ""], ["Prieto", "Pablo", ""], ["Reddy", "Trivikram", ""], ["Roelke", "Alec", ""], ["Samani", "Mahyar", ""], ["Sandberg", "Andreas", ""], ["Setoain", "Javier", ""], ["Shingarov", "Boris", ""], ["Sinclair", "Matthew D.", ""], ["Ta", "Tuan", ""], ["Thakur", "Rahul", ""], ["Travaglini", "Giacomo", ""], ["Upton", "Michael", ""], ["Vaish", "Nilay", ""], ["Vougioukas", "Ilias", ""], ["Wang", "William", ""], ["Wang", "Zhengrong", ""], ["Wehn", "Norbert", ""], ["Weis", "Christian", ""], ["Wood", "David A.", ""], ["Yoon", "Hongil", ""], ["Zulian", "\u00c9der F.", ""]]}, {"id": "2007.03269", "submitter": "Yashwant Temburu Kumar", "authors": "Prathmesh Sawant, Yashwant Temburu, Mandar Datar, Imran Ahmed, Vinayak\n  Shriniwas and Sachin Patkar", "title": "Single Storage Semi-Global Matching for Real Time Depth Processing", "comments": "10 pages, Published in National Conference on Computer Vision,\n  Pattern Recognition, Image Processing and Graphics(NCVPRIPG) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth-map is the key computation in computer vision and robotics. One of the\nmost popular approach is via computation of disparity-map of images obtained\nfrom Stereo Camera. Semi Global Matching (SGM) method is a popular choice for\ngood accuracy with reasonable computation time. To use such compute-intensive\nalgorithms for real-time applications such as for autonomous aerial vehicles,\nblind Aid, etc. acceleration using GPU, FPGA is necessary. In this paper, we\nshow the design and implementation of a stereo-vision system, which is based on\nFPGA-implementation of More Global Matching(MGM). MGM is a variant of SGM. We\nuse 4 paths but store a single cumulative cost value for a corresponding pixel.\nOur stereo-vision prototype uses Zedboard containing an ARM-based Zynq-SoC,\nZED-stereo-camera / ELP stereo-camera / Intel RealSense D435i, and VGA for\nvisualization. The power consumption attributed to the custom FPGA-based\nacceleration of disparity map computation required for depth-map is just 0.72\nwatt. The update rate of the disparity map is realistic 10.5 fps.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:12:25 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sawant", "Prathmesh", ""], ["Temburu", "Yashwant", ""], ["Datar", "Mandar", ""], ["Ahmed", "Imran", ""], ["Shriniwas", "Vinayak", ""], ["Patkar", "Sachin", ""]]}, {"id": "2007.04292", "submitter": "Md Saiful Arefin Mojumder", "authors": "Saiful A. Mojumder, Yifan Sun, Leila Delshadtehrani, Yenai Ma,\n  Trinayan Baruah, Jos\\'e L. Abell\\'an, John Kim, David Kaeli, Ajay Joshi", "title": "HALCONE : A Hardware-Level Timestamp-based Cache Coherence Scheme for\n  Multi-GPU systems", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While multi-GPU (MGPU) systems are extremely popular for compute-intensive\nworkloads, several inefficiencies in the memory hierarchy and data movement\nresult in a waste of GPU resources and difficulties in programming MGPU\nsystems. First, due to the lack of hardware-level coherence, the MGPU\nprogramming model requires the programmer to replicate and repeatedly transfer\ndata between the GPUs' memory. This leads to inefficient use of precious GPU\nmemory. Second, to maintain coherency across an MGPU system, transferring data\nusing low-bandwidth and high-latency off-chip links leads to degradation in\nsystem performance. Third, since the programmer needs to manually maintain data\ncoherence, the programming of an MGPU system to maximize its throughput is\nextremely challenging. To address the above issues, we propose a novel\nlightweight timestamp-based coherence protocol, HALCONE, for MGPU systems and\nmodify the memory hierarchy of the GPUs to support physically shared memory.\nHALCONE replaces the Compute Unit (CU) level logical time counters with cache\nlevel logical time counters to reduce coherence traffic. Furthermore, HALCONE\nintroduces a novel timestamp storage unit (TSU) with no additional performance\noverhead in the main memory to perform coherence actions. Our proposed HALCONE\nprotocol maintains the data coherence in the memory hierarchy of the MGPU with\nminimal performance overhead (less than 1\\%). Using a set of standard MGPU\nbenchmarks, we observe that a 4-GPU MGPU system with shared memory and HALCONE\nperforms, on average, 4.6$\\times$ and 3$\\times$ better than a 4-GPU MGPU system\nwith existing RDMA and with the recently proposed HMG coherence protocol,\nrespectively. We demonstrate the scalability of HALCONE using different GPU\ncounts (2, 4, 8, and 16) and different CU counts (32, 48, and 64 CUs per GPU)\nfor 11 standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:42:24 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Mojumder", "Saiful A.", ""], ["Sun", "Yifan", ""], ["Delshadtehrani", "Leila", ""], ["Ma", "Yenai", ""], ["Baruah", "Trinayan", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Kim", "John", ""], ["Kaeli", "David", ""], ["Joshi", "Ajay", ""]]}, {"id": "2007.04552", "submitter": "Yifan Yuan", "authors": "Yifan Yuan, Mohammad Alian, Yipeng Wang, Ilia Kurakin, Ren Wang,\n  Charlie Tai, Nam Sung Kim", "title": "IOCA: High-Speed I/O-Aware LLC Management for Network-Centric\n  Multi-Tenant Platform", "comments": "Accepted by the 48th IEEE/ACM International Symposium on Computer\n  Architecture (ISCA'21). The title is \"Don't Forget the I/O When Allocating\n  Your LLC\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern server CPUs, last-level cache (LLC) is a critical hardware resource\nthat exerts significant influence on the performance of the workloads, and how\nto manage LLC is a key to the performance isolation and QoS in the cloud with\nmulti-tenancy. In this paper, we argue that besides CPU cores, high-speed\nnetwork I/O is also important for LLC management. This is because of an Intel\narchitectural innovation -- Data Direct I/O (DDIO) -- that directly injects the\ninbound I/O traffic to (part of) the LLC instead of the main memory. We\nsummarize two problems caused by DDIO and show that (1) the default DDIO\nconfiguration may not always achieve optimal performance, (2) DDIO can decrease\nthe performance of non-I/O workloads which share LLC with it by as high as 32%.\n  We then present IOCA, the first LLC management mechanism for network-centric\nplatforms that treats the I/O as the first-class citizen. IOCA monitors and\nanalyzes the performance of the cores, LLC, and DDIO using CPU's hardware\nperformance counters, and adaptively adjusts the number of LLC ways for DDIO or\nthe tenants that demand more LLC capacity. In addition, IOCA dynamically\nchooses the tenants that share its LLC resource with DDIO, to minimize the\nperformance interference by both the tenants and the I/O. Our experiments with\nmultiple microbenchmarks and real-world applications in two major end-host\nnetwork models demonstrate that IOCA can effectively reduce the performance\ndegradation caused by DDIO, with minimal overhead.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:45:54 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:14:42 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yuan", "Yifan", ""], ["Alian", "Mohammad", ""], ["Wang", "Yipeng", ""], ["Kurakin", "Ilia", ""], ["Wang", "Ren", ""], ["Tai", "Charlie", ""], ["Kim", "Nam Sung", ""]]}, {"id": "2007.05596", "submitter": "Mohammad Mohammadinodoushan", "authors": "Mohammad Mohammadinodoushan", "title": "Hardware Implementation of Keyless Encryption Scheme for Internet of\n  Things Based on Image of Memristors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is rapidly increasing the number of connected\ndevices. This causes new concerns towards solutions for authenticating numerous\nIoT devices. Most of these devices are resource-constrained. Therefore, the use\nof long-secret keys, in traditional cryptography schemes can be hard to\nimplement. Also, the key generation, distribution, and storage are very\ncomplex. Moreover, the goal of many reported cyber-attacks is accessing the\nkey. Therefore, researchers have shown an increased interest in designing\nkeyless encryption schemes recently. In this report, we are going to explain\nthe details of the implementation of the keyless protocol by taking advantage\nof known technology modules such as microcontrollers (MCU), and hash functions.\nPhysical Unclonable Functions (PUFs) have been used in many cryptographic\napplications such as Password Management Systems, key exchange, Key Generation.\nIn this report, we are going to explain the details of the hardware\nimplementation of keyless encryption in the MCU. Different kinds of memristors\nhave been used in the past. In this work, a look-up-table containing memristor\ncells value at the various current levels is used since the physical component\nis unavailable yet. The hardware that is used to implement the system is an\nevaluation-board of SAMV71 MCU, which is used to implement the control system\nand hardware hashing.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 20:11:58 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 01:18:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Mohammadinodoushan", "Mohammad", ""]]}, {"id": "2007.05657", "submitter": "Mostafa Rahimi Azghadi", "authors": "Mostafa Rahimi Azghadi, Corey Lammie, Jason K. Eshraghian, Melika\n  Payvand, Elisa Donati, Bernabe Linares-Barranco, and Giacomo Indiveri", "title": "Hardware Implementation of Deep Network Accelerators Towards Healthcare\n  and Biomedical Applications", "comments": "Accepted by IEEE Transactions on Biomedical Circuits and Systems (21\n  pages, 10 figures, 5 tables)", "journal-ref": "IEEE Transactions on Biomedical Circuits and Systems, 2020", "doi": "10.1109/TBCAS.2020.3036081", "report-no": null, "categories": "cs.AR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of dedicated Deep Learning (DL) accelerators and neuromorphic\nprocessors has brought on new opportunities for applying both Deep and Spiking\nNeural Network (SNN) algorithms to healthcare and biomedical applications at\nthe edge. This can facilitate the advancement of medical Internet of Things\n(IoT) systems and Point of Care (PoC) devices. In this paper, we provide a\ntutorial describing how various technologies including emerging memristive\ndevices, Field Programmable Gate Arrays (FPGAs), and Complementary Metal Oxide\nSemiconductor (CMOS) can be used to develop efficient DL accelerators to solve\na wide variety of diagnostic, pattern recognition, and signal processing\nproblems in healthcare. Furthermore, we explore how spiking neuromorphic\nprocessors can complement their DL counterparts for processing biomedical\nsignals. The tutorial is augmented with case studies of the vast literature on\nneural network and neuromorphic hardware as applied to the healthcare domain.\nWe benchmark various hardware platforms by performing a sensor fusion signal\nprocessing task combining electromyography (EMG) signals with computer vision.\nComparisons are made between dedicated neuromorphic processors and embedded AI\naccelerators in terms of inference latency and energy. Finally, we provide our\nanalysis of the field and share a perspective on the advantages, disadvantages,\nchallenges, and opportunities that various accelerators and neuromorphic\nprocessors introduce to healthcare and biomedical domains.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 01:14:08 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 05:45:00 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Azghadi", "Mostafa Rahimi", ""], ["Lammie", "Corey", ""], ["Eshraghian", "Jason K.", ""], ["Payvand", "Melika", ""], ["Donati", "Elisa", ""], ["Linares-Barranco", "Bernabe", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2007.06201", "submitter": "Rourab Paul", "authors": "Rourab Paul, Nimisha Ghosh, Amlan Chakrabarti, Prasant Mahapatra", "title": "The Blockchain Based Auditor on Secret key Life Cycle in Reconfigurable\n  Platform", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing sophistication of cyber attacks, vulnerabilities in high\ncomputing systems and increasing dependency on cryptography to protect our\ndigital data make it more important to keep secret keys safe and secure. Few\nmajor issues on secret keys like incorrect use of keys, inappropriate storage\nof keys, inadequate protection of keys, insecure movement of keys, lack of\naudit logging, insider threats and non-destruction of keys can compromise the\nwhole security system dangerously. In this article, we have proposed and\nimplemented an isolated secret key memory which can log life cycle of secret\nkeys cryptographically using blockchain (BC) technology. We have also\nimplemented a special custom bus interconnect which receives custom crypto\ninstruction from Processing Element (PE). During the execution of crypto\ninstructions, the architecture assures that secret key will never come in the\nprocessor area and the movement of secret keys to various crypto core is\nrecorded cryptographically after the proper authentication process controlled\nby proposed hardware based BC. To the best of our knowledge, this is the first\nwork which uses blockchain based solution to address the issues of the life\ncycle of the secret keys in hardware platform. The additional cost of resource\nusage and timing complexity we spent to implement the proposed idea is very\nnominal. We have used Xilinx Vivado EDA tool and Artix 7 FPGA board.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 06:03:11 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Paul", "Rourab", ""], ["Ghosh", "Nimisha", ""], ["Chakrabarti", "Amlan", ""], ["Mahapatra", "Prasant", ""]]}, {"id": "2007.06563", "submitter": "James Garland", "authors": "James Garland, David Gregg", "title": "HOBFLOPS CNNs: Hardware Optimized Bitslice-Parallel Floating-Point\n  Operations for Convolutional Neural Networks", "comments": "14 pages, 3 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are typically trained using 16- or\n32-bit floating-point (FP) and researchers show that low-precision\nfloating-point (FP) can be highly effective for inference. Low-precision FP can\nbe implemented in field programmable gate array (FPGA) and application-specific\nintegrated circuit (ASIC) accelerators, but existing processors do not\ngenerally support custom precision FP. We propose hardware optimized\nbitslice-parallel floating-point operators (HOBFLOPS), a method of generating\nefficient custom-precision emulated bitslice-parallel software FP arithmetic.\nWe generate custom-precision FP routines optimized using a hardware synthesis\ndesign flow to create circuits. We provide standard cell libraries matching the\nbitwise operations on the target microprocessor architecture, and a\ncode-generator to translate the hardware circuits to bitslice software\nequivalents. We exploit bitslice parallelism to create a very wide (32-512\nelement) vectorized convolutional neural network (CNN) convolution. Hardware\noptimized bitslice-parallel floating-point operators (HOBFLOPS)\nmultiply-accumulate (MAC) performance in CNN convolution on Arm and Intel\nprocessors are compared to Berkeley's SoftFP16 equivalent MAC. HOBFLOPS16\noutperforms SoftFP16 by 8x on Intel AVX512. HOBFLOPS offers arbitrary-precision\nFP with custom range and precision e.g., HOBFLOPS9 performs at 6x the\nperformance of HOBFLOPS16 on Arm Neon. HOBFLOPS allows researchers to prototype\ndifferent levels of custom FP precision in the arithmetic of software CNN\naccelerators. Furthermore, HOBFLOPS fast custom-precision FP CNNs may be\nvaluable in cases where memory bandwidth is limited.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 00:37:35 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 11:56:52 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 16:52:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Garland", "James", ""], ["Gregg", "David", ""]]}, {"id": "2007.06665", "submitter": "Richard Afoakwa", "authors": "Richard Afoakwa, Yiqiao Zhang, Uday Kumar Reddy Vengalam, Zeljko\n  Ignjatovic, and Michael Huang", "title": "CMOS Ising Machines with Coupled Bistable Nodes", "comments": "11 pages, 12 figures, 2 tables, 5 sections,", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ising machines use physics to naturally guide a dynamical system towards an\noptimal state which can be read out as a heuristical solution to a\ncombinatorial optimization problem. Such designs that use nature as a computing\nmechanism can lead to higher performance and/or lower operation costs. Quantum\nannealers are a prominent example of such efforts. However, existing Ising\nmachines are generally bulky and energy intensive. Such disadvantages might\nlead to intrinsic advantages at some larger scale in the future. But for now,\nintegrated electronic designs allow more immediate applications. We propose one\nsuch design that uses bistable nodes, coupled with programmable and variable\nstrengths. The design is fully CMOS compatible for on-chip applications and\ndemonstrates competitive solution quality and significantly superior execution\ntime and energy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:12:50 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Afoakwa", "Richard", ""], ["Zhang", "Yiqiao", ""], ["Vengalam", "Uday Kumar Reddy", ""], ["Ignjatovic", "Zeljko", ""], ["Huang", "Michael", ""]]}, {"id": "2007.07131", "submitter": "Albert Segura", "authors": "Albert Segura, Jose-Maria Arnau, Antonio Gonzalez", "title": "Irregular Accesses Reorder Unit: Improving GPGPU Memory Coalescing for\n  Graph-Based Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": "UPC-DAC-RR-ARCO-2020-1", "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPGPU architectures have become established as the dominant parallelization\nand performance platform achieving exceptional popularization and empowering\ndomains such as regular algebra, machine learning, image detection and\nself-driving cars. However, irregular applications struggle to fully realize\nGPGPU performance as a result of control flow divergence and memory divergence\ndue to irregular memory access patterns.\n  To ameliorate these issues, programmers are obligated to carefully consider\narchitecture features and devote significant efforts to modify the algorithms\nwith complex optimization techniques, which shift programmers priorities yet\nstruggle to quell the shortcomings. We show that in graph-based GPGPU irregular\napplications these inefficiencies prevail, yet we find that it is possible to\nrelax the strict relationship between thread and data processed to empower new\noptimizations.\n  Based on this key idea, we propose the Irregular accesses Reorder Unit (IRU),\na novel hardware extension tightly integrated in the GPGPU pipeline. The IRU\nreorders data processed by the threads on irregular accesses which\nsignificantly improves memory coalescing, and allows increased performance and\nenergy efficiency. Additionally, the IRU is capable of filtering and merging\nduplicated irregular access which further improves graph-based irregular\napplications. Programmers can easily utilize the IRU with a simple API, or\ncompiler optimized generated code with the extended ISA instructions provided.\n  We evaluate our proposal for state-of-the-art graph-based algorithms and a\nwide selection of applications. Results show that the IRU achieves a memory\ncoalescing improvement of 1.32x and a 46% reduction in the overall traffic in\nthe memory hierarchy, which results in 1.33x and 13% improvement in performance\nand energy savings respectively, while incurring in a small 5.6% area overhead.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:51:06 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Segura", "Albert", ""], ["Arnau", "Jose-Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "2007.07328", "submitter": "Thibaud Tonnellier", "authors": "Syed Mohsin Abbas, Thibaud Tonnellier, Furkan Ercan, and Warren J.\n  Gross", "title": "High-Throughput VLSI Architecture for GRAND", "comments": "6 pages, 6 figures, submitted to SiPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guessing Random Additive Noise Decoding (GRAND) is a recently proposed\nuniversal decoding algorithm for linear error correcting codes. Since GRAND\ndoes not depend on the structure of the code, it can be used for any code\nencountered in contemporary communication standards or may even be used for\nrandom linear network coding. This property makes this new algorithm\nparticularly appealing. Instead of trying to decode the received vector, GRAND\nattempts to identify the noise that corrupted the codeword. To that end, GRAND\nrelies on the generation of test error patterns that are successively applied\nto the received vector. In this paper, we propose the first hardware\narchitecture for the GRAND algorithm. Considering GRAND with ABandonment\n(GRANDAB) that limits the number of test patterns, the proposed architecture\nonly needs $2+\\sum_{i=2}^{n} \\left\\lfloor\\frac{i}{2}\\right\\rfloor$ time steps\nto perform the $\\sum_{i=1}^3 \\binom{n}{i}$ queries required when $\\text{AB}=3$.\nFor a code length of $128$, our proposed hardware architecture demonstrates\nonly a fraction ($1.2\\%$) of the total number of performed queries as time\nsteps. Synthesis result using TSMC 65nm CMOS technology shows that average\nthroughputs of $32$ Gbps to $64$ Gbps can be achieved at an SNR of $10$ dB for\na code length of $128$ and code rates rate higher than $0.75$, transmitted over\nan AWGN channel. Comparisons with a decoder tailored for a $(79,64)$ BCH code\nshow that the proposed architecture can achieve a slightly higher average\nthroughput at high SNRs, while obtaining the same decoding performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 19:44:42 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Abbas", "Syed Mohsin", ""], ["Tonnellier", "Thibaud", ""], ["Ercan", "Furkan", ""], ["Gross", "Warren J.", ""]]}, {"id": "2007.07425", "submitter": "Iris Bahar", "authors": "Yanqi Liu, Giuseppe Calderoni, R. Iris Bahar", "title": "Hardware Acceleration of Monte-Carlo Sampling for Energy Efficient\n  Robust Robot Manipulation", "comments": "7 pages. To appear in the International Conference on\n  Field-Programmable Logic and Applications (FPL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms based on Monte-Carlo sampling have been widely adapted in robotics\nand other areas of engineering due to their performance robustness. However,\nthese sampling-based approaches have high computational requirements, making\nthem unsuitable for real-time applications with tight energy constraints. In\nthis paper, we investigate 6 degree-of-freedom (6DoF) pose estimation for robot\nmanipulation using this method, which uses rendering combined with sequential\nMonte-Carlo sampling. While potentially very accurate, the significant\ncomputational complexity of the algorithm makes it less attractive for mobile\nrobots, where runtime and energy consumption are tightly constrained. To\naddress these challenges, we develop a novel hardware implementation of\nMonte-Carlo sampling on an FPGA with lower computational complexity and memory\nusage, while achieving high parallelism and modularization. Our results show\n12X-21X improvements in energy efficiency over low-power and high-end GPU\nimplementations, respectively. Moreover, we achieve real time performance\nwithout compromising accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 01:28:36 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Liu", "Yanqi", ""], ["Calderoni", "Giuseppe", ""], ["Bahar", "R. Iris", ""]]}, {"id": "2007.07539", "submitter": "Andreas Vogel", "authors": "Kyaw L. Oo, Andreas Vogel", "title": "Accelerating Geometric Multigrid Preconditioning with Half-Precision\n  Arithmetic on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the hardware support for half-precision arithmetic on NVIDIA V100 GPUs,\nhigh-performance computing applications can benefit from lower precision at\nappropriate spots to speed up the overall execution time. In this paper, we\ninvestigate a mixed-precision geometric multigrid method to solve large sparse\nsystems of equations stemming from discretization of elliptic PDEs. While the\nfinal solution is always computed with high-precision accuracy, an iterative\nrefinement approach with multigrid preconditioning in lower precision and\nresiduum scaling is employed. We compare the FP64 baseline for Poisson's\nequation to purely FP16 multigrid preconditioning and to the employment of\nFP16-FP32-FP64 combinations within a mesh hierarchy. While the iteration count\nis almost not affected by using lower accuracy, the solver runtime is\nconsiderably decreased due to the reduced memory transfer and a speedup of up\nto 2.5x is gained for the overall solver. We investigate the performance of\nselected kernels with the hierarchical Roofline model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 08:27:33 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Oo", "Kyaw L.", ""], ["Vogel", "Andreas", ""]]}, {"id": "2007.07595", "submitter": "Daniel Ritter", "authors": "Jonas Dann, Daniel Ritter, Holger Fr\\\"oning", "title": "Non-Relational Databases on FPGAs: Survey, Design Decisions, Challenges", "comments": "FPGA, hardware acceleration, non-relational databases, graph,\n  document, key-value; 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-relational database systems (NRDS), such as graph, document, key-value,\nand wide-column, have gained much attention in various trending (business)\napplication domains like smart logistics, social network analysis, and medical\napplications, due to their data model variety and scalability. The broad data\nvariety and sheer size of datasets pose unique challenges for the system design\nand runtime (incl. power consumption). While CPU performance scaling becomes\nincreasingly more difficult, we argue that NRDS can benefit from adding field\nprogrammable gate arrays (FPGAs) as accelerators. However, FPGA-accelerated\nNRDS have not been systematically studied, yet.\n  To facilitate understanding of this emerging domain, we explore the fit of\nFPGA acceleration for NRDS with a focus on data model variety. We define the\nterm NRDS class as a group of non-relational database systems supporting the\nsame data model. This survey describes and categorizes the inherent differences\nand non-trivial trade-offs of relevant NRDS classes as well as their\ncommonalities in the context of common design decisions when building such a\nsystem with FPGAs. For example, we found in the literature that for key-value\nstores the FPGA should be placed into the system as a smart network interface\ncard (SmartNIC) to benefit from direct access of the FPGA to the network.\nHowever, more complex data models and processing of other classes (e.g., graph\nand document) commonly require more elaborate near-data or socket accelerator\nplacements where the FPGA respectively has the only or shared access to main\nmemory. Across the different classes, FPGAs can be used as communication layer\nor for acceleration of operators and data access. We close with open research\nand engineering challenges to outline the future of FPGA-accelerated NRDS.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:22:02 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2007.07654", "submitter": "Hayate Okuhara", "authors": "Ahmed Elnaqib, Hayate Okuhara, Taekwang Jang, Davide Rossi and Luca\n  Benini", "title": "A 0.5GHz 0.35mW LDO-Powered Constant-Slope Phase Interpolator with\n  0.22$\\%$ INL", "comments": "in IEEE Transactions on Circuits and Systems II: Express Briefs", "journal-ref": null, "doi": "10.1109/TCSII.2020.3005246", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clock generators are an essential and critical building block of any\ncommunication link, whether it be wired or wireless, and they are increasingly\ncritical given the push for lower I/O power and higher bandwidth in\nSystems-on-Chip (SoCs) for the Internet-of-Things (IoT). One recurrent issue\nwith clock generators is multiple-phase generation, especially for low-power\napplications; several methods of phase generation have been proposed, one of\nwhich is phase interpolation. We propose a phase interpolator (PI) that employs\nthe concept of constant-slope operation. Consequently, a low-power\nhighly-linear operation is coupled with the wide dynamic range (i.e. phase\nwrapping) capabilities of a PI. Furthermore, the PI is powered by a low-dropout\nregulator (LDO) supporting fast transient operation. Implemented in 65-nm CMOS\ntechnology, it consumes 350$\\mu$W at a 1.2-V supply and a 0.5-GHz clock; it\nachieves energy efficiency 4$\\times$-15$\\times$ lower than state-of-the-art\n(SoA) digital-to-time converters (DTCs) and an integral non-linearity (INL) of\n2.5$\\times$-3.1$\\times$ better than SoA PIs, striking a good balance between\nlinearity and energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:28:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Elnaqib", "Ahmed", ""], ["Okuhara", "Hayate", ""], ["Jang", "Taekwang", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "2007.07759", "submitter": "Nazareno Bruschi", "authors": "Nazareno Bruschi, Angelo Garofalo, Francesco Conti, Giuseppe\n  Tagliavini and Davide Rossi", "title": "Enabling Mixed-Precision Quantized Neural Networks in Extreme-Edge\n  Devices", "comments": "4 pages, 6 figures, published in 17th ACM International Conference on\n  Computing Frontiers (CF '20), May 11--13, 2020, Catania, Italy", "journal-ref": null, "doi": "10.1145/3387902.3394038", "report-no": null, "categories": "cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Quantized Neural Networks (QNN) on advanced\nmicrocontrollers requires optimized software to exploit digital signal\nprocessing (DSP) extensions of modern instruction set architectures (ISA). As\nsuch, recent research proposed optimized libraries for QNNs (from 8-bit to\n2-bit) such as CMSIS-NN and PULP-NN. This work presents an extension to the\nPULP-NN library targeting the acceleration of mixed-precision Deep Neural\nNetworks, an emerging paradigm able to significantly shrink the memory\nfootprint of deep neural networks with negligible accuracy loss. The library,\ncomposed of 27 kernels, one for each permutation of input feature maps,\nweights, and output feature maps precision (considering 8-bit, 4-bit and\n2-bit), enables efficient inference of QNN on parallel ultra-low-power (PULP)\nclusters of RISC-V based processors, featuring the RV32IMCXpulpV2 ISA. The\nproposed solution, benchmarked on an 8-cores GAP-8 PULP cluster, reaches peak\nperformance of 16 MACs/cycle on 8 cores, performing 21x to 25x faster than an\nSTM32H7 (powered by an ARM Cortex M7 processor) with 15x to 21x better energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 15:34:53 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bruschi", "Nazareno", ""], ["Garofalo", "Angelo", ""], ["Conti", "Francesco", ""], ["Tagliavini", "Giuseppe", ""], ["Rossi", "Davide", ""]]}, {"id": "2007.07829", "submitter": "Fernando Gehm Moraes", "authors": "Leonardo Rezende Juracy, Matheus Trevisan Moreira, Alexandre de Morais\n  Amory, Fernando Gehm Moraes", "title": "A Survey of Aging Monitors and Reconfiguration Techniques", "comments": "19 pages, 65 references, 7 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CMOS technology scaling makes aging effects an important concern for the\ndesign and fabrication of integrated circuits. Aging deterioration reduces the\nuseful life of a circuit, making it fail earlier. This deterioration can affect\nall portions of a circuit and impacts its performance and reliability.\nContemporary literature shows solutions to monitor and mitigate aging using\nhardware and software monitoring mechanisms and reconfiguration techniques. The\ngoal of this review of the state-of-the-art is to identify existing monitoring\nand reconfiguration solutions for aging. This survey evaluates the aging\nresearch, focusing the years from 2012 to 2019, and proposes a classification\nfor monitors and reconfiguration techniques. Results show that the most common\nmonitor type used for aging detection is to monitor timing errors, and the most\ncommon reconfiguration technique used to deal with aging is voltage scaling.\nFurthermore, most of the literature contributions are in the digital field,\nusing hardware solutions for monitoring aging in circuits. There are few\nliterature contributions in the analog area, being the scope of this survey in\nthe digital domain. By scrutinizing these solutions, this survey points\ndirections for further research and development of aging monitors and\nreconfiguration techniques\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:52:02 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Juracy", "Leonardo Rezende", ""], ["Moreira", "Matheus Trevisan", ""], ["Amory", "Alexandre de Morais", ""], ["Moraes", "Fernando Gehm", ""]]}, {"id": "2007.08622", "submitter": "Nikita Lazarev", "authors": "Nikita Lazarev, Neil Adit, Shaojie Xiang, Zhiru Zhang, and Christina\n  Delimitrou", "title": "Dagger: Towards Efficient RPCs in Cloud Microservices with Near-Memory\n  Reconfigurable NICs", "comments": "4 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud applications are increasingly relying on hundreds of loosely-coupled\nmicroservices to complete user requests that meet an applications end-to-end\nQoS requirements. Communication time between services accounts for a large\nfraction of the end-to-end latency and can introduce performance\nunpredictability and QoS violations. This work presents our early work on\nDagger, a hardware acceleration platform for networking, designed specifically\nwith the unique qualities of microservices in mind. The Dagger architecture\nrelies on an FPGA-based NIC, closely coupled with the processor over a\nconfigurable memory interconnect, designed to offload and accelerate RPC\nstacks. Unlike the traditional cloud systems that use PCIe links as the NIC I/O\ninterface, we leverage memory-interconnected FPGAs as networking devices to\nprovide the efficiency, transparency, and programmability needed for\nfine-grained microservices. We show that this considerably improves CPU\nutilization and performance for cloud RPCs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:43:06 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 08:23:10 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Lazarev", "Nikita", ""], ["Adit", "Neil", ""], ["Xiang", "Shaojie", ""], ["Zhang", "Zhiru", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2007.08707", "submitter": "Zhi Zhang", "authors": "Zhi Zhang, Yueqiang Cheng, Dongxi Liu, Surya Nepal, Zhi Wang, and\n  Yuval Yarom", "title": "PThammer: Cross-User-Kernel-Boundary Rowhammer through Implicit Accesses", "comments": "Preprint of the work accepted at the International Symposium on\n  Microarchitecture (MICRO) 2020. arXiv admin note: text overlap with\n  arXiv:1912.03076", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rowhammer is a hardware vulnerability in DRAM memory, where repeated access\nto memory can induce bit flips in neighboring memory locations. Being a\nhardware vulnerability, rowhammer bypasses all of the system memory protection,\nallowing adversaries to compromise the integrity and confidentiality of data.\nRowhammer attacks have shown to enable privilege escalation, sandbox escape,\nand cryptographic key disclosures. Recently, several proposals suggest\nexploiting the spatial proximity between the accessed memory location and the\nlocation of the bit flip for a defense against rowhammer. These all aim to deny\nthe attacker's permission to access memory locations near sensitive data. In\nthis paper, we question the core assumption underlying these defenses. We\npresent PThammer, a confused-deputy attack that causes accesses to memory\nlocations that the attacker is not allowed to access. Specifically, PThammer\nexploits the address translation process of modern processors, inducing the\nprocessor to generate frequent accesses to protected memory locations. We\nimplement PThammer, demonstrating that it is a viable attack, resulting in a\nsystem compromise (e.g., kernel privilege escalation). We further evaluate the\neffectiveness of proposed software-only defenses showing that PThammer can\novercome those.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:10:53 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 04:12:39 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zhang", "Zhi", ""], ["Cheng", "Yueqiang", ""], ["Liu", "Dongxi", ""], ["Nepal", "Surya", ""], ["Wang", "Zhi", ""], ["Yarom", "Yuval", ""]]}, {"id": "2007.08952", "submitter": "Alfio Di Mauro", "authors": "Alfio Di Mauro, Francesco Conti, Pasquale Davide Schiavone, Davide\n  Rossi, Luca Benini", "title": "Always-On 674uW @ 4GOP/s Error Resilient Binary Neural Networks with\n  Aggressive SRAM Voltage Scaling on a 22nm IoT End-Node", "comments": "Submitted to ISICAS2020 journal special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) have been shown to be robust to random\nbit-level noise, making aggressive voltage scaling attractive as a power-saving\ntechnique for both logic and SRAMs. In this work, we introduce the first fully\nprogrammable IoT end-node system-on-chip (SoC) capable of executing\nsoftware-defined, hardware-accelerated BNNs at ultra-low voltage. Our SoC\nexploits a hybrid memory scheme where error-vulnerable SRAMs are complemented\nby reliable standard-cell memories to safely store critical data under\naggressive voltage scaling. On a prototype in 22nm FDX technology, we\ndemonstrate that both the logic and SRAM voltage can be dropped to 0.5Vwithout\nany accuracy penalty on a BNN trained for the CIFAR-10 dataset, improving\nenergy efficiency by 2.2X w.r.t. nominal conditions. Furthermore, we show that\nthe supply voltage can be dropped to 0.42V (50% of nominal) while keeping more\nthan99% of the nominal accuracy (with a bit error rate ~1/1000). In this\noperating point, our prototype performs 4Gop/s (15.4Inference/s on the CIFAR-10\ndataset) by computing up to 13binary ops per pJ, achieving 22.8 Inference/s/mW\nwhile keeping within a peak power envelope of 674uW - low enough to enable\nalways-on operation in ultra-low power smart cameras, long-lifetime\nenvironmental sensors, and insect-sized pico-drones.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:56:58 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Di Mauro", "Alfio", ""], ["Conti", "Francesco", ""], ["Schiavone", "Pasquale Davide", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "2007.09109", "submitter": "Mauro Olivieri", "authors": "Abdallah Cheikh, Stefano Sordillo, Antonio Mastrandrea, Francesco\n  Menichelli, Giuseppe Scotti, Mauro Olivieri", "title": "Klessydra-T: Designing Vector Coprocessors for Multi-Threaded\n  Edge-Computing Cores", "comments": "Final revision accepted for publication on IEEE Micro Journal", "journal-ref": "IEEE Micro, 2021", "doi": "10.1109/MM.2021.3050962", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation intensive kernels, such as convolutions, matrix multiplication\nand Fourier transform, are fundamental to edge-computing AI, signal processing\nand cryptographic applications. Interleaved-Multi-Threading (IMT) processor\ncores are interesting to pursue energy efficiency and low hardware cost for\nedge-computing, yet they need hardware acceleration schemes to run heavy\ncomputational workloads. Following a vector approach to accelerate\ncomputations, this study explores possible alternatives to implement vector\ncoprocessing units in RISC-V cores, showing the synergy between IMT and\ndata-level parallelism in the target workloads.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:38:46 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 16:01:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Cheikh", "Abdallah", ""], ["Sordillo", "Stefano", ""], ["Mastrandrea", "Antonio", ""], ["Menichelli", "Francesco", ""], ["Scotti", "Giuseppe", ""], ["Olivieri", "Mauro", ""]]}, {"id": "2007.09361", "submitter": "Anish Krishnakumar", "authors": "Anish Krishnakumar, Samet E. Arda, A. Alper Goksoy, Sumit K. Mandal,\n  Umit Y. Ogras, Anderson L. Sartor, Radu Marculescu", "title": "Runtime Task Scheduling using Imitation Learning for Heterogeneous\n  Many-Core Systems", "comments": "14 pages, 12 figures, 8 tables. Accepted for publication in Embedded\n  Systems Week CODES+ISSS 2020 (Special Issue in IEEE TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2020.3012861", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific systems-on-chip, a class of heterogeneous many-core systems,\nare recognized as a key approach to narrow down the performance and\nenergy-efficiency gap between custom hardware accelerators and programmable\nprocessors. Reaching the full potential of these architectures depends\ncritically on optimally scheduling the applications to available resources at\nruntime. Existing optimization-based techniques cannot achieve this objective\nat runtime due to the combinatorial nature of the task scheduling problem. As\nthe main theoretical contribution, this paper poses scheduling as a\nclassification problem and proposes a hierarchical imitation learning\n(IL)-based scheduler that learns from an Oracle to maximize the performance of\nmultiple domain-specific applications. Extensive evaluations with six streaming\napplications from wireless communications and radar domains show that the\nproposed IL-based scheduler approximates an offline Oracle policy with more\nthan 99% accuracy for performance- and energy-based optimization objectives.\nFurthermore, it achieves almost identical performance to the Oracle with a low\nruntime overhead and successfully adapts to new applications, many-core system\nconfigurations, and runtime variations in application characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:20:21 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 23:37:23 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Krishnakumar", "Anish", ""], ["Arda", "Samet E.", ""], ["Goksoy", "A. Alper", ""], ["Mandal", "Sumit K.", ""], ["Ogras", "Umit Y.", ""], ["Sartor", "Anderson L.", ""], ["Marculescu", "Radu", ""]]}, {"id": "2007.09363", "submitter": "Khushal Sethi", "authors": "Khushal Sethi", "title": "Design Space Exploration of Algorithmic Multi-Port Memories in\n  High-Performance Application-Specific Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory load/store instructions consume an important part in execution time\nand energy consumption in domain-specific accelerators. For designing highly\nparallel systems, available parallelism at each granularity is extracted from\nthe workloads. The maximal use of parallelism at each granularity in these\nhigh-performance designs requires the utilization of multi-port memories.\nCurrently, true multiport designs are less popular because there is no inherent\nEDA support for multiport memory beyond 2-ports, utilizing more ports requires\ncircuit-level implementation and hence a high design time. In this work, we\npresent a framework for Design Space Exploration of Algorithmic Multi-Port\nMemories (AMM) in ASICs. We study different AMM designs in the literature,\ndiscuss how we incorporate them in the Pre-RTL Aladdin Framework with different\nmemory depth, port configurations and banking structures. From our analysis on\nselected applications from the MachSuite (accelerator benchmark suite), we\nunderstand and quantify the potential use of AMMs (as true multiport memories)\nfor high performance in applications with low spatial locality in memory access\npatterns.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:23:25 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sethi", "Khushal", ""]]}, {"id": "2007.09490", "submitter": "Mohammadreza Baharani", "authors": "Mohammadreza Baharani, Ushma Sunil, Kaustubh Manohar, Steven\n  Furgurson, Hamed Tabkhi", "title": "DeepDive: An Integrative Algorithm/Architecture Co-Design for Deep\n  Separable Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Separable Convolutional Neural Networks (DSCNNs) have become the\nemerging paradigm by offering modular networks with structural sparsity in\norder to achieve higher accuracy with relatively lower operations and\nparameters. However, there is a lack of customized architectures that can\nprovide flexible solutions that fit the sparsity of the DSCNNs. This paper\nintroduces DeepDive, which is a fully-functional, vertical co-design framework,\nfor power-efficient implementation of DSCNNs on edge FPGAs. DeepDive's\narchitecture supports crucial heterogeneous Compute Units (CUs) to fully\nsupport DSCNNs with various convolutional operators interconnected with\nstructural sparsity. It offers an FPGA-aware training and online quantization\ncombined with modular synthesizable C++ CUs, customized for DSCNNs. The\nexecution results on Xilinx's ZCU102 FPGA board, demonstrate 47.4 and 233.3\nFPS/Watt for MobileNet-V2 and a compact version of EfficientNet, respectively,\nas two state-of-the-art depthwise separable CNNs. These comparisons showcase\nhow DeepDive improves FPS/Watt by 2.2$\\times$ and 1.51$\\times$ over Jetson Nano\nhigh and low power modes, respectively. It also enhances FPS/Watt about\n2.27$\\times$ and 37.25$\\times$ over two other FPGA implementations. The\nDeepDive output for MobileNetV2 is available at\nhttps://github.com/TeCSAR-UNCC/DeepDive.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 17:50:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Baharani", "Mohammadreza", ""], ["Sunil", "Ushma", ""], ["Manohar", "Kaustubh", ""], ["Furgurson", "Steven", ""], ["Tabkhi", "Hamed", ""]]}, {"id": "2007.09537", "submitter": "Adam Hastings", "authors": "Adam Hastings and Simha Sethumadhavan", "title": "A New Doctrine for Hardware Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we promote the idea that recent woes in hardware security are\nnot because of a lack of technical solutions but rather because market forces\nand incentives prevent those with the ability to fix problems from doing so. At\nthe root of the problem is the fact that hardware security comes at a cost;\nPresent issues in hardware security can be seen as the result of the players in\nthe game of hardware security finding ways of avoiding paying this cost. We\nformulate this idea into a doctrine of security, namely the Doctrine of Shared\nBurdens. Three cases studies---Rowhammer, Spectre, and Meltdown---are\ninterpreted though the lens of this doctrine. Our doctrine illuminates why\nthese problems and exist and what can be done about them.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 23:40:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hastings", "Adam", ""], ["Sethumadhavan", "Simha", ""]]}, {"id": "2007.09578", "submitter": "Mahmood Azhar Qureshi", "authors": "Mahmood Azhar Qureshi and Arslan Munir", "title": "NeuroMAX: A High Throughput, Multi-Threaded, Log-Based Accelerator for\n  Convolutional Neural Networks", "comments": "To be published in ICCAD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) require high throughput hardware\naccelerators for real time applications owing to their huge computational cost.\nMost traditional CNN accelerators rely on single core, linear processing\nelements (PEs) in conjunction with 1D dataflows for accelerating convolution\noperations. This limits the maximum achievable ratio of peak throughput per PE\ncount to unity. Most of the past works optimize their dataflows to attain close\nto a 100% hardware utilization to reach this ratio. In this paper, we introduce\na high throughput, multi-threaded, log-based PE core. The designed core\nprovides a 200% increase in peak throughput per PE count while only incurring a\n6% increase in area overhead compared to a single, linear multiplier PE core\nwith same output bit precision. We also present a 2D weight broadcast dataflow\nwhich exploits the multi-threaded nature of the PE cores to achieve a high\nhardware utilization per layer for various CNNs. The entire architecture, which\nwe refer to as NeuroMAX, is implemented on Xilinx Zynq 7020 SoC at 200 MHz\nprocessing clock. Detailed analysis is performed on throughput, hardware\nutilization, area and power breakdown, and latency to show performance\nimprovement compared to previous FPGA and ASIC designs.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:37:41 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Qureshi", "Mahmood Azhar", ""], ["Munir", "Arslan", ""]]}, {"id": "2007.09822", "submitter": "Yongbin Gu", "authors": "Yongbin Gu, Wenxuan Wu, Yunfan Li and Lizhong Chen", "title": "UVMBench: A Comprehensive Benchmark Suite for Researching Unified\n  Virtual Memory in GPUs", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of Unified Virtual Memory (UVM) in GPUs offers a new\nprogramming model that allows GPUs and CPUs to share the same virtual memory\nspace, which shifts the complex memory management from programmers to GPU\ndriver/ hardware and enables kernel execution even when memory is\noversubscribed. Meanwhile, UVM may also incur considerable performance overhead\ndue to tracking and data migration along with special handling of page faults\nand page table walk. As UVM is attracting significant attention from the\nresearch community to develop innovative solutions to these problems, in this\npaper, we propose a comprehensive UVM benchmark suite named UVMBench to\nfacilitate future research on this important topic. The proposed UVMBench\nconsists of 32 representative benchmarks from a wide range of application\ndomains. The suite also features unified programming implementation and diverse\nmemory access patterns across benchmarks, thus allowing thorough evaluation and\ncomparison with current state-of-the-art. A set of experiments have been\nconducted on real GPUs to verify and analyze the benchmark suite behaviors\nunder various scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 00:28:55 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 21:51:33 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gu", "Yongbin", ""], ["Wu", "Wenxuan", ""], ["Li", "Yunfan", ""], ["Chen", "Lizhong", ""]]}, {"id": "2007.09976", "submitter": "Rajeev Muralidhar", "authors": "Rajeev Muralidhar and Renata Borovica-Gajic and Rajkumar Buyya", "title": "Energy Efficient Computing Systems: Architectures, Abstractions and\n  Modeling to Techniques and Standards", "comments": "35 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1404.4629 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing systems have undergone several inflexion points - while Moore's law\nguided the semiconductor industry to cram more and more transistors and logic\ninto the same volume, the limits of instruction-level parallelism (ILP) and the\nend of Dennard's scaling drove the industry towards multi-core chips. We have\nnow entered the era of domain-specific architectures for new workloads like AI\nand ML. These trends continue, arguably with other limits, along with\nchallenges imposed by tighter integration, extreme form factors and diverse\nworkloads, making systems more complex from an energy efficiency perspective.\nMany research surveys have covered different aspects of techniques in hardware\nand microarchitecture across devices, servers, HPC, data center systems along\nwith software, algorithms, frameworks for energy efficiency and thermal\nmanagement. Somewhat in parallel, the semiconductor industry has developed\ntechniques and standards around specification, modeling and verification of\ncomplex chips; these areas have not been addressed in detail by previous\nresearch surveys. This survey aims to bring these domains together and is\ncomposed of a systematic categorization of key aspects of building energy\nefficient systems - (a) specification - the ability to precisely specify the\npower intent or properties at different layers (b) modeling and simulation of\nthe entire system or subsystem (hardware or software or both) so as to be able\nto perform what-if analysis, (c) techniques used for implementing energy\nefficiency at different levels of the stack, (d) verification techniques used\nto provide guarantees that the functionality of complex designs are preserved,\nand (e) energy efficiency standards and consortiums that aim to standardize\ndifferent aspects of energy efficiency, including cross-layer optimizations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:52:20 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 06:33:02 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Muralidhar", "Rajeev", ""], ["Borovica-Gajic", "Renata", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2007.10330", "submitter": "Behnam Khaleghi", "authors": "Behnam Khaleghi, Sahand Salamat, Anthony Thomas, Fatemeh Asgarinejad,\n  Yeseong Kim, and Tajana Rosing", "title": "SHEARer: Highly-Efficient Hyperdimensional Computing by\n  Software-Hardware Enabled Multifold Approximation", "comments": "A shorter version is accepted in ACM/IEEE International Symposium on\n  Low Power Electronics and Design (ISLPED 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperdimensional computing (HD) is an emerging paradigm for machine learning\nbased on the evidence that the brain computes on high-dimensional, distributed,\nrepresentations of data. The main operation of HD is encoding, which transfers\nthe input data to hyperspace by mapping each input feature to a hypervector,\naccompanied by so-called bundling procedure that simply adds up the\nhypervectors to realize encoding hypervector. Although the operations of HD are\nhighly parallelizable, the massive number of operations hampers the efficiency\nof HD in embedded domain. In this paper, we propose SHEARer, an\nalgorithm-hardware co-optimization to improve the performance and energy\nconsumption of HD computing. We gain insight from a prudent scheme of\napproximating the hypervectors that, thanks to inherent error resiliency of HD,\nhas minimal impact on accuracy while provides high prospect for hardware\noptimization. In contrast to previous works that generate the encoding\nhypervectors in full precision and then ex-post quantizing, we compute the\nencoding hypervectors in an approximate manner that saves a significant amount\nof resources yet affords high accuracy. We also propose a novel FPGA\nimplementation that achieves striking performance through massive parallelism\nwith low power consumption. Moreover, we develop a software framework that\nenables training HD models by emulating the proposed approximate encodings. The\nFPGA implementation of SHEARer achieves an average throughput boost of 104,904x\n(15.7x) and energy savings of up to 56,044x (301x) compared to state-of-the-art\nencoding methods implemented on Raspberry Pi 3 (GeForce GTX 1080 Ti) using\npractical machine learning datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 07:58:44 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Khaleghi", "Behnam", ""], ["Salamat", "Sahand", ""], ["Thomas", "Anthony", ""], ["Asgarinejad", "Fatemeh", ""], ["Kim", "Yeseong", ""], ["Rosing", "Tajana", ""]]}, {"id": "2007.10451", "submitter": "Mathew Hall", "authors": "Mathew Hall and Vaughn Betz", "title": "HPIPE: Heterogeneous Layer-Pipelined and Sparse-Aware CNN Inference for\n  FPGAs", "comments": "8 Pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present both a novel Convolutional Neural Network (CNN) accelerator\narchitecture and a network compiler for FPGAs that outperforms all prior work.\nInstead of having generic processing elements that together process one layer\nat a time, our network compiler statically partitions available device\nresources and builds custom-tailored hardware for each layer of a CNN. By\nbuilding hardware for each layer we can pack our controllers into fewer lookup\ntables and use dedicated routing. These efficiencies enable our accelerator to\nutilize 2x the DSPs and operate at more than 2x the frequency of prior work on\nsparse CNN acceleration on FPGAs. We evaluate the performance of our\narchitecture on both sparse Resnet-50 and dense MobileNet Imagenet classifiers\non a Stratix 10 2800 FPGA. We find that the sparse Resnet-50 model has\nthroughput at a batch size of 1 of 4550 images/s, which is nearly 4x the\nthroughput of NVIDIA's fastest machine learning targeted GPU, the V100, and\noutperforms all prior work on FPGAs.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:17:58 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Hall", "Mathew", ""], ["Betz", "Vaughn", ""]]}, {"id": "2007.10454", "submitter": "Ishan Thakkar", "authors": "Sai Vineel Reddy Chittamuru, Ishan G Thakkar, Sudeep Pasricha, Sairam\n  Sri Vatsavai, Varun Bhat", "title": "Exploiting Process Variations to Secure Photonic NoC Architectures from\n  Snooping Attacks", "comments": "Pre-Print: Accepted in IEEE TCAD Journal on July 16, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compact size and high wavelength-selectivity of microring resonators\n(MRs) enable photonic networks-on-chip (PNoCs) to utilize\ndense-wavelength-division-multiplexing (DWDM) in their photonic waveguides, and\nas a result, attain high bandwidth on-chip data transfers. Unfortunately, a\nHardware Trojan in a PNoC can manipulate the electrical driving circuit of its\nMRs to cause the MRs to snoop data from the neighboring wavelength channels in\na shared photonic waveguide, which introduces a serious security threat. This\npaper presents a framework that utilizes process variation-based authentication\nsignatures along with architecture-level enhancements to protect against\ndata-snooping Hardware Trojans during unicast as well as multicast transfers in\nPNoCs. Evaluation results indicate that our framework can improve hardware\nsecurity across various PNoC architectures with minimal overheads of up to\n14.2% in average latency and of up to 14.6% in energy-delay-product (EDP).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:31:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chittamuru", "Sai Vineel Reddy", ""], ["Thakkar", "Ishan G", ""], ["Pasricha", "Sudeep", ""], ["Vatsavai", "Sairam Sri", ""], ["Bhat", "Varun", ""]]}, {"id": "2007.10702", "submitter": "Jianlei Yang", "authors": "Xueyan Wang, Jianlei Yang, Yinglin Zhao, Yingjie Qi, Meichen Liu,\n  Xingzhou Cheng, Xiaotao Jia, Xiaoming Chen, Gang Qu, Weisheng Zhao", "title": "TCIM: Triangle Counting Acceleration With Processing-In-MRAM\n  Architecture", "comments": "published on DAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triangle counting (TC) is a fundamental problem in graph analysis and has\nfound numerous applications, which motivates many TC acceleration solutions in\nthe traditional computing platforms like GPU and FPGA. However, these\napproaches suffer from the bandwidth bottleneck because TC calculation involves\na large amount of data transfers. In this paper, we propose to overcome this\nchallenge by designing a TC accelerator utilizing the emerging\nprocessing-in-MRAM (PIM) architecture. The true innovation behind our approach\nis a novel method to perform TC with bitwise logic operations (such as\n\\texttt{AND}), instead of the traditional approaches such as matrix\ncomputations. This enables the efficient in-memory implementations of TC\ncomputation, which we demonstrate in this paper with computational\nSpin-Transfer Torque Magnetic RAM (STT-MRAM) arrays. Furthermore, we develop\ncustomized graph slicing and mapping techniques to speed up the computation and\nreduce the energy consumption. We use a device-to-architecture co-simulation\nframework to validate our proposed TC accelerator. The results show that our\ndata mapping strategy could reduce $99.99\\%$ of the computation and $72\\%$ of\nthe memory \\texttt{WRITE} operations. Compared with the existing GPU or FPGA\naccelerators, our in-memory accelerator achieves speedups of $9\\times$ and\n$23.4\\times$, respectively, and a $20.6\\times$ energy efficiency improvement\nover the FPGA accelerator.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:42:17 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Xueyan", ""], ["Yang", "Jianlei", ""], ["Zhao", "Yinglin", ""], ["Qi", "Yingjie", ""], ["Liu", "Meichen", ""], ["Cheng", "Xingzhou", ""], ["Jia", "Xiaotao", ""], ["Chen", "Xiaoming", ""], ["Qu", "Gang", ""], ["Zhao", "Weisheng", ""]]}, {"id": "2007.11112", "submitter": "Christos Kozyrakis", "authors": "Michael Cafarella and David DeWitt and Vijay Gadepally and Jeremy\n  Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and\n  Matei Zaharia", "title": "DBOS: A Proposal for a Data-Centric Operating System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current operating systems are complex systems that were designed before\ntoday's computing environments. This makes it difficult for them to meet the\nscalability, heterogeneity, availability, and security challenges in current\ncloud and parallel computing environments. To address these problems, we\npropose a radically new OS design based on data-centric architecture: all\noperating system state should be represented uniformly as database tables, and\noperations on this state should be made via queries from otherwise stateless\ntasks. This design makes it easy to scale and evolve the OS without\nwhole-system refactoring, inspect and debug system state, upgrade components\nwithout downtime, manage decisions using machine learning, and implement\nsophisticated security features. We discuss how a database OS (DBOS) can\nimprove the programmability and performance of many of today's most important\napplications and propose a plan for the development of a DBOS proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:01:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cafarella", "Michael", ""], ["DeWitt", "David", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Kozyrakis", "Christos", ""], ["Kraska", "Tim", ""], ["Stonebraker", "Michael", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.11195", "submitter": "Ming Ling", "authors": "Ming Ling, Xiaoqian Lu, Guangmin Wang, Jiancong Ge", "title": "Analytical Modeling the Multi-Core Shared Cache Behavior with\n  Considerations of Data-Sharing and Coherence", "comments": "The manuscript has been re-submitted to Microprocessors and\n  Microsystems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the ever worsening \"Power wall\" and \"Memory wall\" problems,\nmulti-core architectures with multilevel cache hierarchies have been widely\naccepted in modern processors. However, the complexity of the architectures\nmakes modeling of shared caches extremely complex. In this paper, we propose a\ndata-sharing aware analytical model for estimating the miss rates of the\ndownstream shared cache under multi-core scenarios. Moreover, the proposed\nmodel can also be integrated with upstream cache analytical models with the\nconsideration of multi-core private cache coherent effects. This integration\navoids time consuming full simulations of the cache architecture that required\nby conventional approaches. We validate our analytical model against gem5\nsimulation results under 13 applications from PARSEC 2.1 benchmark suites.\nCompared to the results from gem5 simulations under 8 hardware configurations\nincluding dual-core and quad-core architectures, the average absolute error of\nthe predicted shared L2 cache miss rates is less than 2% for all\nconfigurations. After integrated with the refined upstream model with coherence\nmisses, the overall average absolute error in 4 hardware configurations is\ndegraded to 8.03% due to the error accumulations. The proposed coherence model\ncan achieve similar accuracies of state of the art approach with only one tenth\ntime overhead. As an application case of the integrated model, we also evaluate\nthe miss rates of 57 different multi-core and multi-level cache configurations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 04:09:18 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 02:47:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ling", "Ming", ""], ["Lu", "Xiaoqian", ""], ["Wang", "Guangmin", ""], ["Ge", "Jiancong", ""]]}, {"id": "2007.11818", "submitter": "Mohammad Behnia", "authors": "Mohammad Behnia, Prateek Sahu, Riccardo Paccagnella, Jiyong Yu, Zirui\n  Zhao, Xiang Zou, Thomas Unterluggauer, Josep Torrellas, Carlos Rozas, Adam\n  Morrison, Frank Mckeen, Fangfei Liu, Ron Gabor, Christopher W. Fletcher,\n  Abhishek Basak, Alaa Alameldeen", "title": "Speculative Interference Attacks: Breaking Invisible Speculation Schemes", "comments": "Updated CR Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent security vulnerabilities that target speculative execution (e.g.,\nSpectre) present a significant challenge for processor design. The highly\npublicized vulnerability uses speculative execution to learn victim secrets by\nchanging cache state. As a result, recent computer architecture research has\nfocused on invisible speculation mechanisms that attempt to block changes in\ncache state due to speculative execution. Prior work has shown significant\nsuccess in preventing Spectre and other vulnerabilities at modest performance\ncosts. In this paper, we introduce speculative interference attacks, which show\nthat prior invisible speculation mechanisms do not fully block these\nspeculation-based attacks. We make two key observations. First, misspeculated\nyounger instructions can change the timing of older, bound-to-retire\ninstructions, including memory operations. Second, changing the timing of a\nmemory operation can change the order of that memory operation relative to\nother memory operations, resulting in persistent changes to the cache state.\nUsing these observations, we demonstrate (among other attack variants) that\nsecret information accessed by mis-speculated instructions can change the order\nof bound-to-retire loads. Load timing changes can therefore leave\nsecret-dependent changes in the cache, even in the presence of invisible\nspeculation mechanisms. We show that this problem is not easy to fix:\nSpeculative interference converts timing changes to persistent cache-state\nchanges, and timing is typically ignored by many cache-based defenses. We\ndevelop a framework to understand the attack and demonstrate concrete\nproof-of-concept attacks against invisible speculation mechanisms. We provide\nsecurity definitions sufficient to block speculative interference attacks;\ndescribe a simple defense mechanism with a high performance cost; and discuss\nhow future research can improve its performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:36:38 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 20:19:50 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 23:41:46 GMT"}, {"version": "v4", "created": "Fri, 23 Apr 2021 16:30:22 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Behnia", "Mohammad", ""], ["Sahu", "Prateek", ""], ["Paccagnella", "Riccardo", ""], ["Yu", "Jiyong", ""], ["Zhao", "Zirui", ""], ["Zou", "Xiang", ""], ["Unterluggauer", "Thomas", ""], ["Torrellas", "Josep", ""], ["Rozas", "Carlos", ""], ["Morrison", "Adam", ""], ["Mckeen", "Frank", ""], ["Liu", "Fangfei", ""], ["Gabor", "Ron", ""], ["Fletcher", "Christopher W.", ""], ["Basak", "Abhishek", ""], ["Alameldeen", "Alaa", ""]]}, {"id": "2007.11976", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "Comparative Analysis of Polynomial and Rational Approximations of\n  Hyperbolic Tangent Function for VLSI Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks yield the state-of-the-art results in many computer\nvision and human machine interface applications such as object detection,\nspeech recognition etc. Since, these networks are computationally expensive,\ncustomized accelerators are designed for achieving the required performance at\nlower cost and power. One of the key building blocks of these neural networks\nis non-linear activation function such as sigmoid, hyperbolic tangent (tanh),\nand ReLU. A low complexity accurate hardware implementation of the activation\nfunction is required to meet the performance and area targets of the neural\nnetwork accelerators. Even though, various methods and implementations of tanh\nactivation function have been published, a comparative study is missing. This\npaper presents comparative analysis of polynomial and rational methods and\ntheir hardware implementation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:31:02 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 13:41:18 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2007.12371", "submitter": "Hans-Christian Ruiz Euler Dr.", "authors": "Hans-Christian Ruiz-Euler, Unai Alegre-Ibarra, Bram van de Ven, Hajo\n  Broersma, Peter A. Bobbert, Wilfred G. van der Wiel", "title": "Dopant Network Processing Units: Towards Efficient Neural-network\n  Emulators with High-capacity Nanoelectronic Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly growing computational demands of deep neural networks require\nnovel hardware designs. Recently, tunable nanoelectronic devices were developed\nbased on hopping electrons through a network of dopant atoms in silicon. These\n\"Dopant Network Processing Units\" (DNPUs) are highly energy-efficient and have\npotentially very high throughput. By adapting the control voltages applied to\nits terminals, a single DNPU can solve a variety of linearly non-separable\nclassification problems. However, using a single device has limitations due to\nthe implicit single-node architecture. This paper presents a promising novel\napproach to neural information processing by introducing DNPUs as high-capacity\nneurons and moving from a single to a multi-neuron framework. By implementing\nand testing a small multi-DNPU classifier in hardware, we show that\nfeed-forward DNPU networks improve the performance of a single DNPU from 77% to\n94% test accuracy on a binary classification task with concentric classes on a\nplane. Furthermore, motivated by the integration of DNPUs with memristor\narrays, we study the potential of using DNPUs in combination with linear\nlayers. We show by simulation that a single-layer MNIST classifier with only 10\nDNPUs achieves over 96% test accuracy. Our results pave the road towards\nhardware neural-network emulators that offer atomic-scale information\nprocessing with low latency and energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:35:44 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ruiz-Euler", "Hans-Christian", ""], ["Alegre-Ibarra", "Unai", ""], ["van de Ven", "Bram", ""], ["Broersma", "Hajo", ""], ["Bobbert", "Peter A.", ""], ["van der Wiel", "Wilfred G.", ""]]}, {"id": "2007.13387", "submitter": "Tomasz Winiarski", "authors": "Tomasz Winiarski, Pawe{\\l} Balsam, Maciej W\\k{e}gierek, Sonia\n  Borodzicz-Ja\\.zd\\.zyk, Wojciech Dudek, Konrad Banachowicz, Janusz\n  Kochanowski, Micha{\\l} Marchel", "title": "A concept of a measuring system for probe kinesthetic parameters\n  identification during echocardiography examination", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echocardiography is the most commonly used imaging technique in clinical\ncardiology. Due to the high demand for this type of examination and the small\nnumber of specialists, there is a need to support the examination process\nthrough telemedicine. Moreover, specialist training can be supported by\nappropriate simulation systems. For (i) creating tailor-made tele-echo robots,\n(ii) creating echo system simulators, and (iii) conducting echo examination\nwith local or remote expert assistance, knowledge about echo probe kinesthetic\nparameters during echocardiography examination is advisable. The article\ndescribes the concept of a measuring system for obtaining such data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:07:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Winiarski", "Tomasz", ""], ["Balsam", "Pawe\u0142", ""], ["W\u0119gierek", "Maciej", ""], ["Borodzicz-Ja\u017cd\u017cyk", "Sonia", ""], ["Dudek", "Wojciech", ""], ["Banachowicz", "Konrad", ""], ["Kochanowski", "Janusz", ""], ["Marchel", "Micha\u0142", ""]]}, {"id": "2007.13516", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "Hardware Implementation of Hyperbolic Tangent Function using Catmull-Rom\n  Spline Interpolation", "comments": "4 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:2007.11976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks yield the state of the art results in many computer\nvision and human machine interface tasks such as object recognition, speech\nrecognition etc. Since, these networks are computationally expensive,\ncustomized accelerators are designed for achieving the required performance at\nlower cost and power. One of the key building blocks of these neural networks\nis non-linear activation function such as sigmoid, hyperbolic tangent (tanh),\nand ReLU. A low complexity accurate hardware implementation of the activation\nfunction is required to meet the performance and area targets of the neural\nnetwork accelerators. This paper presents an implementation of tanh function\nusing the Catmull-Rom spline interpolation. State of the art results are\nachieved using this method with comparatively smaller logic area.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:11:59 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2007.13595", "submitter": "Jianlei Yang", "authors": "Pengcheng Dai, Jianlei Yang, Xucheng Ye, Xingzhou Cheng, Junyu Luo,\n  Linghao Song, Yiran Chen, Weisheng Zhao", "title": "SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional\n  Neural Networks Training", "comments": "published on DAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training Convolutional Neural Networks (CNNs) usually requires a large number\nof computational resources. In this paper, \\textit{SparseTrain} is proposed to\naccelerate CNN training by fully exploiting the sparsity. It mainly involves\nthree levels of innovations: activation gradients pruning algorithm, sparse\ntraining dataflow, and accelerator architecture. By applying a stochastic\npruning algorithm on each layer, the sparsity of back-propagation gradients can\nbe increased dramatically without degrading training accuracy and convergence\nrate. Moreover, to utilize both \\textit{natural sparsity} (resulted from ReLU\nor Pooling layers) and \\textit{artificial sparsity} (brought by pruning\nalgorithm), a sparse-aware architecture is proposed for training acceleration.\nThis architecture supports forward and back-propagation of CNN by adopting\n1-Dimensional convolution dataflow. We have built %a simple compiler to map\nCNNs topology onto \\textit{SparseTrain}, and a cycle-accurate architecture\nsimulator to evaluate the performance and efficiency based on the synthesized\ndesign with $14nm$ FinFET technologies. Evaluation results on AlexNet/ResNet\nshow that \\textit{SparseTrain} could achieve about $2.7 \\times$ speedup and\n$2.2 \\times$ energy efficiency improvement on average compared with the\noriginal training process.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:01:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dai", "Pengcheng", ""], ["Yang", "Jianlei", ""], ["Ye", "Xucheng", ""], ["Cheng", "Xingzhou", ""], ["Luo", "Junyu", ""], ["Song", "Linghao", ""], ["Chen", "Yiran", ""], ["Zhao", "Weisheng", ""]]}, {"id": "2007.13661", "submitter": "Yinjin Fu", "authors": "Yinjin Fu", "title": "CARAM: A Content-Aware Hybrid PCM/DRAM Main Memory System Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Phase-Change Memory (PCM) provides opportunities for\ndirectly connecting persistent memory to main memory bus. While PCM achieves\nhigh read throughput and low standby power, the critical concerns are its poor\nwrite performance and limited durability, especially when compared to DRAM. A\nnaturally inspired design is the hybrid memory architecture that fuses DRAM and\nPCM, so as to exploit the positive aspects of both types of memory.\nUnfortunately, existing solutions are seriously challenged by the limited main\nmemory size, which is the primary bottleneck of in-memory computing. In this\npaper, we introduce a novel Content Aware hybrid PCM/DRAM main memory system\nframework - CARAM, which exploits deduplication to improve line sharing with\nhigh memory efficiency. CARAM effectively reduces write traffic to hybrid\nmemory by removing unnecessary duplicate line writes. It also substantially\nextends available free memory space by coalescing redundant lines in hybrid\nmemory, thereby further improving the wear-leveling efficiency of PCM. To\nobtain high data access performance, we also design a set of acceleration\ntechniques to minimize the overhead caused by extra computation costs. Our\nexperiment results show that CARAM effectively reduces 15%~42% of memory usage\nand improves I/O bandwidth by 13%~116%, while saving 31%~38% energy\nconsumption, compared to the state-of-the-art of hybrid systems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:15:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Fu", "Yinjin", ""]]}, {"id": "2007.13667", "submitter": "Alfio Di Mauro", "authors": "Alfio Di Mauro, Davide Rossi, Antonio Pullini, Philippe Flatresse,\n  Luca Benini", "title": "Performance-Aware Predictive-Model-Based On-Chip Body-Bias Regulation\n  Strategy for an ULP Multi-Core Cluster in 28nm UTBB FD-SOI", "comments": null, "journal-ref": "Integration, Volume 72, 2020, Pages 194-207", "doi": "10.1016/j.vlsi.2019.12.006", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance and reliability of Ultra-Low-Power (ULP) computing platforms\nare adversely affected by environmental temperature and process variations.\nMitigating the effect of these phenomena becomes crucial when these devices\noperate near-threshold, due to the magnification of process variations and to\nthe strong temperature inversion effect that affects advanced technology nodes\nin low-voltage corners, which causes huge overhead due to margining for timing\nclosure. Supporting an extended range of reverse and forward body-bias, UTBB\nFD-SOI technology provides a powerful knob to compensate for such variations.\nIn this work we propose a methodology to maximize energy efficiency at run-time\nexploiting body biasing on a ULP platform operating near-threshold. The\nproposed method relies on on-line performance measurements by means of Process\nMonitoring Blocks (PMBs) coupled with an on-chip low-power body bias generator.\nWe correlate the measurement performed by the PMBs to the maximum achievable\nfrequency of the system, deriving a predictive model able to estimate it with\nan error of 9.7% at 0.7V. To minimize the effect of process variations we\npropose a calibration procedure that allows to use a PMB model affected by only\nthe temperature-induced error, which reduces the frequency estimation error by\n2.4x (from 9.7% to 4%). We finally propose a controller architecture relying on\nthe derived models to automatically regulate at run-time the body bias voltage.\nWe demonstrate that adjusting the body bias voltage against environmental\ntemperature variations leads up to 2X reduction in the leakage power and a 15%\nimprovement on the global energy consumption when the system operates at 0.7V\nand 170MHz\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:22:11 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Di Mauro", "Alfio", ""], ["Rossi", "Davide", ""], ["Pullini", "Antonio", ""], ["Flatresse", "Philippe", ""], ["Benini", "Luca", ""]]}, {"id": "2007.13828", "submitter": "Kevin Kiningham", "authors": "Kevin Kiningham, Christopher Re, Philip Levis", "title": "GRIP: A Graph Neural Network Accelerator Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present GRIP, a graph neural network accelerator architecture designed for\nlow-latency inference. AcceleratingGNNs is challenging because they combine two\ndistinct types of computation: arithmetic-intensive vertex-centric operations\nand memory-intensive edge-centric operations. GRIP splits GNN inference into a\nfixed set of edge- and vertex-centric execution phases that can be implemented\nin hardware. We then specialize each unit for the unique computational\nstructure found in each phase.For vertex-centric phases, GRIP uses a high\nperformance matrix multiply engine coupled with a dedicated memory subsystem\nfor weights to improve reuse. For edge-centric phases, GRIP use multiple\nparallel prefetch and reduction engines to alleviate the irregularity in memory\naccesses. Finally, GRIP supports severalGNN optimizations, including a novel\noptimization called vertex-tiling which increases the reuse of weight data.We\nevaluate GRIP by performing synthesis and place and route for a 28nm\nimplementation capable of executing inference for several widely-used GNN\nmodels (GCN, GraphSAGE, G-GCN, and GIN). Across several benchmark graphs, it\nreduces 99th percentile latency by a geometric mean of 17x and 23x compared to\na CPU and GPU baseline, respectively, while drawing only 5W.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:44:51 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 07:55:13 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kiningham", "Kevin", ""], ["Re", "Christopher", ""], ["Levis", "Philip", ""]]}, {"id": "2007.14371", "submitter": "Augusto Vega", "authors": "Augusto Vega and Aporva Amarnath and John-David Wellman and Hiwot\n  Kassa and Subhankar Pal and Hubertus Franke and Alper Buyuktosunoglu and\n  Ronald Dreslinski and Pradip Bose", "title": "STOMP: A Tool for Evaluation of Scheduling Policies in Heterogeneous\n  Multi-Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of heterogeneous chip multiprocessors in recent years has\nreached unprecedented levels. Traditional homogeneous platforms have shown\nfundamental limitations when it comes to enabling high-performance\nyet-ultra-low-power computing, in particular in application domains with\nreal-time execution deadlines or criticality constraints. By combining the\nright set of general purpose cores and hardware accelerators together, along\nwith proper chip interconnects and memory technology, heterogeneous chip\nmultiprocessors have become an effective high-performance and low-power\ncomputing alternative.\n  One of the challenges of heterogeneous architectures relates to efficient\nscheduling of application tasks (processes, threads) across the variety of\noptions in the chip. As a result, it is key to provide tools to enable\nearly-stage prototyping and evaluation of new scheduling policies for\nheterogeneous platforms. In this paper, we present STOMP (Scheduling Techniques\nOptimization in heterogeneous Multi-Processors), a simulator for fast\nimplementation and evaluation of task scheduling policies in\nmulti-core/multi-processor systems with a convenient interface for \"plugging\"\nin new scheduling policies in a simple manner. Thorough validation of STOMP\nexhibits small relative errors when compared against closed-formed equivalent\nmodels during steady-state analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:29:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Vega", "Augusto", ""], ["Amarnath", "Aporva", ""], ["Wellman", "John-David", ""], ["Kassa", "Hiwot", ""], ["Pal", "Subhankar", ""], ["Franke", "Hubertus", ""], ["Buyuktosunoglu", "Alper", ""], ["Dreslinski", "Ronald", ""], ["Bose", "Pradip", ""]]}, {"id": "2007.14897", "submitter": "Sunwoo Kim", "authors": "Sunwoo Kim, Jooho Wang, Youngho Seo, Sanghun Lee, Yeji Park, Sungkyung\n  Park and Chester Sungchung Park", "title": "Transaction-level Model Simulator for Communication-Limited Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid design space exploration in early design stage is critical to\nalgorithm-architecture co-design for accelerators. In this work, a pre-RTL\ncycle-accurate accelerator simulator based on SystemC transaction-level\nmodeling (TLM), AccTLMSim, is proposed for convolutional neural network (CNN)\naccelerators. The accelerator simulator keeps track of each bus transaction\nbetween accelerator and DRAM, taking into account the communication bandwidth.\nThe simulation results are validated against the implementation results on the\nXilinx Zynq. Using the proposed simulator, it is shown that the communication\nbandwidth is severely affected by DRAM latency and bus protocol overhead. In\naddition, the loop tiling is optimized to maximize the performance under the\nconstraint of on-chip SRAM size. Furthermore, a new performance estimation\nmodel is proposed to speed up the design space exploration. Thanks to the\nproposed simulator and performance estimation model, it is possible to explore\na design space of millions of architectural options within a few tens of\nminutes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:13:47 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Kim", "Sunwoo", ""], ["Wang", "Jooho", ""], ["Seo", "Youngho", ""], ["Lee", "Sanghun", ""], ["Park", "Yeji", ""], ["Park", "Sungkyung", ""], ["Park", "Chester Sungchung", ""]]}, {"id": "2007.15647", "submitter": "Furkan Ercan", "authors": "Furkan Ercan and Warren J. Gross", "title": "Fast Thresholded SC-Flip Decoding of Polar Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SC-Flip (SCF) decoding algorithm shares the attention with the common polar\ncode decoding approaches due to its low-complexity and improved\nerror-correction performance. However, the inefficient criterion for locating\nthe correct bit-flipping position in SCF decoding limits its improvements. Due\nto its improved bit-flipping criterion, Thresholded SCF (TSCF) decoding\nalgorithm exhibits a superior error-correction performance and lower\ncomputational complexity than SCF decoding. However, the parameters of TSCF\ndecoding depend on multiple channel and code parameters, and are obtained via\nMonte-Carlo simulations. Our main goal is to realize TSCF decoding as a\npractical polar decoder implementation. To this end, we first realize an\napproximated threshold value that is independent of the code parameters and\nprecomputations. The proposed approximation has negligible error-correction\nperformance degradation on the TSCF decoding. Then, we validate an alternative\napproach for forming a critical set that does not require precomputations,\nwhich also paves the way to the implementation of the Fast-TSCF decoder.\nCompared to the existing fast SCF implementations, the proposed Fast-TSCF\ndecoder has $0.24$ to $0.41$ dB performance gain at frame error rate of\n$10^{-3}$, without any extra cost. Compared to the TSCF decoding, Fast-TSCF\ndoes not depend on precomputations and requires $87\\%$ fewer decoding steps.\nFinally, implementation results in TSMC 65nm CMOS technology show that the\nFast-TSCF decoder is $20\\%$ and $82\\%$ more area-efficient than the\nstate-of-the-art fast SCF and fast SC-List decoder architectures, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:58:41 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Ercan", "Furkan", ""], ["Gross", "Warren J.", ""]]}, {"id": "2007.15671", "submitter": "Bochen Tan", "authors": "Bochen Tan and Jason Cong", "title": "Optimal Layout Synthesis for Quantum Computing", "comments": "to appear in ICCAD'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the fast development of quantum computing.\nResearchers around the world are eager to run larger and larger quantum\nalgorithms that promise speedups impossible to any classical algorithm.\nHowever, the available quantum computers are still volatile and error-prone.\nThus, layout synthesis, which transforms quantum programs to meet these\nhardware limitations, is a crucial step in the realization of quantum\ncomputing. In this paper, we present two synthesizers, one optimal and one\napproximate but nearly optimal. Although a few optimal approaches to this\nproblem have been published, our optimal synthesizer explores a larger solution\nspace, thus is optimal in a stronger sense. In addition, it reduces time and\nspace complexity exponentially compared to some leading optimal approaches. The\nkey to this success is a more efficient spacetime-based variable encoding of\nthe layout synthesis problem as a mathematical programming problem. By slightly\nchanging our formulation, we arrive at an approximate synthesizer that is even\nmore efficient and outperforms some leading heuristic approaches, in terms of\nadditional gate cost, by up to 100%, and also fidelity by up to 10x on a\ncomprehensive set of benchmark programs and architectures. For a specific\nfamily of quantum programs named QAOA, which is deemed to be a promising\napplication for near-term quantum computers, we further adjust the approximate\nsynthesizer by taking commutation into consideration, achieving up to 75%\nreduction in depth and up to 65% reduction in additional cost compared to the\ntool used in a leading QAOA study.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:05:56 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Tan", "Bochen", ""], ["Cong", "Jason", ""]]}, {"id": "2007.15919", "submitter": "Jan Philipp Thoma", "authors": "Jan Philipp Thoma, Jakob Feldtkeller, Markus Krausz, Tim G\\\"uneysu,\n  Daniel J. Bernstein", "title": "BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has revealed an ever-growing class of microarchitectural\nattacks that exploit speculative execution, a standard feature in modern\nprocessors. Proposed and deployed countermeasures involve a variety of compiler\nupdates, firmware updates, and hardware updates. None of the deployed\ncountermeasures have convincing security arguments, and many of them have\nalready been broken.\n  The obvious way to simplify the analysis of speculative-execution attacks is\nto eliminate speculative execution. This is normally dismissed as being\nunacceptably expensive, but the underlying cost analyses consider only software\nwritten for current instruction-set architectures, so they do not rule out the\npossibility of a new instruction-set architecture providing acceptable\nperformance without speculative execution. A new ISA requires compiler and\nhardware updates, but these are happening in any case.\n  This paper introduces BasicBlocker, a generic ISA modification that works for\nall common ISAs and that allows non-speculative CPUs to obtain most of the\nperformance benefit that would have been provided by speculative execution. To\ndemonstrate the feasibility of BasicBlocker, this paper defines a variant of\nthe RISC-V ISA called BBRISC-V and provides a thorough evaluation on both a\n5-stage in-order soft core and a superscalar out-of-order processor using an\nassociated compiler and a variety of benchmark programs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:30:45 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 10:37:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Thoma", "Jan Philipp", ""], ["Feldtkeller", "Jakob", ""], ["Krausz", "Markus", ""], ["G\u00fcneysu", "Tim", ""], ["Bernstein", "Daniel J.", ""]]}, {"id": "2007.16175", "submitter": "Elmira Karimi", "authors": "Elmira Karimi, Yunsi Fei, David Kaeli", "title": "Hardware/Software Obfuscation against Timing Side-channel Attack on a\n  GPU", "comments": "2020 IEEE International Symposium on Hardware Oriented Security and\n  Trust (HOST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are increasingly being used in security applications, especially for\naccelerating encryption/decryption. While GPUs are an attractive platform in\nterms of performance, the security of these devices raises a number of\nconcerns. One vulnerability is the data-dependent timing information, which can\nbe exploited by adversary to recover the encryption key. Memory system features\nare frequently exploited since they create detectable timing variations. In\nthis paper, our attack model is a coalescing attack, which leverages a critical\nGPU microarchitectural feature -- the coalescing unit. As multiple concurrent\nGPU memory requests can refer to the same cache block, the coalescing unit\ncollapses them into a single memory transaction. The access time of an\nencryption kernel is dependent on the number of transactions. Correlation\nbetween a guessed key value and the associated timing samples can be exploited\nto recover the secret key. In this paper, a series of hardware/software\ncountermeasures are proposed to obfuscate the memory timing side channel,\nmaking the GPU more resilient without impacting performance. Our hardware-based\napproach attempts to randomize the width of the coalescing unit to lower the\nsignal-to-noise ratio. We present a hierarchical Miss Status Holding Register\n(MSHR) design that can merge transactions across different warps. This feature\nboosts performance, while, at the same time, secures the execution. We also\npresent a software-based approach to permute the organization of critical data\nstructures, significantly changing the coalescing behavior and introducing a\nhigh degree of randomness. Equipped with our new protections, the effort to\nlaunch a successful attack is increased up to 1433X . 178X, while also\nimproving encryption/decryption performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:00:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Karimi", "Elmira", ""], ["Fei", "Yunsi", ""], ["Kaeli", "David", ""]]}, {"id": "2007.16179", "submitter": "Saurabh Sinha", "authors": "Rahul Mathur, Chien-Ju Chao, Rossana Liu, Nikhil Tadepalli, Pranavi\n  Chandupatla, Shawn Hung, Xiaoqing Xu, Saurabh Sinha, Jaydeep Kulkarni", "title": "Thermal Analysis of a 3D Stacked High-Performance Commercial\n  Microprocessor using Face-to-Face Wafer Bonding Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D integration technologies are seeing widespread adoption in the\nsemiconductor industry to offset the limitations and slowdown of\ntwo-dimensional scaling. High-density 3D integration techniques such as\nface-to-face wafer bonding with sub-10 $\\mu$m pitch can enable new ways of\ndesigning SoCs using all 3 dimensions, like folding a microprocessor design\nacross multiple 3D tiers. However, overlapping thermal hotspots can be a\nchallenge in such 3D stacked designs due to a general increase in power\ndensity. In this work, we perform a thorough thermal simulation study on\nsign-off quality physical design implementation of a state-of-the-art,\nhigh-performance, out-of-order microprocessor on a 7nm process technology. The\nphysical design of the microprocessor is partitioned and implemented in a\n2-tier, 3D stacked configuration with logic blocks and memory instances in\nseparate tiers (logic-over-memory 3D). The thermal simulation model was\ncalibrated to temperature measurement data from a high-performance, CPU-based\n2D SoC chip fabricated on the same 7nm process technology. Thermal profiles of\ndifferent 3D configurations under various workload conditions are simulated and\ncompared. We find that stacking microprocessor designs in 3D without\nconsidering thermal implications can result in maximum die temperature up to\n12{\\deg}C higher than their 2D counterparts under the worst-case\npower-indicative workload. This increase in temperature would reduce the amount\nof time for which a power-intensive workload can be run before throttling is\nrequired. However, logic-over-memory partitioned 3D CPU implementation can\nmitigate this temperature increase by half, which makes the temperature of the\n3D design only 6$^\\circ$C higher than the 2D baseline. We conclude that using\nthermal aware design partitioning and improved cooling techniques can overcome\nthe thermal challenges associated with 3D stacking.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:08:09 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Mathur", "Rahul", ""], ["Chao", "Chien-Ju", ""], ["Liu", "Rossana", ""], ["Tadepalli", "Nikhil", ""], ["Chandupatla", "Pranavi", ""], ["Hung", "Shawn", ""], ["Xu", "Xiaoqing", ""], ["Sinha", "Saurabh", ""], ["Kulkarni", "Jaydeep", ""]]}]