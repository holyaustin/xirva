[{"id": "1306.0089", "submitter": "Suranjan Chakraborty", "authors": "Amitabha Sinha, Mitrava Sarkar, Soumojit Acharyya, Suranjan\n  Chakraborty", "title": "A Novel Reconfigurable Architecture of a DSP Processor for Efficient\n  Mapping of DSP Functions using Field Programmable DSP Arrays", "comments": "8 Pages, 12 Figures, ACM SIGARCH Computer Architecture News. arXiv\n  admin note: substantial text overlap with arXiv:1305.3251", "journal-ref": "ACM SIGARCH Computer Architecture News, Volume 41 Issue 2, May\n  2013, Pages 1-8", "doi": "10.1145/2490302.2490304", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of modern integrated circuit technologies makes it feasible to\ndevelop cheaper, faster and smaller special purpose signal processing function\ncircuits. Digital Signal processing functions are generally implemented either\non ASICs with inflexibility, or on FPGAs with bottlenecks of relatively smaller\nutilization factor or lower speed compared to ASIC. Field Programmable DSP\nArray (FPDA) is the proposed DSP dedicated device, redolent to FPGA, but with\nbasic fixed common modules (CMs) (like adders, subtractors, multipliers,\nscaling units, shifters) instead of CLBs. This paper introduces the development\nof reconfigurable system architecture with a focus on FPDA that integrates\ndifferent DSP functions like DFT, FFT, DCT, FIR, IIR, and DWT etc. The\nswitching between DSP functions is occurred by reconfiguring the\ninterconnection between CMs. Validation of the proposed architecture has been\nachieved on Virtex5 FPGA. The architecture provides sufficient amount of\nflexibility, parallelism and scalability.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 09:04:40 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Sinha", "Amitabha", ""], ["Sarkar", "Mitrava", ""], ["Acharyya", "Soumojit", ""], ["Chakraborty", "Suranjan", ""]]}, {"id": "1306.1889", "submitter": "Pradeep  Singla", "authors": "Aakash Gupta, Pradeep Singla, Jitendra Gupta, Nitin Maheshwari", "title": "An Improved Structure Of Reversible Adder And Subtractor", "comments": null, "journal-ref": "International Journal of Electronics and Computer Science\n  Engineering, Vol 2, No. 2, pp712-718, June 2013", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world everyday a new technology which is faster, smaller and more\ncomplex than its predecessor is being developed. The increased number of\ntransistors packed onto a chip of a conventional system results in increased\npower consumption that is why Reversible logic has drawn attention of\nResearchers due to its less heat dissipating characteristics. Reversible logic\ncan be imposed over applications such as quantum computing, optical computing,\nquantum dot cellular automata, low power VLSI circuits, DNA computing. This\npaper presents the reversible combinational circuit of adder, subtractor and\nparity preserving subtractor. The suggested circuit in this paper are designed\nusing Feynman, Double Feynman and MUX gates which are better than the existing\none in literature in terms of Quantum cost, Garbage output and Total logical\ncalculations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 07:21:22 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Gupta", "Aakash", ""], ["Singla", "Pradeep", ""], ["Gupta", "Jitendra", ""], ["Maheshwari", "Nitin", ""]]}, {"id": "1306.1916", "submitter": "Kirat Pal Er", "authors": "Kirat Pal Singh, Dilip Kumar", "title": "Performance Evaluation of Low Power MIPS Crypto Processor based on\n  Cryptography Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design and implementation of low power 32-bit\nencrypted and decrypted MIPS processor for Data Encryption Standard (DES),\nTriple DES, Advanced Encryption Standard (AES) based on MIPS pipeline\narchitecture. The organization of pipeline stages has been done in such a way\nthat pipeline can be clocked at high frequency. Encryption and Decryption\nblocks of three standard cryptography algorithms on MIPS processor and\ndependency among themselves are explained in detail with the help of a block\ndiagram. Clock gating technique is used to reduce the power consumption in MIPS\ncrypto processor. This approach results in processor that meets power\nconsumption and performance specification for security applications. Proposed\nImplementation approach concludes higher system performance while reducing\noperating power consumption. Testing results shows that the MIPS crypto\nprocessor operates successfully at a working frequency of 218MHz and a\nbandwidth of 664Mbits/s.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 13:18:48 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Singh", "Kirat Pal", ""], ["Kumar", "Dilip", ""]]}, {"id": "1306.3109", "submitter": "Leonid Yavits", "authors": "Leonid Yavits, Amir Morad, Ran Ginosar", "title": "Computer Architecture with Associative Processor Replacing Last Level\n  Cache and SIMD Accelerator", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  equation 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a novel computer architecture where a last level cache\nand a SIMD accelerator are replaced by an Associative Processor. Associative\nProcessor combines data storage and data processing and provides parallel\ncomputational capabilities and data memory at the same time. An analytic\nperformance model of the new computer architecture is introduced. Comparative\nanalysis supported by simulation shows that this novel architecture may\noutperform a conventional architecture comprising a SIMD coprocessor and a\nshared last level cache while consuming less power.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 14:02:13 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 09:38:43 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Ginosar", "Ran", ""]]}, {"id": "1306.3302", "submitter": "Leonid Yavits", "authors": "Leonid Yavits, Amir Morad, Ran Ginosar", "title": "The Effect of Communication and Synchronization on Amdahl Law in\n  Multicore Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work analyses the effects of sequential-to-parallel synchronization and\ninter-core communication on multicore performance, speedup and scaling. A\nmodification of Amdahl law is formulated, to reflect the finding that parallel\nspeedup is lower than originally predicted, due to these effects. In\napplications with high inter-core communication requirements, the workload\nshould be executed on a small number of cores, and applications of high\nsequential-to-parallel synchronization requirements may better be executed by\nthe sequential core, even when f, the Amdahl fraction of parallelization, is\nvery close to 1. To improve the scalability and performance speedup of a\nmulticore, it is as important to address the synchronization and connectivity\nintensities of parallel algorithms as their parallelization factor.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 06:20:44 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Yavits", "Leonid", ""], ["Morad", "Amir", ""], ["Ginosar", "Ran", ""]]}, {"id": "1306.5501", "submitter": "Hu Li", "authors": "Hu Li, Yuan`an Liu, Dongming Yuan, Hefei Hu", "title": "A Wrapper of PCI Express with FIFO Interfaces based on FPGA", "comments": "5 pages, 8 figures", "journal-ref": "Proceedings of the 2012 International Conference on Industrial\n  Control and Electronics Engineering, ICICEE 2012", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with\nFIFO interfaces. Compared with other PCIE solutions, PWrapper has several\nadvantages such as flexibility, isolation of clock domain, etc. PWrapper is\nimplemented and verified on Vertex -5-FX70T which is a development board\nprovided by Xilinx Inc. Architecture of PWrapper and design of two key modules\nare illustrated, which timing optimization methods have been adopted. Then we\nexplained the advantages and challenges of on-chip interfaces technology based\non FIFOs. The verification results show that PWrapper can achieve the speed of\n1.8Gbps (Giga bits per second).\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 02:50:32 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Li", "Hu", ""], ["Liu", "Yuan`an", ""], ["Yuan", "Dongming", ""], ["Hu", "Hefei", ""]]}, {"id": "1306.6133", "submitter": "Fabio Lorenzo Traversa Ph.D.", "authors": "Fabio Lorenzo Traversa, Fabrizio Bonani, Yuriy V. Pershin,\n  Massimiliano Di Ventra", "title": "Dynamic Computing Random Access Memory", "comments": null, "journal-ref": "Nanotechnology, vol. 25, is. 8, pg. 285201 (10pp), year 2014", "doi": "10.1088/0957-4484/25/28/285201", "report-no": null, "categories": "cs.ET cond-mat.mes-hall cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present von Neumann computing paradigm involves a significant amount of\ninformation transfer between a central processing unit (CPU) and memory, with\nconcomitant limitations in the actual execution speed. However, it has been\nrecently argued that a different form of computation, dubbed memcomputing\n[Nature Physics, 9, 200-202 (2013)] and inspired by the operation of our brain,\ncan resolve the intrinsic limitations of present day architectures by allowing\nfor computing and storing of information on the same physical platform. Here we\nshow a simple and practical realization of memcomputing that utilizes\neasy-to-build memcapacitive systems. We name this architecture Dynamic\nComputing Random Access Memory (DCRAM). We show that DCRAM provides\nmassively-parallel and polymorphic digital logic, namely it allows for\ndifferent logic operations with the same architecture, by varying only the\ncontrol signals. In addition, by taking into account realistic parameters, its\nenergy expenditures can be as low as a few fJ per operation. DCRAM is fully\ncompatible with CMOS technology, can be realized with current fabrication\nfacilities, and therefore can really serve as an alternative to the present\ncomputing technology.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 05:12:46 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 18:31:46 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Traversa", "Fabio Lorenzo", ""], ["Bonani", "Fabrizio", ""], ["Pershin", "Yuriy V.", ""], ["Di Ventra", "Massimiliano", ""]]}]