[{"id": "1806.00318", "submitter": "Zhi-Yue Wang", "authors": "Zhi-yue Wang, Tian-kuan Liu, Qi-jie Tang, Yi Feng, Jian Wang", "title": "A programmable clock generator for automatic Quality Assurance of LOCx2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The upgrade of ATLAS Liquid Argon Calorimeter (LAr) Phase-1 trigger requires\nhigh-speed, low-latency data transmission to read out the Lar Trigger Digitizer\nBoard (LTDB). A dual-channel transmitter ASIC LOCx2 have been designed and\nproduced. In order to ensure all the LOCx2 chips behave properly, a Quality\nAssurance needs to be conducted before assembly. The problem I was trying to\nsolve in this project is to yield a clock signal with continuously adjustable\nfrequency and phase offset to generate and control an eye diagram for the QA.\nBy configuring the registers of an any-frequency generator IC, Si5338, the\nclock signal whose frequency range from 5MHz to 200 MHz have been properly\nproduced. For the purpose of further development, a C-language based DLL which\npacks up the function of adjusting frequency and setting phase offset was\ndesigned and built, and several evaluation was performed to ensure the\nrobustness of DLL.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 12:45:56 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Wang", "Zhi-yue", ""], ["Liu", "Tian-kuan", ""], ["Tang", "Qi-jie", ""], ["Feng", "Yi", ""], ["Wang", "Jian", ""]]}, {"id": "1806.00751", "submitter": "Pengcheng Yao", "authors": "Pengcheng Yao", "title": "An Efficient Graph Accelerator with Parallel Data Conflict Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-specific computing with the support of dedicated accelerator has\ngreatly boosted the graph processing in both efficiency and energy.\nNevertheless, their data conflict management is still sequential in essential\nwhen some vertex needs a large number of conflicting updates at the same time,\nleading to prohibitive performance degradation. This is particularly true for\nprocessing natural graphs.\n  In this paper, we have the insight that the atomic operations for the vertex\nupdating of many graph algorithms (e.g., BFS, PageRank and WCC) are typically\nincremental and simplex. This hence allows us to parallelize the conflicting\nvertex updates in an accumulative manner. We architect a novel graphspecific\naccelerator that can simultaneously process atomic vertex updates for massive\nparallelism on the conflicting data access while ensuring the correctness. A\nparallel accumulator is designed to remove the serialization in atomic\nprotection for conflicting vertex updates through merging their results in\nparallel. Our implementation on Xilinx Virtex UltraScale+ XCVU9P with a wide\nvariety of typical graph algorithms shows that our accelerator achieves an\naverage throughput by 2.36 GTEPS as well as up to 3.14x performance speedup in\ncomparison with state-of-the-art ForeGraph (with single-chip version).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 08:34:34 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yao", "Pengcheng", ""]]}, {"id": "1806.00776", "submitter": "Xiaoyuan Wang", "authors": "Xiaoyuan Wang", "title": "Supporting Superpages and Lightweight Page Migration in Hybrid Memory\n  Systems", "comments": "10 pages,15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpages have long been used to mitigate address translation overhead in\nbig memory systems. However, superpages often preclude lightweight page\nmigration, which is crucial for performance and energy efficiency in hybrid\nmemory systems composed of DRAM and non-volatile memory (NVM). In this paper,\nwe propose a novel memory management mechanism called \\textit{Rainbow} to\nbridge this fundamental conflict between superpages and lightweight page\nmigration. \\textit{Rainbow} manages NVM at the superpage granularity, and uses\nDRAM to cache frequently-accessed (hot) small pages in each superpage.\nCorrespondingly, \\textit{Rainbow} utilizes split TLBs to support different page\nsizes. By introducing an efficient hot page identification mechanism and a\nnovel NVM-to-DRAM address remapping mechanism, \\textit{Rainbow} supports\nlightweight page migration while without splintering superpages. Experimental\nresults show that Rainbow can significantly reduce applications' TLB misses by\n99.8\\%, and improve application performance (IPC) by up to 2.9X (43.0\\% on\naverage) when compared to a state-of-the-art memory migration policy without\nsuperpage support.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:58:27 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Wang", "Xiaoyuan", ""]]}, {"id": "1806.00779", "submitter": "Ye Chi", "authors": "Ye Chi", "title": "Gemini: Reducing DRAM Cache Hit Latency by Hybrid Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Die-stacked DRAM caches are increasingly advocated to bridge the performance\ngap between on-chip Cache and main memory. It is essential to improve DRAM\ncache hit rate and lower cache hit latency simultaneously. Prior DRAM cache\ndesigns fall into two categories according to the data mapping polices:\nset-associative and direct-mapped, achieving either one. In this paper, we\npropose a partial direct-mapped die-stacked DRAM cache to achieve the both\nobjectives simultaneously, called Gemini, which is motivated by the following\nobservations: applying unified mapping policy to different blocks cannot\nachieve high cache hit rate and low hit latency in terms of mapping structure.\nGemini cache classifies data into leading blocks and following blocks, and\nplaces them with static mapping and dynamic mapping respectively in a unified\nset-associative structure. Gemini also designs a replacement policy to balance\nthe different blocks miss penalty and the recency, and provides strategies to\nmitigate cache thrashing due to block type transitions. Experimental results\ndemonstrate that Gemini cache can narrow the hit latency gap with direct-mapped\ncache significantly, from 1.75X to 1.22X on average, and can achieve comparable\nhit rate with set-associative cache. Compared with the state-of-the-art\nbaselines, i.e., enhanced Loh-Hill cache, Gemini improves the IPC by up to 20%\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 12:29:26 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Chi", "Ye", ""]]}, {"id": "1806.01107", "submitter": "Amir Yazdanbakhsh", "authors": "Amir Yazdanbakhsh, Hajar Falahati, Philip J. Wolfe, Kambiz Samadi, Nam\n  Sung Kim, and Hadi Esmaeilzadeh", "title": "GANAX: A Unified MIMD-SIMD Acceleration for Generative Adversarial\n  Networks", "comments": "Proceedings of the 45th International Symposium on Computer\n  Architecture (ISCA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are one of the most recent deep\nlearning models that generate synthetic data from limited genuine datasets.\nGANs are on the frontier as further extension of deep learning into many\ndomains (e.g., medicine, robotics, content synthesis) requires massive sets of\nlabeled data that is generally either unavailable or prohibitively costly to\ncollect. Although GANs are gaining prominence in various fields, there are no\naccelerators for these new models. In fact, GANs leverage a new operator,\ncalled transposed convolution, that exposes unique challenges for hardware\nacceleration. This operator first inserts zeros within the multidimensional\ninput, then convolves a kernel over this expanded array to add information to\nthe embedded zeros. Even though there is a convolution stage in this operator,\nthe inserted zeros lead to underutilization of the compute resources when a\nconventional convolution accelerator is employed. We propose the GANAX\narchitecture to alleviate the sources of inefficiency associated with the\nacceleration of GANs using conventional convolution accelerators, making the\nfirst GAN accelerator design possible. We propose a reorganization of the\noutput computations to allocate compute rows with similar patterns of zeros to\nadjacent processing engines, which also avoids inconsequential multiply-adds on\nthe zeros. This compulsory adjacency reclaims data reuse across these\nneighboring processing engines, which had otherwise diminished due to the\ninserted zeros. The reordering breaks the full SIMD execution model, which is\nprominent in convolution accelerators. Therefore, we propose a unified\nMIMD-SIMD design for GANAX that leverages repeated patterns in the computation\nto create distinct microprograms that execute concurrently in SIMD mode.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 22:54:29 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Yazdanbakhsh", "Amir", ""], ["Falahati", "Hajar", ""], ["Wolfe", "Philip J.", ""], ["Samadi", "Kambiz", ""], ["Kim", "Nam Sung", ""], ["Esmaeilzadeh", "Hadi", ""]]}, {"id": "1806.01108", "submitter": "Ellis Giles", "authors": "Ellis Giles, Kshitij Doshi, Peter Varman", "title": "Hardware Transactional Persistent Memory", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging Persistent Memory technologies (also PM, Non-Volatile DIMMs, Storage\nClass Memory or SCM) hold tremendous promise for accelerating popular\ndata-management applications like in-memory databases. However, programmers now\nneed to deal with ensuring the atomicity of transactions on Persistent Memory\nresident data and maintaining consistency between the order in which processors\nperform stores and that in which the updated values become durable.\n  The problem is specially challenging when high-performance isolation\nmechanisms like Hardware Transactional Memory (HTM) are used for concurrency\ncontrol. This work shows how HTM transactions can be ordered correctly and\natomically into PM by the use of a novel software protocol combined with a\nPersistent Memory Controller, without requiring changes to processor cache\nhardware or HTM protocols. In contrast, previous approaches require significant\nchanges to existing processor microarchitectures. Our approach, evaluated using\nboth micro-benchmarks and the STAMP suite compares well with standard\n(volatile) HTM transactions. It also yields significant gains in throughput and\nlatency in comparison with persistent transactional locking.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:14:01 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Giles", "Ellis", ""], ["Doshi", "Kshitij", ""], ["Varman", "Peter", ""]]}, {"id": "1806.01683", "submitter": "Kamel Abdelouahab", "authors": "Kamel Abdelouahab and Maxime Pelcat and Jocelyn Serot and Fran\\c{c}ois\n  Berry", "title": "Accelerating CNN inference on FPGAs: A Survey", "comments": "Cloning our HAL submission in ArXiv, Technical Report - Universite\n  Clermont Auvergne, January 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are currently adopted to solve an ever\ngreater number of problems, ranging from speech recognition to image\nclassification and segmentation. The large amount of processing required by\nCNNs calls for dedicated and tailored hardware support methods. Moreover, CNN\nworkloads have a streaming nature, well suited to reconfigurable hardware\narchitectures such as FPGAs. The amount and diversity of research on the\nsubject of CNN FPGA acceleration within the last 3 years demonstrates the\ntremendous industrial and academic interest. This paper presents a\nstate-of-the-art of CNN inference accelerators over FPGAs. The computational\nworkloads, their parallelism and the involved memory accesses are analyzed. At\nthe level of neurons, optimizations of the convolutional and fully connected\nlayers are explained and the performances of the different methods compared. At\nthe network level, approximate computing and datapath optimization methods are\ncovered and state-of-the-art approaches compared. The methods and tools\ninvestigated in this survey represent the recent trends in FPGA CNN inference\naccelerators and will fuel the future advances on efficient hardware deep\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 12:24:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Abdelouahab", "Kamel", ""], ["Pelcat", "Maxime", ""], ["Serot", "Jocelyn", ""], ["Berry", "Fran\u00e7ois", ""]]}, {"id": "1806.02011", "submitter": "Jiliang Zhang", "authors": "Jiliang Zhang and Chaoqun Shen", "title": "Set-based Obfuscation for Strong PUFs against Machine Learning Attacks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strong physical unclonable function (PUF) is a promising solution for device\nauthentication in resourceconstrained applications but vulnerable to machine\nlearning attacks. In order to resist such attack, many defenses have been\nproposed in recent years. However, these defenses incur high hardware overhead,\ndegenerate reliability and are inefficient against advanced machine learning\nattacks such as approximation attacks. In order to address these issues, we\npropose a Random Set-based Obfuscation (RSO) for Strong PUFs to resist machine\nlearning attacks. The basic idea is that several stable responses are derived\nfrom the PUF itself and pre-stored as the set for obfuscation in the testing\nphase, and then a true random number generator is used to select any two keys\nto obfuscate challenges and responses with XOR operations. When the number of\nchallenge-response pairs (CRPs) collected by the attacker exceeds the given\nthreshold, the set will be updated immediately. In this way, machine learning\nattacks can be prevented with extremely low hardware overhead. Experimental\nresults show that for a 64x64 Arbiter PUF, when the size of set is 32 and even\nif 1 million CRPs are collected by attackers, the prediction accuracies of\nLogistic regression, support vector machines, artificial neural network,\nconvolutional neural network and covariance matrix adaptive evolutionary\nstrategy are about 50% which is equivalent to the random guessing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 05:24:34 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 01:30:07 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 05:53:24 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 04:36:14 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhang", "Jiliang", ""], ["Shen", "Chaoqun", ""]]}, {"id": "1806.02271", "submitter": "Pritam Bhattacharjee", "authors": "Dhiraj Sarkar, Pritam Bhattacharjee and Alak Majumder", "title": "Data-Dependent Clock Gating approach for Low Power Sequential System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Power dissipation in the sequential systems of modern CPU integrated chips\n(CPU-IC viz., Silicon Chip) is in discussion since the last decade. Researchers\nhave been cultivating many low power design methods to choose the best\npotential candidate for reducing both static and dynamic power of a chip.\nThough, clock gating (CG) has been an accepted technique to control dynamic\npower dissipation, question still loiters on its credibility to handle the\nstatic power of the system. Therefore in this paper, we have revisited the\npopular CG schemes and found out some scope of improvisation to support the\nsimultaneous reduction of static and dynamic power dissipation. Our proposed CG\nis simulated for 90nm CMOS using Cadence Virtuoso and has been tested on a\nconventional Master-Slave Flip-flop at 5GHz clock with a power supply of\n1.1Volt. This assignment clearly depicts its supremacy in terms of power and\ntiming metrics in comparison to the implementation of existing CG schemes.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 14:20:54 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Sarkar", "Dhiraj", ""], ["Bhattacharjee", "Pritam", ""], ["Majumder", "Alak", ""]]}, {"id": "1806.02498", "submitter": "SeyedMohammad Seyedzadeh", "authors": "Seyed Mohammad Seyedzadeh, Alex K. Jones, Rami Melhem", "title": "Mitigating Wordline Crosstalk using Adaptive Trees of Counters", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High access frequency of certain rows in the DRAM may cause data loss in\ncells of physically adjacent rows due to crosstalk. The malicious exploit of\nthis crosstalk by repeatedly accessing a row to induce this effect is known as\nrow hammering. Additionally, inadvertent row hammering may also occur due to\nthe natural weighted nature of applications' access patterns.\n  In this paper, we analyze the efficiency of existing approaches for\nmitigating wordline crosstalk and demonstrate that they have been\nconservatively designed. Given the unbalanced nature of DRAM accesses, a small\ngroup of dynamically allocated counters in banks can deterministically detect\nhot rows and mitigate crosstalk. Based on our findings, we propose a\nCounter-based Adaptive Tree (CAT) approach to mitigate wordline crosstalk using\nadaptive trees of counters to guide appropriate refreshing of vulnerable rows.\nThe key idea is to tune the distribution of the counters to the rows in a bank\nbased on the memory reference patterns. In contrast to deterministic solutions,\nCAT utilizes fewer counters, making it practically feasible to be implemented\non-chip. Compared to existing probabilistic approaches, CAT more precisely\nrefreshes rows vulnerable to crosstalk based on their access frequency.\n  Experimental results on workloads from four benchmark suites show that CAT\nreduces the Crosstalk Mitigation Refresh Power Overhead in quad-core systems to\n7%, which is an improvement over the 21% and 18% incurred in the leading\ndeterministic and probabilistic approaches, respectively. Moreover, CAT incurs\nvery low performance overhead (0.5%). Hardware synthesis evaluation shows that\nCAT can be implemented on-chip with only a nominal area overhead.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 03:22:29 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Seyedzadeh", "Seyed Mohammad", ""], ["Jones", "Alex K.", ""], ["Melhem", "Rami", ""]]}, {"id": "1806.03716", "submitter": "Rozita Teymourzadeh", "authors": "Rozita Teymourzadeh, Mometo Jim Abigo, Mok Vee Hoong", "title": "Static Quantized Radix-2 FFT/IFFT Processor for Constraints Analysis", "comments": null, "journal-ref": "International Journal of Electronics Taylor and Francis 2013, pp\n  1-10", "doi": "10.1080/00207217.2013.780264", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research work focuses on the design of a high-resolution fast Fourier\ntransform (FFT) /inverse fast Fourier transform (IFFT) processors for\nconstraints analysis purpose. Amongst the major setbacks associated with such\nhigh resolution, FFT processors are the high power consumption resulting from\nthe structural complexity and computational inefficiency of floating-point\ncalculations. As such, a parallel pipelined architecture was proposed to\nstatically scale the resolution of the processor to suite adequate trade-off\nconstraints. The quantization was applied to provide an approximation to\naddress the finite word-length constraints of digital signal processing (DSP).\nAn optimum operating mode was proposed, based on the\nsignal-to-quantization-noise ratio (SQNR) as well as the statistical theory of\nquantization, to minimize the tradeoff issues associated with selecting the\nmost application-efficient floating-point processing capability in contrast to\ntheir resolution quality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 20:00:22 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Teymourzadeh", "Rozita", ""], ["Abigo", "Mometo Jim", ""], ["Hoong", "Mok Vee", ""]]}, {"id": "1806.03721", "submitter": "Rozita Teymourzadeh", "authors": "Rozita Teymourzadeh", "title": "VLSI Design Of Advanced Digital Filters", "comments": "Academic Publishing 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Cascaded Integrator Comb filters (CIC) find many applications in recent\nelectronic devices such as frequency selection functions in a digital radio or\nmodem and any filter structure that is required to efficiently process large\nsample rate factor. These filters are normally located after the sigma-delta\nmodulator and have a regular structure. These types of filters do not require\nmultipliers and the coefficient storage unlike in the normal digital FIR and\nIIR filters because of all filter coefficients are unity. Hence, it can be\nefficiently implemented to operate at high speed. Hence, this book describes\nthe Very Large Scale Integration (VLSI) implementation of the CIC filters that\nare suitable for high-performance audio applications.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 20:52:57 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Teymourzadeh", "Rozita", ""]]}, {"id": "1806.04038", "submitter": "Lu Houbing", "authors": "Houbing Lu, Feng Li, Peng Miao, Kun Hu, Zhilei Zhang, Rongqi Sun,\n  Qingli Ma, and Ge Jin", "title": "Development of FEB Configuration Test Board for ATLAS NSW Upgrade", "comments": null, "journal-ref": null, "doi": "10.1109/TNS.2019.2929232", "report-no": null, "categories": "physics.ins-det cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FEB(front end board) configuration test board is developed aiming at\nmeeting the requirement of testing the new generation ASIC(application-specific\nintegrated circuit) chips and its configuration system for ATLAS NSW(New Small\nWheel) upgrade, In this paper, some functions are developed in terms of the\nconfigurations of the key chips on the FEB, VMM3 and TDS2 using GBT-SCA.\nAdditionally, a flexible communication protocol is designed, verifying the\nwhole data link. It provides technical reference for prototype FEB key chip\nconfiguration and data readout, as well as the final system configuration.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:02:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lu", "Houbing", ""], ["Li", "Feng", ""], ["Miao", "Peng", ""], ["Hu", "Kun", ""], ["Zhang", "Zhilei", ""], ["Sun", "Rongqi", ""], ["Ma", "Qingli", ""], ["Jin", "Ge", ""]]}, {"id": "1806.04570", "submitter": "Rozita Teymourzadeh", "authors": "Yazan Samir Algnabi, Rozita Teymourzadeh, Masuri Othman, Md Shabiul\n  Islam", "title": "FPGA Implementation of pipeline Digit-Slicing Multiplier-Less Radix 2\n  power of 2 DIF SDF Butterfly for Fourier Transform Structure", "comments": "European Conference on Antennas and Propagation (EUCAP2011). Pp 4168-\n  4172", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for wireless communication has driven the communication systems to\nhigh performance. However, the main bottleneck that affects the communication\ncapability is the Fast Fourier Transform (FFT), which is the core of most\nmodulators. This paper presents FPGA implementation of pipeline digit-slicing\nmultiplier-less radix 22 DIF (Decimation In Frequency) SDF (single path delay\nfeedback) butterfly for FFT structure. The approach is taken, in order to\nreduce computation complexity in butterfly multiplier, the digit-slicing\nmultiplier-less technique was utilized in the critical path of pipeline\nRadix-22 DIF SDF FFT structure. The proposed design focused on the trade-off\nbetween the speed and active silicon area for the chip implementation. The\nmultiplier input data was sliced into four blocks each one with four bits to\nprocess at the same time in parallel. The new architecture was investigated and\nsimulated with MATLAB software. The Verilog HDL code in Xilinx ISE environment\nwas derived to describe the FFT Butterfly functionality and was downloaded to\nVirtex II FPGA board. Consequently, the Virtex-II FG456 Proto board was used to\nimplement and test the design on the real hardware. As a result, from the\nfindings, the synthesis report indicates the maximum clock frequency of 555.75\nMHz with the total equivalent gate count of 32,146 is a marked and significant\nimprovement over Radix 22 DIF SDF FFT butterfly. In comparison with the\nconventional butterfly architecture design which can only run at a maximum\nclock frequency of 200.102 MHz and the conventional multiplier can only run at\na maximum clock frequency of 221.140 MHz, the proposed system exhibits better\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 21:58:52 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Algnabi", "Yazan Samir", ""], ["Teymourzadeh", "Rozita", ""], ["Othman", "Masuri", ""], ["Islam", "Md Shabiul", ""]]}, {"id": "1806.04572", "submitter": "Rozita Teymourzadeh", "authors": "Rozita Teymourzadeh, Memtode Jim, Mok Vee hong", "title": "Characteristic Analysis of 1024-Point Quantized Radix-2 FFT/IFFT\n  Processor", "comments": "ICSE2012 Proc.International Conference on Semiconductor Electronics\n  (ICSE-2012). Pp 664-668.ISBN: 978-1-4673-2395-6", "journal-ref": null, "doi": "10.1109/SMElec.2012.6417231", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The precise analysis and accurate measurement of harmonic provides a reliable\nscientific industrial application. However, the high-performance DSP processor\nis the important method of electrical harmonic analysis. Hence, in this\nresearch work, the effort was taken to design a novel high-resolution single\n1024-point fast Fourier transform (FFT) and inverse fast Fourier transform\n(IFFT) processors for improvement of the harmonic measurement techniques.\nMeanwhile, the project is started with design and simulation to demonstrate the\nbenefit that is achieved by the proposed 1024-point FFT/IFFT processor. The\npipelined structure is incorporated in order to enhance the system efficiency.\nAs such, a pipelined architecture was proposed to statically scale the\nresolution of the processor to suite adequate trade-off constraints. The\nproposed FFT makes use of programmable fixed-point/floating-point to realize\nhigher precision FFT.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 05:58:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Teymourzadeh", "Rozita", ""], ["Jim", "Memtode", ""], ["hong", "Mok Vee", ""]]}, {"id": "1806.04573", "submitter": "Rozita Teymourzadeh", "authors": "Yazan Samir Algnabi, Furat A. Aldaamee, Rozita Teymourzadeh", "title": "Novel Architecture of Pipeline Radix 2 power of 2 SDF FFT Based on\n  Digit-Slicing Technique", "comments": "ICSE2012 Proc.International Conference on Semiconductor Electronics\n  (ICSE-2012). Pp 470-474. ISBN: 978-1-4673-2395-6. arXiv admin note: text\n  overlap with arXiv:1806.04570", "journal-ref": null, "doi": "10.1109/SMElec.2012.6417188", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prevalent need for very high-speed digital signals processing in wireless\ncommunications has driven the communications system to high-performance levels.\nThe objective of this paper is to propose a novel structure for efficient\nimplementation for the Fast Fourier Transform (FFT) processor to meet the\nrequirement for high-speed wireless communication system standards. Based on\nthe algorithm, architecture analysis, the design of pipeline Radix 2power of 2\nSDF FFT processor based on digit-slicing Multiplier-Less is proposed.\nFurthermore, this paper proposed an optimal constant multiplication arithmetic\ndesign to multiply a fixed point input selectively by one of the several\npresent twiddle factor constants. The proposed architecture was simulated using\nMATLAB software and the Field Programmable Gate Array (FPGA) Virtex 4 was\ntargeted to synthesis the proposed architecture. The design was tested in real\nhardware of TLA5201 logic analyzer and the ISE synthesis report results the\nhigh speed of 669.277 MHz with the total equivalent gate count of 14,854.\nMeanwhile, It can be found as significant improvement over Radix 22 DIF SDF FFT\nprocessor and can be concluded that the proposed pipeline Radix 22 DIF SDF FFT\nprocessor based on digit-slicing multiplier-less is an enable in solving\nproblems that affect the most high-speed wireless communication systems\ncapability in FFT and possesses huge potentials for future related works and\nresearch areas.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 06:05:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Algnabi", "Yazan Samir", ""], ["Aldaamee", "Furat A.", ""], ["Teymourzadeh", "Rozita", ""]]}, {"id": "1806.05179", "submitter": "Khaled Khasawneh", "authors": "Khaled N. Khasawneh, Esmaeil Mohammadian Koruyeh, Chengyu Song, Dmitry\n  Evtyushkin, Dmitry Ponomarev, Nael Abu-Ghazaleh", "title": "SafeSpec: Banishing the Spectre of a Meltdown with Leakage-Free\n  Speculation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speculative execution which is used pervasively in modern CPUs can leave side\neffects in the processor caches and other structures even when the speculated\ninstructions do not commit and their direct effect is not visible. The recent\nMeltdown and Spectre attacks have shown that this behavior can be exploited to\nexpose privileged information to an unprivileged attacker. In particular, the\nattack forces the speculative execution of a code gadget that will carry out\nthe illegal read, which eventually gets squashed, but which leaves a\nside-channel trail that can be used by the attacker to infer the value. Several\nattack variations are possible, allowing arbitrary exposure of the full kernel\nmemory to an unprivileged attacker. In this paper, we introduce a new model\n(SafeSpec) for supporting speculation in a way that is immune to side-channel\nleakage necessary for attacks such as Meltdown and Spectre. In particular,\nSafeSpec stores side effects of speculation in a way that is not visible to the\nattacker while the instructions are speculative. The speculative state is then\neither committed to the main CPU structures if the branch commits, or squashed\nif it does not, making all direct side effects of speculative code invisible.\nThe solution must also address the possibility of a covert channel from\nspeculative instructions to committed instructions before these instructions\nare committed. We show that SafeSpec prevents all three variants of Spectre and\nMeltdown, as well as new variants that we introduce. We also develop a cycle\naccurate model of modified design of an x86-64 processor and show that the\nperformance impact is negligible. We build prototypes of the hardware support\nin a hardware description language to show that the additional overhead is\nsmall. We believe that SafeSpec completely closes this class of attacks, and\nthat it is practical to implement.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:05:04 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 00:59:50 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Khasawneh", "Khaled N.", ""], ["Koruyeh", "Esmaeil Mohammadian", ""], ["Song", "Chengyu", ""], ["Evtyushkin", "Dmitry", ""], ["Ponomarev", "Dmitry", ""], ["Abu-Ghazaleh", "Nael", ""]]}, {"id": "1806.05794", "submitter": "Mohsen Imani", "authors": "Mohsen Imani, Mohammad Samragh, Yeseong Kim, Saransh Gupta, Farinaz\n  Koushanfar, Tajana Rosing", "title": "RAPIDNN: In-Memory Deep Neural Network Acceleration Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have demonstrated effectiveness for various\napplications such as image processing, video segmentation, and speech\nrecognition. Running state-of-the-art DNNs on current systems mostly relies on\neither generalpurpose processors, ASIC designs, or FPGA accelerators, all of\nwhich suffer from data movements due to the limited onchip memory and data\ntransfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,\nwhich processes all DNN operations within the memory to minimize the cost of\ndata movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model\nand maps it into a specialized accelerator, which is designed using\nnon-volatile memory blocks that model four fundamental DNN operations, i.e.,\nmultiplication, addition, activation functions, and pooling. The framework\nextracts representative operands of a DNN model, e.g., weights and input\nvalues, using clustering methods to optimize the model for in-memory\nprocessing. Then, it maps the extracted operands and their precomputed results\ninto the accelerator memory blocks. At runtime, the accelerator identifies\ncomputation results based on efficient in-memory search capability which also\nprovides tunability of approximation to further improve computation efficiency.\nOur evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency\nimprovement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the\nstate-of-the-art DNN accelerators, while ensuring less than 0.3% of quality\nloss.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:07:55 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 05:05:01 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 03:09:32 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 18:36:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Imani", "Mohsen", ""], ["Samragh", "Mohammad", ""], ["Kim", "Yeseong", ""], ["Gupta", "Saransh", ""], ["Koushanfar", "Farinaz", ""], ["Rosing", "Tajana", ""]]}, {"id": "1806.06902", "submitter": "Xuan-Thuan Nguyen Dr", "authors": "Xuan-Thuan Nguyen, Trong-Thuc Hoang, Hong-Thu Nguyen, Katsumi Inoue,\n  and Cong-Kha Pham", "title": "A 1.2-V 162.9-pJ/cycle Bitmap Index Creation Core with 0.31-pW/bit\n  Standby Power on 65-nm SOTB", "comments": "Submitted to IEEE Transactions on Circuits and Systems II: Express\n  Brief", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to maximize the performance during peak workload hours and\nminimize the power consumption during off-peak time plays a significant role in\nthe energy-efficient systems. Our previous work has proposed a high-performance\nmulti-core bitmap index creator (BIC) in a field-programmable gate array that\ncould deliver higher indexing throughput than central processing units and\ngraphics processing units. This brief extends the previous study by focusing on\nthe application-specific integrated circuit implementation of the proposed BIC\nin a 65-nm silicon-on-thin-buried-oxide (SOTB) CMOS process. The BIC chip can\noperate with different supply voltage from 0.4 V to 1.2 V. In the active mode\nwith the supply voltage of 1.2 V, the BIC chip is fully operational at 41 MHz\nand consumes 162.9 pJ/cycle. In the standby mode with the supply voltage of 0.4\nV and clock-gating technique, the power consumption was reduced to 10.6 uW. The\nstandby power is also dramatically reduced to 2.64 nW due to the utilization of\nreverse back-gate biasing technique. This achievement is considerable\nimportance to the energy-efficient systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 19:52:34 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Nguyen", "Xuan-Thuan", ""], ["Hoang", "Trong-Thuc", ""], ["Nguyen", "Hong-Thu", ""], ["Inoue", "Katsumi", ""], ["Pham", "Cong-Kha", ""]]}, {"id": "1806.07480", "submitter": "Julian Stecklina", "authors": "Julian Stecklina and Thomas Prescher", "title": "LazyFP: Leaking FPU Register State using Microarchitectural\n  Side-Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors utilize an increasingly large register set to facilitate\nefficient floating point and SIMD computation. This large register set is a\nburden for operating systems, as its content needs to be saved and restored\nwhen the operating system context switches between tasks. As an optimization,\nthe operating system can defer the context switch of the FPU and SIMD register\nset until the first instruction is executed that needs access to these\nregisters. Meanwhile, the old content is left in place with the hope that the\ncurrent task might not use these registers at all. This optimization is\ncommonly called lazy FPU context switching. To make it possible, a processor\noffers the ability to toggle the availability of instructions utilizing\nfloating point and SIMD registers. If the instructions are turned off, any\nattempt of executing them will generate a fault.\n  In this paper, we present an attack that exploits lazy FPU context switching\nand allows an adversary to recover the FPU and SIMD register set of arbitrary\nprocesses or VMs. The attack works on processors that transiently execute FPU\nor SIMD instructions that follow an instruction generating the fault indicating\nthe first use of FPU or SIMD instructions. On operating systems using lazy FPU\ncontext switching, the FPU and SIMD register content of other processes or\nvirtual machines can then be reconstructed via cache side effects.\n  With SIMD registers not only being used for cryptographic computation, but\nalso increasingly for simple operations, such as copying memory, we argue that\nlazy FPU context switching is a dangerous optimization that needs to be turned\noff in all operating systems, if there is a chance that they run on affected\nprocessors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:59:59 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Stecklina", "Julian", ""], ["Prescher", "Thomas", ""]]}, {"id": "1806.08095", "submitter": "Thomas Preu{\\ss}er", "authors": "Thomas B. Preu{\\ss}er", "title": "Generic and Universal Parallel Matrix Summation with a Flexible\n  Compression Goal for Xilinx FPGAs", "comments": null, "journal-ref": null, "doi": "10.23919/FPL.2017.8056834", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bit matrix compression is a highly relevant operation in computer arithmetic.\nEssentially being a multi-operand addition, it is the key operation behind fast\nmultiplication and many higher-level operations such as multiply-accumulate,\nthe computation of the dot product or the implementation of FIR filters.\nCompressor implementations have been constantly evolving for greater efficiency\nboth in general and in the context of concrete applications or specific\nimplementation technologies. This paper is building on this history and\ndescribes a generic implementation of a bit matrix compressor for Xilinx FPGAs,\nwhich does not require a generator tool. It contributes FPGA-oriented metrics\nfor the evaluation of elementary parallel bit counters, a systematic analysis\nand partial decomposition of previously proposed counters and a fully\nimplemented construction heuristic with a flexible compression target matching\nthe device capabilities. The generic implementation is agnostic of the aspect\nratio of the input matrix and can be used for multiplication the same way as it\ncan be for single-column population count operations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 07:35:37 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Preu\u00dfer", "Thomas B.", ""]]}, {"id": "1806.08862", "submitter": "Yaman Umuroglu", "authors": "Yaman Umuroglu, Lahiru Rasnayake, Magnus Sjalander", "title": "BISMO: A Scalable Bit-Serial Matrix Multiplication Overlay for\n  Reconfigurable Computing", "comments": "To appear at FPL'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-matrix multiplication is a key computational kernel for numerous\napplications in science and engineering, with ample parallelism and data\nlocality that lends itself well to high-performance implementations. Many\nmatrix multiplication-dependent applications can use reduced-precision integer\nor fixed-point representations to increase their performance and energy\nefficiency while still offering adequate quality of results. However, precision\nrequirements may vary between different application phases or depend on input\ndata, rendering constant-precision solutions ineffective. We present BISMO, a\nvectorized bit-serial matrix multiplication overlay for reconfigurable\ncomputing. BISMO utilizes the excellent binary-operation performance of FPGAs\nto offer a matrix multiplication performance that scales with required\nprecision and parallelism. We characterize the resource usage and performance\nof BISMO across a range of parameters to build a hardware cost model, and\ndemonstrate a peak performance of 6.5 TOPS on the Xilinx PYNQ-Z1 board.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 21:30:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Umuroglu", "Yaman", ""], ["Rasnayake", "Lahiru", ""], ["Sjalander", "Magnus", ""]]}, {"id": "1806.09679", "submitter": "Behzad Salami", "authors": "Behzad Salami, Osman Unsal, Adrian Cristal", "title": "On the Resilience of RTL NN Accelerators: Fault Characterization and\n  Mitigation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) is making a strong resurgence in tune with the massive\ngeneration of unstructured data which in turn requires massive computational\nresources. Due to the inherently compute- and power-intensive structure of\nNeural Networks (NNs), hardware accelerators emerge as a promising solution.\nHowever, with technology node scaling below 10nm, hardware accelerators become\nmore susceptible to faults, which in turn can impact the NN accuracy. In this\npaper, we study the resilience aspects of Register-Transfer Level (RTL) model\nof NN accelerators, in particular, fault characterization and mitigation. By\nfollowing a High-Level Synthesis (HLS) approach, first, we characterize the\nvulnerability of various components of RTL NN. We observed that the severity of\nfaults depends on both i) application-level specifications, i.e., NN data\n(inputs, weights, or intermediate), NN layers, and NN activation functions, and\nii) architectural-level specifications, i.e., data representation model and the\nparallelism degree of the underlying accelerator. Second, motivated by\ncharacterization results, we present a low-overhead fault mitigation technique\nthat can efficiently correct bit flips, by 47.3% better than state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 08:52:18 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Salami", "Behzad", ""], ["Unsal", "Osman", ""], ["Cristal", "Adrian", ""]]}, {"id": "1806.11301", "submitter": "Chenyang Xia", "authors": "YouZhe Fan, ChenYang Xia, Ji Chen, Chi-Ying Tsui, Jie Jin, Hui Shen,\n  Bin Li", "title": "A Low-Latency List Successive-Cancellation Decoding Implementation for\n  Polar Codes", "comments": "15 pages, 13 figures, 5 tables", "journal-ref": "IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS, VOL. 34, NO. 2,\n  FEBRUARY 2016", "doi": "10.1109/JSAC.2015.2504318", "report-no": null, "categories": "cs.IT cs.AR eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their provably capacity-achieving performance, polar codes have\nattracted a lot of research interest recently. For a good error-correcting\nperformance, list successive-cancellation decoding (LSCD) with large list size\nis used to decode polar codes. However, as the complexity and delay of the list\nmanagement operation rapidly increase with the list size, the overall latency\nof LSCD becomes large and limits the applicability of polar codes in\nhigh-throughput and latency-sensitive applications. Therefore, in this work,\nthe low-latency implementation for LSCD with large list size is studied.\nSpecifically, at the system level, a selective expansion method is proposed\nsuch that some of the reliable bits are not expanded to reduce the computation\nand latency. At the algorithmic level, a double thresholding scheme is proposed\nas a fast approximate-sorting method for the list management operation to\nreduce the LSCD latency for large list size. A VLSI architecture of the LSCD\nimplementing the selective expansion and double thresholding scheme is then\ndeveloped, and implemented using a UMC 90 nm CMOS technology. Experimental\nresults show that, even for a large list size of 16, the proposed LSCD achieves\na decoding throughput of 460 Mbps at a clock frequency of 658 MHz.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:30:09 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Fan", "YouZhe", ""], ["Xia", "ChenYang", ""], ["Chen", "Ji", ""], ["Tsui", "Chi-Ying", ""], ["Jin", "Jie", ""], ["Shen", "Hui", ""], ["Li", "Bin", ""]]}, {"id": "1806.11547", "submitter": "Philip Colangelo", "authors": "Philip Colangelo, Nasibeh Nasiri, Asit Mishra, Eriko Nurvitadhi,\n  Martin Margala, Kevin Nealis", "title": "Exploration of Low Numeric Precision Deep Learning Inference Using Intel\n  FPGAs", "comments": "To Appear In The 26th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have been shown to maintain reasonable classification accuracy when\nquantized to lower precisions. Quantizing to sub 8-bit activations and weights\ncan result in accuracy falling below an acceptable threshold. Techniques exist\nfor closing the accuracy gap of limited numeric precision typically by\nincreasing computation. This results in a trade-off between throughput and\naccuracy and can be tailored for different networks through various\ncombinations of activation and weight data widths. Hardware architectures like\nFPGAs provide the opportunity for data width specific computation through\nunique logic configurations leading to highly optimized processing that is\nunattainable by full precision networks. Ternary and binary weighted networks\noffer an efficient method of inference for 2-bit and 1-bit data respectively.\nMost hardware architectures can take advantage of the memory storage and\nbandwidth savings that come along with smaller datapaths, but very few\narchitectures can take advantage of limited numeric precision at the\ncomputation level. In this paper, we present a hardware design for FPGAs that\ntakes advantage of bandwidth, memory, power, and computation savings of limited\nnumerical precision data. We provide insights into the trade-offs between\nthroughput and accuracy for various networks and how they map to our framework.\nFurther, we show how limited numeric precision computation can be efficiently\nmapped onto FPGAs for both ternary and binary cases. Starting with Arria 10, we\nshow a 2-bit activation and ternary weighted AlexNet running in hardware that\nachieves 3,700 images per second on the ImageNet dataset with a top-1 accuracy\nof 0.49. Using a hardware modeler designed for our low numeric precision\nframework we project performance most notably for a 55.5 TOPS Stratix 10 device\nrunning a modified ResNet-34 with only 3.7% accuracy degradation compared with\nsingle precision.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 22:00:31 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Colangelo", "Philip", ""], ["Nasiri", "Nasibeh", ""], ["Mishra", "Asit", ""], ["Nurvitadhi", "Eriko", ""], ["Margala", "Martin", ""], ["Nealis", "Kevin", ""]]}, {"id": "1806.11555", "submitter": "Marcelo Fernandes", "authors": "Matheus F. Torquato and Marcelo A. C. Fernandes", "title": "High-Performance Parallel Implementation of Genetic Algorithm on FPGA", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": "10.1007/s00034-019-01037-w", "report-no": null, "categories": "cs.DC cs.AI cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GAs) are used to solve search and optimization problems\nin which an optimal solution can be found using an iterative process with\nprobabilistic and non-deterministic transitions. However, depending on the\nproblem's nature, the time required to find a solution can be high in\nsequential machines due to the computational complexity of genetic algorithms.\nThis work proposes a parallel implementation of a genetic algorithm on\nfield-programmable gate array (FPGA). Optimization of the system's processing\ntime is the main goal of this project. Results associated with the processing\ntime and area occupancy (on FPGA) for various population sizes are analyzed.\nStudies concerning the accuracy of the GA response for the optimization of two\nvariables functions were also evaluated for the hardware implementation.\nHowever, the high-performance implementation proposes in this paper is able to\nwork with more variable from some adjustments on hardware architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:30:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Torquato", "Matheus F.", ""], ["Fernandes", "Marcelo A. C.", ""]]}]