[{"id": "1901.00092", "submitter": "Rabab Ezz-Eldin", "authors": "Hader E. El-hmaily, Rabab Ezz-Eldin, A. I. A. Galal and Hesham\n  F.A.Hamed", "title": "High Performance GNR Power Gating for Low-Voltage CMOS Circuits", "comments": "9 pages, 13 Figures and 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust power gating design using Graphene Nano-Ribbon Field Effect\nTransistors (GNRFET) is proposed using 16nm technology. The Power Gating (PG)\nstructure is composed of GNRFET as a power switch and MOS power gated module.\nThe proposed structure resolves the main drawbacks of the traditional PG design\nfrom the point of view increasing the propagation delay and wake-up time in low\nvoltage regions. GNRFET/MOSFET Conjunction (GMC) is employed to build various\nstructures of PG, GMCPG-SS and GMCPG-NS. In addition to exploiting it to build\ntwo multi-mode PG structures. Circuit analysis for CMOS power gated logic\nmodules ISCAS85 benchmark of 16nm technology is used to evaluate the\nperformance of the proposed GNR power switch is compared to the traditional MOS\none. Leakage power, wake-up time and power delay product are used as\nperformance circuit parameters for the evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 04:55:14 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["El-hmaily", "Hader E.", ""], ["Ezz-Eldin", "Rabab", ""], ["Galal", "A. I. A.", ""], ["Hamed", "Hesham F. A.", ""]]}, {"id": "1901.00121", "submitter": "Ahmad Shawahna", "authors": "Ahmad Shawahna, Sadiq M. Sait, and Aiman El-Maleh", "title": "FPGA-based Accelerators of Deep Learning Networks for Learning and\n  Classification: A Review", "comments": "This article has been accepted for publication in IEEE Access\n  (December, 2018)", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2890150", "report-no": null, "categories": "cs.NE cs.AR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances in digital technologies, and availability of credible\ndata, an area of artificial intelligence, deep learning, has emerged, and has\ndemonstrated its ability and effectiveness in solving complex learning problems\nnot possible before. In particular, convolution neural networks (CNNs) have\ndemonstrated their effectiveness in image detection and recognition\napplications. However, they require intensive CPU operations and memory\nbandwidth that make general CPUs fail to achieve desired performance levels.\nConsequently, hardware accelerators that use application specific integrated\ncircuits (ASICs), field programmable gate arrays (FPGAs), and graphic\nprocessing units (GPUs) have been employed to improve the throughput of CNNs.\nMore precisely, FPGAs have been recently adopted for accelerating the\nimplementation of deep learning networks due to their ability to maximize\nparallelism as well as due to their energy efficiency. In this paper, we review\nrecent existing techniques for accelerating deep learning networks on FPGAs. We\nhighlight the key features employed by the various techniques for improving the\nacceleration performance. In addition, we provide recommendations for enhancing\nthe utilization of FPGAs for CNNs acceleration. The techniques investigated in\nthis paper represent the recent trends in FPGA-based accelerators of deep\nlearning networks. Thus, this review is expected to direct the future advances\non efficient hardware accelerators and to be useful for deep learning\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 09:17:51 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Shawahna", "Ahmad", ""], ["Sait", "Sadiq M.", ""], ["El-Maleh", "Aiman", ""]]}, {"id": "1901.00370", "submitter": "Yaman Umuroglu", "authors": "Yaman Umuroglu, Davide Conficconi, Lahiru Rasnayake, Thomas B.\n  Preusser, Magnus Sjalander", "title": "Optimizing Bit-Serial Matrix Multiplication for Reconfigurable Computing", "comments": "Invited paper at ACM TRETS as extension of FPL'18 paper\n  arXiv:1806.08862", "journal-ref": null, "doi": "10.1145/3337929", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-matrix multiplication is a key computational kernel for numerous\napplications in science and engineering, with ample parallelism and data\nlocality that lends itself well to high-performance implementations. Many\nmatrix multiplication-dependent applications can use reduced-precision integer\nor fixed-point representations to increase their performance and energy\nefficiency while still offering adequate quality of results. However, precision\nrequirements may vary between different application phases or depend on input\ndata, rendering constant-precision solutions ineffective. BISMO, a vectorized\nbit-serial matrix multiplication overlay for reconfigurable computing,\npreviously utilized the excellent binary-operation performance of FPGAs to\noffer a matrix multiplication performance that scales with required precision\nand parallelism. We show how BISMO can be scaled up on Xilinx FPGAs using an\narithmetic architecture that better utilizes 6-LUTs. The improved BISMO\nachieves a peak performance of 15.4 binary TOPS on the Ultra96 board with a\nXilinx UltraScale+ MPSoC.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 14:05:48 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 11:12:17 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Umuroglu", "Yaman", ""], ["Conficconi", "Davide", ""], ["Rasnayake", "Lahiru", ""], ["Preusser", "Thomas B.", ""], ["Sjalander", "Magnus", ""]]}, {"id": "1901.00568", "submitter": "Mohammadkazem Taram", "authors": "Reza Mirosanlou, Mohammadkazem Taram, Zahra Shirmohammadi,\n  Seyed-Ghassem Miremadi", "title": "3DCAM: A Low Overhead Crosstalk Avoidance Mechanism for TSV-Based 3D ICs", "comments": "Initially Accepted to ICCD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three Dimensional Integrated Circuits (3D IC) offer lower power consumption,\nhigher performance, higher bandwidth, and scalability over the conventional two\ndimensional ICs. Through-Silicon Via (TSV) is one of the fabrication mechanisms\nthat connects stacked dies to each other. The large size of TSVs and the\nproximity between them lead to undesirable coupling capacitance. This\ninterference causes mutual influences between adjacent TSVs and produces\ncrosstalk noise. Furthermore, this effect threats the reliability of data\nduring traversal between layers. This paper proposes a mechanism that\nefficiently reduces crosstalk noise between TSVs with lower area overhead as\ncompared to previous works. This mechanism revolves around the fact that\nretaining TSV value in current state can reduce coupling in some cases. To\nevaluate the mechanism, gem5 simulator is used for data extraction and several\nbenchmarks are taken from the SPEC2006 suite. The simulation results show that\nthe proposed mechanism reduces crosstalk noise with only 30% imposed TSV\noverhead while delay decreased up to 25.7% as compared to a recent related\nwork.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 01:05:20 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Mirosanlou", "Reza", ""], ["Taram", "Mohammadkazem", ""], ["Shirmohammadi", "Zahra", ""], ["Miremadi", "Seyed-Ghassem", ""]]}, {"id": "1901.00620", "submitter": "Pengfei Zuo", "authors": "Pengfei Zuo, Yu Hua, Yuan Xie", "title": "A Secure and Persistent Memory System for Non-volatile Memory", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the non-volatile memory, ensuring the security and correctness of\npersistent data is fundamental. However, the security and persistence issues\nare usually studied independently in existing work. To achieve both data\nsecurity and persistence, simply combining existing persistence schemes with\nmemory encryption is inefficient due to crash inconsistency and significant\nperformance degradation. To bridge the gap between security and persistence,\nthis paper proposes SecPM, a Secure and Persistent Memory system, which\nconsists of a counter cache write-through (CWT) scheme and a locality-aware\ncounter write reduction (CWR) scheme. Specifically, SecPM leverages the CWT\nscheme to guarantee the crash consistency via ensuring both the data and its\ncounter are durable before the data flush completes, and leverages the CWR\nscheme to improve the system performance via exploiting the spatial locality of\ncounter storage, log and data writes. We have implemented SecPM in gem5 with\nNVMain and evaluated it using five widely-used workloads. Extensive\nexperimental results demonstrate that SecPM reduces up to half of write\nrequests and speeds up the transaction execution by 1.3-2.0 times via using the\nCWR scheme, and achieves the performance close to an un-encrypted persistent\nmemory system for large transactions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 06:05:56 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zuo", "Pengfei", ""], ["Hua", "Yu", ""], ["Xie", "Yuan", ""]]}, {"id": "1901.01007", "submitter": "Tong Geng", "authors": "Tong Geng, Tianqi Wang, Ang Li, Xi Jin, Martin Herbordt", "title": "FPDeep: Scalable Acceleration of CNN Training on Deeply-Pipelined FPGA\n  Clusters", "comments": "Accepted by IEEE TRANSACTIONS ON COMPUTERS (TC)", "journal-ref": null, "doi": "10.1109/TC.2020.3000118", "report-no": null, "categories": "cs.LG cs.AR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have revolutionized numerous applications, but\nthe demand for ever more performance remains unabated. Scaling DNN computations\nto larger clusters is generally done by distributing tasks in batch mode using\nmethods such as distributed synchronous SGD. Among the issues with this\napproach is that to make the distributed cluster work with high utilization,\nthe workload distributed to each node must be large, which implies nontrivial\ngrowth in the SGD mini-batch size.\n  In this paper, we propose a framework called FPDeep, which uses a hybrid of\nmodel and layer parallelism to configure distributed reconfigurable clusters to\ntrain DNNs. This approach has numerous benefits. First, the design does not\nsuffer from batch size growth. Second, novel workload and weight partitioning\nleads to balanced loads of both among nodes. And third, the entire system is a\nfine-grained pipeline. This leads to high parallelism and utilization and also\nminimizes the time features need to be cached while waiting for\nback-propagation. As a result, storage demand is reduced to the point where\nonly on-chip memory is used for the convolution layers. We evaluate FPDeep with\nthe Alexnet, VGG-16, and VGG-19 benchmarks. Experimental results show that\nFPDeep has good scalability to a large number of FPGAs, with the limiting\nfactor being the FPGA-to-FPGA bandwidth. With 6 transceivers per FPGA, FPDeep\nshows linearity up to 83 FPGAs. Energy efficiency is evaluated with respect to\nGOPs/J. FPDeep provides, on average, 6.36x higher energy efficiency than\ncomparable GPU servers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:54:46 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 19:03:13 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 22:33:11 GMT"}, {"version": "v4", "created": "Sun, 21 Jun 2020 04:17:46 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Geng", "Tong", ""], ["Wang", "Tianqi", ""], ["Li", "Ang", ""], ["Jin", "Xi", ""], ["Herbordt", "Martin", ""]]}, {"id": "1901.02415", "submitter": "Ramtin Zand", "authors": "Ramtin Zand and Ronald F. DeMara", "title": "SNRA: A Spintronic Neuromorphic Reconfigurable Array for In-Circuit\n  Training and Evaluation of Deep Belief Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a spintronic neuromorphic reconfigurable Array (SNRA) is\ndeveloped to fuse together power-efficient probabilistic and in-field\nprogrammable deterministic computing during both training and evaluation phases\nof restricted Boltzmann machines (RBMs). First, probabilistic spin logic\ndevices are used to develop an RBM realization which is adapted to construct\ndeep belief networks (DBNs) having one to three hidden layers of size 10 to 800\nneurons each. Second, we design a hardware implementation for the contrastive\ndivergence (CD) algorithm using a four-state finite state machine capable of\nunsupervised training in N+3 clocks where N denotes the number of neurons in\neach RBM. The functionality of our proposed CD hardware implementation is\nvalidated using ModelSim simulations. We synthesize the developed Verilog HDL\nimplementation of our proposed test/train control circuitry for various DBN\ntopologies where the maximal RBM dimensions yield resource utilization ranging\nfrom 51 to 2,421 lookup tables (LUTs). Next, we leverage spin Hall effect\n(SHE)-magnetic tunnel junction (MTJ) based non-volatile LUTs circuits as an\nalternative for static random access memory (SRAM)-based LUTs storing the\ndeterministic logic configuration to form a reconfigurable fabric. Finally, we\ncompare the performance of our proposed SNRA with SRAM-based configurable\nfabrics focusing on the area and power consumption induced by the LUTs used to\nimplement both CD and evaluation modes. The results obtained indicate more than\n80% reduction in combined dynamic and static power dissipation, while achieving\nat least 50% reduction in device count.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:23:42 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Zand", "Ramtin", ""], ["DeMara", "Ronald F.", ""]]}, {"id": "1901.04989", "submitter": "Marcelo Fernandes", "authors": "Carlos E. B. S. J\\'unior, Matheus F. Torquato and Marcelo A. C.\n  Fernandes", "title": "Application-Specific System Processor for the SHA-1 Hash Algorithm", "comments": "20 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an Application-Specific System Processor (ASSP) hardware\nfor the Secure Hash Algorithm 1 (SHA-1) algorithm. The proposed hardware was\nimplemented in a Field Programmable Gate Array (FPGA) Xilinx Virtex 6\nxc6vlx240t-1ff1156. The throughput and the occupied area were analyzed for\nseveral implementations in parallel instances of the hash algorithm. The\nresults showed that the hardware proposed for the SHA-1 achieved a throughput\nof 0.644 Gbps for a single instance and slightly more than 28 Gbps for 48\ninstances in a single FPGA. Various applications such as password recovery,\npassword validation, and high volume data integrity checking can be performed\nefficiently and quickly with an ASSP for SHA1.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 23:27:37 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["J\u00fanior", "Carlos E. B. S.", ""], ["Torquato", "Matheus F.", ""], ["Fernandes", "Marcelo A. C.", ""]]}, {"id": "1901.05959", "submitter": "Roman Kaplan", "authors": "Roman Kaplan, Leonid Yavits and Ran Ginosar", "title": "BioSEAL: In-Memory Biological Sequence Alignment Accelerator for\n  Large-Scale Genomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome sequences contain hundreds of millions of DNA base pairs. Finding the\ndegree of similarity between two genomes requires executing a compute-intensive\ndynamic programming algorithm, such as Smith-Waterman. Traditional von Neumann\narchitectures have limited parallelism and cannot provide an efficient solution\nfor large-scale genomic data. Approximate heuristic methods (e.g. BLAST) are\ncommonly used. However, they are suboptimal and still compute-intensive. In\nthis work, we present BioSEAL, a Biological SEquence ALignment accelerator.\nBioSEAL is a massively parallel non-von Neumann processing-in-memory\narchitecture for large-scale DNA and protein sequence alignment. BioSEAL is\nbased on resistive content addressable memory, capable of energy-efficient and\nhigh-performance associative processing. We present an associative processing\nalgorithm for entire database sequence alignment on BioSEAL and compare its\nperformance and power consumption with state-of-art solutions. We show that\nBioSEAL can achieve up to 57x speedup and 156x better energy efficiency,\ncompared with existing solutions for genome sequence alignment and protein\nsequence database search.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:13:22 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Kaplan", "Roman", ""], ["Yavits", "Leonid", ""], ["Ginosar", "Ran", ""]]}, {"id": "1901.09315", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, D L Maskell, N E Mastorakis", "title": "Asynchronous Early Output Block Carry Lookahead Adder with Improved\n  Quality of Results", "comments": null, "journal-ref": "Proceedings of 61st MWSCAS 2018, pp. 587-590, 2018", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new asynchronous early output block carry lookahead adder (BCLA)\nincorporating redundant carries is proposed. Compared to the best of existing\nsemi-custom asynchronous carry lookahead adders (CLAs) employing\ndelay-insensitive data encoding and following a 4-phase handshaking, the\nproposed BCLA with redundant carries achieves 13% reduction in forward latency\nand 14.8% reduction in cycle time compared to the best of the existing CLAs\nfeaturing redundant carries with no area or power penalty. A hybrid variant\ninvolving a ripple carry adder (RCA) in the least significant stages i.e.\nBCLA-RCA is also considered that achieves a further 4% reduction in the forward\nlatency and a 2.4% reduction in the cycle time compared to the proposed BCLA\nfeaturing redundant carries without area or power penalties.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 04:46:35 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Balasubramanian", "P", ""], ["Maskell", "D L", ""], ["Mastorakis", "N E", ""]]}, {"id": "1901.09316", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, D L Maskell, N E Mastorakis", "title": "Majority and Minority Voted Redundancy for Safety-Critical Applications", "comments": null, "journal-ref": "Proceedings of 61st MWSCAS 2018, pp. 1102-1105, 2018", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new majority and minority voted redundancy (MMR) scheme is proposed that\ncan provide the same degree of fault tolerance as N-modular redundancy (NMR)\nbut with fewer function units and a less sophisticated voting logic. Example\nNMR and MMR circuits were implemented using a 32/28nm CMOS process and\ncompared. The results show that MMR circuits dissipate less power, occupy less\narea, and encounter less critical path delay than the corresponding NMR\ncircuits while providing the same degree of fault tolerance. Hence the MMR is a\npromising alternative to the NMR to efficiently implement high levels of\nredundancy in safety-critical applications.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 04:50:02 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Balasubramanian", "P", ""], ["Maskell", "D L", ""], ["Mastorakis", "N E", ""]]}, {"id": "1901.09348", "submitter": "Di Gao", "authors": "Di Gao, Dayane Reis, Xiaobo Sharon Hu, Cheng Zhuo", "title": "Eva-CiM: A System-Level Performance and Energy Evaluation Framework for\n  Computing-in-Memory Architectures", "comments": "13 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TCAD.2020.2966484", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing-in-Memory (CiM) architectures aim to reduce costly data transfers\nby performing arithmetic and logic operations in memory and hence relieve the\npressure due to the memory wall. However, determining whether a given workload\ncan really benefit from CiM, which memory hierarchy and what device technology\nshould be adopted by a CiM architecture requires in-depth study that is not\nonly time consuming but also demands significant expertise in architectures and\ncompilers. This paper presents an energy evaluation framework, Eva-CiM, for\nsystems based on CiM architectures. Eva-CiM encompasses a multi-level (from\ndevice to architecture) comprehensive tool chain by leveraging existing\nmodeling and simulation tools such as GEM5, McPAT [2] and DESTINY [3]. To\nsupport high-confidence prediction, rapid design space exploration and ease of\nuse, Eva-CiM introduces several novel modeling/analysis approaches including\nmodels for capturing memory access and dependency-aware ISA traces, and for\nquantifying interactions between the host CPU and CiM modules. Eva-CiM can\nreadily produce energy estimates of the entire system for a given program, a\nprocessor architecture, and the CiM array and technology specifications.\nEva-CiM is validated by comparing with DESTINY [3] and [4], and enables\nfindings including practical contributions from CiM-supported accesses,\nCiM-sensitive benchmarking as well as the pros and cons of increased memory\nsize for CiM. Eva-CiM also enables exploration over different configurations\nand device technologies, showing 1.3-6.0X energy improvement for SRAM and\n2.0-7.9X for FeFET-RAM, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 10:43:54 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 11:49:17 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Gao", "Di", ""], ["Reis", "Dayane", ""], ["Hu", "Xiaobo Sharon", ""], ["Zhuo", "Cheng", ""]]}, {"id": "1901.10351", "submitter": "Aayush Ankit", "authors": "Aayush Ankit, Izzat El Hajj, Sai Rahul Chalamalasetti, Geoffrey Ndu,\n  Martin Foltin, R. Stanley Williams, Paolo Faraboschi, Wen-mei Hwu, John Paul\n  Strachan, Kaushik Roy, Dejan S Milojicic", "title": "PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for\n  Machine Learning Inference", "comments": "Accepted in ASPLOS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memristor crossbars are circuits capable of performing analog matrix-vector\nmultiplications, overcoming the fundamental energy efficiency limitations of\ndigital logic. They have been shown to be effective in special-purpose\naccelerators for a limited set of neural network applications.\n  We present the Programmable Ultra-efficient Memristor-based Accelerator\n(PUMA) which enhances memristor crossbars with general purpose execution units\nto enable the acceleration of a wide variety of Machine Learning (ML) inference\nworkloads. PUMA's microarchitecture techniques exposed through a specialized\nInstruction Set Architecture (ISA) retain the efficiency of in-memory computing\nand analog circuitry, without compromising programmability.\n  We also present the PUMA compiler which translates high-level code to PUMA\nISA. The compiler partitions the computational graph and optimizes instruction\nscheduling and register allocation to generate code for large and complex\nworkloads to run on thousands of spatial cores.\n  We have developed a detailed architecture simulator that incorporates the\nfunctionality, timing, and power models of PUMA's components to evaluate\nperformance and energy consumption. A PUMA accelerator running at 1 GHz can\nreach area and power efficiency of $577~GOPS/s/mm^2$ and $837~GOPS/s/W$,\nrespectively. Our evaluation of diverse ML applications from image recognition,\nmachine translation, and language modelling (5M-800M synapses) shows that PUMA\nachieves up to $2,446\\times$ energy and $66\\times$ latency improvement for\ninference compared to state-of-the-art GPUs. Compared to an\napplication-specific memristor-based accelerator, PUMA incurs small energy\noverheads at similar inference latency and added programmability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:59:54 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 01:42:50 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ankit", "Aayush", ""], ["Hajj", "Izzat El", ""], ["Chalamalasetti", "Sai Rahul", ""], ["Ndu", "Geoffrey", ""], ["Foltin", "Martin", ""], ["Williams", "R. Stanley", ""], ["Faraboschi", "Paolo", ""], ["Hwu", "Wen-mei", ""], ["Strachan", "John Paul", ""], ["Roy", "Kaushik", ""], ["Milojicic", "Dejan S", ""]]}, {"id": "1901.11129", "submitter": "Matthew Walker", "authors": "Matthew J. P. Walker and Jason H. Anderson", "title": "Generic Connectivity-Based CGRA Mapping via Integer Linear Programming", "comments": "8 pages of content; 8 figures; 3 tables; to appear in FCCM 2019; Uses\n  the CGRA-ME framework at http://cgra-me.ece.utoronto.ca/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse-grained reconfigurable architectures (CGRAs) are programmable logic\ndevices with large coarse-grained ALU-like logic blocks, and multi-bit\ndatapath-style routing. CGRAs often have relatively restricted data routing\nnetworks, so they attract CAD mapping tools that use exact methods, such as\nInteger Linear Programming (ILP). However, tools that target general\narchitectures must use large constraint systems to fully describe an\narchitecture's flexibility, resulting in lengthy run-times. In this paper, we\npropose to derive connectivity information from an otherwise generic device\nmodel, and use this to create simpler ILPs, which we combine in an iterative\nschedule and retain most of the exactness of a fully-generic ILP approach. This\nnew approach has a speed-up geometric mean of 5.88x when considering benchmarks\nthat do not hit a time-limit of 7.5 hours on the fully-generic ILP, and 37.6x\notherwise. This was measured using the set of benchmarks used to originally\nevaluate the fully-generic approach and several more benchmarks representing\ncomputation tasks, over three different CGRA architectures. All run-times of\nthe new approach are less than 20 minutes, with 90th percentile time of 410\nseconds. The proposed mapping techniques are integrated into, and evaluated\nusing the open-source CGRA-ME architecture modelling and exploration framework.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 22:35:56 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 21:31:29 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Walker", "Matthew J. P.", ""], ["Anderson", "Jason H.", ""]]}]