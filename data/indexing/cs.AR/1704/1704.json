[{"id": "1704.02677", "submitter": "Xiangyao Yu", "authors": "Xiangyao Yu, Christopher J. Hughes, Nadathur Satish, Onur Mutlu,\n  Srinivas Devadas", "title": "Banshee: Bandwidth-Efficient DRAM Caching Via Software/Hardware\n  Cooperation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Putting the DRAM on the same package with a processor enables several times\nhigher memory bandwidth than conventional off-package DRAM. Yet, the latency of\nin-package DRAM is not appreciably lower than that of off-package DRAM. A\npromising use of in-package DRAM is as a large cache. Unfortunately, most\nprevious DRAM cache designs mainly optimize for hit latency and do not consider\noff-chip bandwidth efficiency as a first-class design constraint. Hence, as we\nshow in this paper, these designs are suboptimal for use with in-package DRAM.\n  We propose a new DRAM cache design, Banshee, that optimizes for both in- and\noff-package DRAM bandwidth efficiency without degrading access latency. The key\nideas are to eliminate the in-package DRAM bandwidth overheads due to costly\ntag accesses through virtual memory mechanism and to incorporate a\nbandwidth-aware frequency-based replacement policy that is biased to reduce\nunnecessary traffic to off-package DRAM. Our extensive evaluation shows that\nBanshee provides significant performance improvement and traffic reduction over\nstate-of-the-art latency-optimized DRAM cache designs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 00:44:42 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Yu", "Xiangyao", ""], ["Hughes", "Christopher J.", ""], ["Satish", "Nadathur", ""], ["Mutlu", "Onur", ""], ["Devadas", "Srinivas", ""]]}, {"id": "1704.03168", "submitter": "Yeong-Jae Woo", "authors": "Yeong-Jae Woo, Sang Lyul Min", "title": "FMMU: A Hardware-Automated Flash Map Management Unit for Scalable\n  Performance of NAND Flash-Based SSDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NAND flash-based Solid State Drives (SSDs), which are widely used from\nembedded systems to enterprise servers, are enhancing performance by exploiting\nthe parallelism of NAND flash memories. To cope with the performance\nimprovement of SSDs, storage systems have rapidly adopted the host interface\nfor SSDs from Serial-ATA, which is used for existing hard disk drives, to\nhigh-speed PCI express. Since NAND flash memory does not allow in-place\nupdates, it requires special software called Flash Translation Layer (FTL), and\nSSDs are equipped with embedded processors to run FTL. Existing SSDs increase\nthe clock frequency of embedded processors or increase the number of embedded\nprocessors in order to prevent FTL from acting as bottleneck of SSD\nperformance, but these approaches are not scalable. This paper proposes a\nhardware-automated Flash Map Management Unit, called FMMU, that handles the\naddress translation process dominating the execution time of the FTL by\nhardware automation. FMMU provides methods for exploiting the parallelism of\nflash memory by processing outstanding requests in a non-blocking manner while\nreducing the number of flash operations. The experimental results show that the\nFMMU reduces the FTL execution time in the map cache hit case and the miss case\nby 44% and 37%, respectively, compared with the existing software-based\napproach operating in 4-core. FMMU also prevents FTL from acting as a\nperformance bottleneck for up to 32-channel, 8-way SSD using PCIe 3.0 x32 host\ninterface.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 06:46:51 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Woo", "Yeong-Jae", ""], ["Min", "Sang Lyul", ""]]}, {"id": "1704.03991", "submitter": "Prashant Jayaprakash Nair", "authors": "Prashant J. Nair", "title": "Architectural Techniques to Enable Reliable and Scalable Memory Systems", "comments": "PhD thesis, Georgia Institute of Technology (May 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High capacity and scalable memory systems play a vital role in enabling our\ndesktops, smartphones, and pervasive technologies like Internet of Things\n(IoT). Unfortunately, memory systems are becoming increasingly prone to faults.\nThis is because we rely on technology scaling to improve memory density, and at\nsmall feature sizes, memory cells tend to break easily. Today, memory\nreliability is seen as the key impediment towards using high-density devices,\nadopting new technologies, and even building the next Exascale supercomputer.\nTo ensure even a bare-minimum level of reliability, present-day solutions tend\nto have high performance, power and area overheads. Ideally, we would like\nmemory systems to remain robust, scalable, and implementable while keeping the\noverheads to a minimum. This dissertation describes how simple cross-layer\narchitectural techniques can provide orders of magnitude higher reliability and\nenable seamless scalability for memory systems while incurring negligible\noverheads.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 04:38:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Nair", "Prashant J.", ""]]}, {"id": "1704.04760", "submitter": "David Patterson David Patterson", "authors": "Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav\n  Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers,\n  Rick Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell,\n  Mike Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami,\n  Rajendra Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug\n  Hogberg, John Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek\n  Jaworski, Alexander Kaplan, Harshit Khaitan, Andy Koch, Naveen Kumar, Steve\n  Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu, Kyle\n  Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony, Kieran\n  Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas\n  Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt\n  Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew\n  Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory\n  Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter,\n  Walter Wang, Eric Wilcox, and Doe Hyun Yoon", "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "comments": "17 pages, 11 figures, 8 tables. To appear at the 44th International\n  Symposium on Computer Architecture (ISCA), Toronto, Canada, June 24-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many architects believe that major improvements in cost-energy-performance\nmust now come from domain-specific hardware. This paper evaluates a custom\nASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since\n2015 that accelerates the inference phase of neural networks (NN). The heart of\nthe TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak\nthroughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed\non-chip memory. The TPU's deterministic execution model is a better match to\nthe 99th-percentile response-time requirement of our NN applications than are\nthe time-varying optimizations of CPUs and GPUs (caches, out-of-order\nexecution, multithreading, multiprocessing, prefetching, ...) that help average\nthroughput more than guaranteed latency. The lack of such features helps\nexplain why, despite having myriad MACs and a big memory, the TPU is relatively\nsmall and low power. We compare the TPU to a server-class Intel Haswell CPU and\nan Nvidia K80 GPU, which are contemporaries deployed in the same datacenters.\nOur workload, written in the high-level TensorFlow framework, uses production\nNN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters'\nNN inference demand. Despite low utilization for some applications, the TPU is\non average about 15X - 30X faster than its contemporary GPU or CPU, with\nTOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the\nTPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and\n200X the CPU.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 12:07:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Jouppi", "Norman P.", ""], ["Young", "Cliff", ""], ["Patil", "Nishant", ""], ["Patterson", "David", ""], ["Agrawal", "Gaurav", ""], ["Bajwa", "Raminder", ""], ["Bates", "Sarah", ""], ["Bhatia", "Suresh", ""], ["Boden", "Nan", ""], ["Borchers", "Al", ""], ["Boyle", "Rick", ""], ["Cantin", "Pierre-luc", ""], ["Chao", "Clifford", ""], ["Clark", "Chris", ""], ["Coriell", "Jeremy", ""], ["Daley", "Mike", ""], ["Dau", "Matt", ""], ["Dean", "Jeffrey", ""], ["Gelb", "Ben", ""], ["Ghaemmaghami", "Tara Vazir", ""], ["Gottipati", "Rajendra", ""], ["Gulland", "William", ""], ["Hagmann", "Robert", ""], ["Ho", "C. Richard", ""], ["Hogberg", "Doug", ""], ["Hu", "John", ""], ["Hundt", "Robert", ""], ["Hurt", "Dan", ""], ["Ibarz", "Julian", ""], ["Jaffey", "Aaron", ""], ["Jaworski", "Alek", ""], ["Kaplan", "Alexander", ""], ["Khaitan", "Harshit", ""], ["Koch", "Andy", ""], ["Kumar", "Naveen", ""], ["Lacy", "Steve", ""], ["Laudon", "James", ""], ["Law", "James", ""], ["Le", "Diemthu", ""], ["Leary", "Chris", ""], ["Liu", "Zhuyuan", ""], ["Lucke", "Kyle", ""], ["Lundin", "Alan", ""], ["MacKean", "Gordon", ""], ["Maggiore", "Adriana", ""], ["Mahony", "Maire", ""], ["Miller", "Kieran", ""], ["Nagarajan", "Rahul", ""], ["Narayanaswami", "Ravi", ""], ["Ni", "Ray", ""], ["Nix", "Kathy", ""], ["Norrie", "Thomas", ""], ["Omernick", "Mark", ""], ["Penukonda", "Narayana", ""], ["Phelps", "Andy", ""], ["Ross", "Jonathan", ""], ["Ross", "Matt", ""], ["Salek", "Amir", ""], ["Samadiani", "Emad", ""], ["Severn", "Chris", ""], ["Sizikov", "Gregory", ""], ["Snelham", "Matthew", ""], ["Souter", "Jed", ""], ["Steinberg", "Dan", ""], ["Swing", "Andy", ""], ["Tan", "Mercedes", ""], ["Thorson", "Gregory", ""], ["Tian", "Bo", ""], ["Toma", "Horia", ""], ["Tuttle", "Erick", ""], ["Vasudevan", "Vijay", ""], ["Walter", "Richard", ""], ["Wang", "Walter", ""], ["Wilcox", "Eric", ""], ["Yoon", "Doe Hyun", ""]]}, {"id": "1704.05044", "submitter": "Amin Jadidi", "authors": "Amin Jadidi, Mohammad Arjomand, Mahmut T. Kandemir, Chita R. Das", "title": "A Study on Performance and Power Efficiency of Dense Non-Volatile Caches\n  in Multi-Core Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3078505.3078547", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel cache design based on Multi-Level Cell\nSpin-Transfer Torque RAM (MLC STTRAM) that can dynamically adapt the set\ncapacity and associativity to use efficiently the full potential of MLC STTRAM.\nWe exploit the asymmetric nature of the MLC storage scheme to build cache lines\nfeaturing heterogeneous performances, that is, half of the cache lines are\nread-friendly, while the other is write-friendly. Furthermore, we propose to\nopportunistically deactivate ways in underutilized sets to convert MLC to\nSingle-Level Cell (SLC) mode, which features overall better performance and\nlifetime. Our ultimate goal is to build a cache architecture that combines the\ncapacity advantages of MLC and performance/energy advantages of SLC. Our\nexperiments show an improvement of 43% in total numbers of conflict misses, 27%\nin memory access latency, 12% in system performance, and 26% in LLC access\nenergy, with a slight degradation in cache lifetime (about 7%) compared to an\nSLC cache.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:41:17 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Jadidi", "Amin", ""], ["Arjomand", "Mohammad", ""], ["Kandemir", "Mahmut T.", ""], ["Das", "Chita R.", ""]]}, {"id": "1704.05138", "submitter": "Wonil Choi", "authors": "Wonil Choi and Mohammad Arjomand and Myoungsoo Jung and Mahmut\n  Kandemir", "title": "Exploiting Data Longevity for Enhancing the Lifetime of Flash-based\n  Storage Class Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage-class memory (SCM) combines the benefits of a solid-state memory,\nsuch as high-performance and robustness, with the archival capabilities and low\ncost of conventional hard-disk magnetic storage. Among candidate solid-state\nnonvolatile memory technologies that could potentially be used to construct\nSCM, flash memory is a well-established technology and have been widely used in\ncommercially available SCM incarnations. Flash-based SCM enables much better\ntradeoffs between performance, space and power than disk-based systems.\nHowever, write endurance is a significant challenge for a flash-based SCM (each\nact of writing a bit may slightly damage a cell, so one flash cell can be\nwritten 10^4--10^5 times, depending on the flash technology, before it becomes\nunusable). This is a well-documented problem and has received a lot of\nattention by manufactures that are using some combination of write reduction\nand wear-leveling techniques for achieving longer lifetime. In an effort to\nimprove flash lifetime, first, by quantifying data longevity in an SCM, we show\nthat a majority of the data stored in a solid-state SCM do not require long\nretention times provided by flash memory (i.e., up to 10 years in modern\ndevices); second, by exploiting retention time relaxation, we propose a novel\nmechanism, called Dense-SLC (D-SLC), which enables us perform multiple writes\ninto a cell during each erase cycle for lifetime extension; and finally, we\ndiscuss the required changes in the flash management software (FTL) in order to\nuse this characteristic for extending the lifetime of the solid-state part of\nan SCM. Using an extensive simulation-based analysis of a flash-based SCM, we\ndemonstrate that D-SLC is able to significantly improve device lifetime\n(between 5.1X and 8.6X) with no performance overhead and also very small\nchanges at the FTL software.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 22:01:00 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Choi", "Wonil", ""], ["Arjomand", "Mohammad", ""], ["Jung", "Myoungsoo", ""], ["Kandemir", "Mahmut", ""]]}, {"id": "1704.07619", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, K Prasad", "title": "Asynchronous Early Output Dual-Bit Full Adders Based on Homogeneous and\n  Heterogeneous Delay-Insensitive Data Encoding", "comments": null, "journal-ref": "WSEAS Transactions on Circuits and Systems, vol. 16, Article #8,\n  pp. 64-73, 2017", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the designs of asynchronous early output dual-bit full\nadders without and with redundant logic (implicit) corresponding to homogeneous\nand heterogeneous delay-insensitive data encoding. For homogeneous\ndelay-insensitive data encoding only dual-rail i.e. 1-of-2 code is used, and\nfor heterogeneous delay-insensitive data encoding 1-of-2 and 1-of-4 codes are\nused. The 4-phase return-to-zero protocol is used for handshaking. To\ndemonstrate the merits of the proposed dual-bit full adder designs, 32-bit\nripple carry adders (RCAs) are constructed comprising dual-bit full adders. The\nproposed dual-bit full adders based 32-bit RCAs incorporating redundant logic\nfeature reduced latency and area compared to their non-redundant counterparts\nwith no accompanying power penalty. In comparison with the weakly indicating\n32-bit RCA constructed using homogeneously encoded dual-bit full adders\ncontaining redundant logic, the early output 32-bit RCA comprising the proposed\nhomogeneously encoded dual-bit full adders with redundant logic reports\ncorresponding reductions in latency and area by 22.2% and 15.1% with no\nassociated power penalty. On the other hand, the early output 32-bit RCA\nconstructed using the proposed heterogeneously encoded dual-bit full adder\nwhich incorporates redundant logic reports respective decreases in latency and\narea than the weakly indicating 32-bit RCA that consists of heterogeneously\nencoded dual-bit full adders with redundant logic by 21.5% and 21.3% with nil\npower overhead. The simulation results obtained are based on a 32/28nm CMOS\nprocess technology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:30:20 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Balasubramanian", "P", ""], ["Prasad", "K", ""]]}, {"id": "1704.08526", "submitter": "Kiran Gupta A", "authors": "Naveen S Naik, Kiran A Gupta", "title": "An Efficient Reconfigurable FIR Digital Filter Using Modified Distribute\n  Arithmetic Technique", "comments": "5 pages,4 figures, journal 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides modified Distributed Arithmetic based technique to\ncompute sum of products saving appreciable number of Multiply And accumulation\nblocks and this consecutively reduces circuit size. In this technique\nmultiplexer based structure is used to reuse the blocks so as to reduce the\nrequired memory locations. In this technique a Carry Look Ahead based adder\ntree is used to have better area-delay product. Designing of FIR filter is done\nusing VHDL and synthesized using Xilinx 12.2 synthesis tool and ISIM simulator.\nThe power analysis is done using Xilinx Xpower analyzer. The proposed structure\nrequires nearly 42% less cells, 40% less LUT flip-flop pairs used, and also 2%\nless power compared with existing structure.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 12:07:52 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Naik", "Naveen S", ""], ["Gupta", "Kiran A", ""]]}, {"id": "1704.08802", "submitter": "Hayden So", "authors": "Hayden Kwok-Hay So and John Wawrzynek", "title": "Proceedings of the 3rd International Workshop on Overlay Architectures\n  for FPGAs (OLAF 2017)", "comments": "3rd International Workshop on Overlay Architectures for FPGAs (OLAF\n  2017) website: see http://olaf.eecs.berkeley.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3rd International Workshop on Overlay Architectures for FPGAs (OLAF 2017)\nwas held on 22 Feb, 2017 as a co-located workshop at the 25th ACM/SIGDA\nInternational Symposium on Field-Programmable Gate Arrays (FPGA 2017). This\nyear, the program committee selected 3 papers and 3 extended abstracts to be\npresented at the workshop, which are subsequently collected in this online\nvolume.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 03:49:08 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 15:06:01 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["So", "Hayden Kwok-Hay", ""], ["Wawrzynek", "John", ""]]}]