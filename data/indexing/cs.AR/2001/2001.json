[{"id": "2001.00856", "submitter": "Karthikeyan Nagarajan", "authors": "Karthikeyan Nagarajan, Asmit De, Mohammad Nasim Imtiaz Khan, Swaroop\n  Ghosh", "title": "TrappeD: DRAM Trojan Designs for Information Leakage and Fault Injection\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the advanced circuit features such as wordline-\n(WL) underdrive (prevents retention failure) and overdrive (assists write)\nemployed in the peripherals of Dynamic RAM (DRAM) memories from a security\nperspective. In an ideal environment, these features ensure fast and reliable\nread and write operations. However, an adversary can re-purpose them by\ninserting Trojans to deliver malicious payloads such as fault injections,\nDenial-of-Service (DoS), and information leakage attacks when activated by the\nadversary. Simulation results indicate that wordline voltage can be increased\nto cause retention failure and thereby launch a DoS attack in DRAM memory.\nFurthermore, two wordlines or bitlines can be shorted to leak information or\ninject faults by exploiting the DRAM's refresh operation. We demonstrate an\ninformation leakage system exploit by implementing TrappeD on RocketChip SoC.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 15:02:11 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Nagarajan", "Karthikeyan", ""], ["De", "Asmit", ""], ["Khan", "Mohammad Nasim Imtiaz", ""], ["Ghosh", "Swaroop", ""]]}, {"id": "2001.01501", "submitter": "Mantas Mikaitis", "authors": "Mantas Mikaitis", "title": "Stochastic Rounding: Algorithms and Hardware Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms and a hardware accelerator for performing stochastic rounding (SR)\nare presented. The main goal is to augment the ARM M4F based multi-core\nprocessor SpiNNaker2 with a more flexible rounding functionality than is\navailable in the ARM processor itself. The motivation of adding such an\naccelerator in hardware is based on our previous results showing improvements\nin numerical accuracy of ODE solvers in fixed-point arithmetic with SR,\ncompared to standard round to nearest or bit truncation rounding modes.\nFurthermore, performing SR purely in software can be expensive, due to\nrequirement of a pseudorandom number generator (PRNG), multiple masking and\nshifting instructions, and an addition operation. Also, saturation of the\nrounded values is included, since rounding is usually followed by saturation,\nwhich is especially important in fixed-point arithmetic due to a narrow dynamic\nrange of representable values. The main intended use of the accelerator is to\nround fixed-point multiplier outputs, which are returned unrounded by the ARM\nprocessor in a wider fixed-point format than the arguments.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 11:40:06 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 11:42:39 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 10:13:59 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 18:06:23 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mikaitis", "Mantas", ""]]}, {"id": "2001.02077", "submitter": "Ahmad Salehi", "authors": "Sayed Ahmad Salehi", "title": "Low-cost Stochastic Number Generators for Stochastic Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic unary computing provides low-area circuits. However, the required\narea consuming stochastic number generators (SNGs) in these circuits can\ndiminish their overall gain in area, particularly if several SNGs are required.\nWe propose area-efficient SNGs by sharing the permuted output of one linear\nfeedback shift register (LFSR) among several SNGs. With no hardware overhead,\nthe proposed architecture generates stochastic bit streams with minimum\nstochastic computing correlation (SCC). Compared to the circular shifting\napproach presented in prior work, our approach produces stochastic bit streams\nwith 67% less average SCC when a 10-bit LFSR is shared between two SNGs. To\ngeneralize our approach, we propose an algorithm to find a set of m\npermutations (n>m>2) with minimum pairwise SCC, for an n-bit LFSR. The search\nspace for finding permutations with exact minimum SCC grows rapidly when n\nincreases and it is intractable to perform a search algorithm using accurately\ncalculated pairwise SCC values, for n>9. We propose a similarity function that\ncan be used in the proposed search algorithm to quickly find a set of\npermutations with SCC values close to the minimum one. We evaluated our\napproach for several applications. The results show that, compared to prior\nwork, it achieves lower MSE with the same (or even lower) area. Additionally,\nbased on simulation results, we show that replacing the comparator component of\nan SNG circuit with a weighted binary generator can reduce SCC.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 20:25:05 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Salehi", "Sayed Ahmad", ""]]}, {"id": "2001.03803", "submitter": "Yongjune Kim", "authors": "Yongjune Kim, Yoocharn Jeon, Cyril Guyot, Yuval Cassuto", "title": "Optimizing the Write Fidelity of MRAMs", "comments": null, "journal-ref": null, "doi": "10.1109/ISIT44484.2020.9173990", "report-no": null, "categories": "cs.IT cs.AR cs.ET math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic random-access memory (MRAM) is a promising memory technology due to\nits high density, non-volatility, and high endurance. However, achieving high\nmemory fidelity incurs significant write-energy costs, which should be reduced\nfor large-scale deployment of MRAMs. In this paper, we formulate an\noptimization problem for maximizing the memory fidelity given energy\nconstraints, and propose a biconvex optimization approach to solve it. The\nbasic idea is to allocate non-uniform write pulses depending on the importance\nof each bit position. The fidelity measure we consider is minimum mean squared\nerror (MSE), for which we propose an iterative water-filling algorithm.\nAlthough the iterative algorithm does not guarantee global optimality, we can\nchoose a proper starting point that decreases the MSE exponentially and\nguarantees fast convergence. For an 8-bit accessed word, the proposed algorithm\nreduces the MSE by a factor of 21.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 22:32:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kim", "Yongjune", ""], ["Jeon", "Yoocharn", ""], ["Guyot", "Cyril", ""], ["Cassuto", "Yuval", ""]]}, {"id": "2001.04039", "submitter": "Huimei Cheng", "authors": "Sai Aparna Aketi and Smriti Gupta and Huimei Cheng and Joycee Mekie\n  and Peter A. Beerel", "title": "SERAD: Soft Error Resilient Asynchronous Design using a Bundled Data\n  Protocol", "comments": null, "journal-ref": null, "doi": "10.1109/TCSI.2020.2965073", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of soft errors due to radiation continues to be a significant\nchallenge for engineers trying to build systems that can handle harsh\nenvironments. Building systems that are Radiation Hardened by Design (RHBD) is\nthe preferred approach, but existing techniques are expensive in terms of\nperformance, power, and/or area. This paper introduces a novel soft-error\nresilient asynchronous bundled-data design template, SERAD, which uses a\ncombination of temporal and spatial redundancy to mitigate Single Event\nTransients (SETs) and upsets (SEUs). SERAD uses Error Detecting Logic (EDL) to\ndetect SETs at the inputs of sequential elements and correct them via\nre-sampling. Because SERAD only pays the delay penalty in the presence of an\nSET, which rarely occurs, its average performance is comparable to the baseline\nsynchronous design. We tested the SERAD design using a combination of Spice and\nVerilog simulations and evaluated its impact on area, frequency, and power on\nan open-core MIPS-like processor using a NCSU 45nm cell library. Our\npost-synthesis results show that the SERAD design consumes less than half of\nthe area of the Triple Modular Redundancy (TMR), exhibits significantly less\nperformance degradation than Glitch Filtering (GF), and consumes no more total\npower than the baseline unhardened design.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 02:47:39 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Aketi", "Sai Aparna", ""], ["Gupta", "Smriti", ""], ["Cheng", "Huimei", ""], ["Mekie", "Joycee", ""], ["Beerel", "Peter A.", ""]]}, {"id": "2001.04504", "submitter": "Paul Whatmough", "authors": "Paul Whatmough, Marco Donato, Glenn Ko, Sae-Kyu Lee, David Brooks,\n  Gu-Yeon Wei", "title": "CHIPKIT: An agile, reusable open-source framework for rapid test chip\n  development", "comments": null, "journal-ref": null, "doi": "10.1109/MM.2020.2995809", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend for domain-specific architectures (DSAs) has led to renewed\ninterest in research test chips to demonstrate new specialized hardware.\nTape-outs also offer huge pedagogical value garnered from real hands-on\nexposure to the whole system stack. However, successful tape-outs demand\nhard-earned experience, and the design process is time consuming and fraught\nwith challenges. Therefore, custom chips have remained the preserve of a small\nnumber of research groups, typically focused on circuit design research. This\npaper describes the CHIPKIT framework. We describe a reusable SoC subsystem\nwhich provides basic IO, an on-chip programmable host, memory and peripherals.\nThis subsystem can be readily extended with new IP blocks to generate custom\ntest chips. We also present an agile RTL development flow, including a code\ngeneration tool calledVGEN. Finally, we outline best practices for full-chip\nvalidation across the entire design cycle.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 19:19:50 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 15:11:59 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Whatmough", "Paul", ""], ["Donato", "Marco", ""], ["Ko", "Glenn", ""], ["Lee", "Sae-Kyu", ""], ["Brooks", "David", ""], ["Wei", "Gu-Yeon", ""]]}, {"id": "2001.04543", "submitter": "Andreas Toftegaard Kristensen", "authors": "Yann Kurzo, Andreas Toftegaard Kristensen, Andreas Burg, and Alexios\n  Balatsoukas-Stimming", "title": "Hardware Implementation of Neural Self-Interference Cancellation", "comments": "Accepted for publication in IEEE Journal on Emerging and Selected\n  Topics in Circuits and Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-band full-duplex systems can transmit and receive information\nsimultaneously on the same frequency band. However, due to the strong\nself-interference caused by the transmitter to its own receiver, the use of\nnon-linear digital self-interference cancellation is essential. In this work,\nwe describe a hardware architecture for a neural network-based non-linear\nself-interference (SI) canceller and we compare it with our own hardware\nimplementation of a conventional polynomial based SI canceller. In particular,\nwe present implementation results for a shallow and a deep neural network SI\ncanceller as well as for a polynomial SI canceller. Our results show that the\ndeep neural network canceller achieves a hardware efficiency of up to $312.8$\nMsamples/s/mm$^2$ and an energy efficiency of up to $0.9$ nJ/sample, which is\n$2.1\\times$ and $2\\times$ better than the polynomial SI canceller,\nrespectively. These results show that NN-based methods applied to\ncommunications are not only useful from a performance perspective, but can also\nbe a very effective means to reduce the implementation complexity.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 21:51:20 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:31:36 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kurzo", "Yann", ""], ["Kristensen", "Andreas Toftegaard", ""], ["Burg", "Andreas", ""], ["Balatsoukas-Stimming", "Alexios", ""]]}, {"id": "2001.04937", "submitter": "Jesus Rodriguez Sanchez", "authors": "Jesus Rodriguez Sanchez, Ove Edfors, Fredrik Rusek, Liang Liu", "title": "Processing Distribution and Architecture Tradeoff for Large Intelligent\n  Surface Implementation", "comments": "Presented at IEEE ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Large Intelligent Surface (LIS) concept has emerged recently as a new\nparadigm for wireless communication, remote sensing and positioning. It\nconsists of a continuous radiating surface placed relatively close to the\nusers, which is able to communicate with users by independent transmission and\nreception (replacing base stations). Despite of its potential, there are a lot\nof challenges from an implementation point of view, with the interconnection\ndata-rate and computational complexity being the most relevant. Distributed\nprocessing techniques and hierarchical architectures are expected to play a\nvital role addressing this while ensuring scalability. In this paper we perform\nalgorithm-architecture codesign and analyze the hardware requirements and\narchitecture trade-offs for a discrete LIS to perform uplink detection. By\ndoing this, we expect to give concrete case studies and guidelines for\nefficient implementation of LIS systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:54:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 09:38:42 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sanchez", "Jesus Rodriguez", ""], ["Edfors", "Ove", ""], ["Rusek", "Fredrik", ""], ["Liu", "Liang", ""]]}, {"id": "2001.04974", "submitter": "Chuteng Zhou", "authors": "Chuteng Zhou, Prad Kadambi, Matthew Mattina, Paul N. Whatmough", "title": "Noisy Machines: Understanding Noisy Neural Networks and Enhancing\n  Robustness to Analog Hardware Errors Using Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning has brought forth a wave of interest in computer\nhardware design to better meet the high demands of neural network inference. In\nparticular, analog computing hardware has been heavily motivated specifically\nfor accelerating neural networks, based on either electronic, optical or\nphotonic devices, which may well achieve lower power consumption than\nconventional digital electronics. However, these proposed analog accelerators\nsuffer from the intrinsic noise generated by their physical components, which\nmakes it challenging to achieve high accuracy on deep neural networks. Hence,\nfor successful deployment on analog accelerators, it is essential to be able to\ntrain deep neural networks to be robust to random continuous noise in the\nnetwork weights, which is a somewhat new challenge in machine learning. In this\npaper, we advance the understanding of noisy neural networks. We outline how a\nnoisy neural network has reduced learning capacity as a result of loss of\nmutual information between its input and output. To combat this, we propose\nusing knowledge distillation combined with noise injection during training to\nachieve more noise robust networks, which is demonstrated experimentally across\ndifferent networks and datasets, including ImageNet. Our method achieves models\nwith as much as two times greater noise tolerance compared with the previous\nbest attempts, which is a significant step towards making analog hardware\npractical for deep learning.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:59:48 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Zhou", "Chuteng", ""], ["Kadambi", "Prad", ""], ["Mattina", "Matthew", ""], ["Whatmough", "Paul N.", ""]]}, {"id": "2001.06598", "submitter": "Nicolas Delfosse", "authors": "Poulami Das, Christopher A. Pattison, Srilatha Manne, Douglas Carmean,\n  Krysta Svore, Moinuddin Qureshi, Nicolas Delfosse", "title": "A Scalable Decoder Micro-architecture for Fault-Tolerant Quantum\n  Computing", "comments": "19 pages, 159 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation promises significant computational advantages over\nclassical computation for some problems. However, quantum hardware suffers from\nmuch higher error rates than in classical hardware. As a result, extensive\nquantum error correction is required to execute a useful quantum algorithm. The\ndecoder is a key component of the error correction scheme whose role is to\nidentify errors faster than they accumulate in the quantum computer and that\nmust be implemented with minimum hardware resources in order to scale to the\nregime of practical applications. In this work, we consider surface code error\ncorrection, which is the most popular family of error correcting codes for\nquantum computing, and we design a decoder micro-architecture for the\nUnion-Find decoding algorithm. We propose a three-stage fully pipelined\nhardware implementation of the decoder that significantly speeds up the\ndecoder. Then, we optimize the amount of decoding hardware required to perform\nerror correction simultaneously over all the logical qubits of the quantum\ncomputer. By sharing resources between logical qubits, we obtain a 67%\nreduction of the number of hardware units and the memory capacity is reduced by\n70%. Moreover, we reduce the bandwidth required for the decoding process by a\nfactor at least 30x using low-overhead compression algorithms. Finally, we\nprovide numerical evidence that our optimized micro-architecture can be\nexecuted fast enough to correct errors in a quantum computer.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 04:44:52 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Das", "Poulami", ""], ["Pattison", "Christopher A.", ""], ["Manne", "Srilatha", ""], ["Carmean", "Douglas", ""], ["Svore", "Krysta", ""], ["Qureshi", "Moinuddin", ""], ["Delfosse", "Nicolas", ""]]}, {"id": "2001.07045", "submitter": "Djordje Jevdjic", "authors": "Javier Picorel, Seyed Alireza Sanaee Kohroudi, Zi Yan, Abhishek\n  Bhattacharjee, Babak Falsafi, Djordje Jevdjic", "title": "SPARTA: A Divide and Conquer Approach to Address Translation for\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual memory (VM) is critical to the usability and programmability of\nhardware accelerators. Unfortunately, implementing accelerator VM efficiently\nis challenging because the area and power constraints make it difficult to\nemploy the large multi-level TLBs used in general-purpose CPUs. Recent research\nproposals advocate a number of restrictions on virtual-to-physical address\nmappings in order to reduce the TLB size or increase its reach. However, such\nrestrictions are unattractive because they forgo many of the original benefits\nof traditional VM, such as demand paging and copy-on-write.\n  We propose SPARTA, a divide and conquer approach to address translation.\nSPARTA splits the address translation into accelerator-side and memory-side\nparts. The accelerator-side translation hardware consists of a tiny TLB\ncovering only the accelerator's cache hierarchy (if any), while the translation\nfor main memory accesses is performed by shared memory-side TLBs. Performing\nthe translation for memory accesses on the memory side allows SPARTA to overlap\ndata fetch with translation, and avoids the replication of TLB entries for data\nshared among accelerators. To further improve the performance and efficiency of\nthe memory-side translation, SPARTA logically partitions the memory space,\ndelegating translation to small and efficient per-partition translation\nhardware. Our evaluation on index-traversal accelerators shows that SPARTA\nvirtually eliminates translation overhead, reducing it by over 30x on average\n(up to 47x) and improving performance by 57%. At the same time, SPARTA requires\nminimal accelerator-side translation hardware, reduces the total number of TLB\nentries in the system, gracefully scales with memory size, and preserves all\nkey VM functionalities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 10:23:12 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Picorel", "Javier", ""], ["Kohroudi", "Seyed Alireza Sanaee", ""], ["Yan", "Zi", ""], ["Bhattacharjee", "Abhishek", ""], ["Falsafi", "Babak", ""], ["Jevdjic", "Djordje", ""]]}, {"id": "2001.07450", "submitter": "Hongliang Tian", "authors": "Youren Shen, Hongliang Tian, Yu Chen, Kang Chen, Runji Wang, Yi Xu,\n  and Yubin Xia", "title": "Occlum: Secure and Efficient Multitasking Inside a Single Enclave of\n  Intel SGX", "comments": null, "journal-ref": null, "doi": "10.1145/3373376.3378469", "report-no": null, "categories": "cs.OS cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intel Software Guard Extensions (SGX) enables user-level code to create\nprivate memory regions called enclaves, whose code and data are protected by\nthe CPU from software and hardware attacks outside the enclaves. Recent work\nintroduces library operating systems (LibOSes) to SGX so that legacy\napplications can run inside enclaves with few or even no modifications. As\nvirtually any non-trivial application demands multiple processes, it is\nessential for LibOSes to support multitasking. However, none of the existing\nSGX LibOSes support multitasking both securely and efficiently.\n  This paper presents Occlum, a system that enables secure and efficient\nmultitasking on SGX. We implement the LibOS processes as SFI-Isolated Processes\n(SIPs). SFI is a software instrumentation technique for sandboxing untrusted\nmodules (called domains). We design a novel SFI scheme named MPX-based,\nMulti-Domain SFI (MMDSFI) and leverage MMDSFI to enforce the isolation of SIPs.\nWe also design an independent verifier to ensure the security guarantees of\nMMDSFI. With SIPs safely sharing the single address space of an enclave, the\nLibOS can implement multitasking efficiently. The Occlum LibOS outperforms the\nstate-of-the-art SGX LibOS on multitasking-heavy workloads by up to 6,600X on\nmicro-benchmarks and up to 500X on application benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 11:42:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Shen", "Youren", ""], ["Tian", "Hongliang", ""], ["Chen", "Yu", ""], ["Chen", "Kang", ""], ["Wang", "Runji", ""], ["Xu", "Yi", ""], ["Xia", "Yubin", ""]]}, {"id": "2001.09599", "submitter": "Matthew Edwards", "authors": "Hardik Jain, Matthew Edwards, Ethan Elenberg, Ankit Singh Rawat,\n  Sriram Vishwanath", "title": "Achieving Multi-Port Memory Performance on Single-Port Memory with\n  Coding Techniques", "comments": "10 pages, 20 figures, ICICT 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many performance critical systems today must rely on performance\nenhancements, such as multi-port memories, to keep up with the increasing\ndemand of memory-access capacity. However, the large area footprints and\ncomplexity of existing multi-port memory designs limit their applicability.\nThis paper explores a coding theoretic framework to address this problem. In\nparticular, this paper introduces a framework to encode data across multiple\nsingle-port memory banks in order to {\\em algorithmically} realize the\nfunctionality of multi-port memory.\n  This paper proposes three code designs with significantly less storage\noverhead compared to the existing replication based emulations of multi-port\nmemories. To further improve performance, we also demonstrate a memory\ncontroller design that utilizes redundancy across coded memory banks to more\nefficiently schedule read and write requests sent across multiple cores.\nFurthermore, guided by DRAM traces, the paper explores {\\em dynamic coding}\ntechniques to improve the efficiency of the coding based memory design. We then\nshow significant performance improvements in critical word read and write\nlatency in the proposed coded-memory design when compared to a traditional\nuncoded-memory design.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:26:21 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Jain", "Hardik", ""], ["Edwards", "Matthew", ""], ["Elenberg", "Ethan", ""], ["Rawat", "Ankit Singh", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "2001.09982", "submitter": "Ahmet Cagri Bagbaba", "authors": "Ahmet Cagri Bagbaba, Maksim Jenihhin, Jaan Raik, Christian Sauer", "title": "Accelerating Transient Fault Injection Campaigns by using Dynamic HDL\n  Slicing", "comments": null, "journal-ref": null, "doi": "10.1109/NORCHIP.2019.8906932", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the complexity of electronic systems for safety-critical\napplications, the cost of safety mechanisms evaluation by fault injection\nsimulation is rapidly going up. To reduce these efforts, we propose a fault\ninjection methodology where Hardware Description Language (HDL) code slicing is\nexploited to accelerate transient fault injection campaigns by pruning fault\nlists and reducing the number of the injections. In particular, the dynamic HDL\nslicing technique provides for a critical fault list and allows avoiding\ninjections at non-critical time-steps. Experimental results on an industrial\ncore show that the proposed methodology can successfully reduce the number of\ninjections by up to 10 percent and speed-up the fault injection campaigns.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 15:17:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Bagbaba", "Ahmet Cagri", ""], ["Jenihhin", "Maksim", ""], ["Raik", "Jaan", ""], ["Sauer", "Christian", ""]]}, {"id": "2001.10715", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Gourav datta, Peter A. Beerel, Massoud Pedram", "title": "qBSA: Logic Design of a 32-bit Block-Skewed RSFQ Arithmetic Logic Unit", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single flux quantum (SFQ) circuits are an attractive beyond-CMOS technology\nbecause they promise two orders of magnitude lower power at clock frequencies\nexceeding 25 GHz.However, every SFQ gate is clocked creating very deep\ngate-level pipelines that are difficult to keep full, particularly for\nsequences that include data-dependent operations. This paper proposes to\nincrease the throughput of SFQ pipelines by re-designing the datapath to accept\nand operate on least-significant bits (LSBs) clock cycles earlier than more\nsignificant bits. This skewed datapath approach reduces the latency of the LSB\nside which can be feedback earlier for use in subsequent data-dependent\noperations increasing their throughput. In particular,we propose to group the\nbits into 4-bit blocks that are operatedon concurrently and create block-skewed\ndatapath units for 32-bit operation. This skewed approach allows a subsequent\ndata-dependent operation to start evaluating as soon as the first 4-bit block\ncompletes. Using this general approach, we developa block-skewed\nMIPS-compatible 32-bit ALU. Our gate-level Verilog design improves the\nthroughput of 32-bit data dependent operations by 2x and 1.5x compared to\npreviously proposed 4-bit bit-slice and 32-bit Ladner-Fischer ALUs\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 07:49:39 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Kundu", "Souvik", ""], ["datta", "Gourav", ""], ["Beerel", "Peter A.", ""], ["Pedram", "Massoud", ""]]}, {"id": "2001.11886", "submitter": "Joel Mandebi Mbongue", "authors": "Joel Mandebi Mbongue, Danielle Tchuinkou Kwadjo and Christophe Bobda", "title": "Exploiting RapidWright in the Automatic Generation of\n  Application-Specific FPGA Overlays", "comments": "This paper is the extended version of the FPT 2019 paper\n  (https://ieeexplore.ieee.org/abstract/document/8977903)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlay architectures implemented on FPGA devices have been proposed as a\nmeans to increase FPGA adoption in general-purpose computing. They provide the\nbenefits of software such as flexibility and programmability, thus making it\neasier to build dedicated compilers. However, existing overlays are generic,\nresource and power hungry with performance usually an order of magnitude lower\nthan bare metal implementations. As a result, FPGA overlays have been confined\nto research and some niche applications. In this paper, we introduce\nApplication-Specific FPGA Overlays (AS-Overlays), which can provide bare-metal\nperformance to FPGA overlays, thus opening doors for broader adoption. Our\napproach is based on the automatic extraction of hardware kernels from data\nflow applications. Extracted kernels are then leveraged for\napplication-specific generation of hardware accelerators. Reconfiguration of\nthe overlay is done with RapidWright which allows to bypass the HDL design\nflow. Through prototyping, we demonstrated the viability and relevance of our\napproach. Experiments show a productivity improvement up to 20x compared to the\nstate of the art FPGA overlays, while achieving over 1.33x higher Fmax than\ndirect FPGA implementation and the possibility of lower resource and power\nconsumption compared to bare metal.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 15:04:23 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 01:58:05 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Mbongue", "Joel Mandebi", ""], ["Kwadjo", "Danielle Tchuinkou", ""], ["Bobda", "Christophe", ""]]}]