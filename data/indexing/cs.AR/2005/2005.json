[{"id": "2005.00044", "submitter": "David Lomet", "authors": "David Lomet (Microsoft Research, Redmond, WA) and Chen Luo (UC Irvine,\n  Irvine, CA)", "title": "Efficiently Reclaiming Space in a Log Structured Store", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A log structured store uses a single write I/O for a number of diverse and\nnon-contiguous pages within a large buffer instead of using a write I/O for\neach page separately. This requires that pages be relocated on every write,\nbecause pages are never updated in place. Instead, pages are dynamically\nremapped on every write. Log structuring was invented for and used initially in\nfile systems. Today, a form of log structuring is used in SSD controllers\nbecause an SSD requires the erasure of a large block of pages before flash\nstorage can be reused. No update-in-place requires that the storage for\nout-of-date pages be reclaimed (garbage collected or \"cleaned\"). We analyze\ncleaning performance and introduce a cleaning strategy that uses a new way to\nprioritize the order in which stale pages are garbage collected. Our cleaning\nstrategy approximates an \"optimal cleaning strategy\". Simulation studies\nconfirm the results of the analysis. This strategy is a significant improvement\nover previous cleaning strategies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:29:42 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lomet", "David", "", "Microsoft Research, Redmond, WA"], ["Luo", "Chen", "", "UC Irvine,\n  Irvine, CA"]]}, {"id": "2005.01016", "submitter": "Andreas Toftegaard Kristensen", "authors": "Andreas Toftegaard Kristensen, Robert Giterman, Alexios\n  Balatsoukas-Stimming, and Andreas Burg", "title": "Lupulus: A Flexible Hardware Accelerator for Neural Networks", "comments": "To be presented at the 2020 International Conference on Acoustics,\n  Speech, and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become indispensable for a wide range of applications,\nbut they suffer from high computational- and memory-requirements, requiring\noptimizations from the algorithmic description of the network to the hardware\nimplementation. Moreover, the high rate of innovation in machine learning makes\nit important that hardware implementations provide a high level of\nprogrammability to support current and future requirements of neural networks.\nIn this work, we present a flexible hardware accelerator for neural networks,\ncalled Lupulus, supporting various methods for scheduling and mapping of\noperations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI\ntechnology and demonstrates a peak performance of 380 GOPS/GHz with latencies\nof 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 07:35:36 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kristensen", "Andreas Toftegaard", ""], ["Giterman", "Robert", ""], ["Balatsoukas-Stimming", "Alexios", ""], ["Burg", "Andreas", ""]]}, {"id": "2005.01206", "submitter": "Yang Zhao", "authors": "Weitao Li, Pengfei Xu, Yang Zhao, Haitong Li, Yuan Xie, Yingyan Lin", "title": "TIMELY: Pushing Data Movements and Interfaces in PIM Accelerators\n  Towards Local and in Time Domain", "comments": "Accepted by 47th International Symposium on Computer Architecture\n  (ISCA'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resistive-random-access-memory (ReRAM) based processing-in-memory (R$^2$PIM)\naccelerators show promise in bridging the gap between Internet of Thing\ndevices' constrained resources and Convolutional/Deep Neural Networks'\n(CNNs/DNNs') prohibitive energy cost. Specifically, R$^2$PIM accelerators\nenhance energy efficiency by eliminating the cost of weight movements and\nimproving the computational density through ReRAM's high density. However, the\nenergy efficiency is still limited by the dominant energy cost of input and\npartial sum (Psum) movements and the cost of digital-to-analog (D/A) and\nanalog-to-digital (A/D) interfaces. In this work, we identify three\nenergy-saving opportunities in R$^2$PIM accelerators: analog data locality,\ntime-domain interfacing, and input access reduction, and propose an innovative\nR$^2$PIM accelerator called TIMELY, with three key contributions: (1) TIMELY\nadopts analog local buffers (ALBs) within ReRAM crossbars to greatly enhance\nthe data locality, minimizing the energy overheads of both input and Psum\nmovements; (2) TIMELY largely reduces the energy of each single D/A (and A/D)\nconversion and the total number of conversions by using time-domain interfaces\n(TDIs) and the employed ALBs, respectively; (3) we develop an only-once input\nread (O$^2$IR) mapping method to further decrease the energy of input accesses\nand the number of D/A conversions. The evaluation with more than 10 CNN/DNN\nmodels and various chip configurations shows that, TIMELY outperforms the\nbaseline R$^2$PIM accelerator, PRIME, by one order of magnitude in energy\nefficiency while maintaining better computational density (up to 31.2$\\times$)\nand throughput (up to 736.6$\\times$). Furthermore, comprehensive studies are\nperformed to evaluate the effectiveness of the proposed ALB, TDI, and O$^2$IR\ninnovations in terms of energy savings and area reduction.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 23:27:51 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Li", "Weitao", ""], ["Xu", "Pengfei", ""], ["Zhao", "Yang", ""], ["Li", "Haitong", ""], ["Xie", "Yuan", ""], ["Lin", "Yingyan", ""]]}, {"id": "2005.01386", "submitter": "Sukanta Dey", "authors": "Sukanta Dey, Sukumar Nandi, and Gaurav Trivedi", "title": "PowerPlanningDL: Reliability-Aware Framework for On-Chip Power Grid\n  Design using Deep Learning", "comments": "Published in proceedings of IEEE/ACM Design, Automation and Test in\n  Europe Conference (DATE) 2020, 6 pages", "journal-ref": "DATE 2020 Proceedings, 1520-1525, IEEE", "doi": "10.23919/DATE48585.2020.9116536", "report-no": null, "categories": "cs.LG cs.AR eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in the complexity of chip designs, VLSI physical design has\nbecome a time-consuming task, which is an iterative design process. Power\nplanning is that part of the floorplanning in VLSI physical design where power\ngrid networks are designed in order to provide adequate power to all the\nunderlying functional blocks. Power planning also requires multiple iterative\nsteps to create the power grid network while satisfying the allowed worst-case\nIR drop and Electromigration (EM) margin. For the first time, this paper\nintroduces Deep learning (DL)-based framework to approximately predict the\ninitial design of the power grid network, considering different reliability\nconstraints. The proposed framework reduces many iterative design steps and\nspeeds up the total design cycle. Neural Network-based multi-target regression\ntechnique is used to create the DL model. Feature extraction is done, and the\ntraining dataset is generated from the floorplans of some of the power grid\ndesigns extracted from the IBM processor. The DL model is trained using the\ngenerated dataset. The proposed DL-based framework is validated using a new set\nof power grid specifications (obtained by perturbing the designs used in the\ntraining phase). The results show that the predicted power grid design is\ncloser to the original design with minimal prediction error (~2%). The proposed\nDL-based approach also improves the design cycle time with a speedup of ~6X for\nstandard power grid benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:01:17 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 06:12:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Dey", "Sukanta", ""], ["Nandi", "Sukumar", ""], ["Trivedi", "Gaurav", ""]]}, {"id": "2005.01445", "submitter": "Siva Kumar Sastry Hari", "authors": "Siva Kumar Sastry Hari, Paolo Rech, Timothy Tsai, Mark Stephenson,\n  Arslan Zulfiqar, Michael Sullivan, Philip Shirvani, Paul Racunas, Joel Emer,\n  Stephen W. Keckler", "title": "Estimating Silent Data Corruption Rates Using a Two-Level Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance and safety-critical system architects must accurately\nevaluate the application-level silent data corruption (SDC) rates of processors\nto soft errors. Such an evaluation requires error propagation all the way from\nparticle strikes on low-level state up to the program output. Existing\napproaches that rely on low-level simulations with fault injection cannot\nevaluate full applications because of their slow speeds, while\napplication-level accelerated fault testing in accelerated particle beams is\noften impractical. We present a new two-level methodology for application\nresilience evaluation that overcomes these challenges. The proposed approach\ndecomposes application failure rate estimation into (1) identifying how\nparticle strikes in low-level unprotected state manifest at the\narchitecture-level, and (2) measuring how such architecture-level\nmanifestations propagate to the program output. We demonstrate the\neffectiveness of this approach on GPU architectures. We also show that using\njust one of the two steps can overestimate SDC rates and produce different\ntrends---the composition of the two is needed for accurate reliability\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:09:47 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hari", "Siva Kumar Sastry", ""], ["Rech", "Paolo", ""], ["Tsai", "Timothy", ""], ["Stephenson", "Mark", ""], ["Zulfiqar", "Arslan", ""], ["Sullivan", "Michael", ""], ["Shirvani", "Philip", ""], ["Racunas", "Paul", ""], ["Emer", "Joel", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2005.01593", "submitter": "Freddy Gabbay", "authors": "Freddy Gabbay, Avi Mendelson and Yinnon Stav", "title": "Electromigration-Aware Architecture for Modern Microprocessors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability is a fundamental requirement in any microprocessor to guarantee\ncorrect execution over its lifetime. The design rules related to reliability\ndepend on the process technology being used and the expected operating\nconditions of the device. To meet reliability requirements, advanced process\ntechnologies (28 nm and below) impose highly challenging design rules. Such\ndesign-for-reliability rules have become a major burden on the flow of VLSI\nimplementation because of the severe physical constraints they impose.\n  This paper focuses on electromigration (EM), which is one of the major\ncritical factors affecting semiconductor reliability. EM is the aging process\nof on-die wires and vias and is induced by excessive current flow that can\ndamage wires and may also significantly impact the integrated-circuit clock\nfrequency. EM exerts a comprehensive global effect on devices because it\nimpacts wires that may reside inside the standard or custom logical cells,\nbetween logical cells, inside memory elements, and within wires that\ninterconnect functional blocks.\n  The design-implementation flow (synthesis and place-and-route) currently\ndetects violations of EM-reliability rules and attempts to solve them. In\ncontrast, this paper proposes a new approach to enhance these flows by using\nEM-aware architecture. Our results show that the proposed solution can relax EM\ndesign efforts in microprocessors and more than double microprocessor lifetime.\nThis work demonstrates this proposed approach for modern microprocessors,\nalthough the principals and ideas can be adapted to other cases as well.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:06:25 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 05:22:32 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Gabbay", "Freddy", ""], ["Mendelson", "Avi", ""], ["Stav", "Yinnon", ""]]}, {"id": "2005.02193", "submitter": "Nils Wistoff", "authors": "Nils Wistoff, Moritz Schneider, Frank K. G\\\"urkaynak, Luca Benini,\n  Gernot Heiser", "title": "Prevention of Microarchitectural Covert Channels on an Open-Source\n  64-bit RISC-V Core", "comments": "6 pages, 7 figures, submitted to CARRV '20, additional appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covert channels enable information leakage across security boundaries of the\noperating system. Microarchitectural covert channels exploit changes in\nexecution timing resulting from competing access to limited hardware resources.\nWe use the recent experimental support for time protection, aimed at preventing\ncovert channels, in the seL4 microkernel and evaluate the efficacy of the\nmechanisms against five known channels on Ariane, an open-source 64-bit\napplication-class RISC-V core. We confirm that without hardware support, these\ndefences are expensive and incomplete. We show that the addition of a\nsingle-instruction extension to the RISC-V ISA, that flushes microarchitectural\nstate, can enable the OS to close all five evaluated covert channels with low\nincrease in context switch costs and negligible hardware overhead. We conclude\nthat such a mechanism is essential for security.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:20:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Wistoff", "Nils", ""], ["Schneider", "Moritz", ""], ["G\u00fcrkaynak", "Frank K.", ""], ["Benini", "Luca", ""], ["Heiser", "Gernot", ""]]}, {"id": "2005.02206", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Best implementations of quaternary adders", "comments": "10 pages, 25 figures, research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of a quaternary 1-digit adder composed of a 2-bit binary\nadder, quaternary to binary decoders and binary to quaternary encoders is\ncompared with several recent implementations of quaternary adders. This simple\nimplementation outperforms all other implementations using only one power\nsupply. It is equivalent to the best other implementation using three power\nsupplies. The best quaternary adder using a 2-bit binary adder, the interface\ncircuits between quaternary and binary levels are just overhead compared to the\nbinary adder. This result shows that the quaternary approach for adders use\nmore transistors, more chip area and more power dissipation than the\ncorresponding binary ones.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:14:27 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "2005.02310", "submitter": "Michael Wong", "authors": "Michael D. Wong, Aatish Kishan Varma, Anirudh Sivaraman", "title": "Testing Compilers for Programmable Switches Through Switch Hardware\n  Simulation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable switches have emerged as powerful and flexible alternatives to\nfixed-function forwarding devices. But because of the unique hardware\nconstraints of network switches, the design and implementation of compilers\ntargeting these devices is tedious and error prone. Despite the important role\nthat compilers play in software development, there is a dearth of tools for\ntesting compilers for programmable network devices. We present Druzhba, a\nprogrammable switch simulator used for testing compilers targeting programmable\npacket-processing substrates. We show that we can model the low-level behavior\nof a switch's programmable hardware. We further show how our machine model can\nbe used by compiler developers to target Druzhba as a compiler backend.\nGenerated machine code programs are fed into Druzhba and tested using a\nfuzzing-based approach that allows compiler developers to test the correctness\nof their compilers. Using a program-synthesis-based compiler as a case study,\nwe demonstrate how Druzhba has been successful in testing compiler-generated\nmachine code for our simulated switch pipeline instruction set.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:15:52 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 15:22:17 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 18:40:15 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wong", "Michael D.", ""], ["Varma", "Aatish Kishan", ""], ["Sivaraman", "Anirudh", ""]]}, {"id": "2005.02506", "submitter": "Jean-Christophe Le Lann", "authors": "Florent Kermarrec and S\\'ebastien Bourdeauducq and Jean-Christophe Le\n  Lann and Hannah Badier", "title": "LiteX: an open-source SoC builder and library based on Migen Python DSL", "comments": "6 pages, OSDA'2019 Open Source Hardware Design, colocated with\n  DATE'19, Florence, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  LiteX is a GitHub-hosted SoC builder / IP library and utilities that can be\nused to create SoCs and full FPGA designs. Besides being open-source and BSD\nlicensed, its originality lies in the fact that its IP components are entirely\ndescribed using Migen Python internal DSL, which simplifies its design in\ndepth. LiteX already supports various softcores CPUs and essential peripherals,\nwith no dependencies on proprietary IP blocks or generators. This paper\nprovides an overview of LiteX: two real SoC designs on FPGA are presented. They\nboth leverage the LiteX approach in terms of design entry, libraries and\nintegration capabilities. The first one is based on RISC-V core, while the\nsecond is based on a LM32 core. In the second use case, we further demonstrate\nthe use of a fully open-source toolchain coupled with LiteX.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:28:48 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kermarrec", "Florent", ""], ["Bourdeauducq", "S\u00e9bastien", ""], ["Lann", "Jean-Christophe Le", ""], ["Badier", "Hannah", ""]]}, {"id": "2005.02550", "submitter": "Hao Zheng", "authors": "Yuting Cao, Hao Zheng, Sandip Ray, Jin Yang", "title": "A Post-Silicon Trace Analysis Approach for System-on-Chip Protocol Debug", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing system-level behavior from silicon traces is a critical\nproblem in post-silicon validation of System-on-Chip designs. Current\nindustrial practice in this area is primarily manual, depending on\ncollaborative insights of the architects, designers, and validators. This paper\npresents a trace analysis approach that exploits architectural models of the\nsystem-level protocols to reconstruct design behavior from partially observed\nsilicon traces in the presence of ambiguous and noisy data. The output of the\napproach is a set of all potential interpretations of a system's internal\nexecutions abstracted to the system-level protocols. To support the trace\nanalysis approach, a companion trace signal selection framework guided by\nsystem-level protocols is also presented, and its impacts on the complexity and\naccuracy of the analysis approach are discussed. That approach and the\nframework have been evaluated on a multi-core system-on-chip prototype that\nimplements a set of common industrial system-level protocols.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:29:15 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Cao", "Yuting", ""], ["Zheng", "Hao", ""], ["Ray", "Sandip", ""], ["Yang", "Jin", ""]]}, {"id": "2005.02678", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Comparing quaternary and binary multipliers", "comments": "7 pages, 15 figures, Research Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the implementation of a 8x8 bit multiplier with two different\nimplementations of a 4x4 quaternary digit multiplier. Interfacing this binary\nmultiplier with quaternary to binary decoders and binary to quaternary encoders\nleads to a 4x4 multiplier that outperforms the best direct implementation of a\n4x4 quaternary multiplier. The far greater complexity of the 1-digit\nmultipliers and 1-digit adders used in this direct implementation compared to\nthe binary 1-bit multipliers and full adders cannot be compensated by the\nreduced count of quaternary operators. As the best quaternary multiplier\nincludes the corresponding binary one, it means that there is no opportunity to\nget less interconnects, less chip area, less power dissipation with the\nquaternary multiplier.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:23:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "2005.03002", "submitter": "Dayane Reis", "authors": "Dayane Reis, Jonathan Takeshita, Taeho Jung, Michael Niemier, and\n  Xiaobo Sharon Hu", "title": "Computing-in-Memory for Performance and Energy Efficient Homomorphic\n  Encryption", "comments": "14 pages", "journal-ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 28, Issue: 11, Nov. 2020)", "doi": "10.1109/TVLSI.2020.3017595", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic encryption (HE) allows direct computations on encrypted data.\nDespite numerous research efforts, the practicality of HE schemes remains to be\ndemonstrated. In this regard, the enormous size of ciphertexts involved in HE\ncomputations degrades computational efficiency. Near-memory Processing (NMP)\nand Computing-in-memory (CiM) - paradigms where computation is done within the\nmemory boundaries - represent architectural solutions for reducing latency and\nenergy associated with data transfers in data-intensive applications such as\nHE. This paper introduces CiM-HE, a Computing-in-memory (CiM) architecture that\ncan support operations for the B/FV scheme, a somewhat homomorphic encryption\nscheme for general computation. CiM-HE hardware consists of customized\nperipherals such as sense amplifiers, adders, bit-shifters, and sequencing\ncircuits. The peripherals are based on CMOS technology, and could support\ncomputations with memory cells of different technologies. Circuit-level\nsimulations are used to evaluate our CiM-HE framework assuming a 6T-SRAM\nmemory. We compare our CiM-HE implementation against (i) two optimized CPU HE\nimplementations, and (ii) an FPGA-based HE accelerator implementation. When\ncompared to a CPU solution, CiM-HE obtains speedups between 4.6x and 9.1x, and\nenergy savings between 266.4x and 532.8x for homomorphic multiplications (the\nmost expensive HE operation). Also, a set of four end-to-end tasks, i.e., mean,\nvariance, linear regression, and inference are up to 1.1x, 7.7x, 7.1x, and 7.5x\nfaster (and 301.1x, 404.6x, 532.3x, and 532.8x more energy efficient). Compared\nto CPU-based HE in a previous work, CiM-HE obtain 14.3x speed-up and >2600x\nenergy savings. Finally, our design offers 2.2x speed-up with 88.1x energy\nsavings compared to a state-of-the-art FPGA-based accelerator.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:10:08 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 15:18:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Reis", "Dayane", ""], ["Takeshita", "Jonathan", ""], ["Jung", "Taeho", ""], ["Niemier", "Michael", ""], ["Hu", "Xiaobo Sharon", ""]]}, {"id": "2005.03775", "submitter": "Paolo Meloni", "authors": "Marco Carreras, Gianfranco Deriu, Luigi Raffo, Luca Benini and Paolo\n  Meloni", "title": "Optimizing Temporal Convolutional Network inference on FPGA-based\n  accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are extensively used in a wide range of\napplications, commonly including computer vision tasks like image and video\nclassification, recognition, and segmentation. Recent research results\ndemonstrate that multilayer(deep) networks involving mono-dimensional\nconvolutions and dilation can be effectively used in time series and sequences\nclassification and segmentation, as well as in tasks involving sequence\nmodelling. These structures, commonly referred to as Temporal Convolutional\nNetworks (TCNs), have been demonstrated to consistently outperform Recurrent\nNeural Networks in terms of accuracy and training time [1]. While FPGA-based\ninference accelerators for classic CNNs are widespread, literature is lacking\nin a quantitative evaluation of their usability on inference for TCN models. In\nthis paper we present such an evaluation, considering a CNN accelerator with\nspecific features supporting TCN kernels as a reference and a set of\nstate-of-the-art TCNs as a benchmark. Experimental results show that, during\nTCN execution, operational intensity can be critical for the overall\nperformance. We propose a convolution scheduling based on batch processing that\ncan boost efficiency up to 96% of theoretical peak performance. Overall we can\nachieve up to 111,8 GOPS/s and power efficiency of 33,9 GOPS/s/W on an\nUltrascale+ ZU3EG (up to 10x speedup and 3x power efficiency improvement with\nrespect to pure software implementation).\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:39:04 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Carreras", "Marco", ""], ["Deriu", "Gianfranco", ""], ["Raffo", "Luigi", ""], ["Benini", "Luca", ""], ["Meloni", "Paolo", ""]]}, {"id": "2005.03842", "submitter": "Ali Hadi Zadeh", "authors": "Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos", "title": "GOBO: Quantizing Attention-Based NLP Models for Low Latency and Energy\n  Efficient Inference", "comments": "Accepted at the 53rd IEEE/ACM International Symposium on\n  Microarchitecture - MICRO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based models have demonstrated remarkable success in various\nnatural language understanding tasks. However, efficient execution remains a\nchallenge for these models which are memory-bound due to their massive number\nof parameters. We present GOBO, a model quantization technique that compresses\nthe vast majority (typically 99.9%) of the 32-bit floating-point parameters of\nstate-of-the-art BERT models and their variants to 3 bits while maintaining\ntheir accuracy. Unlike other quantization methods, GOBO does not require\nfine-tuning nor retraining to compensate for the quantization error. We present\ntwo practical hardware applications of GOBO. In the first GOBO reduces memory\nstorage and traffic and as a result inference latency and energy consumption.\nThis GOBO memory compression mechanism is plug-in compatible with many\narchitectures; we demonstrate it with the TPU, Eyeriss, and an architecture\nusing Tensor Cores-like units. Second, we present a co-designed hardware\narchitecture that also reduces computation. Uniquely, the GOBO architecture\nmaintains most of the weights in 3b even during computation, a property that:\n(1) makes the processing elements area efficient, allowing us to pack more\ncompute power per unit area, (2) replaces most multiply-accumulations with\nadditions, and (3) reduces the off-chip traffic by amplifying on-chip memory\ncapacity.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 03:59:53 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 00:09:30 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zadeh", "Ali Hadi", ""], ["Edo", "Isak", ""], ["Awad", "Omar Mohamed", ""], ["Moshovos", "Andreas", ""]]}, {"id": "2005.04324", "submitter": "Zeke Wang Dr.", "authors": "Zeke Wang, Hongjing Huang, Jie Zhang, Gustavo Alonso", "title": "Benchmarking High Bandwidth Memory on FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs are starting to be enhanced with High Bandwidth Memory (HBM) as a way\nto reduce the memory bandwidth bottleneck encountered in some applications and\nto give the FPGA more capacity to deal with application state. However, the\nperformance characteristics of HBM are still not well specified, especially in\nthe context of FPGAs. In this paper, we bridge the gap between nominal\nspecifications and actual performance by benchmarkingHBM on a state-of-the-art\nFPGA, i.e., a Xilinx Alveo U280 featuring a two-stack HBM subsystem. To this\nend, we propose Shuhai, a benchmarking tool that allows us to demystify all the\nunderlying details of HBM on an FPGA. FPGA-based benchmarking should also\nprovide a more accurate picture of HBM than doing so on CPUs/GPUs, since\nCPUs/GPUs are noisier systems due to their complex control logic and cache\nhierarchy. Since the memory itself is complex, leveraging custom hardware logic\nto benchmark inside an FPGA provides more details as well as accurate and\ndeterministic measurements. We observe that 1) HBM is able to provide up to\n425GB/s memory bandwidth, and 2) how HBM is used has a significant impact on\nperformance, which in turn demonstrates the importance of unveiling the\nperformance characteristics of HBM so as to select the best approach. As a\nyardstick, we also applyShuhaito DDR4to show the differences between HBM and\nDDR4.Shuhai can be easily generalized to other FPGA boards or other generations\nof memory, e.g., HBM3, and DDR3. We will makeShuhaiopen-source, benefiting the\ncommunity\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 00:33:23 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wang", "Zeke", ""], ["Huang", "Hongjing", ""], ["Zhang", "Jie", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2005.04536", "submitter": "Alexis Asseman", "authors": "Alexis Asseman, Nicolas Antoine and Ahmet S. Ozcan", "title": "Accelerating Deep Neuroevolution on Distributed FPGAs for Reinforcement\n  Learning Problems", "comments": "12 pages. Submitted to ACM Journal on Emerging Technologies in\n  Computing Systems: Special Issue on Hardware and Algorithms for Efficient\n  Machine Learning", "journal-ref": null, "doi": "10.1145/3425500", "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning augmented by the representational power of deep neural\nnetworks, has shown promising results on high-dimensional problems, such as\ngame playing and robotic control. However, the sequential nature of these\nproblems poses a fundamental challenge for computational efficiency. Recently,\nalternative approaches such as evolutionary strategies and deep neuroevolution\ndemonstrated competitive results with faster training time on distributed CPU\ncores. Here, we report record training times (running at about 1 million frames\nper second) for Atari 2600 games using deep neuroevolution implemented on\ndistributed FPGAs. Combined hardware implementation of the game console, image\npre-processing and the neural network in an optimized pipeline, multiplied with\nthe system level parallelism enabled the acceleration. These results are the\nfirst application demonstration on the IBM Neural Computer, which is a custom\ndesigned system that consists of 432 Xilinx FPGAs interconnected in a 3D mesh\nnetwork topology. In addition to high performance, experiments also showed\nimprovement in accuracy for all games compared to the CPU-implementation of the\nsame algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:41:39 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Asseman", "Alexis", ""], ["Antoine", "Nicolas", ""], ["Ozcan", "Ahmet S.", ""]]}, {"id": "2005.04737", "submitter": "Behzad Salami", "authors": "Behzad Salami, Osman Unsal, Adrian Cristal", "title": "Power and Accuracy of Multi-Layer Perceptrons (MLPs) under\n  Reduced-voltage FPGA BRAMs Operation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit the aggressive supply voltage underscaling\ntechnique in Block RAMs (BRAMs) of Field Programmable Gate Arrays (FPGAs) to\nimprove the energy efficiency of Multi-Layer Perceptrons (MLPs). Additionally,\nwe evaluate and improve the resilience of this accelerator. Through experiments\non several representative FPGA fabrics, we observe that until a minimum safe\nvoltage level, i.e., Vmin the MLP accuracy is not affected. This safe region\ninvolves a large voltage guardband. Also, it involves a narrower voltage region\nwhere faults start to appear in memories due to the increased circuit delay,\nbut these faults are masked by MLP, and thus, its accuracy is not affected.\nHowever, further undervolting causes significant accuracy loss as a result of\nthe fast-increasing high fault rates. Based on the characterization of these\nundervolting faults, we propose fault mitigation techniques that can\neffectively improve the resilience behavior of such accelerator. Our evaluation\nis based on four FPGA platforms. On average, we achieve >90% energy saving with\na negligible accuracy loss of up to 0.1%.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 18:09:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Salami", "Behzad", ""], ["Unsal", "Osman", ""], ["Cristal", "Adrian", ""]]}, {"id": "2005.04750", "submitter": "Anup Das", "authors": "Shihao Song, Anup Das, Nagarajan Kandasamy", "title": "Exploiting Inter- and Intra-Memory Asymmetries for Data Mapping in\n  Hybrid Tiered-Memories", "comments": "15 pages, 29 figures, accepted at ACM SIGPLAN International Symposium\n  on Memory Management", "journal-ref": null, "doi": "10.1145/3381898.3397215", "report-no": null, "categories": "cs.AR cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern computing systems are embracing hybrid memory comprising of DRAM and\nnon-volatile memory (NVM) to combine the best properties of both memory\ntechnologies, achieving low latency, high reliability, and high density. A\nprominent characteristic of DRAM-NVM hybrid memory is that it has NVM access\nlatency much higher than DRAM access latency. We call this inter-memory\nasymmetry. We observe that parasitic components on a long bitline are a major\nsource of high latency in both DRAM and NVM, and a significant factor\ncontributing to high-voltage operations in NVM, which impact their reliability.\nWe propose an architectural change, where each long bitline in DRAM and NVM is\nsplit into two segments by an isolation transistor. One segment can be accessed\nwith lower latency and operating voltage than the other. By introducing tiers,\nwe enable non-uniform accesses within each memory type (which we call\nintra-memory asymmetry), leading to performance and reliability trade-offs in\nDRAM-NVM hybrid memory. We extend existing NVM-DRAM OS in three ways. First, we\nexploit both inter- and intra-memory asymmetries to allocate and migrate memory\npages between the tiers in DRAM and NVM. Second, we improve the OS's page\nallocation decisions by predicting the access intensity of a newly-referenced\nmemory page in a program and placing it to a matching tier during its initial\nallocation. This minimizes page migrations during program execution, lowering\nthe performance overhead. Third, we propose a solution to migrate pages between\nthe tiers of the same memory without transferring data over the memory channel,\nminimizing channel occupancy and improving performance. Our overall approach,\nwhich we call MNEME, to enable and exploit asymmetries in DRAM-NVM hybrid\ntiered memory improves both performance and reliability for both single-core\nand multi-programmed workloads.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 18:53:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""], ["Kandasamy", "Nagarajan", ""]]}, {"id": "2005.04753", "submitter": "Anup Das", "authors": "Shihao Song, Anup Das, Onur Mutlu, Nagarajan Kandasamy", "title": "Improving Phase Change Memory Performance with Data Content Aware Access", "comments": "18 pages, 21 figures, accepted at ACM SIGPLAN International Symposium\n  on Memory Management (ISMM)", "journal-ref": null, "doi": "10.1145/3381898.3397210", "report-no": null, "categories": "cs.AR cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A prominent characteristic of write operation in Phase-Change Memory (PCM) is\nthat its latency and energy are sensitive to the data to be written as well as\nthe content that is overwritten. We observe that overwriting unknown memory\ncontent can incur significantly higher latency and energy compared to\noverwriting known all-zeros or all-ones content. This is because all-zeros or\nall-ones content is overwritten by programming the PCM cells only in one\ndirection, i.e., using either SET or RESET operations, not both. In this paper,\nwe propose data content aware PCM writes (DATACON), a new mechanism that\nreduces the latency and energy of PCM writes by redirecting these requests to\noverwrite memory locations containing all-zeros or all-ones. DATACON operates\nin three steps. First, it estimates how much a PCM write access would benefit\nfrom overwriting known content (e.g., all-zeros, or all-ones) by\ncomprehensively considering the number of set bits in the data to be written,\nand the energy-latency trade-offs for SET and RESET operations in PCM. Second,\nit translates the write address to a physical address within memory that\ncontains the best type of content to overwrite, and records this translation in\na table for future accesses. We exploit data access locality in workloads to\nminimize the address translation overhead. Third, it re-initializes unused\nmemory locations with known all-zeros or all-ones content in a manner that does\nnot interfere with regular read and write accesses. DATACON overwrites unknown\ncontent only when it is absolutely necessary to do so. We evaluate DATACON with\nworkloads from state-of-the-art machine learning applications, SPEC CPU2017,\nand NAS Parallel Benchmarks. Results demonstrate that DATACON significantly\nimproves system performance and memory system energy consumption compared to\nthe best of performance-oriented state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 19:07:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""], ["Mutlu", "Onur", ""], ["Kandasamy", "Nagarajan", ""]]}, {"id": "2005.07137", "submitter": "Renzo Andri", "authors": "Renzo Andri, Geethan Karunaratne, Lukas Cavigelli, Luca Benini", "title": "ChewBaccaNN: A Flexible 223 TOPS/W BNN Accelerator", "comments": "Accepted at IEEE ISCAS 2021, Daegu, South Korea, 23-26 May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks enable smart IoT devices, as they significantly reduce\nthe required memory footprint and computational complexity while retaining a\nhigh network performance and flexibility. This paper presents ChewBaccaNN, a\n0.7 mm$^2$ sized binary convolutional neural network (CNN) accelerator designed\nin GlobalFoundries 22 nm technology. By exploiting efficient data re-use, data\nbuffering, latch-based memories, and voltage scaling, a throughput of 241 GOPS\nis achieved while consuming just 1.1 mW at 0.4V/154MHz during inference of\nbinary CNNs with up to 7x7 kernels, leading to a peak core energy efficiency of\n223 TOPS/W. ChewBaccaNN's flexibility allows to run a much wider range of\nbinary CNNs than other accelerators, drastically improving the accuracy-energy\ntrade-off beyond what can be captured by the TOPS/W metric. In fact, it can\nperform CIFAR-10 inference at 86.8% accuracy with merely 1.3 $\\mu J$, thus\nexceeding the accuracy while at the same time lowering the energy cost by 2.8x\ncompared to even the most efficient and much larger analog processing-in-memory\ndevices, while keeping the flexibility of running larger CNNs for higher\naccuracy when needed. It also runs a binary ResNet-18 trained on the 1000-class\nILSVRC dataset and improves the energy efficiency by 4.4x over accelerators of\nsimilar flexibility. Furthermore, it can perform inference on a binarized\nResNet-18 trained with 8-bases Group-Net to achieve a 67.5% Top-1 accuracy with\nonly 3.0 mJ/frame -- at an accuracy drop of merely 1.8% from the full-precision\nResNet-18.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:20:00 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 11:09:30 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 09:18:58 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Andri", "Renzo", ""], ["Karunaratne", "Geethan", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "2005.07613", "submitter": "Jawad Haj-Yahya", "authors": "Jawad Haj-Yahya, Mohammed Alser, Jeremie Kim, A. Giray\n  Yagl{\\i}k\\c{c}{\\i}, Nandita Vijaykumar, Efraim Rotem, Onur Mutlu", "title": "SysScale: Exploiting Multi-domain Dynamic Voltage and Frequency Scaling\n  for Energy Efficient Mobile Processors", "comments": "To appear at ISCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three domains in a modern thermally-constrained mobile\nsystem-on-chip (SoC): compute, IO, and memory. We observe that a modern SoC\ntypically allocates a fixed power budget, corresponding to worst-case\nperformance demands, to the IO and memory domains even if they are\nunderutilized. The resulting unfair allocation of the power budget across\ndomains can cause two major issues: 1) the IO and memory domains can operate at\na higher frequency and voltage than necessary, increasing power consumption and\n2) the unused power budget of the IO and memory domains cannot be used to\nincrease the throughput of the compute domain, hampering performance. To avoid\nthese issues, it is crucial to dynamically orchestrate the distribution of the\nSoC power budget across the three domains based on their actual performance\ndemands.\n  We propose SysScale, a new multi-domain power management technique to improve\nthe energy efficiency of mobile SoCs. SysScale is based on three key ideas.\nFirst, SysScale introduces an accurate algorithm to predict the performance\n(e.g., bandwidth and latency) demands of the three SoC domains. Second,\nSysScale uses a new DVFS (dynamic voltage and frequency scaling) mechanism to\ndistribute the SoC power to each domain according to the predicted performance\ndemands. Third, in addition to using a global DVFS mechanism, SysScale uses\ndomain-specialized techniques to optimize the energy efficiency of each domain\nat different operating points.\n  We implement SysScale on an Intel Skylake microprocessor for mobile devices\nand evaluate it using a wide variety of SPEC CPU2006, graphics (3DMark), and\nbattery life workloads (e.g., video playback). On a 2-core Skylake, SysScale\nimproves the performance of SPEC CPU2006 and 3DMark workloads by up to 16% and\n8.9% (9.2% and 7.9% on average), respectively.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 16:08:38 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:46:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Haj-Yahya", "Jawad", ""], ["Alser", "Mohammed", ""], ["Kim", "Jeremie", ""], ["Yagl\u0131k\u00e7\u0131", "A. Giray", ""], ["Vijaykumar", "Nandita", ""], ["Rotem", "Efraim", ""], ["Mutlu", "Onur", ""]]}, {"id": "2005.08098", "submitter": "Zhi-Gang Liu", "authors": "Zhi-Gang Liu, Paul N. Whatmough, Matthew Mattina", "title": "Systolic Tensor Array: An Efficient Structured-Sparse GEMM Accelerator\n  for Mobile CNN Inference", "comments": "Accepted by IEEE Computer Architecture Letters on 3/4/2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) inference on mobile devices demands\nefficient hardware acceleration of low-precision (INT8) general matrix\nmultiplication (GEMM). The systolic array (SA) is a pipelined 2D array of\nprocessing elements (PEs), with very efficient local data movement, well suited\nto accelerating GEMM, and widely deployed in industry. In this work, we\ndescribe two significant improvements to the traditional SA architecture, to\nspecifically optimize for CNN inference. Firstly, we generalize the traditional\nscalar PE, into a Tensor-PE, which gives rise to a family of new Systolic\nTensor Array (STA) microarchitectures. The STA family increases intra-PE\noperand reuse and datapath efficiency, resulting in circuit area and power\ndissipation reduction of as much as 2.08x and 1.36x respectively, compared to\nthe conventional SA at iso-throughput with INT8 operands. Secondly, we extend\nthis design to support a novel block-sparse data format called density-bound\nblock (DBB). This variant (STA-DBB) achieves a 3.14x and 1.97x improvement over\nthe SA baseline at iso-throughput in area and power respectively, when\nprocessing specially-trained DBB-sparse models, while remaining fully backwards\ncompatible with dense models.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:47:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Zhi-Gang", ""], ["Whatmough", "Paul N.", ""], ["Mattina", "Matthew", ""]]}, {"id": "2005.08183", "submitter": "Lutan Zhao", "authors": "Lutan Zhao, Peinan Li, Rui Hou, Michael C. Huang, Jiazhen Li, Lixin\n  Zhang, Xuehai Qian, Dan Meng", "title": "A Lightweight Isolation Mechanism for Secure Branch Predictors", "comments": "13 pages, 10 figures, submitted to MICRO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently exposed vulnerabilities reveal the necessity to improve the security\nof branch predictors. Branch predictors record history about the execution of\ndifferent programs, and such information from different processes are stored in\nthe same structure and thus accessible to each other. This leaves the attackers\nwith the opportunities for malicious training and malicious perception. Instead\nof flush-based or physical isolation of hardware resources, we want to achieve\nisolation of the content in these hardware tables with some lightweight\nprocessing using randomization as follows. (1) Content encoding. We propose to\nuse hardware-based thread-private random numbers to encode the contents of the\nbranch predictor tables (both direction and destination histories) which we\ncall XOR-BP. Specifically, the data is encoded by XOR operation with the key\nbefore written in the table and decoded after read from the table. Such a\nmechanism obfuscates the information adding difficulties to cross-process or\ncross-privilege level analysis and perception. It achieves a similar effect of\nlogical isolation but adds little in terms of space or time overheads. (2)\nIndex encoding. We propose a randomized index mechanism of the branch predictor\n(Noisy-XOR-BP). Similar to the XOR-BP, another thread-private random number is\nused together with the branch instruction address as the input to compute the\nindex of the branch predictor. This randomized indexing mechanism disrupts the\ncorrespondence between the branch instruction address and the branch predictor\nentry, thus increases the noise for malicious perception attacks. Our analyses\nusing an FPGA-based RISC-V processor prototype and additional auxiliary\nsimulations suggest that the proposed mechanisms incur a very small performance\ncost while providing strong protection.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 07:54:11 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 04:19:48 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zhao", "Lutan", ""], ["Li", "Peinan", ""], ["Hou", "Rui", ""], ["Huang", "Michael C.", ""], ["Li", "Jiazhen", ""], ["Zhang", "Lixin", ""], ["Qian", "Xuehai", ""], ["Meng", "Dan", ""]]}, {"id": "2005.08478", "submitter": "Yuan He", "authors": "Yuan He, Jinyu Jiao, Thang Cao, and Masaaki Kondo", "title": "Energy-Efficient On-Chip Networks through Profiled Hybrid Switching", "comments": "To appear in the 30th ACM Great Lakes Symposium on VLSI (GLSVLSI'20),\n  Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual channel flow control is the de facto choice for modern\nnetworks-on-chip to allow better utilization of the link bandwidth through\nbuffering and packet switching, which are also the sources of large power\nfootprint and long per-hop latency. On the other hand, bandwidth can be\nplentiful for parallel workloads under virtual channel flow control. Thus,\ndated but simpler flow controls such as circuit switching can be utilized to\nimprove the energy efficiency of modern networks-on-chip. In this paper, we\npropose to utilize part of the link bandwidth under circuit switching so that\npart of the traffic can be transmitted bufferlessly without routing. Our\nevaluations reveal that this proposal leads to a reduction of energy per flit\nby up to 32% while also provides very competitive latency per flit when\ncompared to networks under virtual channel flow control.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 06:28:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["He", "Yuan", ""], ["Jiao", "Jinyu", ""], ["Cao", "Thang", ""], ["Kondo", "Masaaki", ""]]}, {"id": "2005.09526", "submitter": "Jawar Singh Dr.", "authors": "Abhash Kumar, Jawar Singh, Sai Manohar Beeraka, and Bharat Gupta", "title": "In-memory Implementation of On-chip Trainable and Scalable ANN for AI/ML\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional von Neumann architecture based processors become inefficient in\nterms of energy and throughput as they involve separate processing and memory\nunits, also known as~\\textit{memory wall}. The memory wall problem is further\nexacerbated when massive parallelism and frequent data movement are required\nbetween processing and memory units for real-time implementation of artificial\nneural network (ANN) that enables many intelligent applications. One of the\nmost promising approach to address the memory wall problem is to carry out\ncomputations inside the memory core itself that enhances the memory bandwidth\nand energy efficiency for extensive computations. This paper presents an\nin-memory computing architecture for ANN enabling artificial intelligence (AI)\nand machine learning (ML) applications. The proposed architecture utilizes deep\nin-memory architecture based on standard six transistor (6T) static random\naccess memory (SRAM) core for the implementation of a multi-layered perceptron.\nOur novel on-chip training and inference in-memory architecture reduces energy\ncost and enhances throughput by simultaneously accessing the multiple rows of\nSRAM array per precharge cycle and eliminating the frequent access of data. The\nproposed architecture realizes backpropagation which is the keystone during the\nnetwork training using newly proposed different building blocks such as weight\nupdation, analog multiplication, error calculation, signed analog to digital\nconversion, and other necessary signal control units. The proposed architecture\nwas trained and tested on the IRIS dataset which exhibits $\\approx46\\times$\nmore energy efficient per MAC (multiply and accumulate) operation compared to\nearlier classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:36:39 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kumar", "Abhash", ""], ["Singh", "Jawar", ""], ["Beeraka", "Sai Manohar", ""], ["Gupta", "Bharat", ""]]}, {"id": "2005.09748", "submitter": "Nastaran Hajinazar", "authors": "Nastaran Hajinazar, Pratyush Patel, Minesh Patel, Konstantinos\n  Kanellopoulos, Saugata Ghose, Rachata Ausavarungnirun, Geraldo Francisco de\n  Oliveira Jr., Jonathan Appavoo, Vivek Seshadri, Onur Mutlu", "title": "The Virtual Block Interface: A Flexible Alternative to the Conventional\n  Virtual Memory Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers continue to diversify with respect to system designs, emerging\nmemory technologies, and application memory demands. Unfortunately, continually\nadapting the conventional virtual memory framework to each possible system\nconfiguration is challenging, and often results in performance loss or requires\nnon-trivial workarounds. To address these challenges, we propose a new virtual\nmemory framework, the Virtual Block Interface (VBI). We design VBI based on the\nkey idea that delegating memory management duties to hardware can reduce the\noverheads and software complexity associated with virtual memory. VBI\nintroduces a set of variable-sized virtual blocks (VBs) to applications. Each\nVB is a contiguous region of the globally-visible VBI address space, and an\napplication can allocate each semantically meaningful unit of information\n(e.g., a data structure) in a separate VB. VBI decouples access protection from\nmemory allocation and address translation. While the OS controls which programs\nhave access to which VBs, dedicated hardware in the memory controller manages\nthe physical memory allocation and address translation of the VBs. This\napproach enables several architectural optimizations to (1) efficiently and\nflexibly cater to different and increasingly diverse system configurations, and\n(2) eliminate key inefficiencies of conventional virtual memory. We demonstrate\nthe benefits of VBI with two important use cases: (1) reducing the overheads of\naddress translation (for both native execution and virtual machine\nenvironments), as VBI reduces the number of translation requests and associated\nmemory accesses; and (2) two heterogeneous main memory architectures, where VBI\nincreases the effectiveness of managing fast memory regions. For both cases,\nVBI significanttly improves performance over conventional virtual memory.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:40:54 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Hajinazar", "Nastaran", ""], ["Patel", "Pratyush", ""], ["Patel", "Minesh", ""], ["Kanellopoulos", "Konstantinos", ""], ["Ghose", "Saugata", ""], ["Ausavarungnirun", "Rachata", ""], ["Oliveira", "Geraldo Francisco de", "Jr."], ["Appavoo", "Jonathan", ""], ["Seshadri", "Vivek", ""], ["Mutlu", "Onur", ""]]}, {"id": "2005.10333", "submitter": "Mohammad Sina Kiarostami", "authors": "Mohammad Sina Karvandi, Saleh Khalaj Monfared, Mohammad Sina\n  Kiarostami, Dara Rahmati, Saeid Gorgin", "title": "A Way Around UMIP and Descriptor-Table Exiting via TSX-based\n  Side-Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.OS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, in operating systems, numerous protection mechanisms prevent or\nlimit the user-mode applicationsto access the kernels internal information.\nThis is regularlycarried out by software-based defenses such as Address Space\nLayout Randomization (ASLR) and Kernel ASLR(KASLR). They play pronounced roles\nwhen the security of sandboxed applications such as Web-browser are\nconsidered.Armed with arbitrary write access in the kernel memory, if these\nprotections are bypassed, an adversary could find a suitable where to write in\norder to get an elevation of privilege or code execution in ring 0. In this\npaper, we introduce a reliable method based on Transactional Synchronization\nExtensions (TSX) side-channel leakage to reveal the address of the Global\nDescriptor Table (GDT) and Interrupt Descriptor Table (IDT). We indicate that\nby detecting these addresses, one could execute instructions to sidestep the\nIntels User-Mode InstructionPrevention (UMIP) and the Hypervisor-based\nmitigation and, consequently, neutralized them. The introduced method is\nsuccessfully performed after the most recent patches for Meltdown and Spectre.\nMoreover, the implementation of the proposed approach on different platforms,\nincluding the latest releases of Microsoft Windows, Linux, and, Mac OSX with\nthe latest 9th generation of Intel processors, shows that the proposed\nmechanism is independent from the Operating System implementation. We\ndemonstrate that a combinationof this method with call-gate mechanism\n(available in modernprocessors) in a chain of events will eventually lead toa\nsystem compromise despite the limitations of a super-secure sandboxed\nenvironment in the presence of Windows proprietary Virtualization Based\nSecurity (VBS). Finally, we suggest the software-based mitigation to avoid\nthese issues with an acceptable overhead cost.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:54:38 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 08:57:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Karvandi", "Mohammad Sina", ""], ["Monfared", "Saleh Khalaj", ""], ["Kiarostami", "Mohammad Sina", ""], ["Rahmati", "Dara", ""], ["Gorgin", "Saeid", ""]]}, {"id": "2005.10864", "submitter": "Michael Bechtel", "authors": "Michael Bechtel and Heechul Yun", "title": "Memory-Aware Denial-of-Service Attacks on Shared Cache in Multicore\n  Real-Time Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify that memory performance plays a crucial role in\nthe feasibility and effectiveness for performing denial-of-service attacks on\nshared cache. Based on this insight, we introduce new cache DoS attacks, which\ncan be mounted from the user-space and can cause extreme worst-case execution\ntime (WCET) impacts to cross-core victims -- even if the shared cache is\npartitioned -- by taking advantage of the platform's memory address mapping\ninformation and HugePage support. We deploy these enhanced attacks on two\npopular embedded out-of-order multicore platforms using both synthetic and\nreal-world benchmarks. The proposed DoS attacks achieve up to 111X WCET\nincreases on the tested platforms.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:26:35 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 03:16:44 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bechtel", "Michael", ""], ["Yun", "Heechul", ""]]}, {"id": "2005.10866", "submitter": "Saurabh Sinha", "authors": "Saurabh Sinha, Xiaoqing Xu, Mudit Bhargava, Shidhartha Das, Brian\n  Cline and Greg Yeric", "title": "Stack up your chips: Betting on 3D integration to augment Moore's Law\n  scaling", "comments": "5 pages, 6 Figures, invited talk at S3S conference held in San Jose,\n  October 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D integration, i.e., stacking of integrated circuit layers using parallel or\nsequential processing is gaining rapid industry adoption with the slowdown of\nMoore's law scaling. 3D stacking promises potential gains in performance, power\nand cost but the actual magnitude of gains varies depending on end-application,\ntechnology choices and design. In this talk, we will discuss some key\nchallenges associated with 3D design and how design-for-3D will require us to\nbreak traditional silos of micro-architecture, circuit/physical design and\nmanufacturing technology to work across abstractions to enable the gains\npromised by 3D technologies.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:32:33 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Sinha", "Saurabh", ""], ["Xu", "Xiaoqing", ""], ["Bhargava", "Mudit", ""], ["Das", "Shidhartha", ""], ["Cline", "Brian", ""], ["Yeric", "Greg", ""]]}, {"id": "2005.11357", "submitter": "Xuan Guo", "authors": "Xuan Guo, Robert Mullins", "title": "Accelerate Cycle-Level Full-System Simulation of Multi-Core RISC-V\n  Systems with Binary Translation", "comments": "To be published in the Fourth Workshop on Computer Architecture\n  Research with RISC-V", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has always been difficult to balance the accuracy and performance of ISSs.\nRTL simulators or systems such as gem5 are used to execute programs in a\ncycle-accurate manner but are often prohibitively slow. In contrast, functional\nsimulators such as QEMU can run large benchmarks to completion in a reasonable\ntime yet capture few performance metrics and fail to model complex interactions\nbetween multiple cores.\n  This paper presents a novel multi-purpose simulator that exploits binary\ntranslation to offer fast cycle-level full-system simulations. Its functional\nsimulation mode outperforms QEMU and, if desired, it is possible to switch\nbetween functional and timing modes at run-time. Cycle-level simulations of\nRISC-V multi-core processors are possible at more than 20 MIPS, a useful middle\nground in terms of accuracy and performance with simulation speeds nearly 100\ntimes those of more detailed cycle-accurate models.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:24:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Guo", "Xuan", ""], ["Mullins", "Robert", ""]]}, {"id": "2005.12775", "submitter": "Haocong Luo", "authors": "Haocong Luo, Taha Shahroodi, Hasan Hassan, Minesh Patel, Abdullah\n  Giray Yaglikci, Lois Orosa, Jisung Park, Onur Mutlu", "title": "CLR-DRAM: A Low-Cost DRAM Architecture Enabling Dynamic Capacity-Latency\n  Trade-Off", "comments": "This work is to appear at ISCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DRAM is the prevalent main memory technology, but its long access latency can\nlimit the performance of many workloads. Although prior works provide DRAM\ndesigns that reduce DRAM access latency, their reduced storage capacities\nhinder the performance of workloads that need large memory capacity. Because\nthe capacity-latency trade-off is fixed at design time, previous works cannot\nachieve maximum performance under very different and dynamic workload demands.\n  This paper proposes Capacity-Latency-Reconfigurable DRAM (CLR-DRAM), a new\nDRAM architecture that enables dynamic capacity-latency trade-off at low cost.\nCLR-DRAM allows dynamic reconfiguration of any DRAM row to switch between two\noperating modes: 1) max-capacity mode, where every DRAM cell operates\nindividually to achieve approximately the same storage density as a\ndensity-optimized commodity DRAM chip and 2) high-performance mode, where two\nadjacent DRAM cells in a DRAM row and their sense amplifiers are coupled to\noperate as a single low-latency logical cell driven by a single logical sense\namplifier.\n  We implement CLR-DRAM by adding isolation transistors in each DRAM subarray.\nOur evaluations show that CLR-DRAM can improve system performance and DRAM\nenergy consumption by 18.6% and 29.7% on average with four-core multiprogrammed\nworkloads. We believe that CLR-DRAM opens new research directions for a system\nto adapt to the diverse and dynamically changing memory capacity and access\nlatency demands of workloads.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:53:59 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Luo", "Haocong", ""], ["Shahroodi", "Taha", ""], ["Hassan", "Hasan", ""], ["Patel", "Minesh", ""], ["Yaglikci", "Abdullah Giray", ""], ["Orosa", "Lois", ""], ["Park", "Jisung", ""], ["Mutlu", "Onur", ""]]}, {"id": "2005.13121", "submitter": "Jeremie Kim", "authors": "Jeremie S. Kim and Minesh Patel and A. Giray Yaglikci and Hasan Hassan\n  and Roknoddin Azizi and Lois Orosa and Onur Mutlu", "title": "Revisiting RowHammer: An Experimental Analysis of Modern DRAM Devices\n  and Mitigation Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to shed more light on how RowHammer affects modern and future\ndevices at the circuit-level, we first present an experimental characterization\nof RowHammer on 1580 DRAM chips (408x DDR3, 652x DDR4, and 520x LPDDR4) from\n300 DRAM modules (60x DDR3, 110x DDR4, and 130x LPDDR4) with RowHammer\nprotection mechanisms disabled, spanning multiple different technology nodes\nfrom across each of the three major DRAM manufacturers. Our studies\ndefinitively show that newer DRAM chips are more vulnerable to RowHammer: as\ndevice feature size reduces, the number of activations needed to induce a\nRowHammer bit flip also reduces, to as few as 9.6k (4.8k to two rows each) in\nthe most vulnerable chip we tested.\n  We evaluate five state-of-the-art RowHammer mitigation mechanisms using\ncycle-accurate simulation in the context of real data taken from our chips to\nstudy how the mitigation mechanisms scale with chip vulnerability. We find that\nexisting mechanisms either are not scalable or suffer from prohibitively large\nperformance overheads in projected future devices given our observed trends of\nRowHammer vulnerability. Thus, it is critical to research more effective\nsolutions to RowHammer.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:50:07 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 14:12:29 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Kim", "Jeremie S.", ""], ["Patel", "Minesh", ""], ["Yaglikci", "A. Giray", ""], ["Hassan", "Hasan", ""], ["Azizi", "Roknoddin", ""], ["Orosa", "Lois", ""], ["Mutlu", "Onur", ""]]}, {"id": "2005.14691", "submitter": "Stephen Pruett", "authors": "Stephen Pruett and Yale Patt", "title": "Dynamic Merge Point Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite decades of research, conditional branch mispredictions still pose a\nsignificant problem for performance. Moreover, limit studies on infinite size\npredictors show that many of the remaining branches are impossible to predict\nby current strategies. Our work focuses on mitigating performance loss in the\nface of impossible to predict branches. This paper presents a dynamic merge\npoint predictor, which uses instructions fetched on the wrong path of the\nbranch to dynamically detect the merge point. Our predictor locates the merge\npoint with an accuracy of 95%, even when faced with branches whose direction is\nimpossible to predict. Furthermore, we introduce a novel confidence-cost\nsystem, which identifies costly hard-to-predict branches. Our complete system\nreplaces 58% of all branch mispredictions with a correct merge point\nprediction, effectively reducing MPKI by 43%. This result demonstrates the\npotential for dynamic merge point prediction to significantly improve\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:30:00 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pruett", "Stephen", ""], ["Patt", "Yale", ""]]}]