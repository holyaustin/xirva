[{"id": "1509.00040", "submitter": "Kentaro Sano", "authors": "Kentaro Sano", "title": "DSL-based Design Space Exploration for Temporal and Spatial Parallelism\n  of Custom Stream Computing", "comments": "Presented at Second International Workshop on FPGAs for Software\n  Programmers (FSP 2015) (arXiv:1508.06320)", "journal-ref": null, "doi": null, "report-no": "FSP/2015/06", "categories": "cs.AR cs.CE cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream computation is one of the approaches suitable for FPGA-based custom\ncomputing due to its high throughput capability brought by pipelining with\nregular memory access. To increase performance of iterative stream computation,\nwe can exploit both temporal and spatial parallelism by deepening and\nduplicating pipelines, respectively. However, the performance is constrained by\nseveral factors including available hardware resources on FPGA, an external\nmemory bandwidth, and utilization of pipeline stages, and therefore we need to\nfind the best mix of the different parallelism to achieve the highest\nperformance per power. In this paper, we present a domain-specific language\n(DSL) based design space exploration for temporally and/or spatially parallel\nstream computation with FPGA. We define a DSL where we can easily design a\nhierarchical structure of parallel stream computation with abstract description\nof computation. For iterative stream computation of fluid dynamics simulation,\nwe design hardware structures with a different mix of the temporal and spatial\nparallelism. By measuring the performance and the power consumption, we find\nthe best among them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 12:23:57 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Sano", "Kentaro", ""]]}, {"id": "1509.00042", "submitter": "Hayden Kwok-Hay So", "authors": "Cheng Liu, Ho-Cheung Ng, Hayden Kwok-Hay So", "title": "Automatic Nested Loop Acceleration on FPGAs Using Soft CGRA Overlay", "comments": "Presented at Second International Workshop on FPGAs for Software\n  Programmers (FSP 2015) (arXiv:1508.06320)", "journal-ref": null, "doi": null, "report-no": "FSP/2015/03", "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offloading compute intensive nested loops to execute on FPGA accelerators\nhave been demonstrated by numerous researchers as an effective performance\nenhancement technique across numerous application domains. To construct such\naccelerators with high design productivity, researchers have increasingly\nturned to the use of overlay architectures as an intermediate generation target\nbuilt on top of off-the-shelf FPGAs. However, achieving the desired\nperformance-overhead trade-off remains a major productivity challenge as\ncomplex application-specific customizations over a large design space covering\nmultiple architectural parameters are needed.\n  In this work, an automatic nested loop acceleration framework utilizing a\nregular soft coarse-grained reconfigurable array (SCGRA) overlay is presented.\nGiven high-level resource constraints, the framework automatically customizes\nthe overlay architectural design parameters, high-level compilation options as\nwell as communication between the accelerator and the host processor for\noptimized performance specifically to the given application. In our\nexperiments, at a cost of 10 to 20 minutes additional tools run time, the\nproposed customization process resulted in up to 5 times additional speedup\nover a baseline accelerator generated by the same framework without\ncustomization. Overall, when compared to the equivalent software running on the\nhost ARM processor alone on the Zedboard, the resulting accelerators achieved\nup to 10 times speedup.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 11:50:30 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Liu", "Cheng", ""], ["Ng", "Ho-Cheung", ""], ["So", "Hayden Kwok-Hay", ""]]}, {"id": "1509.02308", "submitter": "Xinxin Mei Ms", "authors": "Xinxin Mei and Xiaowen Chu", "title": "Dissecting GPU Memory Hierarchy through Microbenchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory access efficiency is a key factor in fully utilizing the computational\npower of graphics processing units (GPUs). However, many details of the GPU\nmemory hierarchy are not released by GPU vendors. In this paper, we propose a\nnovel fine-grained microbenchmarking approach and apply it to three generations\nof NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously\nunknown characteristics of their memory hierarchies. Specifically, we\ninvestigate the structures of different GPU cache systems, such as the data\ncache, the texture cache and the translation look-aside buffer (TLB). We also\ninvestigate the throughput and access latency of GPU global memory and shared\nmemory. Our microbenchmark results offer a better understanding of the\nmysterious GPU memory hierarchy, which will facilitate the software\noptimization and modelling of GPU architectures. To the best of our knowledge,\nthis is the first study to reveal the cache properties of Kepler and Maxwell\nGPUs, and the superiority of Maxwell in shared memory performance under bank\nconflict.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:13:02 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 13:20:42 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Mei", "Xinxin", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1509.03575", "submitter": "Ananda Kiran", "authors": "Ananda Kiran, Navdeep Prashar", "title": "FPGA Implementation of High Speed Baugh-Wooley Multiplier using\n  Decomposition Logic", "comments": "6 pages, 3 figures, 2 tables, 3 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Baugh-Wooley algorithm is a well-known iterative algorithm for performing\nmultiplication in digital signal processing applications. Decomposition logic\nis used with Baugh-Wooley algorithm to enhance the speed and to reduce the\ncritical path delay. In this paper a high speed multiplier is designed and\nimplemented using decomposition logic and Baugh-Wooley algorithm. The result is\ncompared with booth multiplier. FPGA based architecture is presented and design\nhas been implemented using Xilinx 12.3 device.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 16:20:07 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Kiran", "Ananda", ""], ["Prashar", "Navdeep", ""]]}, {"id": "1509.03721", "submitter": "Mohsen Ghasempour", "authors": "Mohsen Ghasempour, Jim Garside, Aamer Jaleel and Mikel Luj\\'an", "title": "DReAM: Dynamic Re-arrangement of Address Mapping to Improve the\n  Performance of DRAMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The initial location of data in DRAMs is determined and controlled by the\n'address-mapping' and even modern memory controllers use a fixed and\nrun-time-agnostic address mapping. On the other hand, the memory access pattern\nseen at the memory interface level will dynamically change at run-time. This\ndynamic nature of memory access pattern and the fixed behavior of address\nmapping process in DRAM controllers, implied by using a fixed address mapping\nscheme, means that DRAM performance cannot be exploited efficiently. DReAM is a\nnovel hardware technique that can detect a workload-specific address mapping at\nrun-time based on the application access pattern which improves the performance\nof DRAMs. The experimental results show that DReAM outperforms the best\nevaluated address mapping on average by 9%, for mapping-sensitive workloads, by\n2% for mapping-insensitive workloads, and up to 28% across all the workloads.\nDReAM can be seen as an insurance policy capable of detecting which scenarios\nare not well served by the predefined address mapping.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 08:02:39 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Ghasempour", "Mohsen", ""], ["Garside", "Jim", ""], ["Jaleel", "Aamer", ""], ["Luj\u00e1n", "Mikel", ""]]}, {"id": "1509.03740", "submitter": "Mohsen Ghasempour", "authors": "Mohsen Ghasempour, Aamer Jaleel, Jim Garside, and Mikel Luj\\'an", "title": "HAPPY: Hybrid Address-based Page Policy in DRAMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory controllers have used static page closure policies to decide whether a\nrow should be left open, open-page policy, or closed immediately, close-page\npolicy, after the row has been accessed. The appropriate choice for a\nparticular access can reduce the average memory latency. However, since\napplication access patterns change at run time, static page policies cannot\nguarantee to deliver optimum execution time. Hybrid page policies have been\ninvestigated as a means of covering these dynamic scenarios and are now\nimplemented in state-of-the-art processors. Hybrid page policies switch between\nopen-page and close-page policies while the application is running, by\nmonitoring the access pattern of row hits/conflicts and predicting future\nbehavior. Unfortunately, as the size of DRAM memory increases, fine-grain\ntracking and analysis of memory access patterns does not remain practical. We\npropose a compact memory address-based encoding technique which can improve or\nmaintain the performance of DRAMs page closure predictors while reducing the\nhardware overhead in comparison with state-of-the-art techniques. As a case\nstudy, we integrate our technique, HAPPY, with a state-of-the-art monitor, the\nIntel-adaptive open-page policy predictor employed by the Intel Xeon X5650, and\na traditional Hybrid page policy. We evaluate them across 70 memory intensive\nworkload mixes consisting of single-thread and multi-thread applications. The\nexperimental results show that using the HAPPY encoding applied to the\nIntel-adaptive page closure policy can reduce the hardware overhead by 5X for\nthe evaluated 64 GB memory (up to 40X for a 512 GB memory) while maintaining\nthe prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 13:03:04 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Ghasempour", "Mohsen", ""], ["Jaleel", "Aamer", ""], ["Garside", "Jim", ""], ["Luj\u00e1n", "Mikel", ""]]}, {"id": "1509.04240", "submitter": "Neeraj Misra", "authors": "Neeraj Kumar Misra, Mukesh Kumar Kushwaha, Subodh Wairya and Amit\n  Kumar", "title": "Feasible methodology for optimization of a novel reversible binary\n  compressor", "comments": "13 pages, 9 figures", "journal-ref": "International Journal of VLSI design & Communication Systems\n  (VLSICS) Vol.6, No.4, August 2015", "doi": "10.5121/vlsic.2015.6401", "report-no": null, "categories": "cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Now a day reversible logic is an attractive research area due to its low\npower consumption in the area of VLSI circuit design. The reversible logic gate\nis utilized to optimize power consumption by a feature of retrieving input\nlogic from an output logic because of bijective mapping between input and\noutput. In this manuscript, we design 4 2 and 5 2 reversible compressor\ncircuits using a new type of reversible gate. In addition, we propose new gate,\nnamed as inventive0 gate for optimizing a compressor circuit. The utility of\nthe inventive0 gate is that it can be used as full adder and full subtraction\nwith low value of garbage outputs and quantum cost. An algorithm is shown for\ndesigning a compressor structure. The comparative study shows that the proposed\ncompressor structure outperforms the existing ones in terms of garbage outputs,\nnumber of gates and quantum cost. The compressor can reduce the effect of carry\n(Produce from full adder) of the arithmetic frame design. In addition, we\nimplement a basic reversible gate of MOS transistor with less number of MOS\ntransistor count.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 15:53:39 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Misra", "Neeraj Kumar", ""], ["Kushwaha", "Mukesh Kumar", ""], ["Wairya", "Subodh", ""], ["Kumar", "Amit", ""]]}, {"id": "1509.04268", "submitter": "Kota Naga Srinivasarao Batta", "authors": "Batta Kota Naga Srinivasarao and Indrajit Chakrabarti", "title": "High Speed VLSI Architecture for 3-D Discrete Wavelet Transform", "comments": "Submitting to IET CDS. arXiv admin note: substantial text overlap\n  with arXiv:1509.03836", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory efficient, high throughput parallel lifting\nbased running three dimensional discrete wavelet transform (3-D DWT)\narchitecture. 3-D DWT is constructed by combining the two spatial and four\ntemporal processors. Spatial processor (SP) apply the two dimensional DWT on a\nframe, using lifting based 9/7 filter bank through the row rocessor (RP) in row\ndirection and then apply in the colum direction through column processor (CP).\nTo reduce the temporal memory and the latency, the temporal processor (TP) has\nbeen designed with lifting based 1-D Haar wavelet filter. The proposed\narchitecture replaced the multiplications by pipeline shift-add operations to\nreduce the CPD. Two spatial processors works simultaneously on two adjacent\nframes and provide 2-D DWT coefficients as inputs to the temporal processors.\nTPs apply the one dimensional DWT in temporal direction and provide eight 3-D\nDWT coefficients per clock (throughput). Higher throughput reduces the\ncomputing cycles per frame and enable the lower power consumption.\nImplementation results shows that the proposed architecture has the advantage\nin reduced memory, low power consumption, low latency, and high throughput over\nthe existing designs. The RTL of the proposed architecture is described using\nverilog and synthesized using 90-nm technology CMOS standard cell library and\nresults show that it consumes 43.42 mW power and occupies an area equivalent to\n231.45 K equivalent gate at frequency of 200 MHz. The proposed architecture has\nalso been synthesised for the Xilinx zynq 7020 series field programmable gate\narray (FPGA).\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 03:46:00 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Srinivasarao", "Batta Kota Naga", ""], ["Chakrabarti", "Indrajit", ""]]}, {"id": "1509.04618", "submitter": "Neeraj Misra Kumar", "authors": "Neeraj Kumar Misra, Mukesh Kumar Kushwaha, Subodh Wairya, Amit Kumar", "title": "Cost Efficient Design of Reversible Adder Circuits for Low Power\n  Applications", "comments": "9 pages, 12 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A large amount of research is currently going on in the field of reversible\nlogic, which have low heat dissipation, low power consumption, which is the\nmain factor to apply reversible in digital VLSI circuit design. This paper\nintroduces reversible gate named as Inventive0 gate. The novel gate is\nsynthesis the efficient adder modules with minimum garbage output and gate\ncount. The Inventive0 gate capable of implementing a 4-bit ripple carry adder\nand carry skip adders.It is presented that Inventive0 gate is much more\nefficient and optimized approach as compared to their existing design, in terms\nof gate count, garbage outputs and constant inputs. In addition, some popular\navailable reversible gates are implemented in the MOS transistor design the\nimplementation kept in mind for minimum MOS transistor count and are completely\nreversible in behavior more precise forward and backward computation. Lesser\narchitectural complexity show that the novel designs are compact, fast as well\nas low power.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 16:38:57 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Misra", "Neeraj Kumar", ""], ["Kushwaha", "Mukesh Kumar", ""], ["Wairya", "Subodh", ""], ["Kumar", "Amit", ""]]}, {"id": "1509.06891", "submitter": "Rourab Paul", "authors": "Swagata Mandal, Rourab Paul, Suman Sau, Amlan Chakrabarti and Subhasis\n  Chattopadhyay", "title": "A Novel Method for Soft Error Mitigation in FPGA using Adaptive Cross\n  Parity Code", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field Programmable Gate Arrays (FPGAs) are more prone to be affected by\ntransient faults in presence of radiation and other environmental hazards\ncompared to Application Specific Integrated Circuits (ASICs). Hence, error\nmitigation and recovery techniques are absolutely necessary to protect the FPGA\nhardware from soft errors arising due to such transient faults. In this paper,\na new efficient multi-bit error correcting method for FPGAs is proposed using\nadaptive cross parity check (ACPC) code. ACPC is easy to implement and the\nneeded decoding circuit is also simple. In the proposed scheme total\nconfiguration memory is partitioned into two parts. One part will contain ACPC\nhardware, which is static and assumed to be unaffected by any kind of errors.\nOther portion will store the binary file for logic, which is to be protected\nfrom transient error and is assumed to be dynamically reconfigurable (Partial\nreconfigurable area). Binary file from the secondary memory passes through ACPC\nhardware and the bits for forward error correction (FEC) field are calculated\nbefore entering into the reconfigurable portion. In the runtime scenario, the\ndata from the dynamically reconfigurable portion of the configuration memory\nwill be read back and passed through the ACPC hardware. The ACPC hardware will\ncorrect the errors before the data enters into the dynamic configuration\nmemory. We propose a first of its kind methodology for novel transient fault\ncorrection using ACPC code for FPGAs. To validate the design we have tested the\nproposed methodology with Kintex FPGA. We have also measured different\nparameters like critical path, power consumption, overhead resource and error\ncorrection efficiency to estimate the performance of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 09:02:49 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Mandal", "Swagata", ""], ["Paul", "Rourab", ""], ["Sau", "Suman", ""], ["Chakrabarti", "Amlan", ""], ["Chattopadhyay", "Subhasis", ""]]}, {"id": "1509.08111", "submitter": "Wojciech Zabo{\\l}otny PhD", "authors": "Wojciech M. Zabolotny", "title": "Automatic latency balancing in VHDL-implemented complex pipelined\n  systems", "comments": "Updated bibliography. Small language corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing (equalization) of latency in parallel paths in the pipelined data\nprocessing system is an important problem. Without that the data from different\npaths arrive at the processing blocks in different clock cycles, and incorrect\nresults are produced. Manual correction of latencies is a tedious and\nerror-prone work. This paper presents an automatic method of latency\nequalization in systems described in VHDL. The method is based on simulation\nand is portable between different simulation and synthesis tools. The method\ndoes not increase the complexity of the synthesized design comparing to the\nsolution based on manual latency adjustment. The example implementation of the\nproposed methodology together with a simple design demonstrating its use is\navailable as an open source project under BSD license.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 17:32:42 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 15:00:17 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2015 13:28:24 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zabolotny", "Wojciech M.", ""]]}, {"id": "1509.08972", "submitter": "Arash Ardakani", "authors": "Arash Ardakani, Fran\\c{c}ois Leduc-Primeau, Naoya Onizawa, Takahiro\n  Hanyu and Warren J. Gross", "title": "VLSI Implementation of Deep Neural Network Using Integral Stochastic\n  Computing", "comments": "11 pages, 12 figures", "journal-ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems ,\n  vol.PP, no.99, pp.1-12, 2017", "doi": "10.1109/TVLSI.2017.2654298", "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardware implementation of deep neural networks (DNNs) has recently\nreceived tremendous attention: many applications in fact require high-speed\noperations that suit a hardware implementation. However, numerous elements and\ncomplex interconnections are usually required, leading to a large area\noccupation and copious power consumption. Stochastic computing has shown\npromising results for low-power area-efficient hardware implementations, even\nthough existing stochastic algorithms require long streams that cause long\nlatencies. In this paper, we propose an integer form of stochastic computation\nand introduce some elementary circuits. We then propose an efficient\nimplementation of a DNN based on integral stochastic computing. The proposed\narchitecture has been implemented on a Virtex7 FPGA, resulting in 45% and 62%\naverage reductions in area and latency compared to the best reported\narchitecture in literature. We also synthesize the circuits in a 65 nm CMOS\ntechnology and we show that the proposed integral stochastic architecture\nresults in up to 21% reduction in energy consumption compared to the binary\nradix implementation at the same misclassification rate. Due to fault-tolerant\nnature of stochastic architectures, we also consider a quasi-synchronous\nimplementation which yields 33% reduction in energy consumption w.r.t. the\nbinary radix implementation without any compromise on performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 23:16:18 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 18:30:55 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Ardakani", "Arash", ""], ["Leduc-Primeau", "Fran\u00e7ois", ""], ["Onizawa", "Naoya", ""], ["Hanyu", "Takahiro", ""], ["Gross", "Warren J.", ""]]}, {"id": "1509.09249", "submitter": "Mark Zwolinski", "authors": "Massoud Mokhtarpour Ghahroodi and Mark Zwolinski", "title": "In-Field Logic Repair of Deep Sub-Micron CMOS Processors", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.2435.6566", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra Deep-Sub-Micron CMOS chips have to function correctly and reliably, not\nonly during their early post-fabrication life, but also for their entire life\nspan. In this paper, we present an architectural-level in-field repair\ntechnique. The key idea is to trade area for reliability by adding repair\nfeatures to the system while keeping the power and the performance overheads as\nlow as possible. In the case of permanent faults, spare blocks will replace the\nfaulty blocks on the fly. Meanwhile by shutting down the main logic blocks,\npartial threshold voltage recovery can be achieved which will alleviate the\nageing-related delays and timing issues. The technique can avoid fatal\nshut-downs in the system and will decrease the down-time, hence the\navailability of such a system will be preserved. We have implemented the\nproposed idea on a pipelined processor core using a conventional ASIC design\nflow. The simulation results show that by tolerating about 70% area overhead\nand less than 18% power overhead we can dramatically increase the reliability\nand decrease the downtime of the processor.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:36:32 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Ghahroodi", "Massoud Mokhtarpour", ""], ["Zwolinski", "Mark", ""]]}]