[{"id": "2009.00202", "submitter": "Karthik Sankaranarayanan", "authors": "Karthik Sankaranarayanan, Chit-Kwan Lin and Gautham Chinya", "title": "Helper Without Threads: Customized Prefetching for Delinquent Irregular\n  Loads", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing memory footprints of cloud and big data applications mean that\ndata center CPUs can spend significant time waiting for memory. An attractive\napproach to improving performance in such centralized compute settings is to\nemploy prefetchers that are customized per application, where gains can be\neasily scaled across thousands of machines. Helper thread prefetching is such a\ntechnique but has yet to achieve wide adoption since it requires spare thread\ncontexts or special hardware/firmware support. In this paper, we propose an\ninline software prefetching technique that overcomes these restrictions by\ninserting the helper code into the main thread itself. Our approach is\ncomplementary to and does not interfere with existing hardware prefetchers\nsince we target only delinquent irregular load instructions (those with no\nconstant or striding address patterns). For each chosen load instruction, we\ngenerate and insert a customized software prefetcher extracted from and\nmimicking the application's dataflow, all without access to the application\nsource code. For a set of irregular workloads that are memory-bound, we\ndemonstrate up to 2X single-thread performance improvement on recent high-end\nhardware (Intel Skylake) and up to 83% speedup over a helper thread\nimplementation on the same hardware, due to the absence of thread spawning\noverhead.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 03:22:14 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Sankaranarayanan", "Karthik", ""], ["Lin", "Chit-Kwan", ""], ["Chinya", "Gautham", ""]]}, {"id": "2009.00223", "submitter": "Mitul Nagar", "authors": "Mitul S Nagar, Haresh A Suthar, Chintan Panchal", "title": "RISC micrprocessor verification", "comments": "4 pages, 7 images ans a table. ISBN No: 978-81-906220-3 -5", "journal-ref": "International conference on Knowledge analysis ans research in\n  engineering technology and science 2012", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's microprocessors have grown significantly in complexity and\nfunctionality. Most of today's processors provide at least three levels of\nmemory hierarchy, are heavily pipelined, and support some sort of cache\ncoherency protocol. These features are extremely complex and sophisticated, and\npresent their own set of unique verification challenges. Verification is\nclearly not a point tool, but is part of a process that starts from initial\nproduct conception and is to some degrees complete when the product goes to\nmarket. Functional verification is necessary to verify the functionality at RTL\nlevel. Complex micro-processors like ARM are high performance, low cost and low\npower 32-bit RISC processors. In our paper complex microprocessor is ARM cortex\nM3, developed for the embedded applications having low interrupt latency, low\ngate count, 3- stage pipelining, branch prediction, THUMB and THUMB-2\ninstruction set. Functional verification is used to verify that the circuit\nfull fills each abstract assertion under the implementation mapping. we explore\nseveral aspects of processor design, including caches, pipeline depth, ALUs,\nand bypass logic.The verification was done concurrently with the design\nimplementation of the processor.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 04:48:33 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nagar", "Mitul S", ""], ["Suthar", "Haresh A", ""], ["Panchal", "Chintan", ""]]}, {"id": "2009.00448", "submitter": "Michael Frank", "authors": "Michael P. Frank, Robert W. Brocato, Brian D. Tierney, Nancy A.\n  Missert, Alexander H. Hsia", "title": "Reversible Computing with Fast, Fully Static, Fully Adiabatic CMOS", "comments": "8 pages, 9 figures, submitted to the IEEE International Conference on\n  Rebooting Computing (ICRC 2020)", "journal-ref": null, "doi": null, "report-no": "SAND2020-9040 O", "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To advance the energy efficiency of general digital computing far beyond the\nthermodynamic limits that apply to conventional digital circuits will require\nutilizing the principles of reversible computing. It has been known since the\nearly 1990s that reversible computing based on adiabatic switching is possible\nin CMOS, although almost all of the \"adiabatic\" CMOS logic families in the\nliterature are not actually fully adiabatic, which limits their achievable\nenergy savings. The first CMOS logic style that achieved truly, fully adiabatic\noperation if leakage was negligible (CRL) is not fully static, which leads to a\nnumber of practical engineering difficulties in the presence of certain\nnonidealities. Later, \"static\" adiabatic logic families were described, but\nthey were not actually fully adiabatic, or fully static, and were much slower.\n  In this paper, we describe a new logic family, Static 2-Level Adiabatic Logic\n(S2LAL), which is, to our knowledge, the first CMOS logic family that is both\nfully static, and truly, fully adiabatic (modulo leakage). In addition, S2LAL\nis, we think, the fastest possible such family (among fully pipelined\nsequential circuits), having a latency per logic stage of one \"tick\"\n(transition time), and a minimum clock period (initiation interval) of 8 ticks.\nS2LAL requires 8 phases of a trapezoidal power-clock waveform (plus constant\npower and ground references) to be supplied. We argue that, if implemented in a\nsuitable fabrication process designed to aggressively minimize leakage, S2LAL\nshould be capable of demonstrating a greater level of energy efficiency than\nany other semiconductor-based digital logic family known today.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:47:42 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 15:47:26 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Frank", "Michael P.", ""], ["Brocato", "Robert W.", ""], ["Tierney", "Brian D.", ""], ["Missert", "Nancy A.", ""], ["Hsia", "Alexander H.", ""]]}, {"id": "2009.00457", "submitter": "Harideep Nair", "authors": "Harideep Nair, John Paul Shen, James E. Smith", "title": "Direct CMOS Implementation of Neuromorphic Temporal Neural Networks for\n  Sensory Processing", "comments": "Submission Under Review for an IEEE Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Neural Networks (TNNs) use time as a resource to represent and\nprocess information, mimicking the behavior of the mammalian neocortex. This\nwork focuses on implementing TNNs using off-the-shelf digital CMOS technology.\nA microarchitecture framework is introduced with a hierarchy of building blocks\nincluding: multi-neuron columns, multi-column layers, and multi-layer TNNs. We\npresent the direct CMOS gate-level implementation of the multi-neuron column\nmodel as the key building block for TNNs. Post-synthesis results are obtained\nusing Synopsys tools and the 45 nm CMOS standard cell library. The TNN\nmicroarchitecture framework is embodied in a set of characteristic equations\nfor assessing the total gate count, die area, compute time, and power\nconsumption for any TNN design. We develop a multi-layer TNN prototype of 32M\ngates. In 7 nm CMOS process, it consumes only 1.54 mm^2 die area and 7.26 mW\npower and can process 28x28 images at 107M FPS (9.34 ns per image). We evaluate\nthe prototype's performance and complexity relative to a recent\nstate-of-the-art TNN model.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 20:36:34 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nair", "Harideep", ""], ["Shen", "John Paul", ""], ["Smith", "James E.", ""]]}, {"id": "2009.00637", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Building Application-Specific Overlays on FPGAs with High-Level\n  Customizable IPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlays are virtual, re-configurable architectures that overlay on top of\nphysical FPGA fabrics. An overlay that is specialized for an application, or a\nclass of applications, offers both fast reconfiguration and minimized\nperformance penalty. Such an overlay is usually implemented by hardware\ndesigners in hardware \"assembly\" languages at register-transfer level (RTL).\n  This short article proposes an idea for a software programmer, instead of\nhardware designers, to quickly implement an application-specific overlay using\nhigh-level customizable IPs. These IPs are expressed succinctly by a\nspecification language, whose abstraction level is much higher than RTL but can\nnonetheless expresses many performance-critical loop and data optimizations on\nFPGAs, and thus would offer competitively high performance at a much lower cost\nof maintenance and much easier customizations.\n  We propose new language features to easily put the IPs together into an\noverlay. A compiler automatically implements the specified optimizations to\ngenerate an efficient overlay, exposes a multi-tasking programming interface\nfor the overlay, and inserts a runtime scheduler for scheduling tasks to run on\nthe IPs of the overlay, respecting the dependences between the tasks. While an\napplication written in any language can take advantage of the overlay through\nthe programming interface, we show a particular usage scenario, where the\napplication itself is also succinctly specified in the same language.\n  We describe the new language features for expressing overlays, and illustrate\nthe features with an LU decomposer and a convolutional neural network. A system\nis under construction to implement the language features and workloads.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 18:08:24 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "2009.00715", "submitter": "Fateme Golshan", "authors": "Mohammad Bakhshalipour, Mehran Shakerinava, Fatemeh Golshan, Ali\n  Ansari, Pejman Lotfi-Karman, Hamid Sarbazi-Azad", "title": "A Survey on Recent Hardware Data Prefetching Approaches with An Emphasis\n  on Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data prefetching, i.e., the act of predicting application's future memory\naccesses and fetching those that are not in the on-chip caches, is a well-known\nand widely-used approach to hide the long latency of memory accesses. The\nfruitfulness of data prefetching is evident to both industry and academy:\nnowadays, almost every high-performance processor incorporates a few data\nprefetchers for capturing various access patterns of applications; besides,\nthere is a myriad of proposals for data prefetching in the research literature,\nwhere each proposal enhances the efficiency of prefetching in a specific way.\nIn this survey, we discuss the fundamental concepts in data prefetching and\nstudy state-of-the-art hardware data prefetching approaches. Additional Key\nWords and Phrases: Data Prefetching, Scale-Out Workloads, Server Processors,\nand Spatio-Temporal Correlation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 21:16:51 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Bakhshalipour", "Mohammad", ""], ["Shakerinava", "Mehran", ""], ["Golshan", "Fatemeh", ""], ["Ansari", "Ali", ""], ["Lotfi-Karman", "Pejman", ""], ["Sarbazi-Azad", "Hamid", ""]]}, {"id": "2009.00748", "submitter": "Ali Hadi Zadeh", "authors": "Mostafa Mahmoud, Isak Edo, Ali Hadi Zadeh, Omar Mohamed Awad, Gennady\n  Pekhimenko, Jorge Albericio, and Andreas Moshovos", "title": "TensorDash: Exploiting Sparsity to Accelerate Deep Neural Network\n  Training and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorDash is a hardware level technique for enabling data-parallel MAC units\nto take advantage of sparsity in their input operand streams. When used to\ncompose a hardware accelerator for deep learning, TensorDash can speedup the\ntraining process while also increasing energy efficiency. TensorDash combines a\nlow-cost, sparse input operand interconnect comprising an 8-input multiplexer\nper multiplier input, with an area-efficient hardware scheduler. While the\ninterconnect allows a very limited set of movements per operand, the scheduler\ncan effectively extract sparsity when it is present in the activations, weights\nor gradients of neural networks. Over a wide set of models covering various\napplications, TensorDash accelerates the training process by $1.95{\\times}$\nwhile being $1.89\\times$ more energy-efficient, $1.6\\times$ more energy\nefficient when taking on-chip and off-chip memory accesses into account. While\nTensorDash works with any datatype, we demonstrate it with both\nsingle-precision floating-point units and bfloat16.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 23:39:35 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mahmoud", "Mostafa", ""], ["Edo", "Isak", ""], ["Zadeh", "Ali Hadi", ""], ["Awad", "Omar Mohamed", ""], ["Pekhimenko", "Gennady", ""], ["Albericio", "Jorge", ""], ["Moshovos", "Andreas", ""]]}, {"id": "2009.00804", "submitter": "Zhihui Zhang", "authors": "Zhihui Zhang, Jingwen Leng, Lingxiao Ma, Youshan Miao, Chao Li, Minyi\n  Guo", "title": "Architectural Implications of Graph Neural Networks", "comments": "4 pages, published in IEEE Computer Architecture Letters (CAL) 2020", "journal-ref": "in IEEE Computer Architecture Letters, vol. 19, no. 1, pp. 59-62,\n  1 Jan.-June 2020", "doi": "10.1109/LCA.2020.2988991", "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) represent an emerging line of deep learning\nmodels that operate on graph structures. It is becoming more and more popular\ndue to its high accuracy achieved in many graph-related tasks. However, GNN is\nnot as well understood in the system and architecture community as its\ncounterparts such as multi-layer perceptrons and convolutional neural networks.\nThis work tries to introduce the GNN to our community. In contrast to prior\nwork that only presents characterizations of GCNs, our work covers a large\nportion of the varieties for GNN workloads based on a general GNN description\nframework. By constructing the models on top of two widely-used libraries, we\ncharacterize the GNN computation at inference stage concerning general-purpose\nand application-specific architectures and hope our work can foster more system\nand architecture research for GNNs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 03:36:24 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zhang", "Zhihui", ""], ["Leng", "Jingwen", ""], ["Ma", "Lingxiao", ""], ["Miao", "Youshan", ""], ["Li", "Chao", ""], ["Guo", "Minyi", ""]]}, {"id": "2009.00871", "submitter": "Zhe Lin", "authors": "Zhe Lin, Jieru Zhao, Sharad Sinha, and Wei Zhang", "title": "HL-Pow: A Learning-Based Power Modeling Framework for High-Level\n  Synthesis", "comments": "published as a conference paper in ASP-DAC 2020", "journal-ref": null, "doi": "10.1109/ASP-DAC47756.2020.9045442", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level synthesis (HLS) enables designers to customize hardware designs\nefficiently. However, it is still challenging to foresee the correlation\nbetween power consumption and HLS-based applications at an early design stage.\nTo overcome this problem, we introduce HL-Pow, a power modeling framework for\nFPGA HLS based on state-of-the-art machine learning techniques. HL-Pow\nincorporates an automated feature construction flow to efficiently identify and\nextract features that exert a major influence on power consumption, simply\nbased upon HLS results, and a modeling flow that can build an accurate and\ngeneric power model applicable to a variety of designs with HLS. By using\nHL-Pow, the power evaluation process for FPGA designs can be significantly\nexpedited because the power inference of HL-Pow is established on HLS instead\nof the time-consuming register-transfer level (RTL) implementation flow.\nExperimental results demonstrate that HL-Pow can achieve accurate power\nmodeling that is only 4.67% (24.02 mW) away from onboard power measurement. To\nfurther facilitate power-oriented optimizations, we describe a novel design\nspace exploration (DSE) algorithm built on top of HL-Pow to trade off between\nlatency and power consumption. This algorithm can reach a close approximation\nof the real Pareto frontier while only requiring running HLS flow for 20% of\ndesign points in the entire design space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 07:45:03 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Lin", "Zhe", ""], ["Zhao", "Jieru", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.00881", "submitter": "Debjyoti Bhattacharjee", "authors": "Debjyoti Bhattacharjee and Anupam Chattopadhyay and Srijit Dutta and\n  Ronny Ronen and Shahar Kvatinsky", "title": "CONTRA: Area-Constrained Technology Mapping Framework For Memristive\n  Memory Processing Unit", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive applications are poised to benefit directly from\nprocessing-in-memory platforms, such as memristive Memory Processing Units,\nwhich allow leveraging data locality and performing stateful logic operations.\nDeveloping design automation flows for such platforms is a challenging and\nhighly relevant research problem. In this work, we investigate the problem of\nminimizing delay under arbitrary area constraint for MAGIC-based in-memory\ncomputing platforms. We propose an end-to-end area constrained technology\nmapping framework, CONTRA. CONTRA uses Look-Up Table(LUT) based mapping of the\ninput function on the crossbar array to maximize parallel operations and uses a\nnovel search technique to move data optimally inside the array. CONTRA supports\nbenchmarks in a variety of formats, along with crossbar dimensions as input to\ngenerate MAGIC instructions. CONTRA scales for large benchmarks, as\ndemonstrated by our experiments. CONTRA allows mapping benchmarks to smaller\ncrossbar dimensions than achieved by any other technique before, while allowing\na wide variety of area-delay trade-offs. CONTRA improves the composite metric\nof area-delay product by 2.1x to 13.1x compared to seven existing technology\nmapping approaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:12:58 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Bhattacharjee", "Debjyoti", ""], ["Chattopadhyay", "Anupam", ""], ["Dutta", "Srijit", ""], ["Ronen", "Ronny", ""], ["Kvatinsky", "Shahar", ""]]}, {"id": "2009.01178", "submitter": "Paolo Mantovani", "authors": "Paolo Mantovani, Davide Giri, Giuseppe Di Guglielmo, Luca Piccolboni,\n  Joseph Zuckerman, Emilio G. Cota, Michele Petracca, Christian Pilato and Luca\n  P. Carloni", "title": "Agile SoC Development with Open ESP", "comments": "Invited Paper at the 2020 International Conference On Computer Aided\n  Design (ICCAD) - Special Session on Opensource Tools and Platforms for Agile\n  Development of Specialized Architectures", "journal-ref": null, "doi": "10.1145/3400302.3415753", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ESP is an open-source research platform for heterogeneous SoC design. The\nplatform combines a modular tile-based architecture with a variety of\napplication-oriented flows for the design and optimization of accelerators. The\nESP architecture is highly scalable and strikes a balance between regularity\nand specialization. The companion methodology raises the level of abstraction\nto system-level design and enables an automated flow from software and hardware\ndevelopment to full-system prototyping on FPGA. For application developers, ESP\noffers domain-specific automated solutions to synthesize new accelerators for\ntheir software and to map complex workloads onto the SoC architecture. For\nhardware engineers, ESP offers automated solutions to integrate their\naccelerator designs into the complete SoC. Conceived as a heterogeneous\nintegration platform and tested through years of teaching at Columbia\nUniversity, ESP supports the open-source hardware community by providing a\nflexible platform for agile SoC development.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:38:48 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Mantovani", "Paolo", ""], ["Giri", "Davide", ""], ["Di Guglielmo", "Giuseppe", ""], ["Piccolboni", "Luca", ""], ["Zuckerman", "Joseph", ""], ["Cota", "Emilio G.", ""], ["Petracca", "Michele", ""], ["Pilato", "Christian", ""], ["Carloni", "Luca P.", ""]]}, {"id": "2009.01432", "submitter": "Zhe Lin", "authors": "Zhe Lin, Sharad Sinha, Wei Zhang", "title": "An Ensemble Learning Approach for In-situ Monitoring of FPGA Dynamic\n  Power", "comments": "published as a journal (TCAD) paper in 2018", "journal-ref": null, "doi": "10.1109/TCAD.2018.2859248", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As field-programmable gate arrays become prevalent in critical application\ndomains, their power consumption is of high concern. In this paper, we present\nand evaluate a power monitoring scheme capable of accurately estimating the\nruntime dynamic power of FPGAs in a fine-grained timescale, in order to support\nemerging power management techniques. In particular, we describe a novel and\nspecialized ensemble model which can be decomposed into multiple customized\ndecision-tree-based base learners. To aid in model synthesis, a generic\ncomputer-aided design flow is proposed to generate samples, select features,\ntune hyperparameters and train the ensemble estimator. Besides this, a hardware\nrealization of the trained ensemble estimator is presented for on-chip\nreal-time power estimation. In the experiments, we first show that a single\ndecision tree model can achieve prediction error within 4.51% of a commercial\ngate-level power estimation tool, which is 2.41--6.07x lower than provided by\nthe commonly used linear model. More importantly, we study the extra gains in\ninference accuracy using the proposed ensemble model. Experimental results\nreveal that the ensemble monitoring method can further improve the accuracy of\npower predictions to within a maximum error of 1.90%. Moreover, the lookup\ntable (LUT) overhead of the ensemble monitoring hardware employing up to 64\nbase learners is within 1.22% of the target FPGA, indicating its light-weight\nand scalable characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:39:14 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lin", "Zhe", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.01434", "submitter": "Zhe Lin", "authors": "Zhe Lin, Wei Zhang, Sharad Sinha", "title": "Decision Tree Based Hardware Power Monitoring for Run Time Dynamic Power\n  Management in FPGA", "comments": "published as a conference paper in FPL 2017", "journal-ref": null, "doi": "10.23919/FPL.2017.8056832", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained runtime power management techniques could be promising solutions\nfor power reduction. Therefore, it is essential to establish accurate power\nmonitoring schemes to obtain dynamic power variation in a short period (i.e.,\ntens or hundreds of clock cycles). In this paper, we leverage a\ndecision-tree-based power modeling approach to establish fine-grained hardware\npower monitoring on FPGA platforms. A generic and complete design flow is\ndeveloped to implement the decision tree power model which is capable of\nprecisely estimating dynamic power in a fine-grained manner. A flexible\narchitecture of the hardware power monitoring is proposed, which can be\ninstrumented in any RTL design for runtime power estimation, dispensing with\nthe need for extra power measurement devices. Experimental results of applying\nthe proposed model to benchmarks with different resource types reveal an\naverage error up to 4% for dynamic power estimation. Moreover, the overheads of\narea, power and performance incurred by the power monitoring circuitry are\nextremely low. Finally, we apply our power monitoring technique to the power\nmanagement using phase shedding with an on-chip multi-phase regulator as a\nproof of concept and the results demonstrate 14% efficiency enhancement for the\npower supply of the FPGA internal logic.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:46:12 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lin", "Zhe", ""], ["Zhang", "Wei", ""], ["Sinha", "Sharad", ""]]}, {"id": "2009.01441", "submitter": "Zhe Lin", "authors": "Zhe Lin, Sharad Sinha, Hao Liang, Liang Feng, Wei Zhang", "title": "Scalable Light-Weight Integration of FPGA Based Accelerators with Chip\n  Multi-Processors", "comments": "published as a journal (TMSCS) paper in 2018", "journal-ref": null, "doi": "10.1109/TMSCS.2017.2754378", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multicore systems are migrating from homogeneous systems to\nheterogeneous systems with accelerator-based computing in order to overcome the\nbarriers of performance and power walls. In this trend, FPGA-based accelerators\nare becoming increasingly attractive, due to their excellent flexibility and\nlow design cost. In this paper, we propose the architectural support for\nefficient interfacing between FPGA-based multi-accelerators and\nchip-multiprocessors (CMPs) connected through the network-on-chip (NoC).\nDistributed packet receivers and hierarchical packet senders are designed to\nmaintain scalability and reduce the critical path delay under a heavy task\nload. A dedicated accelerator chaining mechanism is also proposed to facilitate\nintra-FPGA data reuse among accelerators to circumvent prohibitive\ncommunication overhead between the FPGA and processors. In order to evaluate\nthe proposed architecture, a complete system emulation with programmability\nsupport is performed using FPGA prototyping. Experimental results demonstrate\nthat the proposed architecture has high-performance, and is light-weight and\nscalable in characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 04:02:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lin", "Zhe", ""], ["Sinha", "Sharad", ""], ["Liang", "Hao", ""], ["Feng", "Liang", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.01588", "submitter": "Duy Thanh Nguyen", "authors": "Duy Thanh Nguyen, Hyun Kim, and Hyuk-Jae Lee", "title": "Layer-specific Optimization for Mixed Data Flow with Mixed Precision in\n  FPGA Design for CNN-based Object Detectors", "comments": "Accepted for publication in IEEE Transaction on Circuit and System\n  for Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3020569", "report-no": null, "categories": "cs.CV cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) require both intensive computation and\nfrequent memory access, which lead to a low processing speed and large power\ndissipation. Although the characteristics of the different layers in a CNN are\nfrequently quite different, previous hardware designs have employed common\noptimization schemes for them. This paper proposes a layer-specific design that\nemploys different organizations that are optimized for the different layers.\nThe proposed design employs two layer-specific optimizations: layer-specific\nmixed data flow and layer-specific mixed precision. The mixed data flow aims to\nminimize the off-chip access while demanding a minimal on-chip memory (BRAM)\nresource of an FPGA device. The mixed precision quantization is to achieve both\na lossless accuracy and an aggressive model compression, thereby further\nreducing the off-chip access. A Bayesian optimization approach is used to\nselect the best sparsity for each layer, achieving the best trade-off between\nthe accuracy and compression. This mixing scheme allows the entire network\nmodel to be stored in BRAMs of the FPGA to aggressively reduce the off-chip\naccess, and thereby achieves a significant performance enhancement. The model\nsize is reduced by 22.66-28.93 times compared to that in a full-precision\nnetwork with a negligible degradation of accuracy on VOC, COCO, and ImageNet\ndatasets. Furthermore, the combination of mixed dataflow and mixed precision\nsignificantly outperforms the previous works in terms of both throughput,\noff-chip access, and on-chip memory requirement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 11:27:40 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Nguyen", "Duy Thanh", ""], ["Kim", "Hyun", ""], ["Lee", "Hyuk-Jae", ""]]}, {"id": "2009.01982", "submitter": "Casey Duckering", "authors": "Casey Duckering, Jonathan M. Baker, David I. Schuster, Frederic T.\n  Chong", "title": "Virtualized Logical Qubits: A 2.5D Architecture for Error-Corrected\n  Quantum Computing", "comments": "12 pages, 13 figures, In MICRO '20: 53rd IEEE/ACM International\n  Symposium on Microarchitecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current, near-term quantum devices have shown great progress in recent years\nculminating with a demonstration of quantum supremacy. In the medium-term,\nhowever, quantum machines will need to transition to greater reliability\nthrough error correction, likely through promising techniques such as surface\ncodes which are well suited for near-term devices with limited qubit\nconnectivity. We discover quantum memory, particularly resonant cavities with\ntransmon qubits arranged in a 2.5D architecture, can efficiently implement\nsurface codes with substantial hardware savings and performance/fidelity gains.\nSpecifically, we *virtualize logical qubits* by storing them in layers\ndistributed across qubit memories connected to each transmon.\n  Surprisingly, distributing each logical qubit across many memories has a\nminimal impact on fault tolerance and results in substantially more efficient\noperations. Our design permits fast transversal CNOT operations between logical\nqubits sharing the same physical address which are 6x faster than lattice\nsurgery CNOTs. We develop a novel embedding which saves ~10x in transmons with\nanother 2x from an additional optimization for compactness.\n  Although Virtualized Logical Qubits (VLQ) pays a 10x penalty in\nserialization, advantages in the transversal CNOT and area efficiency result in\nperformance comparable to 2D transmon-only architectures. Our simulations show\nfault tolerance comparable to 2D architectures while saving substantial\nhardware. Furthermore, VLQ can produce magic states 1.22x faster for a fixed\nnumber of transmon qubits. This is a critical benchmark for future\nfault-tolerant quantum computers. VLQ substantially reduces the hardware\nrequirements for fault tolerance and puts within reach a proof-of-concept\nexperimental demonstration of around 10 logical qubits, requiring only 11\ntransmons and 9 attached cavities in total.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 02:17:47 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Duckering", "Casey", ""], ["Baker", "Jonathan M.", ""], ["Schuster", "David I.", ""], ["Chong", "Frederic T.", ""]]}, {"id": "2009.02010", "submitter": "Sheng-Chun Kao", "authors": "Sheng-Chun Kao, Geonhwa Jeong, Tushar Krishna", "title": "ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators\n  using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN accelerators provide efficiency by leveraging reuse of\nactivations/weights/outputs during the DNN computations to reduce data movement\nfrom DRAM to the chip. The reuse is captured by the accelerator's dataflow.\nWhile there has been significant prior work in exploring and comparing various\ndataflows, the strategy for assigning on-chip hardware resources (i.e., compute\nand memory) given a dataflow that can optimize for performance/energy while\nmeeting platform constraints of area/power for DNN(s) of interest is still\nrelatively unexplored. The design-space of choices for balancing compute and\nmemory explodes combinatorially, as we show in this work (e.g., as large as\nO(10^(72)) choices for running \\mobilenet), making it infeasible to do\nmanual-tuning via exhaustive searches. It is also difficult to come up with a\nspecific heuristic given that different DNNs and layer types exhibit different\namounts of reuse.\n  In this paper, we propose an autonomous strategy called ConfuciuX to find\noptimized HW resource assignments for a given model and dataflow style.\nConfuciuX leverages a reinforcement learning method, REINFORCE, to guide the\nsearch process, leveraging a detailed HW performance cost model within the\ntraining loop to estimate rewards. We also augment the RL approach with a\ngenetic algorithm for further fine-tuning. ConfuciuX demonstrates the highest\nsample-efficiency for training compared to other techniques such as Bayesian\noptimization, genetic algorithm, simulated annealing, and other RL methods. It\nconverges to the optimized hardware configuration 4.7 to 24 times faster than\nalternate techniques.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 04:59:26 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Kao", "Sheng-Chun", ""], ["Jeong", "Geonhwa", ""], ["Krishna", "Tushar", ""]]}, {"id": "2009.02326", "submitter": "Mojan Javaheripi", "authors": "Mojan Javaheripi, Mohammad Samragh, Gregory Fields, Tara Javidi,\n  Farinaz Koushanfar", "title": "CLEANN: Accelerated Trojan Shield for Embedded Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3400302.3415671", "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CLEANN, the first end-to-end framework that enables online\nmitigation of Trojans for embedded Deep Neural Network (DNN) applications. A\nTrojan attack works by injecting a backdoor in the DNN while training; during\ninference, the Trojan can be activated by the specific backdoor trigger. What\ndifferentiates CLEANN from the prior work is its lightweight methodology which\nrecovers the ground-truth class of Trojan samples without the need for labeled\ndata, model retraining, or prior assumptions on the trigger or the attack. We\nleverage dictionary learning and sparse approximation to characterize the\nstatistical behavior of benign data and identify Trojan triggers. CLEANN is\ndevised based on algorithm/hardware co-design and is equipped with specialized\nhardware to enable efficient real-time execution on resource-constrained\nembedded platforms. Proof of concept evaluations on CLEANN for the\nstate-of-the-art Neural Trojan attacks on visual benchmarks demonstrate its\ncompetitive advantage in terms of attack resiliency and execution overhead.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 05:29:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Javaheripi", "Mojan", ""], ["Samragh", "Mohammad", ""], ["Fields", "Gregory", ""], ["Javidi", "Tara", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "2009.02381", "submitter": "Zhi-Gang Liu", "authors": "Zhi-Gang Liu, Paul N. Whatmough, and Matthew Mattina", "title": "Sparse Systolic Tensor Array for Efficient CNN Hardware Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) inference on mobile devices demands\nefficient hardware acceleration of low-precision (INT8) general matrix\nmultiplication (GEMM). Exploiting data sparsity is a common approach to further\naccelerate GEMM for CNN inference, and in particular, structural sparsity has\nthe advantages of predictable load balancing and very low index overhead. In\nthis paper, we address a key architectural challenge with structural sparsity:\nhow to provide support for a range of sparsity levels while maintaining high\nutilization of the hardware. We describe a time unrolled formulation of\nvariable density-bound block (VDBB) sparsity that allows for a configurable\nnumber of non-zero elements per block, at constant utilization. We then\ndescribe a systolic array microarchitecture that implements this scheme, with\ntwo data reuse optimizations. Firstly, we increase reuse in both operands and\npartial products by increasing the number of MACs per PE. Secondly, we\nintroduce a novel approach of moving the IM2COL transform into the hardware,\nwhich allows us to achieve a 3x data bandwidth expansion just before the\noperands are consumed by the datapath, reducing the SRAM power consumption. The\noptimizations for weight sparsity, activation sparsity and data reuse are all\ninterrelated and therefore the optimal combination is not obvious. Therefore,\nwe perform an design space evaluation to find the pareto-optimal design\ncharacteristics. The resulting design achieves 16.8 TOPS/W in 16nm with modest\n50% model sparsity and scales with model sparsity up to 55.7TOPS/W at 87.5%. As\nwell as successfully demonstrating the variable DBB technique, this result\nsignificantly outperforms previously reported sparse CNN accelerators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 20:17:42 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 21:43:36 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Liu", "Zhi-Gang", ""], ["Whatmough", "Paul N.", ""], ["Mattina", "Matthew", ""]]}, {"id": "2009.02412", "submitter": "Johann Knechtel", "authors": "Mohammed Nabeel, Mohammed Ashraf, Satwik Patnaik, Vassos Soteriou,\n  Ozgur Sinanoglu, Johann Knechtel", "title": "2.5D Root of Trust: Secure System-Level Integration of Untrusted\n  Chiplets", "comments": "[v2] Dedicated, after acceptance and publication, in memory of the\n  late Vassos Soteriou. Besides, scaled down some figures for smaller overall\n  file size", "journal-ref": null, "doi": "10.1109/TC.2020.3020777", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dedicated, after acceptance and publication, in memory of the late Vassos\nSoteriou. For the first time, we leverage the 2.5D interposer technology to\nestablish system-level security in the face of hardware- and software-centric\nadversaries. More specifically, we integrate chiplets (i.e., third-party hard\nintellectual property of complex functionality, like microprocessors) using a\nsecurity-enforcing interposer. Such hardware organization provides a robust\n2.5D root of trust for trustworthy, yet powerful and flexible, computation\nsystems. The security paradigms for our scheme, employed firmly by design and\nconstruction, are: 1) stringent physical separation of trusted from untrusted\ncomponents, and 2) runtime monitoring. The system-level activities of all\nuntrusted commodity chiplets are checked continuously against security policies\nvia physically separated security features. Aside from the security promises,\nthe good economics of outsourced supply chains are still maintained; the system\nvendor is free to procure chiplets from the open market, while only producing\nthe interposer and assembling the 2.5D system oneself. We showcase our scheme\nusing the Cortex-M0 core and the AHB-Lite bus by ARM, building a secure 64-core\nsystem with shared memories. We evaluate our scheme through hardware\nsimulation, considering different threat scenarios. Finally, we devise a\nphysical-design flow for 2.5D systems, based on commercial-grade design tools,\nto demonstrate and evaluate our 2.5D root of trust.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 22:31:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 21:12:21 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Nabeel", "Mohammed", ""], ["Ashraf", "Mohammed", ""], ["Patnaik", "Satwik", ""], ["Soteriou", "Vassos", ""], ["Sinanoglu", "Ozgur", ""], ["Knechtel", "Johann", ""]]}, {"id": "2009.02449", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "Hierarchical Roofline Analysis: How to Collect Data using Performance\n  Tools on Intel CPUs and NVIDIA GPUs", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys a range of methods to collect necessary performance data\non Intel CPUs and NVIDIA GPUs for hierarchical Roofline analysis. As of\nmid-2020, two vendor performance tools, Intel Advisor and NVIDIA Nsight\nCompute, have integrated Roofline analysis into their supported feature set.\nThis paper fills the gap for when these tools are not available, or when users\nwould like a more customized workflow for certain analysis. Specifically, we\nwill discuss how to use Intel Advisor, RRZE LIKWID, Intel SDE and Intel\nAmplifier on Intel architectures, and nvprof, Nsight Compute metrics, and\nNsight Compute section files on NVIDIA architectures. These tools will be used\nto collect information for as many memory/cache levels in the memory hierarchy\nas possible in order to provide insights into application's data reuse and\ncache locality characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 03:14:42 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 05:27:51 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 20:23:56 GMT"}, {"version": "v4", "created": "Sun, 4 Oct 2020 17:04:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2009.02535", "submitter": "Xuan He", "authors": "Xuan He, Kui Cai, and Liang Zhou", "title": "A Class of Optimal Structures for Node Computations in Message Passing\n  Algorithms", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the computations at a node in the message passing algorithms. Assume\nthat the node has incoming and outgoing messages $\\mathbf{x} = (x_1, x_2,\n\\ldots, x_n)$ and $\\mathbf{y} = (y_1, y_2, \\ldots, y_n)$, respectively. In this\npaper, we investigate a class of structures that can be adopted by the node for\ncomputing $\\mathbf{y}$ from $\\mathbf{x}$, where each $y_j, j = 1, 2, \\ldots, n$\nis computed via a binary tree with leaves $\\mathbf{x}$ excluding $x_j$. We have\nthree main contributions regarding this class of structures. First, we prove\nthat the minimum complexity of such a structure is $3n - 6$, and if a structure\nhas such complexity, its minimum latency is $\\delta + \\lceil \\log(n-2^{\\delta})\n\\rceil$ with $\\delta = \\lfloor \\log(n/2) \\rfloor$. Second, we prove that the\nminimum latency of such a structure is $\\lceil \\log(n-1) \\rceil$, and if a\nstructure has such latency, its minimum complexity is $n \\log(n-1)$ when $n-1$\nis a power of two. Third, given $(n, \\tau)$ with $\\tau \\geq \\lceil \\log(n-1)\n\\rceil$, we propose a construction for a structure which likely has the minimum\ncomplexity among structures with latencies at most $\\tau$. Our construction\nmethod runs in $O(n^3 \\log^2(n))$ time, and the obtained structure has\ncomplexity at most (generally much smaller than) $n \\lceil \\log(n) \\rceil - 2$.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 13:52:01 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:26:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["He", "Xuan", ""], ["Cai", "Kui", ""], ["Zhou", "Liang", ""]]}, {"id": "2009.03011", "submitter": "Shaoshan Liu", "authors": "Shaoshan Liu", "title": "Critical Business Decision Making for Technology Startups -- A PerceptIn\n  Case Study", "comments": "to appear in IEEE Engineering Management Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most business decisions are made with analysis, but some are judgment calls\nnot susceptible to analysis due to time or information constraints. In this\narticle, we present a real-life case study of critical business decision making\nof PerceptIn, an autonomous driving technology startup. In early years of\nPerceptIn, PerceptIn had to make a decision on the design of computing systems\nfor its autonomous vehicle products. By providing details on PerceptIn's\ndecision process and the results of the decision, we hope to provide some\ninsights that can be beneficial to entrepreneurs and engineering managers in\ntechnology startups.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:52:20 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Shaoshan", ""]]}, {"id": "2009.03242", "submitter": "Rolf Drechsler", "authors": "Rolf Drechsler", "title": "PolyAdd: Polynomial Formal Verification of Adder Circuits", "comments": "7 pages, 8 figures, published at 24th International Symposium on\n  Design and Diagnostics of Electronic Circuits and Systems (DDECS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only by formal verification approaches functional correctness can be ensured.\nWhile for many circuits fast verification is possible, in other cases the\napproaches fail. In general no efficient algorithms can be given, since the\nunderlying verification problem is NP-complete. In this paper we prove that for\ndifferent types of adder circuits polynomial verification can be ensured based\non BDDs. While it is known that the output functions for addition are\npolynomially bounded, we show in the following that the entire construction\nprocess can be carried out in polynomial time. This is shown for the simple\nRipple Carry Adder, but also for fast adders like the Conditional Sum Adder and\nthe Carry Look Ahead Adder. Properties about the adder function are proven and\nthe core principle of polynomial verification is described that can also be\nextended to other classes of functions and circuit realizations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:10:21 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:51:26 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 10:20:50 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Drechsler", "Rolf", ""]]}, {"id": "2009.03468", "submitter": "Javad Bagherzadeh", "authors": "Javad Bagherzadeh, Vishishtha Bothra, Disha Gujar, Sugandha Gupta,\n  Jinal Shah", "title": "Quad-Core RSA Processor with Countermeasure Against Power Analysis\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Rivest-Shamir-Adleman (RSA) cryptosystem uses modular multiplication for\nencryption and decryption. So, performance of RSA can be drastically improved\nby optimizing modular multiplication. This paper proposes a new parallel,\nhigh-radix Montgomery multiplier for 1024 bits multi-core RSA processor. Each\ncomputation step operates in radix 4. The computation speed is increased by\nmore than 4 times. We also implement a True Random Number Generator based\nresilience block to protect the coprocessor against power attacks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 00:42:53 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bagherzadeh", "Javad", ""], ["Bothra", "Vishishtha", ""], ["Gujar", "Disha", ""], ["Gupta", "Sugandha", ""], ["Shah", "Jinal", ""]]}, {"id": "2009.03846", "submitter": "Soham Chakraborty", "authors": "Soham Chakraborty", "title": "On Architecture to Architecture Mapping for Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping programs from one architecture to another plays a key role in\ntechnologies such as binary translation, decompilation, emulation,\nvirtualization, and application migration. Although multicore architectures are\nubiquitous, the state-of-the-art translation tools do not handle concurrency\nprimitives correctly. Doing so is rather challenging because of the subtle\ndifferences in the concurrency models between architectures.\n  In response, we address various aspects of the challenge. First, we develop\ncorrect and efficient translations between the concurrency models of two\nmainstream architecture families: x86 and ARM (versions 7 and 8). We develop\ndirect mappings between x86 and ARMv8 and ARMv7, and fence elimination\nalgorithms to eliminate redundant fences after direct mapping. Although our\nmapping utilizes ARMv8 as an intermediate model for mapping between x86 and\nARMv7, we argue that it should not be used as an intermediate model in a\ndecompiler because it disallows common compiler transformations.\n  Second, we propose and implement a technique for inserting memory fences for\nsafely migrating programs between different architectures. Our technique checks\nrobustness against x86 and ARM, and inserts fences upon robustness violations.\nOur experiments demonstrate that in most of the programs both our techniques\nintroduce significantly fewer fences compared to naive schemes for porting\napplications across these architectures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:28:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Chakraborty", "Soham", ""]]}, {"id": "2009.03874", "submitter": "Oscar Casta\\~neda", "authors": "Oscar Casta\\~neda, Sven Jacobsson, Giuseppe Durisi, Tom Goldstein,\n  Christoph Studer", "title": "High-Bandwidth Spatial Equalization for mmWave Massive MU-MIMO with\n  Processing-In-Memory", "comments": "To be presented at the IEEE International Symposium on Circuits and\n  Systems (ISCAS) 2020; invited to a special issue in the IEEE Transactions on\n  Circuits and Systems (TCAS)-II", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-digital basestation (BS) architectures enable superior spectral\nefficiency compared to hybrid solutions in massive multi-user MIMO systems.\nHowever, supporting large bandwidths with all-digital architectures at mmWave\nfrequencies is challenging as traditional baseband processing would result in\nexcessively high power consumption and large silicon area. The\nrecently-proposed concept of finite-alphabet equalization is able to address\nboth of these issues by using equalization matrices that contain low-resolution\nentries to lower the power and complexity of high-throughput matrix-vector\nproducts in hardware. In this paper, we explore two different finite-alphabet\nequalization hardware implementations that tightly integrate the memory and\nprocessing elements: (i) a parallel array of multiply-accumulate (MAC) units\nand (ii) a bit-serial processing-in-memory (PIM) architecture. Our all-digital\nVLSI implementation results in 28nm CMOS show that the bit-serial PIM\narchitecture reduces the area and power consumption up to a factor of 2x and\n3x, respectively, when compared to a parallel MAC array that operates at the\nsame throughput.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 17:30:15 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Casta\u00f1eda", "Oscar", ""], ["Jacobsson", "Sven", ""], ["Durisi", "Giuseppe", ""], ["Goldstein", "Tom", ""], ["Studer", "Christoph", ""]]}, {"id": "2009.03945", "submitter": "Freddy Gabbay", "authors": "Freddy Gabbay and Avi Mendelson", "title": "Asymmetric Aging Effect on Modern Microprocessors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability is a crucial requirement in any modern microprocessor to assure\ncorrect execution over its lifetime. As mission critical components are\nbecoming common in commodity systems; e.g., control of autonomous cars, the\ndemand for reliable processing has even further heightened. Latest process\ntechnologies even worsened the situation; thus, microprocessors design has\nbecome highly susceptible to reliability concerns. This paper examines\nasymmetric aging phenomenon, which is a major reliability concern in advanced\nprocess nodes. In this phenomenon, logical elements and memory cells suffer\nfrom unequal timing degradation over time and consequently introduce\nreliability concerns. So far, most studies approached asymmetric aging from\ncircuit or physical design viewpoint, but these solutions were quite limited\nand suboptimal. In this paper we introduce an asymmetric aging aware\nmicro-architecture that aims at reducing its impact. The study is mainly\nfocused on the following subsystems: execution units, register files and the\nmemory hierarchy. Our experiments indicate that the proposed solutions incur\nminimal overhead while significantly mitigating the asymmetric aging stress.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 18:56:14 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Gabbay", "Freddy", ""], ["Mendelson", "Avi", ""]]}, {"id": "2009.04063", "submitter": "Mahmoud Elmohr", "authors": "Mahmoud Khalafalla, Mahmoud A. Elmohr, Catherine Gebotys", "title": "Going Deep: Using deep learning techniques with simplified mathematical\n  models against XOR BR and TBR PUFs (Attacks and Countermeasures)", "comments": "To appear in proceedings of 2020 IEEE International Symposium on\n  Hardware Oriented Security and Trust (HOST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to the study of PUFs vulnerability against modeling\nattacks by evaluating the security of XOR BR PUFs, XOR TBR PUFs, and obfuscated\narchitectures of XOR BR PUF using a simplified mathematical model and deep\nlearning (DL) techniques. Obtained results show that DL modeling attacks could\neasily break the security of 4-input XOR BR PUFs and 4-input XOR TBR PUFs with\nmodeling accuracy $\\sim$ 99%. Similar attacks were executed using single-layer\nneural networks (NN) and support vector machines (SVM) with polynomial kernel\nand the obtained results showed that single NNs failed to break the PUF\nsecurity. Furthermore, SVM results confirmed the same modeling accuracy\nreported in previous research ($\\sim$ 50%). For the first time, this research\nempirically shows that DL networks can be used as powerful modeling techniques\nagainst these complex PUF architectures for which previous conventional machine\nlearning techniques had failed. Furthermore, a detailed scalability analysis is\nconducted on the DL networks with respect to PUFs' stage size and complexity.\nThe analysis shows that the number of layers and hidden neurons inside every\nlayer has a linear relationship with PUFs' stage size, which agrees with the\ntheoretical findings in deep learning. Consequently, A new obfuscated\narchitecture is introduced as a first step to counter DL modeling attacks and\nit showed significant resistance against such attacks (16% - 40% less\naccuracy). This research provides an important step towards prioritizing the\nefforts to introduce new PUF architectures that are more secure and\ninvulnerable to modeling attacks. Moreover, it triggers future discussions on\nthe removal of influential bits and the level of obfuscation needed to confirm\nthat a specific PUF architecture is resistant against powerful DL modeling\nattacks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 01:41:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Khalafalla", "Mahmoud", ""], ["Elmohr", "Mahmoud A.", ""], ["Gebotys", "Catherine", ""]]}, {"id": "2009.04598", "submitter": "Charlene Yang", "authors": "Yunsong Wang, Charlene Yang, Steven Farrell, Yan Zhang, Thorsten\n  Kurth, Samuel Williams", "title": "Time-Based Roofline for Deep Learning Performance Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applications are usually very compute-intensive and require a\nlong run time for training and inference. This has been tackled by researchers\nfrom both hardware and software sides, and in this paper, we propose a\nRoofline-based approach to performance analysis to facilitate the optimization\nof these applications. This approach is an extension of the Roofline model\nwidely used in traditional high-performance computing applications, and it\nincorporates both compute/bandwidth complexity and run time in its formulae to\nprovide insights into deep learning-specific characteristics. We take two sets\nof representative kernels, 2D convolution and long short-term memory, to\nvalidate and demonstrate the use of this new approach, and investigate how\narithmetic intensity, cache locality, auto-tuning, kernel launch overhead, and\nTensor Core usage can affect performance. Compared to the common ad-hoc\napproach, this study helps form a more systematic way to analyze code\nperformance and identify optimization opportunities for deep learning\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 23:29:04 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:11:36 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 21:51:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Wang", "Yunsong", ""], ["Yang", "Charlene", ""], ["Farrell", "Steven", ""], ["Zhang", "Yan", ""], ["Kurth", "Thorsten", ""], ["Williams", "Samuel", ""]]}, {"id": "2009.04600", "submitter": "Kirti Bhanushali", "authors": "Kirti Bhanushali, Chinmay Tembe and W. Rhett Davis", "title": "Development of a Predictive Process Design kit for15-nm FinFETs:\n  FreePDK15", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FinFETs are predicted to advance semiconductorscaling for sub-20nm devices.\nIn order to support their intro-duction into research and universities it is\ncrucial to develop anopen source predictive process design kit. This paper\ndiscussesin detail the design process for such a kit for 15nm FinFETdevices,\ncalled the FreePDK15. The kit consists of a layerstack with thirteen-metal\nlayers based on hierarchical-scalingused in ASIC architecture, Middle-of-Line\nlocal interconnectlayers and a set of Front-End-of-Line layers. The physical\nandgeometrical properties of these layers are defined and theseproperties\ndetermine the density and parasitics of the design. Thedesign rules are laid\ndown considering additional guidelines forprocess variability, challenges\ninvolved in FinFET fabrication anda unique set of design rules are developed\nfor critical dimensions.Layout extraction including modified rules for\ndetermining thegeometrical characteristics of FinFET layouts are implementedand\ndiscussed to obtain successful Layout Versus Schematicchecks for a set of\nlayouts. Moreover, additional parasiticcomponents of a standard FinFET device\nare analyzed andthe parasitic extraction of sample layouts is performed.\nTheseextraction results are then compared and assessed against thevalidation\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 23:39:10 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Bhanushali", "Kirti", ""], ["Tembe", "Chinmay", ""], ["Davis", "W. Rhett", ""]]}, {"id": "2009.04622", "submitter": "Gokul Subramanian Ravi", "authors": "Gokul Subramanian Ravi, Ramon Bertran, Pradip Bose, Mikko Lipasti", "title": "MicroGrad: A Centralized Framework for Workload Cloning and Stress\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MicroGrad, a centralized automated framework that is able to\nefficiently analyze the capabilities, limits and sensitivities of complex\nmodern processors in the face of constantly evolving application domains.\nMicroGrad uses Microprobe, a flexible code generation framework as its back-end\nand a Gradient Descent based tuning mechanism to efficiently enable the\nevolution of the test cases to suit tasks such as Workload Cloning and Stress\nTesting. MicroGrad can interface with a variety of execution infrastructure\nsuch as performance and power simulators as well as native hardware. Further,\nthe modular 'abstract workload model' approach to building MicroGrad allows it\nto be easily extended for further use.\n  In this paper, we evaluate MicroGrad over different use cases and\narchitectures and showcase that MicroGrad can achieve greater than 99\\%\naccuracy across different tasks within few tuning epochs and low resource\nrequirements. We also observe that MicroGrad's accuracy is 25 to 30\\% higher\nthan competing techniques. At the same time, it is 1.5x to 2.5x faster or would\nconsume 35 to 60\\% less compute resources (depending on implementation) over\nalternate mechanisms. Overall, MicroGrad's fast, resource efficient and\naccurate test case generation capability allow it to perform rapid evaluation\nof complex processors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 01:10:44 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ravi", "Gokul Subramanian", ""], ["Bertran", "Ramon", ""], ["Bose", "Pradip", ""], ["Lipasti", "Mikko", ""]]}, {"id": "2009.05230", "submitter": "Suresh Krishna", "authors": "Suresh Krishna, Ravi Krishna", "title": "Accelerating Recommender Systems via Hardware \"scale-in\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's era of \"scale-out\", this paper makes the case that a specialized\nhardware architecture based on \"scale-in\"--placing as many specialized\nprocessors as possible along with their memory systems and interconnect links\nwithin one or two boards in a rack--would offer the potential to boost large\nrecommender system throughput by 12-62x for inference and 12-45x for training\ncompared to the DGX-2 state-of-the-art AI platform, while minimizing the\nperformance impact of distributing large models across multiple processors. By\nanalyzing Facebook's representative model--Deep Learning Recommendation Model\n(DLRM)--from a hardware architecture perspective, we quantify the impact on\nthroughput of hardware parameters such as memory system design, collective\ncommunications latency and bandwidth, and interconnect topology. By focusing on\nconditions that stress hardware, our analysis reveals limitations of existing\nAI accelerators and hardware platforms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 04:51:14 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Krishna", "Suresh", ""], ["Krishna", "Ravi", ""]]}, {"id": "2009.05329", "submitter": "Ali Mahani", "authors": "Mahdi Taheri, Saeideh Sheikhpour, Mohammad Saeed Ansari and Ali Mahani", "title": "DMR-based Technique for Fault Tolerant AES S-box Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high-throughput fault-resilient hardware implementation\nof AES S-box, called HFS-box. If a transient natural or even malicious fault in\neach pipeline stage is detected, the corresponding error signal becomes high\nand as a result, the control unit holds the output of our proposed DMR voter\ntill the fault effect disappears. The proposed low-cost HFS-box provides a high\ncapability of fault-tolerant against transient faults with any duration by\nputting low area overhead, i.e. 137%, and low throughput degradation, i.e.\n11.3%, on the original implementation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:35:34 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Taheri", "Mahdi", ""], ["Sheikhpour", "Saeideh", ""], ["Ansari", "Mohammad Saeed", ""], ["Mahani", "Ali", ""]]}, {"id": "2009.05334", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Wolfgang R\\\"onninger, Thomas Benz, Matheus Cavalcante,\n  Fabian Schuiki, Florian Zaruba, Luca Benini", "title": "An Open-Source Platform for High-Performance Non-Coherent On-Chip\n  Communication", "comments": "14 pages, 24 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-chip communication infrastructure is a central component of modern\nsystems-on-chip (SoCs), and it continues to gain importance as the number of\ncores, the heterogeneity of components, and the on-chip and off-chip bandwidth\ncontinue to grow. Decades of research on on-chip networks enabled\ncache-coherent shared-memory multiprocessors. However, communication fabrics\nthat meet the needs of heterogeneous many-cores and accelerator-rich SoCs,\nwhich are not, or only partially, coherent, are a much less mature research\narea.\n  In this work, we present a modular, topology-agnostic, high-performance\non-chip communication platform. The platform includes components to build and\nlink subnetworks with customizable bandwidth and concurrency properties and\nadheres to a state-of-the-art, industry-standard protocol. We discuss\nmicroarchitectural trade-offs and timing/area characteristics of our modules\nand show that they can be composed to build high-bandwidth (e.g., 2.5 GHz and\n1024 bit data width) end-to-end on-chip communication fabrics (not only network\nswitches but also DMA engines and memory controllers) with high degrees of\nconcurrency. We design and implement a state-of-the-art ML training\naccelerator, where our communication fabric scales to 1024 cores on a die,\nproviding 32 TB/s cross-sectional bandwidth at only 24 ns round-trip latency\nbetween any two cores.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 10:53:37 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Kurth", "Andreas", ""], ["R\u00f6nninger", "Wolfgang", ""], ["Benz", "Thomas", ""], ["Cavalcante", "Matheus", ""], ["Schuiki", "Fabian", ""], ["Zaruba", "Florian", ""], ["Benini", "Luca", ""]]}, {"id": "2009.06034", "submitter": "Zishen Wan", "authors": "Zishen Wan, Bo Yu, Thomas Yuang Li, Jie Tang, Yuhao Zhu, Yu Wang,\n  Arijit Raychowdhury, Shaoshan Liu", "title": "A Survey of FPGA-Based Robotic Computing", "comments": "To appear in IEEE Circuits and Systems Magazine (CAS-M), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches on robotics have shown significant improvement, spanning\nfrom algorithms, mechanics to hardware architectures. Robotics, including\nmanipulators, legged robots, drones, and autonomous vehicles, are now widely\napplied in diverse scenarios. However, the high computation and data complexity\nof robotic algorithms pose great challenges to its applications. On the one\nhand, CPU platform is flexible to handle multiple robotic tasks. GPU platform\nhas higher computational capacities and easy-touse development frameworks, so\nthey have been widely adopted in several applications. On the other hand,\nFPGA-based robotic accelerators are becoming increasingly competitive\nalternatives, especially in latency-critical and power-limited scenarios. With\nspecialized designed hardware logic and algorithm kernels, FPGA-based\naccelerators can surpass CPU and GPU in performance and energy efficiency. In\nthis paper, we give an overview of previous work on FPGA-based robotic\naccelerators covering different stages of the robotic system pipeline. An\nanalysis of software and hardware optimization techniques and main technical\nissues is presented, along with some commercial and space applications, to\nserve as a guide for future work.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:22:08 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 03:10:31 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 04:56:55 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wan", "Zishen", ""], ["Yu", "Bo", ""], ["Li", "Thomas Yuang", ""], ["Tang", "Jie", ""], ["Zhu", "Yuhao", ""], ["Wang", "Yu", ""], ["Raychowdhury", "Arijit", ""], ["Liu", "Shaoshan", ""]]}, {"id": "2009.06156", "submitter": "Oren Segal", "authors": "Philip Colangelo, Oren Segal, Alex Speicher, Martin Margala", "title": "AutoML for Multilayer Perceptron and FPGA Co-design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art Neural Network Architectures (NNAs) are challenging to\ndesign and implement efficiently in hardware. In the past couple of years, this\nhas led to an explosion in research and development of automatic Neural\nArchitecture Search (NAS) tools. AutomML tools are now used to achieve state of\nthe art NNA designs and attempt to optimize for hardware usage and design. Much\nof the recent research in the auto-design of NNAs has focused on convolution\nnetworks and image recognition, ignoring the fact that a significant part of\nthe workload in data centers is general-purpose deep neural networks. In this\nwork, we develop and test a general multilayer perceptron (MLP) flow that can\ntake arbitrary datasets as input and automatically produce optimized NNAs and\nhardware designs. We test the flow on six benchmarks. Our results show we\nexceed the performance of currently published MLP accuracy results and are\ncompetitive with non-MLP based results. We compare general and common GPU\narchitectures with our scalable FPGA design and show we can achieve higher\nefficiency and higher throughput (outputs per second) for the majority of\ndatasets. Further insights into the design space for both accurate networks and\nhigh performing hardware shows the power of co-design by correlating accuracy\nversus throughput, network size versus accuracy, and scaling to\nhigh-performance devices.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 02:37:51 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Colangelo", "Philip", ""], ["Segal", "Oren", ""], ["Speicher", "Alex", ""], ["Margala", "Martin", ""]]}, {"id": "2009.06237", "submitter": "Jinho Lee", "authors": "Kanghyun Choi, Deokki Hong, Hojae Yoon, Joonsang Yu, Youngsok Kim,\n  Jinho Lee", "title": "DANCE: Differentiable Accelerator/Network Co-Exploration", "comments": "Accepted to DAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the ever-increasing computational demand of the DNN execution,\nrecent neural architecture search (NAS) algorithms consider hardware cost\nmetrics into account, such as GPU latency. To further pursue a fast, efficient\nexecution, DNN-specialized hardware accelerators are being designed for\nmultiple purposes, which far-exceeds the efficiency of the GPUs. However, those\nhardware-related metrics have been proven to exhibit non-linear relationships\nwith the network architectures. Therefore it became a chicken-and-egg problem\nto optimize the network against the accelerator, or to optimize the accelerator\nagainst the network. In such circumstances, this work presents DANCE, a\ndifferentiable approach towards the co-exploration of the hardware accelerator\nand network architecture design. At the heart of DANCE is a differentiable\nevaluator network. By modeling the hardware evaluation software with a neural\nnetwork, the relation between the accelerator architecture and the hardware\nmetrics becomes differentiable, allowing the search to be performed with\nbackpropagation. Compared to the naive existing approaches, our method performs\nco-exploration in a significantly shorter time, while achieving superior\naccuracy and hardware cost metrics.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 07:43:27 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 12:14:17 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 04:41:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Choi", "Kanghyun", ""], ["Hong", "Deokki", ""], ["Yoon", "Hojae", ""], ["Yu", "Joonsang", ""], ["Kim", "Youngsok", ""], ["Lee", "Jinho", ""]]}, {"id": "2009.06489", "submitter": "Sara Hooker", "authors": "Sara Hooker", "title": "The Hardware Lottery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware, systems and algorithms research communities have historically had\ndifferent incentive structures and fluctuating motivation to engage with each\nother explicitly. This historical treatment is odd given that hardware and\nsoftware have frequently determined which research ideas succeed (and fail).\nThis essay introduces the term hardware lottery to describe when a research\nidea wins because it is suited to the available software and hardware and not\nbecause the idea is superior to alternative research directions. Examples from\nearly computer science history illustrate how hardware lotteries can delay\nresearch progress by casting successful ideas as failures. These lessons are\nparticularly salient given the advent of domain specialized hardware which make\nit increasingly costly to stray off of the beaten path of research ideas. This\nessay posits that the gains from progress in computing are likely to become\neven more uneven, with certain research directions moving into the fast-lane\nwhile progress on others is further obstructed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 14:49:10 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:58:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Hooker", "Sara", ""]]}, {"id": "2009.06789", "submitter": "Drew Zagieboylo", "authors": "Drew Zagieboylo, G. Edward Suh, Andrew C. Myers", "title": "The Cost of Software-Based Memory Management Without Virtual Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual memory has been a standard hardware feature for more than three\ndecades. At the price of increased hardware complexity, it has simplified\nsoftware and promised strong isolation among colocated processes. In modern\ncomputing systems, however, the costs of virtual memory have increased\nsignificantly. With large memory workloads, virtualized environments, data\ncenter computing, and chips with multiple DMA devices, virtual memory can\ndegrade performance and increase power usage. We therefore explore the\nimplications of building applications and operating systems without relying on\nhardware support for address translation. Primarily, we investigate the\nimplications of removing the abstraction of large contiguous memory segments.\nOur experiments show that the overhead to remove this reliance is surprisingly\nsmall for real programs. We expect this small overhead to be worth the benefit\nof reducing the complexity and energy usage of address translation. In fact, in\nsome cases, performance can even improve when address translation is avoided.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 23:28:30 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zagieboylo", "Drew", ""], ["Suh", "G. Edward", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2009.06896", "submitter": "Lilian Bossuet", "authors": "El Mehdi Benhani (LHC), Cuauhtemoc Mancillas Lopez (CINVESTAV-IPN),\n  Lilian Bossuet (LHC)", "title": "Secure Internal Communication of a Trustzone-Enabled Heterogeneous Soc\n  Lightweight Encryption", "comments": null, "journal-ref": "International Conference on Field-Programmable Technology (ICFPT),\n  Dec 2019, Tianjin, China. pp.239-242", "doi": "10.1109/ICFPT47387.2019.00037", "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security in TrustZone-enabled heterogeneous system-on-chip (SoC) is gaining\nincreasing attention for several years. Mainly because this type of SoC can be\nfound in more and more applications in servers or in the cloud. The inside-SoC\ncommunication layer is one of the main element of heterogeneous SoC; indeed all\nthe data goes through it. Monitoring and controlling inside-SoC communications\nenables to fend off attacks before system corruption. In this article, we study\nthe feasibility of encrypted data exchange between the secure software executed\nin a trusted execution environment (TEE) and the secure logic part of an\nheterogeneous SoC. Experiment are done with a Xilinx Zynq-7010 SoC and two\nlightweight stream ciphers. We show that using lightweight stream ciphers is an\nefficient solution without excessive overheads.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 07:17:38 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Benhani", "El Mehdi", "", "LHC"], ["Lopez", "Cuauhtemoc Mancillas", "", "CINVESTAV-IPN"], ["Bossuet", "Lilian", "", "LHC"]]}, {"id": "2009.07091", "submitter": "Zain Ul Abideen", "authors": "Malik Imran and Zain Ul Abideen and Samuel Pagliarini", "title": "A Systematic Study of Lattice-based NIST PQC Algorithms: from Reference\n  Implementations to Hardware Accelerators", "comments": "38 pages and 8 figures", "journal-ref": "https://www.mdpi.com/2079-9292/9/11/1953", "doi": "10.3390/electronics9111953", "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security of currently deployed public key cryptography algorithms is foreseen\nto be vulnerable against quantum computer attacks. Hence, a community effort\nexists to develop post-quantum cryptography (PQC) algorithms, i.e., algorithms\nthat are resistant to quantum attacks. In this work, we have investigated how\nlattice-based candidate algorithms from the NIST PQC standardization\ncompetition fare when conceived as hardware accelerators. To achieve this, we\nhave assessed the reference implementations of selected algorithms with the\ngoal of identifying what are their basic building blocks. We assume the\nhardware accelerators will be implemented in application specific integrated\ncircuit (ASIC) and the targeted technology in our experiments is a commercial\n65nm node. In order to estimate the characteristics of each algorithm, we have\nassessed their memory requirements, use of multipliers, and how each algorithm\nemploys hashing functions. Furthermore, for these building blocks, we have\ncollected area and power figures for 12 candidate algorithms. For memories, we\nmake use of a commercial memory compiler. For logic, we make use of a standard\ncell library. In order to compare the candidate algorithms fairly, we select a\nreference frequency of operation of 500MHz. Our results reveal that our area\nand power numbers are comparable to the state of the art, despite targeting a\nhigher frequency of operation and a higher security level in our experiments.\nThe comprehensive investigation of lattice-based NIST PQC algorithms performed\nin this paper can be used for guiding ASIC designers when selecting an\nappropriate algorithm while respecting requirements and design constraints.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 13:32:03 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 08:50:24 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 15:55:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Imran", "Malik", ""], ["Abideen", "Zain Ul", ""], ["Pagliarini", "Samuel", ""]]}, {"id": "2009.07332", "submitter": "Siddharth Joshi", "authors": "Zephan M. Enciso, Seyed Hadi Mirfarshbafan, Oscar Casta\\~neda, Clemens\n  JS. Schaefer, Christoph Studer, Siddharth Joshi", "title": "Analog vs. Digital Spatial Transforms: A Throughput, Power, and Area\n  Comparison", "comments": "2020 IEEE 63rd International Midwest Symposium on Circuits and\n  Systems (MWSCAS), Springfield, MA, USA, 2020, pp. 125-128, doi:\n  10.1109/MWSCAS48704.2020.9184566", "journal-ref": null, "doi": "10.1109/MWSCAS48704.2020.9184566", "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial linear transforms that process multiple parallel analog signals to\nsimplify downstream signal processing find widespread use in multi-antenna\ncommunication systems, machine learning inference, data compression, audio and\nultrasound applications, among many others. In the past, a wide range of\nmixed-signal as well as digital spatial transform circuits have been\nproposed---it is, however, a longstanding question whether analog or digital\ntransforms are superior in terms of throughput, power, and area. In this paper,\nwe focus on Hadamard transforms and perform a systematic comparison of\nstate-of-the-art analog and digital circuits implementing spatial transforms in\nthe same 65\\,nm CMOS technology. We analyze the trade-offs between throughput,\npower, and area, and we identify regimes in which mixed-signal or digital\nHadamard transforms are preferable. Our comparison reveals that (i) there is no\nclear winner and (ii) analog-to-digital conversion is often dominating area and\nenergy efficiency---and not the spatial transform.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 19:28:19 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Enciso", "Zephan M.", ""], ["Mirfarshbafan", "Seyed Hadi", ""], ["Casta\u00f1eda", "Oscar", ""], ["Schaefer", "Clemens JS.", ""], ["Studer", "Christoph", ""], ["Joshi", "Siddharth", ""]]}, {"id": "2009.07692", "submitter": "Damla Senol Cali", "authors": "Damla Senol Cali, Gurpreet S. Kalsi, Z\\\"ulal Bing\\\"ol, Can Firtina,\n  Lavanya Subramanian, Jeremie S. Kim, Rachata Ausavarungnirun, Mohammed Alser,\n  Juan Gomez-Luna, Amirali Boroumand, Anant Nori, Allison Scibisz, Sreenivas\n  Subramoney, Can Alkan, Saugata Ghose, Onur Mutlu", "title": "GenASM: A High-Performance, Low-Power Approximate String Matching\n  Acceleration Framework for Genome Sequence Analysis", "comments": "To appear in MICRO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome sequence analysis has enabled significant advancements in medical and\nscientific areas such as personalized medicine, outbreak tracing, and the\nunderstanding of evolution. Unfortunately, it is currently bottlenecked by the\ncomputational power and memory bandwidth limitations of existing systems, as\nmany of the steps in genome sequence analysis must process a large amount of\ndata. A major contributor to this bottleneck is approximate string matching\n(ASM).\n  We propose GenASM, the first ASM acceleration framework for genome sequence\nanalysis. We modify the underlying ASM algorithm (Bitap) to significantly\nincrease its parallelism and reduce its memory footprint, and we design the\nfirst hardware accelerator for Bitap. Our hardware accelerator consists of\nspecialized compute units and on-chip SRAMs that are designed to match the rate\nof computation with memory capacity and bandwidth.\n  We demonstrate that GenASM is a flexible, high-performance, and low-power\nframework, which provides significant performance and power benefits for three\ndifferent use cases in genome sequence analysis: 1) GenASM accelerates read\nalignment for both long reads and short reads. For long reads, GenASM\noutperforms state-of-the-art software and hardware accelerators by 116x and\n3.9x, respectively, while consuming 37x and 2.7x less power. For short reads,\nGenASM outperforms state-of-the-art software and hardware accelerators by 111x\nand 1.9x. 2) GenASM accelerates pre-alignment filtering for short reads, with\n3.7x the performance of a state-of-the-art pre-alignment filter, while\nconsuming 1.7x less power and significantly improving the filtering accuracy.\n3) GenASM accelerates edit distance calculation, with 22-12501x and 9.3-400x\nspeedups over the state-of-the-art software library and FPGA-based accelerator,\nrespectively, while consuming 548-582x and 67x less power.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 13:54:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Cali", "Damla Senol", ""], ["Kalsi", "Gurpreet S.", ""], ["Bing\u00f6l", "Z\u00fclal", ""], ["Firtina", "Can", ""], ["Subramanian", "Lavanya", ""], ["Kim", "Jeremie S.", ""], ["Ausavarungnirun", "Rachata", ""], ["Alser", "Mohammed", ""], ["Gomez-Luna", "Juan", ""], ["Boroumand", "Amirali", ""], ["Nori", "Anant", ""], ["Scibisz", "Allison", ""], ["Subramoney", "Sreenivas", ""], ["Alkan", "Can", ""], ["Ghose", "Saugata", ""], ["Mutlu", "Onur", ""]]}, {"id": "2009.07723", "submitter": "Nikolaos Charalampos Papadopoulos", "authors": "Nikolaos Charalampos Papadopoulos, Vasileios Karakostas, Konstantinos\n  Nikas, Nectarios Koziris and Dionisios N. Pnevmatikatos", "title": "Enabling Virtual Memory Research on RISC-V with a Configurable TLB\n  Hierarchy for the Rocket Chip Generator", "comments": "7 pages, Fourth Workshop on Computer Architecture Research with\n  RISC-V (CARRV2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Rocket Chip Generator uses a collection of parameterized processor\ncomponents to produce RISC-V-based SoCs. It is a powerful tool that can produce\na wide variety of processor designs ranging from tiny embedded processors to\ncomplex multi-core systems. In this paper we extend the features of the Memory\nManagement Unit of the Rocket Chip Generator and specifically the TLB\nhierarchy. TLBs are essential in terms of performance because they mitigate the\noverhead of frequent Page Table Walks, but may harm the critical path of the\nprocessor due to their size and/or associativity. In the original Rocket Chip\nimplementation the L1 Instruction/Data TLB is fully-associative and the shared\nL2 TLB is direct-mapped. We lift these restrictions and design and implement\nconfigurable, set-associative L1 and L2 TLB templates that can create any\norganization from direct-mapped to fully-associative to achieve the desired\nratio of performance and resource utilization, especially for larger TLBs. We\nevaluate different TLB configurations and present performance, area, and\nfrequency results of our design using benchmarks from the SPEC2006 suite on the\nXilinx ZCU102 FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 14:48:10 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Papadopoulos", "Nikolaos Charalampos", ""], ["Karakostas", "Vasileios", ""], ["Nikas", "Konstantinos", ""], ["Koziris", "Nectarios", ""], ["Pnevmatikatos", "Dionisios N.", ""]]}, {"id": "2009.07773", "submitter": "Joseph Gravellier", "authors": "Joseph Gravellier, Jean-Max Dutertre, Yannick Teglia, Philippe Loubet\n  Moundi", "title": "SideLine: How Delay-Lines (May) Leak Secrets from your SoC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the ever-growing need for performance in silicon devices, SoC\nproviders have been increasingly relying on software-hardware cooperation. By\ncontrolling hardware resources such as power or clock management from the\nsoftware, developers earn the possibility to build more flexible and power\nefficient applications. Despite the benefits, these hardware components are now\nexposed to software code and can potentially be misused as open-doors to\njeopardize trusted environments, perform privilege escalation or steal\ncryptographic secrets. In this work, we introduce SideLine, a novel\nside-channel vector based on delay-line components widely implemented in\nhigh-end SoCs. After providing a detailed method on how to access and convert\ndelay-line data into power consumption information, we demonstrate that these\nentities can be used to perform remote power side-channel attacks. We report\nexperiments carried out on two SoCs from distinct vendors and we recount\nseveral core-vs-core attack scenarios in which an adversary process located in\none processor core aims at eavesdropping the activity of a victim process\nlocated in another core. For each scenario, we demonstrate the adversary\nability to fully recover the secret key of an OpenSSL AES running in the victim\ncore. Even more detrimental, we show that these attacks are still practicable\nif the victim or the attacker program runs over an operating system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:05:23 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Gravellier", "Joseph", ""], ["Dutertre", "Jean-Max", ""], ["Teglia", "Yannick", ""], ["Moundi", "Philippe Loubet", ""]]}, {"id": "2009.07811", "submitter": "Bilgesu Bilgin", "authors": "Bilgesu Arif Bilgin, Phillip Stanley-Marbell", "title": "Probabilistic Value-Deviation-Bounded Source-Dependent Bit-Level Channel\n  Adaptation for Approximate Communication", "comments": "14 pages, 10 figures, submitted to IEEE Transactions on Computers", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing systems that can tolerate effects of errors in their communicated\ndata values can trade this tolerance for improved resource efficiency. Many\nimportant applications of computing, such as embedded sensor systems, can\ntolerate errors that are bounded in their distribution of deviation from\ncorrectness (distortion). We present a channel adaptation technique which\nmodulates properties of I/O channels typical in embedded sensor systems, to\nprovide a tradeoff between I/O power dissipation and distortion of communicated\ndata. We provide an efficient-to-compute formulation for the distribution of\ninteger distortion accounting for the distribution of transmitted values. Using\nthis formulation we implement our value-deviation-bounded (VDB) channel\nadaptation. We experimentally quantify the achieved reduction in power\ndissipation on a hardware prototype integrated with the required programmable\nchannel modulation circuitry. We augment these experimental measurements with\nan analysis of the distributions of distortions. We show that our probabilistic\nVDB channel adaptation can provide up to a 2$\\times$ reduction in I/O power\ndissipation. When synthesized for a miniature low-power FPGA intended for use\nin sensor interfaces, a register transfer level implementation of the channel\nadaptation control logic requires only 106 flip-flops and 224 4-input LUTs for\nimplementing per-bit channel adaptation on serialized streams of 8-bit sensor\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:08:29 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Bilgin", "Bilgesu Arif", ""], ["Stanley-Marbell", "Phillip", ""]]}, {"id": "2009.07985", "submitter": "Minesh Patel", "authors": "Minesh Patel, Jeremie S. Kim, Taha Shahroodi, Hasan Hassan, Onur Mutlu", "title": "Bit-Exact ECC Recovery (BEER): Determining DRAM On-Die ECC Functions by\n  Exploiting DRAM Data Retention Characteristics", "comments": "To appear in the MICRO 2020 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing single-cell DRAM error rates have pushed DRAM manufacturers to\nadopt on-die error-correction coding (ECC), which operates entirely within a\nDRAM chip to improve factory yield. The on-die ECC function and its effects on\nDRAM reliability are considered trade secrets, so only the manufacturer knows\nprecisely how on-die ECC alters the externally-visible reliability\ncharacteristics. Consequently, on-die ECC obstructs third-party DRAM customers\n(e.g., test engineers, experimental researchers), who typically design, test,\nand validate systems based on these characteristics.\n  To give third parties insight into precisely how on-die ECC transforms DRAM\nerror patterns during error correction, we introduce Bit-Exact ECC Recovery\n(BEER), a new methodology for determining the full DRAM on-die ECC function\n(i.e., its parity-check matrix) without hardware tools, prerequisite knowledge\nabout the DRAM chip or on-die ECC mechanism, or access to ECC metadata (e.g.,\nerror syndromes, parity information). BEER exploits the key insight that\nnon-intrusively inducing data-retention errors with carefully-crafted test\npatterns reveals behavior that is unique to a specific ECC function.\n  We use BEER to identify the ECC functions of 80 real LPDDR4 DRAM chips with\non-die ECC from three major DRAM manufacturers. We evaluate BEER's correctness\nin simulation and performance on a real system to show that BEER is effective\nand practical across a wide range of on-die ECC functions. To demonstrate\nBEER's value, we propose and discuss several ways that third parties can use\nBEER to improve their design and testing practices. As a concrete example, we\nintroduce and evaluate BEEP, the first error profiling methodology that uses\nthe known on-die ECC function to recover the number and bit-exact locations of\nunobservable raw bit errors responsible for observable post-correction errors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 00:29:10 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Patel", "Minesh", ""], ["Kim", "Jeremie S.", ""], ["Shahroodi", "Taha", ""], ["Hassan", "Hasan", ""], ["Mutlu", "Onur", ""]]}, {"id": "2009.07998", "submitter": "Zecheng He", "authors": "Zecheng He, Guangyuan Hu, Ruby Lee", "title": "New Models for Understanding and Reasoning about Speculative Execution\n  Attacks", "comments": "Accepted to IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectre and Meltdown attacks and their variants exploit hardware performance\noptimization features to cause security breaches. Secret information is\naccessed and leaked through covert or side channels. New attack variants keep\nappearing and we do not have a systematic way to capture the critical\ncharacteristics of these attacks and evaluate why they succeed or fail.\n  In this paper, we provide a new attack-graph model for reasoning about\nspeculative execution attacks. We model attacks as ordered dependency graphs,\nand prove that a race condition between two nodes can occur if there is a\nmissing dependency edge between them. We define a new concept, \"security\ndependency\", between a resource access and its prior authorization operation.\nWe show that a missing security dependency is equivalent to a race condition\nbetween authorization and access, which is a root cause of speculative\nexecution attacks. We show detailed examples of how our attack graph models the\nSpectre and Meltdown attacks, and is generalizable to all the attack variants\npublished so far. This attack model is also very useful for identifying new\nattacks and for generalizing defense strategies. We identify several defense\nstrategies with different performance-security tradeoffs. We show that the\ndefenses proposed so far all fit under one of our defense strategies. We also\nexplain how attack graphs can be constructed and point to this as promising\nfuture work for tool designers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 01:09:23 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 21:32:46 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["He", "Zecheng", ""], ["Hu", "Guangyuan", ""], ["Lee", "Ruby", ""]]}, {"id": "2009.08241", "submitter": "Gagandeep Singh", "authors": "Gagandeep Singh, Dionysios Diamantopoulos, Christoph Hagleitner, Juan\n  Gomez-Luna, Sander Stuijk, Onur Mutlu, Henk Corporaal", "title": "NERO: A Near High-Bandwidth Memory Stencil Accelerator for Weather\n  Prediction Modeling", "comments": "This paper appears in FPL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing climate change calls for fast and accurate weather and climate\nmodeling. However, when solving large-scale weather prediction simulations,\nstate-of-the-art CPU and GPU implementations suffer from limited performance\nand high energy consumption. These implementations are dominated by complex\nirregular memory access patterns and low arithmetic intensity that pose\nfundamental challenges to acceleration. To overcome these challenges, we\npropose and evaluate the use of near-memory acceleration using a reconfigurable\nfabric with high-bandwidth memory (HBM). We focus on compound stencils that are\nfundamental kernels in weather prediction models. By using high-level synthesis\ntechniques, we develop NERO, an FPGA+HBM-based accelerator connected through\nIBM CAPI2 (Coherent Accelerator Processor Interface) to an IBM POWER9 host\nsystem. Our experimental results show that NERO outperforms a 16-core POWER9\nsystem by 4.2x and 8.3x when running two different compound stencil kernels.\nNERO reduces the energy consumption by 22x and 29x for the same two kernels\nover the POWER9 system with an energy efficiency of 1.5 GFLOPS/Watt and 17.3\nGFLOPS/Watt. We conclude that employing near-memory acceleration solutions for\nweather prediction modeling is promising as a means to achieve both high\nperformance and high energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 12:46:17 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Singh", "Gagandeep", ""], ["Diamantopoulos", "Dionysios", ""], ["Hagleitner", "Christoph", ""], ["Gomez-Luna", "Juan", ""], ["Stuijk", "Sander", ""], ["Mutlu", "Onur", ""], ["Corporaal", "Henk", ""]]}, {"id": "2009.08437", "submitter": "Lois Orosa", "authors": "Yaohua Wang, Lois Orosa, Xiangjun Peng, Yang Guo, Saugata Ghose,\n  Minesh Patel, Jeremie S. Kim, Juan G\\'omez Luna, Mohammad Sadrosadati, Nika\n  Mansouri Ghiasi, Onur Mutlu", "title": "FIGARO: Improving System Performance via Fine-Grained In-DRAM Data\n  Relocation and Caching", "comments": "To appear in the MICRO 2020 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DRAM Main memory is a performance bottleneck for many applications due to the\nhigh access latency. In-DRAM caches work to mitigate this latency by augmenting\nregular-latency DRAM with small-but-fast regions of DRAM that serve as a cache\nfor the data held in the regular-latency region of DRAM. While an effective\nin-DRAM cache can allow a large fraction of memory requests to be served from a\nfast DRAM region, the latency savings are often hindered by inefficient\nmechanisms for relocating copies of data into and out of the fast regions.\nExisting in-DRAM caches have two sources of inefficiency: (1) the data\nrelocation granularity is an entire multi-kilobyte row of DRAM; and (2) because\nthe relocation latency increases with the physical distance between the slow\nand fast regions, multiple fast regions are physically interleaved among slow\nregions to reduce the relocation latency, resulting in increased hardware area\nand manufacturing complexity. We propose a new substrate, FIGARO, that uses\nexisting shared global buffers among subarrays within a DRAM bank to provide\nsupport for in-DRAM data relocation across subarrays at the granularity of a\nsingle cache block. FIGARO has a distance-independent latency within a DRAM\nbank, and avoids complex modifications to DRAM. Using FIGARO, we design a\nfine-grained in-DRAM cache called FIGCache. The key idea of FIGCache is to\ncache only small, frequently-accessed portions of different DRAM rows in a\ndesignated region of DRAM. By caching only the parts of each row that are\nexpected to be accessed in the near future, we can pack more of the\nfrequently-accessed data into FIGCache, and can benefit from additional row\nhits in DRAM. Our evaluations show that FIGCache improves the average\nperformance of a system using DDR4 DRAM by 16.3% and reduces average DRAM\nenergy consumption by 7.8% for 8-core workloads, over a conventional system\nwithout in-DRAM caching.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 17:35:46 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Wang", "Yaohua", ""], ["Orosa", "Lois", ""], ["Peng", "Xiangjun", ""], ["Guo", "Yang", ""], ["Ghose", "Saugata", ""], ["Patel", "Minesh", ""], ["Kim", "Jeremie S.", ""], ["Luna", "Juan G\u00f3mez", ""], ["Sadrosadati", "Mohammad", ""], ["Ghiasi", "Nika Mansouri", ""], ["Mutlu", "Onur", ""]]}, {"id": "2009.08605", "submitter": "Siyuan Lu", "authors": "Siyuan Lu, Meiqi Wang, Shuang Liang, Jun Lin, and Zhongfeng Wang", "title": "Hardware Accelerator for Multi-Head Attention and Position-Wise\n  Feed-Forward in the Transformer", "comments": "6 pages, 8 figures. This work has been accepted by IEEE SOCC\n  (System-on-chip Conference) 2020, and peresnted by Siyuan Lu in SOCC2020. It\n  also received the Best Paper Award in the Methdology Track in this conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing hardware accelerators for deep neural networks (DNNs) has been much\ndesired. Nonetheless, most of these existing accelerators are built for either\nconvolutional neural networks (CNNs) or recurrent neural networks (RNNs).\nRecently, the Transformer model is replacing the RNN in the natural language\nprocessing (NLP) area. However, because of intensive matrix computations and\ncomplicated data flow being involved, the hardware design for the Transformer\nmodel has never been reported. In this paper, we propose the first hardware\naccelerator for two key components, i.e., the multi-head attention (MHA)\nResBlock and the position-wise feed-forward network (FFN) ResBlock, which are\nthe two most complex layers in the Transformer. Firstly, an efficient method is\nintroduced to partition the huge matrices in the Transformer, allowing the two\nResBlocks to share most of the hardware resources. Secondly, the computation\nflow is well designed to ensure the high hardware utilization of the systolic\narray, which is the biggest module in our design. Thirdly, complicated\nnonlinear functions are highly optimized to further reduce the hardware\ncomplexity and also the latency of the entire system. Our design is coded using\nhardware description language (HDL) and evaluated on a Xilinx FPGA. Compared\nwith the implementation on GPU with the same setting, the proposed design\ndemonstrates a speed-up of 14.6x in the MHA ResBlock, and 3.4x in the FFN\nResBlock, respectively. Therefore, this work lays a good foundation for\nbuilding efficient hardware accelerators for multiple Transformer networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 03:13:19 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lu", "Siyuan", ""], ["Wang", "Meiqi", ""], ["Liang", "Shuang", ""], ["Lin", "Jun", ""], ["Wang", "Zhongfeng", ""]]}, {"id": "2009.08685", "submitter": "Trista Chen", "authors": "Yu-Sheng Lin, Hung Chang Lu, Yang-Bin Tsao, Yi-Min Chih, Wei-Chao\n  Chen, Shao-Yi Chien", "title": "GrateTile: Efficient Sparse Tensor Tiling for CNN Processing", "comments": "To be published at IEEE Workshop on Signal Processing System (SiPS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose GrateTile, an efficient, hardwarefriendly data storage scheme for\nsparse CNN feature maps (activations). It divides data into uneven-sized\nsubtensors and, with small indexing overhead, stores them in a compressed yet\nrandomly accessible format. This design enables modern CNN accelerators to\nfetch and decompressed sub-tensors on-the-fly in a tiled processing manner.\nGrateTile is suitable for architectures that favor aligned, coalesced data\naccess, and only requires minimal changes to the overall architectural design.\nWe simulate GrateTile with state-of-the-art CNNs and show an average of 55%\nDRAM bandwidth reduction while using only 0.6% of feature map size for indexing\nstorage.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:31:41 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lin", "Yu-Sheng", ""], ["Lu", "Hung Chang", ""], ["Tsao", "Yang-Bin", ""], ["Chih", "Yi-Min", ""], ["Chen", "Wei-Chao", ""], ["Chien", "Shao-Yi", ""]]}, {"id": "2009.09009", "submitter": "Vidya A. Chhabria", "authors": "Vidya A. Chhabria, Vipul Ahuja, Ashwath Prabhu, Nikhil Patil, Palkesh\n  Jain, and Sachin S. Sapatnekar", "title": "Thermal and IR Drop Analysis Using Convolutional Encoder-Decoder\n  Networks", "comments": "Accepted in ASP-DAC 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally expensive temperature and power grid analyses are required\nduring the design cycle to guide IC design. This paper employs encoder-decoder\nbased generative (EDGe) networks to map these analyses to fast and accurate\nimage-to-image and sequence-to-sequence translation tasks. The network takes a\npower map as input and outputs the corresponding temperature or IR drop map. We\npropose two networks: (i) ThermEDGe: a static and dynamic full-chip temperature\nestimator and (ii) IREDGe: a full-chip static IR drop predictor based on input\npower, power grid distribution, and power pad distribution patterns. The models\nare design-independent and must be trained just once for a particular\ntechnology and packaging solution. ThermEDGe and IREDGe are demonstrated to\nrapidly predict the on-chip temperature and IR drop contours in milliseconds\n(in contrast with commercial tools that require several hours or more) and\nprovide an average error of 0.6% and 0.008% respectively.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 18:29:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Chhabria", "Vidya A.", ""], ["Ahuja", "Vipul", ""], ["Prabhu", "Ashwath", ""], ["Patil", "Nikhil", ""], ["Jain", "Palkesh", ""], ["Sapatnekar", "Sachin S.", ""]]}, {"id": "2009.09064", "submitter": "Akash Sridhar", "authors": "Akash Sridhar, Nursultan Kabylkas, Jose Renau", "title": "Load Driven Branch Predictor (LDBP)", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Branch instructions dependent on hard-to-predict load data are the leading\nbranch misprediction contributors. Current state-of-the-art history-based\nbranch predictors have poor prediction accuracy for these branches. Prior\nresearch backs this observation by showing that increasing the size of a\n256-KBit history-based branch predictor to its 1-MBit variant has just a 10%\nreduction in branch mispredictions.\n  We present the novel Load Driven Branch Predictor(LDBP) specifically\ntargeting hard-to-predict branches dependent on a load instruction. Though\nrandom load data determines the outcome for these branches, the load address\nfor most of these data has a predictable pattern. This is an observable\ntemplate in data structures like arrays and maps. Our predictor model exploits\nthis behavior to trigger future loads associated with branches ahead of time\nand use its data to predict the branch's outcome. The predictable loads are\ntracked, and the precomputed outcomes of the branch instruction are buffered\nfor making predictions. Our experimental results show that compared to a\nstandalone 256-Kbit IMLI predictor, when LDBP is augmented with a 150-Kbit\nIMLI, it reduces the average branch mispredictions by 20% and improves average\nIPC by 13.1% for benchmarks from SPEC CINT2006 and GAP benchmark suite.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:01:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sridhar", "Akash", ""], ["Kabylkas", "Nursultan", ""], ["Renau", "Jose", ""]]}, {"id": "2009.09077", "submitter": "Steven Herbst", "authors": "Sung-Jin Kim, Zachary Myers, Steven Herbst, ByongChan Lim, Mark\n  Horowitz", "title": "Open-Source Synthesizable Analog Blocks for High-Speed Link Designs:\n  20-GS/s 5b ENOB Analog-to-Digital Converter and 5-GHz Phase Interpolator", "comments": "2020 IEEE Symposium on VLSI Circuits", "journal-ref": null, "doi": "10.1109/VLSICircuits18222.2020.9162800", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using digital standard cells and digital place-and-route (PnR) tools, we\ncreated a 20 GS/s, 8-bit analog-to-digital converter (ADC) for use in\nhigh-speed serial link applications with an ENOB of 5.6, a DNL of 0.96 LSB, and\nan INL of 2.39 LSB, which dissipated 175 mW in 0.102 mm2 in a 16nm technology.\nThe design is entirely described by HDL so that it can be ported to other\nprocesses with minimal effort and shared as open source.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:04:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kim", "Sung-Jin", ""], ["Myers", "Zachary", ""], ["Herbst", "Steven", ""], ["Lim", "ByongChan", ""], ["Horowitz", "Mark", ""]]}, {"id": "2009.09090", "submitter": "Gururaj Saileshwar", "authors": "Gururaj Saileshwar and Moinuddin Qureshi", "title": "MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical\n  Fully-Associative Design", "comments": "Accepted to appear in USENIX Security 2021. This camera-ready version\n  has an updated Security discussion (Sec-5, Sec-6) and Appendix (new Gem5\n  results) compared to previous Arxiv version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared processor caches are vulnerable to conflict-based side-channel\nattacks, where an attacker can monitor access patterns of a victim by evicting\nvictim cache lines using cache-set conflicts. Recent mitigations propose\nrandomized mapping of addresses to cache lines to obfuscate the locations of\nset-conflicts. However, these are vulnerable to new attacks that discover\nconflicting sets of addresses despite such mitigations, because these designs\nselect eviction-candidates from a small set of conflicting lines.\n  This paper presents Mirage, a practical design for a fully associative cache,\nwherein eviction candidates are selected randomly from all lines resident in\nthe cache, to be immune to set-conflicts. A key challenge for enabling such\ndesigns in large shared caches (containing tens of thousands of cache lines) is\nthe complexity of cache-lookup, as a naive design can require searching through\nall the resident lines. Mirage achieves full-associativity while retaining\npractical set-associative lookups by decoupling placement and replacement,\nusing pointer-based indirection from tag-store to data-store to allow a newly\ninstalled address to globally evict the data of any random resident line. To\neliminate set-conflicts, Mirage provisions extra invalid tags in a\nskewed-associative tag-store design where lines can be installed without\nset-conflict, along with a load-aware skew-selection policy that guarantees the\navailability of sets with invalid tags. Our analysis shows Mirage provides the\nglobal eviction property of a fully-associative cache throughout system\nlifetime (violations of full-associativity, i.e. set-conflicts, occur less than\nonce in 10^4 to 10^17 years), thus offering a principled defense against any\neviction-set discovery and any potential conflict based attacks. Mirage incurs\nlimited slowdown (2%) and 17-20% extra storage compared to a non-secure cache.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:12:18 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 22:39:15 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 01:15:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Saileshwar", "Gururaj", ""], ["Qureshi", "Moinuddin", ""]]}, {"id": "2009.09094", "submitter": "Jawad Haj-Yahya", "authors": "Jawad Haj-Yahya, Mohammed Alser, Jeremie S. Kim, Lois Orosa, Efraim\n  Rotem, Avi Mendelson, Anupam Chattopadhyay, Onur Mutlu", "title": "FlexWatts: A Power- and Workload-Aware Hybrid Power Delivery Network for\n  Energy-Efficient Microprocessors", "comments": "To appear in the MICRO 2020 conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern client processors typically use one of three commonly-used power\ndelivery network (PDN): 1) motherboard voltage regulators (MBVR), 2) integrated\nvoltage regulators (IVR), and 3) low dropout voltage regulators (LDO). We\nobserve that the energy-efficiency of each of these PDNs varies with the\nprocessor power (e.g., thermal design power (TDP) and dynamic power-state) and\nworkload characteristics. This leads to energy inefficiency and performance\nloss, as modern client processors operate across a wide spectrum of power\nconsumption and execute a wide variety of workloads. We propose FlexWatts, a\nhybrid adaptive PDN for modern client processors whose goal is to provide high\nenergy-efficiency across the processor's wide range of power consumption and\nworkloads by dynamically allocating PDNs to processor domains. FlexWatts is\nbased on three key ideas. First, it combines IVRs and LDOs in a novel way to\nshare multiple on-chip and off-chip resources. This hybrid PDN is allocated for\nprocessor domains with a wide power consumption range and it dynamically\nswitches between two modes: IVR-Mode and LDO-Mode, depending on the power\nconsumption. Second, for all other processor domains, FlexWatts statically\nallocates off-chip VRs. Third, FlexWatts introduces a prediction algorithm that\nswitches the hybrid PDN to the mode that is the most beneficial. To evaluate\nthe tradeoffs of PDNs, we develop and open-source PDNspot, the first validated\narchitectural PDN model that enables quantitative analysis of PDN metrics.\nUsing PDNspot, we evaluate FlexWatts on a wide variety of SPEC CPU2006,\n3DMark06, and battery life workloads against IVR, the state-of-the-art PDN in\nmodern client processors. For a 4W TDP processor, FlexWatts improves the\naverage performance of the SPEC CPU2006 and 3DMark06 workloads by 22% and 25%,\nrespectively. FlexWatts has comparable cost and area overhead to IVR.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 21:33:36 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Haj-Yahya", "Jawad", ""], ["Alser", "Mohammed", ""], ["Kim", "Jeremie S.", ""], ["Orosa", "Lois", ""], ["Rotem", "Efraim", ""], ["Mendelson", "Avi", ""], ["Chattopadhyay", "Anupam", ""], ["Mutlu", "Onur", ""]]}, {"id": "2009.09298", "submitter": "Anup Das", "authors": "Adarsha Balaji, Shihao Song, Anup Das, Jeffrey Krichmar, Nikil Dutt,\n  James Shackleford, Nagarajan Kandasamy, Francky Catthoor", "title": "Enabling Resource-Aware Mapping of Spiking Neural Networks via Spatial\n  Decomposition", "comments": "Accepted for publication of IEEE Embedded Systems Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing model complexity, mapping Spiking Neural Network (SNN)-based\napplications to tile-based neuromorphic hardware is becoming increasingly\nchallenging. This is because the synaptic storage resources on a tile, viz. a\ncrossbar, can accommodate only a fixed number of pre-synaptic connections per\npost-synaptic neuron. For complex SNN models that have many pre-synaptic\nconnections per neuron, some connections may need to be pruned after training\nto fit onto the tile resources, leading to a loss in model quality, e.g.,\naccuracy. In this work, we propose a novel unrolling technique that decomposes\na neuron function with many pre-synaptic connections into a sequence of\nhomogeneous neural units, where each neural unit is a function computation\nnode, with two pre-synaptic connections. This spatial decomposition technique\nsignificantly improves crossbar utilization and retains all pre-synaptic\nconnections, resulting in no loss of the model quality derived from connection\npruning. We integrate the proposed technique within an existing SNN mapping\nframework and evaluate it using machine learning applications on the DYNAP-SE\nstate-of-the-art neuromorphic hardware. Our results demonstrate an average 60%\nlower crossbar requirement, 9x higher synapse utilization, 62% lower wasted\nenergy on the hardware, and between 0.8% and 4.6% increase in model quality.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:04:46 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Balaji", "Adarsha", ""], ["Song", "Shihao", ""], ["Das", "Anup", ""], ["Krichmar", "Jeffrey", ""], ["Dutt", "Nikil", ""], ["Shackleford", "James", ""], ["Kandasamy", "Nagarajan", ""], ["Catthoor", "Francky", ""]]}, {"id": "2009.09388", "submitter": "Altug Sural", "authors": "Altu\\u{g} S\\\"ural, E. G\\\"oksu Sezer, Ertu\\u{g}rul\n  Kola\\u{g}as{\\i}o\\u{g}lu, Veerle Derudder and Kaoutar Bertrand", "title": "Tb/s Polar Successive Cancellation Decoder 16nm ASIC Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an efficient ASIC implementation of successive\ncancellation (SC) decoder for polar codes. SC is a low-complexity depth-first\nsearch decoding algorithm, favorable for beyond-5G applications that require\nextremely high throughput and low power. The ASIC implementation of SC in this\nwork exploits many techniques including pipelining and unrolling to achieve\nTb/s data throughput without compromising power and area metrics. To reduce the\ncomplexity of the implementation, an adaptive log-likelihood ratio (LLR)\nquantization scheme is used. This scheme optimizes bit precision of the\ninternal LLRs within the range of 1-5 bits by considering irregular\npolarization and entropy of LLR distribution in SC decoder. The performance\ncost of this scheme is less than 0.2 dB when the code block length is 1024 bits\nand the payload is 854 bits. Furthermore, some computations in SC take large\nspace with high degree of parallelization while others take longer time steps.\nTo optimize these computations and reduce both memory and latency, register\nreduction/balancing (R-RB) method is used. The final decoder architecture is\ncalled optimized polar SC (OPSC). The post-placement-routing results at 16nm\nFinFet ASIC technology show that OPSC decoder achieves 1.2 Tb/s coded\nthroughput on 0.79 mm$^2$ area with 0.95 pJ/bit energy efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 08:46:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["S\u00fcral", "Altu\u011f", ""], ["Sezer", "E. G\u00f6ksu", ""], ["Kola\u011fas\u0131o\u011flu", "Ertu\u011frul", ""], ["Derudder", "Veerle", ""], ["Bertrand", "Kaoutar", ""]]}, {"id": "2009.09487", "submitter": "Simeon Babatunde", "authors": "Simeon Babatunde, Nirnay Jain, Vishwas Powar", "title": "Long Range Communication on Batteryless Devices", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bulk of the existing Wireless Sensor Network (WSN) nodes are usually battery\npowered, stationary and mostly designed for short distance communication, with\nlittle to no consideration for constrained devices that operate solely on\nharvested energy. On many occasions, batteries and beefy super-capacitors are\nused to power these WSN, but these systems are prone to service-life\ndegradation and current-leakages. Most of the systems implementing super\ncapacitors do not account for leakages after exceeding the charge cycle\nthreshold. Frequent battery maintenance and replacement at scale is\nnon-trivial, labor-intensive and challenging task, especially on sensing nodes\ndeployed in extreme harsh environments with limited human intervention. In this\npaper, we present the technique for achieving Kilometer range communication on\nbatteryless constraint devices by harnessing the capabilities of LoRa\ntechnology.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 18:04:58 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Babatunde", "Simeon", ""], ["Jain", "Nirnay", ""], ["Powar", "Vishwas", ""]]}, {"id": "2009.09603", "submitter": "Ryan Kim", "authors": "Kamil Khan, Sudeep Pasricha, Ryan Gary Kim", "title": "A Survey of Resource Management for Processing-in-Memory and Near-Memory\n  Processing Architectures", "comments": "Accepted to appear in Journal of Low Power Electronics and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to amount of data involved in emerging deep learning and big data\napplications, operations related to data movement have quickly become the\nbottleneck. Data-centric computing (DCC), as enabled by processing-in-memory\n(PIM) and near-memory processing (NMP) paradigms, aims to accelerate these\ntypes of applications by moving the computation closer to the data. Over the\npast few years, researchers have proposed various memory architectures that\nenable DCC systems, such as logic layers in 3D stacked memories or charge\nsharing based bitwise operations in DRAM. However, application-specific memory\naccess patterns, power and thermal concerns, memory technology limitations, and\ninconsistent performance gains complicate the offloading of computation in DCC\nsystems. Therefore, designing intelligent resource management techniques for\ncomputation offloading is vital for leveraging the potential offered by this\nnew paradigm. In this article, we survey the major trends in managing PIM and\nNMP-based DCC systems and provide a review of the landscape of resource\nmanagement techniques employed by system designers for such systems.\nAdditionally, we discuss the future challenges and opportunities in DCC\nmanagement.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 04:10:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Khan", "Kamil", ""], ["Pasricha", "Sudeep", ""], ["Kim", "Ryan Gary", ""]]}, {"id": "2009.09787", "submitter": "Ali Mahani", "authors": "Mahdi Taheri, Hamed Zandevakili and Ali Mahani", "title": "A high-performance MEMRISTOR-based Smith-Waterman DNA sequence alignment\n  Using FPNI structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to present a new re-configuration sequencing method for\ndifference of read lengths that may take place as input data in which is\ncrucial drawbacks lay impact on DNA sequencing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 12:06:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Taheri", "Mahdi", ""], ["Zandevakili", "Hamed", ""], ["Mahani", "Ali", ""]]}, {"id": "2009.10443", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Francesco Sgherzi, Marco D. Santambrogio", "title": "A reduced-precision streaming SpMV architecture for Personalized\n  PageRank on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-vector multiplication is often employed in many data-analytic\nworkloads in which low latency and high throughput are more valuable than exact\nnumerical convergence. FPGAs provide quick execution times while offering\nprecise control over the accuracy of the results thanks to reduced-precision\nfixed-point arithmetic. In this work, we propose a novel streaming\nimplementation of Coordinate Format (COO) sparse matrix-vector multiplication,\nand study its effectiveness when applied to the Personalized PageRank\nalgorithm, a common building block of recommender systems in e-commerce\nwebsites and social networks. Our implementation achieves speedups up to 6x\nover a reference floating-point FPGA architecture and a state-of-the-art\nmulti-threaded CPU implementation on 8 different data-sets, while preserving\nthe numerical fidelity of the results and reaching up to 42x higher energy\nefficiency compared to the CPU implementation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 10:44:46 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Parravicini", "Alberto", ""], ["Sgherzi", "Francesco", ""], ["Santambrogio", "Marco D.", ""]]}, {"id": "2009.10656", "submitter": "Franyell Silfa", "authors": "Franyell Silfa, Jose Maria Arnau, and Antonio Gonzalez", "title": "E-BATCH: Energy-Efficient and High-Throughput RNN Batching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) inference exhibits low hardware utilization\ndue to the strict data dependencies across time-steps. Batching multiple\nrequests can increase throughput. However, RNN batching requires a large amount\nof padding since the batched input sequences may largely differ in length.\nSchemes that dynamically update the batch every few time-steps avoid padding.\nHowever, they require executing different RNN layers in a short timespan,\ndecreasing energy efficiency. Hence, we propose E-BATCH, a low-latency and\nenergy-efficient batching scheme tailored to RNN accelerators. It consists of a\nruntime system and effective hardware support. The runtime concatenates\nmultiple sequences to create large batches, resulting in substantial energy\nsavings. Furthermore, the accelerator notifies it when the evaluation of a\nsequence is done, so that a new sequence can be immediately added to a batch,\nthus largely reducing the amount of padding. E-BATCH dynamically controls the\nnumber of time-steps evaluated per batch to achieve the best trade-off between\nlatency and energy efficiency for the given hardware platform. We evaluate\nE-BATCH on top of E-PUR and TPU. In E-PUR, E-BATCH improves throughput by 1.8x\nand energy-efficiency by 3.6x, whereas in TPU, it improves throughput by 2.1x\nand energy-efficiency by 1.6x, over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 16:22:23 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Silfa", "Franyell", ""], ["Arnau", "Jose Maria", ""], ["Gonzalez", "Antonio", ""]]}, {"id": "2009.10976", "submitter": "Dingqing Yang", "authors": "Dingqing Yang, Amin Ghasemazar, Xiaowei Ren, Maximilian Golub, Guy\n  Lemieux, Mieszko Lis", "title": "Procrustes: a Dataflow and Accelerator for Sparse Deep Neural Network\n  Training", "comments": "Appears in the Proceedings of the 53$^\\mathit{rd}$ IEEE/ACM\n  International Symposium on Microarchitecture (MICRO 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of DNN pruning has led to the development of energy-efficient\ninference accelerators that support pruned models with sparse weight and\nactivation tensors. Because the memory layouts and dataflows in these\narchitectures are optimized for the access patterns during\n$\\mathit{inference}$, however, they do not efficiently support the emerging\nsparse $\\mathit{training}$ techniques.\n  In this paper, we demonstrate (a) that accelerating sparse training requires\na co-design approach where algorithms are adapted to suit the constraints of\nhardware, and (b) that hardware for sparse DNN training must tackle constraints\nthat do not arise in inference accelerators. As proof of concept, we adapt a\nsparse training algorithm to be amenable to hardware acceleration; we then\ndevelop dataflow, data layout, and load-balancing techniques to accelerate it.\n  The resulting system is a sparse DNN training accelerator that produces\npruned models with the same accuracy as dense models without first training,\nthen pruning, and finally retraining, a dense model. Compared to training the\nequivalent unpruned models using a state-of-the-art DNN accelerator without\nsparse training support, Procrustes consumes up to 3.26$\\times$ less energy and\noffers up to 4$\\times$ speedup across a range of models, while pruning weights\nby an order of magnitude and maintaining unpruned accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 07:39:55 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yang", "Dingqing", ""], ["Ghasemazar", "Amin", ""], ["Ren", "Xiaowei", ""], ["Golub", "Maximilian", ""], ["Lemieux", "Guy", ""], ["Lis", "Mieszko", ""]]}, {"id": "2009.11389", "submitter": "Yuze Chi", "authors": "Yuze Chi, Licheng Guo, Jason Lau, Young-kyu Choi, Jie Wang and Jason\n  Cong", "title": "Extending High-Level Synthesis for Task-Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C/C++/OpenCL-based high-level synthesis (HLS) becomes more and more popular\nfor field-programmable gate array (FPGA) accelerators in many application\ndomains in recent years, thanks to its competitive quality of results (QoR) and\nshort development cycles compared with the traditional register-transfer level\ndesign approach. Yet, limited by the sequential C semantics, it remains\nchallenging to adopt the same highly productive high-level programming approach\nin many other application domains, where coarse-grained tasks run in parallel\nand communicate with each other at a fine-grained level. While current HLS\ntools do support task-parallel programs, the productivity is greatly limited\n(1) in the code development cycle due to the poor programmability, (2) in the\ncorrectness verification cycle due to restricted software simulation, and (3)\nin the QoR tuning cycle due to slow code generation. Such limited productivity\noften defeats the purpose of HLS and hinder programmers from adopting HLS for\ntask-parallel FPGA accelerators.\n  In this paper, we extend the HLS C++ language and present a fully automated\nframework with programmer-friendly interfaces, unconstrained software\nsimulation, and fast hierarchical code generation to overcome these limitations\nand demonstrate how task-parallel programs can be productively supported in\nHLS. Experimental results based on a wide range of real-world task-parallel\nprograms show that, on average, the lines of kernel and host code are reduced\nby 22% and 51%, respectively, which considerably improves the programmability.\nThe correctness verification and the iterative QoR tuning cycles are both\ngreatly shortened by 3.2x and 6.8x, respectively. Our work is open-source at\nhttps://github.com/UCLA-VAST/tapa/.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 21:35:39 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 03:26:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chi", "Yuze", ""], ["Guo", "Licheng", ""], ["Lau", "Jason", ""], ["Choi", "Young-kyu", ""], ["Wang", "Jie", ""], ["Cong", "Jason", ""]]}, {"id": "2009.11442", "submitter": "Kyle Kuan", "authors": "Kyle Kuan and Tosiron Adegbija", "title": "A Study of Runtime Adaptive Prefetching for STTRAM L1 Caches", "comments": "To appear in IEEE International Conference on Computer Design (ICCD),\n  October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spin-Transfer Torque RAM (STTRAM) is a promising alternative to SRAM in\non-chip caches due to several advantages. These advantages include\nnon-volatility, low leakage, high integration density, and CMOS compatibility.\nPrior studies have shown that relaxing and adapting the STTRAM retention time\nto runtime application needs can substantially reduce overall cache energy\nwithout significant latency overheads, due to the lower STTRAM write energy and\nlatency in shorter retention times. In this paper, as a first step towards\nefficient prefetching across the STTRAM cache hierarchy, we study prefetching\nin reduced retention STTRAM L1 caches. Using SPEC CPU 2017 benchmarks, we\nanalyze the energy and latency impact of different prefetch distances in\ndifferent STTRAM cache retention times for different applications. We show that\nexpired_unused_prefetches---the number of unused prefetches expired by the\nreduced retention time STTRAM cache---can accurately determine the best\nretention time for energy consumption and access latency. This new metric can\nalso provide insights into the best prefetch distance for memory bandwidth\nconsumption and prefetch accuracy. Based on our analysis and insights, we\npropose Prefetch-Aware Retention time Tuning (PART) and Retention time-based\nPrefetch Control (RPC). Compared to a base STTRAM cache, PART and RPC\ncollectively reduced the average cache energy and latency by 22.24% and 24.59%,\nrespectively. When the base architecture was augmented with the\nstate-of-the-art near-side prefetch throttling (NST), PART+RPC reduced the\naverage cache energy and latency by 3.50% and 3.59%, respectively, and reduced\nthe hardware overhead by 54.55%\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 01:37:02 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kuan", "Kyle", ""], ["Adegbija", "Tosiron", ""]]}, {"id": "2009.11621", "submitter": "Cemil Cem Gursoy", "authors": "C. C. Gursoy (1), M. Jenihhin (1), A. S. Oyeniran (1), D. Piumatti\n  (2), J. Raik (1), M. Sonza Reorda (2), R. Ubar (1) ((1) Tallinn University of\n  Technology - Tallinn, Estonia, (2) Politecnico di Torino, Dip. Automatica e\n  Informatica - Torino, Italy)", "title": "New categories of Safe Faults in a processor-based Embedded System", "comments": "2019 IEEE 22nd International Symposium on Design and Diagnostics of\n  Electronic Circuits & Systems (DDECS)", "journal-ref": null, "doi": "10.1109/DDECS.2019.8724642", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of safe faults (i.e., faults which are guaranteed not to\nproduce any failure) in an electronic system is a crucial step when analyzing\nits dependability and its test plan development. Unfortunately, safe fault\nidentification is poorly supported by available EDA tools, and thus remains an\nopen problem. The complexity growth of modern systems used in safety-critical\napplications further complicates their identification. In this article, we\nidentify some classes of safe faults within an embedded system based on a\npipelined processor. A new method for automating the safe fault identification\nis also proposed. The safe faults belonging to each class are identified\nresorting to Automatic Test Pattern Generation (ATPG) techniques. The proposed\nmethodology is applied to a sample system built around the OpenRisc1200 open\nsource processor.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 11:58:51 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Gursoy", "C. C.", ""], ["Jenihhin", "M.", ""], ["Oyeniran", "A. S.", ""], ["Piumatti", "D.", ""], ["Raik", "J.", ""], ["Reorda", "M. Sonza", ""], ["Ubar", "R.", ""]]}, {"id": "2009.12495", "submitter": "Xiaobing Chen", "authors": "Xiaobing Chen, Yuke Wang, Xinfeng Xie, Xing Hu, Abanti Basak, Ling\n  Liang, Mingyu Yan, Lei Deng, Yufei Ding, Zidong Du, Yunji Chen, Yuan Xie", "title": "Rubik: A Hierarchical Architecture for Efficient Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional network (GCN) emerges as a promising direction to learn\nthe inductive representation in graph data commonly used in widespread\napplications, such as E-commerce, social networks, and knowledge graphs.\nHowever, learning from graphs is non-trivial because of its mixed computation\nmodel involving both graph analytics and neural network computing. To this end,\nwe decompose the GCN learning into two hierarchical paradigms: graph-level and\nnode-level computing. Such a hierarchical paradigm facilitates the software and\nhardware accelerations for GCN learning.\n  We propose a lightweight graph reordering methodology, incorporated with a\nGCN accelerator architecture that equips a customized cache design to fully\nutilize the graph-level data reuse. We also propose a mapping methodology aware\nof data reuse and task-level parallelism to handle various graphs inputs\neffectively. Results show that Rubik accelerator design improves energy\nefficiency by 26.3x to 1375.2x than GPU platforms across different datasets and\nGCN models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 01:20:37 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chen", "Xiaobing", ""], ["Wang", "Yuke", ""], ["Xie", "Xinfeng", ""], ["Hu", "Xing", ""], ["Basak", "Abanti", ""], ["Liang", "Ling", ""], ["Yan", "Mingyu", ""], ["Deng", "Lei", ""], ["Ding", "Yufei", ""], ["Du", "Zidong", ""], ["Chen", "Yunji", ""], ["Xie", "Yuan", ""]]}, {"id": "2009.12617", "submitter": "Lawrence Stewart", "authors": "Lawrence C. Stewart and Carlo Pascoe and Brian W. Sherman and Martin\n  Herbordt and Vipin Sachdeva", "title": "Particle Mesh Ewald for Molecular Dynamics in OpenCL on an FPGA Cluster", "comments": "Accepted as a poster at FCCM21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular Dynamics (MD) simulations play a central role in physics-driven\ndrug discovery. MD applications often use the Particle Mesh Ewald (PME)\nalgorithm to accelerate electrostatic force computations, but efficient\nparallelization has proven difficult due to the high communication requirements\nof distributed 3D FFTs. In this paper, we present the design and implementation\nof a scalable PME algorithm that runs on a cluster of Intel Stratix 10 FPGAs\nand can handle FFT sizes appropriate to address real-world drug discovery\nprojects (grids up to $128^3$). To our knowledge, this is the first work to\nfully integrate all aspects of the PME algorithm (charge spreading, 3D\nFFT/IFFT, and force interpolation) within a distributed FPGA framework. The\ndesign is fully implemented with OpenCL for flexibility and ease of development\nand uses 100 Gbps links for direct FPGA-to-FPGA communications without the need\nfor host interaction. We present experimental data up to 4 FPGAs (e.g., 206\nmicroseconds per timestep for a 65536 atom simulation and $64^3$ 3D FFT),\noutperforming GPUs. Additionally, we discuss design scalability on clusters\nwith differing topologies up to 64 FPGAs (with expected performance greater\nthan all known GPU implementations) and integration with other hardware\ncomponents to form a complete molecular dynamics application. We predict\nbest-case performance of 6.6 microseconds per timestep on 64 FPGAs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 15:12:20 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:44:38 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 02:53:34 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 15:13:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Stewart", "Lawrence C.", ""], ["Pascoe", "Carlo", ""], ["Sherman", "Brian W.", ""], ["Herbordt", "Martin", ""], ["Sachdeva", "Vipin", ""]]}, {"id": "2009.13443", "submitter": "Mahmoud Yassien Shams El Den", "authors": "Amira. A. Elsonbaty and Mahmoud Shams", "title": "The Smart Parking Management System", "comments": "12 pages, 15 figures", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 12, No 4, August 2020", "doi": "10.5121/ijcsit.2020.12405", "report-no": null, "categories": "cs.CY cs.AR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With growing, Car parking increases with the number of car users. With the\nincreased use of smartphones and their applications, users prefer mobile\nphone-based solutions. This paper proposes the Smart Parking Management System\n(SPMS) that depends on Arduino parts, Android applications, and based on IoT.\nThis gave the client the ability to check available parking spaces and reserve\na parking spot. IR sensors are utilized to know if a car park space is allowed.\nIts area data are transmitted using the WI-FI module to the server and are\nrecovered by the mobile application which offers many options attractively and\nwith no cost to users and lets the user check reservation details. With IoT\ntechnology, the smart parking system can be connected wirelessly to easily\ntrack available locations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 16:08:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Elsonbaty", "Amira. A.", ""], ["Shams", "Mahmoud", ""]]}, {"id": "2009.13664", "submitter": "Eugene Tam", "authors": "Eugene Tam, Shenfei Jiang, Paul Duan, Shawn Meng, Yue Pang, Cayden\n  Huang, Yi Han, Jacke Xie, Yuanjun Cui, Jinsong Yu, Minggui Lu", "title": "Breaking the Memory Wall for AI Chip with a New Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep learning have led to the widespread adoption of\nartificial intelligence (AI) in applications such as computer vision and\nnatural language processing. As neural networks become deeper and larger, AI\nmodeling demands outstrip the capabilities of conventional chip architectures.\nMemory bandwidth falls behind processing power. Energy consumption comes to\ndominate the total cost of ownership. Currently, memory capacity is\ninsufficient to support the most advanced NLP models. In this work, we present\na 3D AI chip, called Sunrise, with near-memory computing architecture to\naddress these three challenges. This distributed, near-memory computing\narchitecture allows us to tear down the performance-limiting memory wall with\nan abundance of data bandwidth. We achieve the same level of energy efficiency\non 40nm technology as competing chips on 7nm technology. By moving to similar\ntechnologies as other AI chips, we project to achieve more than ten times the\nenergy efficiency, seven times the performance of the current state-of-the-art\nchips, and twenty times of memory capacity as compared with the best chip in\neach benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 22:34:10 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Tam", "Eugene", ""], ["Jiang", "Shenfei", ""], ["Duan", "Paul", ""], ["Meng", "Shawn", ""], ["Pang", "Yue", ""], ["Huang", "Cayden", ""], ["Han", "Yi", ""], ["Xie", "Jacke", ""], ["Cui", "Yuanjun", ""], ["Yu", "Jinsong", ""], ["Lu", "Minggui", ""]]}, {"id": "2009.13765", "submitter": "EPTCS", "authors": "Mertcan Temel (University of Texas at Austin)", "title": "RP-Rewriter: An Optimized Rewriter for Large Terms in ACL2", "comments": "In Proceedings ACL2 2020, arXiv:2009.12521", "journal-ref": "EPTCS 327, 2020, pp. 61-74", "doi": "10.4204/EPTCS.327.5", "report-no": null, "categories": "cs.LO cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RP-Rewriter (Retain-Property) is a verified clause processor that can use\nsome of the existing ACL2 rewrite rules to prove conjectures through term\nrewriting. Optimized for conjectures that can expand into large terms, the\nrewriter tries to mimic some of the ACL2 rewriting heuristics but also adds\nsome extra features. It can attach side-conditions to terms that help the\nrewriter retain properties about them and prevent possibly some very expensive\nbackchaining. The rewriter supports user-defined complex meta rules that can\nreturn a special structure to prevent redundant rewriting. Additionally, it can\nstore fast alists even when values are not quoted. RP-Rewriter is utilized for\ntwo applications, multiplier design proofs and SVEX simplification, which\ninvolve very large terms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 04:10:41 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Temel", "Mertcan", "", "University of Texas at Austin"]]}, {"id": "2009.13903", "submitter": "Georg Hager", "authors": "Christie L. Alappat, Jan Laukemann, Thomas Gruber, Georg Hager,\n  Gerhard Wellein, Nils Meyer, Tilo Wettig", "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector\n  Multiplication on A64FX", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The A64FX CPU powers the current number one supercomputer on the Top500 list.\nAlthough it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. Generating\nefficient code for such a new architecture requires a good understanding of its\nperformance features. Using these features, we construct the\nExecution-Cache-Memory (ECM) performance model for the A64FX processor in the\nFX700 supercomputer and validate it using streaming loops. We also identify\narchitectural peculiarities and derive optimization hints. Applying the ECM\nmodel to sparse matrix-vector multiplication (SpMV), we motivate why the CRS\nmatrix storage format is inappropriate and how the SELL-C-sigma format with\nsuitable code optimizations can achieve bandwidth saturation for SpMV.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 09:52:59 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Alappat", "Christie L.", ""], ["Laukemann", "Jan", ""], ["Gruber", "Thomas", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Meyer", "Nils", ""], ["Wettig", "Tilo", ""]]}, {"id": "2009.14381", "submitter": "Atefeh Sohrabizadeh", "authors": "Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong", "title": "AutoDSE: Enabling Software Programmers Design Efficient FPGA\n  Accelerators", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adopting FPGA as an accelerator in datacenters is becoming mainstream for\ncustomized computing, but the fact that FPGAs are hard to program creates a\nsteep learning curve for software programmers. Even with the help of high-level\nsynthesis (HLS), accelerator designers still have to manually perform code\nreconstruction and cumbersome parameter tuning to achieve the optimal\nperformance. While many learning models have been leveraged by existing work to\nautomate the design of efficient accelerators, the unpredictability of modern\nHLS tools becomes a major obstacle for them to maintain high accuracy. In this\npaper, we address this problem by incorporating an automated DSE\nframework-AutoDSE- that leverages bottleneck-guided gradient optimizer to\nsystematically find abetter design point. AutoDSE finds the bottleneck of the\ndesign in each step and focuses on high-impact parameters to overcome that,\nwhich is similar to the approach an expert would take. The experimental results\nshow that AutoDSE is able to find the design point that achieves, on the\ngeometric mean, 19.9x speedup over one CPU core for Machsuite and Rodinia\nbenchmarks and 1.04x over the manually designed HLS accelerated vision kernels\nin Xilinx Vitis libraries yet with 26x reduction of their optimization pragmas.\nWith less than one optimization pragma per design on average, we are making\nprogress towards democratizing customizable computing by enabling software\nprogrammers to design efficient FPGA accelerators.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:38:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sohrabizadeh", "Atefeh", ""], ["Yu", "Cody Hao", ""], ["Gao", "Min", ""], ["Cong", "Jason", ""]]}, {"id": "2009.14469", "submitter": "Tianyue Lu", "authors": "Tianyue Lu, Haiyang Pan, Mingyu Chen", "title": "System measurement of Intel AEP Optane DIMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, memory wall has been a great performance bottleneck of\ncomputer system. To overcome it, Non-Volatile Main Memory (NVMM) technology has\nbeen discussed widely to provide a much larger main memory capacity. Last year,\nIntel released AEP Optane DIMM, which provides hundreds of GB capacity as a\npromising replacement of traditional DRAM memory. But as most key parameters of\nAEP is not open to users, there is a need to get to know them because they will\nguide a direction of further NVMM research. In this paper, we focus on\nmeasuring performance and architecture features of AEP DIMM. Together, we\nexplore the design of DRAM cache which is an important part of DRAM-AEP hybrid\nmemory system. As a result, we estimate the write latency of AEP DIMM which has\nnot been measured accurately. And, we discover the current design parameters of\nDRAM cache, such as tag organization, cache associativity and set index\nmapping. All of these features are first published on academic paper which are\ngreatly helpful to future NVMM optimizations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 06:37:55 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Lu", "Tianyue", ""], ["Pan", "Haiyang", ""], ["Chen", "Mingyu", ""]]}, {"id": "2009.14685", "submitter": "\\\"Omer Faruk Irmak", "authors": "\\\"Omer Faruk Irmak, Arda Yurdakul", "title": "An Embedded RISC-V Core with Fast Modular Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest concerns in IoT is privacy and security. Encryption and\nauthentication need big power budgets, which battery-operated IoT end-nodes do\nnot have. Hardware accelerators designed for specific cryptographic operations\nprovide little to no flexibility for future updates. Custom instruction\nsolutions are smaller in area and provide more flexibility for new methods to\nbe implemented. One drawback of custom instructions is that the processor has\nto wait for the operation to finish. Eventually, the response time of the\ndevice to real-time events gets longer. In this work, we propose a processor\nwith an extended custom instruction for modular multiplication, which blocks\nthe processor, typically, two cycles for any size of modular multiplication\nwhen used in Partial Execution mode. We adopted embedded and compressed\nextensions of RISC-V for our proof-of-concept CPU. Our design is benchmarked on\nrecent cryptographic algorithms in the field of elliptic-curve cryptography.\nOur CPU with 128-bit modular multiplication operates at 136MHz on ASIC and\n81MHz on FPGA. It achieves up to 13x speed up on software implementations while\nreducing overall power consumption by up to 95\\% with 41\\% average area\noverhead over our base architecture.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:02:36 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Irmak", "\u00d6mer Faruk", ""], ["Yurdakul", "Arda", ""]]}]