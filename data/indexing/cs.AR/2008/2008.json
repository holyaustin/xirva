[{"id": "2008.00023", "submitter": "Gregory D. Hager", "authors": "Gregory D. Hager, Mark D. Hill, and Katherine Yelick", "title": "Opportunities and Challenges for Next Generation Computing", "comments": "A Computing Community Consortium (CCC) white paper, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing has dramatically changed nearly every aspect of our lives, from\nbusiness and agriculture to communication and entertainment. As a nation, we\nrely on computing in the design of systems for energy, transportation and\ndefense; and computing fuels scientific discoveries that will improve our\nfundamental understanding of the world and help develop solutions to major\nchallenges in health and the environment. Computing has changed our world, in\npart, because our innovations can run on computers whose performance and\ncost-performance has improved a million-fold over the last few decades. A\ndriving force behind this has been a repeated doubling of the transistors per\nchip, dubbed Moore's Law. A concomitant enabler has been Dennard Scaling that\nhas permitted these performance doublings at roughly constant power, but, as we\nwill see, both trends face challenges. Consider for a moment the impact of\nthese two trends over the past 30 years. A 1980's supercomputer (e.g. a Cray 2)\nwas rated at nearly 2 Gflops and consumed nearly 200 KW of power. At the time,\nit was used for high performance and national-scale applications ranging from\nweather forecasting to nuclear weapons research. A computer of similar\nperformance now fits in our pocket and consumes less than 10 watts. What would\nbe the implications of a similar computing/power reduction over the next 30\nyears - that is, taking a petaflop-scale machine (e.g. the Cray XK7 which\nrequires about 500 KW for 1 Pflop (=1015 operations/sec) performance) and\nrepeating that process? What is possible with such a computer in your pocket?\nHow would it change the landscape of high capacity computing? In the remainder\nof this paper, we articulate some opportunities and challenges for dramatic\nperformance improvements of both personal to national scale computing, and\ndiscuss some \"out of the box\" possibilities for achieving computing at this\nscale.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 18:16:49 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hager", "Gregory D.", ""], ["Hill", "Mark D.", ""], ["Yelick", "Katherine", ""]]}, {"id": "2008.00062", "submitter": "Marie Nguyen", "authors": "Marie Nguyen, Nathan Serafin and James C. Hoe", "title": "Partial Reconfiguration for Design Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGA designers have traditionally shared a similar design methodology with\nASIC designers. Most notably, at design time, FPGA designers commit to a fixed\nallocation of logic resources to modules in a design. At runtime, some of the\noccupied resources could be left idle or under-utilized due to hard-to-avoid\nsources of inefficiencies (e.g., operation dependencies). With partial\nreconfiguration (PR), FPGA resources can be re-allocated over time. Therefore,\nusing PR, a designer can attempt to reduce idleness and under-utilization with\nbetter area-time scheduling.\n  In this paper, we explain when, how, and why PR-style designs can improve\nover the performance-area Pareto front of ASIC-style designs (without PR). We\nfirst introduce the concept of area-time volume to explain why PR-style designs\ncan improve upon ASIC-style designs. We identify resource under-utilization as\nan opportunity that can be exploited by PR-style designs. We then present a\nfirst-order analytical model to help a designer decide if a PR-style design can\nbe beneficial. When it is the case, the model points to the most suitable PR\nexecution strategy and provides an estimate of the improvement. The model is\nvalidated in three case studies.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 20:07:15 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nguyen", "Marie", ""], ["Serafin", "Nathan", ""], ["Hoe", "James C.", ""]]}, {"id": "2008.00095", "submitter": "Bryan Donyanavard", "authors": "Bryan Donyanavard, Amir M. Rahmani, Axel Jantsch, Onur Mutlu, and\n  Nikil Dutt", "title": "Intelligent Management of Mobile Systems through Computational\n  Self-Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime resource management for many-core systems is increasingly complex.\nThe complexity can be due to diverse workload characteristics with conflicting\ndemands, or limited shared resources such as memory bandwidth and power.\nResource management strategies for many-core systems must distribute shared\nresource(s) appropriately across workloads, while coordinating the high-level\nsystem goals at runtime in a scalable and robust manner.\n  To address the complexity of dynamic resource management in many-core\nsystems, state-of-the-art techniques that use heuristics have been proposed.\nThese methods lack the formalism in providing robustness against unexpected\nruntime behavior. One of the common solutions for this problem is to deploy\nclassical control approaches with bounds and formal guarantees. Traditional\ncontrol theoretic methods lack the ability to adapt to (1) changing goals at\nruntime (i.e., self-adaptivity), and (2) changing dynamics of the modeled\nsystem (i.e., self-optimization).\n  In this chapter, we explore adaptive resource management techniques that\nprovide self-optimization and self-adaptivity by employing principles of\ncomputational self-awareness, specifically reflection. By supporting these\nself-awareness properties, the system can reason about the actions it takes by\nconsidering the significance of competing objectives, user requirements, and\noperating conditions while executing unpredictable workloads.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 21:40:39 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Donyanavard", "Bryan", ""], ["Rahmani", "Amir M.", ""], ["Jantsch", "Axel", ""], ["Mutlu", "Onur", ""], ["Dutt", "Nikil", ""]]}, {"id": "2008.00171", "submitter": "Vamsee Reddy Kommareddy", "authors": "Vamsee Reddy Kommareddy, Clayton Hughes, Simon David Hammond and Amro\n  Awad", "title": "DeACT: Architecture-Aware Virtual Memory Support for Fabric Attached\n  Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth of data has driven technology providers to develop new\nprotocols, such as cache coherent interconnects and memory semantic fabrics, to\nhelp users and facilities leverage advances in memory technologies to satisfy\nthese growing memory and storage demands. Using these new protocols,\nfabric-attached memories (FAM) can be directly attached to a system\ninterconnect and be easily integrated with a variety of processing elements\n(PEs). Moreover, systems that support FAM can be smoothly upgraded and allow\nmultiple PEs to share the FAM memory pools using well-defined protocols. The\nsharing of FAM between PEs allows efficient data sharing, improves memory\nutilization, reduces cost by allowing flexible integration of different PEs and\nmemory modules from several vendors, and makes it easier to upgrade the system.\nOne promising use-case for FAMs is in High-Performance Compute (HPC) systems,\nwhere the underutilization of memory is a major challenge. However, adopting\nFAMs in HPC systems brings new challenges. In addition to cost, flexibility,\nand efficiency, one particular problem that requires rethinking is virtual\nmemory support for security and performance. To address these challenges, this\npaper presents decoupled access control and address translation (DeACT), a\nnovel virtual memory implementation that supports HPC systems equipped with\nFAM. Compared to the state-of-the-art two-level translation approach, DeACT\nachieves speedup of up to 4.59x (1.8x on average) without compromising\nsecurity.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 05:06:32 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kommareddy", "Vamsee Reddy", ""], ["Hughes", "Clayton", ""], ["Hammond", "Simon David", ""], ["Awad", "Amro", ""]]}, {"id": "2008.00176", "submitter": "Furkan Eris", "authors": "Furkan Eris, Sadullah Canakci, Cansu Demirkiran, Ajay Joshi", "title": "Custom Tailored Suite of Random Forests for Prefetcher Adaptation", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To close the gap between memory and processors, and in turn improve\nperformance, there has been an abundance of work in the area of\ndata/instruction prefetcher designs. Prefetchers are deployed in each level of\nthe memory hierarchy, but typically, each prefetcher gets designed without\ncomprehensively accounting for other prefetchers in the system. As a result,\nthese individual prefetcher designs do not always complement each other, and\nthat leads to low average performance gains and/or many negative outliers. In\nthis work, we propose SuitAP (Suite of random forests for Adaptation of\nPrefetcher system configuration), which is a hardware prefetcher adapter that\nuses a suite of random forests to determine at runtime which prefetcher should\nbe ON at each memory level, such that they complement each other. Compared to a\ndesign with no prefetchers, using SuitAP we improve IPC by 46% on average\nacross traces generated from SPEC2017 suite with 12KB overhead. Moreover, we\nalso reduce negative outliers using SuitAP.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 05:43:38 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Eris", "Furkan", ""], ["Canakci", "Sadullah", ""], ["Demirkiran", "Cansu", ""], ["Joshi", "Ajay", ""]]}, {"id": "2008.00211", "submitter": "Shamendra Egodawela Mr.", "authors": "S.M.K.C.S.B. Egodawela, H.M.D.M.B. Herath, R.D. Ranaweera, J.V.\n  Wijayakulasooriya", "title": "Device to Remotely Track and Locate the Position of a Child for Safety", "comments": "6 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parents are always worried about the wellbeing of their children. As per the\nStatistics Report 2017 by Missing Children Europe Organization, a child is\nreported missing every 2 minutes. Due to the imminent threat, parents are prone\nto buy their children mobile phones to keep in touch with them. However, giving\na Mobile phone to a child can cause issues including cyber bullying, improper\nuse of social networks, access to mature age and illicit content on the\ninternet and possibly, phone theft. As an effort to tackle some of those\nissues, this paper proposes a solution which enables parents to call, locate\nand track their children using a child-friendly mobile device. The common\nscenario the device would come to play is in enhancing the safety of a child\nwho would travel alone on a typical route; for instance a child who walks from\nhome to school and back. The device can be calibrated to keep track of a\ntypical route of travel. Then, if the device de-tects some deviation from the\nusual route, it would trigger a notification to parents. A probability matrix\nbased nov-el algorithm is introduced to detect route deviation. De-sign details\nof the mobile device, along with the details of the route deviation detection\nalgorithm are presented in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 08:11:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Egodawela", "S. M. K. C. S. B.", ""], ["Herath", "H. M. D. M. B.", ""], ["Ranaweera", "R. D.", ""], ["Wijayakulasooriya", "J. V.", ""]]}, {"id": "2008.00216", "submitter": "Aneeqa Fatima", "authors": "Aneeqa Fatima and Igor L. Markov", "title": "Faster Schr\\\"odinger-style simulation of quantum circuits", "comments": "14 pages, 15 figures, 4 tables. Version 2 : Additional optimizations;\n  improved simulation runtimes; profiling data; comparisons with the latest IBM\n  QISKit simulator; dispelled apparent limitations of techniques. Version 3 :\n  Ablation experiments and images for the code snippets", "journal-ref": "HPCA 2021", "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.DC cs.ET physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent demonstrations of superconducting quantum computers by Google and IBM\nand trapped-ion computers from IonQ fueled new research in quantum algorithms,\ncompilation into quantum circuits, and empirical algorithmics. While online\naccess to quantum hardware remains too limited to meet the demand, simulating\nquantum circuits on conventional computers satisfies many needs. We advance\nSchr\\\"odinger-style simulation of quantum circuits that is useful standalone\nand as a building block in layered simulation algorithms, both cases are\nillustrated in our results. Our algorithmic contributions show how to simulate\nmultiple quantum gates at once, how to avoid floating-point multiplies, how to\nbest use instruction-level and thread-level parallelism as well as CPU cache,\nand how to leverage these optimizations by reordering circuit gates. While not\ndescribed previously, these techniques implemented by us supported published\nhigh-performance distributed simulations up to 64 qubits. To show additional\nimpact, we benchmark our simulator against Microsoft, IBM and Google simulators\non hard circuits from Google.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 08:47:24 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 06:49:12 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 18:20:50 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Fatima", "Aneeqa", ""], ["Markov", "Igor L.", ""]]}, {"id": "2008.00329", "submitter": "Christina Delimitrou", "authors": "Neeraj Kulkarni, Gonzalo Gonzalez-Pumariega, Amulya Khurana, Christine\n  Shoemaker, Christina Delimitrou, and David Albonesi", "title": "CuttleSys: Data-Driven Resource Management forInteractive Applications\n  on Reconfigurable Multicores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-tenancy for latency-critical applications leads to re-source\ninterference and unpredictable performance. Core reconfiguration opens up more\nopportunities for colocation,as it allows the hardware to adjust to the dynamic\nperformance and power needs of a specific mix of co-scheduled applications.\nHowever, reconfigurability also introduces challenges, as even for a small\nnumber of reconfigurable cores, exploring the design space becomes more time-\nand resource-demanding.\n  We present CuttleSys, a runtime for reconfigurable multi-cores that leverages\nscalable and lightweight data mining to quickly identify suitable core and\ncache configurations for a set of co-scheduled applications. The runtime\ncombines collaborative filtering to infer the behavior of each job on every\ncore and cache configuration, with Dynamically Dimensioned Search to\nefficiently explore the configuration space. We evaluate CuttleSys on\nmulticores with tens of reconfigurable cores and show up to 2.46x and 1.55x\nperformance improvements compared to core-level gating and oracle-like\nasymmetric multicores respectively, under stringent power constraints.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 20:00:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Kulkarni", "Neeraj", ""], ["Gonzalez-Pumariega", "Gonzalo", ""], ["Khurana", "Amulya", ""], ["Shoemaker", "Christine", ""], ["Delimitrou", "Christina", ""], ["Albonesi", "David", ""]]}, {"id": "2008.00638", "submitter": "Dibakar Gope", "authors": "Dibakar Gope, Jesse Beu, Matthew Mattina", "title": "High Throughput Matrix-Matrix Multiplication between Asymmetric\n  Bit-Width Operands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplications between asymmetric bit-width operands, especially\nbetween 8- and 4-bit operands are likely to become a fundamental kernel of many\nimportant workloads including neural networks and machine learning. While\nexisting SIMD matrix multiplication instructions for symmetric bit-width\noperands can support operands of mixed precision by zero- or sign-extending the\nnarrow operand to match the size of the other operands, they cannot exploit the\nbenefit of narrow bit-width of one of the operands. We propose a new SIMD\nmatrix multiplication instruction that uses mixed precision on its inputs (8-\nand 4-bit operands) and accumulates product values into narrower 16-bit output\naccumulators, in turn allowing the SIMD operation at 128-bit vector width to\nprocess a greater number of data elements per instruction to improve processing\nthroughput and memory bandwidth utilization without increasing the register\nread- and write-port bandwidth in CPUs. The proposed asymmetric-operand-size\nSIMD instruction offers 2x improvement in throughput of matrix multiplication\nin comparison to throughput obtained using existing symmetric-operand-size\ninstructions while causing negligible (0.05%) overflow from 16-bit accumulators\nfor representative machine learning workloads. The asymmetric-operand-size\ninstruction not only can improve matrix multiplication throughput in CPUs, but\nalso can be effective to support multiply-and-accumulate (MAC) operation\nbetween 8- and 4-bit operands in state-of-the-art DNN hardware accelerators\n(e.g., systolic array microarchitecture in Google TPU, etc.) and offer similar\nimprovement in matrix multiply performance seamlessly without violating the\nvarious implementation constraints. We demonstrate how a systolic array\narchitecture designed for symmetric-operand-size instructions could be modified\nto support an asymmetric-operand-sized instruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 04:12:31 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gope", "Dibakar", ""], ["Beu", "Jesse", ""], ["Mattina", "Matthew", ""]]}, {"id": "2008.00806", "submitter": "Guojin Chen", "authors": "Guojin Chen, Wanli Chen, Yuzhe Ma, Haoyu Yang and Bei Yu", "title": "DAMO: Deep Agile Mask Optimization for Full Chip Scale", "comments": null, "journal-ref": null, "doi": "10.1145/3400302.3415705", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous scaling of the VLSI system leaves a great challenge on\nmanufacturing and optical proximity correction (OPC) is widely applied in\nconventional design flow for manufacturability optimization. Traditional\ntechniques conducted OPC by leveraging a lithography model and suffered from\nprohibitive computational overhead, and mostly focused on optimizing a single\nclip without addressing how to tackle the full chip. In this paper, we present\nDAMO, a high performance and scalable deep learning-enabled OPC system for full\nchip scale. It is an end-to-end mask optimization paradigm which contains a\nDeep Lithography Simulator (DLS) for lithography modeling and a Deep Mask\nGenerator (DMG) for mask pattern generation. Moreover, a novel layout splitting\nalgorithm customized for DAMO is proposed to handle the full chip OPC problem.\nExtensive experiments show that DAMO outperforms the state-of-the-art OPC\nsolutions in both academia and industrial commercial toolkit.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:28:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 10:08:58 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chen", "Guojin", ""], ["Chen", "Wanli", ""], ["Ma", "Yuzhe", ""], ["Yang", "Haoyu", ""], ["Yu", "Bei", ""]]}, {"id": "2008.00961", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Z\\\"ulal Bing\\\"ol, Damla Senol Cali, Jeremie Kim,\n  Saugata Ghose, Can Alkan, Onur Mutlu", "title": "Accelerating Genome Analysis: A Primer on an Ongoing Journey", "comments": "This is an extended and updated version of a paper published in IEEE\n  Micro, vol. 40, no. 5, pp. 65-75, 1 Sept.-Oct. 2020,\n  https://doi.org/10.1109/MM.2020.3013728", "journal-ref": "IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 1 2020", "doi": "10.1109/MM.2020.3013728", "report-no": null, "categories": "cs.AR q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome analysis fundamentally starts with a process known as read mapping,\nwhere sequenced fragments of an organism's genome are compared against a\nreference genome. Read mapping is currently a major bottleneck in the entire\ngenome analysis pipeline, because state-of-the-art genome sequencing\ntechnologies are able to sequence a genome much faster than the computational\ntechniques employed to analyze the genome. We describe the ongoing journey in\nsignificantly improving the performance of read mapping. We explain\nstate-of-the-art algorithmic methods and hardware-based acceleration\napproaches. Algorithmic approaches exploit the structure of the genome as well\nas the structure of the underlying hardware. Hardware-based acceleration\napproaches exploit specialized microarchitectures or various execution\nparadigms (e.g., processing inside or near memory). We conclude with the\nchallenges of adopting these hardware-accelerated read mappers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:27:31 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 08:37:43 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Alser", "Mohammed", ""], ["Bing\u00f6l", "Z\u00fclal", ""], ["Cali", "Damla Senol", ""], ["Kim", "Jeremie", ""], ["Ghose", "Saugata", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "2008.01219", "submitter": "Haoqiang Guo", "authors": "Haoqiang Guo, Lu Peng, Jian Zhang, Fang Qi, Lide Duan", "title": "Hardware Accelerator for Adversarial Attacks on Deep Learning Neural\n  Networks", "comments": "IGSC'2019 (https://shirazi21.wixsite.com/igsc2019archive) Best paper\n  award", "journal-ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC)", "doi": "10.1109/IGSC48788.2019.8957192", "report-no": null, "categories": "eess.SP cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies identify that Deep learning Neural Networks (DNNs) are\nvulnerable to subtle perturbations, which are not perceptible to human visual\nsystem but can fool the DNN models and lead to wrong outputs. A class of\nadversarial attack network algorithms has been proposed to generate robust\nphysical perturbations under different circumstances. These algorithms are the\nfirst efforts to move forward secure deep learning by providing an avenue to\ntrain future defense networks, however, the intrinsic complexity of them\nprevents their broader usage.\n  In this paper, we propose the first hardware accelerator for adversarial\nattacks based on memristor crossbar arrays. Our design significantly improves\nthe throughput of a visual adversarial perturbation system, which can further\nimprove the robustness and security of future deep learning systems. Based on\nthe algorithm uniqueness, we propose four implementations for the adversarial\nattack accelerator ($A^3$) to improve the throughput, energy efficiency, and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:55:41 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Guo", "Haoqiang", ""], ["Peng", "Lu", ""], ["Zhang", "Jian", ""], ["Qi", "Fang", ""], ["Duan", "Lide", ""]]}, {"id": "2008.01869", "submitter": "Mostafa Darvishi", "authors": "Mostafa Darvishi", "title": "Optimum Reconfiguration of Routing Interconnection Network in APSoC\n  Fabrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated algorithm for optimum configuration of\nrouting interconnection network in Xilinx Zynq-7000 All programmable\nsystem-on-chip (APSoC) fabrics. A method to configure circuits with optimum\nrouting resources is presented along with their performance parameters with and\nwithout the proposed algorithm. The proposed algorithm enables full control\nover routing resources for using different interconnection types in order to\ncreate routing-based circuit-under-test. The algorithm proposes the routing\ntechniques through the 2-D array of switch matrices inside the interconnection\nnetwork and automatically identifies the involved programmable interconnection\npoints associated with a node. An experimental setup is proposed to measure the\nperformance parameters such as slack time and power with and without the\napplied algorithm on the APSoC routing resources. The proposed setup requires\nno external equipment such as manufactured equipments or external instruments\nfor performance measurement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 22:37:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Darvishi", "Mostafa", ""]]}, {"id": "2008.01957", "submitter": "Wei Song", "authors": "Wei Song, Boya Li, Zihan Xue, Zhenzhen Li, Wenhao Wang, Peng Liu", "title": "Randomized Last-Level Caches Are Still Vulnerable to Cache Side-Channel\n  Attacks! But We Can Fix It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cache randomization has recently been revived as a promising defense against\nconflict-based cache side-channel attacks. As two of the latest\nimplementations, CEASER-S and ScatterCache both claim to thwart conflict-based\ncache side-channel attacks using randomized skewed caches. Unfortunately, our\nexperiments show that an attacker can easily find a usable eviction set within\nthe chosen remap period of CEASER-S and increasing the number of partitions\nwithout dynamic remapping, such as ScatterCache, cannot eliminate the threat.\nBy quantitatively analyzing the access patterns left by various attacks in the\nLLC, we have newly discovered several problems with the hypotheses and\nimplementations of randomized caches, which are also overlooked by the research\non conflict-based cache side-channel attack.\n  However, cache randomization is not a false hope and it is an effective\ndefense that should be widely adopted in future processors. The newly\ndiscovered problems are corresponding to flaws associated with the existing\nimplementation of cache randomization and are fixable. Several new defense\ntechniques are proposed in this paper. our experiments show that all the newly\ndiscovered vulnerabilities of existing randomized caches are fixed within the\ncurrent performance budget. We also argue that randomized set-associative\ncaches can be sufficiently strengthened and possess a better chance to be\nactually adopted in commercial processors than their skewed counterparts as\nthey introduce less overhaul to the existing cache structure.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 06:47:52 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Song", "Wei", ""], ["Li", "Boya", ""], ["Xue", "Zihan", ""], ["Li", "Zhenzhen", ""], ["Wang", "Wenhao", ""], ["Liu", "Peng", ""]]}, {"id": "2008.02078", "submitter": "Mahesh Chandra", "authors": "Mahesh Chandra", "title": "A Novel Method for Scalable VLSI Implementation of Hyperbolic Tangent\n  Function", "comments": "arXiv admin note: text overlap with arXiv:2007.11976 and\n  arXiv:2007.13516", "journal-ref": "IEEE Design and Test; 2021", "doi": "10.1109/MDAT.2021.3063308", "report-no": null, "categories": "cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic tangent and Sigmoid functions are used as non-linear activation\nunits in the artificial and deep neural networks. Since, these networks are\ncomputationally expensive, customized accelerators are designed for achieving\nthe required performance at lower cost and power. The activation function and\nMAC units are the key building blocks of these neural networks. A low\ncomplexity and accurate hardware implementation of the activation function is\nrequired to meet the performance and area targets of such neural network\naccelerators. Moreover, a scalable implementation is required as the recent\nstudies show that the DNNs may use different precision in different layers.\nThis paper presents a novel method based on trigonometric expansion properties\nof the hyperbolic function for hardware implementation which can be easily\ntuned for different accuracy and precision requirements.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 04:57:08 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Chandra", "Mahesh", ""]]}, {"id": "2008.02189", "submitter": "Anakha Vasanthakumari Babu", "authors": "Anakha V Babu, Osvaldo Simeone, Bipin Rajendran", "title": "SpinAPS: A High-Performance Spintronic Accelerator for Probabilistic\n  Spiking Neural Networks", "comments": "25 pages, 10 figures, Submitted to Elsevier Neural Networks for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a high-performance and high-throughput hardware accelerator for\nprobabilistic Spiking Neural Networks (SNNs) based on Generalized Linear Model\n(GLM) neurons, that uses binary STT-RAM devices as synapses and digital CMOS\nlogic for neurons. The inference accelerator, termed \"SpinAPS\" for Spintronic\nAccelerator for Probabilistic SNNs, implements a principled direct learning\nrule for first-to-spike decoding without the need for conversion from\npre-trained ANNs. The proposed solution is shown to achieve comparable\nperformance with an equivalent ANN on handwritten digit and human activity\nrecognition benchmarks. The inference engine, SpinAPS, is shown through\nsoftware emulation tools to achieve 4x performance improvement in terms of\nGSOPS/W/mm2 when compared to an equivalent SRAM-based design. The architecture\nleverages probabilistic spiking neural networks that employ first-to-spike\ndecoding rule to make inference decisions at low latencies, achieving 75% of\nthe test performance in as few as 4 algorithmic time steps on the handwritten\ndigit benchmark. The accelerator also exhibits competitive performance with\nother memristor-based DNN/SNN accelerators and state-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:37:47 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Babu", "Anakha V", ""], ["Simeone", "Osvaldo", ""], ["Rajendran", "Bipin", ""]]}, {"id": "2008.02215", "submitter": "Stefan Szeider", "authors": "Johannes K. Fichte, Markus Hecher, Stefan Szeider", "title": "A Time Leap Challenge for SAT Solving", "comments": "Authors' version of a paper which is to appear in the proceedings of\n  CP'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the impact of hardware advancement and algorithm advancement for\nSAT solving over the last two decades. In particular, we compare 20-year-old\nSAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old\nhardware. Our findings show that the progress on the algorithmic side has at\nleast as much impact as the progress on the hardware side.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:33:41 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Szeider", "Stefan", ""]]}, {"id": "2008.02300", "submitter": "Md Saiful Arefin Mojumder", "authors": "Saiful A. Mojumder, Yifan Sun, Leila Delshadtehrani, Yenai Ma,\n  Trinayan Baruah, Jos\\'e L. Abell\\'an, John Kim, David Kaeli and Ajay Joshi", "title": "MGPU-TSM: A Multi-GPU System with Truly Shared Memory", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sizes of GPU applications are rapidly growing. They are exhausting the\ncompute and memory resources of a single GPU, and are demanding the move to\nmultiple GPUs. However, the performance of these applications scales\nsub-linearly with GPU count because of the overhead of data movement across\nmultiple GPUs. Moreover, a lack of hardware support for coherency exacerbates\nthe problem because a programmer must either replicate the data across GPUs or\nfetch the remote data using high-overhead off-chip links. To address these\nproblems, we propose a multi-GPU system with truly shared memory (MGPU-TSM),\nwhere the main memory is physically shared across all the GPUs. We eliminate\nremote accesses and avoid data replication using an MGPU-TSM system, which\nsimplifies the memory hierarchy. Our preliminary analysis shows that MGPU-TSM\nwith 4 GPUs performs, on average, 3.9x? better than the current best performing\nmulti-GPU configuration for standard application benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:10:28 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 04:52:31 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mojumder", "Saiful A.", ""], ["Sun", "Yifan", ""], ["Delshadtehrani", "Leila", ""], ["Ma", "Yenai", ""], ["Baruah", "Trinayan", ""], ["Abell\u00e1n", "Jos\u00e9 L.", ""], ["Kim", "John", ""], ["Kaeli", "David", ""], ["Joshi", "Ajay", ""]]}, {"id": "2008.02565", "submitter": "Nandan Kumar Jha", "authors": "Nandan Kumar Jha and Sparsh Mittal", "title": "Modeling Data Reuse in Deep Neural Networks by Taking Data-Types into\n  Cognizance", "comments": "Accepted at IEEE Transactions on Computers (Special Issue on\n  Machine-Learning Architectures and Accelerators) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, researchers have focused on reducing the model size and\nnumber of computations (measured as \"multiply-accumulate\" or MAC operations) of\nDNNs. The energy consumption of a DNN depends on both the number of MAC\noperations and the energy efficiency of each MAC operation. The former can be\nestimated at design time; however, the latter depends on the intricate data\nreuse patterns and underlying hardware architecture. Hence, estimating it at\ndesign time is challenging. This work shows that the conventional approach to\nestimate the data reuse, viz. arithmetic intensity, does not always correctly\nestimate the degree of data reuse in DNNs since it gives equal importance to\nall the data types. We propose a novel model, termed \"data type aware weighted\narithmetic intensity\" ($DI$), which accounts for the unequal importance of\ndifferent data types in DNNs. We evaluate our model on 25 state-of-the-art DNNs\non two GPUs. We show that our model accurately models data-reuse for all\npossible data reuse patterns for different types of convolution and different\ntypes of layers. We show that our model is a better indicator of the energy\nefficiency of DNNs. We also show its generality using the central limit\ntheorem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 10:38:07 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Jha", "Nandan Kumar", ""], ["Mittal", "Sparsh", ""]]}, {"id": "2008.02584", "submitter": "Shilpa Mayannavar", "authors": "Shilpa Mayannavar and Uday Wali", "title": "Design of Reconfigurable Multi-Operand Adder for Massively Parallel\n  Processing", "comments": "26 pages, 18 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a systematic study and implementation of a reconfigurable\ncombinatorial multi-operand adder for use in Deep Learning systems. The size of\ncarry changes with the number of operands and hence a reliable algorithm to\nestimate exact number of carry bits is needed for optimal implementation of a\nreconfigurable multi-operand adder. A combinatorial multi-operand adder can be\nfaster compared to a sequential implementation using a two operand adder. Use\ncases for such adders occur in modern processors for deep neural networks. Such\nprocessors require massively parallel computing resources on chip. This paper\npresents a method to estimate the upper bound on the size of carry. A method to\ncompute the exact number of carry bits required for a multi-operand addition\noperation. A fast combinatorial parallel 4-operand adder module is presented.\nAn algorithm to reconfigure these adder modules to implement larger adders is\nalso described. Further, the paper presents two compact but slower iterative\nstructures that implement multi-operand addition, iterating with one column at\na time till the entire word is covered. Such serial/iterative operations are\nslow but occupy small space while parallel operations are fast but use large\nsilicon area on chip. Interestingly, the area-to-throughput ratio of two\narchitectures can tilt in favor of slower, smaller and large number units\ninstead of the fewer numbers of fast and large compute units. A lemma presented\nin the paper may be used to identify the condition when such tilt occurs.\nPotentially, this can save silicon space and increase the throughput of chips\nfor high performance computing. Simulation results of a 16 operand adder and\nusing an set of 4-operand adders for use in neural networks have been\npresented. Simulation results show that performance gain improves as the number\nof operations or operands increases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:29:55 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 15:08:44 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Mayannavar", "Shilpa", ""], ["Wali", "Uday", ""]]}, {"id": "2008.03107", "submitter": "Qian Lou", "authors": "Qian Lou and Sarath Janga and Lei Jiang", "title": "Helix: Algorithm/Architecture Co-design for Accelerating Nanopore Genome\n  Base-calling", "comments": "12 pages, 26 figures, The 29th International Conference on Parallel\n  Architectures and Compilation Techniques (PACT'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanopore genome sequencing is the key to enabling personalized medicine,\nglobal food security, and virus surveillance. The state-of-the-art base-callers\nadopt deep neural networks (DNNs) to translate electrical signals generated by\nnanopore sequencers to digital DNA symbols. A DNN-based base-caller consumes\n$44.5\\%$ of total execution time of a nanopore sequencing pipeline. However, it\nis difficult to quantize a base-caller and build a power-efficient\nprocessing-in-memory (PIM) to run the quantized base-caller. In this paper, we\npropose a novel algorithm/architecture co-designed PIM, Helix, to\npower-efficiently and accurately accelerate nanopore base-calling. From\nalgorithm perspective, we present systematic error aware training to minimize\nthe number of systematic errors in a quantized base-caller. From architecture\nperspective, we propose a low-power SOT-MRAM-based ADC array to process\nanalog-to-digital conversion operations and improve power efficiency of prior\nDNN PIMs. Moreover, we revised a traditional NVM-based dot-product engine to\naccelerate CTC decoding operations, and create a SOT-MRAM binary comparator\narray to process read voting. Compared to state-of-the-art PIMs, Helix improves\nbase-calling throughput by $6\\times$, throughput per Watt by $11.9\\times$ and\nper $mm^2$ by $7.5\\times$ without degrading base-calling accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 22:17:19 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Lou", "Qian", ""], ["Janga", "Sarath", ""], ["Jiang", "Lei", ""]]}, {"id": "2008.03124", "submitter": "Md Obaidul Hossen", "authors": "Md Obaidul Hossen, Yang Zhang, Hesam Fathi Moghadam, Yue Zhang,\n  Michael Dayringer, Muhannad S Bakir", "title": "Design Space Exploration of Power Delivery For Advanced Packaging\n  Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a design space exploration of power delivery networks is\nperformed for multi-chip 2.5-D and 3-D IC technologies. The focus of the paper\nis the effective placement of the voltage regulator modules (VRMs) for power\nsupply noise (PSN) suppression. Multiple on-package VRM configurations have\nbeen analyzed and compared. Additionally, 3D IC chip-on-VRM and\nbackside-of-the-package VRM configurations are studied. From the PSN\nperspective, the 3D IC chip-on-VRM case suppresses the PSN the most even with\nhigh current density hotspots. The paper also studies the impact of different\nparameters such as VRM-chip distance on the package, on-chip decoupling\ncapacitor density, etc. on the PSN.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:56:02 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hossen", "Md Obaidul", ""], ["Zhang", "Yang", ""], ["Moghadam", "Hesam Fathi", ""], ["Zhang", "Yue", ""], ["Dayringer", "Michael", ""], ["Bakir", "Muhannad S", ""]]}, {"id": "2008.03378", "submitter": "Kyeongho Lee", "authors": "Kyeongho Lee, Jinho Jeong, Sungsoo Cheon, Woong Choi, Jongsun Park", "title": "Bit Parallel 6T SRAM In-memory Computing with Reconfigurable\n  Bit-Precision", "comments": "6 pages", "journal-ref": "2020 Design Automation Conference", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents 6T SRAM cell-based bit-parallel in-memory computing (IMC)\narchitecture to support various computations with reconfigurable bit-precision.\nIn the proposed technique, bit-line computation is performed with a short WL\nfollowed by BL boosting circuits, which can reduce BL computing delays. By\nperforming carry-propagation between each near-memory circuit, bit-parallel\ncomplex computations are also enabled by iterating operations with low latency.\nIn addition, reconfigurable bit-precision is also supported based on\ncarry-propagation size. Our 128KB in/near memory computing architecture has\nbeen implemented using a 28nm CMOS process, and it can achieve 2.25GHz clock\nfrequency at 1.0V with 5.2% of area overhead. The proposed architecture also\nachieves 0.68, 8.09 TOPS/W for the parallel addition and multiplication,\nrespectively. In addition, the proposed work also supports a wide range of\nsupply voltage, from 0.6V to 1.1V.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:09:13 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Lee", "Kyeongho", ""], ["Jeong", "Jinho", ""], ["Cheon", "Sungsoo", ""], ["Choi", "Woong", ""], ["Park", "Jongsun", ""]]}, {"id": "2008.03578", "submitter": "Naorin Hossain", "authors": "Naorin Hossain, Caroline Trippel, Margaret Martonosi", "title": "TransForm: Formally Specifying Transistency Models and Synthesizing\n  Enhanced Litmus Tests", "comments": "*This is an updated version of the TransForm paper that features\n  updated results reflecting performance optimizations and software bug fixes.\n  14 pages, 11 figures, Proceedings of the 47th Annual International Symposium\n  on Computer Architecture (ISCA)", "journal-ref": null, "doi": "10.1109/ISCA45697.2020.00076", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory consistency models (MCMs) specify the legal ordering and visibility of\nshared memory accesses in a parallel program. Traditionally, instruction set\narchitecture (ISA) MCMs assume that relevant program-visible memory ordering\nbehaviors only result from shared memory interactions that take place between\nuser-level program instructions. This assumption fails to account for virtual\nmemory (VM) implementations that may result in additional shared memory\ninteractions between user-level program instructions and both 1) system-level\noperations (e.g., address remappings and translation lookaside buffer\ninvalidations initiated by system calls) and 2) hardware-level operations\n(e.g., hardware page table walks and dirty bit updates) during a user-level\nprogram's execution. These additional shared memory interactions can impact the\nobservable memory ordering behaviors of user-level programs. Thus, memory\ntransistency models (MTMs) have been coined as a superset of MCMs to\nadditionally articulate VM-aware consistency rules. However, no prior work has\nenabled formal MTM specifications, nor methods to support their automated\nanalysis.\n  To fill the above gap, this paper presents the TransForm framework. First,\nTransForm features an axiomatic vocabulary for formally specifying MTMs.\nSecond, TransForm includes a synthesis engine to support the automated\ngeneration of litmus tests enhanced with MTM features (i.e., enhanced litmus\ntests, or ELTs) when supplied with a TransForm MTM specification. As a case\nstudy, we formally define an estimated MTM for Intel x86 processors, called\nx86t_elt, that is based on observations made by an ELT-based evaluation of an\nIntel x86 MTM implementation from prior work and available public\ndocumentation. Given x86t_elt and a synthesis bound as input, TransForm's\nsynthesis engine successfully produces a set of ELTs including relevant ELTs\nfrom prior work.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 18:48:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:01:28 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hossain", "Naorin", ""], ["Trippel", "Caroline", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2008.03752", "submitter": "Pengfei Zuo", "authors": "Pengfei Zuo, Yu Hua, Ling Liang, Xinfeng Xie, Xing Hu, Yuan Xie", "title": "SEALing Neural Network Models in Secure Deep Learning Accelerators", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) accelerators are increasingly deployed on edge devices to\nsupport fast local inferences. However, they suffer from a new security\nproblem, i.e., being vulnerable to physical access based attacks. An adversary\ncan easily obtain the entire neural network (NN) model by physically snooping\nthe GDDR memory bus that connects the accelerator chip with DRAM memory.\nTherefore, memory encryption becomes important for DL accelerators on edge\ndevices to improve the security of NN models. Nevertheless, we observe that\ntraditional memory encryption solutions that have been efficiently used in CPU\nsystems cause significant performance degradation when directly used in DL\naccelerators. The main reason comes from the big bandwidth gap between the GDDR\nmemory bus and the encryption engine. To address this problem, our paper\nproposes SEAL, a Secure and Efficient Accelerator scheme for deep Learning.\nSEAL enhances the performance of the encrypted DL accelerator from two aspects,\ni.e., improving the data access bandwidth and the efficiency of memory\nencryption. Specifically, to improve the data access bandwidth, SEAL leverages\na criticality-aware smart encryption scheme which identifies partial data that\nhave no impact on the security of NN models and allows them to bypass the\nencryption engine, thus reducing the amount of data to be encrypted. To improve\nthe efficiency of memory encryption, SEAL leverages a colocation mode\nencryption scheme to eliminate memory accesses from counters used for\nencryption by co-locating data and their counters. Our experimental results\ndemonstrate that, compared with traditional memory encryption solutions, SEAL\nachieves 1.4 ~ 1.6 times IPC improvement and reduces the inference latency by\n39% ~ 60%. Compared with a baseline accelerator without memory encryption, SEAL\ncompromises only 5% ~ 7% IPC for significant security improvement.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 15:53:42 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zuo", "Pengfei", ""], ["Hua", "Yu", ""], ["Liang", "Ling", ""], ["Xie", "Xinfeng", ""], ["Hu", "Xing", ""], ["Xie", "Yuan", ""]]}, {"id": "2008.04436", "submitter": "Saavan Patel", "authors": "Saavan Patel, Lili Chen, Philip Canoza, Sayeef Salahuddin", "title": "Ising Model Optimization Problems on a FPGA Accelerated Restricted\n  Boltzmann Machine", "comments": "17 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems, particularly NP-Hard Combinatorial Optimization\nproblems, are some of the hardest computing problems with no known polynomial\ntime algorithm existing. Recently there has been interest in using dedicated\nhardware to accelerate the solution to these problems, with physical annealers\nand quantum adiabatic computers being some of the state of the art. In this\nwork we demonstrate usage of the Restricted Boltzmann Machine (RBM) as a\nstochastic neural network capable of solving these problems efficiently. We\nshow that by mapping the RBM onto a reconfigurable Field Programmable Gate\nArray (FPGA), we can effectively hardware accelerate the RBM's stochastic\nsampling algorithm. We benchmark the RBM against the DWave 2000Q Quantum\nAdiabatic Computer and the Optical Coherent Ising Machine on two such\noptimization problems: the MAX-CUT problem and finding the ground state of a\nSherrington-Kirkpatrick (SK) spin glass. On these problems, the hardware\naccelerated RBM shows best in class performance compared to these other\naccelerators, with an empirical scaling performance of $\\mathcal{O}(e^{-N})$\nfor probability of reaching the ground state compared to a similar empirical\n$\\mathcal{O}(e^{-N})$ for the CIM (with the RBM showing a constant factor of\nimprovement over the CIM) and empirical $\\mathcal{O}(e^{-N^2})$ for the DWave\nAnnealer. The results show up to $10^7$x and $10^5$x time to solution\nimprovement compared to the DWave 2000Q on the MAX-CUT and SK problems\nrespectively, along with a $150$x and $1000$x performance increase compared to\nthe Coherent Ising Machine annealer on those problems. By using commodity\nhardware running at room temperature for acceleration, the RBM also has greater\npotential for immediate and scalable use.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 22:08:45 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:00:29 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Patel", "Saavan", ""], ["Chen", "Lili", ""], ["Canoza", "Philip", ""], ["Salahuddin", "Sayeef", ""]]}, {"id": "2008.04449", "submitter": "Rosario Cammarota", "authors": "Rosario Cammarota, Matthias Schunter, Anand Rajan, Fabian Boemer,\n  \\'Agnes Kiss, Amos Treiber, Christian Weinert, Thomas Schneider, Emmanuel\n  Stapf, Ahmad-Reza Sadeghi, Daniel Demmler, Huili Chen, Siam Umar Hussain,\n  Sadegh Riazi, Farinaz Koushanfar, Saransh Gupta, Tajan Simunic Rosing,\n  Kamalika Chaudhuri, Hamid Nejatollahi, Nikil Dutt, Mohsen Imani, Kim Laine,\n  Anuj Dubey, Aydin Aysu, Fateme Sadat Hosseini, Chengmo Yang, Eric Wallace,\n  Pamela Norton", "title": "Trustworthy AI Inference Systems: An Industry Research View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we provide an industry research view for approaching the\ndesign, deployment, and operation of trustworthy Artificial Intelligence (AI)\ninference systems. Such systems provide customers with timely, informed, and\ncustomized inferences to aid their decision, while at the same time utilizing\nappropriate security protection mechanisms for AI models. Additionally, such\nsystems should also use Privacy-Enhancing Technologies (PETs) to protect\ncustomers' data at any time.\n  To approach the subject, we start by introducing trends in AI inference\nsystems. We continue by elaborating on the relationship between Intellectual\nProperty (IP) and private data protection in such systems. Regarding the\nprotection mechanisms, we survey the security and privacy building blocks\ninstrumental in designing, building, deploying, and operating private AI\ninference systems. For example, we highlight opportunities and challenges in AI\nsystems using trusted execution environments combined with more recent advances\nin cryptographic techniques to protect data in use. Finally, we outline areas\nof further development that require the global collective attention of\nindustry, academia, and government researchers to sustain the operation of\ntrustworthy AI inference systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 23:05:55 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Cammarota", "Rosario", ""], ["Schunter", "Matthias", ""], ["Rajan", "Anand", ""], ["Boemer", "Fabian", ""], ["Kiss", "\u00c1gnes", ""], ["Treiber", "Amos", ""], ["Weinert", "Christian", ""], ["Schneider", "Thomas", ""], ["Stapf", "Emmanuel", ""], ["Sadeghi", "Ahmad-Reza", ""], ["Demmler", "Daniel", ""], ["Chen", "Huili", ""], ["Hussain", "Siam Umar", ""], ["Riazi", "Sadegh", ""], ["Koushanfar", "Farinaz", ""], ["Gupta", "Saransh", ""], ["Rosing", "Tajan Simunic", ""], ["Chaudhuri", "Kamalika", ""], ["Nejatollahi", "Hamid", ""], ["Dutt", "Nikil", ""], ["Imani", "Mohsen", ""], ["Laine", "Kim", ""], ["Dubey", "Anuj", ""], ["Aysu", "Aydin", ""], ["Hosseini", "Fateme Sadat", ""], ["Yang", "Chengmo", ""], ["Wallace", "Eric", ""], ["Norton", "Pamela", ""]]}, {"id": "2008.05685", "submitter": "P Balasubramanian", "authors": "P Balasubramanian, D L Maskell, N E Mastorakis", "title": "Area Optimized Quasi Delay Insensitive Majority Voter for TMR\n  Applications", "comments": null, "journal-ref": "Proceedings of IEEE 3rd European Conference on Electrical\n  Engineering and Computer Science, pp. 37-44, 2019, Athens, Greece", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mission-critical and safety-critical applications generally tend to\nincorporate triple modular redundancy (TMR) to embed fault tolerance in their\nphysical implementations. In a TMR realization, an original function block,\nwhich may be a circuit or a system, and two exact copies of the function block\nare used to successfully overcome any temporary fault or permanent failure of\nan arbitrary function block during the routine operation. The corresponding\noutputs of the function blocks are majority voted using 3-input majority voters\nwhose outputs define the outputs of a TMR realization. Hence, a 3-input\nmajority voter forms an important component of a TMR realization. Many\nsynchronous majority voters and an asynchronous non-delay insensitive majority\nvoter have been presented in the literature. Recently, quasi delay insensitive\n(QDI) asynchronous majority voters for TMR applications were also discussed in\nthe literature. In this regard, this paper presents a new QDI asynchronous\nmajority voter for TMR applications, which is better optimized in area compared\nto the existing QDI majority voters. The proposed QDI majority voter requires\n30.2% less area compared to the best of the existing QDI majority voters, and\nthis could be useful for resource-constrained fault tolerance applications. The\nexample QDI TMR circuits were implemented using a 32/28nm complementary metal\noxide semiconductor (CMOS) process. The delay insensitive dual rail code was\nused for data encoding, and 4-phase return-to-zero and return-to-one handshake\nprotocols were used for data communication.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 04:21:49 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Balasubramanian", "P", ""], ["Maskell", "D L", ""], ["Mastorakis", "N E", ""]]}, {"id": "2008.06112", "submitter": "Onur Mutlu", "authors": "Onur Mutlu", "title": "Intelligent Architectures for Intelligent Machines", "comments": "To appear in VLSI DAT/TSA 2020 conference proceedings as a plenary\n  keynote paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing is bottlenecked by data. Large amounts of application data\noverwhelm storage capability, communication capability, and computation\ncapability of the modern machines we design today. As a result, many key\napplications' performance, efficiency and scalability are bottlenecked by data\nmovement. In this keynote talk, we describe three major shortcomings of modern\narchitectures in terms of 1) dealing with data, 2) taking advantage of the vast\namounts of data, and 3) exploiting different semantic properties of application\ndata. We argue that an intelligent architecture should be designed to handle\ndata well. We show that handling data well requires designing architectures\nbased on three key principles: 1) data-centric, 2) data-driven, 3) data-aware.\nWe give several examples for how to exploit each of these principles to design\na much more efficient and high performance computing system. We especially\ndiscuss recent research that aims to fundamentally reduce memory latency and\nenergy, and practically enable computation close to data, with at least two\npromising novel directions: 1) performing massively-parallel bulk operations in\nmemory by exploiting the analog operational properties of memory, with low-cost\nchanges, 2) exploiting the logic layer in 3D-stacked memory technology in\nvarious ways to accelerate important data-intensive applications. We discuss\nhow to enable adoption of such fundamentally more intelligent architectures,\nwhich we believe are key to efficiency, performance, and sustainability. We\nconclude with some guiding principles for future computing architecture and\nsystem designs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 21:06:47 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Mutlu", "Onur", ""]]}, {"id": "2008.06177", "submitter": "Shaahin Angizi", "authors": "Shaahin Angizi, Naima Ahmed Fahmi, Wei Zhang, Deliang Fan", "title": "PANDA: Processing-in-MRAM Accelerated De Bruijn Graph based DNA Assembly", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spurred by widening gap between data processing speed and data communication\nspeed in Von-Neumann computing architectures, some bioinformatic applications\nhave harnessed the computational power of Processing-in-Memory (PIM) platforms.\nHowever, the performance of PIMs unavoidably diminishes when dealing with such\ncomplex applications seeking bulk bit-wise comparison or addition operations.\nIn this work, we present an efficient Processing-in-MRAM Accelerated De Bruijn\nGraph based DNA Assembly platform named PANDA based on an optimized and\nhardware-friendly genome assembly algorithm. PANDA is able to assemble\nlarge-scale DNA sequence data-set from all-pair overlaps. We first design PANDA\nplatform that exploits MRAM as a computational memory and converts it to a\npotent processing unit for genome assembly. PANDA can execute not only\nefficient bulk bit-wise X(N)OR-based comparison/addition operations heavily\nrequired for the genome assembly task but a full-set of 2-/3-input logic\noperations inside MRAM chip. We then develop a highly parallel and step-by-step\nhardware-friendly DNA assembly algorithm for PANDA that only requires the\ndeveloped in-memory logic operations. The platform is then configured with a\nnovel data partitioning and mapping technique that provides local storage and\nprocessing to fully utilize the algorithm-level's parallelism. The cross-layer\nsimulation results demonstrate that PANDA reduces the run time and power,\nrespectively, by a factor of 18 and 11 compared with CPU. Besides, speed-ups of\nup-to 2-4x can be obtained over recent processing-in-MRAM platforms to perform\nthe same task.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:30:31 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Angizi", "Shaahin", ""], ["Fahmi", "Naima Ahmed", ""], ["Zhang", "Wei", ""], ["Fan", "Deliang", ""]]}, {"id": "2008.06502", "submitter": "Florian Zaruba", "authors": "Florian Zaruba, Fabian Schuiki and Luca Benini", "title": "Manticore: A 4096-core RISC-V Chiplet Architecture for Ultra-efficient\n  Floating-point Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-parallel problems demand ever growing floating-point (FP) operations per\nsecond under tight area- and energy-efficiency constraints. In this work, we\npresent Manticore, a general-purpose, ultra-efficient chiplet-based\narchitecture for data-parallel FP workloads. We have manufactured a prototype\nof the chiplet's computational core in Globalfoundries 22FDX process and\ndemonstrate more than 5x improvement in energy efficiency on FP intensive\nworkloads compared to CPUs and GPUs. The compute capability at high energy and\narea efficiency is provided by Snitch clusters containing eight small integer\ncores, each controlling a large FPU. The core supports two custom ISA\nextensions: The SSR extension elides explicit load and store instructions by\nencoding them as register reads and writes. The FREP extension decouples the\ninteger core from the FPU allowing floating-point instructions to be issued\nindependently. These two extensions allow the single-issue core to minimize its\ninstruction fetch bandwidth and saturate the instruction bandwidth of the FPU,\nachieving FPU utilization above 90%, with more than 40% of core area dedicated\nto the FPU.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 08:54:00 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:06:25 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Zaruba", "Florian", ""], ["Schuiki", "Fabian", ""], ["Benini", "Luca", ""]]}, {"id": "2008.06571", "submitter": "Xingfu Wu", "authors": "Xingfu Wu and Aniruddha Marathe and Siddhartha Jana and Ondrej Vysocky\n  and Jophin John and Andrea Bartolini and Lubomir Riha and Michael Gerndt and\n  Valerie Taylor and Sridutt Bhalachandra", "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack", "comments": "to be published in Energy Efficient HPC State of Practice 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently utilizing procured power and optimizing performance of scientific\napplications under power and energy constraints are challenging. The HPC\nPowerStack defines a software stack to manage power and energy of\nhigh-performance computing systems and standardizes the interfaces between\ndifferent components of the stack. This survey paper presents the findings of a\nworking group focused on the end-to-end tuning of the PowerStack. First, we\nprovide a background on the PowerStack layer-specific tuning efforts in terms\nof their high-level objectives, the constraints and optimization goals,\nlayer-specific telemetry, and control parameters, and we list the existing\nsoftware solutions that address those challenges. Second, we propose the\nPowerStack end-to-end auto-tuning framework, identify the opportunities in\nco-tuning different layers in the PowerStack, and present specific use cases\nand solutions. Third, we discuss the research opportunities and challenges for\ncollective auto-tuning of two or more management layers (or domains) in the\nPowerStack. This paper takes the first steps in identifying and aggregating the\nimportant R&D challenges in streamlining the optimization efforts across the\nlayers of the PowerStack.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 20:57:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Xingfu", ""], ["Marathe", "Aniruddha", ""], ["Jana", "Siddhartha", ""], ["Vysocky", "Ondrej", ""], ["John", "Jophin", ""], ["Bartolini", "Andrea", ""], ["Riha", "Lubomir", ""], ["Gerndt", "Michael", ""], ["Taylor", "Valerie", ""], ["Bhalachandra", "Sridutt", ""]]}, {"id": "2008.06741", "submitter": "Brian Crafton Mr.", "authors": "Brian Crafton and Samuel Spetalnick and Gauthaman Murali and Tushar\n  Krishna and Sung-Kyu Lim and Arijit Raychowdhury", "title": "Breaking Barriers: Maximizing Array Utilization for Compute In-Memory\n  Fabrics", "comments": "6 pages, 9 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute in-memory (CIM) is a promising technique that minimizes data\ntransport, the primary performance bottleneck and energy cost of most data\nintensive applications. This has found wide-spread adoption in accelerating\nneural networks for machine learning applications. Utilizing a crossbar\narchitecture with emerging non-volatile memories (eNVM) such as dense resistive\nrandom access memory (RRAM) or phase change random access memory (PCRAM),\nvarious forms of neural networks can be implemented to greatly reduce power and\nincrease on chip memory capacity. However, compute in-memory faces its own\nlimitations at both the circuit and the device levels. Although compute\nin-memory using the crossbar architecture can greatly reduce data transport,\nthe rigid nature of these large fixed weight matrices forfeits the flexibility\nof traditional CMOS and SRAM based designs. In this work, we explore the\ndifferent synchronization barriers that occur from the CIM constraints.\nFurthermore, we propose a new allocation algorithm and data flow based on input\ndata distributions to maximize utilization and performance for compute-in\nmemory based designs. We demonstrate a 7.47$\\times$ performance improvement\nover a naive allocation method for CIM accelerators on ResNet18.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 15:49:50 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Crafton", "Brian", ""], ["Spetalnick", "Samuel", ""], ["Murali", "Gauthaman", ""], ["Krishna", "Tushar", ""], ["Lim", "Sung-Kyu", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2008.06967", "submitter": "Yu Feng", "authors": "Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, Yuhao Zhu", "title": "Mesorasi: Architecture Support for Point Cloud Analytics via\n  Delayed-Aggregation", "comments": null, "journal-ref": "Proceedings of the 53nd (2020) Annual IEEE/ACM International\n  Symposium on Microarchitecture", "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analytics is poised to become a key workload on battery-powered\nembedded and mobile platforms in a wide range of emerging application domains,\nsuch as autonomous driving, robotics, and augmented reality, where efficiency\nis paramount. This paper proposes Mesorasi, an algorithm-architecture\nco-designed system that simultaneously improves the performance and energy\nefficiency of point cloud analytics while retaining its accuracy. Our extensive\ncharacterizations of state-of-the-art point cloud algorithms show that, while\nstructurally reminiscent of convolutional neural networks (CNNs), point cloud\nalgorithms exhibit inherent compute and memory inefficiencies due to the unique\ncharacteristics of point cloud data. We propose delayed-aggregation, a new\nalgorithmic primitive for building efficient point cloud algorithms.\nDelayed-aggregation hides the performance bottlenecks and reduces the compute\nand memory redundancies by exploiting the approximately distributive property\nof key operations in point cloud algorithms. Delayed-aggregation let point\ncloud algorithms achieve 1.6x speedup and 51.1% energy reduction on a mobile\nGPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the\nalgorithmic benefits, we propose minor extensions to contemporary CNN\naccelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC)\nwithout modifying other SoC components. With additional hardware support,\nMesorasi achieves up to 3.6x speedup.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 18:11:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Feng", "Yu", ""], ["Tian", "Boyuan", ""], ["Xu", "Tiancheng", ""], ["Whatmough", "Paul", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2008.07127", "submitter": "Francesco Conti", "authors": "Alessio Burrello, Angelo Garofalo, Nazareno Bruschi, Giuseppe\n  Tagliavini, Davide Rossi, Francesco Conti", "title": "DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT\n  MCUs", "comments": "14 pages, 12 figures, 4 tables, 2 listings. Accepted for publication\n  in IEEE Transactions on Computers\n  (https://ieeexplore.ieee.org/document/9381618)", "journal-ref": null, "doi": "10.1109/TC.2021.3066883", "report-no": null, "categories": "cs.DC cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Deep Neural Networks (DNNs) on end-nodes at the extreme\nedge of the Internet-of-Things is a critical enabler to support pervasive Deep\nLearning-enhanced applications. Low-Cost MCU-based end-nodes have limited\non-chip memory and often replace caches with scratchpads, to reduce area\noverheads and increase energy efficiency -- requiring explicit DMA-based memory\ntransfers between different levels of the memory hierarchy. Mapping modern DNNs\non these systems requires aggressive topology-dependent tiling and\ndouble-buffering. In this work, we propose DORY (Deployment Oriented to memoRY)\n- an automatic tool to deploy DNNs on low cost MCUs with typically less than\n1MB of on-chip SRAM memory. DORY abstracts tiling as a Constraint Programming\n(CP) problem: it maximizes L1 memory utilization under the topological\nconstraints imposed by each DNN layer. Then, it generates ANSI C code to\norchestrate off- and on-chip transfers and computation phases. Furthermore, to\nmaximize speed, DORY augments the CP formulation with heuristics promoting\nperformance-effective tile sizes. As a case study for DORY, we target\nGreenWaves Technologies GAP8, one of the most advanced parallel ultra-low power\nMCU-class devices on the market. On this device, DORY achieves up to 2.5x\nbetter MAC/cycle than the GreenWaves proprietary software solution and 18.1x\nbetter than the state-of-the-art result on an STM32-F746 MCU on single layers.\nUsing our tool, GAP-8 can perform end-to-end inference of a 1.0-MobileNet-128\nnetwork consuming just 63 pJ/MAC on average @ 4.3 fps - 15.4x better than an\nSTM32-F746. We release all our developments - the DORY framework, the optimized\nbackend kernels, and the related heuristics - as open-source software.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 07:30:54 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 15:11:36 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 15:08:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Burrello", "Alessio", ""], ["Garofalo", "Angelo", ""], ["Bruschi", "Nazareno", ""], ["Tagliavini", "Giuseppe", ""], ["Rossi", "Davide", ""], ["Conti", "Francesco", ""]]}, {"id": "2008.07171", "submitter": "Siddharth Rai", "authors": "Siddharth Rai, Trevor E. Carlson", "title": "CARGO : Context Augmented Critical Region Offload for Network-bound\n  datacenter Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network bound applications, like a database server executing OLTP queries or\na caching server storing objects for a dynamic web applications, are essential\nservices that consumers and businesses use daily. These services run on a large\ndatacenters and are required to meet predefined Service Level Objectives (SLO),\nor latency targets, with high probability. Thus, efficient datacenter\napplications should optimize their execution in terms of power and performance.\nHowever, to support large scale data storage, these workloads make heavy use of\npointer connected data structures (e.g., hash table, large fan-out tree, trie)\nand exhibit poor instruction and memory level parallelism. Our experiments show\nthat due to long memory access latency, these workloads occupy processor\nresources (e.g., ROB entries, RS buffers, LS queue entries etc.) for a\nprolonged period of time that delay the processing of subsequent requests.\nDelayed execution not only increases request processing latency, but also\nseverely effects an application throughput and power-efficiency. To overcome\nthis limitation, we present CARGO, a novel mechanism to overlap queuing latency\nand request processing by executing select instructions on an application\ncritical path at the network interface card (NIC) while requests wait for\nprocessor resources to become available. Our mechanism dynamically identifies\nthe critical instructions and includes the register state needed to compute the\nlong latency memory accesses. This context-augmented critical region is often\nexecuted at the NIC well before execution begins at the core, effectively\nprefetching the data ahead of time. Across a variety of interactive datacenter\napplications, our proposal improves latency, throughput, and power efficiency\nby 2.7X, 2.7X, and 1.5X, respectively, while incurring a modest amount storage\noverhead.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:21:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rai", "Siddharth", ""], ["Carlson", "Trevor E.", ""]]}, {"id": "2008.08883", "submitter": "Kris Nikov", "authors": "Kris Nikov (1), Mohammad Hosseinabady (1), Rafael Asenjo (2), Andr\\'es\n  Rodr\\'iguezz (2), Angeles Navarro (2) and Jose Nunez-Yanez (1) ((1)\n  University of Bristol, UK, (2) Universidad de M\\'alaga, Spain)", "title": "High-Performance Simultaneous Multiprocessing for Heterogeneous\n  System-on-Chip", "comments": "7 pages, 5 figures, 1 table Presented at the 13th International\n  Workshop on Programmability and Architectures for Heterogeneous Multicores,\n  2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "MULTIPROG/2020/4", "categories": "cs.DC cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a methodology for simultaneous heterogeneous computing,\nnamed ENEAC, where a quad core ARM Cortex-A53 CPU works in tandem with a\npreprogrammed on-board FPGA accelerator. A heterogeneous scheduler distributes\nthe tasks optimally among all the resources and all compute units run\nasynchronously, which allows for improved performance for irregular workloads.\nENEAC achieves up to 17\\% performance improvement \\ignore{and 14\\% energy usage\nreduction,} when using all platform resources compared to just using the FPGA\naccelerators and up to 865\\% performance increase \\ignore{and up to 89\\% energy\nusage decrease} when using just the CPU. The workflow uses existing commercial\ntools and C/C++ as a single programming language for both accelerator design\nand CPU programming for improved productivity and ease of verification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:53:32 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Nikov", "Kris", ""], ["Hosseinabady", "Mohammad", ""], ["Asenjo", "Rafael", ""], ["Rodr\u00edguezz", "Andr\u00e9s", ""], ["Navarro", "Angeles", ""], ["Nunez-Yanez", "Jose", ""]]}, {"id": "2008.09442", "submitter": "Sumon Bose Mr.", "authors": "Bapi Kar, Pradeep Kumar Gopalakrishnan, Sumon Kumar Bose, Mohendra\n  Roy, and Arindam Basu", "title": "ADIC: Anomaly Detection Integrated Circuit in 65nm CMOS utilizing\n  Approximate Computing", "comments": "12", "journal-ref": null, "doi": "10.1109/TVLSI.2020.3016939", "report-no": null, "categories": "eess.SP cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a low-power anomaly detection integrated circuit\n(ADIC) based on a one-class classifier (OCC) neural network. The ADIC achieves\nlow-power operation through a combination of (a) careful choice of algorithm\nfor online learning and (b) approximate computing techniques to lower average\nenergy. In particular, online pseudoinverse update method (OPIUM) is used to\ntrain a randomized neural network for quick and resource efficient learning. An\nadditional 42% energy saving can be achieved when a lighter version of OPIUM\nmethod is used for training with the same number of data samples lead to no\nsignificant compromise on the quality of inference. Instead of a single\nclassifier with large number of neurons, an ensemble of K base learner approach\nis chosen to reduce learning memory by a factor of K. This also enables\napproximate computing by dynamically varying the neural network size based on\nanomaly detection. Fabricated in 65nm CMOS, the ADIC has K = 7 Base Learners\n(BL) with 32 neurons in each BL and dissipates 11.87pJ/OP and 3.35pJ/OP during\nlearning and inference respectively at Vdd = 0.75V when all 7 BLs are enabled.\nFurther, evaluated on the NASA bearing dataset, approximately 80% of the chip\ncan be shut down for 99% of the lifetime leading to an energy efficiency of\n0.48pJ/OP, an 18.5 times reduction over full-precision computing running at Vdd\n= 1.2V throughout the lifetime.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 12:18:07 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Kar", "Bapi", ""], ["Gopalakrishnan", "Pradeep Kumar", ""], ["Bose", "Sumon Kumar", ""], ["Roy", "Mohendra", ""], ["Basu", "Arindam", ""]]}, {"id": "2008.09633", "submitter": "Renato J Cintra", "authors": "A. Borges Jr., R. J. Cintra, D. F. G. Coelho, V. S. Dimitrov", "title": "Low-complexity Architecture for AR(1) Inference", "comments": "7 pages, 3 tables, 4 figures", "journal-ref": "Electronics Letters 56 (14), 732-734, 2020", "doi": "10.1049/el.2019.4030", "report-no": null, "categories": "eess.SP cs.AR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Letter, we propose a low-complexity estimator for the correlation\ncoefficient based on the signed $\\operatorname{AR}(1)$ process. The introduced\napproximation is suitable for implementation in low-power hardware\narchitectures. Monte Carlo simulations reveal that the proposed estimator\nperforms comparably to the competing methods in literature with maximum error\nin order of $10^{-2}$. However, the hardware implementation of the introduced\nmethod presents considerable advantages in several relevant metrics, offering\nmore than 95% reduction in dynamic power and doubling the maximum operating\nfrequency when compared to the reference method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:16:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Borges", "A.", "Jr."], ["Cintra", "R. J.", ""], ["Coelho", "D. F. G.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "2008.09954", "submitter": "Yiming Gan", "authors": "Yiming Gan, Yuxian Qiu, Jingwen Leng, Minyi Guo, Yuhao Zhu", "title": "Ptolemy: Architecture Support for Robust Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is vulnerable to adversarial attacks, where carefully-crafted\ninput perturbations could mislead a well-trained Deep Neural Network to produce\nincorrect results. Today's countermeasures to adversarial attacks either do not\nhave capability to detect adversarial samples at inference time, or introduce\nprohibitively high overhead to be practical at inference time.\n  We propose Ptolemy, an algorithm-architecture co-designed system that detects\nadversarial attacks at inference time with low overhead and high accuracy.We\nexploit the synergies between DNN inference and imperative program execution:\nan input to a DNN uniquely activates a set of neurons that contribute\nsignificantly to the inference output, analogous to the sequence of basic\nblocks exercised by an input in a conventional program. Critically, we observe\nthat adversarial samples tend to activate distinctive paths from those of\nbenign inputs. Leveraging this insight, we propose an adversarial sample\ndetection framework, which uses canary paths generated from offline profiling\nto detect adversarial samples at runtime. The Ptolemy compiler along with the\nco-designed hardware enable efficient execution by exploiting the unique\nalgorithmic characteristics. Extensive evaluations show that Ptolemy achieves\nhigher or similar adversarial example detection accuracy than today's\nmechanisms with a much lower runtime (as low as 2%) overhead.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 04:04:01 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Gan", "Yiming", ""], ["Qiu", "Yuxian", ""], ["Leng", "Jingwen", ""], ["Guo", "Minyi", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2008.10169", "submitter": "Vikram Sharma Mailthody", "authors": "Zaid Qureshi, Vikram Sharma Mailthody, Seung Won Min, I-Hsin Chung,\n  Jinjun Xiong, Wen-mei Hwu", "title": "Tearing Down the Memory Wall", "comments": "SRC Techcon 2020 paper. Discusses vision of GPU-Centric architecture,\n  Erudite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a vision for the Erudite architecture that redefines the compute\nand memory abstractions such that memory bandwidth and capacity become\nfirst-class citizens along with compute throughput. In this architecture, we\nenvision coupling a high-density, massively parallel memory technology like\nFlash with programmable near-data accelerators, like the streaming\nmultiprocessors in modern GPUs. Each accelerator has a local pool of\nstorage-class memory that it can access at high throughput by initiating very\nlarge numbers of overlapping requests that help to tolerate long access\nlatency. The accelerators can also communicate with each other and remote\nmemory through a high-throughput low-latency interconnect. As a result, systems\nbased on the Erudite architecture scale compute and memory bandwidth at the\nsame rate, tearing down the notorious memory wall that has plagued computer\narchitecture for generations. In this paper, we present the motivation,\nrationale, design, benefit, and research challenges for Erudite.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:07:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Qureshi", "Zaid", ""], ["Mailthody", "Vikram Sharma", ""], ["Min", "Seung Won", ""], ["Chung", "I-Hsin", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2008.10604", "submitter": "Kris Nikov", "authors": "Kris Nikov (1), Jose L. Nunez-Yanez (1) and Matthew Horsnell (2) ((1)\n  University of Bristol, UK, (2) ARM Ltd., UK)", "title": "Evaluation of hybrid run-time power models for the ARM big.LITTLE\n  architecture", "comments": "6 pages, 8 fugures. 2015 IEEE 13th International Conference on\n  Embedded and Ubiquitous Computing", "journal-ref": "2015 IEEE 13th International Conference on Embedded and Ubiquitous\n  Computing, Porto, 2015, pp. 205-210", "doi": "10.1109/EUC.2015.32", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneous processors, formed by binary compatible CPU cores with\ndifferent microarchitectures, enable energy reductions by better matching\nprocessing capabilities and software application requirements. This new\nhardware platform requires novel techniques to manage power and energy to fully\nutilize its capabilities, particularly regarding the mapping of workloads to\nappropriate cores. In this paper we validate relevant published work related to\npower modelling for heterogeneous systems and propose a new approach for\ndeveloping run-time power models that uses a hybrid set of physical predictors,\nperformance events and CPU state information. We demonstrate the accuracy of\nthis approach compared with the state-of-the-art and its applicability to\nenergy aware scheduling. Our results are obtained on a commercially available\nplatform built around the Samsung Exynos 5 Octa SoC, which features the ARM\nbig.LITTLE heterogeneous architecture.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:30:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Nikov", "Kris", ""], ["Nunez-Yanez", "Jose L.", ""], ["Horsnell", "Matthew", ""]]}, {"id": "2008.10682", "submitter": "Sachin Sapatnekar", "authors": "Tonmoy Dhar, Kishor Kunal, Yaguang Li, Meghna Madhusudan, Jitesh\n  Poojary, Arvind K. Sharma, Wenbin Xu, Steven M. Burns, Ramesh Harjani, Jiang\n  Hu, Desmond A. Kirkpatrick, Parijat Mukherjee, Sachin S. Sapatnekar, and\n  Soner Yaldiz", "title": "ALIGN: A System for Automating Analog Layout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ALIGN (\"Analog Layout, Intelligently Generated from Netlists\") is an\nopen-source automatic layout generation flow for analog circuits. ALIGN\ntranslates an input SPICE netlist to an output GDSII layout, specific to a\ngiven technology, as specified by a set of design rules. The flow first\nautomatically detects hierarchies in the circuit netlist and translates layout\nsynthesis to a problem of hierarchical block assembly. At the lowest level,\nparameterized cells are generated using an abstraction of the design rules;\nthese blocks are then assembled under geometric and electrical constraints to\nbuild the circuit layout. ALIGN has been applied to generate layouts for a\ndiverse set of analog circuit families: low frequency analog blocks, wireline\ncircuits, wireless circuits, and power delivery circuits.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 20:03:10 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dhar", "Tonmoy", ""], ["Kunal", "Kishor", ""], ["Li", "Yaguang", ""], ["Madhusudan", "Meghna", ""], ["Poojary", "Jitesh", ""], ["Sharma", "Arvind K.", ""], ["Xu", "Wenbin", ""], ["Burns", "Steven M.", ""], ["Harjani", "Ramesh", ""], ["Hu", "Jiang", ""], ["Kirkpatrick", "Desmond A.", ""], ["Mukherjee", "Parijat", ""], ["Sapatnekar", "Sachin S.", ""], ["Yaldiz", "Soner", ""]]}, {"id": "2008.10802", "submitter": "Jorge Gonzalez", "authors": "Jorge Gonzalez, Alexander Gazman, Maarten Hattink, Mauricio G. Palma,\n  Meisam Bahadori, Ruth Rubio-Noriega, Lois Orosa, Madeleine Glick, Onur Mutlu,\n  Keren Bergman, Rodolfo Azevedo", "title": "Optically Connected Memory for Disaggregated Data Centers", "comments": "This work is to appear at SBAC-PAD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in integrated photonics enable the implementation of\nreconfigurable, high-bandwidth, and low energy-per-bit interconnects in\nnext-generation data centers. We propose and evaluate an Optically Connected\nMemory (OCM) architecture that disaggregates the main memory from the\ncomputation nodes in data centers. OCM is based on micro-ring resonators\n(MRRs), and it does not require any modification to the DRAM memory modules. We\ncalculate energy consumption from real photonic devices and integrate them into\na system simulator to evaluate performance. Our results show that (1) OCM is\ncapable of interconnecting four DDR4 memory channels to a computing node using\ntwo fibers with 1.07 pJ energy-per-bit consumption and (2) OCM performs up to\n5.5x faster than a disaggregated memory with 40G PCIe NIC connectors to\ncomputing nodes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 03:58:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Gonzalez", "Jorge", ""], ["Gazman", "Alexander", ""], ["Hattink", "Maarten", ""], ["Palma", "Mauricio G.", ""], ["Bahadori", "Meisam", ""], ["Rubio-Noriega", "Ruth", ""], ["Orosa", "Lois", ""], ["Glick", "Madeleine", ""], ["Mutlu", "Onur", ""], ["Bergman", "Keren", ""], ["Azevedo", "Rodolfo", ""]]}, {"id": "2008.11326", "submitter": "Charlene Yang", "authors": "Charlene Yang", "title": "8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other\n  Tricks", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance optimization can be a daunting task especially as the hardware\narchitecture becomes more and more complex. This paper takes a kernel from the\nMaterials Science code BerkeleyGW, and demonstrates a few performance analysis\nand optimization techniques. Despite challenges such as high register usage,\nlow occupancy, complex data access patterns, and the existence of several\nlong-latency instructions, we have achieved 3.7 TFLOP/s of double-precision\nperformance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of\nthe theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of\nthe more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of\ntechniques used to analyze this OpenACC kernel and optimize its performance are\nshown, including the use of hierarchical Roofline performance model and the\nperformance tool Nsight Compute. This kernel exhibits computational\ncharacteristics that are commonly seen in many high-performance computing (HPC)\napplications, and are expected to be very helpful to a general audience of HPC\ndevelopers and computational scientists, as they pursue more performance on\nNVIDIA GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 01:09:24 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 01:45:05 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 05:08:36 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 20:21:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yang", "Charlene", ""]]}, {"id": "2008.11367", "submitter": "Chao-Hsuan Huang", "authors": "Chao-Hsuan Huang, Ishan G Thakkar", "title": "Mitigating the Latency-Area Tradeoffs for DRAM Design with\n  Coarse-Grained Monolithic 3D (M3D) Integration", "comments": "Accepted in ICCD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, the DRAM latency has not scaled proportionally with its\ndensity due to the cost-centric mindset of the DRAM industry. Prior work has\nshown that this shortcoming can be overcome by reducing the critical length of\nDRAM access path. However, doing so decreases DRAM area-efficiency,\nexacerbating the latency-area tradeoffs for DRAM design. In this paper, we show\nthat reorganizing DRAM cell-arrays using the emerging monolithic 3D (M3D)\nintegration technology can mitigate these fundamental latency-area tradeoffs.\nBased on our evaluation results for PARSEC benchmarks, our designed M3D DRAM\ncell-array organizations can yield up to 9.56% less latency, up to 4.96% less\npower consumption, and up to 21.21% less energy-delay product (EDP), with up to\n14% less DRAM die area, com-pared to the conventional 2D DDR4 DRAM.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 03:55:22 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Huang", "Chao-Hsuan", ""], ["Thakkar", "Ishan G", ""]]}, {"id": "2008.11591", "submitter": "Dip Sankar Banerjee", "authors": "Rajat Bhattacharjya, Vishesh Mishra, Saurabh Singh, Kaustav Goswami,\n  Dip Sankar Banerjee", "title": "An Approximate Carry Estimating Simultaneous Adder with Rectification", "comments": "To appear at the 30th ACM Great Lakes Symposium on VLSI", "journal-ref": null, "doi": "10.1145/3386263.3406928", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing has in recent times found significant applications\ntowards lowering power, area, and time requirements for arithmetic operations.\nSeveral works done in recent years have furthered approximate computing along\nthese directions. In this work, we propose a new approximate adder that employs\na carry prediction method. This allows parallel propagation of the carry\nallowing faster calculations. In addition to the basic adder design, we also\npropose a rectification logic which would enable higher accuracy for larger\ncomputations. Experimental results show that our adder produces results 91.2%\nfaster than the conventional ripple-carry adder. In terms of accuracy, the\naddition of rectification logic to the basic design produces results that are\nmore accurate than state-of-the-art adders like SARA and BCSA by 74%.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:47:30 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bhattacharjya", "Rajat", ""], ["Mishra", "Vishesh", ""], ["Singh", "Saurabh", ""], ["Goswami", "Kaustav", ""], ["Banerjee", "Dip Sankar", ""]]}, {"id": "2008.11632", "submitter": "Weizhe Hua", "authors": "Weizhe Hua, Muhammad Umar, Zhiru Zhang, G. Edward Suh", "title": "GuardNN: Secure DNN Accelerator for Privacy-Preserving Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes GuardNN, a secure deep neural network (DNN) accelerator,\nwhich provides strong hardware-based protection for user data and model\nparameters even in an untrusted environment. GuardNN shows that the\narchitecture and protection can be customized for a specific application to\nprovide strong confidentiality and integrity protection with negligible\noverhead. The design of the GuardNN instruction set reduces the TCB to just the\naccelerator and enables confidentiality protection without the overhead of\nintegrity protection. GuardNN also introduces a new application-specific memory\nprotection scheme to minimize the overhead of memory encryption and integrity\nverification. The scheme shows that most of the off-chip meta-data in today's\nstate-of-the-art memory protection can be removed by exploiting the known\nmemory access patterns of a DNN accelerator. GuardNN is implemented as an FPGA\nprototype, which demonstrates effective protection with less than 2%\nperformance overhead for inference over a variety of modern DNN models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 15:43:50 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Hua", "Weizhe", ""], ["Umar", "Muhammad", ""], ["Zhang", "Zhiru", ""], ["Suh", "G. Edward", ""]]}, {"id": "2008.11669", "submitter": "Yewei Zhang", "authors": "Yewei Zhang (Student Member, IEEE), Kejie Huang (Senior Member, IEEE),\n  Rui Xiao (Student Member, IEEE), and Haibin Shen", "title": "An 8-bit In Resistive Memory Computing Core with Regulated Passive\n  Neuron and Bit Line Weight Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of Artificial Intelligence (AI) and Internet of Things\n(IoT) increases the requirement for edge computing with low power and\nrelatively high processing speed devices. The Computing-In-Memory(CIM) schemes\nbased on emerging resistive Non-Volatile Memory(NVM) show great potential in\nreducing the power consumption for AI computing. However, the device\ninconsistency of the non-volatile memory may significantly degenerate the\nperformance of the neural network. In this paper, we propose a low power\nResistive RAM (RRAM) based CIM core to not only achieve high computing\nefficiency but also greatly enhance the robustness by bit line regulator and\nbit line weight mapping algorithm. The simulation results show that the power\nconsumption of our proposed 8-bit CIM core is only 3.61mW (256*256). The SFDR\nand SNDR of the CIM core achieve 59.13 dB and 46.13 dB, respectively. The\nproposed bit line weight mapping scheme improves the top-1 accuracy by 2.46%\nand 3.47% for AlexNet and VGG16 on ImageNet Large Scale Visual Recognition\nCompetition 2012 (ILSVRC 2012) in 8-bit mode, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:50:37 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Zhang", "Yewei", "", "Student Member, IEEE"], ["Huang", "Kejie", "", "Senior Member, IEEE"], ["Xiao", "Rui", "", "Student Member, IEEE"], ["Shen", "Haibin", ""]]}, {"id": "2008.12243", "submitter": "Giuseppe Tagliavini", "authors": "Fabio Montagna, Stefan Mach, Simone Benatti, Angelo Garofalo,\n  Gianmarco Ottavi, Luca Benini, Davide Rossi, Giuseppe Tagliavini", "title": "A transprecision floating-point cluster for efficient near-sensor data\n  analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications in the domain of near-sensor computing require the\nadoption of floating-point arithmetic to reconcile high precision results with\na wide dynamic range. In this paper, we propose a multi-core computing cluster\nthat leverages the fined-grained tunable principles of transprecision computing\nto provide support to near-sensor applications at a minimum power budget. Our\ndesign - based on the open-source RISC-V architecture - combines\nparallelization and sub-word vectorization with near-threshold operation,\nleading to a highly scalable and versatile system. We perform an exhaustive\nexploration of the design space of the transprecision cluster on a\ncycle-accurate FPGA emulator, with the aim to identify the most efficient\nconfigurations in terms of performance, energy efficiency, and area efficiency.\nWe also provide a full-fledged software stack support, including a parallel\nruntime and a compilation toolchain, to enable the development of end-to-end\napplications. We perform an experimental assessment of our design on a set of\nbenchmarks representative of the near-sensor processing domain, complementing\nthe timing results with a post place-&-route analysis of the power consumption.\nFinally, a comparison with the state-of-the-art shows that our solution\noutperforms the competitors in energy efficiency, reaching a peak of 97\nGflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision\nvectors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:38:26 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Montagna", "Fabio", ""], ["Mach", "Stefan", ""], ["Benatti", "Simone", ""], ["Garofalo", "Angelo", ""], ["Ottavi", "Gianmarco", ""], ["Benini", "Luca", ""], ["Rossi", "Davide", ""], ["Tagliavini", "Giuseppe", ""]]}, {"id": "2008.12501", "submitter": "Kazuichi Oe", "authors": "Kazuichi Oe", "title": "Analysis of Interference between RDMA and Local Access on Hybrid Memory\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can use a hybrid memory system consisting of DRAM and Intel Optane DC\nPersistent Memory (We call it DCPM in this paper) as DCPM is now commercially\navailable since April 2019. Even if the latency for DCPM is several times\nhigher than that for DRAM, the capacity for DCPM is several times higher than\nthat for DRAM and the cost of DCPM is also several times lower than that for\nDRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory\nsystem could improve the performance for in-memory database systems and virtual\nmachine (VM) systems because these systems often consume a large amount of\nmemory. Moreover, a high-speed shared storage system can be implemented by\naccessing DCPM via remote direct memory access (RDMA). I assume that some of\nthe DCPM is often assigned as a shared area among other remote servers because\napplications executed on a server with a hybrid memory system often cannot use\nthe entire capacity of DCPM. This paper evaluates the interference between\nlocal memory access and RDMA from a remote server. As a result, I indicate that\nthe interference on this hybrid memory system is significantly different from\nthat on a conventional DRAM-only memory system. I also believe that some kind\nof throttling implementation is needed when this interference occures.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 06:52:41 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Oe", "Kazuichi", ""]]}, {"id": "2008.12745", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Hanchen Ye, Junsong Wang, Yonghua Lin, Jinjun Xiong,\n  Wen-mei Hwu, Deming Chen", "title": "DNNExplorer: A Framework for Modeling and Exploring a Novel Paradigm of\n  FPGA-based DNN Accelerator", "comments": "Published as a conference paper at International Conference on\n  Computer Aided Design 2020 (ICCAD'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing FPGA-based DNN accelerators typically fall into two design\nparadigms. Either they adopt a generic reusable architecture to support\ndifferent DNN networks but leave some performance and efficiency on the table\nbecause of the sacrifice of design specificity. Or they apply a layer-wise\ntailor-made architecture to optimize layer-specific demands for computation and\nresources but loose the scalability of adaptation to a wide range of DNN\nnetworks. To overcome these drawbacks, this paper proposes a novel FPGA-based\nDNN accelerator design paradigm and its automation tool, called DNNExplorer, to\nenable fast exploration of various accelerator designs under the proposed\nparadigm and deliver optimized accelerator architectures for existing and\nemerging DNN networks. Three key techniques are essential for DNNExplorer's\nimproved performance, better specificity, and scalability, including (1) a\nunique accelerator design paradigm with both high-dimensional design space\nsupport and fine-grained adjustability, (2) a dynamic design space to\naccommodate different combinations of DNN workloads and targeted FPGAs, and (3)\na design space exploration (DSE) engine to generate optimized accelerator\narchitectures following the proposed paradigm by simultaneously considering\nboth FPGAs' computation and memory resources and DNN networks' layer-wise\ncharacteristics and overall complexity. Experimental results show that, for the\nsame FPGAs, accelerators generated by DNNExplorer can deliver up to 4.2x higher\nperformances (GOP/s) than the state-of-the-art layer-wise pipelined solutions\ngenerated by DNNBuilder for VGG-like DNN with 38 CONV layers. Compared to\naccelerators with generic reusable computation units, DNNExplorer achieves up\nto 2.0x and 4.4x DSP efficiency improvement than a recently published\naccelerator design from academia (HybridDNN) and a commercial DNN accelerator\nIP (Xilinx DPU), respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:03:49 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 21:10:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Ye", "Hanchen", ""], ["Wang", "Junsong", ""], ["Lin", "Yonghua", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Chen", "Deming", ""]]}, {"id": "2008.13430", "submitter": "Oriol Arcas-Abella", "authors": "Oriol Arcas-Abella and Abhinav Agarwal", "title": "Architectural Analysis of FPGA Technology Impact", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of high-level languages for designing hardware is gaining popularity\nsince they increase design productivity by providing higher abstractions.\nHowever, one drawback of such abstraction level has been the difficulty of\nrelating the low-level implementation problems back to the original high-level\ndesign, which is paramount for architectural optimization. In this work\n(developed between April 2013 and April 2014), we propose a methodology to\nanalyze the effects of technology over the architecture, and to generate\narchitectural-level area, delay and power metrics. Such feedback allows the\ndesigner to quickly gauge the impact of architectural decisions on the quality\nof generated hardware and opens the door to automatic architectural analysis.\nWe demonstrate the use of our technique on three FPGA platforms using two\ndesigns: a Reed-Solomon error correction decoder and a 32-bit pipelined\nprocessor implementation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:41:51 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Arcas-Abella", "Oriol", ""], ["Agarwal", "Abhinav", ""]]}, {"id": "2008.13664", "submitter": "Thomas Lange", "authors": "Thomas Lange, Aneesh Balakrishnan, Maximilien Glorieux, Dan\n  Alexandrescu, Luca Sterpone", "title": "Machine Learning Clustering Techniques for Selective Mitigation of\n  Critical Design Features", "comments": null, "journal-ref": null, "doi": "10.1109/IOLTS50870.2020.9159751", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective mitigation or selective hardening is an effective technique to\nobtain a good trade-off between the improvements in the overall reliability of\na circuit and the hardware overhead induced by the hardening techniques.\nSelective mitigation relies on preferentially protecting circuit instances\naccording to their susceptibility and criticality. However, ranking circuit\nparts in terms of vulnerability usually requires computationally intensive\nfault-injection simulation campaigns. This paper presents a new methodology\nwhich uses machine learning clustering techniques to group flip-flops with\nsimilar expected contributions to the overall functional failure rate, based on\nthe analysis of a compact set of features combining attributes from static\nelements and dynamic elements. Fault simulation campaigns can then be executed\non a per-group basis, significantly reducing the time and cost of the\nevaluation. The effectiveness of grouping similar sensitive flip-flops by\nmachine learning clustering algorithms is evaluated on a practical\nexample.Different clustering algorithms are applied and the results are\ncompared to an ideal selective mitigation obtained by exhaustive\nfault-injection simulation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 15:03:16 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 15:48:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lange", "Thomas", ""], ["Balakrishnan", "Aneesh", ""], ["Glorieux", "Maximilien", ""], ["Alexandrescu", "Dan", ""], ["Sterpone", "Luca", ""]]}]