[{"id": "2012.00050", "submitter": "Anup Das", "authors": "Shihao Song, Anup Das, Onur Mutlu, Nagarajan Kandasamy", "title": "Aging-Aware Request Scheduling for Non-Volatile Main Memory", "comments": "To appear in ASP-DAC 2021", "journal-ref": null, "doi": "10.1145/3394885.3431529", "report-no": null, "categories": "cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern computing systems are embracing non-volatile memory (NVM) to implement\nhigh-capacity and low-cost main memory. Elevated operating voltages of NVM\naccelerate the aging of CMOS transistors in the peripheral circuitry of each\nmemory bank. Aggressive device scaling increases power density and temperature,\nwhich further accelerates aging, challenging the reliable operation of\nNVM-based main memory. We propose HEBE, an architectural technique to mitigate\nthe circuit aging-related problems of NVM-based main memory. HEBE is built on\nthree contributions. First, we propose a new analytical model that can\ndynamically track the aging in the peripheral circuitry of each memory bank\nbased on the bank's utilization. Second, we develop an intelligent memory\nrequest scheduler that exploits this aging model at run time to de-stress the\nperipheral circuitry of a memory bank only when its aging exceeds a critical\nthreshold. Third, we introduce an isolation transistor to decouple parts of a\nperipheral circuit operating at different voltages, allowing the decoupled\nlogic blocks to undergo long-latency de-stress operations independently and off\nthe critical path of memory read and write accesses, improving performance. We\nevaluate HEBE with workloads from the SPEC CPU2017 Benchmark suite. Our results\nshow that HEBE significantly improves both performance and lifetime of\nNVM-based main memory.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:07:10 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Song", "Shihao", ""], ["Das", "Anup", ""], ["Mutlu", "Onur", ""], ["Kandasamy", "Nagarajan", ""]]}, {"id": "2012.00102", "submitter": "Aqeeb Iqbal Arka", "authors": "Aqeeb Iqbal Arka, Biresh Kumar Joardar, Ryan Gary Kim, Dae Hyun Kim,\n  Janardhan Rao Doppa, and Partha Pratim Pande", "title": "HeM3D: Heterogeneous Manycore Architecture Based on Monolithic 3D\n  Vertical Integration", "comments": "This work has been accepted in ACM Transactions on Design Automation\n  of Electronic Systems", "journal-ref": null, "doi": "10.1145/3424239", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous manycore architectures are the key to efficiently execute\ncompute- and data-intensive applications. Through silicon via (TSV)-based 3D\nmanycore system is a promising solution in this direction as it enables\nintegration of disparate computing cores on a single system. However, the\nachievable performance of conventional through-silicon-via (TSV)-based 3D\nsystems is ultimately bottlenecked by the horizontal wires (wires in each\nplanar die). Moreover, current TSV 3D architectures suffer from thermal\nlimitations. Hence, TSV-based architectures do not realize the full potential\nof 3D integration. Monolithic 3D (M3D) integration, a breakthrough technology\nto achieve - More Moore and More Than Moore - and opens up the possibility of\ndesigning cores and associated network routers using multiple layers by\nutilizing monolithic inter-tier vias (MIVs) and hence, reducing the effective\nwire length. Compared to TSV-based 3D ICs, M3D offers the true benefits of\nvertical dimension for system integration: the size of a MIV used in M3D is\nover 100x smaller than a TSV. In this work, we demonstrate how M3D-enabled\nvertical core and uncore elements offer significant performance and thermal\nimprovements in manycore heterogeneous architectures compared to its TSV-based\ncounterpart. To overcome the difficult optimization challenges due to the large\ndesign space and complex interactions among the heterogeneous components (CPU,\nGPU, Last Level Cache, etc.) in an M3D-based manycore chip, we leverage novel\ndesign-space exploration algorithms to trade-off different objectives. The\nproposed M3D-enabled heterogeneous architecture, called HeM3D, outperforms its\nstate-of-the-art TSV-equivalent counterpart by up to 18.3% in execution time\nwhile being up to 19 degrees Celcius cooler.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:23:41 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 02:07:11 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Arka", "Aqeeb Iqbal", ""], ["Joardar", "Biresh Kumar", ""], ["Kim", "Ryan Gary", ""], ["Kim", "Dae Hyun", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""]]}, {"id": "2012.00158", "submitter": "Benjamin Cho", "authors": "Benjamin Y. Cho, Jeageun Jung, Mattan Erez", "title": "Accelerating Bandwidth-Bound Deep Learning Inference with Main-Memory\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DL inference queries play an important role in diverse internet services and\na large fraction of datacenter cycles are spent on processing DL inference\nqueries. Specifically, the matrix-matrix multiplication (GEMM) operations of\nfully-connected MLP layers dominate many inference tasks. We find that the GEMM\noperations for datacenter DL inference tasks are memory bandwidth bound,\ncontrary to common assumptions: (1) strict query latency constraints force\nsmall-batch operation, which limits reuse and increases bandwidth demands; and\n(2) large and colocated models require reading the large weight matrices from\nmain memory, again requiring high bandwidth without offering reuse\nopportunities. We demonstrate the large potential of accelerating these\nsmall-batch GEMMs with processing in the main CPU memory. We develop a novel\nGEMM execution flow and corresponding memory-side address-generation logic that\nexploits GEMM locality and enables long-running PIM kernels despite the complex\naddress-mapping functions employed by the CPU that would otherwise destroy\nlocality. Our evaluation of StepStone variants at the channel, device, and\nwithin-device PIM levels, along with optimizations that balance parallelism\nbenefits with data-distribution overheads demonstrate $12\\times$ better minimum\nlatency than a CPU and $2.8\\times$ greater throughput for strict query latency\nconstraints. End-to-end performance analysis of recent recommendation and\nlanguage models shows that StepStone PIM outperforms a fast CPU (by up to\n$16\\times$) and prior main-memory acceleration approaches (by up to $2.4\\times$\ncompared to the best prior approach).\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 23:22:42 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Cho", "Benjamin Y.", ""], ["Jung", "Jeageun", ""], ["Erez", "Mattan", ""]]}, {"id": "2012.00581", "submitter": "Marzieh Hashemipour-Nazar", "authors": "Marzieh Hashemipour-Nazari, Kees Goossens and Alexios\n  Balatsoukas-Stimming", "title": "Hardware Implementation of Iterative Projection-Aggregation Decoding of\n  Reed-Muller Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a simplification and a corresponding hardware\narchitecture for hard-decision recursive projection-aggregation (RPA) decoding\nof Reed-Muller (RM) codes. In particular, we transform the recursive structure\nof RPA decoding into a simpler and iterative structure with minimal\nerror-correction degradation. Our simulation results for RM(7,3) show that the\nproposed simplification has a small error-correcting performance degradation\n(0.005 in terms of channel crossover probability) while reducing the average\nnumber of computations by up to 40%. In addition, we describe the first fully\nparallel hardware architecture for simplified RPA decoding. We present FPGA\nimplementation results for an RM(6,3) code on a Xilinx Virtex-7 FPGA showing\nthat our proposed architecture achieves a throughput of 171 Mbps at a frequency\nof 80 MHz.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:38:06 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hashemipour-Nazari", "Marzieh", ""], ["Goossens", "Kees", ""], ["Balatsoukas-Stimming", "Alexios", ""]]}, {"id": "2012.01114", "submitter": "Mingfei Yu", "authors": "Mingfei Yu and Masahiro Fujita", "title": "Parallel Scheduling Self-attention Mechanism: Generalization and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, self-attention is shining in the field of deep\nlearning, especially in the domain of natural language processing(NLP). Its\nimpressive effectiveness, along with ubiquitous implementations, have aroused\nour interest in efficiently scheduling the data-flow of corresponding\ncomputations onto architectures with many computing units to realize parallel\ncomputing. In this paper, based on the theory of self-attention mechanism and\nstate-of-the-art realization of self-attention in language models, we propose a\ngeneral scheduling algorithm, which is derived from the optimum scheduling for\nsmall instances solved by a satisfiability checking(SAT) solver, to parallelize\ntypical computations of self-attention. Strategies for further optimization on\nskipping redundant computations are put forward as well, with which reductions\nof almost 25% and 50% of the original computations are respectively achieved\nfor two widely-adopted application schemes of self-attention. With the proposed\noptimization adopted, we have correspondingly come up with another two\nscheduling algorithms. The proposed algorithms are applicable regardless of\nproblem sizes, as long as the number of input vectors is divisible to the\nnumber of computing units available in the architecture. Due to the complexity\nof proving the correctness of the algorithms mathematically for general cases,\nwe have conducted experiments to reveal their validity, together with the\nsuperior quality of the solutions provided by which, by solving SAT problems\nfor particular instances.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 12:04:16 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yu", "Mingfei", ""], ["Fujita", "Masahiro", ""]]}, {"id": "2012.01153", "submitter": "Sumit Darak Dr", "authors": "Neelam Singh, S. V. Sai Santosh, Sumit J. Darak", "title": "Towards Intelligent Reconfigurable Wireless Physical Layer (PHY)", "comments": null, "journal-ref": "OJCAS Special Section on Circuits, Systems, and Algorithms for\n  Beyond 5G and towards 6G, 2020", "doi": null, "report-no": null, "categories": "eess.SY cs.AR cs.SY eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Next-generation wireless networks are getting significant attention because\nthey promise 10-factor enhancement in mobile broadband along with the potential\nto enable new heterogeneous services. Services include massive machine type\ncommunications desired for Industrial 4.0 along with ultra-reliable low latency\nservices for remote healthcare and vehicular communications. In this paper, we\npresent the design of an intelligent and reconfigurable physical layer (PHY) to\nbring these services to reality. First, we design and implement the\nreconfigurable PHY via a hardware-software co-design approach on system-on-chip\nconsisting of the ARM processor and field-programmable gate array (FPGA). The\nreconfigurable PHY is then made intelligent by augmenting it with online\nmachine learning (OML) based decision-making algorithm. Such PHY can learn the\nenvironment (for example, wireless channel) and dynamically adapt the\ntransceivers' configuration (i.e., modulation scheme, word-length) and select\nthe wireless channel on-the-fly. Since the environment is unknown and changes\nwith time, we make the OML architecture reconfigurable to enable dynamic switch\nbetween various OML algorithms on-the-fly. We have demonstrated the functional\ncorrectness of the proposed architecture for different environments and\nword-lengths. The detailed throughput, latency, and complexity analysis\nvalidate the feasibility and importance of the proposed intelligent and\nreconfigurable PHY in next-generation networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 12:53:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Singh", "Neelam", ""], ["Santosh", "S. V. Sai", ""], ["Darak", "Sumit J.", ""]]}, {"id": "2012.01267", "submitter": "Daniel Etiemble", "authors": "Daniel Etiemble", "title": "Multivalued circuits and Interconnect issues", "comments": "6 pages, 12 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many papers have presented multi-valued circuits in various technologies as a\nsolution to reduce or solve interconnection issues in binary circuits. This\nassumption is discussed. While 4-valued signaling could divide by two the\nnumber of interconnects between building blocks, it turns out that circuit\ndesigners use interconnect standards based on differential pairs such as PCIe,\nInfiniband, RapidIO, NVLink, etc. Doubling the number of binary signals is a\nbetter solution than using single-ended quaternary signals. The design of\nquaternary basic gates, adders and multipliers are compared with the\ncorresponding binary ones.\n  At each level, the transistor count ratio between quaternary and binary\ncircuits is greater than the x2 information ratio between base 4 and base 2.\nQuaternary signaling is not a solution, either between or within circuit blocks .\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 15:22:01 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Etiemble", "Daniel", ""]]}, {"id": "2012.01353", "submitter": "Yiming Gan", "authors": "Yiming Gan, Bo Yu, Boyuan Tian, Leimeng Xu, Wei Hu, Shaoshan Liu,\n  Qiang Liu, Yanjun Zhang, Jie Tang, Yuhao Zhu", "title": "Eudoxus: Characterizing and Accelerating Localization in Autonomous\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop and commercialize autonomous machines, such as logistic robots and\nself-driving cars, around the globe. A critical challenge to our -- and any --\nautonomous machine is accurate and efficient localization under resource\nconstraints, which has fueled specialized localization accelerators recently.\nPrior acceleration efforts are point solutions in that they each specialize for\na specific localization algorithm. In real-world commercial deployments,\nhowever, autonomous machines routinely operate under different environments and\nno single localization algorithm fits all the environments. Simply stacking\ntogether point solutions not only leads to cost and power budget overrun, but\nalso results in an overly complicated software stack.\n  This paper demonstrates our new software-hardware co-designed framework for\nautonomous machine localization, which adapts to different operating scenarios\nby fusing fundamental algorithmic primitives. Through characterizing the\nsoftware framework, we identify ideal acceleration candidates that contribute\nsignificantly to the end-to-end latency and/or latency variation. We show how\nto co-design a hardware accelerator to systematically exploit the parallelisms,\nlocality, and common building blocks inherent in the localization framework. We\nbuild, deploy, and evaluate an FPGA prototype on our next-generation\nself-driving cars. To demonstrate the flexibility of our framework, we also\ninstantiate another FPGA prototype targeting drones, which represent mobile\nautonomous machines. We achieve about 2x speedup and 4x energy reduction\ncompared to widely-deployed, optimized implementations on general-purpose\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:50:34 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 04:07:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Gan", "Yiming", ""], ["Yu", "Bo", ""], ["Tian", "Boyuan", ""], ["Xu", "Leimeng", ""], ["Hu", "Wei", ""], ["Liu", "Shaoshan", ""], ["Liu", "Qiang", ""], ["Zhang", "Yanjun", ""], ["Tang", "Jie", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2012.01571", "submitter": "Jeffrey Young", "authors": "Patrick Lavin, Jeffrey Young, Rich Vuduc, Jonathan Beard", "title": "Online Model Swapping in Architectural Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As systems and applications grow more complex, detailed simulation takes an\never increasing amount of time. The prospect of increased simulation time\nresulting in slower design iteration forces architects to use simpler models,\nsuch as spreadsheets, when they want to iterate quickly on a design. However,\nthe task of migrating from a simple simulation to one with more detail often\nrequires multiple executions to find where simple models could be effective,\nwhich could be more expensive than running the detailed model in the first\nplace. Also, architects must often rely on intuition to choose these simpler\nmodels, further complicating the problem.\n  In this work, we present a method of bridging the gap between simple and\ndetailed simulation by monitoring simulation behavior online and automatically\nswapping out detailed models with simpler statistical approximations. We\ndemonstrate the potential of our methodology by implementing it in the\nopen-source simulator SVE-Cachesim to swap out the level one data cache (L1D)\nwithin a memory hierarchy. This proof of concept demonstrates that our\ntechnique can handle a non-trivial use-case in not just approximation of local\ntime-invariant statistics, but also those that vary with time (e.g., the L1D is\na form of a time-series function), and downstream side-effects (e.g., the L1D\nfilters accesses for the level two cache). Our simulation swaps out the\nbuilt-in cache model with only an 8% error in the simulated cycle count while\nusing the approximated cache models for over 90% of the simulation, and our\nsimpler models require two to eight times less computation per \"execution\" of\nthe model\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 22:12:24 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Lavin", "Patrick", ""], ["Young", "Jeffrey", ""], ["Vuduc", "Rich", ""], ["Beard", "Jonathan", ""]]}, {"id": "2012.02037", "submitter": "Lukas Burgholzer", "authors": "Lukas Burgholzer, Robert Wille and Richard Kueng", "title": "Characteristics of Reversible Circuits for Error Detection", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider error detection via simulation for reversible\ncircuit architectures. We rigorously prove that reversibility augments the\nperformance of this simple error detection protocol to a considerable degree. A\nsingle randomly generated input is guaranteed to unveil a single error with a\nprobability that only depends on the size of the error, not the size of the\ncircuit itself. Empirical studies confirm that this behavior typically extends\nto multiple errors as well. In conclusion, reversible circuits offer\ncharacteristics that reduce masking effects -- a desirable feature that is in\nstark contrast to irreversible circuit architectures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:22:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Burgholzer", "Lukas", ""], ["Wille", "Robert", ""], ["Kueng", "Richard", ""]]}, {"id": "2012.02453", "submitter": "Samhita Varambally", "authors": "B. Samhita Varambally, Naman Sehgal", "title": "Optimising Design Verification Using Machine Learning: An Open Source\n  Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the complexity of Integrated Circuits increasing, design verification\nhas become the most time consuming part of the ASIC design flow. Nearly 70% of\nthe SoC design cycle is consumed by verification. The most commonly used\napproach to test all corner cases is through the use of Constrained Random\nVerification. Random stimulus is given in order to hit all possible\ncombinations and test the design thoroughly. However, this approach often\nrequires significant human expertise to reach all corner cases. This paper\npresents an alternative using Machine Learning to generate the input stimulus.\nThis will allow for faster thorough verification of the design with less human\nintervention. Furthermore, it is proposed to use the open source verification\nenvironment 'Cocotb'. Based on Python, it is simple, intuitive and has a vast\nlibrary of functions for machine learning applications. This makes it more\nconvenient to use than the bulkier approach using traditional Hardware\nVerification Languages such as System Verilog or Specman E.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:18:05 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Varambally", "B. Samhita", ""], ["Sehgal", "Naman", ""]]}, {"id": "2012.02695", "submitter": "Ramtin Zand", "authors": "Ramtin Zand", "title": "A Single-Cycle MLP Classifier Using Analog MRAM-based Neurons and\n  Synapses", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.01238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, spin-orbit torque (SOT) magnetoresistive random-access memory\n(MRAM) devices are leveraged to realize sigmoidal neurons and binarized\nsynapses for a single-cycle analog in-memory computing (IMC) architecture.\nFirst, an analog SOT-MRAM-based neuron bitcell is proposed which achieves a 12x\nreduction in power-area-product compared to the previous most power- and\narea-efficient analog sigmoidal neuron design. Next, proposed neuron and\nsynapse bit cells are used within memory subarrays to form an analog IMC-based\nmultilayer perceptron (MLP) architecture for the MNIST pattern recognition\napplication. The architecture-level results exhibit that our analog IMC\narchitecture achieves at least two and four orders of magnitude performance\nimprovement compared to a mixed-signal analog/digital IMC architecture and a\ndigital GPU implementation, respectively while realizing a comparable\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:04:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Zand", "Ramtin", ""]]}, {"id": "2012.02715", "submitter": "Leila Delshadtehrani", "authors": "Leila Delshadtehrani, Sadullah Canakci, Manuel Egele, Ajay Joshi", "title": "Efficient Sealable Protection Keys for RISC-V", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous increase in the number of software-based attacks, there\nhas been a growing effort towards isolating sensitive data and trusted software\ncomponents from untrusted third-party components. A hardware-assisted\nintra-process isolation mechanism enables software developers to partition a\nprocess into isolated components and in turn secure sensitive data from\nuntrusted components. However, most of the existing hardware-assisted\nintra-process isolation mechanisms in modern processors, such as ARM and IBM\nPower, rely on costly kernel operations for switching between trusted and\nuntrusted domains. Recently, Intel introduced a new hardware feature for\nintra-process memory isolation, called Memory Protection Keys (MPK), which\nenables a user-space process to switch the domains in an efficient way. While\nthe efficiency of Intel MPK enables developers to leverage it for common use\ncases such as Code-Pointer Integrity, the limited number of unique domains (16)\nprohibits its use in cases such as OpenSSL where a large number of domains are\nrequired. Moreover, Intel MPK suffers from the protection key use-after-free\nvulnerability. To address these shortcomings, in this paper, we propose an\nefficient intra-process isolation technique for the RISC-V open ISA, called\nSealPK, which supports up to 1024 unique domains. SealPK prevents the\nprotection key use-after-free problem by leveraging a lazy de-allocation\napproach. To further strengthen SealPK, we devise three novel sealing features\nto protect the allocated domains, their associated pages, and their permissions\nfrom modifications or tampering by an attacker. To demonstrate the feasibility\nof our design, we implement SealPK on a RISC-V Rocket processor, provide the OS\nsupport for it, and prototype our design on an FPGA. We demonstrate the\nefficiency of SealPK by leveraging it to implement an isolated shadow stack on\nour FPGA prototype.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:58:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Delshadtehrani", "Leila", ""], ["Canakci", "Sadullah", ""], ["Egele", "Manuel", ""], ["Joshi", "Ajay", ""]]}, {"id": "2012.02791", "submitter": "Jeremy Blackstone", "authors": "Jeremy Blackstone (University of California, San Diego, USA), Wei Hu\n  (Northwestern Polytechnical University, China), Alric Althoff, Armaiti\n  Ardeshiricham (University of California, San Diego, USA), Lu Zhang\n  (Northwestern Polytechnical University, China), Ryan Kastner (University of\n  California, San Diego, USA)", "title": "A Unified Model for Gate Level Propagation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic hardware verification techniques (e.g., X-propagation and\nfault-propagation) and more recent hardware security verification techniques\nbased on information flow tracking (IFT) aim to understand how information\npasses, affects, and otherwise modifies a circuit. These techniques all have\nseparate usage scenarios, but when dissected into their core functionality,\nthey relate in a fundamental manner. In this paper, we develop a common\nframework for gate level propagation analysis. We use our model to generate\nsynthesizable propagation logic to use in standard EDA tools. To justify our\nmodel, we prove that Precise Hardware IFT is equivalent to gate level\nX-propagation and imprecise fault propagation. We also show that the difference\nbetween Precise Hardware IFT and fault propagation is not significant for\n74X-series and '85 ISCAS benchmarks with more than 313 gates and the difference\nbetween imprecise hardware IFT and Precise Hardware IFT is almost always\nsignificant regardless of size.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 17:44:58 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Blackstone", "Jeremy", "", "University of California, San Diego, USA"], ["Hu", "Wei", "", "Northwestern Polytechnical University, China"], ["Althoff", "Alric", "", "University of California, San Diego, USA"], ["Ardeshiricham", "Armaiti", "", "University of California, San Diego, USA"], ["Zhang", "Lu", "", "Northwestern Polytechnical University, China"], ["Kastner", "Ryan", "", "University of\n  California, San Diego, USA"]]}, {"id": "2012.02890", "submitter": "Hao Luan", "authors": "Alan Gatherer, Ashish Shrivastava, Hao Luan, Asheesh Kashyap, Zhenguo\n  Gu, Miguel Dajer", "title": "Towards a Domain Specific Solution for a New Generation of Wireless\n  Modems", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless cellular System on Chip (SoC) are experiencing unprecedented demands\non data rate, latency use case variety. 5G wireless technologies require a\nmassive number of antennas and complex signal processing to improve bandwidth\nand spectral efficiency. The Internet of Things is causing a proliferation in\nthe number of connected devices, and service categories, such as ultra-reliable\nlow latency, which will produce new use cases, such as self-driving cars,\nrobotic factories, and remote surgery. In addressing these challenges, we can\nno longer rely on faster cores, or even more silicon. Modem software\ndevelopment is becoming increasingly error prone and difficult as the\ncomplexity of the applications and the architectures increase.\n  In this report we propose a Wireless Domain Specific Solution that takes a\nDataflow acceleration approach and addresses the need of the SoC to support\ndataflows that change with use case and user activity, while maintaining the\nFirm Real Time High Availability with low probability of Heisenbugs that is\nrequired in cellular modems. We do this by developing a Domain Specific\nArchitecture that describes the requirements in a suitably abstracted dataflow\nDomain Specific language. A toolchain is described that automates translation\nof those requirements in an efficient and robust manner and provides formal\nguarantees against Heisenbugs. The dataflow native DSA supports the toolchain\noutput with specialized processing, data management and control features with\nhigh performance and low power, and recovers rapidly from dropped dataflows\nwhile continuing to achieve the real time requirements.\n  This report focuses on the dataflow acceleration in the DSA and the part of\nthe automated toolchain that formally checks the performance and correctness of\nsoftware running on this dataflow hardware. Results are presented and a summary\nof future work is given.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:08:58 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gatherer", "Alan", ""], ["Shrivastava", "Ashish", ""], ["Luan", "Hao", ""], ["Kashyap", "Asheesh", ""], ["Gu", "Zhenguo", ""], ["Dajer", "Miguel", ""]]}, {"id": "2012.02973", "submitter": "Matheus Cavalcante", "authors": "Matheus Cavalcante, Samuel Riedel, Antonio Pullini, Luca Benini", "title": "MemPool: A Shared-L1 Memory Many-Core Cluster with a Low-Latency\n  Interconnect", "comments": "Accepted for publication in the Design, Automation and Test in Europe\n  (DATE) Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge in scaling shared-L1 multi-core clusters towards many-core\n(more than 16 cores) configurations is to ensure low-latency and efficient\naccess to the L1 memory. In this work we demonstrate that it is possible to\nscale up the shared-L1 architecture: We present MemPool, a 32 bit many-core\nsystem with 256 fast RV32IMA \"Snitch\" cores featuring application-tunable\nexecution units, running at 700 MHz in typical conditions (TT/0.80\nV/25{\\deg}C). MemPool is easy to program, with all the cores sharing a global\nview of a large L1 scratchpad memory pool, accessible within at most 5 cycles.\nIn MemPool's physical-aware design, we emphasized the exploration, design, and\noptimization of the low-latency processor-to-L1-memory interconnect. We compare\nthree candidate topologies, analyzing them in terms of latency, throughput, and\nback-end feasibility. The chosen topology keeps the average latency at fewer\nthan 6 cycles, even for a heavy injected load of 0.33 request/core/cycle. We\nalso propose a lightweight addressing scheme that maps each core private data\nto a memory bank accessible within one cycle, which leads to performance gains\nof up to 20% in real-world signal processing benchmarks. The addressing scheme\nis also highly efficient in terms of energy consumption since requests to local\nbanks consume only half of the energy required to access remote banks. Our\ndesign achieves competitive performance with respect to an ideal,\nnon-implementable full-crossbar baseline.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 08:18:47 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cavalcante", "Matheus", ""], ["Riedel", "Samuel", ""], ["Pullini", "Antonio", ""], ["Benini", "Luca", ""]]}, {"id": "2012.03112", "submitter": "Juan G\\'omez-Luna", "authors": "Onur Mutlu, Saugata Ghose, Juan G\\'omez-Luna, Rachata Ausavarungnirun", "title": "A Modern Primer on Processing in Memory", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.03988", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern computing systems are overwhelmingly designed to move data to\ncomputation. This design choice goes directly against at least three key trends\nin computing that cause performance, scalability and energy bottlenecks: (1)\ndata access is a key bottleneck as many important applications are increasingly\ndata-intensive, and memory bandwidth and energy do not scale well, (2) energy\nconsumption is a key limiter in almost all computing platforms, especially\nserver and mobile systems, (3) data movement, especially off-chip to on-chip,\nis very expensive in terms of bandwidth, energy and latency, much more so than\ncomputation. These trends are especially severely-felt in the data-intensive\nserver and energy-constrained mobile systems of today. At the same time,\nconventional memory technology is facing many technology scaling challenges in\nterms of reliability, energy, and performance. As a result, memory system\narchitects are open to organizing memory in different ways and making it more\nintelligent, at the expense of higher cost. The emergence of 3D-stacked memory\nplus logic, the adoption of error correcting codes inside the latest DRAM\nchips, proliferation of different main memory standards and chips, specialized\nfor different purposes (e.g., graphics, low-power, high bandwidth, low\nlatency), and the necessity of designing new solutions to serious reliability\nand security issues, such as the RowHammer phenomenon, are an evidence of this\ntrend. This chapter discusses recent research that aims to practically enable\ncomputation close to data, an approach we call processing-in-memory (PIM). PIM\nplaces computation mechanisms in or near where the data is stored (i.e., inside\nthe memory chips, in the logic layer of 3D-stacked memory, or in the memory\ncontrollers), so that data movement between the computation units and memory is\nreduced or eliminated.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:59:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mutlu", "Onur", ""], ["Ghose", "Saugata", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Ausavarungnirun", "Rachata", ""]]}, {"id": "2012.03177", "submitter": "Yixing Li", "authors": "Akshay Dua, Yixing Li, Fengbo Ren", "title": "Systolic-CNN: An OpenCL-defined Scalable Run-time-flexible FPGA\n  Accelerator Architecture for Accelerating Convolutional Neural Network\n  Inference in Cloud/Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Systolic-CNN, an OpenCL-defined scalable,\nrun-time-flexible FPGA accelerator architecture, optimized for accelerating the\ninference of various convolutional neural networks (CNNs) in multi-tenancy\ncloud/edge computing. The existing OpenCL-defined FPGA accelerators for CNN\ninference are insufficient due to limited flexibility for supporting multiple\nCNN models at run time and poor scalability resulting in underutilized FPGA\nresources and limited computational parallelism. Systolic-CNN adopts a highly\npipelined and paralleled 1-D systolic array architecture, which efficiently\nexplores both spatial and temporal parallelism for accelerating CNN inference\non FPGAs. Systolic-CNN is highly scalable and parameterized, which can be\neasily adapted by users to achieve up to 100% utilization of the coarse-grained\ncomputation resources (i.e., DSP blocks) for a given FPGA. Systolic-CNN is also\nrun-time-flexible in the context of multi-tenancy cloud/edge computing, which\ncan be time-shared to accelerate a variety of CNN models at run time without\nthe need of recompiling the FPGA kernel hardware nor reprogramming the FPGA.\nThe experiment results based on an Intel Arria/Stratix 10 GX FPGA Development\nboard show that the optimized single-precision implementation of Systolic-CNN\ncan achieve an average inference latency of 7ms/2ms, 84ms/33ms, 202ms/73ms,\n1615ms/873ms, and 900ms/498ms per image for accelerating AlexNet, ResNet-50,\nResNet-152, RetinaNet, and Light-weight RetinaNet, respectively. Codes are\navailable at https://github.com/PSCLab-ASU/Systolic-CNN.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 03:53:11 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Dua", "Akshay", ""], ["Li", "Yixing", ""], ["Ren", "Fengbo", ""]]}, {"id": "2012.03402", "submitter": "Adrian Wheeldon", "authors": "Adrian Wheeldon, Alex Yakovlev, Rishad Shafik, Jordan Morris", "title": "Low-Latency Asynchronous Logic Design for Inference at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern internet of things (IoT) devices leverage machine learning inference\nusing sensed data on-device rather than offloading them to the cloud. Commonly\nknown as inference at-the-edge, this gives many benefits to the users,\nincluding personalization and security. However, such applications demand high\nenergy efficiency and robustness. In this paper we propose a method for reduced\narea and power overhead of self-timed early-propagative asynchronous inference\ncircuits, designed using the principles of learning automata. Due to natural\nresilience to timing as well as logic underpinning, the circuits are tolerant\nto variations in environment and supply voltage whilst enabling the lowest\npossible latency. Our method is exemplified through an inference datapath for a\nlow power machine learning application. The circuit builds on the Tsetlin\nmachine algorithm further enhancing its energy efficiency. Average latency of\nthe proposed circuit is reduced by 10x compared with the synchronous\nimplementation whilst maintaining similar area. Robustness of the proposed\ncircuit is proven through post-synthesis simulation with 0.25 V to 1.2 V\nsupply. Functional correctness is maintained and latency scales with gate delay\nas voltage is decreased.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 00:40:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wheeldon", "Adrian", ""], ["Yakovlev", "Alex", ""], ["Shafik", "Rishad", ""], ["Morris", "Jordan", ""]]}, {"id": "2012.03481", "submitter": "Juergen Wassner", "authors": "Mario Fischer, Juergen Wassner (Department of Engineering and\n  Architecture, Lucerne University of Applied Sciences and Arts, Switzerland)", "title": "BinArray: A Scalable Hardware Accelerator for Binary Approximated CNNs", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": "10.1109/CCWC51732.2021.9375963", "report-no": null, "categories": "cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have become state-of-the art for\ncomputer vision and other signal processing tasks due to their superior\naccuracy. In recent years, large efforts have been made to reduce the\ncomputational costs of CNNs in order to achieve real-time operation on\nlow-power embedded devices.\n  Towards this goal we present BinArray, a custom hardware accelerator for CNNs\nwith binary approximated weights. The binary approximation used in this paper\nis an improved version of a network compression technique initially suggested\nin [1]. It drastically reduces the number of multiplications required per\ninference with no or very little accuracy degradation. BinArray easily scales\nand allows to compromise between hardware resource usage and throughput by\nmeans of three design parameters transparent to the user. Furthermore, it is\npossible to select between high accuracy or throughput dynamically during\nruntime. BinArray has been optimized at the register transfer level and\noperates at 400 MHz as instruction-set processor within a heterogenous\nXC7Z045-2 FPGA-SoC platform.\n  Experimental results show that BinArray scales to match the performance of\nother accelerators like EdgeTPU [2] for different network sizes. Even for the\nlargest MobileNet only 50% of the target device and only 96 DSP blocks are\nutilized.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:02:40 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Fischer", "Mario", "", "Department of Engineering and\n  Architecture, Lucerne University of Applied Sciences and Arts, Switzerland"], ["Wassner", "Juergen", "", "Department of Engineering and\n  Architecture, Lucerne University of Applied Sciences and Arts, Switzerland"]]}, {"id": "2012.03666", "submitter": "Shane Gilroy", "authors": "Shane Gilroy", "title": "Impact of Power Supply Noise on Image Sensor Performance in Automotive\n  Applications", "comments": null, "journal-ref": "Waterford Institute of Technology 2016", "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.CV cs.RO eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vision Systems are quickly becoming a large component of Active Automotive\nSafety Systems. In order to be effective in critical safety applications these\nsystems must produce high quality images in both daytime and night-time\nscenarios in order to provide the large informational content required for\nsoftware analysis in applications such as lane departure, pedestrian detection\nand collision detection. The challenge in capturing high quality images in low\nlight scenarios is that the signal to noise ratio is greatly reduced, which can\nresult in noise becoming the dominant factor in a captured image, thereby\nmaking these safety systems less effective at night. Research has been\nundertaken to develop a systematic method of characterising image sensor\nperformance in response to electrical noise in order to improve the design and\nperformance of automotive cameras in low light scenarios. The root cause of\nimage row noise has been established and a mathematical algorithm for\ndetermining the magnitude of row noise in an image has been devised. An\nautomated characterisation method has been developed to allow performance\ncharacterisation in response to a large frequency spectrum of electrical noise\non the image sensor power supply. Various strategies of improving image sensor\nperformance for low light applications have also been proposed from the\nresearch outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:25:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gilroy", "Shane", ""]]}, {"id": "2012.03672", "submitter": "Jun Xiong", "authors": "Xiong Jun", "title": "FPGA deep learning acceleration based on convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In view of the large amount of calculation and long calculation time of\nconvolutional neural network (CNN), this paper proposes a convolutional neural\nnetwork hardware accelerator based on field programmable logic gate array\n(FPGA). First, through in-depth analysis of the forward operation principle of\nthe convolutional layer and exploration of the parallelism of the convolutional\nlayer operation, a hardware architecture of input channel parallelism, output\nchannel parallelism and convolution window deep pipeline is designed. Then in\nthe above architecture, a fully parallel multiplication-addition tree module is\ndesigned to accelerate the convolution operation and an efficient window buffer\nmodule to implement the pipeline operation of the convolution window. The final\nexperimental results show that the energy efficiency ratio of the accelerator\nproposed in this article reaches 32.73 GOPS/W, which is 34% higher than the\nexisting solution, and the performance reaches 317.86 GOPS.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:20:44 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Jun", "Xiong", ""]]}, {"id": "2012.04105", "submitter": "Ayaz Akram", "authors": "Ayaz Akram and Jason Lowe-Power", "title": "The Tribes of Machine Learning and the Realm of Computer Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques have influenced the field of computer\narchitecture like many other fields. This paper studies how the fundamental\nmachine learning techniques can be applied towards computer architecture\nproblems. We also provide a detailed survey of computer architecture research\nthat employs different machine learning methods. Finally, we present some\nfuture opportunities and the outstanding challenges that need to be overcome to\nexploit full potential of machine learning for computer architecture.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 23:10:51 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Akram", "Ayaz", ""], ["Lowe-Power", "Jason", ""]]}, {"id": "2012.04210", "submitter": "Ahmet Inci", "authors": "Ahmet Inci, Evgeny Bolotin, Yaosheng Fu, Gal Dalal, Shie Mannor, David\n  Nellans, Diana Marculescu", "title": "The Architectural Implications of Distributed Reinforcement Learning on\n  CPU-GPU Systems", "comments": "To appear in the proceedings of the 6th Workshop on Energy Efficient\n  Machine Learning and Cognitive Computing (EMC2) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With deep reinforcement learning (RL) methods achieving results that exceed\nhuman capabilities in games, robotics, and simulated environments, continued\nscaling of RL training is crucial to its deployment in solving complex\nreal-world problems. However, improving the performance scalability and power\nefficiency of RL training through understanding the architectural implications\nof CPU-GPU systems remains an open problem. In this work we investigate and\nimprove the performance and power efficiency of distributed RL training on\nCPU-GPU systems by approaching the problem not solely from the GPU\nmicroarchitecture perspective but following a holistic system-level analysis\napproach. We quantify the overall hardware utilization on a state-of-the-art\ndistributed RL training framework and empirically identify the bottlenecks\ncaused by GPU microarchitectural, algorithmic, and system-level design choices.\nWe show that the GPU microarchitecture itself is well-balanced for\nstate-of-the-art RL frameworks, but further investigation reveals that the\nnumber of actors running the environment interactions and the amount of\nhardware resources available to them are the primary performance and power\nefficiency limiters. To this end, we introduce a new system design metric,\nCPU/GPU ratio, and show how to find the optimal balance between CPU and GPU\nresources when designing scalable and efficient CPU-GPU systems for RL\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:50:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Inci", "Ahmet", ""], ["Bolotin", "Evgeny", ""], ["Fu", "Yaosheng", ""], ["Dalal", "Gal", ""], ["Mannor", "Shie", ""], ["Nellans", "David", ""], ["Marculescu", "Diana", ""]]}, {"id": "2012.04240", "submitter": "Sung-En Chang", "authors": "Sung-En Chang, Yanyu Li, Mengshu Sun, Runbin Shi, Hayden K.-H. So,\n  Xuehai Qian, Yanzhi Wang, Xue Lin", "title": "Mix and Match: A Novel FPGA-Centric Deep Neural Network Quantization\n  Framework", "comments": "Accepted by High-Performance Computer Architecture (HPCA'2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) have achieved extraordinary performance in\nvarious application domains. To support diverse DNN models, efficient\nimplementations of DNN inference on edge-computing platforms, e.g., ASICs,\nFPGAs, and embedded systems, are extensively investigated. Due to the huge\nmodel size and computation amount, model compression is a critical step to\ndeploy DNN models on edge devices. This paper focuses on weight quantization, a\nhardware-friendly model compression approach that is complementary to weight\npruning. Unlike existing methods that use the same quantization scheme for all\nweights, we propose the first solution that applies different quantization\nschemes for different rows of the weight matrix. It is motivated by (1) the\ndistribution of the weights in the different rows are not the same; and (2) the\npotential of achieving better utilization of heterogeneous FPGA hardware\nresources. To achieve that, we first propose a hardware-friendly quantization\nscheme named sum-of-power-of-2 (SP2) suitable for Gaussian-like weight\ndistribution, in which the multiplication arithmetic can be replaced with logic\nshifter and adder, thereby enabling highly efficient implementations with the\nFPGA LUT resources. In contrast, the existing fixed-point quantization is\nsuitable for Uniform-like weight distribution and can be implemented\nefficiently by DSP. Then to fully explore the resources, we propose an\nFPGA-centric mixed scheme quantization (MSQ) with an ensemble of the proposed\nSP2 and the fixed-point schemes. Combining the two schemes can maintain, or\neven increase accuracy due to better matching with weight distributions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 06:25:07 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 00:54:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Chang", "Sung-En", ""], ["Li", "Yanyu", ""], ["Sun", "Mengshu", ""], ["Shi", "Runbin", ""], ["So", "Hayden K. -H.", ""], ["Qian", "Xuehai", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "2012.04559", "submitter": "Ahmet Inci", "authors": "Ahmet Inci, Mehmet Meric Isgenc, Diana Marculescu", "title": "DeepNVM++: Cross-Layer Modeling and Optimization Framework of\n  Non-Volatile Memories for Deep Learning", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies such as spin-transfer torque magnetic\nrandom access memory (STT-MRAM) and spin-orbit torque magnetic random access\nmemory (SOT-MRAM) have significant advantages compared to conventional SRAM due\nto their non-volatility, higher cell density, and scalability features. While\nprevious work has investigated several architectural implications of NVM for\ngeneric applications, in this work we present DeepNVM++, a framework to\ncharacterize, model, and analyze NVM-based caches in GPU architectures for deep\nlearning (DL) applications by combining technology-specific circuit-level\nmodels and the actual memory behavior of various DL workloads. We present both\niso-capacity and iso-area performance and energy analysis for systems whose\nlast-level caches rely on conventional SRAM and emerging STT-MRAM and SOT-MRAM\ntechnologies. In the iso-capacity case, STT-MRAM and SOT-MRAM provide up to\n3.8x and 4.7x energy-delay product (EDP) reduction and 2.4x and 2.8x area\nreduction compared to conventional SRAM, respectively. Under iso-area\nassumptions, STT-MRAM and SOT-MRAM provide up to 2x and 2.3x EDP reduction and\naccommodate 2.3x and 3.3x cache capacity when compared to SRAM, respectively.\nWe also perform a scalability analysis and show that STT-MRAM and SOT-MRAM\nachieve orders of magnitude EDP reduction when compared to SRAM for large cache\ncapacities. Our comprehensive cross-layer framework is demonstrated on\nSTT-/SOT-MRAM technologies and can be used for the characterization, modeling,\nand analysis of any NVM technology for last-level caches in GPUs for DL\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:53:25 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Inci", "Ahmet", ""], ["Isgenc", "Mehmet Meric", ""], ["Marculescu", "Diana", ""]]}, {"id": "2012.05079", "submitter": "Chang Hyun Park", "authors": "Chang Hyun Park (1), Ilias Vougioukas (2), Andreas Sandberg (2), David\n  Black-Schaffer (1) ((1) Uppsala University, (2) Arm Research)", "title": "Page Tables: Keeping them Flat and Hot (Cached)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As memory capacity has outstripped TLB coverage, applications that use large\ndata sets suffer from frequent page table walks. The underlying problem is that\ntraditional tree-based page tables are designed to save memory capacity through\nthe use of fine-grained indirection, which is a poor match for today's systems\nwhich have significant memory capacity but high latencies for indirections.\n  In this work we reduce the penalty of page table walks by both reducing the\nnumber of indirections required for page walks and by reducing the number of\nmain memory accesses required for the indirections. We achieve this by first\nflattening the page table by allocating page table nodes in large pages. This\nresults in fewer levels in the page table, which reduces the number of\nindirections needed to walk it. While the use of large pages in the page table\ndoes waste memory capacity for unused entries, this cost is small compared to\nusing large pages for data as the page table itself is far smaller. Second we\nprioritize caching page table entries during phases of high TLB misses. This\nslightly increases data misses, but does so during phases of low-locality, and\nthe resulting decrease in page walk latency outweighs this loss.\n  By flattening the page table we are able to reduce the number of indirections\nneeded for a page walk from 4 to 2 in a non-virtualized system and from 24 to 8\nunder virtualization. However, the actual effect is much smaller as the Page\nWalker/Paging Structure Cache already captures most locality in the first two\nlevels of the page table. Overall we are able to reduce the number of main\nmemory accesses per page walk from 1.6 to 1.0 for non-virtualized systems and\nfrom 4.6 to 3.0 for virtualized systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:30:56 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Park", "Chang Hyun", "", "Uppsala University"], ["Vougioukas", "Ilias", "", "Arm Research"], ["Sandberg", "Andreas", "", "Arm Research"], ["Black-Schaffer", "David", "", "Uppsala University"]]}, {"id": "2012.05136", "submitter": "Iv\\'an P\\'erez Mr", "authors": "Iv\\'an P\\'erez (1), Enrique Vallejo (1), Ram\\'on Beivide (1) ((1)\n  University of Cantabria)", "title": "Efficient Bypass in Mesh and Torus NoCs", "comments": "14 pages, 16 figures, LaTeX; this review is an update of the preprint\n  to the accepted manuscript of the paper; the final version of this work has\n  been published in the Journal of SystemArchitecture, DOI:\n  https://doi.org/10.1016/j.sysarc.2020.101832", "journal-ref": "Journal of Systems Architecture Volume 108, September 2020, 101832", "doi": "10.1016/j.sysarc.2020.101832", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Minimizing latency and power are key goals in the design of NoC routers.\nDifferent proposals combine lookahead routing and router bypass to skip the\narbitration and buffering, reducing router delay. However, the conditions to\nuse them requires completely empty buffers in the intermediate routers. This\nrestricts the amount of flits that use the bypass pipeline especially at medium\nand high loads, increasing latency and power.\n  This paper presents NEBB, Non-Empty Buffer Bypass, a mechanism that allows to\nbypass flits even if the buffers to bypass are not empty. The mechanism applies\nto wormhole and virtual-cut-through, each of them with different advantages.\nNEBB-Hybrid is proposed to employ the best flow control in each situation. The\nmechanism is extended to torus topologies, using FBFC and shared buffers.\n  The proposals have been evaluated using Booksim, showing up to 75% reduction\nof the buffered flits for single-flit packets, which translates into latency\nand dynamic power reductions of up to 30% and 23% respectively. For bimodal\ntraffic, these improvements are 20 and 21% respectively. Additionally, the\nbypass utilization is largely independent of the number of VCs when using\nshared buffers and very competitive with few private ones, allowing to simplify\nthe allocation mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:16:22 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 08:35:21 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["P\u00e9rez", "Iv\u00e1n", ""], ["Vallejo", "Enrique", ""], ["Beivide", "Ram\u00f3n", ""]]}, {"id": "2012.05181", "submitter": "Qinzhe Wu", "authors": "Qinzhe Wu, Jonathan Beard, Ashen Ekanayake, Andreas Gerstlauer, Lizy\n  K. John", "title": "Virtual-Link: A Scalable Multi-Producer, Multi-Consumer Message Queue\n  Architecture for Cross-Core Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-core communication is increasingly a bottleneck as the number of\nprocessing elements increase per system-on-chip. Typical hardware solutions to\ncross-core communication are often inflexible; while software solutions are\nflexible, they have performance scaling limitations. A key problem, as we will\nshow, is that of shared state in software-based message queue mechanisms. This\npaper proposes Virtual-Link (VL), a novel light-weight communication mechanism\nwith hardware support to facilitate M:N lock-free data movement. VL reduces the\namount of coherent shared state, which is a bottleneck for many approaches, to\nzero. VL provides further latency benefit by keeping data on the fast path\n(i.e., within the on-chip interconnect). VL enables directed cache-injection\n(stashing) between PEs on the coherence bus, reducing the latency for\ncore-to-core communication. VL is particularly effective for fine-grain tasks\non streaming data. Evaluation on a full system simulator with 7 benchmarks\nshows that VL achieves a 2.09x speedup over state-of-the-art software-based\ncommunication mechanisms, while reducing memory traffic by 61%.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:17:50 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 19:51:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wu", "Qinzhe", ""], ["Beard", "Jonathan", ""], ["Ekanayake", "Ashen", ""], ["Gerstlauer", "Andreas", ""], ["John", "Lizy K.", ""]]}, {"id": "2012.05419", "submitter": "Harideep Nair", "authors": "Harideep Nair, Prabhu Vellaisamy, Santha Bhasuthkar, and John Paul\n  Shen", "title": "A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based\n  Neuromorphic Processors", "comments": "This work is dated and will be superseded by a forthcoming work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of highly-optimized custom macro extensions is developed for a 7nm CMOS\ncell library for implementing Temporal Neural Networks (TNNs) that can mimic\nbrain-like sensory processing with extreme energy efficiency. A TNN prototype\n(13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area\nand consumes only 1.69mW.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 02:31:57 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 22:01:18 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Nair", "Harideep", ""], ["Vellaisamy", "Prabhu", ""], ["Bhasuthkar", "Santha", ""], ["Shen", "John Paul", ""]]}, {"id": "2012.05550", "submitter": "Anna Hermann", "authors": "Ulrich Brenner, Anna Hermann, Jannik Silvanus", "title": "Constructing Depth-Optimum Circuits for Adders and AND-OR Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the fundamental problem of constructing depth-optimum circuits for\nbinary addition. More precisely, as in literature, we consider the following\nproblem: Given auxiliary inputs $t_0, \\dotsc, t_{m-1}$, so-called generate and\npropagate signals, construct a depth-optimum circuit over the basis {AND2, OR2}\ncomputing all $n$ carry bits of an $n$-bit adder, where $m=2n-1$. In fact,\ncarry bits are AND-OR paths, i.e., Boolean functions of the form $t_0 \\lor (\nt_1 \\land (t_2 \\lor ( \\dots t_{m-1}) \\dots ))$. Classical approaches construct\nso-called prefix circuits which do not achieve a competitive depth. For\ninstance, the popular construction by Kogge and Stone is only a\n$2$-approximation. A lower bound on the depth of any prefix circuit is $1.44\n\\log_2 m$ + const, while recent non-prefix circuits have a depth of $\\log_2 m$\n+ $\\log_2 \\log_2 m$ + const. However, it is unknown whether any of these\npolynomial-time approaches achieves the optimum depth for all $m$.\n  We present a new exponential-time algorithm solving the problem optimally.\nThe previously best exact algorithm with a running time of $\\mathcal O(2.45^m)$\nis viable only for $m \\leq 29$. Our algorithm is significantly faster: We\nachieve a running time of $\\mathcal O(2.02^m)$ and apply sophisticated pruning\nstrategies to improve practical running times dramatically. This allows us to\ncompute optimum circuits for all $m \\leq 64$. Combining these computational\nresults with new theoretical insights, we derive the optimum depths of\n$2^k$-bit adder circuits for all $k \\leq 13$, previously known only for $k \\leq\n4$.\n  In fact, we solve a more general problem occurring in VLSI design: $delay$\noptimization of a $generalization$ of AND-OR paths where AND and OR do not\nnecessarily alternate. Our algorithm arises from our new structure theorem\nwhich characterizes delay-optimum generalized AND-OR path circuits.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:42:18 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Brenner", "Ulrich", ""], ["Hermann", "Anna", ""], ["Silvanus", "Jannik", ""]]}, {"id": "2012.06272", "submitter": "Zhe Lin", "authors": "Zhe Lin, Sharad Sinha, Wei Zhang", "title": "Hard-ODT: Hardware-Friendly Online Decision Tree Learning Algorithm and\n  System", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.01431", "journal-ref": null, "doi": "10.1109/TCAD.2020.3043328", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are machine learning models commonly used in various\napplication scenarios. In the era of big data, traditional decision tree\ninduction algorithms are not suitable for learning large-scale datasets due to\ntheir stringent data storage requirement. Online decision tree learning\nalgorithms have been devised to tackle this problem by concurrently training\nwith incoming samples and providing inference results. However, even the most\nup-to-date online tree learning algorithms still suffer from either high memory\nusage or high computational intensity with dependency and long latency, making\nthem challenging to implement in hardware. To overcome these difficulties, we\nintroduce a new quantile-based algorithm to improve the induction of the\nHoeffding tree, one of the state-of-the-art online learning models. The\nproposed algorithm is light-weight in terms of both memory and computational\ndemand, while still maintaining high generalization ability. A series of\noptimization techniques dedicated to the proposed algorithm have been\ninvestigated from the hardware perspective, including coarse-grained and\nfine-grained parallelism, dynamic and memory-based resource sharing, pipelining\nwith data forwarding. Following this, we present Hard-ODT, a high-performance,\nhardware-efficient and scalable online decision tree learning system on a\nfield-programmable gate array (FPGA) with system-level optimization techniques.\nPerformance and resource utilization are modeled for the complete learning\nsystem for early and fast analysis of the trade-off between various design\nmetrics. Finally, we propose a design flow in which the proposed learning\nsystem is applied to FPGA run-time power monitoring as a case study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:06:44 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Lin", "Zhe", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""]]}, {"id": "2012.06373", "submitter": "Julien Launay", "authors": "Julien Launay, Iacopo Poli, Kilian M\\\"uller, Gustave Pariente, Igor\n  Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan", "title": "Hardware Beyond Backpropagation: a Photonic Co-Processor for Direct\n  Feedback Alignment", "comments": "6 pages, 2 figures, 1 table. Oral at the Beyond Backpropagation\n  Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scaling hypothesis motivates the expansion of models past trillions of\nparameters as a path towards better performance. Recent significant\ndevelopments, such as GPT-3, have been driven by this conjecture. However, as\nmodels scale-up, training them efficiently with backpropagation becomes\ndifficult. Because model, pipeline, and data parallelism distribute parameters\nand gradients over compute nodes, communication is challenging to orchestrate:\nthis is a bottleneck to further scaling. In this work, we argue that\nalternative training methods can mitigate these issues, and can inform the\ndesign of extreme-scale training hardware. Indeed, using a synaptically\nasymmetric method with a parallelizable backward pass, such as Direct Feedback\nAlignement, communication needs are drastically reduced. We present a photonic\naccelerator for Direct Feedback Alignment, able to compute random projections\nwith trillions of parameters. We demonstrate our system on benchmark tasks,\nusing both fully-connected and graph convolutional networks. Our hardware is\nthe first architecture-agnostic photonic co-processor for training neural\nnetworks. This is a significant step towards building scalable hardware, able\nto go beyond backpropagation, and opening new avenues for deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:20:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Launay", "Julien", ""], ["Poli", "Iacopo", ""], ["M\u00fcller", "Kilian", ""], ["Pariente", "Gustave", ""], ["Carron", "Igor", ""], ["Daudet", "Laurent", ""], ["Krzakala", "Florent", ""], ["Gigan", "Sylvain", ""]]}, {"id": "2012.06959", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Jieyang Chen, Jesun S Firoz, Jiajia Li, Shuaiwen Leon\n  Song, Kevin Barker, Mark Raugas, Ang Li", "title": "Fast and Scalable Sparse Triangular Solver for Multi-GPU Based HPC\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Designing efficient and scalable sparse linear algebra kernels on modern\nmulti-GPU based HPC systems is a daunting task due to significant irregular\nmemory references and workload imbalance across the GPUs. This is particularly\nthe case for Sparse Triangular Solver (SpTRSV) which introduces additional\ntwo-dimensional computation dependencies among subsequent computation steps.\nDependency information is exchanged and shared among GPUs, thus warrant for\nefficient memory allocation, data partitioning, and workload distribution as\nwell as fine-grained communication and synchronization support. In this work,\nwe demonstrate that directly adopting unified memory can adversely affect the\nperformance of SpTRSV on multi-GPU architectures, despite linking via fast\ninterconnect like NVLinks and NVSwitches. Alternatively, we employ the latest\nNVSHMEM technology based on Partitioned Global Address Space programming model\nto enable efficient fine-grained communication and drastic synchronization\noverhead reduction. Furthermore, to handle workload imbalance, we propose a\nmalleable task-pool execution model which can further enhance the utilization\nof GPUs. By applying these techniques, our experiments on the NVIDIA multi-GPU\nsupernode V100-DGX-1 and DGX-2 systems demonstrate that our design can achieve\non average 3.53x (up to 9.86x) speedup on a DGX-1 system and 3.66x (up to\n9.64x) speedup on a DGX-2 system with 4-GPUs over the Unified-Memory design.\nThe comprehensive sensitivity and scalability studies also show that the\nproposed zero-copy SpTRSV is able to fully utilize the computing and\ncommunication resources of the multi-GPU system.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 04:35:29 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Xie", "Chenhao", ""], ["Chen", "Jieyang", ""], ["Firoz", "Jesun S", ""], ["Li", "Jiajia", ""], ["Song", "Shuaiwen Leon", ""], ["Barker", "Kevin", ""], ["Raugas", "Mark", ""], ["Li", "Ang", ""]]}, {"id": "2012.07242", "submitter": "Andrew Boutros", "authors": "Andrew Boutros, Mathew Hall, Nicolas Papernot, Vaughn Betz", "title": "Neighbors From Hell: Voltage Attacks Against Deep Learning Accelerators\n  on Multi-Tenant FPGAs", "comments": "Published in the 2020 proceedings of the International Conference of\n  Field-Programmable Technology (ICFPT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Field-programmable gate arrays (FPGAs) are becoming widely used accelerators\nfor a myriad of datacenter applications due to their flexibility and energy\nefficiency. Among these applications, FPGAs have shown promising results in\naccelerating low-latency real-time deep learning (DL) inference, which is\nbecoming an indispensable component of many end-user applications. With the\nemerging research direction towards virtualized cloud FPGAs that can be shared\nby multiple users, the security aspect of FPGA-based DL accelerators requires\ncareful consideration. In this work, we evaluate the security of DL\naccelerators against voltage-based integrity attacks in a multitenant FPGA\nscenario. We first demonstrate the feasibility of such attacks on a\nstate-of-the-art Stratix 10 card using different attacker circuits that are\nlogically and physically isolated in a separate attacker role, and cannot be\nflagged as malicious circuits by conventional bitstream checkers. We show that\naggressive clock gating, an effective power-saving technique, can also be a\npotential security threat in modern FPGAs. Then, we carry out the attack on a\nDL accelerator running ImageNet classification in the victim role to evaluate\nthe inherent resilience of DL models against timing faults induced by the\nadversary. We find that even when using the strongest attacker circuit, the\nprediction accuracy of the DL accelerator is not compromised when running at\nits safe operating frequency. Furthermore, we can achieve 1.18-1.31x higher\ninference performance by over-clocking the DL accelerator without affecting its\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:59:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Boutros", "Andrew", ""], ["Hall", "Mathew", ""], ["Papernot", "Nicolas", ""], ["Betz", "Vaughn", ""]]}, {"id": "2012.08071", "submitter": "Matthew Adiletta", "authors": "Matthew Joseph Adiletta and Brian Flanagan", "title": "Optimization Techniques to Improve Inference Performance of a Forward\n  Propagating Neural Network on an FPGA", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes an optimized implementation of a Forward Propagating\nClassification Neural Network which has been previously trained. The\nimplementation described highlights a novel means of using Python scripts to\ngenerate a Verilog hardware implementation. The characteristics of this\nimplementation include optimizations to scale input data, use selected addends\ninstead of multiplication functions, hardware friendly activation functions and\nsimplified output selection. Inference performance comparison of a 28x28 pixel\n'hand-written' recognition NN between a software implementation on an Intel i7\nvs a Xilinx FPGA will be detailed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 03:38:51 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Adiletta", "Matthew Joseph", ""], ["Flanagan", "Brian", ""]]}, {"id": "2012.08320", "submitter": "Enzo Rucci", "authors": "Roberto Millon and Emmanuel Frati and Enzo Rucci", "title": "A Comparative Study between HLS and HDL on SoC for Image Processing\n  Applications", "comments": null, "journal-ref": null, "doi": "10.37537/rev.elektron.4.2.117.2020", "report-no": "Revista elektronVol 4, No 2 (2020) 100-106", "categories": "cs.AR cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The increasing complexity in today's systems and the limited market times\ndemand new development tools for FPGA. Currently, in addition to traditional\nhardware description languages (HDLs), there are high-level synthesis (HLS)\ntools that increase the abstraction level in system development. Despite the\ngreater simplicity of design and testing, HLS has some drawbacks in describing\nharware. This paper presents a comparative study between HLS and HDL for FPGA,\nusing a Sobel filter as a case study in the image processing field. The results\nshow that the HDL implementation is slightly better than the HLS version\nconsidering resource usage and response time. However, the programming effort\nrequired in the HDL solution is significantly larger than in the HLS\ncounterpart.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:28:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Millon", "Roberto", ""], ["Frati", "Emmanuel", ""], ["Rucci", "Enzo", ""]]}, {"id": "2012.09646", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Arnaud Delamare, Marco Arnaboldi, Marco D.\n  Santambrogio", "title": "DAG-based Scheduling with Resource Sharing for Multi-task Applications\n  in a Polyglot GPU Runtime", "comments": "10 pages, to be published in 2021 IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GPUs are readily available in cloud computing and personal devices, but their\nuse for data processing acceleration has been slowed down by their limited\nintegration with common programming languages such as Python or Java. Moreover,\nusing GPUs to their full capabilities requires expert knowledge of asynchronous\nprogramming. In this work, we present a novel GPU run time scheduler for\nmulti-task GPU computations that transparently provides asynchronous execution,\nspace-sharing, and transfer-computation overlap without requiring in advance\nany information about the program dependency structure. We leverage the GrCUDA\npolyglot API to integrate our scheduler with multiple high-level languages and\nprovide a platform for fast prototyping and easy GPU acceleration. We validate\nour work on 6 benchmarks created to evaluate task-parallelism and show an\naverage of 44% speedup against synchronous execution, with no execution time\nslowdown compared to hand-optimized host code written using the C++ CUDA Graphs\nAPI.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:54:07 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 10:11:15 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Parravicini", "Alberto", ""], ["Delamare", "Arnaud", ""], ["Arnaboldi", "Marco", ""], ["Santambrogio", "Marco D.", ""]]}, {"id": "2012.09852", "submitter": "Hanrui Wang", "authors": "Hanrui Wang and Zhekai Zhang and Song Han", "title": "SpAtten: Efficient Sparse Attention Architecture with Cascade Token and\n  Head Pruning", "comments": "Published as a conference paper in HPCA 2021; 15 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The attention mechanism is becoming increasingly popular in Natural Language\nProcessing (NLP) applications, showing superior performance than convolutional\nand recurrent architectures. However, general-purpose platforms such as CPUs\nand GPUs are inefficient when performing attention inference due to complicated\ndata movement and low arithmetic intensity. Moreover, existing NN accelerators\nmainly focus on optimizing convolutional or recurrent models, and cannot\nefficiently support attention. In this paper, we present SpAtten, an efficient\nalgorithm-architecture co-design that leverages token sparsity, head sparsity,\nand quantization opportunities to reduce the attention computation and memory\naccess. Inspired by the high redundancy of human languages, we propose the\nnovel cascade token pruning to prune away unimportant tokens in the sentence.\nWe also propose cascade head pruning to remove unessential heads. Cascade\npruning is fundamentally different from weight pruning since there is no\ntrainable weight in the attention mechanism, and the pruned tokens and heads\nare selected on the fly. To efficiently support them on hardware, we design a\nnovel top-k engine to rank token and head importance scores with high\nthroughput. Furthermore, we propose progressive quantization that first fetches\nMSBs only and performs the computation; if the confidence is low, it fetches\nLSBs and recomputes the attention outputs, trading computation for memory\nreduction.\n  Extensive experiments on 30 benchmarks show that, on average, SpAtten reduces\nDRAM access by 10.0x with no accuracy loss, and achieves 1.6x, 3.0x, 162x, 347x\nspeedup, and 1,4x, 3.2x, 1193x, 4059x energy savings over A3 accelerator,\nMNNFast accelerator, TITAN Xp GPU, Xeon CPU, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:07 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 03:49:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Hanrui", ""], ["Zhang", "Zhekai", ""], ["Han", "Song", ""]]}, {"id": "2012.09918", "submitter": "M. Hassan Najafi", "authors": "Mohsen Riahi Alam and M. Hassan Najafi and Nima TaheriNejad", "title": "Sorting in Memristive Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorting is needed in many application domains. The data is read from memory\nand sent to a general purpose processor or application specific hardware for\nsorting. The sorted data is then written back to the memory. Reading/writing\ndata from/to memory and transferring data between memory and processing unit\nincur a large latency and energy overhead. In this work, we develop, to the\nbest of our knowledge, the first architectures for in-memory sorting of data.\nWe propose two architectures. The first architecture is applicable to the\nconventional format of representing data, weighted binary radix. The second\narchitecture is proposed for the developing unary processing systems where data\nis encoded as uniform unary bitstreams. The two architectures have different\nadvantages and disadvantages, making one or the other more suitable for a\nspecific application. However, the common property of both is a significant\nreduction in the processing time compared to prior sorting designs. Our\nevaluations show on average 37x and 138x energy reduction for binary and unary\ndesigns, respectively, as compared to conventional CMOS off-memory sorting\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:25:02 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 00:51:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Alam", "Mohsen Riahi", ""], ["Najafi", "M. Hassan", ""], ["TaheriNejad", "Nima", ""]]}, {"id": "2012.10597", "submitter": "Vidya A. Chhabria", "authors": "Vidya A. Chhabria, Yanqing Zhang, Haoxing Ren, Ben Keller, Brucek\n  Khailany, and Sachin S. Sapatnekar", "title": "MAVIREC: ML-Aided Vectored IR-DropEstimation and Classification", "comments": "6 pages paper. This has been reviewed at Design Automation and Test\n  Conference 2021 and has been accepted as a four page paper. This is a longer\n  version of that", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectored IR drop analysis is a critical step in chip signoff that checks the\npower integrity of an on-chip power delivery network. Due to the prohibitive\nruntimes of dynamic IR drop analysis, the large number of test patterns must be\nwhittled down to a small subset of worst-case IR vectors. Unlike the\ntraditional slow heuristic method that select a few vectors with incomplete\ncoverage, MAVIREC uses machine learning techniques -- 3D convolutions and\nregression-like layers -- for accurately recommending a larger subset of test\npatterns that exercise worst-case scenarios. In under 30 minutes, MAVIREC\nprofiles 100K-cycle vectors and provides better coverage than a\nstate-of-the-art industrial flow. Further, MAVIREC's IR drop predictor shows\n10x speedup with under 4mV RMSE relative to an industrial flow.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 04:55:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chhabria", "Vidya A.", ""], ["Zhang", "Yanqing", ""], ["Ren", "Haoxing", ""], ["Keller", "Ben", ""], ["Khailany", "Brucek", ""], ["Sapatnekar", "Sachin S.", ""]]}, {"id": "2012.10848", "submitter": "Gengyu Rao", "authors": "Gengyu Rao, Jingji Chen, Jason Yik, and Xuehai Qian", "title": "IntersectX: An Efficient Accelerator for Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern mining applications try to find all embeddings that match\nspecific patterns. Compared to the traditional graph computation, graph mining\napplications are computation-intensive. The state-of-the-art method, pattern\nenumeration, constructs the embeddings that match the pattern. The key\noperation -- intersection -- of two edge lists, poses challenges to\nconventional architectures and requires substantial execution time. In this\npaper, we propose IntersectX, a vertically designed accelerator for pattern\nenumeration with stream instruction set extension and architectural supports\nbased on conventional processor. The stream based ISA can be considered as a\nnatural extension to the traditional instructions that operate on scalar\nvalues. We develop the IntersectX architecture composed of specialized\nmechanisms that efficiently implement the stream ISA extensions, including: (1)\nStream Mapping Table (SMT) that records the mapping between stream ID and\nstream register; (2) the read-only Stream Cache (S-Cache) that enables\nefficient stream data movements; (3) tracking the dependency between streams\nwith a property of intersection; (4) Stream Value Processing Unit (SVPU) that\nimplements sparse value computations; and (5) the nested intersection\ntranslator that generates micro-op sequences for implementing nested\nintersections. We implement IntersectX ISA and architecture on zSim, and test\nit with seven popular graph mining applications\n(triangle/three-chain/tailed-traingle counting, 3-motif mining, 4/5-clique\ncounting, and FSM) on ten real graphs. We develop our own implementation of\nAutoMine (InHouseAutomine). The results show that IntersectX significantly\noutperforms InHouseAutomine on CPU, on average 10.7 times and up to 83.9 times\n; and GRAMER, a state-of-the-art graph pattern mining accelerator, based on\nexhaustive check, on average 40.1 times and up to 181.8 times.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 05:40:33 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 07:33:42 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 05:13:02 GMT"}, {"version": "v4", "created": "Mon, 19 Apr 2021 05:17:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rao", "Gengyu", ""], ["Chen", "Jingji", ""], ["Yik", "Jason", ""], ["Qian", "Xuehai", ""]]}, {"id": "2012.11233", "submitter": "Beatrice Bussolino", "authors": "Maurizio Capra, Beatrice Bussolino, Alberto Marchisio, Guido Masera,\n  Maurizio Martina, Muhammad Shafique", "title": "Hardware and Software Optimizations for Accelerating Deep Neural\n  Networks: Survey of Current Trends, Challenges, and the Road Ahead", "comments": "Accepted for publication in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3039858", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, Machine Learning (ML) is becoming ubiquitous in everyday life.\nDeep Learning (DL) is already present in many applications ranging from\ncomputer vision for medicine to autonomous driving of modern cars as well as\nother sectors in security, healthcare, and finance. However, to achieve\nimpressive performance, these algorithms employ very deep networks, requiring a\nsignificant computational power, both during the training and inference time. A\nsingle inference of a DL model may require billions of multiply-and-accumulated\noperations, making the DL extremely compute- and energy-hungry. In a scenario\nwhere several sophisticated algorithms need to be executed with limited energy\nand low latency, the need for cost-effective hardware platforms capable of\nimplementing energy-efficient DL execution arises. This paper first introduces\nthe key properties of two brain-inspired models like Deep Neural Network (DNN),\nand Spiking Neural Network (SNN), and then analyzes techniques to produce\nefficient and high-performance designs. This work summarizes and compares the\nworks for four leading platforms for the execution of algorithms such as CPU,\nGPU, FPGA and ASIC describing the main solutions of the state-of-the-art,\ngiving much prominence to the last two solutions since they offer greater\ndesign flexibility and bear the potential of high energy-efficiency, especially\nfor the inference process. In addition to hardware solutions, this paper\ndiscusses some of the important security issues that these DNN and SNN models\nmay have during their execution, and offers a comprehensive section on\nbenchmarking, explaining how to assess the quality of different networks and\nhardware systems designed for them.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:27:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Capra", "Maurizio", ""], ["Bussolino", "Beatrice", ""], ["Marchisio", "Alberto", ""], ["Masera", "Guido", ""], ["Martina", "Maurizio", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2012.11331", "submitter": "Simon Wiedemann", "authors": "Simon Wiedemann, Suhas Shivapakash, Pablo Wiedemann, Daniel Becking,\n  Wojciech Samek, Friedel Gerfers, Thomas Wiegand", "title": "FantastIC4: A Hardware-Software Co-Design Approach for Efficiently\n  Running 4bit-Compact Multilayer Perceptrons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing demand for deploying deep learning models to the \"edge\", it\nis paramount to develop techniques that allow to execute state-of-the-art\nmodels within very tight and limited resource constraints. In this work we\npropose a software-hardware optimization paradigm for obtaining a highly\nefficient execution engine of deep neural networks (DNNs) that are based on\nfully-connected layers. Our approach is centred around compression as a means\nfor reducing the area as well as power requirements of, concretely, multilayer\nperceptrons (MLPs) with high predictive performances. Firstly, we design a\nnovel hardware architecture named FantastIC4, which (1) supports the efficient\non-chip execution of multiple compact representations of fully-connected layers\nand (2) minimizes the required number of multipliers for inference down to only\n4 (thus the name). Moreover, in order to make the models amenable for efficient\nexecution on FantastIC4, we introduce a novel entropy-constrained training\nmethod that renders them to be robust to 4bit quantization and highly\ncompressible in size simultaneously. The experimental results show that we can\nachieve throughputs of 2.45 TOPS with a total power consumption of 3.6W on a\nVirtual Ultrascale FPGA XCVU440 device implementation, and achieve a total\npower efficiency of 20.17 TOPS/W on a 22nm process ASIC version. When compared\nto the other state-of-the-art accelerators designed for the Google Speech\nCommand (GSC) dataset, FantastIC4 is better by 51$\\times$ in terms of\nthroughput and 145$\\times$ in terms of area efficiency (GOPS/W).\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 19:10:04 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wiedemann", "Simon", ""], ["Shivapakash", "Suhas", ""], ["Wiedemann", "Pablo", ""], ["Becking", "Daniel", ""], ["Samek", "Wojciech", ""], ["Gerfers", "Friedel", ""], ["Wiegand", "Thomas", ""]]}, {"id": "2012.11334", "submitter": "Viacheslav Dubeyko", "authors": "Viacheslav Dubeyko", "title": "Cognitive Computing in Data-centric Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge is the most precious asset of humankind. People extract the\nexperience from the data that provide for us the reality through the feelings.\nGenerally speaking, it is possible to see the analogy of knowledge elaboration\nbetween humankind's way and the artificial system's way. Digital data are the\n\"feelings\" of an artificial system, and it needs to invent a method of\nextraction of knowledge from the Universe of data.\n  The cognitive computing paradigm implies that a system should be able to\nextract the knowledge from raw data without any human-made algorithm. The first\nstep of the paradigm is analysis of raw data streams through the discovery of\nrepeatable patterns of data. The knowledge of relationships among the patterns\nprovides a way to see the structures and to generalize the concepts with the\ngoal to synthesize new statements. The cognitive computing paradigm is capable\nof mimicking the human's ability to generalize the notions. It is possible to\nsay that the generalization step provides the basis for discovering the\nabstract notions, revealing the abstract relations of patterns and general\nrules of structure synthesis.\n  If anyone continues the process of structure generalization, then it is\npossible to build the multi-level hierarchy of abstract notions. Moreover,\ndiscovering the generalized classes of notions is the first step towards a\nparadigm of artificial analytical thinking. The most critical possible\nresponsibility of cognitive computing could be the classification of data and\nrecognition of input data stream's states. The synthesis of new statements\ncreates the foundation for the foreseeing the possible data states and\nelaboration of knowledge about new data classes by employing synthesis and\nchecking the hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:39:53 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dubeyko", "Viacheslav", ""]]}, {"id": "2012.11473", "submitter": "Nicolas Derumigny", "authors": "Nicolas Derumigny, Fabian Gruber, Th\\'eophile Bastian, Christophe\n  Guillon, Louis-Noel Pouchet, Fabrice Rastello", "title": "From micro-OPs to abstract resources: constructing a simpler CPU\n  performance model through microbenchmarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes PALMED, a tool that automatically builds a resource\nmapping, a performance model for pipelined, super-scalar, out-of-order CPU\narchitectures. Resource mappings describe the execution of a program by\nassigning instructions in the program to abstract resources. They can be used\nto predict the throughput of basic blocks or as a machine model for the backend\nof an optimizing compiler. PALMED does not require hardware performance\ncounters, and relies solely on runtime measurements to construct resource\nmappings. This allows it to model not only execution port usage, but also other\nlimiting resources, such as the frontend or the reorder buffer. Also, thanks to\na dual representation of resource mappings, our algorithm for constructing\nmappings scales to large instruction sets, like that of x86. We evaluate the\nalgorithmic contribution of the paper in two ways. First by showing that our\napproach can reverse engineering an accurate resource mapping from an\nidealistic performance model produced by an existing port-mapping. We also\nevaluate the pertinence of our dual representation, as opposed to the standard\nport-mapping, for throughput modeling by extracting a representative set of\nbasic-blocks from the compiled binaries of the Spec CPU 2017 benchmarks and\ncomparing the throughput predicted by existing machine models to that produced\nby PALMED.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:42:50 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:03:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Derumigny", "Nicolas", ""], ["Gruber", "Fabian", ""], ["Bastian", "Th\u00e9ophile", ""], ["Guillon", "Christophe", ""], ["Pouchet", "Louis-Noel", ""], ["Rastello", "Fabrice", ""]]}, {"id": "2012.11890", "submitter": "Geraldo Francisco De Oliveira Junior", "authors": "Nastaran Hajinazar, Geraldo F. Oliveira, Sven Gregorio, Jo\\~ao Dinis\n  Ferreira, Nika Mansouri Ghiasi, Minesh Patel, Mohammed Alser, Saugata Ghose,\n  Juan G\\'omez-Luna, Onur Mutlu", "title": "SIMDRAM: A Framework for Bit-Serial SIMD Processing Using DRAM", "comments": "Extended abstract of the full paper to appear in ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Processing-using-DRAM has been proposed for a limited set of basic operations\n(i.e., logic operations, addition). However, in order to enable the full\nadoption of processing-using-DRAM, it is necessary to provide support for more\ncomplex operations. In this paper, we propose SIMDRAM, a flexible\ngeneral-purpose processing-using-DRAM framework that enables massively-parallel\ncomputation of a wide range of operations by using each DRAM column as an\nindependent SIMD lane to perform bit-serial operations. SIMDRAM consists of\nthree key steps to enable a desired operation in DRAM: (1) building an\nefficient majority-based representation of the desired operation, (2) mapping\nthe operation input and output operands to DRAM rows and to the required DRAM\ncommands that produce the desired operation, and (3) executing the operation.\nThese three steps ensure efficient computation of any arbitrary and complex\noperation in DRAM. The first two steps give users the flexibility to\nefficiently implement and compute any desired operation in DRAM. The third step\ncontrols the execution flow of the in-DRAM computation, transparently from the\nuser. We comprehensively evaluate SIMDRAM's reliability, area overhead,\noperation throughput, and energy efficiency using a wide range of operations\nand seven diverse real-world kernels to demonstrate its generality. Our results\nshow that SIMDRAM provides up to 5.1x higher operation throughput and 2.5x\nhigher energy efficiency than a state-of-the-art in-DRAM computing mechanism,\nand up to 2.5x speedup for real-world kernels while incurring less than 1% DRAM\nchip area overhead. Compared to a CPU and a high-end GPU, SIMDRAM is 257x and\n31x more energy-efficient, while providing 93x and 6x higher operation\nthroughput, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 09:14:28 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Hajinazar", "Nastaran", ""], ["Oliveira", "Geraldo F.", ""], ["Gregorio", "Sven", ""], ["Ferreira", "Jo\u00e3o Dinis", ""], ["Ghiasi", "Nika Mansouri", ""], ["Patel", "Minesh", ""], ["Alser", "Mohammed", ""], ["Ghose", "Saugata", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Mutlu", "Onur", ""]]}, {"id": "2012.12178", "submitter": "Jisung Park", "authors": "Jisung Park, Myungsuk Kim, Myoungjun Chun, Lois Orosa, Jihong Kim, and\n  Onur Mutlu", "title": "Reducing Solid-State Drive Read Latency by Optimizing Read-Retry\n  (Extended Abstract)", "comments": "Extended abstract of the full paper to appear in ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D NAND flash memory with advanced multi-level cell techniques provides high\nstorage density, but suffers from significant performance degradation due to a\nlarge number of read-retry operations. Although the read-retry mechanism is\nessential to ensuring the reliability of modern NAND flash memory, it can\nsignificantly increase the read latency of an SSD by introducing multiple retry\nsteps that read the target page again with adjusted read-reference voltage\nvalues. Through a detailed analysis of the read mechanism and rigorous\ncharacterization of 160 real 3D NAND flash memory chips, we find new\nopportunities to reduce the read-retry latency by exploiting two advanced\nfeatures widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ\ncommand and 2) strong ECC engine. First, we can reduce the read-retry latency\nusing the advanced CACHE READ command that allows a NAND flash chip to perform\nconsecutive reads in a pipelined manner. Second, there exists a large\nECC-capability margin in the final retry step that can be used for reducing the\nchip-level read latency. Based on our new findings, we develop two new\ntechniques that effectively reduce the read-retry latency: 1) Pipelined\nRead-Retry (PR$^2$) and 2) Adaptive Read-Retry (AR$^2$). PR$^2$ reduces the\nlatency of a read-retry operation by pipelining consecutive retry steps using\nthe CACHE READ command. AR$^2$ shortens the latency of each retry step by\ndynamically reducing the chip-level read latency depending on the current\noperating conditions that determine the ECC-capability margin. Our evaluation\nusing twelve real-world workloads shows that our proposal improves SSD response\ntime by up to 31.5% (17% on average) over a state-of-the-art baseline with only\nsmall changes to the SSD controller.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:16:17 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 10:49:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Park", "Jisung", ""], ["Kim", "Myungsuk", ""], ["Chun", "Myoungjun", ""], ["Orosa", "Lois", ""], ["Kim", "Jihong", ""], ["Mutlu", "Onur", ""]]}, {"id": "2012.12381", "submitter": "Onur Mutlu", "authors": "Onur Mutlu", "title": "Intelligent Architectures for Intelligent Computing Systems", "comments": "To appear as an invited talk and accompanying summary paper at DATE\n  2021 conference. arXiv admin note: substantial text overlap with\n  arXiv:2008.06112", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing is bottlenecked by data. Large amounts of application data\noverwhelm storage capability, communication capability, and computation\ncapability of the modern machines we design today. We argue that an intelligent\narchitecture should be designed to handle data well. We show that handling data\nwell requires designing architectures based on three key principles: 1)\ndata-centric, 2) data-driven, 3) dataaware. We give several examples for how to\nexploit each of these principles to design a much more efficient and high\nperformance computing system. We discuss how to enable adoption of such\nfundamentally more intelligent architectures, which we believe are key to\nefficiency, performance, and sustainability. We conclude with some guiding\nprinciples for future computing architecture and system designs. This\naccompanying short paper provides a summary of the associated invited talk at\nDATE 2021 and points the reader to further work that may be beneficial to\nexamine.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:01:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Mutlu", "Onur", ""]]}, {"id": "2012.12563", "submitter": "Jan Moritz Joseph", "authors": "Jan Moritz Joseph, Ananda Samajdar, Lingjun Zhu, Rainer Leupers,\n  Sung-Kyu Lim, Thilo Pionteck, Tushar Krishna", "title": "Architecture, Dataflow and Physical Design Implications of 3D-ICs for\n  DNN-Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The everlasting demand for higher computing power for deep neural networks\n(DNNs) drives the development of parallel computing architectures. 3D\nintegration, in which chips are integrated and connected vertically, can\nfurther increase performance because it introduces another level of spatial\nparallelism. Therefore, we analyze dataflows, performance, area, power and\ntemperature of such 3D-DNN-accelerators. Monolithic and TSV-based stacked\n3D-ICs are compared against 2D-ICs. We identify workload properties and\narchitectural parameters for efficient 3D-ICs and achieve up to 9.14x speedup\nof 3D vs. 2D. We discuss area-performance trade-offs. We demonstrate\napplicability as the 3D-IC draws similar power as 2D-ICs and is not thermal\nlimited.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:49:51 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 08:26:04 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 10:32:17 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Joseph", "Jan Moritz", ""], ["Samajdar", "Ananda", ""], ["Zhu", "Lingjun", ""], ["Leupers", "Rainer", ""], ["Lim", "Sung-Kyu", ""], ["Pionteck", "Thilo", ""], ["Krishna", "Tushar", ""]]}, {"id": "2012.12700", "submitter": "Jingzhe Guo", "authors": "Jingzhe Guo, Mingsheng Ying", "title": "Software Pipelining for Quantum Loop Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for performing software pipelining on quantum for-loop\nprograms, exploiting parallelism in and across iterations. We redefine concepts\nthat are useful in program optimization, including array aliasing, instruction\ndependency and resource conflict, this time in optimization of quantum\nprograms. Using the redefined concepts, we present a software pipelining\nalgorithm exploiting instruction-level parallelism in quantum loop programs.\nThe optimization method is then evaluated on some test cases, including popular\napplications like QAOA, and compared with several baseline results. The\nevaluation results show that our approach outperforms loop optimizers\nexploiting only in-loop optimization chances by reducing total depth of the\nloop program to close to the optimal program depth obtained by full loop\nunrolling, while generating much smaller code in size. This is the first step\ntowards optimization of a quantum program with such loop control flow as far as\nwe know.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:27:05 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guo", "Jingzhe", ""], ["Ying", "Mingsheng", ""]]}, {"id": "2012.13600", "submitter": "Chang Gao", "authors": "Chang Gao, Antonio Rios-Navarro, Xi Chen, Shih-Chii Liu, Tobi Delbruck", "title": "EdgeDRNN: Recurrent Neural Network Accelerator for Edge Inference", "comments": null, "journal-ref": "in IEEE Journal on Emerging and Selected Topics in Circuits and\n  Systems, vol. 10, no. 4, pp. 419-432, Dec. 2020", "doi": "10.1109/JETCAS.2020.3040300", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-latency, low-power portable recurrent neural network (RNN) accelerators\noffer powerful inference capabilities for real-time applications such as IoT,\nrobotics, and human-machine interaction. We propose a lightweight Gated\nRecurrent Unit (GRU)-based RNN accelerator called EdgeDRNN that is optimized\nfor low-latency edge RNN inference with batch size of 1. EdgeDRNN adopts the\nspiking neural network inspired delta network algorithm to exploit temporal\nsparsity in RNNs. Weights are stored in inexpensive DRAM which enables EdgeDRNN\nto compute large multi-layer RNNs on the most inexpensive FPGA. The sparse\nupdates reduce DRAM weight memory access by a factor of up to 10x and the delta\ncan be varied dynamically to trade-off between latency and accuracy. EdgeDRNN\nupdates a 5 million parameter 2-layer GRU-RNN in about 0.5ms. It achieves\nlatency comparable with a 92W Nvidia 1080 GPU. It outperforms NVIDIA Jetson\nNano, Jetson TX2 and Intel Neural Compute Stick 2 in latency by 5X. For a batch\nsize of 1, EdgeDRNN achieves a mean effective throughput of 20.2GOp/s and a\nwall plug power efficiency that is over 4X higher than the commercial edge AI\nplatforms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 16:26:05 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gao", "Chang", ""], ["Rios-Navarro", "Antonio", ""], ["Chen", "Xi", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""]]}, {"id": "2012.13645", "submitter": "Sujan Kumar Gonugondla", "authors": "Sujan Kumar Gonugondla, Charbel Sakr, Hassan Dbouk, Naresh R. Shanbhag", "title": "Fundamental Limits on Energy-Delay-Accuracy of In-memory Architectures\n  in Inference Applications", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper obtains fundamental limits on the computational precision of\nin-memory computing architectures (IMCs). An IMC noise model and associated SNR\nmetrics are defined and their interrelationships analyzed to show that the\naccuracy of IMCs is fundamentally limited by the compute SNR\n($\\text{SNR}_{\\text{a}}$) of its analog core, and that activation, weight and\noutput precision needs to be assigned appropriately for the final output SNR\n$\\text{SNR}_{\\text{T}} \\rightarrow \\text{SNR}_{\\text{a}}$. The minimum\nprecision criterion (MPC) is proposed to minimize the ADC precision. Three\nin-memory compute models - charge summing (QS), current summing (IS) and charge\nredistribution (QR) - are shown to underlie most known IMCs. Noise, energy and\ndelay expressions for the compute models are developed and employed to derive\nexpressions for the SNR, ADC precision, energy, and latency of IMCs. The\ncompute SNR expressions are validated via Monte Carlo simulations in a 65 nm\nCMOS process. For a 512 row SRAM array, it is shown that: 1) IMCs have an upper\nbound on their maximum achievable $\\text{SNR}_{\\text{a}}$ due to constraints on\nenergy, area and voltage swing, and this upper bound reduces with technology\nscaling for QS-based architectures; 2) MPC enables $\\text{SNR}_{\\text{T}}\n\\rightarrow \\text{SNR}_{\\text{a}}$ to be realized with minimal ADC precision;\n3) QS-based (QR-based) architectures are preferred for low (high) compute SNR\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 23:33:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gonugondla", "Sujan Kumar", ""], ["Sakr", "Charbel", ""], ["Dbouk", "Hassan", ""], ["Shanbhag", "Naresh R.", ""]]}, {"id": "2012.14318", "submitter": "Wenpeng He", "authors": "Wenpeng He, Dan Feng, Fang Wang, Yue Li, Mengting Lu", "title": "IRO: Integrity and Reliability Enhanced Ring ORAM", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory security and reliability are two of the major design concerns in cloud\ncomputing systems. State-of-the-art memory security-reliability co-designs\n(e.g. Synergy) have achieved a good balance on performance, confidentiality,\nintegrity, and reliability. However, these works merely rely on encryption to\nensure data confidentiality, which has been proven unable to prevent\ninformation leakage from memory access patterns. Ring ORAM is an attractive\nconfidential protection protocol to hide memory access patterns to the\nuntrusted storage system. Unfortunately, it does not compatible with the\nsecurity-reliability co-designs. A forced combination would result in more\nsevere performance loss.\n  In this paper, we propose IRO, an Integrity and Reliability enhanced Ring\nORAM design. To reduce the overhead of integrity verification, we propose a low\noverhead integrity tree RIT and use a Minimum Update Subtree Tree (MUST) to\nreduce metadata update overhead. To improve memory reliability, we present\nSecure Replication to provide channel-level error resilience for the ORAM tree\nand use the mirrored channel technique to guarantee the reliability of the\nMUST. Last, we use the error correction pointer (ECP) to repair permanent\nmemory cell fault to further improve device reliability and lifetime. A compact\nmetadata design is used to reduce the storage and consulting overhead of the\nECP.\n  IRO provides strong security and reliability guarantees, while the resulting\nstorage and performance overhead is very small. Our evaluation shows that IRO\nonly increases 7.54% execution time on average over the Baseline under two\nchannels four AES-GCM units setting. With enough AES-GCM units to perform\nconcurrent MAC computing, IRO can reduce 2.14% execution time of the Baseline.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 16:00:39 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 02:27:56 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["He", "Wenpeng", ""], ["Feng", "Dan", ""], ["Wang", "Fang", ""], ["Li", "Yue", ""], ["Lu", "Mengting", ""]]}, {"id": "2012.14332", "submitter": "Mario Michael Krell", "authors": "Sourabh Kulkarni and Mario Michael Krell and Seth Nabarro and Csaba\n  Andras Moritz", "title": "Hardware-accelerated Simulation-based Inference of Stochastic\n  Epidemiology Models for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Epidemiology models are central in understanding and controlling large scale\npandemics. Several epidemiology models require simulation-based inference such\nas Approximate Bayesian Computation (ABC) to fit their parameters to\nobservations. ABC inference is highly amenable to efficient hardware\nacceleration. In this work, we develop parallel ABC inference of a stochastic\nepidemiology model for COVID-19. The statistical inference framework is\nimplemented and compared on Intel Xeon CPU, NVIDIA Tesla V100 GPU and the\nGraphcore Mk1 IPU, and the results are discussed in the context of their\ncomputational architectures. Results show that GPUs are 4x and IPUs are 30x\nfaster than Xeon CPUs. Extensive performance analysis indicates that the\ndifference between IPU and GPU can be attributed to higher communication\nbandwidth, closeness of memory to compute, and higher compute power in the IPU.\nThe proposed framework scales across 16 IPUs, with scaling overhead not\nexceeding 8% for the experiments performed. We present an example of our\nframework in practice, performing inference on the epidemiology model across\nthree countries, and giving a brief overview of the results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 03:01:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kulkarni", "Sourabh", ""], ["Krell", "Mario Michael", ""], ["Nabarro", "Seth", ""], ["Moritz", "Csaba Andras", ""]]}, {"id": "2012.15731", "submitter": "Jayneel Gandhi", "authors": "Pradeep Fernando, Irina Calciu, Jayneel Gandhi, Aasheesh Kolli and Ada\n  Gavrilovska", "title": "Persistence and Synchronization: Friends or Foes?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emerging non-volatile memory (NVM) technologies promise memory speed\nbyte-addressable persistent storage with a load/store interface. However,\nprogramming applications to directly manipulate NVM data is complex and\nerror-prone. Applications generally employ libraries that hide the low-level\ndetails of the hardware and provide a transactional programming model to\nachieve crash-consistency. Furthermore, applications continue to expect\ncorrectness during concurrent executions, achieved through the use of\nsynchronization. To achieve this, applications seek well-known ACID guarantees.\nHowever, realizing this presents designers of transactional systems with a\nrange of choices in how to combine several low-level techniques, given target\nhardware features and workload characteristics.\n  In this paper, we provide a comprehensive evaluation of the impact of\ncombining existing crash-consistency and synchronization methods for achieving\nperformant and correct NVM transactional systems. We consider different\nhardware characteristics, in terms of support for hardware transactional memory\n(HTM) and the boundaries of the persistence domain (transient or persistent\ncaches). By characterizing persistent transactional systems in terms of their\nproperties, we make it possible to better understand the tradeoffs of different\nimplementations and to arrive at better design choices for providing ACID\nguarantees. We use both real hardware with Intel Optane DC persistent memory\nand simulation to evaluate a persistent version of hardware transactional\nmemory, a persistent version of software transactional memory, and undo/redo\nlogging. Through our empirical study, we show two major factors that impact the\ncost of supporting persistence in transactional systems: the persistence domain\n(transient or persistent caches) and application characteristics, such as\ntransaction size and parallelism.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 03:28:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fernando", "Pradeep", ""], ["Calciu", "Irina", ""], ["Gandhi", "Jayneel", ""], ["Kolli", "Aasheesh", ""], ["Gavrilovska", "Ada", ""]]}]