[{"id": "2104.00192", "submitter": "Zishen Wan", "authors": "Zishen Wan, Yuyang Zhang, Arijit Raychowdhury, Bo Yu, Yanjun Zhang,\n  Shaoshan Liu", "title": "An Energy-Efficient Quad-Camera Visual System for Autonomous Machines on\n  FPGA Platform", "comments": "To appear in IEEE International Conference on Artificial Intelligence\n  Circuits and Systems (AICAS), June 6-9, 2021, Virtual", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our past few years' of commercial deployment experiences, we identify\nlocalization as a critical task in autonomous machine applications, and a great\nacceleration target. In this paper, based on the observation that the visual\nfrontend is a major performance and energy consumption bottleneck, we present\nour design and implementation of an energy-efficient hardware architecture for\nORB (Oriented-Fast and Rotated- BRIEF) based localization system on FPGAs. To\nsupport our multi-sensor autonomous machine localization system, we present\nhardware synchronization, frame-multiplexing, and parallelization techniques,\nwhich are integrated in our design. Compared to Nvidia TX1 and Intel i7, our\nFPGA-based implementation achieves 5.6x and 3.4x speedup, as well as 3.0x and\n34.6x power reduction, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:42:16 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wan", "Zishen", ""], ["Zhang", "Yuyang", ""], ["Raychowdhury", "Arijit", ""], ["Yu", "Bo", ""], ["Zhang", "Yanjun", ""], ["Liu", "Shaoshan", ""]]}, {"id": "2104.00828", "submitter": "Yifan Sun", "authors": "Yifan Sun, Yixuan Zhang, Ali Mosallaei, Michael D. Shah, Cody Dunne,\n  David Kaeli", "title": "Daisen: A Framework for Visualizing Detailed GPU Execution", "comments": "EuroVis Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphics Processing Units (GPUs) have been widely used to accelerate\nartificial intelligence, physics simulation, medical imaging, and information\nvisualization applications. To improve GPU performance, GPU hardware designers\nneed to identify performance issues by inspecting a huge amount of\nsimulator-generated traces. Visualizing the execution traces can reduce the\ncognitive burden of users and facilitate making sense of behaviors of GPU\nhardware components. In this paper, we first formalize the process of GPU\nperformance analysis and characterize the design requirements of visualizing\nexecution traces based on a survey study and interviews with GPU hardware\ndesigners. We contribute data and task abstraction for GPU performance\nanalysis. Based on our task analysis, we propose Daisen, a framework that\nsupports data collection from GPU simulators and provides visualization of the\nsimulator-generated GPU execution traces. Daisen features a data abstraction\nand trace format that can record simulator-generated GPU execution traces.\nDaisen also includes a web-based visualization tool that helps GPU hardware\ndesigners examine GPU execution traces, identify performance bottlenecks, and\nverify performance improvement. Our qualitative evaluation with GPU hardware\ndesigners demonstrates that the design of Daisen reflects the typical workflow\nof GPU hardware designers. Using Daisen, participants were able to effectively\nidentify potential performance bottlenecks and opportunities for performance\nimprovement. The open-sourced implementation of Daisen can be found at\ngitlab.com/akita/vis. Supplemental materials including a demo video, survey\nquestions, evaluation study guide, and post-study evaluation survey are\navailable at osf.io/j5ghq.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 00:52:37 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sun", "Yifan", ""], ["Zhang", "Yixuan", ""], ["Mosallaei", "Ali", ""], ["Shah", "Michael D.", ""], ["Dunne", "Cody", ""], ["Kaeli", "David", ""]]}, {"id": "2104.01446", "submitter": "Christian Pilato", "authors": "Christian Pilato and Francesco Regazzoni", "title": "High-Level Synthesis of Security Properties via Software-Level\n  Abstractions", "comments": "Accepted for presentation at the 1st Workshop on Languages, Tools,\n  and Techniques for Accelerator Design (LATTE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level synthesis (HLS) is a key component for the hardware acceleration\nof applications, especially thanks to the diffusion of reconfigurable devices\nin many domains, from data centers to edge devices. HLS reduces development\ntimes by allowing designers to raise the abstraction level and use automated\nmethods for hardware generation. Since security concerns are becoming more and\nmore relevant for data-intensive applications, we investigate how to abstract\nsecurity properties and use HLS for their integration with the accelerator\nfunctionality. We use the case of dynamic information flow tracking, showing\nhow classic software-level abstractions can be efficiently used to hide\nimplementation details to the designers.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 16:39:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Pilato", "Christian", ""], ["Regazzoni", "Francesco", ""]]}, {"id": "2104.01448", "submitter": "Christian Pilato", "authors": "Stephanie Soldavini and Christian Pilato", "title": "Compiler Infrastructure for Specializing Domain-Specific Memory\n  Templates", "comments": "Accepted for presentation at the 1st Workshop on Languages, Tools,\n  and Techniques for Accelerator Design (LATTE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized hardware accelerators are becoming important for more and more\napplications. Thanks to specialization, they can achieve high performance and\nenergy efficiency but their design is complex and time consuming. This problem\nis exacerbated when large amounts of data must be processed, like in modern big\ndata and machine learning applications. The designer has not only to optimize\nthe accelerator logic but also produce efficient memory architectures. To\nsimplify this process, we propose a multi-level compilation flow that\nspecializes a domain-specific memory template to match data, application, and\ntechnology requirements.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 17:04:11 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Soldavini", "Stephanie", ""], ["Pilato", "Christian", ""]]}, {"id": "2104.01699", "submitter": "Ankit Wagle", "authors": "Ankit Wagle and Sunil Khatri and Sarma Vrudhula", "title": "A Configurable BNN ASIC using a Network of Programmable Threshold Logic\n  Standard Cells", "comments": null, "journal-ref": null, "doi": "10.1109/ICCD50377.2020.00079", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents TULIP, a new architecture for a binary neural network\n(BNN) that uses an optimal schedule for executing the operations of an\narbitrary BNN. It was constructed with the goal of maximizing energy efficiency\nper classification. At the top-level, TULIP consists of a collection of unique\nprocessing elements (TULIP-PEs) that are organized in a SIMD fashion. Each\nTULIP-PE consists of a small network of binary neurons, and a small amount of\nlocal memory per neuron. The unique aspect of the binary neuron is that it is\nimplemented as a mixed-signal circuit that natively performs the inner-product\nand thresholding operation of an artificial binary neuron. Moreover, the binary\nneuron, which is implemented as a single CMOS standard cell, is reconfigurable,\nand with a change in a single parameter, can implement all standard operations\ninvolved in a BNN. We present novel algorithms for mapping arbitrary nodes of a\nBNN onto the TULIP-PEs. TULIP was implemented as an ASIC in TSMC 40nm-LP\ntechnology. To provide a fair comparison, a recently reported BNN that employs\na conventional MAC-based arithmetic processor was also implemented in the same\ntechnology. The results show that TULIP is consistently 3X more\nenergy-efficient than the conventional design, without any penalty in\nperformance, area, or accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 21:28:11 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wagle", "Ankit", ""], ["Khatri", "Sunil", ""], ["Vrudhula", "Sarma", ""]]}, {"id": "2104.01812", "submitter": "Aneesh Balakrishnan", "authors": "Aneesh Balakrishnan, Thomas Lange, Maximilien Glorieux, Dan\n  Alexandrescu and Maksim Jenihhin", "title": "Modeling Gate-Level Abstraction Hierarchy Using Graph Convolutional\n  Neural Networks to Predict Functional De-Rating Factors", "comments": "13 Figures, 7 pages for conference ( 1 extra page (page number 1) is\n  added for arxive about license agreement), Conference: 2019 NASA/ESA\n  Conference on Adaptive Hardware and Systems (AHS)", "journal-ref": null, "doi": "10.1109/AHS.2019.00007", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is proposing a methodology for modeling a gate-level netlist using\na Graph Convolutional Network (GCN). The model predicts the overall functional\nde-rating factors of sequential elements of a given circuit. In the preliminary\nphase of the work, the important goal is making a GCN which able to take a\ngate-level netlist as input information after transforming it into the\nProbabilistic Bayesian Graph in the form of Graph Modeling Language (GML). This\npart enables the GCN to learn the structural information of netlist in graph\ndomains. In the second phase of the work, the modeled GCN trained with the a\nfunctional de-rating factor of a very low number of individual sequential\nelements (flip-flops). The third phase includes understanding of GCN models\naccuracy to model an arbitrary circuit netlist. The designed model was\nvalidated for two circuits. One is the IEEE 754 standard double precision\nfloating point adder and the second one is the 10-Gigabit Ethernet MAC\nIEEE802.3 standard. The predicted results compared to the standard fault\ninjection campaign results of the error called Single EventUpset (SEU). The\nvalidated results are graphically pictured in the form of the histogram and\nsorted probabilities and evaluated with the Confidence Interval (CI) metric\nbetween the predicted and simulated fault injection results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 08:38:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Balakrishnan", "Aneesh", ""], ["Lange", "Thomas", ""], ["Glorieux", "Maximilien", ""], ["Alexandrescu", "Dan", ""], ["Jenihhin", "Maksim", ""]]}, {"id": "2104.01900", "submitter": "Aneesh Balakrishnan", "authors": "Aneesh Balakrishnan, Thomas Lange, Maximilien Glorieux, Dan\n  Alexandrescu and Maksim Jenihhin", "title": "The Validation of Graph Model-Based, Gate Level Low-Dimensional Feature\n  Data for Machine Learning Applications", "comments": "7 pages for conference, Number of Figures: 6, Conference: 2019 IEEE\n  Nordic Circuits and Systems Conference (NORCAS): NORCHIP and International\n  Symposium of System-on-Chip (SoC)", "journal-ref": null, "doi": "10.1109/NORCHIP.2019.8906974", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to traditional fault injection-based methodologies and to\nexplore the applicability of modern machine learning algorithms in the field of\nreliability engineering, this paper proposes a systemic framework that explores\ngate-level netlist circuit abstractions to extract and exploit relevant feature\nrepresentations in a low-dimensional vector space. A scalable feature learning\nmethod on a graphical domain called node2vec algorithm had been utilized for\nefficiently extracting structural features of the netlist, providing a valuable\ndatabase to exercise a selection of machine learning (ML) or deep learning (DL)\nalgorithms aiming at predicting fault propagation metrics. The current work\nproposes to model the gate-level netlist as a Probabilistic Bayesian Graph\n(PGB) in the form of a Graph Modeling Language (GML) format. To accomplish this\ngoal, a Verilog Procedural Interface (VPI) library linked to standard\nsimulation tools has been built to map gate-level netlist into the graph model.\nThe extracted features have been used for predicting the Functional Derating\n(FDR) factors of individual flip-flops of a given circuit through Support\nVector Machine (SVM) and Deep Neural Network (DNN) algorithms. The results of\nthe approach have been compared against data obtained through first-principles\napproaches. The whole experiment was implemented on the features extracted from\nthe 10-Gigabit Ethernet MAC IEEE 802.3 standard circuit.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:27:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Balakrishnan", "Aneesh", ""], ["Lange", "Thomas", ""], ["Glorieux", "Maximilien", ""], ["Alexandrescu", "Dan", ""], ["Jenihhin", "Maksim", ""]]}, {"id": "2104.01908", "submitter": "Aneesh Balakrishnan", "authors": "Aneesh Balakrishnan, Thomas Lange, Maximilien Glorieux, Dan\n  Alexandrescu, Maksim Jenihhin", "title": "Composing Graph Theory and Deep Neural Networks to Evaluate SEU Type\n  Soft Error Effects", "comments": "5 pages for conference, Number of figures: 3, Conference: 2020 9th\n  Mediterranean Conference on Embedded Computing (MECO)", "journal-ref": null, "doi": "10.1109/MECO49872.2020.9134279", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly shrinking technology node and voltage scaling increase the\nsusceptibility of Soft Errors in digital circuits. Soft Errors are\nradiation-induced effects while the radiation particles such as Alpha, Neutrons\nor Heavy Ions, interact with sensitive regions of microelectronic\ndevices/circuits. The particle hit could be a glancing blow or a penetrating\nstrike. A well apprehended and characterized way of analyzing soft error\neffects is the fault-injection campaign, but that typically acknowledged as\ntime and resource-consuming simulation strategy. As an alternative to\ntraditional fault injection-based methodologies and to explore the\napplicability of modern graph based neural network algorithms in the field of\nreliability modeling, this paper proposes a systematic framework that explores\ngate-level abstractions to extract and exploit relevant feature representations\nat low-dimensional vector space. The framework allows the extensive prediction\nanalysis of SEU type soft error effects in a given circuit. A scalable and\ninductive type representation learning algorithm on graphs called GraphSAGE has\nbeen utilized for efficiently extracting structural features of the gate-level\nnetlist, providing a valuable database to exercise a downstream machine\nlearning or deep learning algorithm aiming at predicting fault propagation\nmetrics. Functional Failure Rate (FFR): the predicted fault propagating metric\nof SEU type fault within the gate-level circuit abstraction of the 10-Gigabit\nEthernet MAC (IEEE 802.3) standard circuit.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:40:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Balakrishnan", "Aneesh", ""], ["Lange", "Thomas", ""], ["Glorieux", "Maximilien", ""], ["Alexandrescu", "Dan", ""], ["Jenihhin", "Maksim", ""]]}, {"id": "2104.01929", "submitter": "Nik Sultana", "authors": "Andr\\'e DeHon, Hans Giesen, Nik Sultana, Yuanlong Xiao", "title": "Meta-level issues in Offloading: Scoping, Composition, Development, and\n  their Automation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.NI cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper argues for an accelerator development toolchain that takes into\naccount the whole system containing the accelerator. With whole-system\nvisibility, the toolchain can better assist accelerator scoping and composition\nin the context of the expected workloads and intended performance objectives.\nDespite being focused on the 'meta-level' of accelerators, this would build on\nexisting and ongoing DSLs and toolchains for accelerator design. Basing this on\nour experience in programmable networking and reconfigurable-hardware\nprogramming, we propose an integrative approach that relies on three\nactivities: (i) generalizing the focus of acceleration to offloading to\naccommodate a broader variety of non-functional needs -- such as security and\npower use -- while using similar implementation approaches, (ii) discovering\nwhat to offload, and to what hardware, through semi-automated analysis of a\nwhole system that might compose different offload choices that changeover time,\n(iii) connecting with research and state-of-the-art approaches for using\ndomain-specific languages (DSLs) and high-level synthesis (HLS) systems for\ncustom offload development. We outline how this integration can drive new\ndevelopment tooling that accepts models of programs and resources to assist\nsystem designers through design-space exploration for the accelerated system.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:16:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["DeHon", "Andr\u00e9", ""], ["Giesen", "Hans", ""], ["Sultana", "Nik", ""], ["Xiao", "Yuanlong", ""]]}, {"id": "2104.02162", "submitter": "Ercan Kalali", "authors": "Ercan Kalali and Rene van Leuken", "title": "Near-Precise Parameter Approximation for Multiple Multiplications on A\n  Single DSP Block", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiply-accumulate (MAC) operation is the main computation unit for DSP\napplications. DSP blocks are one of the efficient solutions to implement MACs\nin FPGA's. However, since the DSP blocks have wide multiplier and adder blocks,\nMAC operations using low bit-length parameters lead to an underutilization\nproblem. Hence, an efficient approximation technique is introduced. The\ntechnique includes manipulation and approximation of the low bit-length\nfixed-point parameters based upon a Single DSP - Multiple Multiplication (SDMM)\nexecution. The SDMM changes the traditional MAC implementation in the DSP block\nby separating multiplication and accumulation operations. While the accumulator\nhardware available in the DSP block is used for multiple parameter\nmultiplication, parallel LUTs are employed for the accumulation part of the MAC\noperation. The accuracy of the developed optimization technique was evaluated\nfor different CNN weight bit precisions using the Alexnet and VGG-16 networks\nand the Tiny ImageNet dataset. The optimization can be implemented without loss\nof accuracy in almost all cases, while it causes slight accuracy losses in a\nfew cases. Through these optimizations, the SDMM is performed at the cost of a\nsmall hardware overhead. For example, a single DSP block executes 3 8-bit\nfixed-point parameter multiplications. As a result of our optimizations, the\nparameters are represented in a different format on off-chip memory, providing\nup to 33% compression without any hardware cost. The compression rate can be\nfurther increased by up to 97% when used in conjunction with other compression\nmethods for the VGG-16. Reaching this compression rate requires extra hardware\ncost. A prototype systolic array architecture was implemented employing our\noptimizations on a Xilinx Zynq FPGA. It reduced the number of DSP blocks by\n66.6%, 75%, and 83.3% for 8, 6, and 4-bit input variables, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:26:19 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kalali", "Ercan", ""], ["van Leuken", "Rene", ""]]}, {"id": "2104.02184", "submitter": "Malte J. Rasch", "authors": "Malte J. Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio\n  Carta, Cindy Goldberg, Kaoutar El Maghraoui, Abu Sebastian, Vijay Narayanan", "title": "A flexible and fast PyTorch toolkit for simulating training and\n  inference on analog crossbar arrays", "comments": "Submitted to AICAS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the IBM Analog Hardware Acceleration Kit, a new and first of a\nkind open source toolkit to simulate analog crossbar arrays in a convenient\nfashion from within PyTorch (freely available at\nhttps://github.com/IBM/aihwkit). The toolkit is under active development and is\ncentered around the concept of an \"analog tile\" which captures the computations\nperformed on a crossbar array. Analog tiles are building blocks that can be\nused to extend existing network modules with analog components and compose\narbitrary artificial neural networks (ANNs) using the flexibility of the\nPyTorch framework. Analog tiles can be conveniently configured to emulate a\nplethora of different analog hardware characteristics and their non-idealities,\nsuch as device-to-device and cycle-to-cycle variations, resistive device\nresponse curves, and weight and output noise. Additionally, the toolkit makes\nit possible to design custom unit cell configurations and to use advanced\nanalog optimization algorithms such as Tiki-Taka. Moreover, the backward and\nupdate behavior can be set to \"ideal\" to enable hardware-aware training\nfeatures for chips that target inference acceleration only. To evaluate the\ninference accuracy of such chips over time, we provide statistical programming\nnoise and drift models calibrated on phase-change memory hardware. Our new\ntoolkit is fully GPU accelerated and can be used to conveniently estimate the\nimpact of material properties and non-idealities of future analog technology on\nthe accuracy for arbitrary ANNs.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 22:59:35 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Rasch", "Malte J.", ""], ["Moreda", "Diego", ""], ["Gokmen", "Tayfun", ""], ["Gallo", "Manuel Le", ""], ["Carta", "Fabio", ""], ["Goldberg", "Cindy", ""], ["Maghraoui", "Kaoutar El", ""], ["Sebastian", "Abu", ""], ["Narayanan", "Vijay", ""]]}, {"id": "2104.02188", "submitter": "Yaosheng Fu", "authors": "Yaosheng Fu, Evgeny Bolotin, Niladrish Chatterjee, David Nellans,\n  Stephen W. Keckler", "title": "GPU Domain Specialization via Composable On-Package Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As GPUs scale their low precision matrix math throughput to boost deep\nlearning (DL) performance, they upset the balance between math throughput and\nmemory system capabilities. We demonstrate that converged GPU design trying to\naddress diverging architectural requirements between FP32 (or larger) based HPC\nand FP16 (or smaller) based DL workloads results in sub-optimal configuration\nfor either of the application domains. We argue that a Composable On-PAckage\nGPU (COPAGPU) architecture to provide domain-specialized GPU products is the\nmost practical solution to these diverging requirements. A COPA-GPU leverages\nmulti-chip-module disaggregation to support maximal design reuse, along with\nmemory system specialization per application domain. We show how a COPA-GPU\nenables DL-specialized products by modular augmentation of the baseline GPU\narchitecture with up to 4x higher off-die bandwidth, 32x larger on-package\ncache, 2.3x higher DRAM bandwidth and capacity, while conveniently supporting\nscaled-down HPC-oriented designs. This work explores the microarchitectural\ndesign necessary to enable composable GPUs and evaluates the benefits\ncomposability can provide to HPC, DL training, and DL inference. We show that\nwhen compared to a converged GPU design, a DL-optimized COPA-GPU featuring a\ncombination of 16x larger cache capacity and 1.6x higher DRAM bandwidth scales\nper-GPU training and inference performance by 31% and 35% respectively and\nreduces the number of GPU instances by 50% in scale-out training scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 23:06:50 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fu", "Yaosheng", ""], ["Bolotin", "Evgeny", ""], ["Chatterjee", "Niladrish", ""], ["Nellans", "David", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2104.02199", "submitter": "Mehdi Sadi", "authors": "Kaniz Mishty, Mehdi Sadi", "title": "Designing Efficient and High-performance AI Accelerators with Customized\n  STT-MRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we demonstrate the design of efficient and high-performance\nAI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable\ncore. Based on model-driven detailed design space exploration, we present the\ndesign methodology of an innovative scratchpad-assisted on-chip STT-MRAM based\nbuffer system for high-performance accelerators. Using analytically derived\nexpression of memory occupancy time of AI model weights and activation maps,\nthe volatility of STT-MRAM is adjusted with process and temperature variation\naware scaling of thermal stability factor to optimize the retention time,\nenergy, read/write latency, and area of STT-MRAM. From the analysis of modern\nAI workloads and accelerator implementation in 14nm technology, we verify the\nefficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an\nSRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%\npower savings at iso-accuracy. Furthermore, with a relaxed bit error rate and\nnegligible AI accuracy trade-off, the designed STT-AI Ultra accelerator\nachieves 75.4%, and 3.5% savings in area and power, respectively over regular\nSRAM-based accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:34:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mishty", "Kaniz", ""], ["Sadi", "Mehdi", ""]]}, {"id": "2104.02251", "submitter": "Xiaofan Zhang", "authors": "Xiaofan Zhang, Hanchen Ye, Deming Chen", "title": "Being-ahead: Benchmarking and Exploring Accelerators for\n  Hardware-Efficient AI Deployment", "comments": "Published at MLSys'21 Workshop on Benchmarking Machine Learning\n  Workloads on Emerging Hardware. arXiv admin note: text overlap with\n  arXiv:2008.12745", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Customized hardware accelerators have been developed to provide improved\nperformance and efficiency for DNN inference and training. However, the\nexisting hardware accelerators may not always be suitable for handling various\nDNN models as their architecture paradigms and configuration tradeoffs are\nhighly application-specific. It is important to benchmark the accelerator\ncandidates in the earliest stage to gather comprehensive performance metrics\nand locate the potential bottlenecks. Further demands also emerge after\nbenchmarking, which require adequate solutions to address the bottlenecks and\nimprove the current designs for targeted workloads. To achieve these goals, in\nthis paper, we leverage an automation tool called DNNExplorer for benchmarking\ncustomized DNN hardware accelerators and exploring novel accelerator designs\nwith improved performance and efficiency. Key features include (1) direct\nsupport to popular machine learning frameworks for DNN workload analysis and\naccurate analytical models for fast accelerator benchmarking; (2) a novel\naccelerator design paradigm with high-dimensional design space support and\nfine-grained adjustability to overcome the existing design drawbacks; and (3) a\ndesign space exploration (DSE) engine to generate optimized accelerators by\nconsidering targeted AI workloads and available hardware resources. Results\nshow that accelerators adopting the proposed novel paradigm can deliver up to\n4.2X higher throughput (GOP/s) than the state-of-the-art pipeline design in\nDNNBuilder and up to 2.0X improved efficiency than the recently published\ngeneric design in HybridDNN given the same DNN model and resource budgets. With\nDNNExplorer's benchmarking and exploration features, we can be ahead at\nbuilding and optimizing customized AI accelerators and enable more efficient AI\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:33:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Xiaofan", ""], ["Ye", "Hanchen", ""], ["Chen", "Deming", ""]]}, {"id": "2104.02676", "submitter": "Pedro Filipe Silva", "authors": "Pedro Filipe Silva, Jo\\~ao Bispo, Nuno Paulino", "title": "Building Beyond HLS: Graph Analysis and Others", "comments": "3 pages, 1 table. Accepted at LATTE '21, an ASPLOS workshop. Slightly\n  differs from accepted version: includes some corrections and phrasing changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High-Level Synthesis has introduced reconfigurable logic to a new world --\nthat of software development. The newest wave of HLS tools has been successful,\nand the future looks bright. But is HLS the end-all-be-all to FPGA\nacceleration? Is it enough to allow non-experts to program FPGAs successfully,\neven when dealing with troublesome data structures and complex control flows --\nsuch as those often encountered in graph algorithms? We take a look at the\npanorama of adoption of HLS by the software community, focusing on graph\nanalysis in particular in order to generalise to \\textit{FPGA-unfriendly}\nproblems. We argue for the existence of shortcomings in current HLS development\nflows which hinder adoption, and present our perspective on the path forward,\nincluding how these issues may be remedied via higher-level tooling.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:07:45 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Silva", "Pedro Filipe", ""], ["Bispo", "Jo\u00e3o", ""], ["Paulino", "Nuno", ""]]}, {"id": "2104.02804", "submitter": "Alisha Menon", "authors": "Alisha Menon, Anirudh Natarajan, Reva Agashe, Daniel Sun, Melvin\n  Aristio, Harrison Liew, Yakun Sophia Shao, Jan M. Rabaey", "title": "Efficient emotion recognition using hyperdimensional computing with\n  combinatorial channel encoding and cellular automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a hardware-optimized approach to emotion recognition based on\nthe efficient brain-inspired hyperdimensional computing (HDC) paradigm is\nproposed. Emotion recognition provides valuable information for human-computer\ninteractions, however the large number of input channels (>200) and modalities\n(>3) involved in emotion recognition are significantly expensive from a memory\nperspective. To address this, methods for memory reduction and optimization are\nproposed, including a novel approach that takes advantage of the combinatorial\nnature of the encoding process, and an elementary cellular automaton. HDC with\nearly sensor fusion is implemented alongside the proposed techniques achieving\ntwo-class multi-modal classification accuracies of >76% for valence and >73%\nfor arousal on the multi-modal AMIGOS and DEAP datasets, almost always better\nthan state of the art. The required vector storage is seamlessly reduced by 98%\nand the frequency of vector requests by at least 1/5. The results demonstrate\nthe potential of efficient hyperdimensional computing for low-power,\nmulti-channeled emotion recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:46:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Menon", "Alisha", ""], ["Natarajan", "Anirudh", ""], ["Agashe", "Reva", ""], ["Sun", "Daniel", ""], ["Aristio", "Melvin", ""], ["Liew", "Harrison", ""], ["Shao", "Yakun Sophia", ""], ["Rabaey", "Jan M.", ""]]}, {"id": "2104.03024", "submitter": "Rolf Drechsler", "authors": "Rolf Drechsler", "title": "Polynomial Circuit Verification using BDDs", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification is one of the central tasks during circuit design. While most of\nthe approaches have exponential worst-case behaviour, in the following\ntechniques are discussed for proving polynomial circuit verification based on\nBinary Decision Diagrams (BDDs). It is shown that for circuits with specific\nstructural properties, like e.g. tree-like circuits, and circuits based on\nmultiplexers derived from BDDs complete formal verification can be carried out\nin polynomial time and space.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:56:42 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Drechsler", "Rolf", ""]]}, {"id": "2104.03142", "submitter": "Jos\\'e Moreira", "authors": "Jos\\'e E. Moreira, Kit Barton, Steven Battle, Peter Bergner, Ramon\n  Bertran, Puneeth Bhat, Pedro Caldeira, David Edelsohn, Gordon Fossum, Brad\n  Frey, Nemanja Ivanovic, Chip Kerchner, Vincent Lim, Shakti Kapoor, Tulio\n  Machado Filho, Silvia Melitta Mueller, Brett Olsson, Satish Sadasivam,\n  Baptiste Saleil, Bill Schmidt, Rajalakshmi Srinivasaraghavan, Shricharan\n  Srivatsan, Brian Thompto, Andreas Wagner, Nelson Wu", "title": "A matrix math facility for Power ISA(TM) processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power ISA(TM) Version 3.1 has introduced a new family of matrix math\ninstructions, collectively known as the Matrix-Multiply Assist (MMA) facility.\nThe instructions in this facility implement numerical linear algebra operations\non small matrices and are meant to accelerate computation-intensive kernels,\nsuch as matrix multiplication, convolution and discrete Fourier transform.\nThese instructions have led to a power- and area-efficient implementation of a\nhigh throughput math engine in the future POWER10 processor. Performance per\ncore is 4 times better, at constant frequency, than the previous generation\nPOWER9 processor. We also advocate the use of compiler built-ins as the\npreferred way of leveraging these instructions, which we illustrate through\ncase studies covering matrix multiplication and convolution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:17:32 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Moreira", "Jos\u00e9 E.", ""], ["Barton", "Kit", ""], ["Battle", "Steven", ""], ["Bergner", "Peter", ""], ["Bertran", "Ramon", ""], ["Bhat", "Puneeth", ""], ["Caldeira", "Pedro", ""], ["Edelsohn", "David", ""], ["Fossum", "Gordon", ""], ["Frey", "Brad", ""], ["Ivanovic", "Nemanja", ""], ["Kerchner", "Chip", ""], ["Lim", "Vincent", ""], ["Kapoor", "Shakti", ""], ["Filho", "Tulio Machado", ""], ["Mueller", "Silvia Melitta", ""], ["Olsson", "Brett", ""], ["Sadasivam", "Satish", ""], ["Saleil", "Baptiste", ""], ["Schmidt", "Bill", ""], ["Srinivasaraghavan", "Rajalakshmi", ""], ["Srivatsan", "Shricharan", ""], ["Thompto", "Brian", ""], ["Wagner", "Andreas", ""], ["Wu", "Nelson", ""]]}, {"id": "2104.03420", "submitter": "Kaiqi Zhang", "authors": "Kaiqi Zhang, Cole Hawkins, Xiyuan Zhang, Cong Hao, Zheng Zhang", "title": "On-FPGA Training with Ultra Memory Reduction: A Low-Precision Tensor\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various hardware accelerators have been developed for energy-efficient and\nreal-time inference of neural networks on edge devices. However, most training\nis done on high-performance GPUs or servers, and the huge memory and computing\ncosts prevent training neural networks on edge devices. This paper proposes a\nnovel tensor-based training framework, which offers orders-of-magnitude memory\nreduction in the training process. We propose a novel rank-adaptive tensorized\nneural network model, and design a hardware-friendly low-precision algorithm to\ntrain this model. We present an FPGA accelerator to demonstrate the benefits of\nthis training method on edge devices. Our preliminary FPGA implementation\nachieves $59\\times$ speedup and $123\\times$ energy reduction compared to\nembedded CPU, and $292\\times$ memory reduction over a standard full-size\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:27:39 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 21:58:12 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Kaiqi", ""], ["Hawkins", "Cole", ""], ["Zhang", "Xiyuan", ""], ["Hao", "Cong", ""], ["Zhang", "Zheng", ""]]}, {"id": "2104.03780", "submitter": "Michael J. Klaiber", "authors": "Michael J. Klaiber, Axel J. Acosta, Ingo Feldner, Falk Rehm", "title": "Enabling Cross-Domain Communication: How to Bridge the Gap between AI\n  and HW Engineers", "comments": "LATTE 2021 Workshop on Languages, Tools, and Techniques for\n  Accelerator Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key issue in system design is the lack of communication between hardware,\nsoftware and domain expert. Recent research work shows progress in automatic\nHW/SW co-design flows of neural accelerators that seems to make this kind of\ncommunication obsolete. Most real-world systems, however, are a composition of\nmultiple processing units, communication networks and memories. A HW/SW\nco-design process of (reconfigurable) neural accelerators, therefore, is an\nimportant sub-problem towards a common co-design methodology. The ultimate\nchallenge is to define the constraints for the design space exploration on\nsystem level - a task which requires deep knowledge and understanding of\nhardware architectures, mapping of workloads onto hardware and the application\ndomain, e.g. artificial intelligence.\n  For most projects, these skills are distributed among several people or even\ndifferent teams which is one of the major reasons why there is no established\nend-to-end development methodology for digital systems. This position paper\ndiscusses possibilities how to establish such a methodology for systems that\ninclude (reconfigurable) dedicated accelerators and outlines the central role\nthat languages and tools play in the process.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:05:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Klaiber", "Michael J.", ""], ["Acosta", "Axel J.", ""], ["Feldner", "Ingo", ""], ["Rehm", "Falk", ""]]}, {"id": "2104.03814", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Xinmiao Zhang", "title": "Algorithmic Obfuscation for LDPC Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to protect intellectual property against untrusted foundry, many\nlogic-locking schemes have been developed. The main idea of logic locking is to\ninsert a key-controlled block into a circuit to make the circuit function\nincorrectly without right keys. However, in the case that the algorithm\nimplemented by the circuit is naturally fault-tolerant or self-correcting,\nexisting logic-locking schemes do not affect the system performance much even\nif wrong keys are used. One example is low-density parity-check (LDPC)\nerror-correcting decoder, which has broad applications in digital\ncommunications and storage. This paper proposes two algorithmic-level\nobfuscation methods for LDPC decoders. By modifying the decoding process and\nlocking the stopping criterion, our new designs substantially degrade the\ndecoder throughput and/or error-correcting performance when the wrong key is\nused. Besides, our designs are also resistant to the SAT, AppSAT and removal\nattacks. For an example LDPC decoder, our proposed methods reduce the\nthroughput to less than 1/3 and/or increase the decoder error rate by at least\ntwo orders of magnitude with only 0.33% area overhead.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:49:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhou", "Jingbo", ""], ["Zhang", "Xinmiao", ""]]}, {"id": "2104.03868", "submitter": "Kubilay Ahmet K\\\"u\\c{c}\\\"uk", "authors": "Kubilay Ahmet K\\\"u\\c{c}\\\"uk, Andrew Martin", "title": "CRC: Fully General Model of Confidential Remote Computing", "comments": "37 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital services have been offered through remote systems for decades. The\nquestions of how these systems can be built in a trustworthy manner and how\ntheir security properties can be understood are given fresh impetus by recent\nhardware developments, allowing a fuller, more general, exploration of the\npossibilities than has previously been seen in the literature. Drawing on and\nconsolidating the disparate strains of research, technologies and methods\nemployed throughout the adaptation of confidential computing, we present a\nnovel, dedicated Confidential Remote Computing (CRC) model. CRC proposes a\ncompact solution for next-generation applications to be built on strong\nhardware-based security primitives, control of secure software products'\ntrusted computing base, and a way to make correct use of proofs and evidence\nreports generated by the attestation mechanisms. The CRC model illustrates the\ntrade-offs between decentralisation, task size and transparency overhead. We\nconclude the paper with six lessons learned from our approach, and suggest two\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:22:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Kubilay Ahmet", ""], ["Martin", "Andrew", ""]]}, {"id": "2104.04003", "submitter": "Aninda Manocha", "authors": "Marcelo Orenes-Vera, Aninda Manocha, David Wentzlaff, and Margaret\n  Martonosi", "title": "AutoSVA: Democratizing Formal Verification of RTL Module Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern SoC design relies on the ability to separately verify IP blocks\nrelative to their own specifications. Formal verification (FV) using\nSystemVerilog Assertions (SVA) is an effective method to exhaustively verify\nblocks at unit-level. Unfortunately, FV has a steep learning curve and requires\nengineering effort that discourages hardware designers from using it during RTL\nmodule development. We propose AutoSVA, a framework to automatically generate\nFV testbenches that verify liveness and safety of control logic involved in\nmodule interactions. We demonstrate AutoSVA's effectiveness and efficiency on\ndeadlock-critical modules of widely-used open-source hardware projects.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:36:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Orenes-Vera", "Marcelo", ""], ["Manocha", "Aninda", ""], ["Wentzlaff", "David", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2104.04763", "submitter": "Manu Awasthi", "authors": "Varun Gohil, Sumit Walia, Joycee Mekie, Manu Awasthi", "title": "Fixed-Posit: A Floating-Point Representation for Error-Resilient\n  Applications", "comments": "This is a pre-print for the paper version that has been accepted to\n  TCAS-II, Express Briefs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, almost all computer systems use IEEE-754 floating point to represent\nreal numbers. Recently, posit was proposed as an alternative to IEEE-754\nfloating point as it has better accuracy and a larger dynamic range. The\nconfigurable nature of posit, with varying number of regime and exponent bits,\nhas acted as a deterrent to its adoption. To overcome this shortcoming, we\npropose fixed-posit representation where the number of regime and exponent bits\nare fixed, and present the design of a fixed-posit multiplier. We evaluate the\nfixed-posit multiplier on error-resilient applications of AxBench and OpenBLAS\nbenchmarks as well as neural networks. The proposed fixed-posit multiplier has\n47%, 38.5%, 22% savings for power, area and delay respectively when compared to\nposit multipliers and up to 70%, 66%, 26% savings in power, area and delay\nrespectively when compared to 32-bit IEEE-754 multiplier. These savings are\naccompanied with minimal output quality loss (1.2% average relative error)\nacross OpenBLAS and AxBench workloads. Further, for neural networks like\nResNet-18 on ImageNet we observe a negligible accuracy loss (0.12%) on using\nthe fixed-posit multiplier.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:36:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gohil", "Varun", ""], ["Walia", "Sumit", ""], ["Mekie", "Joycee", ""], ["Awasthi", "Manu", ""]]}, {"id": "2104.05112", "submitter": "Zishen Wan", "authors": "Tian Gao, Zishen Wan, Yuyang Zhang, Bo Yu, Yanjun Zhang, Shaoshan Liu,\n  Arijit Raychowdhury", "title": "iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo\n  Matching on FPGA Platform", "comments": "Equal contributions from first two authors. Accepted by IEEE\n  International Conference on Artificial Intelligence Circuits and Systems\n  (AICAS), June 6-9, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is a critical task for robot navigation and autonomous\nvehicles, providing the depth estimation of surroundings. Among all stereo\nmatching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best\ntradeoffs between efficiency and accuracy. However, due to the inherent\niterative process and unpredictable memory access pattern, ELAS can only run at\n1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on\nlow-power platforms. In this paper, we propose an energy-efficient architecture\nfor real-time ELAS-based stereo matching on FPGA platform. Moreover, the\noriginal computational-intensive and irregular triangulation module is reformed\nin a regular manner with points interpolation, which is much more\nhardware-friendly. Optimizations, including memory management, parallelism, and\npipelining, are further utilized to reduce memory footprint and improve\nthroughput. Compared with Intel i7 CPU and the state-of-the-art CPU+FPGA\nimplementation, our FPGA realization achieves up to 38.4x and 3.32x frame rate\nimprovement, and up to 27.1x and 1.13x energy efficiency improvement,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 21:22:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gao", "Tian", ""], ["Wan", "Zishen", ""], ["Zhang", "Yuyang", ""], ["Yu", "Bo", ""], ["Zhang", "Yanjun", ""], ["Liu", "Shaoshan", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2104.05119", "submitter": "Jawad Haj-Yahya", "authors": "Jawad Haj-Yahya, Jisung Park, Rahul Bera, Juan G\\'omez Luna, Efraim\n  Rotem, Taha Shahroodi, Jeremie Kim, Onur Mutlu", "title": "BurstLink: Techniques for Energy-Efficient Conventional and Virtual\n  Reality Video Display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional planar video streaming is the most popular application in mobile\nsystems and the rapid growth of 360 video content and virtual reality (VR)\ndevices are accelerating the adoption of VR video streaming. Unfortunately,\nvideo streaming consumes significant system energy due to the high power\nconsumption of the system components (e.g., DRAM, display interfaces, and\ndisplay panel) involved in this process.\n  We propose BurstLink, a novel system-level technique that improves the energy\nefficiency of planar and VR video streaming. BurstLink is based on two key\nideas. First, BurstLink directly transfers a decoded video frame from the host\nsystem to the display panel, bypassing the host DRAM. To this end, we extend\nthe display panel with a double remote frame buffer (DRFB), instead of the\nDRAM's double frame buffer, so that the system can directly update the DRFB\nwith a new frame while updating the panel's pixels with the current frame\nstored in the DRFB. Second, BurstLink transfers a complete decoded frame to the\ndisplay panel in a single burst, using the maximum bandwidth of modern display\ninterfaces. Unlike conventional systems where the frame transfer rate is\nlimited by the pixel-update throughput of the display panel, BurstLink can\nalways take full advantage of the high bandwidth of modern display interfaces\nby decoupling the frame transfer from the pixel update as enabled by the DRFB.\nThis direct and burst frame transfer of BurstLink significantly reduces energy\nconsumption in video display by reducing access to the host DRAM and increasing\nthe system's residency at idle power states.\n  We evaluate BurstLink using an analytical power model that we rigorously\nvalidate on a real modern mobile system. Our evaluation shows that BurstLink\nreduces system energy consumption for 4K planar and VR video streaming by 41%\nand 33%, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:03:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Haj-Yahya", "Jawad", ""], ["Park", "Jisung", ""], ["Bera", "Rahul", ""], ["Luna", "Juan G\u00f3mez", ""], ["Rotem", "Efraim", ""], ["Shahroodi", "Taha", ""], ["Kim", "Jeremie", ""], ["Mutlu", "Onur", ""]]}, {"id": "2104.05532", "submitter": "Sam Ainsworth", "authors": "Sam Ainsworth", "title": "GhostMinion: A Strictness-Ordered Cache System for Spectre Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-order speculation, a technique ubiquitous since the early 1990s,\nremains a fundamental security flaw. Via attacks such as Spectre and Meltdown,\nan attacker can trick a victim, in an otherwise entirely correct program, into\nleaking its secrets through the effects of misspeculated execution, in a way\nthat is entirely invisible to the programmer's model. This has serious\nimplications for application sandboxing and inter-process communication.\n  Designing efficient mitigations, that preserve the performance of\nout-of-order execution, has been a challenge. The speculation-hiding techniques\nin the literature have been shown to not close such channels comprehensively,\nallowing adversaries to redesign attacks. Strong, precise guarantees are\nnecessary, but at the same time mitigations must achieve high performance to be\nadopted. We present Strictness Ordering, a new constraint system that shows how\nwe can comprehensively eliminate transient side channel attacks, while still\nallowing complex speculation and data forwarding between speculative\ninstructions. We then present GhostMinion, a cache modification built using a\nvariety of new techniques designed to provide Strictness Order at only 2.5%\noverhead.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:57:56 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ainsworth", "Sam", ""]]}, {"id": "2104.06214", "submitter": "Zhe Zhou", "authors": "Zhe Zhou, Bizhao Shi, Zhe Zhang, Yijin Guan, Guangyu Sun, Guojie Luo", "title": "BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant\n  Weight Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art\nalgorithms for analyzing non-euclidean graph data. By applying deep-learning to\nextract high-level representations from graph structures, GNNs achieve\nextraordinary accuracy and great generalization ability in various tasks.\nHowever, with the ever-increasing graph sizes, more and more complicated GNN\nlayers, and higher feature dimensions, the computational complexity of GNNs\ngrows exponentially. How to inference GNNs in real time has become a\nchallenging problem, especially for some resource-limited edge-computing\nplatforms.\n  To tackle this challenge, we propose BlockGNN, a software-hardware co-design\napproach to realize efficient GNN acceleration. At the algorithm level, we\npropose to leverage block-circulant weight matrices to greatly reduce the\ncomplexity of various GNN models. At the hardware design level, we propose a\npipelined CirCore architecture, which supports efficient block-circulant\nmatrices computation. Basing on CirCore, we present a novel BlockGNN\naccelerator to compute various GNNs with low latency. Moreover, to determine\nthe optimal configurations for diverse deployed tasks, we also introduce a\nperformance and resource model that helps choose the optimal hardware\nparameters automatically. Comprehensive experiments on the ZC706 FPGA platform\ndemonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\\times$\nspeedup compared to the baseline HyGCN architecture and $111.9\\times$ energy\nreduction compared to the Intel Xeon CPU platform.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:09:22 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 16:43:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhou", "Zhe", ""], ["Shi", "Bizhao", ""], ["Zhang", "Zhe", ""], ["Guan", "Yijin", ""], ["Sun", "Guangyu", ""], ["Luo", "Guojie", ""]]}, {"id": "2104.06535", "submitter": "Hamza Khan", "authors": "Hamza Khan, Asma Khan, Zainab Khan, Lun Bin Huang, Kun Wang and Lei He", "title": "NPE: An FPGA-based Overlay Processor for Natural Language Processing", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3431920.3439477", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, transformer-based models have shown state-of-the-art results\nfor Natural Language Processing (NLP). In particular, the introduction of the\nBERT language model brought with it breakthroughs in tasks such as question\nanswering and natural language inference, advancing applications that allow\nhumans to interact naturally with embedded devices. FPGA-based overlay\nprocessors have been shown as effective solutions for edge image and video\nprocessing applications, which mostly rely on low precision linear matrix\noperations. In contrast, transformer-based NLP techniques employ a variety of\nhigher precision nonlinear operations with significantly higher frequency. We\npresent NPE, an FPGA-based overlay processor that can efficiently execute a\nvariety of NLP models. NPE offers software-like programmability to the end user\nand, unlike FPGA designs that implement specialized accelerators for each\nnonlinear function, can be upgraded for future NLP models without requiring\nreconfiguration. We demonstrate that NPE can meet real-time conversational AI\nlatency targets for the BERT language model with $4\\times$ lower power than\nCPUs and $6\\times$ lower power than GPUs. We also show NPE uses $3\\times$ fewer\nFPGA resources relative to comparable BERT network-specific accelerators in the\nliterature. NPE provides a cost-effective and power-efficient FPGA-based\nsolution for Natural Language Processing at the edge.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:34:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Khan", "Hamza", ""], ["Khan", "Asma", ""], ["Khan", "Zainab", ""], ["Huang", "Lun Bin", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2104.06968", "submitter": "Haris Javaid", "authors": "Haris Javaid, Ji Yang, Nathania Santoso, Mohit Upadhyay,\n  Sundararajarao Mohan, Chengchen Hu, Gordon Brebner", "title": "Blockchain Machine: A Network-Attached Hardware Accelerator for\n  Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how Hyperledger Fabric, one of the most popular\npermissioned blockchains, can benefit from network-attached acceleration. The\nscalability and peak performance of Fabric is primarily limited by the\nbottlenecks present in its block validation/commit phase. We propose Blockchain\nMachine, a hardware accelerator coupled with a hardware-friendly communication\nprotocol, to act as the validator peer. It can be adapted to applications and\ntheir smart contracts, and is targeted for a server with network-attached FPGA\nacceleration card. The Blockchain Machine retrieves blocks and their\ntransactions in hardware directly from the network interface, which are then\nvalidated through a configurable and efficient block-level and\ntransaction-level pipeline. The validation results are then transferred to the\nhost CPU where non-bottleneck operations are executed. From our implementation\nintegrated with Fabric v1.4 LTS, we observed up to 17x speedup in block\nvalidation when compared to the software-only validator peer, with commit\nthroughput of up to 95,600 tps (~4.5x improvement over the best reported in\nliterature).\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:51:12 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Javaid", "Haris", ""], ["Yang", "Ji", ""], ["Santoso", "Nathania", ""], ["Upadhyay", "Mohit", ""], ["Mohan", "Sundararajarao", ""], ["Hu", "Chengchen", ""], ["Brebner", "Gordon", ""]]}, {"id": "2104.07582", "submitter": "Maciej Besta", "authors": "Maciej Besta, Raghavendra Kanakagiri, Grzegorz Kwasniewski, Rachata\n  Ausavarungnirun, Jakub Ber\\'anek, Konstantinos Kanellopoulos, Kacper Janda,\n  Zur Vonarburg-Shmaria, Lukas Gianinazzi, Ioana Stefan, Juan G\\'omez Luna,\n  Marcin Copik, Lukas Kapp-Schwoerer, Salvatore Di Girolamo, Marek Konieczny,\n  Onur Mutlu, Torsten Hoefler", "title": "SISA: Set-Centric Instruction Set Architecture for Graph Mining on\n  Processing-in-Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple graph algorithms such as PageRank have recently been the target of\nnumerous hardware accelerators. Yet, there also exist much more complex graph\nmining algorithms for problems such as clustering or maximal clique listing.\nThese algorithms are memory-bound and thus could be accelerated by hardware\ntechniques such as Processing-in-Memory (PIM). However, they also come with\nnon-straightforward parallelism and complicated memory access patterns. In this\nwork, we address this with a simple yet surprisingly powerful observation:\noperations on sets of vertices, such as intersection or union, form a large\npart of many complex graph mining algorithms, and can offer rich and simple\nparallelism at multiple levels. This observation drives our cross-layer design,\nin which we (1) expose set operations using a novel programming paradigm, (2)\nexpress and execute these operations efficiently with carefully designed\nset-centric ISA extensions called SISA, and (3) use PIM to accelerate SISA\ninstructions. The key design idea is to alleviate the bandwidth needs of SISA\ninstructions by mapping set operations to two types of PIM: in-DRAM bulk\nbitwise computing for bitvectors representing high-degree vertices, and\nnear-memory logic layers for integer arrays representing low-degree vertices.\nSet-centric SISA-enhanced algorithms are efficient and outperform hand-tuned\nbaselines, offering more than 10x speedup over the established Bron-Kerbosch\nalgorithm for listing maximal cliques. We deliver more than 10 SISA set-centric\nalgorithm formulations, illustrating SISA's wide applicability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:37:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Besta", "Maciej", ""], ["Kanakagiri", "Raghavendra", ""], ["Kwasniewski", "Grzegorz", ""], ["Ausavarungnirun", "Rachata", ""], ["Ber\u00e1nek", "Jakub", ""], ["Kanellopoulos", "Konstantinos", ""], ["Janda", "Kacper", ""], ["Vonarburg-Shmaria", "Zur", ""], ["Gianinazzi", "Lukas", ""], ["Stefan", "Ioana", ""], ["Luna", "Juan G\u00f3mez", ""], ["Copik", "Marcin", ""], ["Kapp-Schwoerer", "Lukas", ""], ["Di Girolamo", "Salvatore", ""], ["Konieczny", "Marek", ""], ["Mutlu", "Onur", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2104.07699", "submitter": "Jo\\~ao Dinis Ferreira", "authors": "Jo\\~ao Dinis Ferreira, Gabriel Falcao, Juan G\\'omez-Luna, Mohammed\n  Alser, Lois Orosa, Mohammad Sadrosadati, Jeremie S. Kim, Geraldo F. Oliveira,\n  Taha Shahroodi, Anant Nori, Onur Mutlu", "title": "pLUTo: In-DRAM Lookup Tables to Enable Massively Parallel\n  General-Purpose Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data movement between main memory and the processor is a significant\ncontributor to the execution time and energy consumption of memory-intensive\napplications. This data movement bottleneck can be alleviated using\nProcessing-in-Memory (PiM), which enables computation inside the memory chip.\nHowever, existing PiM architectures often lack support for complex operations,\nsince supporting these operations increases design complexity, chip area, and\npower consumption.\n  We introduce pLUTo (processing-in-memory with lookup table [LUT] operations),\na new DRAM substrate that leverages the high area density of DRAM to enable the\nmassively parallel storing and querying of lookup tables (LUTs). The use of\nLUTs enables the efficient execution of complex operations in-memory, which has\nbeen a long-standing challenge in the domain of PiM. When running a\nstate-of-the-art binary neural network in a single DRAM subarray, pLUTo\noutperforms the baseline CPU and GPU implementations by $33\\times$ and\n$8\\times$, respectively, while simultaneously achieving energy savings of\n$110\\times$ and $80\\times$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:10:22 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ferreira", "Jo\u00e3o Dinis", ""], ["Falcao", "Gabriel", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Alser", "Mohammed", ""], ["Orosa", "Lois", ""], ["Sadrosadati", "Mohammad", ""], ["Kim", "Jeremie S.", ""], ["Oliveira", "Geraldo F.", ""], ["Shahroodi", "Taha", ""], ["Nori", "Anant", ""], ["Mutlu", "Onur", ""]]}, {"id": "2104.07735", "submitter": "Jaume Abella", "authors": "Hamid Tabani (1), Fabio Mazzocchetti (1 and 2), Pedro Benedicte (1 and\n  2), Jaume Abella (1), Francisco J. Cazorla (1) ((1) Barcelona Supercomputing\n  Center (BSC), (2) Universitat Politecnica de Catalunya (UPC))", "title": "Performance Analysis and Optimization Opportunities for NVIDIA\n  Automotive GPUs", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2021.02.008", "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Advanced Driver Assistance Systems (ADAS) and Autonomous Driving (AD) bring\nunprecedented performance requirements for automotive systems. Graphic\nProcessing Unit (GPU) based platforms have been deployed with the aim of\nmeeting these requirements, being NVIDIA Jetson TX2 and its high-performance\nsuccessor, NVIDIA AGX Xavier, relevant representatives. However, to what extent\nhigh-performance GPU configurations are appropriate for ADAS and AD workloads\nremains as an open question.\n  This paper analyzes this concern and provides valuable insights on this\nquestion by modeling two recent automotive NVIDIA GPU-based platforms, namely\nTX2 and AGX Xavier. In particular, our work assesses their microarchitectural\nparameters against relevant benchmarks, identifying GPU setups delivering\nincreased performance within a similar cost envelope, or decreasing hardware\ncosts while preserving original performance levels. Overall, our analysis\nidentifies opportunities for the optimization of automotive GPUs to further\nincrease system efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:30:08 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Tabani", "Hamid", "", "1 and 2"], ["Mazzocchetti", "Fabio", "", "1 and 2"], ["Benedicte", "Pedro", "", "1 and\n  2"], ["Abella", "Jaume", ""], ["Cazorla", "Francisco J.", ""]]}, {"id": "2104.07776", "submitter": "Jonas Dann", "authors": "Jonas Dann and Daniel Ritter and Holger Fr\\\"oning", "title": "Demystifying Memory Access Patterns of FPGA-Based Graph Processing\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in reprogrammable hardware (e.g., FPGAs) and memory\ntechnology (e.g., DDR4, HBM) promise to solve performance problems inherent to\ngraph processing like irregular memory access patterns on traditional hardware\n(e.g., CPU). While several of these graph accelerators were proposed in recent\nyears, it remains difficult to assess their performance and compare them on\ncommon graph workloads and accelerator platforms, due to few open source\nimplementations and excessive implementation effort.\n  In this work, we build on a simulation environment for graph processing\naccelerators, to make several existing accelerator approaches comparable. This\nallows us to study relevant performance dimensions such as partitioning schemes\nand memory technology, among others. The evaluation yields insights into the\nstrengths and weaknesses of current graph processing accelerators along these\ndimensions, and features a novel in-depth comparison.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:53:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dann", "Jonas", ""], ["Ritter", "Daniel", ""], ["Fr\u00f6ning", "Holger", ""]]}, {"id": "2104.08009", "submitter": "Andreas Kurth", "authors": "Andreas Kurth, Fabian Schuiki, Luca Benini", "title": "Implementing CNN Layers on the Manticore Cluster-Based Many-Core\n  Architecture", "comments": "Technical report. 18 pages, 4 figures, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document presents implementations of fundamental convolutional neural\nnetwork (CNN) layers on the Manticore cluster-based many-core architecture and\ndiscusses their characteristics and trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:07:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kurth", "Andreas", ""], ["Schuiki", "Fabian", ""], ["Benini", "Luca", ""]]}, {"id": "2104.08323", "submitter": "David Stutz", "authors": "David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele", "title": "Random and Adversarial Bit Error Robustness: Energy-Efficient and Secure\n  DNN Accelerators", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.13977", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) accelerators received considerable attention in\nrecent years due to the potential to save energy compared to mainstream\nhardware. Low-voltage operation of DNN accelerators allows to further reduce\nenergy consumption significantly, however, causes bit-level failures in the\nmemory storing the quantized DNN weights. Furthermore, DNN accelerators have\nbeen shown to be vulnerable to adversarial attacks on voltage controllers or\nindividual bits. In this paper, we show that a combination of robust\nfixed-point quantization, weight clipping, as well as random bit error training\n(RandBET) or adversarial bit error training (AdvBET) improves robustness\nagainst random or adversarial bit errors in quantized DNN weights\nsignificantly. This leads not only to high energy savings for low-voltage\noperation as well as low-precision quantization, but also improves security of\nDNN accelerators. Our approach generalizes across operating voltages and\naccelerators, as demonstrated on bit errors from profiled SRAM arrays, and\nachieves robustness against both targeted and untargeted bit-level attacks.\nWithout losing more than 0.8%/2% in test accuracy, we can reduce energy\nconsumption on CIFAR10 by 20%/30% for 8/4-bit quantization using RandBET.\nAllowing up to 320 adversarial bit errors, AdvBET reduces test error from above\n90% (chance level) to 26.22% on CIFAR10.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:11:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stutz", "David", ""], ["Chandramoorthy", "Nandhini", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "2104.08333", "submitter": "Shawkat Khairullah", "authors": "Farah Natiq Kassab bashi, Shawkat S Khairullah", "title": "A survey on Dependable Digital Systems using FPGAs: Current Methods and\n  Challenges", "comments": "8 pages, 2 figures", "journal-ref": "International Journal of Advances in Computer and Electronics\n  Engineering, Vol. 5, No. 12, pp. 1-8, December 2020", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fault tolerance is increasingly being use to design Dependable Digital\nSystems (DDS), which refers to the capability of a system to keep performing\nits intended functions in existence of faults. DDS are typically used in\nSafety-critical system (SCS) such as medical (I&C) devices, Nuclear power\nPlants (I&C) devices and Aerospace (I&C) systems, the failure in these systems\ncan cause harm to environment, death, injury to people. Different fault\ntolerance techniques were developed to overcome these issues and that has led\nto increase the reliability and dependability of applications on Field\nProgrammable Gate Arrays (FPGAs). In this paper, multiple related works are\npresent dealing with different types of faults and fault tolerance methods in\nFPGA based systems. Furthermore, a comparison between the evaluation metrics of\nprevious works of Fault Tolerant (FT) techniques like hardware redundancy\noverhead, time delay, reliability, and performance are also present.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:23:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["bashi", "Farah Natiq Kassab", ""], ["Khairullah", "Shawkat S", ""]]}, {"id": "2104.08335", "submitter": "Suchita Pati", "authors": "Suchita Pati, Shaizeen Aga, Nuwan Jayasena, Matthew D. Sinclair", "title": "Demystifying BERT: Implications for Accelerator Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning in natural language processing (NLP), as realized using\nmodels like BERT (Bi-directional Encoder Representation from Transformer), has\nsignificantly improved language representation with models that can tackle\nchallenging language problems. Consequently, these applications are driving the\nrequirements of future systems. Thus, we focus on BERT, one of the most popular\nNLP transfer learning algorithms, to identify how its algorithmic behavior can\nguide future accelerator design. To this end, we carefully profile BERT\ntraining and identify key algorithmic behaviors which are worthy of attention\nin accelerator design.\n  We observe that while computations which manifest as matrix multiplication\ndominate BERT's overall runtime, as in many convolutional neural networks,\nmemory-intensive computations also feature prominently. We characterize these\ncomputations, which have received little attention so far. Further, we also\nidentify heterogeneity in compute-intensive BERT computations and discuss\nsoftware and possible hardware mechanisms to further optimize these\ncomputations. Finally, we discuss implications of these behaviors as networks\nget larger and use distributed training environments, and how techniques such\nas micro-batching and mixed-precision training scale. Overall, our analysis\nidentifies holistic solutions to optimize systems for BERT-like models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:06:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pati", "Suchita", ""], ["Aga", "Shaizeen", ""], ["Jayasena", "Nuwan", ""], ["Sinclair", "Matthew D.", ""]]}, {"id": "2104.08378", "submitter": "Jeff Pool", "authors": "Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\n  Stosic, Ganesh Venkatesh, Chong Yu, Paulius Micikevicius", "title": "Accelerating Sparse Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural network model sizes have dramatically increased, so has the\ninterest in various techniques to reduce their parameter counts and accelerate\ntheir execution. An active area of research in this field is sparsity -\nencouraging zero values in parameters that can then be discarded from storage\nor computations. While most research focuses on high levels of sparsity, there\nare challenges in universally maintaining model accuracy as well as achieving\nsignificant speedups over modern matrix-math hardware. To make sparsity\nadoption practical, the NVIDIA Ampere GPU architecture introduces sparsity\nsupport in its matrix-math units, Tensor Cores. We present the design and\nbehavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern\nthat leads to twice the math throughput of dense matrix units. We also describe\na simple workflow for training networks that both satisfy 2:4 sparsity pattern\nrequirements and maintain accuracy, verifying it on a wide range of common\ntasks and model architectures. This workflow makes it easy to prepare accurate\nmodels for efficient deployment on Sparse Tensor Cores.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:27:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Asit", ""], ["Latorre", "Jorge Albericio", ""], ["Pool", "Jeff", ""], ["Stosic", "Darko", ""], ["Stosic", "Dusan", ""], ["Venkatesh", "Ganesh", ""], ["Yu", "Chong", ""], ["Micikevicius", "Paulius", ""]]}, {"id": "2104.08559", "submitter": "Yujie Cui", "authors": "Yujie Cui, Xu Cheng", "title": "Abusing Cache Line Dirty States to Leak Information in Commercial\n  Processors", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caches have been used to construct various types of covert and side channels\nto leak information. Most of the previous cache channels exploit the timing\ndifference between cache hits and cache misses. However, we introduce a new and\nbroader classification of cache covert channel attacks: Hit+Miss, Hit+Hit,\nMiss+Miss. We highlight that cache misses (or cache hits) in different states\nmay have more significant time differences, which can be used as timing\nchannels. Based on the classification, We propose a new type of stable and\nstealthy Miss+Miss cache channel.\n  The write-back caches are widely deployed in modern processors. This paper\npresents in detail how to use replacement latency difference to construct\ntiming-based channels (calles WB channel) to leak information in the write-back\ncache: any modification to a cache line by a sender will set the cache line to\nthe dirty state, and the receiver can observe this through measuring the\nlatency to replace this cache set. We also demonstrate how senders could\nexploit a different number of dirty cache lines in a cache set to improve\ntransmission bandwidth with symbols encoding multiple bits. The peak\ntransmission bandwidths of the WB channels in commercial systems can vary\nbetween 1300 to 4400 Kbps per cache set in the hyper-threaded setting without\nshared memory between the sender and the receiver. Different from most existing\ncache channels that always target specific memory addresses, the new WB\nchannels focus on the cache set and cache line states, making the channel hard\nto be disturbed by other processes on the core and can still work in the cache\nusing a random replacement policy. We also analyzed the stealthiness of WB\nchannels from the perspective of the number of cache loads and cache miss\nrates. Further, This paper discusses and evaluates possible defenses. The paper\nfinishes by discussing various forms of side-channel attacks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:46:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cui", "Yujie", ""], ["Cheng", "Xu", ""]]}, {"id": "2104.08699", "submitter": "Mao Ye", "authors": "Mao Ye", "title": "FOX: Hardware-Assisted File Auditing for Direct Access NVM-Hosted\n  Filesystems", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With emerging non-volatile memories entering the mainstream market, several\noperating systems start to incorporate new changes and optimizations. One major\nOS support is the direct-access for files, which enables efficient access for\nfiles hosted in byte-addressable NVM systems. With DAX-enabled filesystems,\nfiles can be accessed directly similar to memory with typical load/store\noperations. Despite its efficiency, the frequently used system call of direct\naccess is troublesome for system auditing. File system auditing is mandatory\nand widely used because auditing logs can help detect anomalies, suspicious\nfile accesses, or be used as an evidence in digital forensics. However, the\nfrequent and long-time usage of direct access call blinds the operating system\nor file system from tracking process operations to shared files after the\ninitial page faults. This might results in imprecise casualty analysis and\nleads to false conclusion for attack detection. To remedy the tension between\nenabling fine-grained file system auditing and leveraging the performance of\nNVM-hosted file systems, we propose a novel hardware-assisted auditing scheme,\nFOX. FOX enables file system auditing through lightweight hardware-software\nchanges which can monitor every read or write event for mapped files on NVM.\nAdditionally, we propose the optimized schemes, that enable auditing\nflexibility for selected files/memory range. By prototyping FOX on a full\nsystem simulator, Gem5, we observe a relatively small reduced throughput and an\nacceptable extra writes compared to our baseline. Compared to other\ninstrumentation-based software schemes, our scheme is low-overhead and secure.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 03:47:40 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 02:10:23 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ye", "Mao", ""]]}, {"id": "2104.08734", "submitter": "Ashish Gondimalla", "authors": "Ashish Gondimalla, Sree Charan Gundabolu, T.N. Vijaykumar, and Mithuna\n  Thottethodi", "title": "Barrier-Free Large-Scale Sparse Tensor Accelerator (BARISTA) For\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are emerging as powerful tools for\nvisual recognition. Recent architecture proposals for sparse CNNs exploit zeros\nin the feature maps and filters for performance and energy without losing\naccuracy. Sparse architectures that exploit two-sided sparsity in both feature\nmaps and filters have been studied only at small scales (e.g., 1K\nmultiply-accumulate(MAC) units). However, to realize their advantages in full,\nthe sparse architectures have to be scaled up to levels of the dense\narchitectures (e.g., 32K MACs in the TPU). Such scaling is challenging since\nachieving reuse through broadcasts incurs implicit barrier cost raises the\ninter-related issues of load imbalance, buffering, and on-chip bandwidth\ndemand. SparTen, a previous scheme, addresses one aspect of load balancing but\nnot other aspects, nor the other issues of buffering and bandwidth. To that\nend, we propose the barrier-free large-scale sparse tensor accelerator\n(BARISTA). BARISTA (1) is the first architecture for scaling up sparse CNN\naccelerators; (2) reduces on-chip bandwidth demand by telescoping\nrequest-combining the input map requests and snarfing the filter requests; (3)\nreduces buffering via basic buffer sharing and avoids the ensuing barriers\nbetween consecutive input maps by coloring the output buffers; (4) load\nbalances intra-filter work via dynamic round-robin work assignment; and (5)\nemploys hierarchical buffering which achieves high cache bandwidth via a few,\nwide, shared buffers and low buffering via narrower, private buffers at the\ncompute. Our simulations show that, on average, barista performs 5.4x, 2.2x,\n1.7x, 2.5x better than a dense, a one-sided, a naively-scaled two-sided, and an\niso-area two-sided architecture, respectively. Using 45-nm technology, ASIC\nsynthesis of our RTL design for four clusters of 8K MACs at 1 GHz clock speed,\nreports 213 mm$^2$ area and 170 W power.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:13:31 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 22:19:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gondimalla", "Ashish", ""], ["Gundabolu", "Sree Charan", ""], ["Vijaykumar", "T. N.", ""], ["Thottethodi", "Mithuna", ""]]}, {"id": "2104.09252", "submitter": "Nima TaheriNejad", "authors": "Lukas Baischer, Matthias Wess, Nima TaheriNejad", "title": "Learning on Hardware: A Tutorial on Neural Network Accelerators and\n  Co-Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have the advantage that they can take into\naccount a large number of parameters, which enables them to solve complex\ntasks. In computer vision and speech recognition, they have a better accuracy\nthan common algorithms, and in some tasks, they boast an even higher accuracy\nthan human experts. With the progress of DNNs in recent years, many other\nfields of application such as diagnosis of diseases and autonomous driving are\ntaking advantage of them. The trend at DNNs is clear: The network size is\ngrowing exponentially, which leads to an exponential increase in computational\neffort and required memory size. For this reason, optimized hardware\naccelerators are used to increase the performance of the inference of neuronal\nnetworks. However, there are various neural network hardware accelerator\nplatforms, such as graphics processing units (GPUs), application specific\nintegrated circuits (ASICs) and field programmable gate arrays (FPGAs). Each of\nthese platforms offer certain advantages and disadvantages. Also, there are\nvarious methods for reducing the computational effort of DNNs, which are\ndifferently suitable for each hardware accelerator. In this article an overview\nof existing neural network hardware accelerators and acceleration methods is\ngiven. Their strengths and weaknesses are shown and a recommendation of\nsuitable applications is given. In particular, we focus on acceleration of the\ninference of convolutional neural networks (CNNs) used for image recognition\ntasks. Given that there exist many different hardware architectures. FPGA-based\nimplementations are well-suited to show the effect of DNN optimization methods\non accuracy and throughput. For this reason, the focus of this work is more on\nFPGA-based implementations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 12:50:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Baischer", "Lukas", ""], ["Wess", "Matthias", ""], ["TaheriNejad", "Nima", ""]]}, {"id": "2104.09480", "submitter": "Caleb Terrill", "authors": "Caleb Terrill, Linfang Wang, Sean Chen, Chester Hulse, Calvin Kuo,\n  Richard Wesel, Dariush Divsalar", "title": "FPGA Implementations of Layered MinSum LDPC Decoders Using RCQ Message\n  Passing", "comments": "6 pages, 5 figures, submitted to GLOBECOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniform message quantization techniques such as\nreconstruction-computation-quantization (RCQ) improve error-correction\nperformance and decrease hardware complexity of low-density parity-check (LDPC)\ndecoders that use a flooding schedule. Layered MinSum RCQ (L-msRCQ) enables\nmessage quantization to be utilized for layered decoders and irregular LDPC\ncodes. We investigate field-programmable gate array (FPGA) implementations of\nL-msRCQ decoders. Three design methods for message quantization are presented,\nwhich we name the Lookup, Broadcast, and Dribble methods. The decoding\nperformance and hardware complexity of these schemes are compared to a layered\noffset MinSum (OMS) decoder. Simulation results on a (16384, 8192)\nprotograph-based raptor-like (PBRL) LDPC code show that a 4-bit L-msRCQ decoder\nusing the Broadcast method can achieve a 0.03 dB improvement in\nerror-correction performance while using 12% fewer registers than the OMS\ndecoder. A Broadcast-based 3-bit L-msRCQ decoder uses 15% fewer lookup tables,\n18% fewer registers, and 13% fewer routed nets than the OMS decoder, but\nresults in a 0.09 dB loss in performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:40:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Terrill", "Caleb", ""], ["Wang", "Linfang", ""], ["Chen", "Sean", ""], ["Hulse", "Chester", ""], ["Kuo", "Calvin", ""], ["Wesel", "Richard", ""], ["Divsalar", "Dariush", ""]]}, {"id": "2104.09487", "submitter": "Mayank Goel", "authors": "Mayank Goel and Gourav Singal", "title": "Android OS CASE STUDY", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Android is a mobile operating system based on a modified version of the Linux\nkernel and other open source software, designed primarily for touchscreen\nmobile devices such as smartphones and tablets. It is an operating system for\nlow powered devices that run on battery and are full of hardware like Global\nPositioning System (GPS) receivers, cameras, light and orientation sensors,\nWi-Fi and LTE (4G telephony) connectivity and a touch screen. Like all\noperating systems, Android enables applications to make use of the hardware\nfeatures through abstraction and provide a defined environment for\napplications. The study includes following topic: Background And History\nAndroid Architecture Kernel And StartUp Process Process Management Deadlock CPU\nScheduling Memory Management Storage Management I/O Battery Optimization\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:46:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Goel", "Mayank", ""], ["Singal", "Gourav", ""]]}, {"id": "2104.09502", "submitter": "A. Yavuz Oruc", "authors": "A. Yavuz Oruc, A. Atmaca, Y. Nevzat Sengun, A. Semi Yeniyol", "title": "CodeAPeel: An Integrated and Layered Learning Technology For Computer\n  Architecture Courses", "comments": "Minor revision and some typos are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a versatile, multi-layered technology to help support\nteaching and learning core computer architecture concepts. This technology,\ncalled CodeAPeel is already implemented in one particular form to describe\ninstruction processing in compiler, assembly, and machine layers of a generic\ninstruction set architecture by a comprehensive simulation of its\nfetch-decode-execute cycle as well as animation of the behavior of its CPU\nregisters, RAM, VRAM, STACK memories, various control registers, and graphics\nscreen. Unlike most educational CPU simulators that simulate a real processor\nsuch as MIPS or RISC-V, CodeAPeel is designed and implemented as a generic RISC\ninstruction set architecture simulator with both scalar and vector instructions\nto provide a dual-mode processor simulator as described by Flynn's\nclassification of SISD and SIMD processors. Vectorization of operations is\nbuilt into the instruction repertoire of CodeAPeel, making it straightforward\nto simulate such processors with powerful vector instructions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:04:49 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:15:43 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 19:56:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Oruc", "A. Yavuz", ""], ["Atmaca", "A.", ""], ["Sengun", "Y. Nevzat", ""], ["Yeniyol", "A. Semi", ""]]}, {"id": "2104.09611", "submitter": "Jisung Park", "authors": "Jisung Park, Myungsuk Kim, Myoungjun Chun, Lois Orosa, Jihong Kim,\n  Onur Mutlu", "title": "Reducing Solid-State Drive Read Latency by Optimizing Read-Retry", "comments": "Full paper to appear in ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D NAND flash memory with advanced multi-level cell techniques provides high\nstorage density, but suffers from significant performance degradation due to a\nlarge number of read-retry operations. Although the read-retry mechanism is\nessential to ensuring the reliability of modern NAND flash memory, it can\nsignificantly increase the read latency of an SSD by introducing multiple retry\nsteps that read the target page again with adjusted read-reference voltage\nvalues. Through a detailed analysis of the read mechanism and rigorous\ncharacterization of 160 real 3D NAND flash memory chips, we find new\nopportunities to reduce the read-retry latency by exploiting two advanced\nfeatures widely adopted in modern NAND flash-based SSDs: 1) the CACHE READ\ncommand and 2) strong ECC engine. First, we can reduce the read-retry latency\nusing the advanced CACHE READ command that allows a NAND flash chip to perform\nconsecutive reads in a pipelined manner. Second, there exists a large\nECC-capability margin in the final retry step that can be used for reducing the\nchip-level read latency. Based on our new findings, we develop two new\ntechniques that effectively reduce the read-retry latency: 1) Pipelined\nRead-Retry (PR$^2$) and 2) Adaptive Read-Retry (AR$^2$). PR$^2$ reduces the\nlatency of a read-retry operation by pipelining consecutive retry steps using\nthe CACHE READ command. AR$^2$ shortens the latency of each retry step by\ndynamically reducing the chip-level read latency depending on the current\noperating conditions that determine the ECC-capability margin. Our evaluation\nusing twelve real-world workloads shows that our proposal improves SSD response\ntime by up to 31.5% (17% on average) over a state-of-the-art baseline with only\nsmall changes to the SSD controller.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:37:44 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Park", "Jisung", ""], ["Kim", "Myungsuk", ""], ["Chun", "Myoungjun", ""], ["Orosa", "Lois", ""], ["Kim", "Jihong", ""], ["Mutlu", "Onur", ""]]}, {"id": "2104.09616", "submitter": "Callie Hao", "authors": "Lixiang Li, Yao Chen, Zacharie Zirnheld, Pan Li, and Cong Hao", "title": "MELOPPR: Software/Hardware Co-design for Memory-efficient Low-latency\n  Personalized PageRank", "comments": "Accepted by IEEE Design Automation Conference, 2021 (DAC'21). Six\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized PageRank (PPR) is a graph algorithm that evaluates the\nimportance of the surrounding nodes from a source node. Widely used in social\nnetwork related applications such as recommender systems, PPR requires\nreal-time responses (latency) for a better user experience. Existing works\neither focus on algorithmic optimization for improving precision while\nneglecting hardware implementations or focus on distributed global graph\nprocessing on large-scale systems for improving throughput rather than response\ntime. Optimizing low-latency local PPR algorithm with a tight memory budget on\nedge devices remains unexplored. In this work, we propose a memory-efficient,\nlow-latency PPR solution, namely MeLoPPR, with largely reduced memory\nrequirement and a flexible trade-off between latency and precision. MeLoPPR is\ncomposed of stage decomposition and linear decomposition and exploits the node\nscore sparsity: Through stage and linear decomposition, MeLoPPR breaks the\ncomputation on a large graph into a set of smaller sub-graphs, that\nsignificantly saves the computation memory; Through sparsity exploitation,\nMeLoPPR selectively chooses the sub-graphs that contribute the most to the\nprecision to reduce the required computation. In addition, through\nsoftware/hardware co-design, we propose a hardware implementation on a hybrid\nCPU and FPGA accelerating platform, that further speeds up the sub-graph\ncomputation. We evaluate the proposed MeLoPPR on memory-constrained devices\nincluding a personal laptop and Xilinx Kintex-7 KC705 FPGA using six real-world\ngraphs. First, MeLoPPR demonstrates significant memory saving by 1.5x to 13.4x\non CPU and 73x to 8699x on FPGA. Second, MeLoPPR allows flexible trade-offs\nbetween precision and execution time: when the precision is 80%, the speedup on\nCPU is up to 15x and up to 707x on FPGA; when the precision is around 90%, the\nspeedup is up to 70x on FPGA.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:37:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Lixiang", ""], ["Chen", "Yao", ""], ["Zirnheld", "Zacharie", ""], ["Li", "Pan", ""], ["Hao", "Cong", ""]]}, {"id": "2104.09768", "submitter": "Carl-Johannes Johnsen", "authors": "Carl-Johannes Johnsen, Alberte Thegler, Kenneth Skovhede and Brian\n  Vinter", "title": "SME: A High Productivity FPGA Tool for Software Programmers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For several decades, the CPU has been the standard model to use in the\nmajority of computing. While the CPU does excel in some areas, heterogeneous\ncomputing, such as reconfigurable hardware, is showing increasing potential in\nareas like parallelization, performance, and power usage. This is especially\nprominent in problems favoring deep pipelining or tight latency requirements.\nHowever, due to the nature of these problems, they can be hard to program, at\nleast for software developers. Synchronous Message Exchange (SME) is a runtime\nenvironment that allows development, testing and verification of hardware\ndesigns for FPGA devices in C#, with access to modern debugging and code\nfeatures. The goal is to create a framework for software developers to easily\nimplement systems for FPGA devices without having to obtain heavy hardware\nprogramming knowledge. This article presents a short introduction to the SME\nmodel as well as new updates to SME. Lastly, a selection of student projects\nand examples will be presented in order to show how it is possible to create\nquite complex structures in SME, even by students with no hardware experience.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 05:41:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Johnsen", "Carl-Johannes", ""], ["Thegler", "Alberte", ""], ["Skovhede", "Kenneth", ""], ["Vinter", "Brian", ""]]}, {"id": "2104.09798", "submitter": "Alireza Khadem", "authors": "Alireza Khadem, Haojie Ye, Trevor Mudge", "title": "CoDR: Computation and Data Reuse Aware CNN Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computation and Data Reuse is critical for the resource-limited Convolutional\nNeural Network (CNN) accelerators. This paper presents Universal Computation\nReuse to exploit weight sparsity, repetition, and similarity simultaneously in\na convolutional layer. Moreover, CoDR decreases the cost of weight memory\naccess by proposing a customized Run-Length Encoding scheme and the number of\nmemory accesses to the intermediate results by introducing an input and output\nstationary dataflow. Compared to two recent compressed CNN accelerators with\nthe same area of 2.85 mm^2, CoDR decreases SRAM access by 5.08x and 7.99x, and\nconsumes 3.76x and 6.84x less energy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:20:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Khadem", "Alireza", ""], ["Ye", "Haojie", ""], ["Mudge", "Trevor", ""]]}, {"id": "2104.09820", "submitter": "Bo Zhang", "authors": "Bo Zhang, Pedro V. Sander, Chi-Ying Tsui, Amine Bermak", "title": "Microshift: An Efficient Image Compression Algorithm for Hardware", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2880227", "report-no": null, "categories": "eess.IV cs.AR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an image compression algorithm called Microshift.\nWe employ an algorithm hardware co-design methodology, yielding a\nhardware-friendly compression approach with low power consumption. In our\nmethod, the image is first micro-shifted, then the sub-quantized values are\nfurther compressed. Two methods, the FAST and MRF model, are proposed to\nrecover the bit-depth by exploiting the spatial correlation of natural images.\nBoth methods can decompress images progressively. Our compression algorithm\ncompresses images to 1.25 bits per pixel on average with PSNR of 33.16 dB,\noutperforming other on-chip compression algorithms. Then, we propose a hardware\narchitecture and implement the algorithm on an FPGA and ASIC. The results on\nthe VLSI design further validate the low hardware complexity and high power\nefficiency, showing our method is promising, particularly for low-power\nwireless vision sensor networks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:14:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Bo", ""], ["Sander", "Pedro V.", ""], ["Tsui", "Chi-Ying", ""], ["Bermak", "Amine", ""]]}, {"id": "2104.10415", "submitter": "YUqiong Qi", "authors": "Yuqiong Qi and Yang Hu and Haibin Wu and Shen Li and Haiyu Mao and\n  Xiaochun Ye and Dongrui Fan and Ninghui Sun", "title": "Tackling Variabilities in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The state-of-the-art driving automation system demands extreme computational\nresources to meet rigorous accuracy and latency requirements. Though emerging\ndriving automation computing platforms are based on ASIC to provide better\nperformance and power guarantee, building such an accelerator-based computing\nplatform for driving automation still present challenges. First, the workloads\nmix and performance requirements exposed to driving automation system present\nsignificant variability. Second, with more cameras/sensors integrated in a\nfuture fully autonomous driving vehicle, a heterogeneous multi-accelerator\narchitecture substrate is needed that requires a design space exploration for a\nnew form of parallelism. In this work, we aim to extensively explore the above\nsystem design challenges and these challenges motivate us to propose a\ncomprehensive framework that synergistically handles the heterogeneous hardware\naccelerator design principles, system design criteria, and task scheduling\nmechanism. Specifically, we propose a novel heterogeneous multi-core AI\naccelerator (HMAI) to provide the hardware substrate for the driving automation\ntasks with variability. We also define system design criteria to better utilize\nhardware resources and achieve increased throughput while satisfying the\nperformance and energy restrictions. Finally, we propose a deep reinforcement\nlearning (RL)-based task scheduling mechanism FlexAI, to resolve task mapping\nissue. Experimental results show that with FlexAI scheduling, basically 100%\ntasks in each driving route can be processed by HMAI within their required\nperiod to ensure safety, and FlexAI can also maximally reduce the breaking\ndistance up to 96% as compared to typical heuristics and guided\nrandom-search-based algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:51:40 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Qi", "Yuqiong", ""], ["Hu", "Yang", ""], ["Wu", "Haibin", ""], ["Li", "Shen", ""], ["Mao", "Haiyu", ""], ["Ye", "Xiaochun", ""], ["Fan", "Dongrui", ""], ["Sun", "Ninghui", ""]]}, {"id": "2104.10712", "submitter": "Haowen Fang", "authors": "Haowen Fang, Brady Taylor, Ziru Li, Zaidao Mei, Hai Li, Qinru Qiu", "title": "Neuromorphic Algorithm-hardware Codesign for Temporal Pattern Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing and spiking neural networks (SNN) mimic the behavior\nof biological systems and have drawn interest for their potential to perform\ncognitive tasks with high energy efficiency. However, some factors such as\ntemporal dynamics and spike timings prove critical for information processing\nbut are often ignored by existing works, limiting the performance and\napplications of neuromorphic computing. On one hand, due to the lack of\neffective SNN training algorithms, it is difficult to utilize the temporal\nneural dynamics. Many existing algorithms still treat neuron activation\nstatistically. On the other hand, utilizing temporal neural dynamics also poses\nchallenges to hardware design. Synapses exhibit temporal dynamics, serving as\nmemory units that hold historical information, but are often simplified as a\nconnection with weight. Most current models integrate synaptic activations in\nsome storage medium to represent membrane potential and institute a hard reset\nof membrane potential after the neuron emits a spike. This is done for its\nsimplicity in hardware, requiring only a \"clear\" signal to wipe the storage\nmedium, but destroys temporal information stored in the neuron.\n  In this work, we derive an efficient training algorithm for Leaky Integrate\nand Fire neurons, which is capable of training a SNN to learn complex spatial\ntemporal patterns. We achieved competitive accuracy on two complex datasets. We\nalso demonstrate the advantage of our model by a novel temporal pattern\nassociation task. Codesigned with this algorithm, we have developed a CMOS\ncircuit implementation for a memristor-based network of neuron and synapses\nwhich retains critical neural dynamics with reduced complexity. This circuit\nimplementation of the neuron model is simulated to demonstrate its ability to\nreact to temporal spiking patterns with an adaptive threshold.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:23:31 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 03:41:52 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fang", "Haowen", ""], ["Taylor", "Brady", ""], ["Li", "Ziru", ""], ["Mei", "Zaidao", ""], ["Li", "Hai", ""], ["Qiu", "Qinru", ""]]}, {"id": "2104.10941", "submitter": "Aneesh Balakrishnan", "authors": "Dan Alexandrescu, Aneesh Balakrishnan, Thomas Lange and Maximilien\n  Glorieux", "title": "Enabling Cross-Layer Reliability and Functional Safety Assessment\n  Through ML-Based Compact Models", "comments": "7 pages (paper) + 1 page copyright statement, Number of figures: 3,\n  Conference: 2020 IEEE 26th International Symposium on On-Line Testing and\n  Robust System Design (IOLTS)", "journal-ref": null, "doi": "10.1109/IOLTS50870.2020.9159750", "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical design flows are hierarchical and rely on assembling many individual\ntechnology elements from standard cells to complete boards. Providers use\ncompact models to provide simplified views of their products to their users.\nDesigners group simpler elements in more complex structures and have to manage\nthe corresponding propagation of reliability and functional safety information\nthrough the hierarchy of the system, accompanied by the obvious problems of IP\nconfidentiality, possibility of reverse engineering and so on. This paper\nproposes a machine-learning-based approach to integrate the many individual\nmodels of a subsystem's elements in a single compact model that can be re-used\nand assembled further up in the hierarchy. The compact models provide\nconsistency, accuracy and confidentiality, allowing technology, IP, component,\nsub-system or system providers to accompany their offering with high-quality\nreliability and functional safety compact models that can be safely and\naccurately consumed by their users.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:13:57 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Alexandrescu", "Dan", ""], ["Balakrishnan", "Aneesh", ""], ["Lange", "Thomas", ""], ["Glorieux", "Maximilien", ""]]}, {"id": "2104.11359", "submitter": "Mingsheng Ying", "authors": "Mingsheng Ying", "title": "Model Checking for Verification of Quantum Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this talk, we will describe a framework for assertion-based verification\n(ABV) of quantum circuits by applying model checking techniques for quantum\nsystems developed in our previous work, in which:\n  (i) Noiseless and noisy quantum circuits are modelled as operator- and\nsuper-operator-valued transition systems, respectively, both of which can be\nfurther represented by tensor networks.\n  (ii) Quantum assertions are specified by a temporal extension of Birkhoff-von\nNeumann quantum logic. Their semantics is defined based on the design decision:\nthey will be used in verification of quantum circuits by simulation on\nclassical computers or human reasoning rather than by quantum physics\nexperiments (e.g. testing through measurements);\n  (iii) Algorithms for reachability analysis and model checking of quantum\ncircuits are developed based on contraction of tensor networks. We observe that\nmany optimisation techniques for computing relational products used in\nBDD-based model checking algorithms can be generalised for contracting tensor\nnetworks of quantum circuits.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:43:37 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ying", "Mingsheng", ""]]}, {"id": "2104.11469", "submitter": "Jan Philipp Thoma", "authors": "Jan Philipp Thoma, Christian Niesler, Dominic Funke, Gregor Leander,\n  Pierre Mayr, Nils Pohl, Lucas Davi, Tim G\\\"uneysu", "title": "ClepsydraCache -- Preventing Cache Attacks with Time-Based Evictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both the shift towards attacks on the microarchitectural CPU level and the\nongoing transition towards cloud computing and shared VM hosts have\nincreasingly drawn attention towards cache attacks. In these fields of\napplication, cache side-channels lay the cornerstone that is leveraged by\nattackers to exfiltrate secret information from the CPU microarchitecture. We\nbuild upon the observation that current cache side-channel attacks mostly\nexploit the architectural visibility of conflicting cache addresses. With\nClepsydraCache, we break away this foundation by unraveling the linkage between\ncache evictions and accesses to conflicting addresses. Our solution takes a new\napproach that assigns each cache entry a random time-to-live to reduce the\namount of cache conflicts. By making those conflicts unobservable to an\nattacker, ClepsydraCache efficiently protects against attacks like Prime+Probe\nand Flush+Reload. Furthermore, our solution is applicable to large last-level\ncaches which are the most common targets for cache attacks. We implement\nClepsydraCache using the Gem5 simulator and provide a proof-of-concept hardware\ndesign and simulation using 65-nm CMOS technology. ClepsydraCache matches the\nperformance of traditional cache architectures while improving the system\nsecurity against cache attacks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 08:36:49 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Thoma", "Jan Philipp", ""], ["Niesler", "Christian", ""], ["Funke", "Dominic", ""], ["Leander", "Gregor", ""], ["Mayr", "Pierre", ""], ["Pohl", "Nils", ""], ["Davi", "Lucas", ""], ["G\u00fcneysu", "Tim", ""]]}, {"id": "2104.11678", "submitter": "Johnathan Alsop", "authors": "Johnathan Alsop, Weon Taek Na, Matthew D. Sinclair, Samuel Grayson,\n  Sarita V. Adve", "title": "A Case for Fine-grain Coherence Specialization in Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware specialization is becoming a key enabler of energyefficient\nperformance. Future systems will be increasingly heterogeneous, integrating\nmultiple specialized and programmable accelerators, each with different memory\ndemands. Traditionally, communication between accelerators has been\ninefficient, typically orchestrated through explicit DMA transfers between\ndifferent address spaces. More recently, industry has proposed unified coherent\nmemory which enables implicit data movement and more data reuse, but often\nthese interfaces limit the coherence flexibility available to heterogeneous\nsystems. This paper demonstrates the benefits of fine-grained coherence\nspecialization for heterogeneous systems. We propose an architecture that\nenables low-complexity independent specialization of each individual coherence\nrequest in heterogeneous workloads by building upon a simple and flexible\nbaseline coherence interface, Spandex. We then describe how to optimize\nindividual memory requests to improve cache reuse and performance-critical\nmemory latency in emerging heterogeneous workloads. Collectively, our\ntechniques enable significant gains, reducing execution time by up to 61% or\nnetwork traffic by up to 99% while adding minimal complexity to the Spandex\nprotocol.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:14:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Alsop", "Johnathan", ""], ["Na", "Weon Taek", ""], ["Sinclair", "Matthew D.", ""], ["Grayson", "Samuel", ""], ["Adve", "Sarita V.", ""]]}, {"id": "2104.11725", "submitter": "Sinan Aksoy", "authors": "Sinan Aksoy, Stephen Young, Jesun Firoz, Roberto Gioiosa, Mark Raugas,\n  Juan Escobedo", "title": "SpectralFly: Ramanujan Graphs as Flexible and Efficient Interconnection\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, graph theoretic considerations have become increasingly\nimportant in the design of HPC interconnection topologies. One approach is to\nseek optimal or near-optimal families of graphs with respect to a particular\ngraph theoretic property, such as diameter. In this work, we consider\ntopologies which optimize the spectral gap. In particular, we study a novel HPC\ntopology, SpectralFly, designed around the Ramanujan graph construction of\nLubotzky, Phillips, and Sarnak (LPS). We show combinatorial properties, such as\ndiameter, bisection bandwidth, average path length, and resilience to link\nfailure, of SpectralFly topologies are better than, or comparable to, similarly\nconstrained DragonFly, SlimFly, and BundleFly topologies. Additionally, we\nsimulate the performance of SpectralFly topologies on a representative sample\nof physics-inspired HPC workloads using the Structure Simulation Toolkit\nMacroscale Element Library simulator and demonstrate considerable benefit to\nusing the LPS construction as the basis of the SpectralFly topology.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 17:21:55 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Aksoy", "Sinan", ""], ["Young", "Stephen", ""], ["Firoz", "Jesun", ""], ["Gioiosa", "Roberto", ""], ["Raugas", "Mark", ""], ["Escobedo", "Juan", ""]]}, {"id": "2104.12339", "submitter": "Liancheng Jia", "authors": "Liancheng Jia, Zizhang Luo, Liqiang Lu, Yun Liang", "title": "TensorLib: A Spatial Accelerator Generation Framework for Tensor Algebra", "comments": "accepted to DAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tensor algebra finds applications in various domains, and these applications,\nespecially when accelerated on spatial hardware accelerators, can deliver high\nperformance and low power. Spatial hardware accelerator exhibits complex design\nspace. Prior approaches based on manual implementation lead to low programming\nproductivity, rendering thorough design space exploration impossible. In this\npaper, we propose TensorLib, a framework for generating spatial hardware\naccelerator for tensor algebra applications. TensorLib is motivated by the\nobservation that, different dataflows share common hardware modules, which can\nbe reused across different designs. To build such a framework, TensorLib first\nuses Space-Time Transformation to explore different dataflows, which can\ncompactly represent the hardware dataflow using a simple transformation matrix.\nNext, we identify the common structures of different dataflows and build\nparameterized hardware module templates with Chisel. Our generation framework\ncan select the needed hardware modules for each dataflow, connect the modules\nusing a specified interconnection pattern, and automatically generate the\ncomplete hardware accelerator design. TensorLib remarkably improves the\nproductivity for the development and optimization of spatial hardware\narchitecture, providing a rich design space with trade-offs in performance,\narea, and power. Experiments show that TensorLib can automatically generate\nhardware designs with different dataflows and achieve 21\\% performance\nimprovement on FPGA compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 04:16:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jia", "Liancheng", ""], ["Luo", "Zizhang", ""], ["Lu", "Liqiang", ""], ["Liang", "Yun", ""]]}, {"id": "2104.12350", "submitter": "Varun Sharma", "authors": "Varun Sharma and Paul Chow", "title": "A PGAS Communication Library for Heterogeneous Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a heterogeneous communication library for clusters of\nprocessors and FPGAs. This library, Shoal, supports the Partitioned Global\nAddress Space (PGAS) memory model for applications. PGAS is a shared memory\nmodel for clusters that creates a distinction between local and remote memory\naccess. Through Shoal and its common application programming interface for\nhardware and software, applications can be more freely migrated to the optimal\nplatform and deployed onto dynamic cluster topologies.\n  The library is tested using a thorough suite of microbenchmarks to establish\nlatency and throughput performance. We also show an implementation of the\nJacobi iterative method that demonstrates the ease with which applications can\nbe moved between platforms to yield faster run times. Through this work, we\nhave demonstrated the feasibility of using a PGAS programming model for\nmulti-node heterogeneous platforms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 05:05:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sharma", "Varun", ""], ["Chow", "Paul", ""]]}, {"id": "2104.12760", "submitter": "Alexander Rucker", "authors": "Alexander Rucker, Matthew Vilim, Tian Zhao, Yaqi Zhang, Raghu\n  Prabhakar, Kunle Olukotun", "title": "Capstan: A Vector RDA for Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Capstan: a scalable, parallel-patterns-based,\nreconfigurable-dataflow accelerator (RDA) for sparse and dense tensor\napplications. Instead of designing for one application, we start with common\nsparse data formats, each of which supports multiple applications. Using a\ndeclarative programming model, Capstan supports application-independent sparse\niteration and memory primitives that can be mapped to vectorized,\nhigh-performance hardware. We optimize random-access sparse memories with\nconfigurable out-of-order execution to increase SRAM random-access throughput\nfrom 32% to 80%.\n  For a variety of sparse applications, Capstan with DDR4 memory is 22x faster\nthan a multi-core CPU baseline, while Capstan with HBM2 memory is 17x faster\nthan an Nvidia V100 GPU. For sparse applications that can be mapped to\nPlasticine, a recent dense RDA, Capstan is 7.6x to 365x faster and only 13%\nlarger.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:52:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rucker", "Alexander", ""], ["Vilim", "Matthew", ""], ["Zhao", "Tian", ""], ["Zhang", "Yaqi", ""], ["Prabhakar", "Raghu", ""], ["Olukotun", "Kunle", ""]]}, {"id": "2104.13671", "submitter": "Pritam Majumder", "authors": "Pritam Majumder, Jiayi Huang, Sungkeun Kim, Abdullah Muzahid, Dylan\n  Siegers, Chia-Che Tsai, and Eun Jung Kim", "title": "Continual Learning Approach for Improving the Data and Computation\n  Mapping in Near-Memory Processing System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.NI cs.OS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The resurgence of near-memory processing (NMP) with the advent of big data\nhas shifted the computation paradigm from processor-centric to memory-centric\ncomputing. To meet the bandwidth and capacity demands of memory-centric\ncomputing, 3D memory has been adopted to form a scalable memory-cube network.\nAlong with NMP and memory system development, the mapping for placing data and\nguiding computation in the memory-cube network has become crucial in driving\nthe performance improvement in NMP. However, it is very challenging to design a\nuniversal optimal mapping for all applications due to unique application\nbehavior and intractable decision space. In this paper, we propose an\nartificially intelligent memory mapping scheme, AIMM, that optimizes data\nplacement and resource utilization through page and computation remapping. Our\nproposed technique involves continuously evaluating and learning the impact of\nmapping decisions on system performance for any application. AIMM uses a neural\nnetwork to achieve a near-optimal mapping during execution, trained using a\nreinforcement learning algorithm that is known to be effective for exploring a\nvast design space. We also provide a detailed AIMM hardware design that can be\nadopted as a plugin module for various NMP systems. Our experimental evaluation\nshows that AIMM improves the baseline NMP performance in single and multiple\nprogram scenario by up to 70% and 50%, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:50:35 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Majumder", "Pritam", ""], ["Huang", "Jiayi", ""], ["Kim", "Sungkeun", ""], ["Muzahid", "Abdullah", ""], ["Siegers", "Dylan", ""], ["Tsai", "Chia-Che", ""], ["Kim", "Eun Jung", ""]]}, {"id": "2104.13997", "submitter": "Sheng-Chun Kao", "authors": "Sheng-Chun Kao, Tushar Krishna", "title": "Domain-specific Genetic Algorithm for Multi-tenant DNNAccelerator\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Learning continues to drive a variety of applications in datacenters\nand HPC, there is a growing trend towards building large accelerators with\nseveral sub-accelerator cores/chiplets. This work looks at the problem of\nsupporting multi-tenancy on such accelerators. In particular, we focus on the\nproblem of mapping layers from several DNNs simultaneously on an accelerator.\nGiven the extremely large search space, we formulate the search as an\noptimization problem and develop a specialized genetic algorithm called G#\nwithcustom operators to enable structured sample-efficient exploration. We\nquantitatively compare G# with several common heuristics, state-of-the-art\noptimization methods, and reinforcement learning methods across different\naccelerator set-tings (large/small accelerators) and different sub-accelerator\nconfigurations (homogeneous/heterogeneous), and observeG# can consistently find\nbetter solutions. Further, to enable real-time scheduling, we also demonstrate\na method to generalize the learnt schedules and transfer them to the next batch\nof jobs, reducing schedule compute time to near zero.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:57:55 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:41:36 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kao", "Sheng-Chun", ""], ["Krishna", "Tushar", ""]]}, {"id": "2104.14124", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Motoki Yoshinaga, Hongxing Gao, Wei Tao, Dongchao Wen,\n  Junjie Liu, Kinya Osa, Masami Kato", "title": "Condensation-Net: Memory-Efficient Network Architecture with\n  Cross-Channel Pooling Layers and Virtual Feature Maps", "comments": "Camera-ready version for CVPR 2019 workshop (Embedded Vision\n  Workshop)", "journal-ref": "2019 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "doi": "10.1109/CVPRW.2019.00024", "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \"Lightweight convolutional neural networks\" is an important research topic in\nthe field of embedded vision. To implement image recognition tasks on a\nresource-limited hardware platform, it is necessary to reduce the memory size\nand the computational cost. The contribution of this paper is stated as\nfollows. First, we propose an algorithm to process a specific network\narchitecture (Condensation-Net) without increasing the maximum memory storage\nfor feature maps. The architecture for virtual feature maps saves 26.5% of\nmemory bandwidth by calculating the results of cross-channel pooling before\nstoring the feature map into the memory. Second, we show that cross-channel\npooling can improve the accuracy of object detection tasks, such as face\ndetection, because it increases the number of filter weights. Compared with\nTiny-YOLOv2, the improvement of accuracy is 2.0% for quantized networks and\n1.5% for full-precision networks when the false-positive rate is 0.1. Last but\nnot the least, the analysis results show that the overhead to support the\ncross-channel pooling with the proposed hardware architecture is negligible\nsmall. The extra memory cost to support Condensation-Net is 0.2% of the total\nsize, and the extra gate count is only 1.0% of the total size.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:44:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Yoshinaga", "Motoki", ""], ["Gao", "Hongxing", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Liu", "Junjie", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14125", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Wei Tao, Deyu Wang, Dongchao Wen, Kinya Osa, Masami Kato", "title": "Hardware Architecture of Embedded Inference Accelerator and Analysis of\n  Algorithms for Depthwise and Large-Kernel Convolutions", "comments": "Camera-ready version for ECCV 2020 workshop (Embedded Vision\n  Workshop)", "journal-ref": "ECCV 2020 Workshops, LNCS 12539, pp. 3-17, 2020", "doi": "10.1007/978-3-030-68238-5_1", "report-no": null, "categories": "cs.CV cs.AR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to handle modern convolutional neural networks (CNNs) efficiently, a\nhardware architecture of CNN inference accelerator is proposed to handle\ndepthwise convolutions and regular convolutions, which are both essential\nbuilding blocks for embedded-computer-vision algorithms. Different from related\nworks, the proposed architecture can support filter kernels with different\nsizes with high flexibility since it does not require extra costs for\nintra-kernel parallelism, and it can generate convolution results faster than\nthe architecture of the related works. The experimental results show the\nimportance of supporting depthwise convolutions and dilated convolutions with\nthe proposed hardware architecture. In addition to depthwise convolutions with\nlarge-kernels, a new structure called DDC layer, which includes the combination\nof depthwise convolutions and dilated convolutions, is also analyzed in this\npaper. For face detection, the computational costs decrease by 30%, and the\nmodel size decreases by 20% when the DDC layers are applied to the network. For\nimage classification, the accuracy is increased by 1% by simply replacing $3\n\\times 3$ filters with $5 \\times 5$ filters in depthwise convolutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:45:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Tao", "Wei", ""], ["Wang", "Deyu", ""], ["Wen", "Dongchao", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14126", "submitter": "Tse-Wei Chen", "authors": "Tse-Wei Chen, Deyu Wang, Wei Tao, Dongchao Wen, Lingxiao Yin, Tadayuki\n  Ito, Kinya Osa, Masami Kato", "title": "CASSOD-Net: Cascaded and Separable Structures of Dilated Convolution for\n  Embedded Vision Systems and Applications", "comments": "Camera-ready version for CVPR 2021 workshop (Embedded Vision\n  Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of view (FOV) of convolutional neural networks is highly related to\nthe accuracy of inference. Dilated convolutions are known as an effective\nsolution to the problems which require large FOVs. However, for general-purpose\nhardware or dedicated hardware, it usually takes extra time to handle dilated\nconvolutions compared with standard convolutions. In this paper, we propose a\nnetwork module, Cascaded and Separable Structure of Dilated (CASSOD)\nConvolution, and a special hardware system to handle the CASSOD networks\nefficiently. A CASSOD-Net includes multiple cascaded $2 \\times 2$ dilated\nfilters, which can be used to replace the traditional $3 \\times 3$ dilated\nfilters without decreasing the accuracy of inference. Two example applications,\nface detection and image segmentation, are tested with dilated convolutions and\nthe proposed CASSOD modules. The new network for face detection achieves higher\naccuracy than the previous work with only 47% of filter weights in the dilated\nconvolution layers of the context module. Moreover, the proposed hardware\nsystem can accelerate the computations of dilated convolutions, and it is 2.78\ntimes faster than traditional hardware systems when the filter size is $3\n\\times 3$.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 05:45:24 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chen", "Tse-Wei", ""], ["Wang", "Deyu", ""], ["Tao", "Wei", ""], ["Wen", "Dongchao", ""], ["Yin", "Lingxiao", ""], ["Ito", "Tadayuki", ""], ["Osa", "Kinya", ""], ["Kato", "Masami", ""]]}, {"id": "2104.14155", "submitter": "Kathleen Feng", "authors": "Jackson Melchert, Kathleen Feng, Caleb Donovick, Ross Daly, Clark\n  Barrett, Mark Horowitz, Pat Hanrahan, Priyanka Raina", "title": "Automated Design Space Exploration of CGRA Processing Element\n  Architectures using Frequent Subgraph Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of a coarse-grained reconfigurable array (CGRA) processing\nelement (PE) has a significant effect on the performance and energy efficiency\nof an application running on the CGRA. This paper presents an automated\napproach for generating specialized PE architectures for an application or an\napplication domain. Frequent subgraphs mined from a set of applications are\nmerged to form a PE architecture specialized to that application domain. For\nthe image processing and machine learning domains, we generate specialized PEs\nthat are up to 10.5x more energy efficient and consume 9.1x less area than a\nbaseline PE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:32:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Melchert", "Jackson", ""], ["Feng", "Kathleen", ""], ["Donovick", "Caleb", ""], ["Daly", "Ross", ""], ["Barrett", "Clark", ""], ["Horowitz", "Mark", ""], ["Hanrahan", "Pat", ""], ["Raina", "Priyanka", ""]]}, {"id": "2104.14594", "submitter": "Kaveh Akbarzadeh-Sherbaf", "authors": "Kaveh Akbarzadeh-Sherbaf, Mikaeel Bahmani, Danial Ghiaseddin, Saeed\n  Safari, Abdol-Hossein Vahabie", "title": "A Novel Approximate Hamming Weight Computing for Spiking Neural\n  Networks: an FPGA Friendly Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hamming weights of sparse and long binary vectors are important modules in\nmany scientific applications, particularly in spiking neural networks that are\nof our interest. To improve both area and latency of their FPGA\nimplementations, we propose a method inspired from synaptic transmission\nfailure for exploiting FPGA lookup tables to compress long input vectors. To\nevaluate the effectiveness of this approach, we count the number of `1's of the\ncompressed vector using a simple linear adder. We classify the compressors into\nshallow ones with up to two levels of lookup tables and deep ones with more\nthan two levels. The architecture generated by this approach shows up to 82%\nand 35% reductions for different configurations of shallow compressors in area\nand latency respectively. Moreover, our simulation results show that\ncalculating the Hamming weight of a 1024-bit vector of a spiking neural network\nby the use of only deep compressors preserves the chaotic behavior of the\nnetwork while slightly impacts on the learning performance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:27:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Akbarzadeh-Sherbaf", "Kaveh", ""], ["Bahmani", "Mikaeel", ""], ["Ghiaseddin", "Danial", ""], ["Safari", "Saeed", ""], ["Vahabie", "Abdol-Hossein", ""]]}, {"id": "2104.14885", "submitter": "Dimitrios Antoniadis", "authors": "Dimitrios Antoniadis, Peilong Feng, Andrea Mifsud, Timothy G.\n  Constandinou", "title": "Open-Source Memory Compiler for Automatic RRAM Generation and\n  Verification", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of open-source memory compilers in academia typically causes\nsignificant delays in research and design implementations. This paper presents\nan open-source memory compiler that is irectly integrated within the Cadence\nVirtuoso environment using physical verification tools provided by Mentor\nGraphics (Calibre). It facilitates the entire memory generation process from\nnetlist generation to layout implementation, and physical implementation\nverification. To the best of our knowledge, this is the first open-source\nmemory compiler that has been developed specifically to automate Resistive\nRandom Access Memory (RRAM) generation. RRAM holds the promise of achieving\nhigh speed, high density and non-volatility. A novel RRAM architecture,\nadditionally is proposed, and a number of generated RRAM arrays are evaluated\nto identify their worst case control line parasitics and worst case settling\ntime across the memristors of their cells. The total capacitance of lines SEL,\nN and P is 5.83 fF/cell, 3.31 fF/cell and 2.48 fF/cell respectively, while the\ntotal calculated resistance for SEL is 1.28 Ohm/cell and 0.14 Ohm/cell for both\nN and P lines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:15:55 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Antoniadis", "Dimitrios", ""], ["Feng", "Peilong", ""], ["Mifsud", "Andrea", ""], ["Constandinou", "Timothy G.", ""]]}, {"id": "2104.15052", "submitter": "Hongzuo Xu", "authors": "Zhiyue Wu, Hongzuo Xu, Guansong Pang, Fengyuan Yu, Yijie Wang, Songlei\n  Jian, Yongjun Wang", "title": "DRAM Failure Prediction in AIOps: Empirical Evaluation, Challenges and\n  Opportunities", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DRAM failure prediction is a vital task in AIOps, which is crucial to\nmaintain the reliability and sustainable service of large-scale data centers.\nHowever, limited work has been done on DRAM failure prediction mainly due to\nthe lack of public available datasets. This paper presents a comprehensive\nempirical evaluation of diverse machine learning techniques for DRAM failure\nprediction using a large-scale multi-source dataset, including more than three\nmillions of records of kernel, address, and mcelog data, provided by Alibaba\nCloud through PAKDD 2021 competition. Particularly, we first formulate the\nproblem as a multi-class classification task and exhaustively evaluate seven\npopular/state-of-the-art classifiers on both the individual and multiple data\nsources. We then formulate the problem as an unsupervised anomaly detection\ntask and evaluate three state-of-the-art anomaly detectors. Further, based on\nthe empirical results and our experience of attending this competition, we\ndiscuss major challenges and present future research opportunities in this\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:20:22 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 02:59:45 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wu", "Zhiyue", ""], ["Xu", "Hongzuo", ""], ["Pang", "Guansong", ""], ["Yu", "Fengyuan", ""], ["Wang", "Yijie", ""], ["Jian", "Songlei", ""], ["Wang", "Yongjun", ""]]}]