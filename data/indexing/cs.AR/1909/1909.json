[{"id": "1909.00553", "submitter": "Prashant Jayaprakash Nair", "authors": "Seokin Hong, Bulent Abali, Alper Buyuktosunoglu, Michael B. Healy, and\n  Prashant J. Nair", "title": "Touch\\'e: Towards Ideal and Efficient Cache Compression By Mitigating\n  Tag Area Overheads", "comments": "Keywords: Compression, Caches, Tag Array, Data Array, Hashing", "journal-ref": "Proceedings of the 52nd Annual IEEE/ACM International Symposium on\n  Microarchitecture, October 2019, Pages 453-465", "doi": "10.1145/3352460.3358281", "report-no": null, "categories": "cs.AR cs.DC cs.PF cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression is seen as a simple technique to increase the effective cache\ncapacity. Unfortunately, compression techniques either incur tag area overheads\nor restrict data placement to only include neighboring compressed cache blocks\nto mitigate tag area overheads. Ideally, we should be able to place arbitrary\ncompressed cache blocks without any placement restrictions and tag area\noverheads.\n  This paper proposes Touch\\'e, a framework that enables storing multiple\narbitrary compressed cache blocks within a physical cacheline without any tag\narea overheads. The Touch\\'e framework consists of three components. The first\ncomponent, called the ``Signature'' (SIGN) engine, creates shortened signatures\nfrom the tag addresses of compressed blocks. Due to this, the SIGN engine can\nstore multiple signatures in each tag entry. On a cache access, the physical\ncacheline is accessed only if there is a signature match (which has a\nnegligible probability of false positive). The second component, called the\n``Tag Appended Data'' (TADA) mechanism, stores the full tag addresses with\ndata. TADA enables Touch\\'e to detect false positive signature matches by\nensuring that the actual tag address is available for comparison. The third\ncomponent, called the ``Superblock Marker'' (SMARK) mechanism, uses a unique\nmarker in the tag entry to indicate the occurrence of compressed cache blocks\nfrom neighboring physical addresses in the same cacheline. Touch\\'e is\ncompletely hardware-based and achieves an average speedup of 12\\% (ideal 13\\%)\nwhen compared to an uncompressed baseline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:39:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hong", "Seokin", ""], ["Abali", "Bulent", ""], ["Buyuktosunoglu", "Alper", ""], ["Healy", "Michael B.", ""], ["Nair", "Prashant J.", ""]]}, {"id": "1909.00557", "submitter": "Ye Yu", "authors": "Ye Yu, and Niraj K. Jha", "title": "SPRING: A Sparsity-Aware Reduced-Precision Monolithic 3D CNN Accelerator\n  Architecture for Training and Inference", "comments": null, "journal-ref": null, "doi": "10.1109/TETC.2020.3003328", "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs outperform traditional machine learning algorithms across a wide range\nof applications. However, their computational complexity makes it necessary to\ndesign efficient hardware accelerators. Most CNN accelerators focus on\nexploring dataflow styles that exploit computational parallelism. However,\npotential performance speedup from sparsity has not been adequately addressed.\nThe computation and memory footprint of CNNs can be significantly reduced if\nsparsity is exploited in network evaluations. To take advantage of sparsity,\nsome accelerator designs explore sparsity encoding and evaluation on CNN\naccelerators. However, sparsity encoding is just performed on activation or\nweight and only in inference. It has been shown that activation and weight also\nhave high sparsity levels during training. Hence, sparsity-aware computation\nshould also be considered in training. To further improve performance and\nenergy efficiency, some accelerators evaluate CNNs with limited precision.\nHowever, this is limited to the inference since reduced precision sacrifices\nnetwork accuracy if used in training. In addition, CNN evaluation is usually\nmemory-intensive, especially in training. In this paper, we propose SPRING, a\nSParsity-aware Reduced-precision Monolithic 3D CNN accelerator for trainING and\ninference. SPRING supports both CNN training and inference. It uses a binary\nmask scheme to encode sparsities in activation and weight. It uses the\nstochastic rounding algorithm to train CNNs with reduced precision without\naccuracy loss. To alleviate the memory bottleneck in CNN evaluation, especially\nin training, SPRING uses an efficient monolithic 3D NVM interface to increase\nmemory bandwidth. Compared to GTX 1080 Ti, SPRING achieves 15.6X, 4.2X and\n66.0X improvements in performance, power reduction, and energy efficiency,\nrespectively, for CNN training, and 15.5X, 4.5X and 69.1X improvements for\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:59:54 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 01:29:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yu", "Ye", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1909.04551", "submitter": "Hyunbin Park", "authors": "Hyunbin Park, Dohyun Kim, and Shiho Kim", "title": "TMA: Tera-MACs/W Neural Hardware Inference Accelerator with a\n  Multiplier-less Massive Parallel Processor", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally intensive Inference tasks of Deep neural networks have\nenforced revolution of new accelerator architecture to reduce power consumption\nas well as latency. The key figure of merit in hardware inference accelerators\nis the number of multiply-and-accumulation operations per watt (MACs/W), where,\nthe state-of-the-arts MACs/W remains several hundreds Giga-MACs/W. We propose a\nTera-MACS/W neural hardware inference Accelerator (TMA) with 8-bit activations\nand scalable integer weights less than 1-byte. The architectures main feature\nis configurable neural processing element for matrix-vector operations. The\nproposed neural processing element has Multiplier-less Massive Parallel\nProcessor to work without any multiplications, which makes it attractive for\nenergy efficient high-performance neural network applications. We benchmark our\nsystems latency, power, and performance using Alexnet trained on ImageNet.\nFinally, we compared our accelerators throughput and power consumption to the\nprior works. The proposed accelerator outperforms the state of the art in terms\nof energy and area achieving 2.3 TMACS/W@1.0 V, 65 nm CMOS technology.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 14:18:13 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Park", "Hyunbin", ""], ["Kim", "Dohyun", ""], ["Kim", "Shiho", ""]]}, {"id": "1909.04554", "submitter": "Jan Moritz Joseph", "authors": "Jan Moritz Joseph, Lennart Bamberg, Dominik Ermel, Behnam Razi\n  Perjikolaei, Anna Drewes, Alberto Garc\\'ia-Oritz, Thilo Pionteck", "title": "NoCs in Heterogeneous 3D SoCs: Co-Design of Routing Strategies and\n  Microarchitectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous 3D System-on-Chips (3D SoCs) are the most promising design\nparadigm to combine sensing and computing within a single chip. A special\ncharacteristic of communication networks in heterogeneous 3D SoCs is the\nvarying latency and throughput in each layer. As shown in this work, this\nvariance drastically degrades the network performance. We contribute a\nco-design of routing algorithms and router microarchitecture that allows to\novercome these performance limitations. We analyze the challenges of\nheterogeneity: Technology-aware models are proposed for communication and\nthereby identify layers in which packets are transmitted slower. The\ncommunication models are precise for latency and throughput under zero load.\nThe technology model has an area error and a timing error of less than 7.4% for\nvarious commercial technologies from 90 to 28nm. Second, we demonstrate how to\novercome limitations of heterogeneity by proposing two novel routing algorithms\ncalled Z+(XY)Z- and ZXYZ that enhance latency by up to 6.5x compared to\nconventional dimension order routing. Furthermore, we propose a high\nvertical-throughput router microarchitecture that is adjusted to the routing\nalgorithms and that fully overcomes the limitations of slower layers. We\nachieve an increased throughput of 2 to 4x compared to a conventional router.\nThereby, the dynamic power of routers is reduced by up to 41.1% and we achieve\nimproved flit latency of up to 2.26x at small total router area costs between\n2.1% and 10.4% for realistic technologies and application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:05:53 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Joseph", "Jan Moritz", ""], ["Bamberg", "Lennart", ""], ["Ermel", "Dominik", ""], ["Perjikolaei", "Behnam Razi", ""], ["Drewes", "Anna", ""], ["Garc\u00eda-Oritz", "Alberto", ""], ["Pionteck", "Thilo", ""]]}, {"id": "1909.04841", "submitter": "Mohammadkazem Taram", "authors": "Mohammadkazem Taram, Ashish Venkat, Dean Tullsen", "title": "Packet Chasing: Spying on Network Packets over a Cache Side-Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Packet Chasing, an attack on the network that does not\nrequire access to the network, and works regardless of the privilege level of\nthe process receiving the packets. A spy process can easily probe and discover\nthe exact cache location of each buffer used by the network driver. Even more\nuseful, it can discover the exact sequence in which those buffers are used to\nreceive packets. This then enables packet frequency and packet sizes to be\nmonitored through cache side channels. This allows both covert channels between\na sender and a remote spy with no access to the network, as well as direct\nattacks that can identify, among other things, the web page access patterns of\na victim on the network. In addition to identifying the potential attack, this\nwork proposes a software-based short-term mitigation as well as a light-weight,\nadaptive, cache partitioning mitigation that blocks the interference of I/O and\nCPU requests in the last-level cache.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:03:14 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 03:29:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Taram", "Mohammadkazem", ""], ["Venkat", "Ashish", ""], ["Tullsen", "Dean", ""]]}, {"id": "1909.05009", "submitter": "Michaela Blott", "authors": "Michaela Blott, Lisa Halder, Miriam Leeser, Linda Doyle", "title": "QuTiBench: Benchmarking Neural Networks on Heterogeneous Hardware", "comments": null, "journal-ref": "ACM JETC 2019", "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks have become one of the most successful universal machine\nlearning algorithms. They play a key role in enabling machine vision and speech\nrecognition for example. Their computational complexity is enormous and comes\nalong with equally challenging memory requirements, which limits deployment in\nparticular within energy constrained, embedded environments. In order to\naddress these implementation challenges, a broad spectrum of new customized and\nheterogeneous hardware architectures have emerged, often accompanied with\nco-designed algorithms to extract maximum benefit out of the hardware.\nFurthermore, numerous optimization techniques are being explored for neural\nnetworks to reduce compute and memory requirements while maintaining accuracy.\nThis results in an abundance of algorithmic and architectural choices, some of\nwhich fit specific use cases better than others.\n  For system level designers, there is currently no good way to compare the\nvariety of hardware, algorithm and optimization options. While there are many\nbenchmarking efforts in this field, they cover only subsections of the embedded\ndesign space. None of the existing benchmarks support essential algorithmic\noptimizations such as quantization, an important technique to stay on chip, or\nspecialized heterogeneous hardware architectures. We propose a novel benchmark\nsuite, QuTiBench, that addresses this need. QuTiBench is a novel multi-tiered\nbenchmarking methodology that supports algorithmic optimizations such as\nquantization and helps system developers understand the benefits and\nlimitations of these novel compute architectures in regard to specific neural\nnetworks and will help drive future innovation. We invite the community to\ncontribute to QuTiBench in order to support the full spectrum of choices in\nimplementing machine learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:37:29 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 16:34:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Blott", "Michaela", ""], ["Halder", "Lisa", ""], ["Leeser", "Miriam", ""], ["Doyle", "Linda", ""]]}, {"id": "1909.06559", "submitter": "Taposh Roy", "authors": "Taposh Dutta-Roy", "title": "Instructional Level Parallelism", "comments": "Branch Prediction, Exceptions, Instructional level parallelism,\n  pipelining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a review of the developments in Instruction level parallelism.\nIt takes into account all the changes made in speeding up the execution. The\nvarious drawbacks and dependencies due to pipelining are discussed and various\nsolutions to overcome them are also incorporated. It goes ahead in the last\nsection to explain where is the new research leading us.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 08:36:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Dutta-Roy", "Taposh", ""]]}, {"id": "1909.06892", "submitter": "Shubham Jain", "authors": "Shubham Jain, Sumeet Kumar Gupta, Anand Raghunathan", "title": "TiM-DNN: Ternary in-Memory accelerator for Deep Neural Networks", "comments": "12 pages, 18 figures, Accepted in IEEE Transactions on Very Large\n  Scale Integration (VLSI) Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of lower precision has emerged as a popular technique to optimize the\ncompute and storage requirements of complex Deep Neural Networks (DNNs). In the\nquest for lower precision, recent studies have shown that ternary DNNs (which\nrepresent weights and activations by signed ternary values) represent a\npromising sweet spot, achieving accuracy close to full-precision networks on\ncomplex tasks. We propose TiM-DNN, a programmable in-memory accelerator that is\nspecifically designed to execute ternary DNNs. TiM-DNN supports various ternary\nrepresentations including unweighted {-1,0,1}, symmetric weighted {-a,0,a}, and\nasymmetric weighted {-a,0,b} ternary systems. The building blocks of TiM-DNN\nare TiM tiles -- specialized memory arrays that perform massively parallel\nsigned ternary vector-matrix multiplications with a single access. TiM tiles\nare in turn composed of Ternary Processing Cells (TPCs), bit-cells that\nfunction as both ternary storage units and signed ternary multiplication units.\nWe evaluate an implementation of TiM-DNN in 32nm technology using an\narchitectural simulator calibrated with SPICE simulations and RTL synthesis. We\nevaluate TiM-DNN across a suite of state-of-the-art DNN benchmarks including\nboth deep convolutional and recurrent neural networks. A 32-tile instance of\nTiM-DNN achieves a peak performance of 114 TOPs/s, consumes 0.9W power, and\noccupies 1.96mm2 chip area, representing a 300X and 388X improvement in TOPS/W\nand TOPS/mm2, respectively, compared to an NVIDIA Tesla V100 GPU. In comparison\nto specialized DNN accelerators, TiM-DNN achieves 55X-240X and 160X-291X\nimprovement in TOPS/W and TOPS/mm2, respectively. Finally, when compared to a\nwell-optimized near-memory accelerator for ternary DNNs, TiM-DNN demonstrates\n3.9x-4.7x improvement in system-level energy and 3.2x-4.2x speedup,\nunderscoring the potential of in-memory computing for ternary DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 21:43:19 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 03:59:26 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 02:42:18 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Jain", "Shubham", ""], ["Gupta", "Sumeet Kumar", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1909.07973", "submitter": "Xiaoyu Yu", "authors": "Xiaoyu Yu, Yuwei Wang, Jie Miao, Ephrem Wu, Heng Zhang, Yu Meng, Bo\n  Zhang, Biao Min, Dewei Chen, Jianlin Gao", "title": "A Data-Center FPGA Acceleration Platform for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/FPL.2019.00032", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensive computation is entering data centers with multiple workloads of\ndeep learning. To balance the compute efficiency, performance, and total cost\nof ownership (TCO), the use of a field-programmable gate array (FPGA) with\nreconfigurable logic provides an acceptable acceleration capacity and is\ncompatible with diverse computation-sensitive tasks in the cloud. In this\npaper, we develop an FPGA acceleration platform that leverages a unified\nframework architecture for general-purpose convolutional neural network (CNN)\ninference acceleration at a data center. To overcome the computation bound,\n4,096 DSPs are assembled and shaped as supertile units (SUs) for different\ntypes of convolution, which provide up to 4.2 TOP/s 16-bit fixed-point\nperformance at 500 MHz. The interleaved-task-dispatching method is proposed to\nmap the computation across the SUs, and the memory bound is solved by a\ndispatching-assembling buffering model and broadcast caches. For various\nnon-convolution operators, a filter processing unit is designed for\ngeneral-purpose filter-like/pointwise operators. In the experiment, the\nperformances of CNN models running on server-class CPUs, a GPU, and an FPGA are\ncompared. The results show that our design achieves the best FPGA peak\nperformance and a throughput at the same level as that of the state-of-the-art\nGPU in data centers, with more than 50 times lower latency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 03:32:21 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Yu", "Xiaoyu", ""], ["Wang", "Yuwei", ""], ["Miao", "Jie", ""], ["Wu", "Ephrem", ""], ["Zhang", "Heng", ""], ["Meng", "Yu", ""], ["Zhang", "Bo", ""], ["Min", "Biao", ""], ["Chen", "Dewei", ""], ["Gao", "Jianlin", ""]]}, {"id": "1909.09731", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi and Kim Laine and Blake Pelton and Wei Dai", "title": "HEAX: An Architecture for Computing on Encrypted Data", "comments": "To appear in proceedings of ACM ASPLOS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in cloud computing, concerns surrounding data\nprivacy, security, and confidentiality also have been increased significantly.\nNot only cloud providers are susceptible to internal and external hacks, but\nalso in some scenarios, data owners cannot outsource the computation due to\nprivacy laws such as GDPR, HIPAA, or CCPA. Fully Homomorphic Encryption (FHE)\nis a groundbreaking invention in cryptography that, unlike traditional\ncryptosystems, enables computation on encrypted data without ever decrypting\nit. However, the most critical obstacle in deploying FHE at large-scale is the\nenormous computation overhead.\n  In this paper, we present HEAX, a novel hardware architecture for FHE that\nachieves unprecedented performance improvement. HEAX leverages multiple levels\nof parallelism, ranging from ciphertext-level to fine-grained modular\narithmetic level. Our first contribution is a new highly-parallelizable\narchitecture for number-theoretic transform (NTT) which can be of independent\ninterest as NTT is frequently used in many lattice-based cryptography systems.\nBuilding on top of NTT engine, we design a novel architecture for computation\non homomorphically encrypted data. We also introduce several techniques to\nenable an end-to-end, fully pipelined design as well as reducing on-chip memory\nconsumption. Our implementation on reconfigurable hardware demonstrates\n164-268x performance improvement for a wide range of FHE parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 22:27:06 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 21:17:05 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Laine", "Kim", ""], ["Pelton", "Blake", ""], ["Dai", "Wei", ""]]}, {"id": "1909.10154", "submitter": "Taposh Dutta Roy", "authors": "Taposh Dutta Roy", "title": "Implementation of Goldschmidt's Algorithm with hardware reduction", "comments": "Goldschmidt's Algorithm, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Division algorithms have been developed to reduce latency and to improve the\nefficiency of the processors. Floating point division is considered as a high\nlatency operation. This papers looks into one such division algorithm, examines\nthe hardware block diagram and suggests an alternative path which may be cost\neffective.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 04:40:37 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Roy", "Taposh Dutta", ""]]}, {"id": "1909.12195", "submitter": "Victor Eijkhout", "authors": "Victor Eijkhout, Margaret Myers, John McCalpin", "title": "Appearances of the Birthday Paradox in High Performance Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an elementary statistical analysis of two High Performance Computing\nissues, processor cache mapping and network port mapping. In both cases we find\nthat, as in the birthday paradox, random assignment leads to more frequent\ncoincidences than one expects a priori. Since these correspond to contention\nfor limited resources, this phenomenon has important consequences for\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 15:35:28 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Eijkhout", "Victor", ""], ["Myers", "Margaret", ""], ["McCalpin", "John", ""]]}, {"id": "1909.12221", "submitter": "Aditya Kamath", "authors": "Aditya K Kamath, Leslie Monis, A Tarun Karthik, and Basavaraj Talawar", "title": "Storage Class Memory: Principles, Problems, and Possibilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage Class Memory (SCM) is a class of memory technology which has recently\nbecome viable for use. Their namearises from the fact that they exhibit\nnon-volatility of data, similar to secondary storage while also having\nlatencies comparable toprimary memory and byte-addressibility. In this area,\nPhase Change Memory (PCM), Spin-Transfer-Torque Random Access Memory(STT-RAM),\nand Resistive RAM (ReRAM) have emerged as the major contenders for commercial\nand industrial use. In this paper, wedescribe how these memory types function,\nwhile highlighting the problems of endurance and performance that these memory\ntypesface. We also discuss the future possibilities of Multi-Level Cells\n(MLCs), as well as how SCM can be used to construct accelerators.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:20:19 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kamath", "Aditya K", ""], ["Monis", "Leslie", ""], ["Karthik", "A Tarun", ""], ["Talawar", "Basavaraj", ""]]}, {"id": "1909.12302", "submitter": "Kartik Ramkrishnan", "authors": "Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, Pen Chung Yew", "title": "New Attacks and Defenses for Randomized Caches", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last level cache is vulnerable to timing based side channel attacks\nbecause it is shared by the attacker and the victim processes even if they are\nlocated on different cores. These timing attacks evict the victim cache lines\nusing small conflict groups(SCG), and monitor the cache to observe when the\nvictim uses these cache lines again. A conflict group is a collection of cache\nlines which will evict the target cache line. Randomization is often used by\ndefenses to prevent creation of SCGs.\n  We introduce new attacks to demonstrate that the current randomization\nschemes require an extremely high refresh rate to be secure, on average a 15\\%\nperformance overhead, and upto 50\\% in the worst case. Next, we propose a new\nrandomization strategy using an indirection table, which mitigates this issue.\nAddresses of cache lines are encrypted and used to lookup the indirection table\nentry. Each indirection table entry stores a mapping to a randomly chosen cache\nset. The cache line is placed into this randomly chosen set. The encryption key\nchanges upto 50x faster than CEASER's default rate, by using evictions to\ntrigger the re-randomization. Instead of moving cache lines, this mechanism\nre-randomizes one iTable entry at a time, whenever the cache lines\ncorresponding to the iTable entry are naturally evicted. Thus, the miss rate is\nnot much worse than the baseline.\n  We quantitatively show that our scheme does almost as well as a fully\nassociative cache to defend against these attacks. We also demonstrate new\nattacks that target the iTable by oversubscribing its entries, and\nquantitatively show that our scheme is resilient against new attacks for\ntrillions of years. We estimate low area ( < 7\\%) and power overhead compared\nto a baseline inclusive last-level cache. Lastly, we evaluate a low performance\noverhead (<4%) using the SPECrate 2017 and PARSEC 3.0 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 01:54:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Ramkrishnan", "Kartik", ""], ["Zhai", "Antonia", ""], ["McCamant", "Stephen", ""], ["Yew", "Pen Chung", ""]]}, {"id": "1909.12373", "submitter": "Lizhong Chen", "authors": "Drew D. Penney and Lizhong Chen", "title": "A Survey of Machine Learning Applied to Computer Architecture Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has enabled significant benefits in diverse fields, but,\nwith a few exceptions, has had limited impact on computer architecture. Recent\nwork, however, has explored broader applicability for design, optimization, and\nsimulation. Notably, machine learning based strategies often surpass prior\nstate-of-the-art analytical, heuristic, and human-expert approaches. This paper\nreviews machine learning applied system-wide to simulation and run-time\noptimization, and in many individual components, including memory systems,\nbranch predictors, networks-on-chip, and GPUs. The paper further analyzes\ncurrent practice to highlight useful design strategies and identify areas for\nfuture work, based on optimized implementation strategies, opportune extensions\nto existing work, and ambitious long term possibilities. Taken together, these\nstrategies and techniques present a promising future for increasingly automated\narchitectural design.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:23:46 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Penney", "Drew D.", ""], ["Chen", "Lizhong", ""]]}, {"id": "1909.13168", "submitter": "Sandeep Srinivasan", "authors": "William Hughes, Sandeep Srinivasan, Rohit Suvarna, Maithilee Kulkarni", "title": "Optimizing Design Verification using Machine Learning: Doing better than\n  Random", "comments": "9 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As integrated circuits have become progressively more complex, constrained\nrandom stimulus has become ubiquitous as a means of stimulating a designs\nfunctionality and ensuring it fully meets expectations. In theory, random\nstimulus allows all possible combinations to be exercised given enough time,\nbut in practice with highly complex designs a purely random approach will have\ndifficulty in exercising all possible combinations in a timely fashion. As a\nresult it is often necessary to steer the Design Verification (DV) environment\nto generate hard to hit combinations. The resulting constrained-random approach\nis powerful but often relies on extensive human expertise to guide the DV\nenvironment in order to fully exercise the design. As designs become more\ncomplex, the guidance aspect becomes progressively more challenging and time\nconsuming often resulting in design schedules in which the verification time to\nhit all possible design coverage points is the dominant schedule limitation.\nThis paper describes an approach which leverages existing constrained-random DV\nenvironment tools but which further enhances them using supervised learning and\nreinforcement learning techniques. This approach provides better than random\nresults in a highly automated fashion thereby ensuring DV objectives of full\ndesign coverage can be achieved on an accelerated timescale and with fewer\nresources.\n  Two hardware verification examples are presented, one of a Cache Controller\ndesign and one using the open-source RISCV-Ariane design and Google's RISCV\nRandom Instruction Generator. We demonstrate that a machine-learning based\napproach can perform significantly better on functional coverage and reaching\ncomplex hard-to-hit states than a random or constrained-random approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 23:23:57 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hughes", "William", ""], ["Srinivasan", "Sandeep", ""], ["Suvarna", "Rohit", ""], ["Kulkarni", "Maithilee", ""]]}, {"id": "1909.13271", "submitter": "Thierry Tambe", "authors": "Thierry Tambe, En-Yu Yang, Zishen Wan, Yuntian Deng, Vijay Janapa\n  Reddi, Alexander Rush, David Brooks, Gu-Yeon Wei", "title": "AdaptivFloat: A Floating-point based Data Type for Resilient Deep\n  Learning Inference", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional hardware-friendly quantization methods, such as fixed-point or\ninteger, tend to perform poorly at very low word sizes as their shrinking\ndynamic ranges cannot adequately capture the wide data distributions commonly\nseen in sequence transduction models. We present AdaptivFloat, a floating-point\ninspired number representation format for deep learning that dynamically\nmaximizes and optimally clips its available dynamic range, at a layer\ngranularity, in order to create faithful encoding of neural network parameters.\nAdaptivFloat consistently produces higher inference accuracies compared to\nblock floating-point, uniform, IEEE-like float or posit encodings at very low\nprecision ($\\leq$ 8-bit) across a diverse set of state-of-the-art neural\nnetwork topologies. And notably, AdaptivFloat is seen surpassing baseline FP32\nperformance by up to +0.3 in BLEU score and -0.75 in word error rate at weight\nbit widths that are $\\leq$ 8-bit. Experimental results on a deep neural network\n(DNN) hardware accelerator, exploiting AdaptivFloat logic in its computational\ndatapath, demonstrate per-operation energy and area that is 0.9$\\times$ and\n1.14$\\times$, respectively, that of equivalent bit width integer-based\naccelerator variants.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 12:41:46 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:00:21 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 09:30:21 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Tambe", "Thierry", ""], ["Yang", "En-Yu", ""], ["Wan", "Zishen", ""], ["Deng", "Yuntian", ""], ["Reddi", "Vijay Janapa", ""], ["Rush", "Alexander", ""], ["Brooks", "David", ""], ["Wei", "Gu-Yeon", ""]]}, {"id": "1909.13318", "submitter": "Arish Sateesan", "authors": "S. Arish and R.K. Sharma", "title": "Run-time reconfigurable multi-precision floating point multiplier design\n  for high speed, low-power applications", "comments": null, "journal-ref": "2015 2nd International Conference on Signal Processing and\n  Integrated Networks (SPIN)", "doi": "10.1109/SPIN.2015.7095315", "report-no": null, "categories": "cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating point multiplication is one of the crucial operations in many\napplication domains such as image processing, signal processing etc. But every\napplication requires different working features. Some need high precision, some\nneed low power consumption, low latency etc. But IEEE-754 format is not really\nflexible for these specifications and also design is complex. Optimal run-time\nreconfigurable hardware implementations may need the use of custom\nfloating-point formats that do not necessarily follow IEEE specified sizes. In\nthis paper, we present a run-time-reconfigurable floating point multiplier\nimplemented on FPGA with custom floating point format for different\napplications. This floating point multiplier can have 6 modes of operations\ndepending on the accuracy or application requirement. With the use of optimal\ndesign with custom IPs (Intellectual Properties), a better implementation is\ndone by truncating the inputs before multiplication. And a combination of\nKaratsuba algorithm and Urdhva-Tiryagbhyam algorithm (Vedic Mathematics) is\nused to implement unsigned binary multiplier. This further increases the\nefficiency of the multiplier.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 16:47:40 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2020 21:48:39 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Arish", "S.", ""], ["Sharma", "R. K.", ""]]}, {"id": "1909.13396", "submitter": "Caiwen Ding", "authors": "Caiwen Ding, Shuo Wang, Ning Liu, Kaidi Xu, Yanzhi Wang and Yun Liang", "title": "REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object\n  Detection on FPGAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs), as the basis of object detection, will play a\nkey role in the development of future autonomous systems with full autonomy.\nThe autonomous systems have special requirements of real-time, energy-efficient\nimplementations of DNNs on a power-constrained system. Two research thrusts are\ndedicated to performance and energy efficiency enhancement of the inference\nphase of DNNs. The first one is model compression techniques while the second\nis efficient hardware implementation. Recent works on extremely-low-bit CNNs\nsuch as the binary neural network (BNN) and XNOR-Net replace the traditional\nfloating-point operations with binary bit operations which significantly\nreduces the memory bandwidth and storage requirement. However, it suffers from\nnon-negligible accuracy loss and underutilized digital signal processing (DSP)\nblocks of FPGAs. To overcome these limitations, this paper proposes REQ-YOLO, a\nresource-aware, systematic weight quantization framework for object detection,\nconsidering both algorithm and hardware resource aspects in object detection.\nWe adopt the block-circulant matrix method and propose a heterogeneous weight\nquantization using the Alternating Direction Method of Multipliers (ADMM), an\neffective optimization technique for general, non-convex optimization problems.\nTo achieve real-time, highly-efficient implementations on FPGA, we present the\ndetailed hardware implementation of block circulant matrices on CONV layers and\ndevelop an efficient processing element (PE) structure supporting the\nheterogeneous weight quantization, CONV dataflow and pipelining techniques,\ndesign optimization, and a template-based automatic synthesis framework to\noptimally exploit hardware resource. Experimental results show that our\nproposed REQ-YOLO framework can significantly compress the YOLO model while\nintroducing very small accuracy degradation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 23:21:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ding", "Caiwen", ""], ["Wang", "Shuo", ""], ["Liu", "Ning", ""], ["Xu", "Kaidi", ""], ["Wang", "Yanzhi", ""], ["Liang", "Yun", ""]]}, {"id": "1909.13807", "submitter": "Jan Moritz Joseph", "authors": "Jan Moritz Joseph and Dominik Ermel and Lennart Bamberg and Alberto\n  Garc\\'ia-Ortiz and Thilo Pionteck", "title": "System-level optimization of Network-on-Chips for heterogeneous 3D\n  System-on-Chips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a system-level design of Networks-on-Chip for 3D heterogeneous\nSystem-on-Chip (SoC), the locations of components, routers and vertical links\nare determined from an application model and technology parameters. In\nconventional methods, the two inputs are accounted for separately; here, we\ndefine an integrated problem that considers both application model and\ntechnology parameters. We show that this problem does not allow for exact\nsolution in reasonable time, as common for many design problems. Therefore, we\ncontribute a heuristic by proposing design steps, which are based on separation\nof intralayer and interlayer communication. The advantage is that this new\nproblem can be solved with well-known methods. We use 3D Vision SoC case\nstudies to quantify the advantages and the practical usability of the proposed\noptimization approach. We achieve up to 18.8% reduced white space and up to\n12.4% better network performance in comparison to conventional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 16:03:28 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 14:47:25 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Joseph", "Jan Moritz", ""], ["Ermel", "Dominik", ""], ["Bamberg", "Lennart", ""], ["Garc\u00eda-Ortiz", "Alberto", ""], ["Pionteck", "Thilo", ""]]}]