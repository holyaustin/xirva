[{"id": "1210.0065", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Granular association rule mining through parametric rough sets for cold\n  start recommendation", "comments": "The theory part of the paper should be replaced by formal context\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rules reveal patterns hide in many-to-many relationships\nwhich are common in relational databases. In recommender systems, these rules\nare appropriate for cold start recommendation, where a customer or a product\nhas just entered the system. An example of such rules might be \"40% men like at\nleast 30% kinds of alcohol; 45% customers are men and 6% products are alcohol.\"\nMining such rules is a challenging problem due to pattern explosion. In this\npaper, we propose a new type of parametric rough sets on two universes to study\nthis problem. The model is deliberately defined such that the parameter\ncorresponds to one threshold of rules. With the lower approximation operator in\nthe new parametric rough sets, a backward algorithm is designed for the rule\nmining problem. Experiments on two real world data sets show that the new\nalgorithm is significantly faster than the existing sandwich algorithm. This\nstudy indicates a new application area, namely recommender systems, of\nrelational data mining, granular computing and rough sets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 01:19:33 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2013 11:53:43 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1210.0595", "submitter": "Amir Hosein Asiaee", "authors": "Amir H. Asiaee, Prashant Doshi, Todd Minning, Satya Sahoo, Priti\n  Parikh, Amit Sheth, Rick L. Tarleton", "title": "From Questions to Effective Answers: On the Utility of Knowledge-Driven\n  Querying Systems for Life Sciences Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We compare two distinct approaches for querying data in the context of the\nlife sciences. The first approach utilizes conventional databases to store the\ndata and intuitive form-based interfaces to facilitate easy querying of the\ndata. These interfaces could be seen as implementing a set of \"pre-canned\"\nqueries commonly used by the life science researchers that we study. The second\napproach is based on semantic Web technologies and is knowledge (model) driven.\nIt utilizes a large OWL ontology and same datasets as before but associated as\nRDF instances of the ontology concepts. An intuitive interface is provided that\nallows the formulation of RDF triples-based queries. Both these approaches are\nbeing used in parallel by a team of cell biologists in their daily research\nactivities, with the objective of gradually replacing the conventional approach\nwith the knowledge-driven one. This provides us with a valuable opportunity to\ncompare and qualitatively evaluate the two approaches. We describe several\nbenefits of the knowledge-driven approach in comparison to the traditional way\nof accessing data, and highlight a few limitations as well. We believe that our\nanalysis not only explicitly highlights the specific benefits and limitations\nof semantic Web technologies in our context but also contributes toward\neffective ways of translating a question in a researcher's mind into precise\ncomputational queries with the intent of obtaining effective answers from the\ndata. While researchers often assume the benefits of semantic Web technologies,\nwe explicitly illustrate these in practice.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 22:10:30 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Asiaee", "Amir H.", ""], ["Doshi", "Prashant", ""], ["Minning", "Todd", ""], ["Sahoo", "Satya", ""], ["Parikh", "Priti", ""], ["Sheth", "Amit", ""], ["Tarleton", "Rick L.", ""]]}, {"id": "1210.0758", "submitter": "Daniele Cerra", "authors": "Daniele Cerra and Mihai Datcu", "title": "A fast compression-based similarity measure with applications to\n  content-based image retrieval", "comments": "Pre-print", "journal-ref": "Journal of Visual Communication and Image Representation, vol. 23,\n  no. 2, pp. 293-302, 2012", "doi": "10.1016/j.jvcir.2011.10.009", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression-based similarity measures are effectively employed in\napplications on diverse data types with a basically parameter-free approach.\nNevertheless, there are problems in applying these techniques to\nmedium-to-large datasets which have been seldom addressed. This paper proposes\na similarity measure based on compression with dictionaries, the Fast\nCompression Distance (FCD), which reduces the complexity of these methods,\nwithout degradations in performance. On its basis a content-based color image\nretrieval system is defined, which can be compared to state-of-the-art methods\nbased on invariant color features. Through the FCD a better understanding of\ncompression-based techniques is achieved, by performing experiments on datasets\nwhich are larger than the ones analyzed so far in literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:04:49 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""]]}, {"id": "1210.0852", "submitter": "Winfried G\\\"odert", "authors": "Winfried G\\\"odert", "title": "Detecting multiword phrases in mathematical text corpora", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for detecting multiword phrases in mathematical text\ncorpora. The method used is based on characteristic features of mathematical\nterminology. It makes use of a software tool named Lingo which allows to\nidentify words by means of previously defined dictionaries for specific word\nclasses as adjectives, personal names or nouns. The detection of multiword\ngroups is done algorithmically. Possible advantages of the method for indexing\nand information retrieval and conclusions for applying dictionary-based methods\nof automatic indexing instead of stemming procedures are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 17:41:58 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["G\u00f6dert", "Winfried", ""]]}, {"id": "1210.0999", "submitter": "Pierrick Tranouez", "authors": "Thomas Palfray (LITIS), David H\\'ebert (LITIS), St\\'ephane Nicolas\n  (LITIS), Pierrick Tranouez (LITIS), Thierry Paquet (LITIS)", "title": "Logical segmentation for article extraction in digitized old newspapers", "comments": "ACM Document Engineering, France (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newspapers are documents made of news item and informative articles. They are\nnot meant to be red iteratively: the reader can pick his items in any order he\nfancies. Ignoring this structural property, most digitized newspaper archives\nonly offer access by issue or at best by page to their content. We have built a\ndigitization workflow that automatically extracts newspaper articles from\nimages, which allows indexing and retrieval of information at the article\nlevel. Our back-end system extracts the logical structure of the page to\nproduce the informative units: the articles. Each image is labelled at the\npixel level, through a machine learning based method, then the page logical\nstructure is constructed up from there by the detection of structuring entities\nsuch as horizontal and vertical separators, titles and text lines. This logical\nstructure is stored in a METS wrapper associated to the ALTO file produced by\nthe system including the OCRed text. Our front-end system provides a web high\ndefinition visualisation of images, textual indexing and retrieval facilities,\nsearching and reading at the article level. Articles transcriptions can be\ncollaboratively corrected, which as a consequence allows for better indexing.\nWe are currently testing our system on the archives of the Journal de Rouen,\none of France eldest local newspaper. These 250 years of publication amount to\n300 000 pages of very variable image quality and layout complexity. Test year\n1808 can be consulted at plair.univ-rouen.fr.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 06:24:25 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Palfray", "Thomas", "", "LITIS"], ["H\u00e9bert", "David", "", "LITIS"], ["Nicolas", "St\u00e9phane", "", "LITIS"], ["Tranouez", "Pierrick", "", "LITIS"], ["Paquet", "Thierry", "", "LITIS"]]}, {"id": "1210.1441", "submitter": "An Zeng", "authors": "Duanbing Chen, An Zeng, Giulio Cimini, Yi-Cheng Zhang", "title": "Adaptive social recommendation in a multiple category landscape", "comments": "7 pages, 5 figures", "journal-ref": "Eur. Phys. J. B. 86, 61 (2013)", "doi": "10.1140/epjb/e2012-30899-9", "report-no": null, "categories": "physics.soc-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People in the Internet era have to cope with the information overload,\nstriving to find what they are interested in, and usually face this situation\nby following a limited number of sources or friends that best match their\ninterests. A recent line of research, namely adaptive social recommendation,\nhas therefore emerged to optimize the information propagation in social\nnetworks and provide users with personalized recommendations. Validation of\nthese methods by agent-based simulations often assumes that the tastes of users\nand can be represented by binary vectors, with entries denoting users'\npreferences. In this work we introduce a more realistic assumption that users'\ntastes are modeled by multiple vectors. We show that within this framework the\nsocial recommendation process has a poor outcome. Accordingly, we design novel\nmeasures of users' taste similarity that can substantially improve the\nprecision of the recommender system. Finally, we discuss the issue of enhancing\nthe recommendations' diversity while preserving their accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 13:48:41 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Chen", "Duanbing", ""], ["Zeng", "An", ""], ["Cimini", "Giulio", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1210.1626", "submitter": "Yao Hengshuai", "authors": "Hengshuai Yao", "title": "Discovering and Leveraging the Most Valuable Links for Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the Web, visits of a page are often introduced by one or more valuable\nlinking sources. Indeed, good back links are valuable resources for Web pages\nand sites. We propose to discovering and leveraging the best backlinks of pages\nfor ranking. Similar to PageRank, MaxRank scores are updated {recursively}. In\nparticular, with probability $\\lambda$, the MaxRank of a document is updated\nfrom the backlink source with the maximum score; with probability $1-\\lambda$,\nthe MaxRank of a document is updated from a random backlink source. MaxRank has\nan interesting relation to PageRank. When $\\lambda=0$, MaxRank reduces to\nPageRank; when $\\lambda=1$, MaxRank only looks at the best backlink it thinks.\nEmpirical results on Wikipedia shows that the global authorities are very\ninfluential; Overall large $\\lambda$s (but smaller than 1) perform best: the\nconvergence is dramatically faster than PageRank, but the performance is still\ncomparable. We study the influence of these sources and propose a few measures\nsuch as the times of being the best backlink for others, and related properties\nof the proposed algorithm. The introduction of best backlink sources provides\nnew insights for link analysis. Besides ranking, our method can be used to\ndiscover the most valuable linking sources for a page or Website, which is\nuseful for both search engines and site owners.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 02:05:54 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Yao", "Hengshuai", ""]]}, {"id": "1210.2752", "submitter": "Andrea Capocci", "authors": "Andrea Capocci, Andrea Baldassarri, Vito D. P. Servedio, Vittorio\n  Loreto", "title": "Statistical Properties of Inter-arrival Times Distribution in Social\n  Tagging Systems", "comments": "6 pages, 10 figures; Proceedings of the 20th ACM conference on\n  Hypertext and hypermedia, 2009", "journal-ref": null, "doi": "10.1145/1557914.1557955", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Folksonomies provide a rich source of data to study social patterns taking\nplace on the World Wide Web. Here we study the temporal patterns of users'\ntagging activity. We show that the statistical properties of inter-arrival\ntimes between subsequent tagging events cannot be explained without taking into\naccount correlation in users' behaviors. This shows that social interaction in\ncollaborative tagging communities shapes the evolution of folksonomies. A\nconsensus formation process involving the usage of a small number of tags for a\ngiven resources is observed through a numerical and analytical analysis of some\nwell-known folksonomy datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 20:47:33 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Capocci", "Andrea", ""], ["Baldassarri", "Andrea", ""], ["Servedio", "Vito D. P.", ""], ["Loreto", "Vittorio", ""]]}, {"id": "1210.3131", "submitter": "Alireza Nemaney Pour", "authors": "Shekoofeh Ghiam, Alireza Nemaney Pour", "title": "A Survey on Web Spam Detection Methods: Taxonomy", "comments": "16 pages, 7 figures, 7 tables", "journal-ref": null, "doi": "10.5121/ijnsa.2012.4510", "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web spam refers to some techniques, which try to manipulate search engine\nranking algorithms in order to raise web page position in search engine\nresults. In the best case, spammers encourage viewers to visit their sites, and\nprovide undeserved advertisement gains to the page owner. In the worst case,\nthey use malicious contents in their pages and try to install malware on the\nvictims machine. Spammers use three kinds of spamming techniques to get higher\nscore in ranking. These techniques are Link based techniques, hiding techniques\nand content-based techniques. Existing spam pages cause distrust to search\nengine results. This not only wastes the time of visitors, but also wastes lots\nof search engine resources. Hence spam detection methods have been proposed as\na solution for web spam in order to reduce negative effects of spam pages.\nExperimental results show that some of these techniques are working well and\ncan find spam pages more accurate than the others. This paper classifies web\nspam techniques and the related detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 05:58:41 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Ghiam", "Shekoofeh", ""], ["Pour", "Alireza Nemaney", ""]]}, {"id": "1210.3241", "submitter": "Vit Novacek", "authors": "Vit Novacek", "title": "Distributional Framework for Emergent Knowledge Acquisition and its\n  Application to Automated Document Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a framework for representation and acquisition of\nknowledge emerging from large samples of textual data. We utilise a\ntensor-based, distributional representation of simple statements extracted from\ntext, and show how one can use the representation to infer emergent knowledge\npatterns from the textual data in an unsupervised manner. Examples of the\npatterns we investigate in the paper are implicit term relationships or\nconjunctive IF-THEN rules. To evaluate the practical relevance of our approach,\nwe apply it to annotation of life science articles with terms from MeSH (a\ncontrolled biomedical vocabulary and thesaurus).\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 14:10:25 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Novacek", "Vit", ""]]}, {"id": "1210.3312", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres-Moreno", "title": "Artex is AnotheR TEXt summarizer", "comments": "11 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1209.3126", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes Artex, another algorithm for Automatic Text\nSummarization. In order to rank sentences, a simple inner product is calculated\nbetween each sentence, a document vector (text topic) and a lexical vector\n(vocabulary used by a sentence). Summaries are then generated by assembling the\nhighest ranked sentences. No ruled-based linguistic post-processing is\nnecessary in order to obtain summaries. Tests over several datasets (coming\nfrom Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),\nevaluation campaigns, etc.) in French, English and Spanish have shown that\nsummarizer achieves interesting results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 18:21:01 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "1210.3865", "submitter": "Chao-Lin Liu", "authors": "Chien-Liang Chen, Chao-Lin Liu, Yuan-Chen Chang, and Hsiang-Ping Tsai", "title": "Opinion Mining for Relating Subjective Expressions and Annual Earnings\n  in US Financial Statements", "comments": "24 pages, 3 figures, 13 tables, partially appeared in two conference\n  proceedings: (1) Proceedings of the IEEE International Conference on\n  e-Business Engineering 2011 and (2) Proceedings of the 2011 Conference on\n  Technologies and Applications of Artificial Intelligence; Journal of\n  Information Science and Engineering, 29(3), May 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial statements contain quantitative information and manager's\nsubjective evaluation of firm's financial status. Using information released in\nU.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for\nquality financial decisions. To extract such opinioned statements from the\nreports, we built tagging models based on the conditional random field (CRF)\ntechniques, considering a variety of combinations of linguistic factors\nincluding morphology, orthography, predicate-argument structure, syntax, and\nsimple semantics. Our results show that the CRF models are reasonably effective\nto find opinion holders in experiments when we adopted the popular MPQA corpus\nfor training and testing. The contribution of our paper is to identify opinion\npatterns in multiword expressions (MWEs) forms rather than in single word\nforms.\n  We find that the managers of corporations attempt to use more optimistic\nwords to obfuscate negative financial performance and to accentuate the\npositive financial performance. Our results also show that decreasing earnings\nwere often accompanied by ambiguous and mild statements in the reporting year\nand that increasing earnings were stated in assertive and positive way.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 00:58:11 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Chen", "Chien-Liang", ""], ["Liu", "Chao-Lin", ""], ["Chang", "Yuan-Chen", ""], ["Tsai", "Hsiang-Ping", ""]]}, {"id": "1210.3926", "submitter": "Julian McAuley", "authors": "Julian McAuley, Jure Leskovec, Dan Jurafsky", "title": "Learning Attitudes and Attributes from Multi-Aspect Reviews", "comments": "11 pages, 6 figures, extended version of our ICDM 2012 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of online reviews consist of plain-text feedback together with a\nsingle numeric score. However, there are multiple dimensions to products and\nopinions, and understanding the `aspects' that contribute to users' ratings may\nhelp us to better understand their individual preferences. For example, a\nuser's impression of an audiobook presumably depends on aspects such as the\nstory and the narrator, and knowing their opinions on these aspects may help us\nto recommend better products. In this paper, we build models for rating systems\nin which such dimensions are explicit, in the sense that users leave separate\nratings for each aspect of a product. By introducing new corpora consisting of\nfive million reviews, rated with between three and six aspects, we evaluate our\nmodels on three prediction tasks: First, we use our model to uncover which\nparts of a review discuss which of the rated aspects. Second, we use our model\nto summarize reviews, which for us means finding the sentences that best\nexplain a user's rating. Finally, since aspect ratings are optional in many of\nthe datasets we consider, we use our model to recover those ratings that are\nmissing from a user's evaluation. Our model matches state-of-the-art approaches\non existing small-scale datasets, while scaling to the real-world datasets we\nintroduce. Moreover, our model is able to `disentangle' content and sentiment\nwords: we automatically learn content words that are indicative of a particular\naspect as well as the aspect-specific sentiment words that are indicative of a\nparticular rating.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 07:36:57 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 16:14:35 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["McAuley", "Julian", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1210.4008", "submitter": "Augusto Santos", "authors": "Augusto Dias Pereira dos Santos, Leandro Krug Wives, Luis Otavio\n  Alvares", "title": "Location-Based Events Detection on Micro-Blogs", "comments": "10 pages, 5 figures, submitted and rejected for SBBD 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of social networks generates enormous amounts of data that\ncan be used for many types of analysis. Some of these data have temporal and\ngeographical information, which can be used for comprehensive examination. In\nthis paper, we propose a new method to analyze the massive volume of messages\navailable in Twitter to identify places in the world where topics such as TV\nshows, climate change, disasters, and sports are emerging. The proposed method\nis based on a neural network that is used to detect outliers from a time\nseries, which is built upon statistical data from tweets located on different\npolitical divisions (i.e., countries, cities). The outliers are used to\nidentify topics within an abnormal behavior in Twitter. The effectiveness of\nour method is evaluated in an online environment indicating new findings on\nmodeling local people's behavior from different places.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 12:45:39 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Santos", "Augusto Dias Pereira dos", ""], ["Wives", "Leandro Krug", ""], ["Alvares", "Luis Otavio", ""]]}, {"id": "1210.4850", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox", "title": "Markov Determinantal Point Processes", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-26-35", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal point process (DPP) is a random process useful for modeling\nthe combinatorial problem of subset selection. In particular, DPPs encourage a\nrandom subset Y to contain a diverse set of items selected from a base set Y.\nFor example, we might use a DPP to display a set of news headlines that are\nrelevant to a user's interests while covering a variety of topics. Suppose,\nhowever, that we are asked to sequentially select multiple diverse sets of\nitems, for example, displaying new headlines day-by-day. We might want these\nsets to be diverse not just individually but also through time, offering\nheadlines today that are unlike the ones shown yesterday. In this paper, we\nconstruct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The\nproposed M-DPP defines a stationary process that maintains DPP margins.\nCrucially, the induced union process Zt = Yt u Yt-1 is also marginally\nDPP-distributed. Jointly, these properties imply that the sequence of random\nsets are encouraged to be diverse both at a given time step as well as across\ntime steps. We describe an exact, efficient sampling procedure, and a method\nfor incrementally learning a quality measure over items in the base set Y based\non external preferences. We apply the M-DPP to the task of sequentially\ndisplaying diverse and relevant news articles to a user with topic preferences.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:35:39 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Kulesza", "Alex", ""], ["Fox", "Emily B.", ""]]}, {"id": "1210.4869", "submitter": "Guang Ling", "authors": "Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King", "title": "Response Aware Model-Based Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-501-510", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on recommender systems mainly focus on fitting the ratings\nprovided by users. However, the response patterns, i.e., some items are rated\nwhile others not, are generally ignored. We argue that failing to observe such\nresponse patterns can lead to biased parameter estimation and sub-optimal model\nperformance. Although several pieces of work have tried to model users'\nresponse patterns, they miss the effectiveness and interpretability of the\nsuccessful matrix factorization collaborative filtering approaches. To bridge\nthe gap, in this paper, we unify explicit response models and PMF to establish\nthe Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We\nshow that RAPMF subsumes PMF as a special case. Empirically we demonstrate the\nmerits of RAPMF from various aspects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:40:52 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Ling", "Guang", ""], ["Yang", "Haiqin", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "1210.4871", "submitter": "Hui Lin", "authors": "Hui Lin, Jeff A. Bilmes", "title": "Learning Mixtures of Submodular Shells with Application to Document\n  Summarization", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-479-490", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a mixture of submodular \"shells\" in a\nlarge-margin setting. A submodular shell is an abstract submodular function\nthat can be instantiated with a ground set and a set of parameters to produce a\nsubmodular function. A mixture of such shells can then also be so instantiated\nto produce a more complex submodular function. What our algorithm learns are\nthe mixture weights over such shells. We provide a risk bound guarantee when\nlearning in a large-margin structured-prediction setting using a projected\nsubgradient method when only approximate submodular optimization is possible\n(such as with submodular function maximization). We apply this method to the\nproblem of multi-document summarization and produce the best results reported\nso far on the widely used NIST DUC-05 through DUC-07 document summarization\ncorpora.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lin", "Hui", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1210.4914", "submitter": "Jason Weston", "authors": "Jason Weston, John Blitzer", "title": "Latent Structured Ranking", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-903-913", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many latent (factorized) models have been proposed for recommendation tasks\nlike collaborative filtering and for ranking tasks like document or image\nretrieval and annotation. Common to all those methods is that during inference\nthe items are scored independently by their similarity to the query in the\nlatent embedding space. The structure of the ranked list (i.e. considering the\nset of items returned as a whole) is not taken into account. This can be a\nproblem because the set of top predictions can be either too diverse (contain\nresults that contradict each other) or are not diverse enough. In this paper we\nintroduce a method for learning latent structured rankings that improves over\nexisting methods by providing the right blend of predictions at the top of the\nranked list. Particular emphasis is put on making this method scalable.\nEmpirical results on large scale image annotation and music recommendation\ntasks show improvements over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:08 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Weston", "Jason", ""], ["Blitzer", "John", ""]]}, {"id": "1210.4920", "submitter": "Seppo Virtanen", "authors": "Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell", "title": "Factorized Multi-Modal Topic Model", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-843-851", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal data collections, such as corpora of paired images and text\nsnippets, require analysis methods beyond single-view component and topic\nmodels. For continuous observations the current dominant approach is based on\nextensions of canonical correlation analysis, factorizing the variation into\ncomponents shared by the different modalities and those private to each of\nthem. For count data, multiple variants of topic models attempting to tie the\nmodalities together have been presented. All of these, however, lack the\nability to learn components private to one modality, and consequently will try\nto force dependencies even between minimally correlating modalities. In this\nwork we combine the two approaches by presenting a novel HDP-based topic model\nthat automatically learns both shared and private topics. The model is shown to\nbe especially useful for querying the contents of one domain given samples of\nthe other.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:57:22 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Virtanen", "Seppo", ""], ["Jia", "Yangqing", ""], ["Klami", "Arto", ""], ["Darrell", "Trevor", ""]]}, {"id": "1210.5560", "submitter": "Santiago M. Mola-Velasco", "authors": "Santiago M. Mola-Velasco", "title": "Wikipedia Vandalism Detection Through Machine Learning: Feature Review\n  and New Proposals: Lab Report for PAN at CLEF 2010", "comments": "Published in CLEF 2010 LABs and Workshops, Notebook Papers, 22-23\n  September 2010, Padua, Italy. 2010, ISBN 978-88-904810-0-0. First position at\n  the 1st International Competition on Wikipedia Vandalism Detection (PAN @\n  CLEF 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Wikipedia is an online encyclopedia that anyone can edit. In this open model,\nsome people edits with the intent of harming the integrity of Wikipedia. This\nis known as vandalism. We extend the framework presented in (Potthast, Stein,\nand Gerling, 2008) for Wikipedia vandalism detection. In this approach, several\nvandalism indicating features are extracted from edits in a vandalism corpus\nand are fed to a supervised learning algorithm. The best performing classifiers\nwere LogitBoost and Random Forest. Our classifier, a Random Forest, obtained an\nAUC of 0.92236, ranking in the first place of the PAN'10 Wikipedia vandalism\ndetection task.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 23:12:43 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Mola-Velasco", "Santiago M.", ""]]}, {"id": "1210.5581", "submitter": "Chao-Lin Liu", "authors": "Chia-Chi Tsai, Chao-Lin Liu, Wei-Jie Huang, Man-Kwan Shan", "title": "Hidden Trends in 90 Years of Harvard Business Review", "comments": "6 pages, 14 figures, Proceedings of 2012 International Conference on\n  Technologies and Applications of Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we demonstrate and discuss results of our mining the abstracts\nof the publications in Harvard Business Review between 1922 and 2012.\nTechniques for computing n-grams, collocations, basic sentiment analysis, and\nnamed-entity recognition were employed to uncover trends hidden in the\nabstracts. We present findings about international relationships, sentiment in\nHBR's abstracts, important international companies, influential technological\ninventions, renown researchers in management theories, US presidents via\nchronological analyses.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 06:09:11 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Tsai", "Chia-Chi", ""], ["Liu", "Chao-Lin", ""], ["Huang", "Wei-Jie", ""], ["Shan", "Man-Kwan", ""]]}, {"id": "1210.5898", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Guantao Jin, Qingfeng Liu, Wei-Yun Chiu, Yih-Soong Yu", "title": "Some Chances and Challenges in Applying Language Technologies to\n  Historical Studies in Chinese", "comments": "15 pages, 9 figures, 2 tables; partially appeared in the Proceedings\n  of the Third International Conference of Digital Archives and Digital\n  Humanities", "journal-ref": "International Journal of Computational Linguistics and Chinese\n  Language Processing, 16(1-2), 27-46, 2011", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We report applications of language technology to analyzing historical\ndocuments in the Database for the Study of Modern Chinese Thoughts and\nLiterature (DSMCTL). We studied two historical issues with the reported\ntechniques: the conceptualization of \"huaren\" (Chinese people) and the attempt\nto institute constitutional monarchy in the late Qing dynasty. We also discuss\nresearch challenges for supporting sophisticated issues using our experience\nwith DSMCTL, the Database of Government Officials of the Republic of China, and\nthe Dream of the Red Chamber. Advanced techniques and tools for lexical,\nsyntactic, semantic, and pragmatic processing of language information, along\nwith more thorough data collection, are needed to strengthen the collaboration\nbetween historians and computer scientists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 13:56:46 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Jin", "Guantao", ""], ["Liu", "Qingfeng", ""], ["Chiu", "Wei-Yun", ""], ["Yu", "Yih-Soong", ""]]}, {"id": "1210.6113", "submitter": "EPTCS", "authors": "Sergio L\\'opez (Universitat Polit\\`ecnica de Val\\`encia), Josep Silva\n  (Universitat Polit\\`ecnica de Val\\`encia), David Insa (Universitat\n  Polit\\`ecnica de Val\\`encia)", "title": "Using the DOM Tree for Content Extraction", "comments": "In Proceedings WWV 2012, arXiv:1210.5783", "journal-ref": "EPTCS 98, 2012, pp. 46-59", "doi": "10.4204/EPTCS.98.6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main information of a webpage is usually mixed between menus,\nadvertisements, panels, and other not necessarily related information; and it\nis often difficult to automatically isolate this information. This is precisely\nthe objective of content extraction, a research area of widely interest due to\nits many applications. Content extraction is useful not only for the final\nhuman user, but it is also frequently used as a preprocessing stage of\ndifferent systems that need to extract the main content in a web document to\navoid the treatment and processing of other useless information. Other\ninteresting application where content extraction is particularly used is\ndisplaying webpages in small screens such as mobile phones or PDAs. In this\nwork we present a new technique for content extraction that uses the DOM tree\nof the webpage to analyze the hierarchical relations of the elements in the\nwebpage. Thanks to this information, the technique achieves a considerable\nrecall and precision. Using the DOM structure for content extraction gives us\nthe benefits of other approaches based on the syntax of the webpage (such as\ncharacters, words and tags), but it also gives us a very precise information\nregarding the related components in a block, thus, producing very cohesive\nblocks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 02:55:06 GMT"}], "update_date": "2012-10-24", "authors_parsed": [["L\u00f3pez", "Sergio", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia"], ["Silva", "Josep", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia"], ["Insa", "David", "", "Universitat\n  Polit\u00e8cnica de Val\u00e8ncia"]]}, {"id": "1210.6287", "submitter": "Parikshit Ram", "authors": "Ryan R. Curtin, Parikshit Ram, Alexander G. Gray", "title": "Fast Exact Max-Kernel Search", "comments": "Under submission in SIAM Data Mining conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an $O(n\n\\log n)$ algorithm to index any set of objects (points in $\\Real^\\dims$ or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n$O(\\log n)$ algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 16:51:31 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 19:14:20 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Ram", "Parikshit", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1210.6510", "submitter": "Stephane Cordier", "authors": "St\\'ephane Cordier (MAPMO)", "title": "A measure of similarity between scientific journals and of diversity of\n  a list of publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this note is to propose a definition of the scientific diversity\nand corollarly, a measure of the \"interdisciplinarity\" of collaborations. With\nrespect to previous studies, the proposed approach consists of 2 steps : first,\nthe definition of similarity between journals and second, these similarities\nare used to characterize the homogeneity (or, on the contrary the diversity) of\na publication list (that can be for one individual or a team).\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 12:37:26 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Cordier", "St\u00e9phane", "", "MAPMO"]]}, {"id": "1210.7056", "submitter": "Zhongqi Lu", "authors": "Zhongqi Lu and Erheng Zhong and Lili Zhao and Wei Xiang and Weike Pan\n  and Qiang Yang", "title": "Selective Transfer Learning for Cross Domain Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) aims to predict users' ratings on items\naccording to historical user-item preference data. In many real-world\napplications, preference data are usually sparse, which would make models\noverfit and fail to give accurate predictions. Recently, several research works\nshow that by transferring knowledge from some manually selected source domains,\nthe data sparseness problem could be mitigated. However for most cases, parts\nof source domain data are not consistent with the observations in the target\ndomain, which may misguide the target domain model building. In this paper, we\npropose a novel criterion based on empirical prediction error and its variance\nto better capture the consistency across domains in CF settings. Consequently,\nwe embed this criterion into a boosting framework to perform selective\nknowledge transfer. Comparing to several state-of-the-art methods, we show that\nour proposed selective transfer learning framework can significantly improve\nthe accuracy of rating prediction tasks on several real-world recommendation\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:36:57 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Lu", "Zhongqi", ""], ["Zhong", "Erheng", ""], ["Zhao", "Lili", ""], ["Xiang", "Wei", ""], ["Pan", "Weike", ""], ["Yang", "Qiang", ""]]}, {"id": "1210.7350", "submitter": "Jimmy Lin", "authors": "Gilad Mishne, Jeff Dalton, Zhenghua Li, Aneesh Sharma, Jimmy Lin", "title": "Fast Data in the Era of Big Data: Twitter's Real-Time Related Query\n  Suggestion Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture behind Twitter's real-time related query\nsuggestion and spelling correction service. Although these tasks have received\nmuch attention in the web search literature, the Twitter context introduces a\nreal-time \"twist\": after significant breaking news events, we aim to provide\nrelevant results within minutes. This paper provides a case study illustrating\nthe challenges of real-time data processing in the era of \"big data\". We tell\nthe story of how our system was built twice: our first implementation was built\non a typical Hadoop-based analytics stack, but was later replaced because it\ndid not meet the latency requirements necessary to generate meaningful\nreal-time results. The second implementation, which is the system deployed in\nproduction, is a custom in-memory processing engine specifically designed for\nthe task. This experience taught us that the current typical usage of Hadoop as\na \"big data\" platform, while great for experimentation, is not well suited to\nlow-latency processing, and points the way to future work on data analytics\nplatforms that can handle \"big\" as well as \"fast\" data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 17:20:42 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Mishne", "Gilad", ""], ["Dalton", "Jeff", ""], ["Li", "Zhenghua", ""], ["Sharma", "Aneesh", ""], ["Lin", "Jimmy", ""]]}, {"id": "1210.7599", "submitter": "Krunoslav Zubrinic", "authors": "Krunoslav Zubrinic, Damir Kalpic, Mario Milicevic", "title": "The automatic creation of concept maps from documents written using\n  morphologically rich languages", "comments": "ISSN 0957-4174", "journal-ref": "Expert Systems with Applications, Volume 39, Issue 16, 2012, Pages\n  12709-12718", "doi": "10.1016/j.eswa.2012.04.065", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept map is a graphical tool for representing knowledge. They have been\nused in many different areas, including education, knowledge management,\nbusiness and intelligence. Constructing of concept maps manually can be a\ncomplex task; an unskilled person may encounter difficulties in determining and\npositioning concepts relevant to the problem area. An application that\nrecommends concept candidates and their position in a concept map can\nsignificantly help the user in that situation. This paper gives an overview of\ndifferent approaches to automatic and semi-automatic creation of concept maps\nfrom textual and non-textual sources. The concept map mining process is\ndefined, and one method suitable for the creation of concept maps from\nunstructured textual sources in highly inflected languages such as the Croatian\nlanguage is described in detail. Proposed method uses statistical and data\nmining techniques enriched with linguistic tools. With minor adjustments, that\nmethod can also be used for concept map mining from textual sources in other\nmorphologically rich languages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 09:18:34 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 17:46:02 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Zubrinic", "Krunoslav", ""], ["Kalpic", "Damir", ""], ["Milicevic", "Mario", ""]]}, {"id": "1210.7917", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "The Model of Semantic Concepts Lattice For Data Mining Of Microblogs", "comments": "In Ukrainian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model of semantic concept lattice for data mining of microblogs has been\nproposed in this work. It is shown that the use of this model is effective for\nthe semantic relations analysis and for the detection of associative rules of\nkey words.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2012 07:25:46 GMT"}], "update_date": "2012-10-31", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1210.8436", "submitter": "Ciprian Chelba", "authors": "Maryam Kamvar and Ciprian Chelba", "title": "Optimal size, freshness and time-frame for voice search vocabulary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to optimize the vocabulary for a voice\nsearch language model. The metric we optimize over is the out-of-vocabulary\n(OoV) rate since it is a strong indicator of user experience. In a departure\nfrom the usual way of measuring OoV rates, web search logs allow us to compute\nthe per-session OoV rate and thus estimate the percentage of users that\nexperience a given OoV rate. Under very conservative text normalization, we\nfind that a voice search vocabulary consisting of 2 to 2.5 million words\nextracted from 1 week of search query data will result in an aggregate OoV rate\nof 1%; at that size, the same OoV rate will also be experienced by 90% of\nusers. The number of words included in the vocabulary is a stable indicator of\nthe OoV rate. Altering the freshness of the vocabulary or the duration of the\ntime window over which the training data is gathered does not significantly\nchange the OoV rate. Surprisingly, a significantly larger vocabulary\n(approximately 10 million words) is required to guarantee OoV rates below 1%\nfor 95% of the users.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 18:52:01 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Kamvar", "Maryam", ""], ["Chelba", "Ciprian", ""]]}]