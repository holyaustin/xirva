[{"id": "1901.00171", "submitter": "Shengze Yu", "authors": "Shengze Yu, Xin Wang, Wenwu Zhu, Peng Cui and Jingdong Wang", "title": "Disparity-preserved Deep Cross-platform Association for Cross-platform\n  Video Recommendation", "comments": null, "journal-ref": "Proceedings of the Twenty-Eighth International Joint Conference on\n  Artificial Intelligence. (2019) 4635-4641", "doi": "10.24963/ijcai.2019/644", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-platform recommendation aims to improve recommendation accuracy through\nassociating information from different platforms. Existing cross-platform\nrecommendation approaches assume all cross-platform information to be\nconsistent with each other and can be aligned. However, there remain two\nunsolved challenges: i) there exist inconsistencies in cross-platform\nassociation due to platform-specific disparity, and ii) data from distinct\nplatforms may have different semantic granularities. In this paper, we propose\na cross-platform association model for cross-platform video recommendation,\ni.e., Disparity-preserved Deep Cross-platform Association (DCA), taking\nplatform-specific disparity and granularity difference into consideration. The\nproposed DCA model employs a partially-connected multi-modal autoencoder, which\nis capable of explicitly capturing platform-specific information, as well as\nutilizing nonlinear mapping functions to handle granularity differences. We\nthen present a cross-platform video recommendation approach based on the\nproposed DCA model. Extensive experiments for our cross-platform recommendation\nframework on real-world dataset demonstrate that the proposed DCA model\nsignificantly outperform existing cross-platform recommendation methods in\nterms of various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 16:00:50 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 09:44:49 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Yu", "Shengze", ""], ["Wang", "Xin", ""], ["Zhu", "Wenwu", ""], ["Cui", "Peng", ""], ["Wang", "Jingdong", ""]]}, {"id": "1901.00306", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald, Simone Kopeinik, Elisabeth Lex", "title": "The TagRec Framework as a Toolkit for the Development of Tag-Based\n  Recommender Systems", "comments": "https://github.com/learning-layers/TagRec", "journal-ref": "UMAP 2017 Conference (Late-Breaking Results track)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have become important tools to support users in\nidentifying relevant content in an overloaded information space. To ease the\ndevelopment of recommender systems, a number of recommender frameworks have\nbeen proposed that serve a wide range of application domains. Our TagRec\nframework is one of the few examples of an open-source framework tailored\ntowards developing and evaluating tag-based recommender systems. In this paper,\nwe present the current, updated state of TagRec, and we summarize and reflect\non four use cases that have been implemented with TagRec: (i) tag\nrecommendations, (ii) resource recommendations, (iii) recommendation\nevaluation, and (iv) hashtag recommendations. To date, TagRec served the\ndevelopment and/or evaluation process of tag-based recommender systems in two\nlarge scale European research projects, which have been described in 17\nresearch papers. Thus, we believe that this work is of interest for both\nresearchers and practitioners of tag-based recommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 09:32:27 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Kowald", "Dominik", ""], ["Kopeinik", "Simone", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1901.00399", "submitter": "Oren Halvani", "authors": "Oren Halvani, Christian Winter, Lukas Graner", "title": "Unary and Binary Classification Approaches and their Implications for\n  Authorship Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving indexed documents, not by their topical content but their writing\nstyle opens the door for a number of applications in information retrieval\n(IR). One application is to retrieve textual content of a certain author X,\nwhere the queried IR system is provided beforehand with a set of reference\ntexts of X. Authorship verification (AV), which is a research subject in the\nfield of digital text forensics, is suitable for this purpose. The task of AV\nis to determine if two documents (i.e. an indexed and a reference document)\nhave been written by the same author X. Even though AV represents a unary\nclassification problem, a number of existing approaches consider it as a binary\nclassification task. However, the underlying classification model of an AV\nmethod has a number of serious implications regarding its prerequisites,\nevaluability, and applicability. In our comprehensive literature review, we\nobserved several misunderstandings regarding the differentiation of unary and\nbinary AV approaches that require consideration. The objective of this paper\nis, therefore, to clarify these by proposing clear criteria and new properties\nthat aim to improve the characterization of existing and future AV approaches.\nGiven both, we investigate the applicability of eleven existing unary and\nbinary AV methods as well as four generic unary classification algorithms on\ntwo self-compiled corpora. Furthermore, we highlight an important issue\nconcerning the evaluation of AV methods based on fixed decision criterions,\nwhich has not been paid attention in previous AV studies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 16:04:16 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Halvani", "Oren", ""], ["Winter", "Christian", ""], ["Graner", "Lukas", ""]]}, {"id": "1901.00400", "submitter": "Nicolas Pr\\\"ollochs", "authors": "Bernhard Lutz, Nicolas Pr\\\"ollochs, Dirk Neumann", "title": "Sentence-Level Sentiment Analysis of Financial News Using Distributed\n  Text Representations and Multi-Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers and financial professionals require robust computerized tools\nthat allow users to rapidly operationalize and assess the semantic textual\ncontent in financial news. However, existing methods commonly work at the\ndocument-level while deeper insights into the actual structure and the\nsentiment of individual sentences remain blurred. As a result, investors are\nrequired to apply the utmost attention and detailed, domain-specific knowledge\nin order to assess the information on a fine-grained basis. To facilitate this\nmanual process, this paper proposes the use of distributed text representations\nand multi-instance learning to transfer information from the document-level to\nthe sentence-level. Compared to alternative approaches, this method features\nsuperior predictive performance while preserving context and interpretability.\nOur analysis of a manually-labeled dataset yields a predictive accuracy of up\nto 69.90%, exceeding the performance of alternative approaches by at least 3.80\npercentage points. Accordingly, this study not only benefits investors with\nregard to their financial decision-making, but also helps companies to\ncommunicate their messages as intended.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 16:30:21 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Lutz", "Bernhard", ""], ["Pr\u00f6llochs", "Nicolas", ""], ["Neumann", "Dirk", ""]]}, {"id": "1901.00401", "submitter": "Yi Luan", "authors": "Yi Luan", "title": "Information Extraction from Scientific Literature for Method\n  Recommendation", "comments": "Thesis Proposal. arXiv admin note: text overlap with arXiv:1708.06075", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a research community grows, more and more papers are published each year.\nAs a result there is increasing demand for improved methods for finding\nrelevant papers, automatically understanding the key ideas and recommending\npotential methods for a target problem. Despite advances in search engines, it\nis still hard to identify new technologies according to a researcher's need.\nDue to the large variety of domains and extremely limited annotated resources,\nthere has been relatively little work on leveraging natural language processing\nin scientific recommendation. In this proposal, we aim at making scientific\nrecommendations by extracting scientific terms from a large collection of\nscientific papers and organizing the terms into a knowledge graph. In\npreliminary work, we trained a scientific term extractor using a small amount\nof annotated data and obtained state-of-the-art performance by leveraging large\namount of unannotated papers through applying multiple semi-supervised\napproaches. We propose to construct a knowledge graph in a way that can make\nminimal use of hand annotated data, using only the extracted terms,\nunsupervised relational signals such as co-occurrence, and structural external\nresources such as Wikipedia. Latent relations between scientific terms can be\nlearned from the graph. Recommendations will be made through graph inference\nfor both observed and unobserved relational pairs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 00:54:39 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Luan", "Yi", ""]]}, {"id": "1901.00415", "submitter": "Dai Tran", "authors": "Dai Hoang Tran, Zawar Hussain, Wei Emma Zhang, Nguyen Lu Dang Khoa,\n  Nguyen H. Tran, Quan Z. Sheng", "title": "Deep Autoencoder for Recommender Systems: Parameter Influence Analysis", "comments": "11 pages, ACIS 2018,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have recently attracted many researchers in the deep\nlearning community. The state-of-the-art deep neural network models used in\nrecommender systems are typically multilayer perceptron and deep Autoencoder\n(DAE), among which DAE usually shows better performance due to its superior\ncapability to reconstruct the inputs. However, we found existing DAE\nrecommendation systems that have similar implementations on similar datasets\nresult in vastly different parameter settings. In this work, we have built a\nflexible DAE model, named FlexEncoder that uses configurable parameters and\nunique features to analyse the parameter influences on the prediction accuracy\nof recommender systems. This will help us identify the best-performance\nparameters given a dataset. Extensive evaluation on the MovieLens datasets are\nconducted, which drives our conclusions on the influences of DAE parameters.\nSpecifically, we find that DAE parameters strongly affect the prediction\naccuracy of the recommender systems, and the effect is transferable to similar\ndatasets in a larger size. We open our code to public which could benefit both\nnew users for DAE -- they can quickly understand how DAE works for\nrecommendation systems, and experienced DAE users -- it easier for them to tune\nthe parameters on different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 03:56:19 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Tran", "Dai Hoang", ""], ["Hussain", "Zawar", ""], ["Zhang", "Wei Emma", ""], ["Khoa", "Nguyen Lu Dang", ""], ["Tran", "Nguyen H.", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "1901.00431", "submitter": "Dimitrios Rafailidis Dr", "authors": "Dimitrios Rafailidis and Yannis Manolopoulos", "title": "The Technological Gap Between Virtual Assistants and Recommendation\n  Systems", "comments": "6 pages, Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual assistants, also known as intelligent conversational systems such as\nGoogle's Virtual Assistant and Apple's Siri, interact with human-like responses\nto users' queries and finish specific tasks. Meanwhile, existing recommendation\ntechnologies model users' evolving, diverse and multi-aspect preferences to\ngenerate recommendations in various domains/applications, aiming to improve the\ncitizens' daily life by making suggestions. The repertoire of actions is no\nlonger limited to the one-shot presentation of recommendation lists, which can\nbe insufficient when the goal is to offer decision support for the user, by\nquickly adapting to his/her preferences through conversations. Such an\ninteractive mechanism is currently missing from recommendation systems. This\narticle sheds light on the gap between virtual assistants and recommendation\nsystems in terms of different technological aspects. In particular, we try to\nanswer the most fundamental research question, which are the missing\ntechnological factors to implement a personalized intelligent conversational\nagent for producing accurate recommendations while taking into account how\nusers behave under different conditions. The goal is, instead of adapting\nhumans to machines, to actually provide users with better recommendation\nservices so that machines will be adapted to humans in daily life.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 00:50:03 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 13:46:09 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Rafailidis", "Dimitrios", ""], ["Manolopoulos", "Yannis", ""]]}, {"id": "1901.00439", "submitter": "Oguzhan Gencoglu", "authors": "Oguzhan Gencoglu", "title": "Deep Representation Learning for Clustering of Health Tweets", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has been a prominent social media platform for mining\npopulation-level health data and accurate clustering of health-related tweets\ninto topics is important for extracting relevant health insights. In this work,\nwe propose deep convolutional autoencoders for learning compact representations\nof health-related tweets, further to be employed in clustering. We compare our\nmethod to several conventional tweet representation methods including\nbag-of-words, term frequency-inverse document frequency, Latent Dirichlet\nAllocation and Non-negative Matrix Factorization with 3 different clustering\nalgorithms. Our results show that the clustering performance using proposed\nrepresentation learning scheme significantly outperforms that of conventional\nmethods for all experiments of different number of clusters. In addition, we\npropose a constraint on the learned representations during the neural network\ntraining in order to further enhance the clustering performance. All in all,\nthis study introduces utilization of deep neural network-based architectures,\ni.e., deep convolutional autoencoders, for learning informative representations\nof health-related tweets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 00:31:22 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gencoglu", "Oguzhan", ""]]}, {"id": "1901.00450", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dmitry Bogdanov, Jisang Yoon, KwangSeob Kim and Xavier\n  Serra", "title": "Automatic playlist continuation using a hybrid recommender system\n  combining features from text and audio", "comments": "5 pages", "journal-ref": "Proceeding RecSys Challenge '18 Proceedings of the ACM Recommender\n  Systems Challenge 2018", "doi": "10.1145/3267471.3267473", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ACM RecSys Challenge 2018 focuses on music recommendation in the context\nof automatic playlist continuation. In this paper, we describe our approach to\nthe problem and the final hybrid system that was submitted to the challenge by\nour team Cocoplaya. This system consists in combining the recommendations\nproduced by two different models using ranking fusion. The first model is based\non Matrix Factorization and it incorporates information from tracks' audio and\nplaylist titles. The second model generates recommendations based on typical\ntrack co-occurrences considering their proximity in the playlists. The proposed\napproach is efficient and achieves a good overall performance, with our model\nranked 4th on the creative track of the challenge leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 16:49:06 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ferraro", "Andres", ""], ["Bogdanov", "Dmitry", ""], ["Yoon", "Jisang", ""], ["Kim", "KwangSeob", ""], ["Serra", "Xavier", ""]]}, {"id": "1901.00597", "submitter": "Yun He", "authors": "Yun He, Haochen Chen, Ziwei Zhu and James Caverlee", "title": "Pseudo-Implicit Feedback for Alleviating Data Sparsity in Top-K\n  Recommendation", "comments": "Accepted by ICDM'18 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose PsiRec, a novel user preference propagation recommender that\nincorporates pseudo-implicit feedback for enriching the original sparse\nimplicit feedback dataset. Three of the unique characteristics of PsiRec are:\n(i) it views user-item interactions as a bipartite graph and models\npseudo-implicit feedback from this perspective; (ii) its random walks-based\napproach extracts graph structure information from this bipartite graph, toward\nestimating pseudo-implicit feedback; and (iii) it adopts a Skip-gram inspired\nmeasure of confidence in pseudo-implicit feedback that captures the pointwise\nmutual information between users and items. This pseudo-implicit feedback is\nultimately incorporated into a new latent factor model to estimate user\npreference in cases of extreme sparsity. PsiRec results in improvements of\n21.5% and 22.7% in terms of Precision@10 and Recall@10 over state-of-the-art\nCollaborative Denoising Auto-Encoders. Our implementation is available at\nhttps://github.com/heyunh2015/PsiRecICDM2018.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 03:28:40 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["He", "Yun", ""], ["Chen", "Haochen", ""], ["Zhu", "Ziwei", ""], ["Caverlee", "James", ""]]}, {"id": "1901.00671", "submitter": "Leila Ben Othman", "authors": "Leila Ben Othman", "title": "Une nouvelle approche de compl\\'etion des valeurs manquantes dans les\n  bases de donn\\'ees", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  When tackling real-life datasets, it is common to face the existence of\nscrambled missing values within data. Considered as 'dirty data', usually it is\nremoved during a pre-processing step. Starting from the fact that 'making up\nthis missing data is better than throwing out it away', we present a new\napproach trying to complete missing data. The main singularity of the\nintroduced approach is that it sheds light on a fruitful synergy between\ngeneric basis of association rules and the topic of missing values handling. In\nfact, beyond interesting compactness rate, such generic association rules make\nit possible to get a considerable reduction of conflicts during the completion\nstep. A new metric called 'Robustness' is also introduced, and aims to select\nthe robust association rule for the completion of a missing value whenever a\nconflict appears. Carried out experiments on benchmark datasets confirm the\nsoundness of our approach. Thus, it reduces conflict during the completion step\nwhile offering a high percentage of correct completion accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 10:30:14 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Othman", "Leila Ben", ""]]}, {"id": "1901.01003", "submitter": "Xiangmin Zhou", "authors": "Xiangmin Zhou, Dong Qin, Xiaolu Lu, Lei Chen, Yanchun Zhang", "title": "Online Social Media Recommendation over Streams", "comments": "This paper appears at 35th IEEE International Conference on Data\n  Engineering (ICDE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As one of the most popular services over online communities, the social\nrecommendation has attracted increasing research efforts recently. Among all\nthe recommendation tasks, an important one is social item recommendation over\nhigh speed social media streams. Existing streaming recommendation techniques\nare not effective for handling social users with diverse interests. Meanwhile,\napproaches for recommending items to a particular user are not efficient when\napplied to a huge number of users over high speed streams. In this paper, we\npropose a novel framework for the social recommendation over streaming\nenvironments. Specifically, we first propose a novel Bi-Layer Hidden Markov\nModel (BiHMM) that adaptively captures the behaviors of social users and their\ninteractions with influential official accounts to predict their long-term and\nshort-term interests. Then, we design a new probabilistic entity matching\nscheme for effectively identifying the relevance score of a streaming item to a\nuser. Following that, we propose a novel indexing scheme called {\\Tree} for\nimproving the efficiency of our solution. Extensive experiments are conducted\nto prove the high performance of our approach in terms of the recommendation\nquality and time cost.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:40:58 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Zhou", "Xiangmin", ""], ["Qin", "Dong", ""], ["Lu", "Xiaolu", ""], ["Chen", "Lei", ""], ["Zhang", "Yanchun", ""]]}, {"id": "1901.01418", "submitter": "Maria Papadopouli", "authors": "Evripidis Tzamousis and Maria Papadopouli", "title": "On hybrid modular recommendation systems for video streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recommendation systems aim to improve the user engagement by recommending\nappropriate personalized content to users, exploiting information about their\npreferences. We propose the enabler, a hybrid recommendation system which\nemploys various machine-learning (ML) algorithms for learning an efficient\ncombination of several recommendation algorithms and selects the best blending\nfor a given input.Specifically, it integrates three layers, namely, the trainer\nwhich trains the underlying recommenders, the blender which determines the most\nefficient combination of the recommenders, and the tester for assessing the\nperformance of the system. The enabler incorporates a variety of recommendation\nalgorithms that span from collaborative filtering and content-based techniques\nto ones based on neural networks. It uses the nested cross validation for\nautomatically selecting the best ML algorithm along with its hyper-parameter\nvalues for the given input, according to a specific metric. The enabler can be\neasily extended to include other recommenders and blenders. The enabler has\nbeen extensively evaluated in the context of video-streaming. It outperforms\nvarious other algorithms, when tested on the Movielens 1M benchmark\ndataset.encouraging results. Moreover For example, it achieves an RMSE of\n0.8206, compared to the state-of-the-art performance of the AutoRec and SVD,\n0.827 and 0.845, respectively. A pilot web-based recommendation system was\ndeveloped and tested in the production environment of a large telecom operator\nin Greece. Volunteer customers of the video-streaming service provided by the\ntelecom operator employed the system in the context of an out-in-the-wild field\nstudy with a post-analysis of the enabler, using the collected ratings of the\npilot, demonstrated that it significantly outperforms several popular\nrecommendation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 14:02:56 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tzamousis", "Evripidis", ""], ["Papadopouli", "Maria", ""]]}, {"id": "1901.01588", "submitter": "Yue Zhao", "authors": "Yue Zhao, Zain Nasrullah, Zheng Li", "title": "PyOD: A Python Toolbox for Scalable Outlier Detection", "comments": "7 pages, 1 figure, version 2 (published in JMLR Volume 20, MLOSS\n  track)", "journal-ref": "Journal of Machine Learning Research (JMLR), 20(96):1-7, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PyOD is an open-source Python toolbox for performing scalable outlier\ndetection on multivariate data. Uniquely, it provides access to a wide range of\noutlier detection algorithms, including established outlier ensembles and more\nrecent neural network-based approaches, under a single, well-documented API\ndesigned for use by both practitioners and researchers. With robustness and\nscalability in mind, best practices such as unit testing, continuous\nintegration, code coverage, maintainability checks, interactive examples and\nparallelization are emphasized as core components in the toolbox's development.\nPyOD is compatible with both Python 2 and 3 and can be installed through Python\nPackage Index (PyPI) or https://github.com/yzhao062/pyod.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 18:29:35 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 19:07:19 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Zhao", "Yue", ""], ["Nasrullah", "Zain", ""], ["Li", "Zheng", ""]]}, {"id": "1901.02053", "submitter": "Anish Acharya", "authors": "Anish Acharya", "title": "Detecting the Trend in Musical Taste over the Decade -- A Novel Feature\n  Extraction Algorithm to Classify Musical Content with Simple Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel feature selection algorithm to classify Songs into\ndifferent groups. Classification of musical content is often a non-trivial job\nand still relatively less explored area. The main idea conveyed in this article\nis to come up with a new feature selection scheme that does the classification\njob elegantly and with high accuracy but with simpler but wisely chosen small\nnumber of features thus being less prone to over-fitting. This uses a very\nbasic general idea about the structure of the audio signal which is generally\nin the shape of a trapezium. So, using this general idea of the Musical\nCommunity we propose three frames to be considered and analyzed for feature\nextraction for each of the audio signal -- opening, stanzas and closing -- and\nit has been established with the help of a lot of experiments that this scheme\nleads to much efficient classification with less complex features in a low\ndimensional feature space thus is also a computationally less expensive method.\nStep by step analysis of feature extraction, feature ranking, dimensionality\nreduction using PCA has been carried in this article. Sequential Forward\nselection (SFS) algorithm is used to explore the most significant features both\nwith the raw Fisher Discriminant Ratio (FDR) and also with the significant\neigen-values after PCA. Also during classification extensive validation and\ncross validation has been done in a monte-carlo manner to ensure validity of\nthe claims.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 03:53:55 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Acharya", "Anish", ""]]}, {"id": "1901.02055", "submitter": "Molka Tounsi Dhouib", "authors": "C\\'edric Lopez (TEXTE), Molka Dhouib (I3S, WIMMICS), Elena Cabrio\n  (WIMMICS), Catherine Faron Zucker (I3S, WIMMICS), Fabien Gandon (UCA,\n  WIMMICS), Fr\\'ed\\'erique Segond", "title": "SMILK, linking natural language and data from the web", "comments": "in French", "journal-ref": "RIA - Revue d'Intelligence Artificielle, 2018", "doi": "10.3166/RIA.32.287-312", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the SMILK Joint Lab, we studied the use of Natural Language\nProcessing to: (1) enrich knowledge bases and link data on the web, and\nconversely (2) use this linked data to contribute to the improvement of text\nanalysis and the annotation of textual content, and to support knowledge\nextraction. The evaluation focused on brand-related information retrieval in\nthe field of cosmetics. This article describes each step of our approach: the\ncreation of ProVoc, an ontology to describe products and brands; the automatic\npopulation of a knowledge base mainly based on ProVoc from heterogeneous\ntextual resources; and the evaluation of an application which that takes the\nform of a browser plugin providing additional knowledge to users browsing the\nweb.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 12:35:19 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Lopez", "C\u00e9dric", "", "TEXTE"], ["Dhouib", "Molka", "", "I3S, WIMMICS"], ["Cabrio", "Elena", "", "WIMMICS"], ["Zucker", "Catherine Faron", "", "I3S, WIMMICS"], ["Gandon", "Fabien", "", "UCA,\n  WIMMICS"], ["Segond", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1901.02296", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dmitry Bogdanov, Kyumin Choi, Xavier Serra", "title": "Using offline metrics and user behavior analysis to combine multiple\n  systems for music recommendation", "comments": null, "journal-ref": "Conference on Recommender Systems (RecSys) 2018, REVEAL Workshop", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are many offline metrics that can be used as a reference for evaluation\nand optimization of the performance of recommender systems. Hybrid\nrecommendation approaches are commonly used to improve some of those metrics by\ncombining different systems. In this work we focus on music recommendation and\npropose a new way to improve recommendations, with respect to a desired metric\nof choice, by combining multiple systems for each user individually based on\ntheir expected performance. Essentially, our approach consists in predicting an\nexpected error that each system will produce for each user based on their\nprevious activity. To this end, we propose to train regression models for\ndifferent metrics predicting the performance of each system based on a number\nof features characterizing previous user behavior in the system. We then use\ndifferent fusion strategies to combine recommendations generated by each\nsystem. Following this approach one can optimize the final hybrid system with\nrespect to the desired metric of choice. As a proof of concept, we conduct\nexperiments combining two recommendation systems, a Matrix Factorization model\nand a popularity-based recommender. We use the data provided by Melon, a Korean\nmusic streaming service, to train and evaluate the performance of the systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 13:26:08 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ferraro", "Andres", ""], ["Bogdanov", "Dmitry", ""], ["Choi", "Kyumin", ""], ["Serra", "Xavier", ""]]}, {"id": "1901.02660", "submitter": "Lakmal Meegahapola", "authors": "Vijini Mallawaarachchi, Lakmal Meegahapola, Roshan Alwis, Eranga\n  Nimalarathna, Dulani Meedeniya, Sampath Jayarathna", "title": "Change Detection and Notification of Webpages: A Survey", "comments": "ACM Computing Surveys", "journal-ref": "Change Detection and Notification of Web Pages: A Survey. ACM\n  Comput. Surv. 53, 1, Article 15 (February 2020), 35 pages", "doi": "10.1145/3369876", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the currently available webpages are dynamic in nature and are\nchanging frequently. New content gets added to webpages and existing content\ngets updated or deleted. Hence, people find it useful to be alert for changes\nin webpages which contain information valuable to them. In the current context,\nkeeping track of these webpages and getting alerts about different changes have\nbecome significantly challenging. Change Detection and Notification (CDN)\nsystems were introduced to automate this monitoring process and notify users\nwhen changes occur in webpages. This survey classifies and analyzes different\naspects of CDN systems and different techniques used for each aspect.\nFurthermore, the survey highlights the current challenges and areas of\nimprovement present within the field of research.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 10:20:40 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 17:29:40 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 10:54:45 GMT"}, {"version": "v4", "created": "Sun, 16 Feb 2020 22:35:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Mallawaarachchi", "Vijini", ""], ["Meegahapola", "Lakmal", ""], ["Alwis", "Roshan", ""], ["Nimalarathna", "Eranga", ""], ["Meedeniya", "Dulani", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "1901.02780", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Ond\\v{r}ej Bojar", "title": "Sentiment Analysis of Czech Texts: An Algorithmic Survey", "comments": "7 pages, 2 figures, 7 tables. Published in proceedings of the 11th\n  International Conference on Agents and Artificial Intelligence - ICAART 2019\n  and can be found at\n  http://www.scitepress.org/PublicationsDetail.aspx?ID=1InVq6xKdwE=&t=1 The\n  paper content is identical to the previous one, only updated publication\n  metadata", "journal-ref": null, "doi": "10.5220/0007695709730979", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the area of online communication, commerce and transactions, analyzing\nsentiment polarity of texts written in various natural languages has become\ncrucial. While there have been a lot of contributions in resources and studies\nfor the English language, \"smaller\" languages like Czech have not received much\nattention. In this survey, we explore the effectiveness of many existing\nmachine learning algorithms for sentiment analysis of Czech Facebook posts and\nproduct reviews. We report the sets of optimal parameter values for each\nalgorithm and the scores in both datasets. We finally observe that support\nvector machines are the best classifier and efforts to increase performance\neven more with bagging, boosting or voting ensemble schemes fail to do so.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 15:30:39 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 16:01:10 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1901.03136", "submitter": "Franziska Horn", "authors": "Lea Helmers, Franziska Horn, Franziska Biegler, Tim Oppermann,\n  Klaus-Robert M\\\"uller", "title": "Automating the search for a patent's prior art with a full text\n  similarity search", "comments": null, "journal-ref": "PLoS ONE 14(3): e0212103 (2019)", "doi": "10.1371/journal.pone.0212103", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More than ever, technical inventions are the symbol of our society's advance.\nPatents guarantee their creators protection against infringement. For an\ninvention being patentable, its novelty and inventiveness have to be assessed.\nTherefore, a search for published work that describes similar inventions to a\ngiven patent application needs to be performed. Currently, this so-called\nsearch for prior art is executed with semi-automatically composed keyword\nqueries, which is not only time consuming, but also prone to errors. In\nparticular, errors may systematically arise by the fact that different keywords\nfor the same technical concepts may exist across disciplines. In this paper, a\nnovel approach is proposed, where the full text of a given patent application\nis compared to existing patents using machine learning and natural language\nprocessing techniques to automatically detect inventions that are similar to\nthe one described in the submitted document. Various state-of-the-art\napproaches for feature extraction and document comparison are evaluated. In\naddition to that, the quality of the current search process is assessed based\non ratings of a domain expert. The evaluation results show that our automated\napproach, besides accelerating the search process, also improves the search\nresults for prior art with respect to their quality.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 13:04:25 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 19:45:29 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Helmers", "Lea", ""], ["Horn", "Franziska", ""], ["Biegler", "Franziska", ""], ["Oppermann", "Tim", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1901.03298", "submitter": "Kashif Ahmad", "authors": "Kashif Ahmad, Konstantin Pogorelov, Michael Riegler, Olga Ostroukhova,\n  Paal Halvorsen, Nicola Conci, Rozenn Dahyot", "title": "Automatic detection of passable roads after floods in remote sensed and\n  social media data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of floods classification and floods\naftermath detection utilizing both social media and satellite imagery.\nAutomatic detection of disasters such as floods is still a very challenging\ntask. The focus lies on identifying passable routes or roads during floods. Two\nnovel solutions are presented, which were developed for two corresponding tasks\nat the MediaEval 2018 benchmarking challenge. The tasks are (i) identification\nof images providing evidence for road passability and (ii) differentiation and\ndetection of passable and non-passable roads in images from two complementary\nsources of information. For the first challenge, we mainly rely on object and\nscene-level features extracted through multiple deep models pre-trained on the\nImageNet and Places datasets. The object and scene-level features are then\ncombined using early, late and double fusion techniques. To identify whether or\nnot it is possible for a vehicle to pass a road in satellite images, we rely on\nConvolutional Neural Networks and a transfer learning-based classification\napproach. The evaluation of the proposed methods are carried out on the\nlarge-scale datasets provided for the benchmark competition. The results\ndemonstrate significant improvement in the performance over the recent\nstate-of-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:57:19 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Ahmad", "Kashif", ""], ["Pogorelov", "Konstantin", ""], ["Riegler", "Michael", ""], ["Ostroukhova", "Olga", ""], ["Halvorsen", "Paal", ""], ["Conci", "Nicola", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1901.03489", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Bruce Croft, Yongfeng Zhang, Johanne R. Trippas and\n  Minghui Qiu", "title": "User Intent Prediction in Information-seeking Conversations", "comments": "Accepted to CHIIR 2019", "journal-ref": null, "doi": "10.1145/3295750.3298924", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational assistants are being progressively adopted by the general\npopulation. However, they are not capable of handling complicated\ninformation-seeking tasks that involve multiple turns of information exchange.\nDue to the limited communication bandwidth in conversational search, it is\nimportant for conversational assistants to accurately detect and predict user\nintent in information-seeking conversations. In this paper, we investigate two\naspects of user intent prediction in an information-seeking setting. First, we\nextract features based on the content, structural, and sentiment\ncharacteristics of a given utterance, and use classic machine learning methods\nto perform user intent prediction. We then conduct an in-depth feature\nimportance analysis to identify key features in this prediction task. We find\nthat structural features contribute most to the prediction performance. Given\nthis finding, we construct neural classifiers to incorporate context\ninformation and achieve better performance without feature engineering. Our\nfindings can provide insights into the important factors and effective methods\nof user intent prediction in information-seeking conversations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 05:53:13 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Croft", "Bruce", ""], ["Zhang", "Yongfeng", ""], ["Trippas", "Johanne R.", ""], ["Qiu", "Minghui", ""]]}, {"id": "1901.03491", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Bruce Croft, Falk Scholer and Yongfeng Zhang", "title": "Answer Interaction in Non-factoid Question Answering Systems", "comments": "Accepted to CHIIR 2019", "journal-ref": null, "doi": "10.1145/3295750.3298946", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval systems are evolving from document retrieval to answer\nretrieval. Web search logs provide large amounts of data about how people\ninteract with ranked lists of documents, but very little is known about\ninteraction with answer texts. In this paper, we use Amazon Mechanical Turk to\ninvestigate three answer presentation and interaction approaches in a\nnon-factoid question answering setting. We find that people perceive and react\nto good and bad answers very differently, and can identify good answers\nrelatively quickly. Our results provide the basis for further investigation of\neffective answer interaction and feedback methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 06:02:22 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 05:09:48 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Croft", "Bruce", ""], ["Scholer", "Falk", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "1901.03526", "submitter": "Md Saiful Islam", "authors": "Humayun Kayesh, Md. Saiful Islam and Junhu Wang", "title": "On Event Causality Detection in Tweets", "comments": null, "journal-ref": "Griffith University Technical Report, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Twitter has become a great source of user-generated information\nabout events. Very often people report causal relationships between events in\ntheir tweets. Automatic detection of causality information in these events\nmight play an important role in predictive event analytics. Existing approaches\ninclude both rule-based and data-driven supervised methods. However, it is\nchallenging to correctly identify event causality using only linguistic rules\ndue to the highly unstructured nature and grammatical incorrectness of social\nmedia short text such as tweets. Also, it is difficult to develop a data-driven\nsupervised method for event causality detection in tweets due to insufficient\ncontextual information. This paper proposes a novel event context word\nextension technique based on background knowledge. To demonstrate the\neffectiveness of our proposed event context word extension technique, we\ndevelop a feed-forward neural network based approach to detect event causality\nfrom tweets. Extensive experiments demonstrate the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 09:39:55 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Kayesh", "Humayun", ""], ["Islam", "Md. Saiful", ""], ["Wang", "Junhu", ""]]}, {"id": "1901.03756", "submitter": "Esube Bekele", "authors": "Esube Bekele and Wallace Lawson", "title": "The Deeper, the Better: Analysis of Person Attributes Recognition", "comments": "8 pages, 34 png figures and 1 pdf figure, uses FG2019.sty, submitted\n  to FG2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In person attributes recognition, we describe a person in terms of their\nappearance. Typically, this includes a wide range of traits including age,\ngender, clothing, and footwear. Although this could be used in a wide variety\nof scenarios, it generally is applied to video surveillance, where attribute\nrecognition is impacted by low resolution, and other issues such as variable\npose, occlusion and shadow. Recent approaches have used deep convolutional\nneural networks (CNNs) to improve the accuracy in person attribute recognition.\nHowever, many of these networks are relatively shallow and it is unclear to\nwhat extent they use contextual cues to improve classification accuracy. In\nthis paper, we propose deeper methods for person attribute recognition.\nInterpreting the reasons behind the classification is highly important, as it\ncan provide insight into how the classifier is making decisions. Interpretation\nsuggests that deeper networks generally take more contextual information into\nconsideration, which helps improve classification accuracy and\ngeneralizability. We present experimental analysis and results for whole body\nattributes using the PA-100K and PETA datasets and facial attributes using the\nCelebA dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 21:52:57 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Bekele", "Esube", ""], ["Lawson", "Wallace", ""]]}, {"id": "1901.03888", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Maurizio Morisio", "title": "Hybrid Recommender Systems: A Systematic Literature Review", "comments": "38 pages, 9 figures, 14 tables. The final authenticated version is\n  available online at\n  https://content.iospress.com/articles/intelligent-data-analysis/ida163209", "journal-ref": "Intelligent Data Analysis, vol. 21, no. 6, pp. 1487-1524, 2017", "doi": "10.3233/IDA-163209", "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems are software tools used to generate and provide\nsuggestions for items and other entities to the users by exploiting various\nstrategies. Hybrid recommender systems combine two or more recommendation\nstrategies in different ways to benefit from their complementary advantages.\nThis systematic literature review presents the state of the art in hybrid\nrecommender systems of the last decade. It is the first quantitative review\nwork completely focused in hybrid recommenders. We address the most relevant\nproblems considered and present the associated data mining and recommendation\ntechniques used to overcome them. We also explore the hybridization classes\neach hybrid recommender belongs to, the application domains, the evaluation\nprocess and proposed future research directions. Based on our findings, most of\nthe studies combine collaborative filtering with another technique often in a\nweighted way. Also cold-start and data sparsity are the two traditional and top\nproblems being addressed in 23 and 22 studies each, while movies and movie\ndatasets are still widely used by most of the authors. As most of the studies\nare evaluated by comparisons with similar methods using accuracy metrics,\nproviding more credible and user oriented evaluations remains a typical\nchallenge. Besides this, newer challenges were also identified such as\nresponding to the variation of user context, evolving user tastes or providing\ncross-domain recommendations. Being a hot topic, hybrid recommenders represent\na good basis with which to respond accordingly by exploring newer opportunities\nsuch as contextualizing recommendations, involving parallel hybrid algorithms,\nprocessing larger datasets, etc.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 18:12:44 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1901.04085", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Kyunghyun Cho", "title": "Passage Re-ranking with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural models pretrained on a language modeling task, such as ELMo\n(Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et\nal., 2018), have achieved impressive results on various natural language\nprocessing tasks such as question-answering and natural language inference. In\nthis paper, we describe a simple re-implementation of BERT for query-based\npassage re-ranking. Our system is the state of the art on the TREC-CAR dataset\nand the top entry in the leaderboard of the MS MARCO passage retrieval task,\noutperforming the previous state of the art by 27% (relative) in MRR@10. The\ncode to reproduce our results is available at\nhttps://github.com/nyu-dl/dl4marco-bert\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 23:27:58 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 14:05:34 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 02:25:25 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 22:04:21 GMT"}, {"version": "v5", "created": "Tue, 14 Apr 2020 14:57:40 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1901.04216", "submitter": "Klesti Hoxha", "authors": "Klesti Hoxha and Artur Baxhaku", "title": "Albanian Language Identification in Text Documents", "comments": null, "journal-ref": "Buletini i Shkencave te Natyres, Vol. 23, 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the accuracy of standard and state-of-the-art\nlanguage identification methods in identifying Albanian in written text\ndocuments. A dataset consisting of news articles written in Albanian has been\nconstructed for this purpose. We noticed a considerable decrease of accuracy\nwhen using test documents that miss the Albanian alphabet letters \" \\\"E \" and \"\n\\c{C} \" and created a custom training corpus that solved this problem by\nachieving an accuracy of more than 99%. Based on our experiments, the most\nperforming language identification methods for Albanian use a na\\\"ive Bayes\nclassifier and n-gram based classification features.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 10:05:52 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Hoxha", "Klesti", ""], ["Baxhaku", "Artur", ""]]}, {"id": "1901.04268", "submitter": "Zehang Lin", "authors": "Zhenguo Yang, Zehang Lin, Peipei Kang, Jianming Lv, Qing Li and Wenyin\n  Liu", "title": "Learning Shared Semantic Space with Correlation Alignment for\n  Cross-modal Event Retrieval", "comments": "22 pages, submitted to ACM Transactions on Multimedia Computing\n  Communications and Applications(ACM TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn shared semantic space with correlation\nalignment (${S}^{3}CA$) for multimodal data representations, which aligns\nnonlinear correlations of multimodal data distributions in deep neural networks\ndesigned for heterogeneous data. In the context of cross-modal (event)\nretrieval, we design a neural network with convolutional layers and\nfully-connected layers to extract features for images, including images on\nFlickr-like social media. Simultaneously, we exploit a fully-connected neural\nnetwork to extract semantic features for texts, including news articles from\nnews media. In particular, nonlinear correlations of layer activations in the\ntwo neural networks are aligned with correlation alignment during the joint\ntraining of the networks. Furthermore, we project the multimodal data into a\nshared semantic space for cross-modal (event) retrieval, where the distances\nbetween heterogeneous data samples can be measured directly. In addition, we\ncontribute a Wiki-Flickr Event dataset, where the multimodal data samples are\nnot describing each other in pairs like the existing paired datasets, but all\nof them are describing semantic events. Extensive experiments conducted on both\npaired and unpaired datasets manifest the effectiveness of ${S}^{3}CA$,\noutperforming the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 12:48:53 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 13:19:55 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 00:31:51 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Yang", "Zhenguo", ""], ["Lin", "Zehang", ""], ["Kang", "Peipei", ""], ["Lv", "Jianming", ""], ["Li", "Qing", ""], ["Liu", "Wenyin", ""]]}, {"id": "1901.04277", "submitter": "Kashif Ahmad", "authors": "Naina Said, Kashif Ahmad, Michael Regular, Konstantin Pogorelov, Laiq\n  Hassan, Nasir Ahmad, Nicola Conci", "title": "Natural Disasters Detection in Social Media and Satellite imagery: a\n  survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of natural disaster-related multimedia content got great\nattention in recent years. Being one of the most important sources of\ninformation, social media have been crawled over the years to collect and\nanalyze disaster-related multimedia content. Satellite imagery has also been\nwidely explored for disasters analysis. In this paper, we survey the existing\nliterature on disaster detection and analysis of the retrieved information from\nsocial media and satellites. Literature on disaster detection and analysis of\nrelated multimedia content on the basis of the nature of the content can be\ncategorized into three groups, namely (i) disaster detection in text; (ii)\nanalysis of disaster-related visual content from social media; and (iii)\ndisaster detection in satellite imagery. We extensively review different\napproaches proposed in these three domains. Furthermore, we also review\nbenchmarking datasets available for the evaluation of disaster detection\nframeworks. Moreover, we provide a detailed discussion on the insights obtained\nfrom the literature review, and identify future trends and challenges, which\nwill provide an important starting point for the researchers in the field.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 13:06:25 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Said", "Naina", ""], ["Ahmad", "Kashif", ""], ["Regular", "Michael", ""], ["Pogorelov", "Konstantin", ""], ["Hassan", "Laiq", ""], ["Ahmad", "Nasir", ""], ["Conci", "Nicola", ""]]}, {"id": "1901.04321", "submitter": "Thom Lake", "authors": "Thom Lake, Sinead A. Williamson, Alexander T. Hawk, Christopher C.\n  Johnson, Benjamin P. Wing", "title": "Large-scale Collaborative Filtering with Product Embeddings", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of machine learning techniques to large-scale personalized\nrecommendation problems is a challenging task. Such systems must make sense of\nenormous amounts of implicit feedback in order to understand user preferences\nacross numerous product categories. This paper presents a deep learning based\nsolution to this problem within the collaborative filtering with implicit\nfeedback framework. Our approach combines neural attention mechanisms, which\nallow for context dependent weighting of past behavioral signals, with\nrepresentation learning techniques to produce models which obtain extremely\nhigh coverage, can easily incorporate new information as it becomes available,\nand are computationally efficient. Offline experiments demonstrate significant\nperformance improvements when compared to several alternative methods from the\nliterature. Results from an online setting show that the approach compares\nfavorably with current production techniques used to produce personalized\nproduct recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 17:28:59 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Lake", "Thom", ""], ["Williamson", "Sinead A.", ""], ["Hawk", "Alexander T.", ""], ["Johnson", "Christopher C.", ""], ["Wing", "Benjamin P.", ""]]}, {"id": "1901.04375", "submitter": "Bahareh Sarrafzadeh", "authors": "Bahareh Sarrafzadeh, Ahmed Hassan Awadallah, Christopher H. Lin,\n  Chia-Jung Lee, Milad Shokouhi and Susan T. Dumais", "title": "Characterizing and Predicting Email Deferral Behavior", "comments": null, "journal-ref": "WSDM 2019", "doi": "10.1145/3289600.3291028", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email triage involves going through unhandled emails and deciding what to do\nwith them. This familiar process can become increasingly challenging as the\nnumber of unhandled email grows. During a triage session, users commonly defer\nhandling emails that they cannot immediately deal with to later. These deferred\nemails, are often related to tasks that are postponed until the user has more\ntime or the right information to deal with them. In this paper, through\nqualitative interviews and a large-scale log analysis, we study when and what\nenterprise email users tend to defer. We found that users are more likely to\ndefer emails when handling them involves replying, reading carefully, or\nclicking on links and attachments. We also learned that the decision to defer\nemails depends on many factors such as user's workload and the importance of\nthe sender. Our qualitative results suggested that deferring is very common,\nand our quantitative log analysis confirms that 12% of triage sessions and 16%\nof daily active users had at least one deferred email on weekdays. We also\ndiscuss several deferral strategies such as marking emails as unread and\nflagging that are reported by our interviewees, and illustrate how such\npatterns can be also observed in user logs. Inspired by the characteristics of\ndeferred emails and contextual factors involved in deciding if an email should\nbe deferred, we train a classifier for predicting whether a recently triaged\nemail is actually deferred. Our experimental results suggests that deferral can\nbe classified with modest effectiveness. Overall, our work provides novel\ninsights about how users handle their emails and how deferral can be modeled.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:09:53 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sarrafzadeh", "Bahareh", ""], ["Awadallah", "Ahmed Hassan", ""], ["Lin", "Christopher H.", ""], ["Lee", "Chia-Jung", ""], ["Shokouhi", "Milad", ""], ["Dumais", "Susan T.", ""]]}, {"id": "1901.04672", "submitter": "Hye Young Paik", "authors": "Rahul Anand, Hye-Young Paik, Cheng Wang", "title": "Integrating and querying similar tables from PDF documents using deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amount of public data produced by enterprises are in semi-structured\nPDF form. Tabular data extraction from reports and other published data in PDF\nformat is of interest for various data consolidation purposes such as analysing\nand aggregating financial reports of a company. Queries into the structured\ntabular data in PDF format are normally processed in an unstructured manner\nthrough means like text-match. This is mainly due to that the binary format of\nPDF documents is optimized for layout and rendering and do not have great\nsupport for automated parsing of data. Moreover, even the same table type in\nPDF files varies in schema, row or column headers, which makes it difficult for\na query plan to cover all relevant tables. This paper proposes a deep learning\nbased method to enable SQL-like query and analysis of financial tables from\nannual reports in PDF format. This is achieved through table type\nclassification and nearest row search. We demonstrate that using word embedding\ntrained on Google news for header match clearly outperforms the text-match\nbased approach in traditional database. We also introduce a practical system\nthat uses this technology to query and analyse finance tables in PDF documents\nfrom various sources.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 06:20:59 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Anand", "Rahul", ""], ["Paik", "Hye-Young", ""], ["Wang", "Cheng", ""]]}, {"id": "1901.04704", "submitter": "Zhi-Hong Deng", "authors": "Zhi-Hong Deng, Ling Huang, Chang-Dong Wang, Jian-Huang Lai, Philip S.\n  Yu", "title": "DeepCF: A Unified Framework of Representation Learning and Matching\n  Function Learning in Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, recommendation can be viewed as a matching problem, i.e., match\nproper items for proper users. However, due to the huge semantic gap between\nusers and items, it's almost impossible to directly match users and items in\ntheir initial representation spaces. To solve this problem, many methods have\nbeen studied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning-based CF methods try to map users and items\ninto a common representation space. In this case, the higher similarity between\na user and an item in that space implies they match better. Matching function\nlearning-based CF methods try to directly learn the complex matching function\nthat maps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the limited\nexpressiveness of dot product and the weakness in capturing low-rank relations\nrespectively. To this end, we propose a general framework named DeepCF, short\nfor Deep Collaborative Filtering, to combine the strengths of the two types of\nmethods and overcome such flaws. Extensive experiments on four publicly\navailable datasets demonstrate the effectiveness of the proposed DeepCF\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 08:25:00 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Deng", "Zhi-Hong", ""], ["Huang", "Ling", ""], ["Wang", "Chang-Dong", ""], ["Lai", "Jian-Huang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1901.04993", "submitter": "Xinli Yu T", "authors": "Xinli Yu, Zheng Chen, Wei-Shih Yang, Xiaohua Hu, Erjia Yan", "title": "Large-Scale Joint Topic, Sentiment & User Preference Analysis for Online\n  Reviews", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2017.8258000", "report-no": null, "categories": "cs.IR cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a non-trivial reconstruction of a previous joint\ntopic-sentiment-preference review model TSPRA with stick-breaking\nrepresentation under the framework of variational inference (VI) and stochastic\nvariational inference (SVI). TSPRA is a Gibbs Sampling based model that solves\ntopics, word sentiments and user preferences altogether and has been shown to\nachieve good performance, but for large data set it can only learn from a\nrelatively small sample. We develop the variational models vTSPRA and svTSPRA\nto improve the time use, and our new approach is capable of processing millions\nof reviews. We rebuild the generative process, improve the rating regression,\nsolve and present the coordinate-ascent updates of variational parameters, and\nshow the time complexity of each iteration is theoretically linear to the\ncorpus size, and the experiments on Amazon data sets show it converges faster\nthan TSPRA and attains better results given the same amount of time. In\naddition, we tune svTSPRA into an online algorithm ovTSPRA that can monitor\noscillations of sentiment and preference overtime. Some interesting\nfluctuations are captured and possible explanations are provided. The results\ngive strong visual evidence that user preference is better treated as an\nindependent factor from sentiment.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:33:20 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Yu", "Xinli", ""], ["Chen", "Zheng", ""], ["Yang", "Wei-Shih", ""], ["Hu", "Xiaohua", ""], ["Yan", "Erjia", ""]]}, {"id": "1901.05052", "submitter": "Jonathan Dumas", "authors": "Jonathan Dumas and Bertrand Corn\\'elusse", "title": "Classification of load forecasting studies by forecasting problem to\n  select load forecasting techniques and methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key contribution of this paper is to propose a classification into two\ndimensions of the load forecasting studies to decide which forecasting tools to\nuse in which case. This classification aims to provide a synthetic view of the\nrelevant forecasting techniques and methodologies by forecasting problem. In\naddition, the key principles of the main techniques and methodologies used are\nsummarized along with the reviews of these papers.\n  The classification process relies on two couples of parameters that define a\nforecasting problem. Each article is classified with key information about the\ndataset used and the forecasting tools implemented: the forecasting techniques\n(probabilistic or deterministic) and methodologies, the data cleansing\ntechniques, and the error metrics.\n  The process to select the articles reviewed in this paper was conducted into\ntwo steps. First, a set of load forecasting studies was built based on relevant\nload forecasting reviews and forecasting competitions. The second step\nconsisted in selecting the most relevant studies of this set based on the\nfollowing criteria: the quality of the description of the forecasting\ntechniques and methodologies implemented, the description of the results, and\nthe contributions.\n  This paper can be read in two passes. The first one by identifying the\nforecasting problem of interest to select the corresponding class into one of\nthe four classification tables. Each one references all the articles classified\nacross a forecasting horizon. They provide a synthetic view of the forecasting\ntools used by articles addressing similar forecasting problems. Then, a second\nlevel composed of four Tables summarizes key information about the forecasting\ntools and the results of these studies. The second pass consists in reading the\nkey principles of the main techniques and methodologies of interest and the\nreviews of the articles.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 10:03:16 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 15:05:34 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Dumas", "Jonathan", ""], ["Corn\u00e9lusse", "Bertrand", ""]]}, {"id": "1901.05227", "submitter": "Amit Awekar", "authors": "Manash Pratim Barman and Kavish Dahekar and Abhinav Anshuman and Amit\n  Awekar", "title": "It's Only Words And Words Are All I Have", "comments": "Accepted for ECIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The central idea of this paper is to demonstrate the strength of lyrics for\nmusic mining and natural language processing (NLP) tasks using the distributed\nrepresentation paradigm. For music mining, we address two prediction tasks for\nsongs: genre and popularity. Existing works for both these problems have two\nmajor bottlenecks. First, they represent lyrics using handcrafted features that\nrequire intricate knowledge of language and music. Second, they consider lyrics\nas a weak indicator of genre and popularity. We overcome both the bottlenecks\nby representing lyrics using distributed representation. In our work, genre\nidentification is a multi-class classification task whereas popularity\nprediction is a binary classification task. We achieve an F1 score of around\n0.6 for both the tasks using only lyrics. Distributed representation of words\nis now heavily used for various NLP algorithms. We show that lyrics can be used\nto improve the quality of this representation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 10:58:22 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Barman", "Manash Pratim", ""], ["Dahekar", "Kavish", ""], ["Anshuman", "Abhinav", ""], ["Awekar", "Amit", ""]]}, {"id": "1901.05497", "submitter": "Efi Karra Taniskidou", "authors": "Efi Karra Taniskidou, George Papadakis, George Giannakopoulos, Manolis\n  Koubarakis", "title": "Comparative Analysis of Content-based Personalized Microblog\n  Recommendations [Experiments and Analysis]", "comments": "15 pages, EDBT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging platforms constitute a popular means of real-time communication\nand information sharing. They involve such a large volume of user-generated\ncontent that their users suffer from an information deluge. To address it,\nnumerous recommendation methods have been proposed to organize the posts a user\nreceives according to her interests. The content-based methods typically build\na text-based model for every individual user to capture her tastes and then\nrank the posts in her timeline according to their similarity with that model.\nEven though content-based methods have attracted lots of interest in the data\nmanagement community, there is no comprehensive evaluation of the main factors\nthat affect their performance. These are: (i) the representation model that\nconverts an unstructured text into a structured representation that elucidates\nits characteristics, (ii) the source of the microblog posts that compose the\nuser models, and (iii) the type of user's posting activity. To cover this gap,\nwe systematically examine the performance of 9 state-of-the-art representation\nmodels in combination with 13 representation sources and 3 user types over a\nlarge, real dataset from Twitter comprising 60 users. We also consider a wide\nrange of 223 plausible configurations for the representation models in order to\nassess their robustness with respect to their internal parameters. To\nfacilitate the interpretation of our experimental results, we introduce a novel\ntaxonomy of representation models. Our analysis provides novel insights into\nthe performance and functionality of the main factors determining the\nperformance of content-based recommendation in microblogs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 19:14:59 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Taniskidou", "Efi Karra", ""], ["Papadakis", "George", ""], ["Giannakopoulos", "George", ""], ["Koubarakis", "Manolis", ""]]}, {"id": "1901.05743", "submitter": "Icaro Cavalcante Dourado", "authors": "Icaro Cavalcante Dourado, Daniel Carlos Guimar\\~aes Pedronette,\n  Ricardo da Silva Torres", "title": "Unsupervised Graph-based Rank Aggregation for Improved Retrieval", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipm.2019.03.008", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust and comprehensive graph-based rank aggregation\napproach, used to combine results of isolated ranker models in retrieval tasks.\nThe method follows an unsupervised scheme, which is independent of how the\nisolated ranks are formulated. Our approach is able to combine arbitrary\nmodels, defined in terms of different ranking criteria, such as those based on\ntextual, image or hybrid content representations.\n  We reformulate the ad-hoc retrieval problem as a document retrieval based on\nfusion graphs, which we propose as a new unified representation model capable\nof merging multiple ranks and expressing inter-relationships of retrieval\nresults automatically. By doing so, we claim that the retrieval system can\nbenefit from learning the manifold structure of datasets, thus leading to more\neffective results. Another contribution is that our graph-based aggregation\nformulation, unlike existing approaches, allows for encapsulating contextual\ninformation encoded from multiple ranks, which can be directly used for\nranking, without further computations and post-processing steps over the\ngraphs. Based on the graphs, a novel similarity retrieval score is formulated\nusing an efficient computation of minimum common subgraphs. Finally, another\nbenefit over existing approaches is the absence of hyperparameters.\n  A comprehensive experimental evaluation was conducted considering diverse\nwell-known public datasets, composed of textual, image, and multimodal\ndocuments. Performed experiments demonstrate that our method reaches top\nperformance, yielding better effectiveness scores than state-of-the-art\nbaseline methods and promoting large gains over the rankers being fused, thus\ndemonstrating the successful capability of the proposal in representing queries\nbased on a unified graph-based model of rank fusions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 11:55:04 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 23:09:28 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Dourado", "Icaro Cavalcante", ""], ["Pedronette", "Daniel Carlos Guimar\u00e3es", ""], ["Torres", "Ricardo da Silva", ""]]}, {"id": "1901.06125", "submitter": "Dawei Chen", "authors": "Dawei Chen, Cheng Soon Ong, Aditya Krishna Menon", "title": "Cold-start Playlist Recommendation with Multitask Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Playlist recommendation involves producing a set of songs that a user might\nenjoy. We investigate this problem in three cold-start scenarios: (i) cold\nplaylists, where we recommend songs to form new personalised playlists for an\nexisting user; (ii) cold users, where we recommend songs to form new playlists\nfor a new user; and (iii) cold songs, where we recommend newly released songs\nto extend users' existing playlists. We propose a flexible multitask learning\nmethod to deal with all three settings. The method learns from user-curated\nplaylists, and encourages songs in a playlist to be ranked higher than those\nthat are not by minimising a bipartite ranking loss. Inspired by an equivalence\nbetween bipartite ranking and binary classification, we show how one can\nefficiently approximate an optimal solution of the multitask learning objective\nby minimising a classification loss. Empirical results on two real playlist\ndatasets show the proposed approach has good performance for cold-start\nplaylist recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 08:14:27 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Chen", "Dawei", ""], ["Ong", "Cheng Soon", ""], ["Menon", "Aditya Krishna", ""]]}, {"id": "1901.06168", "submitter": "Jan Trienes", "authors": "Jan Trienes, Krisztian Balog", "title": "Identifying Unclear Questions in Community Question Answering Websites", "comments": "Proceedings of the 41th European Conference on Information Retrieval\n  (ECIR '19), 2019", "journal-ref": null, "doi": "10.1007/978-3-030-15712-8_18", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thousands of complex natural language questions are submitted to community\nquestion answering websites on a daily basis, rendering them as one of the most\nimportant information sources these days. However, oftentimes submitted\nquestions are unclear and cannot be answered without further clarification\nquestions by expert community members. This study is the first to investigate\nthe complex task of classifying a question as clear or unclear, i.e., if it\nrequires further clarification. We construct a novel dataset and propose a\nclassification approach that is based on the notion of similar questions. This\napproach is compared to state-of-the-art text classification baselines. Our\nmain finding is that the similar questions approach is a viable alternative\nthat can be used as a stepping stone towards the development of supportive user\ninterfaces for question formulation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 10:29:20 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Trienes", "Jan", ""], ["Balog", "Krisztian", ""]]}, {"id": "1901.06257", "submitter": "Jun Suzuki", "authors": "Jun Suzuki, Yoshihiko Suhara, Hiroyuki Toda, Kyosuke Nishida", "title": "Personalized Visited-POI Assignment to Individual Raw GPS Trajectories", "comments": "31 pages, 10 figures", "journal-ref": "ACM Transactions on Spatial Algorithms and Systems (TSAS) Volume 5\n  Issue 3, September 2019", "doi": "10.1145/3317667", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge discovery from GPS trajectory data is an important topic in several\nscientific areas, including data mining, human behavior analysis, and user\nmodeling. This paper proposes a task that assigns personalized visited-POIs.\nIts goal is to estimate fine-grained and pre-defined locations (i.e., points of\ninterest (POI)) that are actually visited by users and assign visited-location\ninformation to the corresponding span of their (personal) GPS trajectories. We\nalso introduce a novel algorithm to solve this assignment task. First, we\nexhaustively extract stay-points as candidates for significant locations using\na variant of a conventional stay-point extraction method. Then we select\nsignificant locations and simultaneously assign visited-POIs to them by\nconsidering various aspects, which we formulate in integer linear programming.\nExperimental results conducted on an actual user dataset show that our method\nachieves higher accuracy in the visited-POI assignment task than the various\ncascaded procedures of conventional methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:25:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Suzuki", "Jun", ""], ["Suhara", "Yoshihiko", ""], ["Toda", "Hiroyuki", ""], ["Nishida", "Kyosuke", ""]]}, {"id": "1901.06274", "submitter": "Jyoti Prakash Singh", "authors": "Sunil Saumya, Jyoti Prakash Singh, Abdullah Mohammed Baabdullah,\n  Nripendra P. Rana, Yogesh k. Dwivedi", "title": "Ranking Online Consumer Reviews", "comments": null, "journal-ref": "Electronic Commerce Research and Applications, 2018", "doi": "10.1016/j.elerap.2018.03.008", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The product reviews are posted online in the hundreds and even in the\nthousands for some popular products. Handling such a large volume of\ncontinuously generated online content is a challenging task for buyers,\nsellers, and even researchers. The purpose of this study is to rank the\noverwhelming number of reviews using their predicted helpfulness score. The\nhelpfulness score is predicted using features extracted from review text data,\nproduct description data and customer question-answer data of a product using\nrandom-forest classifier and gradient boosting regressor. The system is made to\nclassify the reviews into low or high quality by random-forest classifier. The\nhelpfulness score of the high-quality reviews is only predicted using gradient\nboosting regressor. The helpfulness score of the low-quality reviews is not\ncalculated because they are never going to be in the top k reviews. They are\njust added at the end of the review list to the review-listing website. The\nproposed system provides fair review placement on review listing pages and\nmaking all high-quality reviews visible to customers on the top. The\nexperimental results on data from two popular Indian e-commerce websites\nvalidate our claim, as 3-4 new high-quality reviews are placed in the top ten\nreviews along with 5-6 old reviews based on review helpfulness. Our findings\nindicate that inclusion of features from product description data and customer\nquestion-answer data improves the prediction accuracy of the helpfulness score.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 13:33:21 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Saumya", "Sunil", ""], ["Singh", "Jyoti Prakash", ""], ["Baabdullah", "Abdullah Mohammed", ""], ["Rana", "Nripendra P.", ""], ["Dwivedi", "Yogesh k.", ""]]}, {"id": "1901.07005", "submitter": "Canwen Xu", "authors": "Canwen Xu, Jing Li, Xiangyang Luo, Jiaxin Pei, Chenliang Li and\n  Donghong Ji", "title": "DLocRL: A Deep Learning Pipeline for Fine-Grained Location Recognition\n  and Linking in Tweets", "comments": "7 pages, 4 figures, accepted by The Web Conf (WWW) 2019; final\n  version", "journal-ref": null, "doi": "10.1145/3308558.3313491", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the prevalence of social media and smart devices,\npeople causally reveal their locations such as shops, hotels, and restaurants\nin their tweets. Recognizing and linking such fine-grained location mentions to\nwell-defined location profiles are beneficial for retrieval and recommendation\nsystems. In this paper, we propose DLocRL, a new deep learning pipeline for\nfine-grained location recognition and linking in tweets, and verify its\neffectiveness on a real-world Twitter dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 17:36:19 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 15:48:52 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 14:20:55 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Xu", "Canwen", ""], ["Li", "Jing", ""], ["Luo", "Xiangyang", ""], ["Pei", "Jiaxin", ""], ["Li", "Chenliang", ""], ["Ji", "Donghong", ""]]}, {"id": "1901.07199", "submitter": "Guangneng Hu", "authors": "Guangneng Hu, Yu Zhang, and Qiang Yang", "title": "Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain\n  Collaborative Filtering with Text", "comments": "11 pages, 7 figures, a full version for the WWW 2019 short paper", "journal-ref": "WWW 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is the key technique for recommender systems\n(RSs). CF exploits user-item behavior interactions (e.g., clicks) only and\nhence suffers from the data sparsity issue. One research thread is to integrate\nauxiliary information such as product reviews and news titles, leading to\nhybrid filtering methods. Another thread is to transfer knowledge from other\nsource domains such as improving the movie recommendation with the knowledge\nfrom the book domain, leading to transfer learning methods. In real-world life,\nno single service can satisfy a user's all information needs. Thus it motivates\nus to exploit both auxiliary and source information for RSs in this paper. We\npropose a novel neural model to smoothly enable Transfer Meeting Hybrid (TMH)\nmethods for cross-domain recommendation with unstructured text in an end-to-end\nmanner. TMH attentively extracts useful content from unstructured text via a\nmemory module and selectively transfers knowledge from a source domain via a\ntransfer network. On two real-world datasets, TMH shows better performance in\nterms of three ranking metrics by comparing with various baselines. We conduct\nthorough analyses to understand how the text content and transferred knowledge\nhelp the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 08:05:34 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hu", "Guangneng", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1901.07352", "submitter": "Robin Haunschild", "authors": "Robin Haunschild and Werner Marx", "title": "Discovering seminal works with marker papers", "comments": "26 pages, 7 figures, 4 tables, Keywords: Bibliometrics, RPYS,\n  RPYS-CO, marker paper, seminal papers, historical roots, DFT, Web of Science,\n  Microsoft Academic, CAplus; earlier version was presented at 8th\n  International Workshop on Bibliometric-Enhanced Information Retrieval, BIR\n  2019; Cologne; Germany; 14 April 2019; current version has been accepted for\n  publication in Scientometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometric information retrieval in databases can employ different\nstrategies. Com-monly, queries are performed by searching in title, abstract\nand/or author keywords (author vocabulary). More advanced queries employ\ndatabase keywords to search in a controlled vo-cabulary. Queries based on\nsearch terms can be augmented with their citing papers if a re-search field\ncannot be curtailed by the search query alone. Here, we present another\nstrategy to discover the most important papers of a research field. A marker\npaper is used to reveal the most important works for the relevant community.\nAll papers co-cited with the marker paper are analyzed using reference\npublication year spectroscopy (RPYS). For demonstration of the marker paper\napproach, density functional theory (DFT) is used as a research field.\nCompari-sons between a prior RPYS on a publication set compiled using a\nkeyword-based search in a controlled vocabulary and three different co-citation\nRPYS (RPYS-CO) analyses show very similar results. Similarities and differences\nare discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:45:59 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 14:22:51 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 16:38:58 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 13:40:12 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Haunschild", "Robin", ""], ["Marx", "Werner", ""]]}, {"id": "1901.07555", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Robin Burke and Bamshad Mobasher", "title": "Managing Popularity Bias in Recommender Systems with Personalized\n  Re-ranking", "comments": "arXiv admin note: text overlap with arXiv:1802.05382", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommender systems suffer from popularity bias: popular items are\nrecommended frequently while less popular, niche products, are recommended\nrarely or not at all. However, recommending the ignored products in the `long\ntail' is critical for businesses as they are less likely to be discovered. In\nthis paper, we introduce a personalized diversification re-ranking approach to\nincrease the representation of less popular items in recommendations while\nmaintaining acceptable recommendation accuracy. Our approach is a\npost-processing step that can be applied to the output of any recommender\nsystem. We show that our approach is capable of managing popularity bias more\neffectively, compared with an existing method based on regularization. We also\nexamine both new and existing metrics to measure the coverage of long-tail\nitems in the recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 17:53:39 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 16:24:05 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 15:55:43 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 15:56:11 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1901.07601", "submitter": "Yanshan Wang", "authors": "Sijia Liu, Yanshan Wang, Andrew Wen, Liwei Wang, Na Hong, Feichen\n  Shen, Steven Bedrick, William Hersh, Hongfang Liu", "title": "CREATE: Cohort Retrieval Enhanced by Analysis of Text from Electronic\n  Health Records using OMOP Common Data Model", "comments": null, "journal-ref": null, "doi": "10.2196/17376", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Widespread adoption of electronic health records (EHRs) has\nenabled secondary use of EHR data for clinical research and healthcare\ndelivery. Natural language processing (NLP) techniques have shown promise in\ntheir capability to extract the embedded information in unstructured clinical\ndata, and information retrieval (IR) techniques provide flexible and scalable\nsolutions that can augment the NLP systems for retrieving and ranking relevant\nrecords. Methods: In this paper, we present the implementation of Cohort\nRetrieval Enhanced by Analysis of Text from EHRs (CREATE), a cohort retrieval\nsystem that can execute textual cohort selection queries on both structured and\nunstructured EHR data. CREATE is a proof-of-concept system that leverages a\ncombination of structured queries and IR techniques on NLP results to improve\ncohort retrieval performance while adopting the Observational Medical Outcomes\nPartnership (OMOP) Common Data Model (CDM) to enhance model portability. The\nNLP component empowered by cTAKES is used to extract CDM concepts from textual\nqueries. We design a hierarchical index in Elasticsearch to support CDM concept\nsearch utilizing IR techniques and frameworks. Results: Our case study on 5\ncohort identification queries evaluated using the IR metric, P@5 (Precision at\n5) at both the patient-level and document-level, demonstrates that CREATE\nachieves an average P@5 of 0.90, which outperforms systems using only\nstructured data or only unstructured data with average P@5s of 0.54 and 0.74,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 20:05:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Sijia", ""], ["Wang", "Yanshan", ""], ["Wen", "Andrew", ""], ["Wang", "Liwei", ""], ["Hong", "Na", ""], ["Shen", "Feichen", ""], ["Bedrick", "Steven", ""], ["Hersh", "William", ""], ["Liu", "Hongfang", ""]]}, {"id": "1901.07734", "submitter": "Yunjuan Wang", "authors": "Yunjuan Wang and Theja Tulabandhula", "title": "Thompson Sampling for a Fatigue-aware Online Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider an online recommendation setting, where a platform\nrecommends a sequence of items to its users at every time period. The users\nrespond by selecting one of the items recommended or abandon the platform due\nto fatigue from seeing less useful items. Assuming a parametric stochastic\nmodel of user behavior, which captures positional effects of these items as\nwell as the abandoning behavior of users, the platform's goal is to recommend\nsequences of items that are competitive to the single best sequence of items in\nhindsight, without knowing the true user model a priori. Naively applying a\nstochastic bandit algorithm in this setting leads to an exponential dependence\non the number of items. We propose a new Thompson sampling based algorithm with\nexpected regret that is polynomial in the number of items in this combinatorial\nsetting, and performs extremely well in practice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 05:52:10 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 16:26:08 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wang", "Yunjuan", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1901.07773", "submitter": "Huu-Hiep Nguyen", "authors": "Huu Hiep Nguyen", "title": "Boosting Frequent Itemset Mining via Early Stopping Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining frequent itemsets from a transaction database has emerged as a\nfundamental problem in data mining and committed itself as a building block for\nmany pattern mining tasks. In this paper, we present a general technique to\nreduce support checking time in existing depth-first search generate-and-test\nschemes such as Eclat/dEclat and PrePost+. Our technique allows infrequent\ncandidate itemsets to be detected early. The technique is based on an\nearly-stopping criterion and is general enough to be applicable in many\nfrequent itemset mining algorithms. We have applied the technique to two\nTID-list based schemes (Eclat/dEclat) and one N-list based scheme (PrePost+).\nOur technique has been tested over a variety of datasets and confirmed its\neffectiveness in runtime reduction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 08:43:50 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nguyen", "Huu Hiep", ""]]}, {"id": "1901.07878", "submitter": "Christian Otto", "authors": "Christian Otto and Sebastian Holzki and Ralph Ewerth", "title": "\"Is this an example image?\" -- Predicting the Relative Abstractness\n  Level of Image and Text", "comments": "14 pages, 6 figures, accepted at ECIR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful multimodal search and retrieval requires the automatic\nunderstanding of semantic cross-modal relations, which, however, is still an\nopen research problem. Previous work has suggested the metrics cross-modal\nmutual information and semantic correlation to model and predict cross-modal\nsemantic relations of image and text. In this paper, we present an approach to\npredict the (cross-modal) relative abstractness level of a given image-text\npair, that is whether the image is an abstraction of the text or vice versa.\nFor this purpose, we introduce a new metric that captures this specific\nrelationship between image and text at the Abstractness Level (ABS). We present\na deep learning approach to predict this metric, which relies on an autoencoder\narchitecture that allows us to significantly reduce the required amount of\nlabeled training data. A comprehensive set of publicly available scientific\ndocuments has been gathered. Experimental results on a challenging test set\ndemonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 13:42:02 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Otto", "Christian", ""], ["Holzki", "Sebastian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1901.08079", "submitter": "Asma Ben Abacha", "authors": "Asma Ben Abacha and Dina Demner-Fushman", "title": "A Question-Entailment Approach to Question Answering", "comments": null, "journal-ref": "BMC Bioinformatics 20, 511 (2019)", "doi": "10.1186/s12859-019-3119-4", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in large-scale information retrieval (IR) is to develop\nfine-grained and domain-specific methods to answer natural language questions.\nDespite the availability of numerous sources and datasets for answer retrieval,\nQuestion Answering (QA) remains a challenging problem due to the difficulty of\nthe question understanding and answer extraction tasks. One of the promising\ntracks investigated in QA is to map new questions to formerly answered\nquestions that are `similar'. In this paper, we propose a novel QA approach\nbased on Recognizing Question Entailment (RQE) and we describe the QA system\nand resources that we built and evaluated on real medical questions. First, we\ncompare machine learning and deep learning methods for RQE using different\nkinds of datasets, including textual inference, question similarity and\nentailment in both the open and clinical domains. Second, we combine IR models\nwith the best RQE method to select entailed questions and rank the retrieved\nanswers. To study the end-to-end QA approach, we built the MedQuAD collection\nof 47,457 question-answer pairs from trusted medical sources, that we introduce\nand share in the scope of this paper. Following the evaluation process used in\nTREC 2017 LiveQA, we find that our approach exceeds the best results of the\nmedical task with a 29.8% increase over the best official score. The evaluation\nresults also support the relevance of question entailment for QA and highlight\nthe effectiveness of combining IR and RQE for future QA efforts. Our findings\nalso show that relying on a restricted set of reliable answer sources can bring\na substantial improvement in medical QA.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 19:02:27 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Abacha", "Asma Ben", ""], ["Demner-Fushman", "Dina", ""]]}, {"id": "1901.08203", "submitter": "Sungkyun Chang", "authors": "Sungkyun Chang, Seungjin Lee and Kyogu Lee", "title": "Sequential Skip Prediction with Few-shot in Streamed Music Contents", "comments": "4 pages, ACM International Conference on Web Search and Data Mining\n  (WSDM) Cup 2019 Workshop, February 2019, Melbourne, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an outline of the algorithms submitted for the WSDM Cup\n2019 Spotify Sequential Skip Prediction Challenge (team name: mimbres). In the\nchallenge, complete information including acoustic features and user\ninteraction logs for the first half of a listening session is provided. Our\ngoal is to predict whether the individual tracks in the second half of the\nsession will be skipped or not, only given acoustic features. We proposed two\ndifferent kinds of algorithms that were based on metric learning and sequence\nlearning. The experimental results showed that the sequence learning approach\nperformed significantly better than the metric learning approach. Moreover, we\nconducted additional experiments to find that significant performance gain can\nbe achieved using complete user log information.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 02:51:43 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:24:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chang", "Sungkyun", ""], ["Lee", "Seungjin", ""], ["Lee", "Kyogu", ""]]}, {"id": "1901.08286", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Dong Wang, Yan Zhang", "title": "Neural IR Meets Graph Embedding: A Ranking Model for Product Search", "comments": "A preliminary version of the work to appear in TheWebConf'19\n  (formerly, WWW'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural models for information retrieval are becoming increasingly\npopular. They provide effective approaches for product search due to their\ncompetitive advantages in semantic matching. However, it is challenging to use\ngraph-based features, though proved very useful in IR literature, in these\nneural approaches. In this paper, we leverage the recent advances in graph\nembedding techniques to enable neural retrieval models to exploit\ngraph-structured data for automatic feature extraction. The proposed approach\ncan not only help to overcome the long-tail problem of click-through data, but\nalso incorporate external heterogeneous information to improve search results.\nExtensive experiments on a real-world e-commerce dataset demonstrate\nsignificant improvement achieved by our proposed approach over multiple strong\nbaselines both as an individual retrieval model and as a feature used in\nlearning-to-rank frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:48:16 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Zhang", "Yuan", ""], ["Wang", "Dong", ""], ["Zhang", "Yan", ""]]}, {"id": "1901.08351", "submitter": "Yuan Xia", "authors": "Xia Yuan, Liao xiaoli, Li Shilei, Shi Qinwen, Wu Jinfa, Li Ke", "title": "Extracting PICO elements from RCT abstracts using 1-2gram analysis and\n  multitask classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The core of evidence-based medicine is to read and analyze numerous papers in\nthe medical literature on a specific clinical problem and summarize the\nauthoritative answers to that problem. Currently, to formulate a clear and\nfocused clinical problem, the popular PICO framework is usually adopted, in\nwhich each clinical problem is considered to consist of four parts:\npatient/problem (P), intervention (I), comparison (C) and outcome (O). In this\nstudy, we compared several classification models that are commonly used in\ntraditional machine learning. Next, we developed a multitask classification\nmodel based on a soft-margin SVM with a specialized feature engineering method\nthat combines 1-2gram analysis with TF-IDF analysis. Finally, we trained and\ntested several generic models on an open-source data set from BioNLP 2018. The\nresults show that the proposed multitask SVM classification model based on\n1-2gram TF-IDF features exhibits the best performance among the tested models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 11:14:49 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Yuan", "Xia", ""], ["xiaoli", "Liao", ""], ["Shilei", "Li", ""], ["Qinwen", "Shi", ""], ["Jinfa", "Wu", ""], ["Ke", "Li", ""]]}, {"id": "1901.08406", "submitter": "Bharat Gaind", "authors": "Anusha Holla, Bharat Gaind, Vikas Reddy Katta, Abhishek Kundu, S\n  Kamalesh", "title": "Hybrid NER System for Multi-Source Offer Feeds", "comments": "Published in the Global Journal of Engineering Science and Researches\n  (ISSN 2348 - 8034, Pg. 69-77) after getting accepted in the International\n  Conference on Recent Trends In Computational Engineering and Technologies\n  (ICRTCET'18), May 17-18, 2018, Bengaluru, India. Journal Link -\n  http://www.gjesr.com/ICRTCET-18.html", "journal-ref": "Global Journal of Engineering Science and Researches (ICRTCET-18)\n  (2019) 69-77", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data available across the web is largely unstructured. Offers published by\nmultiple sources like banks, digital wallets, merchants, etc., are one of the\nmost accessed advertising data in today's world. This data gets accessed by\nmillions of people on a daily basis and is easily interpreted by humans, but\nsince it is largely unstructured and diverse, using an algorithmic way to\nextract meaningful information out of these offers is hard. Identifying the\nessential offer entities (for instance, its amount, the product on which the\noffer is applicable, the merchant providing the offer, etc.) from these offers\nplays a vital role in targeting the right customers to improve sales. This work\npresents and evaluates various existing Named Entity Recognizer (NER) models\nwhich can identify the required entities from offer feeds. We also propose a\nnovel Hybrid NER model constructed by two-level stacking of Conditional Random\nField, Bidirectional LSTM and Spacy models at the first level and an SVM\nclassifier at the second. The proposed hybrid model has been tested on offer\nfeeds collected from multiple sources and has shown better performance in the\noffer domain when compared to the existing models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 13:53:04 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 11:07:50 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Holla", "Anusha", ""], ["Gaind", "Bharat", ""], ["Katta", "Vikas Reddy", ""], ["Kundu", "Abhishek", ""], ["Kamalesh", "S", ""]]}, {"id": "1901.08422", "submitter": "Georgios Pitsilis", "authors": "Georgios K. Pitsilis, Heri Ramampiaro, Helge Langseth", "title": "Securing Tag-based recommender systems against profile injection\n  attacks: A comparative study. (Extended Report)", "comments": "20 pages, 5 figures, 4 tables, Extended report of paper presented at\n  \"Late Breaking Results\" poster session, RecSys 2018, October 2-7, Vancouver,\n  BC, Canada", "journal-ref": null, "doi": null, "report-no": "arXiv:1808.10550; arXiv:1809.04106", "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the challenges related to attacks on collaborative\ntagging systems, which often comes in a form of malicious annotations or\nprofile injection attacks. In particular, we study various countermeasures\nagainst two types of such attacks for social tagging systems, the Overload\nattack and the Piggyback attack. The countermeasure schemes studied here\ninclude baseline classifiers such as, Naive Bayes filter and Support Vector\nMachine, as well as a Deep Learning approach. Our evaluation performed over\nsynthetic spam data generated from del.icio.us dataset, shows that in most\ncases, Deep Learning can outperform the classical solutions, providing\nhigh-level protection against threats.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 14:20:11 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Pitsilis", "Georgios K.", ""], ["Ramampiaro", "Heri", ""], ["Langseth", "Helge", ""]]}, {"id": "1901.08458", "submitter": "Bharat Gaind", "authors": "Bharat Gaind, Varun Syal, Sneha Padgalwar", "title": "Emotion Detection and Analysis on Social Media", "comments": "Published in the Global Journal of Engineering Science and Researches\n  (ISSN 2348 - 8034, Pg. 78-89) after getting accepted in the International\n  Conference on Recent Trends In Computational Engineering and Technologies\n  (ICRTCET'18), May 17-18, 2018, Bengaluru, India. Journal Link -\n  http://www.gjesr.com/ICRTCET-18.html", "journal-ref": "Global Journal of Engineering Science and Researches (ICRTCET-18)\n  (2019) 78-89", "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of detection, classification and\nquantification of emotions of text in any form. We consider English text\ncollected from social media like Twitter, which can provide information having\nutility in a variety of ways, especially opinion mining. Social media like\nTwitter and Facebook is full of emotions, feelings and opinions of people all\nover the world. However, analyzing and classifying text on the basis of\nemotions is a big challenge and can be considered as an advanced form of\nSentiment Analysis. This paper proposes a method to classify text into six\ndifferent Emotion-Categories: Happiness, Sadness, Fear, Anger, Surprise and\nDisgust. In our model, we use two different approaches and combine them to\neffectively extract these emotions from text. The first approach is based on\nNatural Language Processing, and uses several textual features like emoticons,\ndegree words and negations, Parts Of Speech and other grammatical analysis. The\nsecond approach is based on Machine Learning classification algorithms. We have\nalso successfully devised a method to automate the creation of the training-set\nitself, so as to eliminate the need of manual annotation of large datasets.\nMoreover, we have managed to create a large bag of emotional words, along with\ntheir emotion-intensities. On testing, it is shown that our model provides\nsignificant accuracy in classifying tweets taken from Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 15:35:00 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 16:17:20 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gaind", "Bharat", ""], ["Syal", "Varun", ""], ["Padgalwar", "Sneha", ""]]}, {"id": "1901.08593", "submitter": "Ehsan Mohammadi Dr", "authors": "Ehsan Mohammadi, Mike Thelwall", "title": "Readership Data and Research Impact", "comments": null, "journal-ref": "Handbook of Quantitative Science and Technology Research, 2019", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading academic publications is a key scholarly activity. Scholars accessing\nand recording academic publications online are producing new types of\nreadership data. These include publisher, repository, and academic social\nnetwork download statistics as well as online reference manager records. This\nchapter discusses the use of download and reference manager data for research\nevaluation and library collection development. The focus is on the validity and\napplication of readership data as an impact indicator for academic publications\nacross different disciplines. Mendeley is particularly promising in this\nregard, although all data sources are not subjected to rigorous quality control\nand can be manipulated.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 01:42:56 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Mohammadi", "Ehsan", ""], ["Thelwall", "Mike", ""]]}, {"id": "1901.08901", "submitter": "Drimik Roy Chowdhury", "authors": "Namrata Chaudhary, Drimik Roy Chowdhury", "title": "Expanding Click and Buy rates: Exploration of evaluation metrics that\n  measure the impact of personalized recommendation engines on e-commerce\n  platforms", "comments": null, "journal-ref": "http://www.researchpublish.com/journal/IJCSITR/Issue-4-October-2018-December-2018/0", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the most appropriate recommendation model for an e-commerce\nbusiness, a live evaluation should be performed on the shopping website to\nmeasure the influence of personalization in real-time. The aim of this paper is\nto introduce and justify two new metrics -- CTR NoRepeat and Click & Buy rate\n-- which stem from the standard metrics, Click-through(CTR) and Buy-through\nrate(BTR), respectively. The former variation tackles the issue of\noverestimation of clicks in the original CTR while the latter accounts for\nnoting purchases of products that have been previously clicked, in order to\nvalidate that the buy included in the metric is a result of customer\ninteractions. A significance test for independence of two means is conducted\nfor multiple datasets, between each of the new metrics and its respective\nparent to determine the novelty and necessity of the variants. The\nPearson-correlation coefficient is calculated to assess the strength of the\nlinear relationships and conclude on the predictability factor amongst the\naforementioned factors to investigate unknown connections between customer\nclicks and buys. Additionally, other metrics such as hits per customer, buyers\nper customer, clicks per customer etc. are introduced that help explain\nindicators of customer behavior on the e-commerce website in reference.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 21:54:39 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Chaudhary", "Namrata", ""], ["Chowdhury", "Drimik Roy", ""]]}, {"id": "1901.08907", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, Minyi Guo", "title": "Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation", "comments": "In Proceedings of The 2019 Web Conference (WWW 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering often suffers from sparsity and cold start problems\nin real recommendation scenarios, therefore, researchers and engineers usually\nuse side information to address the issues and improve the performance of\nrecommender systems. In this paper, we consider knowledge graphs as the source\nof side information. We propose MKR, a Multi-task feature learning approach for\nKnowledge graph enhanced Recommendation. MKR is a deep end-to-end framework\nthat utilizes knowledge graph embedding task to assist recommendation task. The\ntwo tasks are associated by cross&compress units, which automatically share\nlatent features and learn high-order interactions between items in recommender\nsystems and entities in the knowledge graph. We prove that cross&compress units\nhave sufficient capability of polynomial approximation, and show that MKR is a\ngeneralized framework over several representative methods of recommender\nsystems and multi-task learning. Through extensive experiments on real-world\ndatasets, we demonstrate that MKR achieves substantial gains in movie, book,\nmusic, and news recommendation, over state-of-the-art baselines. MKR is also\nshown to be able to maintain a decent performance even if user-item\ninteractions are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 11:36:21 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Fuzheng", ""], ["Zhao", "Miao", ""], ["Li", "Wenjie", ""], ["Xie", "Xing", ""], ["Guo", "Minyi", ""]]}, {"id": "1901.08910", "submitter": "Francois Belletti", "authors": "Francois Belletti, Karthik Lakshmanan, Walid Krichene, Yi-Fan Chen,\n  John Anderson", "title": "Scalable Realistic Recommendation Datasets through Fractal Expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender System research suffers currently from a disconnect between the\nsize of academic data sets and the scale of industrial production systems. In\norder to bridge that gap we propose to generate more massive user/item\ninteraction data sets by expanding pre-existing public data sets. User/item\nincidence matrices record interactions between users and items on a given\nplatform as a large sparse matrix whose rows correspond to users and whose\ncolumns correspond to items. Our technique expands such matrices to larger\nnumbers of rows (users), columns (items) and non zero values (interactions)\nwhile preserving key higher order statistical properties. We adapt the\nKronecker Graph Theory to user/item incidence matrices and show that the\ncorresponding fractal expansions preserve the fat-tailed distributions of user\nengagements, item popularity and singular value spectra of user/item\ninteraction matrices. Preserving such properties is key to building large\nrealistic synthetic data sets which in turn can be employed reliably to\nbenchmark Recommender Systems and the systems employed to train them. We\nprovide algorithms to produce such expansions and apply them to the MovieLens\n20 million data set comprising 20 million ratings of 27K movies by 138K users.\nThe resulting expanded data set has 10 billion ratings, 864K items and 2\nmillion users in its smaller version and can be scaled up or down. A larger\nversion features 655 billion ratings, 7 million items and 17 million users.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 22:18:25 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 18:37:54 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 19:44:06 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Belletti", "Francois", ""], ["Lakshmanan", "Karthik", ""], ["Krichene", "Walid", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "1901.08977", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni, Michele Lepri, Alfredo Milani", "title": "Topological and Semantic Graph-based Author Disambiguation on DBLP Data\n  in Neo4j", "comments": "Pre-print of article presented at AIKE (Artificial Intelligence and\n  Knowledge Engineering) IEEE Conference, September 2018, Laguna Hills,\n  California (USA)", "journal-ref": "AIKE 2018: 239-243", "doi": "10.1109/AIKE.2018.00054", "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we introduce a novel method for entity resolution author\ndisambiguation in bibliographic networks. Such a method is based on a 2-steps\nnetwork traversal using topological similarity measures for rating candidate\nnodes. Topological similarity is widely used in the Link Prediction application\ndomain to assess the likelihood of an unknown link. A similarity function can\nbe a good approximation for equality, therefore can be used to disambiguate,\nbasing on the hypothesis that authors with many common co-authors are similar.\nOur method has experimented on a graph-based representation of the public DBLP\nComputer Science database. The results obtained are extremely encouraging\nregarding Precision, Accuracy, and Specificity. Further good aspects are the\nlocality of the method for disambiguation assessment which avoids the need to\nknow the global network, and the exploitation of only a few data, e.g. author\nname and paper title (i.e., co-authorship data).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 16:49:53 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Franzoni", "Valentina", ""], ["Lepri", "Michele", ""], ["Milani", "Alfredo", ""]]}, {"id": "1901.09037", "submitter": "Ziwei Xu", "authors": "Ziwei Xu (Polytech Nantes, DUKe, LS2N), Mounira Harzallah (LINA),\n  Fabrice Guillet (LINA)", "title": "Comparing of Term Clustering Frameworks for Modular Ontology Learning", "comments": null, "journal-ref": "10th International Joint Conference on Knowledge Discovery,\n  Knowledge Engineering and Knowledge Management, Sep 2018, Seville, Spain.\n  SCITEPRESS - Science and Technology Publications; SCITEPRESS - Science and\n  Technology Publications, pp.128-135, 2018, Proceedings of the 10th\n  International Joint Conference on Knowledge Discovery, Knowledge Engineering\n  and Knowledge Management - Volume 2: KEOD", "doi": "10.5220/0006960401280135", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to use term clustering to build a modular ontology according\nto core ontology from domain-specific text. The acquisition of semantic\nknowledge focuses on noun phrase appearing with the same syntactic roles in\nrelation to a verb or its preposition combination in a sentence. The\nconstruction of this co-occurrence matrix from context helps to build feature\nspace of noun phrases, which is then transformed to several encoding\nrepresentations including feature selection and dimensionality reduction. In\naddition, the content has also been presented with the construction of word\nvectors. These representations are clustered respectively with K-Means and\nAffinity Propagation (AP) methods, which differentiate into the term clustering\nframeworks. Due to the randomness of K-Means, iteration efforts are adopted to\nfind the optimal parameter. The frameworks are evaluated extensively where AP\nshows dominant effectiveness for co-occurred terms and NMF encoding technique\nis salient by its promising facilities in feature compression.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 15:04:02 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Xu", "Ziwei", "", "Polytech Nantes, DUKe, LS2N"], ["Harzallah", "Mounira", "", "LINA"], ["Guillet", "Fabrice", "", "LINA"]]}, {"id": "1901.09118", "submitter": "Michael Bloodgood", "authors": "Michael Altschuler and Michael Bloodgood", "title": "Stopping Active Learning based on Predicted Change of F Measure for Text\n  Classification", "comments": "8 pages, 12 tables; published in Proceedings of the 2019 IEEE 13th\n  International Conference on Semantic Computing (ICSC), Newport Beach, CA,\n  USA, pages 47-54, January 2019", "journal-ref": "In Proceedings of the 2019 IEEE 13th International Conference on\n  Semantic Computing (ICSC), pages 47-54, Newport Beach, CA, USA, January 2019.\n  IEEE", "doi": "10.1109/ICOSC.2019.8665646", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During active learning, an effective stopping method allows users to limit\nthe number of annotations, which is cost effective. In this paper, a new\nstopping method called Predicted Change of F Measure will be introduced that\nattempts to provide the users an estimate of how much performance of the model\nis changing at each iteration. This stopping method can be applied with any\nbase learner. This method is useful for reducing the data annotation bottleneck\nencountered when building text classification systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 00:01:27 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 23:56:41 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Altschuler", "Michael", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1901.09126", "submitter": "Michael Bloodgood", "authors": "Garrett Beatty, Ethan Kochis and Michael Bloodgood", "title": "The Use of Unlabeled Data versus Labeled Data for Stopping Active\n  Learning for Text Classification", "comments": "8 pages, 4 figures, 3 tables; published in Proceedings of the IEEE\n  13th International Conference on Semantic Computing (ICSC), Newport Beach,\n  CA, USA, pages 287-294, January 2019", "journal-ref": "In Proceedings of the 2019 IEEE 13th International Conference on\n  Semantic Computing (ICSC), pages 287-294, Newport Beach, CA, USA, January\n  2019. IEEE", "doi": "10.1109/ICOSC.2019.8665546", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation of training data is the major bottleneck in the creation of text\nclassification systems. Active learning is a commonly used technique to reduce\nthe amount of training data one needs to label. A crucial aspect of active\nlearning is determining when to stop labeling data. Three potential sources for\ninforming when to stop active learning are an additional labeled set of data,\nan unlabeled set of data, and the training data that is labeled during the\nprocess of active learning. To date, no one has compared and contrasted the\nadvantages and disadvantages of stopping methods based on these three\ninformation sources. We find that stopping methods that use unlabeled data are\nmore effective than methods that use labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 00:27:02 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 23:36:51 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Beatty", "Garrett", ""], ["Kochis", "Ethan", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1901.09334", "submitter": "Roshni Chakraborty", "authors": "Roshni Chakraborty, Abhijeet Kharat, Apalak Khatua, Sourav Kumar\n  Dandapat, Joydeep Chandra", "title": "Predicting Tomorrow's Headline using Today's Twitter Deliberations", "comments": "This paper was accepted in CIKM Workshop on News Recommendation and\n  Analytics (INRA), 2018, Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the popularity of news article is a challenging task. Existing\nliterature mostly focused on article contents and polarity to predict\npopularity. However, existing research has not considered the users' preference\ntowards a particular article. Understanding users' preference is an important\naspect for predicting the popularity of news articles. Hence, we consider the\nsocial media data, from the Twitter platform, to address this research gap. In\nour proposed model, we have considered the users' involvement as well as the\nusers' reaction towards an article to predict the popularity of the article. In\nshort, we are predicting tomorrow's headline by probing today's Twitter\ndiscussion. We have considered 300 political news article from the New York\nPost, and our proposed approach has outperformed other baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 08:00:09 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Chakraborty", "Roshni", ""], ["Kharat", "Abhijeet", ""], ["Khatua", "Apalak", ""], ["Dandapat", "Sourav Kumar", ""], ["Chandra", "Joydeep", ""]]}, {"id": "1901.09451", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes,\n  Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi,\n  Adam Tauman Kalai", "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a\n  High-Stakes Setting", "comments": "Accepted at ACM Conference on Fairness, Accountability, and\n  Transparency (ACM FAT*), 2019", "journal-ref": null, "doi": "10.1145/3287560.3287572", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale study of gender bias in occupation classification, a\ntask where the use of machine learning may lead to negative outcomes on\npeoples' lives. We analyze the potential allocation harms that can result from\nsemantic representation bias. To do so, we study the impact on occupation\nclassification of including explicit gender indicators---such as first names\nand pronouns---in different semantic representations of online biographies.\nAdditionally, we quantify the bias that remains when these indicators are\n\"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit\ngender indicators. As we demonstrate, differences in true positive rates\nbetween genders are correlated with existing gender imbalances in occupations,\nwhich may compound these imbalances.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 22:36:16 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Romanov", "Alexey", ""], ["Wallach", "Hanna", ""], ["Chayes", "Jennifer", ""], ["Borgs", "Christian", ""], ["Chouldechova", "Alexandra", ""], ["Geyik", "Sahin", ""], ["Kenthapadi", "Krishnaram", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1901.09613", "submitter": "Sungchul Choi", "authors": "Hongjun Jeon, Wonchul Seo, Eunjeong Lucy Park, Sungchul Choi", "title": "Hybrid Machine Learning Approach to Popularity Prediction of Newly\n  Released Contents for Online Video Streaming Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the industry of video content providers such as VOD and IPTV, predicting\nthe popularity of video contents in advance is critical not only from a\nmarketing perspective but also from a network optimization perspective. By\npredicting whether the content will be successful or not in advance, the\ncontent file, which is large, is efficiently deployed in the proper service\nproviding server, leading to network cost optimization. Many previous studies\nhave done view count prediction research to do this. However, the studies have\nbeen making predictions based on historical view count data from users. In this\ncase, the contents had been published to the users and already deployed on a\nservice server. These approaches make possible to efficiently deploy a content\nalready published but are impossible to use for a content that is not be\npublished. To address the problems, this research proposes a hybrid machine\nlearning approach to the classification model for the popularity prediction of\nnewly video contents which is not published. In this paper, we create a new\nvariable based on the related content of the specific content and divide entire\ndataset by the characteristics of the contents. Next, the prediction is\nperformed using XGBoosting and deep neural net based model according to the\ndata characteristics of the cluster. Our model uses metadata for contents for\nprediction, so we use categorical embedding techniques to solve the sparsity of\ncategorical variables and make them learn efficiently for the deep neural net\nmodel. As well, we use the FTRL-proximal algorithm to solve the problem of the\nview-count volatility of video content. We achieve overall better performance\nthan the previous standalone method with a dataset from one of the top\nstreaming service company.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:42:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Jeon", "Hongjun", ""], ["Seo", "Wonchul", ""], ["Park", "Eunjeong Lucy", ""], ["Choi", "Sungchul", ""]]}, {"id": "1901.09851", "submitter": "Brian Brost", "authors": "Brian Brost, Rishabh Mehrotra and Tristan Jehan", "title": "The Music Streaming Sessions Dataset", "comments": "Web conference 2019 version with updated link to dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the core of many important machine learning problems faced by online\nstreaming services is a need to model how users interact with the content they\nare served. Unfortunately, there are no public datasets currently available\nthat enable researchers to explore this topic. In order to spur that research,\nwe release the Music Streaming Sessions Dataset (MSSD), which consists of 160\nmillion listening sessions and associated user actions. Furthermore, we provide\naudio features and metadata for the approximately 3.7 million unique tracks\nreferred to in the logs. This is the largest collection of such track metadata\ncurrently available to the public. This dataset enables research on important\nproblems including how to model user listening and interaction behaviour in\nstreaming, as well as Music Information Retrieval (MIR), and session-based\nsequential recommendations. Additionally, a subset of sessions were collected\nusing a uniformly random recommendation setting, enabling their use for\ncounterfactual evaluation of such sequential recommendations. Finally, we\nprovide an analysis of user behavior and suggest further research problems\nwhich can be addressed using the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 14:22:08 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 18:31:43 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Brost", "Brian", ""], ["Mehrotra", "Rishabh", ""], ["Jehan", "Tristan", ""]]}, {"id": "1901.09888", "submitter": "Muhammad Ammad-Ud-Din Ph.D.", "authors": "Muhammad Ammad-ud-din, Elena Ivannikova, Suleiman A. Khan, Were\n  Oyomno, Qiang Fu, Kuan Eeik Tan and Adrian Flanagan", "title": "Federated Collaborative Filtering for Privacy-Preserving Personalized\n  Recommendation System", "comments": "12 pages, 2 figures, 2 tables, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing interest in user privacy is leading to new privacy preserving\nmachine learning paradigms. In the Federated Learning paradigm, a master\nmachine learning model is distributed to user clients, the clients use their\nlocally stored data and model for both inference and calculating model updates.\nThe model updates are sent back and aggregated on the server to update the\nmaster model then redistributed to the clients. In this paradigm, the user data\nnever leaves the client, greatly enhancing the user' privacy, in contrast to\nthe traditional paradigm of collecting, storing and processing user data on a\nbackend server beyond the user's control. In this paper we introduce, as far as\nwe are aware, the first federated implementation of a Collaborative Filter. The\nfederated updates to the model are based on a stochastic gradient approach. As\na classical case study in machine learning, we explore a personalized\nrecommendation system based on users' implicit feedback and demonstrate the\nmethod's applicability to both the MovieLens and an in-house dataset. Empirical\nvalidation confirms a collaborative filter can be federated without a loss of\naccuracy compared to a standard implementation, hence enhancing the user's\nprivacy in a widely used recommender application while maintaining recommender\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 14:18:38 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ammad-ud-din", "Muhammad", ""], ["Ivannikova", "Elena", ""], ["Khan", "Suleiman A.", ""], ["Oyomno", "Were", ""], ["Fu", "Qiang", ""], ["Tan", "Kuan Eeik", ""], ["Flanagan", "Adrian", ""]]}, {"id": "1901.10133", "submitter": "C Ravindranath Chowdary", "authors": "Shashank Yadav, Tejas Shimpi, C. Ravindranath Chowdary, Prashant\n  Sharma, Deepansh Agrawal, Shivang Agarwal", "title": "Structuring an unordered text document", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Segmenting an unordered text document into different sections is a very\nuseful task in many text processing applications like multiple document\nsummarization, question answering, etc. This paper proposes structuring of an\nunordered text document based on the keywords in the document. We test our\napproach on Wikipedia documents using both statistical and predictive methods\nsuch as the TextRank algorithm and Google's USE (Universal Sentence Encoder).\nFrom our experimental results, we show that the proposed model can effectively\nstructure an unordered document into sections.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 06:53:21 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Yadav", "Shashank", ""], ["Shimpi", "Tejas", ""], ["Chowdary", "C. Ravindranath", ""], ["Sharma", "Prashant", ""], ["Agrawal", "Deepansh", ""], ["Agarwal", "Shivang", ""]]}, {"id": "1901.10185", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Hong Liu, Xiaoshuai Sun, Yongjian Wu,\n  Yunsheng Wu", "title": "Towards Optimal Discrete Online Hashing with Balanced Similarity", "comments": "8 pages, 11 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When facing large-scale image datasets, online hashing serves as a promising\nsolution for online retrieval and prediction tasks. It encodes the online\nstreaming data into compact binary codes, and simultaneously updates the hash\nfunctions to renew codes of the existing dataset. To this end, the existing\nmethods update hash functions solely based on the new data batch, without\ninvestigating the correlation between such new data and the existing dataset.\nIn addition, existing works update the hash functions using a relaxation\nprocess in its corresponding approximated continuous space. And it remains as\nan open problem to directly apply discrete optimizations in online hashing. In\nthis paper, we propose a novel supervised online hashing method, termed\nBalanced Similarity for Online Discrete Hashing (BSODH), to solve the above\nproblems in a unified framework. BSODH employs a well-designed hashing\nalgorithm to preserve the similarity between the streaming data and the\nexisting dataset via an asymmetric graph regularization. We further identify\nthe \"data-imbalance\" problem brought by the constructed asymmetric graph, which\nrestricts the application of discrete optimization in our problem. Therefore, a\nnovel balanced similarity is further proposed, which uses two equilibrium\nfactors to balance the similar and dissimilar weights and eventually enables\nthe usage of discrete optimizations. Extensive experiments conducted on three\nwidely-used benchmarks demonstrate the advantages of the proposed method over\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 09:04:55 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 16:27:25 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Sun", "Xiaoshuai", ""], ["Wu", "Yongjian", ""], ["Wu", "Yunsheng", ""]]}, {"id": "1901.10197", "submitter": "Hiteshwar Azad", "authors": "Hiteshwar Kumar Azad, Akshay Deepak", "title": "A new approach for query expansion using Wikipedia and WordNet", "comments": "20 pages, 17 figures. arXiv admin note: text overlap with\n  arXiv:1708.00247", "journal-ref": "Information Sciences, 2019", "doi": "10.1016/j.ins.2019.04.019", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion (QE) is a well-known technique used to enhance the\neffectiveness of information retrieval. QE reformulates the initial query by\nadding similar terms that help in retrieving more relevant results. Several\napproaches have been proposed in literature producing quite favorable results,\nbut they are not evenly favorable for all types of queries (individual and\nphrase queries). One of the main reasons for this is the use of the same kind\nof data sources and weighting scheme while expanding both the individual and\nthe phrase query terms. As a result, the holistic relationship among the query\nterms is not well captured or scored. To address this issue, we have presented\na new approach for QE using Wikipedia and WordNet as data sources.\nSpecifically, Wikipedia gives rich expansion terms for phrase terms, while\nWordNet does the same for individual terms. We have also proposed novel\nweighting schemes for expansion terms: in-link score (for terms extracted from\nWikipedia) and a tf-idf based scheme (for terms extracted from WordNet). In the\nproposed Wikipedia-WordNet-based QE technique (WWQE), we weigh the expansion\nterms twice: first, they are scored by the weighting scheme individually, and\nthen, the weighting scheme scores the selected expansion terms concerning the\nentire query using correlation score. The proposed approach gains improvements\nof 24% on the MAP score and 48% on the GMAP score over unexpanded queries on\nthe FIRE dataset. Experimental results achieve a significant improvement over\nindividual expansion and other related state-of-the-art approaches. We also\nanalyzed the effect on retrieval effectiveness of the proposed technique by\nvarying the number of expansion terms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 10:01:22 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 10:15:19 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Azad", "Hiteshwar Kumar", ""], ["Deepak", "Akshay", ""]]}, {"id": "1901.10200", "submitter": "Carl Henning Lubba", "authors": "Carl H Lubba and Sarab S Sethi and Philip Knaute and Simon R Schultz\n  and Ben D Fulcher and Nick S Jones", "title": "catch22: CAnonical Time-series CHaracteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Capturing the dynamical properties of time series concisely as interpretable\nfeature vectors can enable efficient clustering and classification for\ntime-series applications across science and industry. Selecting an appropriate\nfeature-based representation of time series for a given application can be\nachieved through systematic comparison across a comprehensive time-series\nfeature library, such as those in the hctsa toolbox. However, this approach is\ncomputationally expensive and involves evaluating many similar features,\nlimiting the widespread adoption of feature-based representations of time\nseries for real-world applications. In this work, we introduce a method to\ninfer small sets of time-series features that (i) exhibit strong classification\nperformance across a given collection of time-series problems, and (ii) are\nminimally redundant. Applying our method to a set of 93 time-series\nclassification datasets (containing over 147000 time series) and using a\nfiltered version of the hctsa feature library (4791 features), we introduce a\ngenerically useful set of 22 CAnonical Time-series CHaracteristics, catch22.\nThis dimensionality reduction, from 4791 to 22, is associated with an\napproximately 1000-fold reduction in computation time and near linear scaling\nwith time-series length, despite an average reduction in classification\naccuracy of just 7%. catch22 captures a diverse and interpretable signature of\ntime series in terms of their properties, including linear and non-linear\nautocorrelation, successive differences, value distributions and outliers, and\nfluctuation scaling properties. We provide an efficient implementation of\ncatch22, accessible from many programming environments, that facilitates\nfeature-based time-series analysis for scientific, industrial, financial and\nmedical applications using a common language of interpretable time-series\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 10:06:33 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 12:23:27 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Lubba", "Carl H", ""], ["Sethi", "Sarab S", ""], ["Knaute", "Philip", ""], ["Schultz", "Simon R", ""], ["Fulcher", "Ben D", ""], ["Jones", "Nick S", ""]]}, {"id": "1901.10219", "submitter": "Ming Siang Huang", "authors": "Ming-Siang Huang, Po-Ting Lai, Richard Tzong-Han Tsai, Wen-Lian Hsu", "title": "Revised JNLPBA Corpus: A Revised Version of Biomedical NER Corpus for\n  Relation Extraction Task", "comments": "17 pages", "journal-ref": "Briefings in Bioinformatics, 2020, bbaa054", "doi": "10.1093/bib/bbaa054", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of biomedical named entity recognition (BNER) and biomedical\nrelation extraction (BRE) researches promotes the development of text mining in\nbiological domains. As a cornerstone of BRE, robust BNER system is required to\nidentify the mentioned NEs in plain texts for further relation extraction\nstage. However, the current BNER corpora, which play important roles in these\ntasks, paid less attention to achieve the criteria for BRE task. In this study,\nwe present Revised JNLPBA corpus, the revision of JNLPBA corpus, to broaden the\napplicability of a NER corpus from BNER to BRE task. We preserve the original\nentity types including protein, DNA, RNA, cell line and cell type while all the\nabstracts in JNLPBA corpus are manually curated by domain experts again basis\non the new annotation guideline focusing on the specific NEs instead of general\nterms. Simultaneously, several imperfection issues in JNLPBA are pointed out\nand made up in the new corpus. To compare the adaptability of different NER\nsystems in Revised JNLPBA and JNLPBA corpora, the F1-measure was measured in\nthree open sources NER systems including BANNER, Gimli and NERSuite. In the\nsame circumstance, all the systems perform average 10% better in Revised JNLPBA\nthan in JNLPBA. Moreover, the cross-validation test is carried out which we\ntrain the NER systems on JNLPBA/Revised JNLPBA corpora and access the\nperformance in both protein-protein interaction extraction (PPIE) and\nbiomedical event extraction (BEE) corpora to confirm that the newly refined\nRevised JNLPBA is a competent NER corpus in biomedical relation application.\nThe revised JNLPBA corpus is freely available at\niasl-btm.iis.sinica.edu.tw/BNER/Content/Revised_JNLPBA.zip.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:12:58 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Huang", "Ming-Siang", ""], ["Lai", "Po-Ting", ""], ["Tsai", "Richard Tzong-Han", ""], ["Hsu", "Wen-Lian", ""]]}, {"id": "1901.10262", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis, Maarten de Rijke", "title": "Optimizing Ranking Models in an Online Setting", "comments": "European Conference on Information Retrieval (ECIR) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Learning to Rank (OLTR) methods optimize ranking models by directly\ninteracting with users, which allows them to be very efficient and responsive.\nAll OLTR methods introduced during the past decade have extended on the\noriginal OLTR method: Dueling Bandit Gradient Descent (DBGD). Recently, a\nfundamentally different approach was introduced with the Pairwise\nDifferentiable Gradient Descent (PDGD) algorithm. To date the only comparisons\nof the two approaches are limited to simulations with cascading click models\nand low levels of noise. The main outcome so far is that PDGD converges at\nhigher levels of performance and learns considerably faster than DBGD-based\nmethods. However, the PDGD algorithm assumes cascading user behavior,\npotentially giving it an unfair advantage. Furthermore, the robustness of both\nmethods to high levels of noise has not been investigated. Therefore, it is\nunclear whether the reported advantages of PDGD over DBGD generalize to\ndifferent experimental conditions. In this paper, we investigate whether the\nprevious conclusions about the PDGD and DBGD comparison generalize from ideal\nto worst-case circumstances. We do so in two ways. First, we compare the\ntheoretical properties of PDGD and DBGD, by taking a critical look at\npreviously proven properties in the context of ranking. Second, we estimate an\nupper and lower bound on the performance of methods by simulating both ideal\nuser behavior and extremely difficult behavior, i.e., almost-random\nnon-cascading user models. Our findings show that the theoretical bounds of\nDBGD do not apply to any common ranking model and, furthermore, that the\nperformance of DBGD is substantially worse than PDGD in both ideal and\nworst-case circumstances. These results reproduce previously published findings\nabout the relative performance of PDGD vs. DBGD and generalize them to\nextremely noisy and non-cascading circumstances.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:04:54 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1901.10263", "submitter": "Simon Razniewski", "authors": "Cuong Xuan Chu, Simon Razniewski, Gerhard Weikum", "title": "TiFi: Taxonomy Induction for Fictional Domains [Extended version]", "comments": "Extended version of The Web Conference 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomies are important building blocks of structured knowledge bases, and\ntheir construction from text sources and Wikipedia has received much attention.\nIn this paper we focus on the construction of taxonomies for fictional domains,\nusing noisy category systems from fan wikis or text extraction as input. Such\nfictional domains are archetypes of entity universes that are poorly covered by\nWikipedia, such as also enterprise-specific knowledge bases or highly\nspecialized verticals. Our fiction-targeted approach, called TiFi, consists of\nthree phases: (i) category cleaning, by identifying candidate categories that\ntruly represent classes in the domain of interest, (ii) edge cleaning, by\nselecting subcategory relationships that correspond to class subsumption, and\n(iii) top-level construction, by mapping classes onto a subset of high-level\nWordNet categories. A comprehensive evaluation shows that TiFi is able to\nconstruct taxonomies for a diverse range of fictional domains such as Lord of\nthe Rings, The Simpsons or Greek Mythology with very high precision and that it\noutperforms state-of-the-art baselines for taxonomy induction by a substantial\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:07:13 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chu", "Cuong Xuan", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1901.10332", "submitter": "Zhuoran Liu", "authors": "Zhuoran Liu, Zhengyu Zhao, Martha Larson", "title": "Who's Afraid of Adversarial Queries? The Impact of Image Modifications\n  on Content-based Image Retrieval", "comments": "To appear at the ACM International Conference on Multimedia Retrieval\n  (ICMR 2019). Our code is available at https://github.com/liuzrcc/PIRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An adversarial query is an image that has been modified to disrupt\ncontent-based image retrieval (CBIR) while appearing nearly untouched to the\nhuman eye. This paper presents an analysis of adversarial queries for CBIR\nbased on neural, local, and global features. We introduce an innovative neural\nimage perturbation approach, called Perturbations for Image Retrieval Error\n(PIRE), that is capable of blocking neural-feature-based CBIR. PIRE differs\nsignificantly from existing approaches that create images adversarial with\nrespect to CNN classifiers because it is unsupervised, i.e., it needs no\nlabelled data from the data set to which it is applied. Our experimental\nanalysis demonstrates the surprising effectiveness of PIRE in blocking CBIR,\nand also covers aspects of PIRE that must be taken into account in practical\nsettings, including saving images, image quality and leaking adversarial\nqueries into the background collection. Our experiments also compare PIRE (a\nneural approach) with existing keypoint removal and injection approaches (which\nmodify local features). Finally, we discuss the challenges that face multimedia\nresearchers in the future study of adversarial queries.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 15:09:14 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 11:28:35 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:57:18 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Liu", "Zhuoran", ""], ["Zhao", "Zhengyu", ""], ["Larson", "Martha", ""]]}, {"id": "1901.10496", "submitter": "Trond Linjordet", "authors": "Trond Linjordet and Krisztian Balog", "title": "Impact of Training Dataset Size on Neural Answer Selection Models", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is held as a truism that deep neural networks require large datasets to\ntrain effective models. However, large datasets, especially with high-quality\nlabels, can be expensive to obtain. This study sets out to investigate (i) how\nlarge a dataset must be to train well-performing models, and (ii) what impact\ncan be shown from fractional changes to the dataset size. A practical method to\ninvestigate these questions is to train a collection of deep neural answer\nselection models using fractional subsets of varying sizes of an initial\ndataset. We observe that dataset size has a conspicuous lack of effect on the\ntraining of some of these models, bringing the underlying algorithms into\nquestion.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 19:00:21 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Linjordet", "Trond", ""], ["Balog", "Krisztian", ""]]}, {"id": "1901.10696", "submitter": "Javier Parapar", "authors": "Javier Parapar, David E. Losada, Manuel A. Presedo-Quindimil, Alvaro\n  Barreiro", "title": "Using Score Distributions to Compare Statistical Significance Tests for\n  Information Retrieval Evaluation", "comments": "Preprint of our JASIST paper", "journal-ref": null, "doi": "10.1002/asi.24203", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance tests can provide evidence that the observed\ndifference in performance between two methods is not due to chance. In\nInformation Retrieval, some studies have examined the validity and suitability\nof such tests for comparing search systems. We argue here that current methods\nfor assessing the reliability of statistical tests suffer from some\nmethodological weaknesses, and we propose a novel way to study significance\ntests for retrieval evaluation. Using Score Distributions, we model the output\nof multiple search systems, produce simulated search results from such models,\nand compare them using various significance tests. A key strength of this\napproach is that we assess statistical tests under perfect knowledge about the\ntruth or falseness of the null hypothesis. This new method for studying the\npower of significance tests in Information Retrieval evaluation is formal and\ninnovative. Following this type of analysis, we found that both the sign test\nand Wilcoxon signed test have more power than the permutation test and the\nt-test. The sign test and Wilcoxon signed test also have a good behavior in\nterms of type I errors. The bootstrap test shows few type I errors, but it has\nless power than the other methods tested.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 07:26:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parapar", "Javier", ""], ["Losada", "David E.", ""], ["Presedo-Quindimil", "Manuel A.", ""], ["Barreiro", "Alvaro", ""]]}, {"id": "1901.10710", "submitter": "Xue Li", "authors": "Xue Li, Zhipeng Luo, Hao Sun, Jianjin Zhang, Weihao Han, Xianqi Chu,\n  Liangjie Zhang, Qi Zhang", "title": "Learning Fast Matching Models from Weak Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel training scheme for fast matching models in\nSearch Ads, which is motivated by the real challenges in model training. The\nfirst challenge stems from the pursuit of high throughput, which prohibits the\ndeployment of inseparable architectures, and hence greatly limits the model\naccuracy. The second problem arises from the heavy dependency on human provided\nlabels, which are expensive and time-consuming to collect, yet how to leverage\nunlabeled search log data is rarely studied. The proposed training framework\ntargets on mitigating both issues, by treating the stronger but undeployable\nmodels as annotators, and learning a deployable model from both human provided\nrelevance labels and weakly annotated search log data. Specifically, we first\nconstruct multiple auxiliary tasks from the enumerated relevance labels, and\ntrain the annotators by jointly learning from those related tasks. The\nannotation models are then used to assign scores to both labeled and unlabeled\ntraining samples. The deployable model is firstly learnt on the scored\nunlabeled data, and then fine-tuned on scored labeled data, by leveraging both\nlabels and scores via minimizing the proposed label-aware weighted loss.\nAccording to our experiments, compared with the baseline that directly learns\nfrom relevance labels, training by the proposed framework outperforms it by a\nlarge margin, and improves data efficiency substantially by dispensing with 80%\nlabeled samples. The proposed framework allows us to improve the fast matching\nmodel by learning from stronger annotators while keeping its architecture\nunchanged. Meanwhile, our training framework offers a principled manner to\nleverage search log data in the training phase, which could effectively\nalleviate our dependency on human provided labels.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 08:42:04 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 02:15:11 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 00:55:00 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Li", "Xue", ""], ["Luo", "Zhipeng", ""], ["Sun", "Hao", ""], ["Zhang", "Jianjin", ""], ["Han", "Weihao", ""], ["Chu", "Xianqi", ""], ["Zhang", "Liangjie", ""], ["Zhang", "Qi", ""]]}, {"id": "1901.10816", "submitter": "Mohamad Yaser Jaradeh", "authors": "Mohamad Yaser Jaradeh, Allard Oelen, Kheir Eddine Farfar, Manuel\n  Prinz, Jennifer D'Souza, G\\'abor Kismih\\'ok, Markus Stocker, S\\\"oren Auer", "title": "Open Research Knowledge Graph: Next Generation Infrastructure for\n  Semantic Scholarly Knowledge", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite improved digital access to scholarly knowledge in recent decades,\nscholarly communication remains exclusively document-based. In this form,\nscholarly knowledge is hard to process automatically. In this paper, we present\nthe first steps towards a knowledge graph based infrastructure that acquires\nscholarly knowledge in machine actionable form thus enabling new possibilities\nfor scholarly knowledge curation, publication and processing. The primary\ncontribution is to present, evaluate and discuss multi-modal scholarly\nknowledge acquisition, combining crowdsourced and automated techniques. We\npresent the results of the first user evaluation of the infrastructure with the\nparticipants of a recent international conference. Results suggest that users\nwere intrigued by the novelty of the proposed infrastructure and by the\npossibilities for innovative scholarly knowledge processing it could enable.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:34:45 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 08:50:41 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 14:55:14 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Jaradeh", "Mohamad Yaser", ""], ["Oelen", "Allard", ""], ["Farfar", "Kheir Eddine", ""], ["Prinz", "Manuel", ""], ["D'Souza", "Jennifer", ""], ["Kismih\u00f3k", "G\u00e1bor", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1901.11281", "submitter": "Vincent Labatut", "authors": "Etienne Papegnies (LIA), Vincent Labatut (LIA), Richard Dufour (LIA),\n  Georges Linares (LIA)", "title": "Conversational Networks for Automatic Online Moderation", "comments": null, "journal-ref": "IEEE Transactions on Computational Social Systems, 2019,\n  https://ieeexplore.ieee.org/document/8629298", "doi": "10.1109/tcss.2018.2887240", "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moderation of user-generated content in an online community is a challenge\nthat has great socio-economical ramifications. However, the costs incurred by\ndelegating this work to human agents are high. For this reason, an automatic\nsystem able to detect abuse in user-generated content is of great interest.\nThere are a number of ways to tackle this problem, but the most commonly seen\nin practice are word filtering or regular expression matching. The main\nlimitations are their vulnerability to intentional obfuscation on the part of\nthe users, and their context-insensitive nature. Moreover, they are\nlanguage-dependent and may require appropriate corpora for training. In this\npaper, we propose a system for automatic abuse detection that completely\ndisregards message content. We first extract a conversational network from raw\nchat logs and characterize it through topological measures. We then use these\nas features to train a classifier on our abuse detection task. We thoroughly\nassess our system on a dataset of user comments originating from a French\nMassively Multiplayer Online Game. We identify the most appropriate network\nextraction parameters and discuss the discriminative power of our features,\nrelatively to their topological and temporal nature. Our method reaches an\nF-measure of 83.89 when using the full feature set, improving on existing\napproaches. With a selection of the most discriminative features, we\ndramatically cut computing time while retaining most of the performance\n(82.65).\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:23:57 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Papegnies", "Etienne", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Dufour", "Richard", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1901.11372", "submitter": "Gianmaria Silvello", "authors": "Giacomo Rocco and Gianmaria Silvello", "title": "An InfoVis Tool for Interactive Component-Based Evaluation", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an InfoVis tool based on Sankey diagrams for the\nexploration of large combinatorial combinations of IR components - the Grid of\nPoints (GoP). The goal of this tool is to ease the comprehension of the\nbehavior of single IR components within fully functioning off-the-shelf IR\nsystems without recurring to complex statistical tools.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:27:52 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Rocco", "Giacomo", ""], ["Silvello", "Gianmaria", ""]]}, {"id": "1901.11459", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani", "title": "Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning\n  and its Application to Cross-Lingual Text Classification", "comments": "28 pages, 4 figures", "journal-ref": "Forthcoming in the ACM Transactions on Information Systems, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual Text Classification (CLC) consists of automatically\nclassifying, according to a common set C of classes, documents each written in\none of a set of languages L, and doing so more accurately than when naively\nclassifying each document via its corresponding language-specific classifier.\nIn order to obtain an increase in the classification accuracy for a given\nlanguage, the system thus needs to also leverage the training examples written\nin the other languages. We tackle multilabel CLC via funnelling, a new ensemble\nlearning method that we propose here. Funnelling consists of generating a\ntwo-tier classification system where all documents, irrespectively of language,\nare classified by the same (2nd-tier) classifier. For this classifier all\ndocuments are represented in a common, language-independent feature space\nconsisting of the posterior probabilities generated by 1st-tier,\nlanguage-dependent classifiers. This allows the classification of all test\ndocuments, of any language, to benefit from the information present in all\ntraining documents, of any language. We present substantial experiments, run on\npublicly available multilingual text collections, in which funnelling is shown\nto significantly outperform a number of state-of-the-art baselines. All code\nand datasets (in vector form) are made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:32:08 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 16:06:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Esuli", "Andrea", ""], ["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}]