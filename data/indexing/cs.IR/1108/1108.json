[{"id": "1108.0363", "submitter": "Fabian Steeg", "authors": "Fabian Steeg", "title": "Typesafe Modeling in Text Mining", "comments": "63 pages, in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the concept of annotation-based agents, this report introduces tools\nand a formal notation for defining and running text mining experiments using a\nstatically typed domain-specific language embedded in Scala. Using machine\nlearning for classification as an example, the framework is used to develop and\ndocument text mining experiments, and to show how the concept of generic,\ntypesafe annotation corresponds to a general information model that goes beyond\ntext processing.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 17:46:20 GMT"}], "update_date": "2011-08-02", "authors_parsed": [["Steeg", "Fabian", ""]]}, {"id": "1108.0748", "submitter": "Rathipriya R", "authors": "R. Rathipriya, K. Thangavel, J. Bagyamani", "title": "Binary Particle Swarm Optimization based Biclustering of Web usage Data", "comments": null, "journal-ref": null, "doi": "10.5120/3001-4036", "report-no": null, "categories": "cs.IR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web mining is the nontrivial process to discover valid, novel, potentially\nuseful knowledge from web data using the data mining techniques or methods. It\nmay give information that is useful for improving the services offered by web\nportals and information access and retrieval tools. With the rapid development\nof biclustering, more researchers have applied the biclustering technique to\ndifferent fields in recent years. When biclustering approach is applied to the\nweb usage data it automatically captures the hidden browsing patterns from it\nin the form of biclusters. In this work, swarm intelligent technique is\ncombined with biclustering approach to propose an algorithm called Binary\nParticle Swarm Optimization (BPSO) based Biclustering for Web Usage Data. The\nmain objective of this algorithm is to retrieve the global optimal bicluster\nfrom the web usage data. These biclusters contain relationships between web\nusers and web pages which are useful for the E-Commerce applications like web\nadvertising and marketing. Experiments are conducted on real dataset to prove\nthe efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 05:54:26 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 06:42:45 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Rathipriya", "R.", ""], ["Thangavel", "K.", ""], ["Bagyamani", "J.", ""]]}, {"id": "1108.0895", "submitter": "Ping Li", "authors": "Ping Li and Christian Konig", "title": "Accurate Estimators for Improving Minwise Hashing and b-Bit Minwise\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing is the standard technique in the context of search and\ndatabases for efficiently estimating set (e.g., high-dimensional 0/1 vector)\nsimilarities. Recently, b-bit minwise hashing was proposed which significantly\nimproves upon the original minwise hashing in practice by storing only the\nlowest b bits of each hashed value, as opposed to using 64 bits. b-bit hashing\nis particularly effective in applications which mainly concern sets of high\nsimilarities (e.g., the resemblance >0.5). However, there are other important\napplications in which not just pairs of high similarities matter. For example,\nmany learning algorithms require all pairwise similarities and it is expected\nthat only a small fraction of the pairs are similar. Furthermore, many\napplications care more about containment (e.g., how much one object is\ncontained by another object) than the resemblance. In this paper, we show that\nthe estimators for minwise hashing and b-bit minwise hashing used in the\ncurrent practice can be systematically improved and the improvements are most\nsignificant for set pairs of low resemblance and high containment.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2011 17:08:11 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Christian", ""]]}, {"id": "1108.1228", "submitter": "Dominic Tsang", "authors": "Dominic Tsang, Sanjay Chawla", "title": "An index for regular expression queries: Design and implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The like regular expression predicate has been part of the SQL standard since\nat least 1989. However, despite its popularity and wide usage, database vendors\nprovide only limited indexing support for regular expression queries which\nalmost always require a full table scan.\n  In this paper we propose a rigorous and robust approach for providing\nindexing support for regular expression queries. Our approach consists of\nformulating the indexing problem as a combinatorial optimization problem. We\nbegin with a database, abstracted as a collection of strings. From this data\nset we generate a query workload. The input to the optimization problem is the\ndatabase and the workload. The output is a set of multigrams (substrings) which\ncan be used as keys to records which satisfy the query workload. The multigrams\ncan then be integrated with the data structure (like B+ trees) to provide\nindexing support for the queries. We provide a deterministic and a randomized\napproximation algorithm (with provable guarantees) to solve the optimization\nproblem. Extensive experiments on synthetic data sets demonstrate that our\napproach is accurate and efficient.\n  We also present a case study on PROSITE patterns - which are complex regular\nexpression signatures for classes of proteins. Again, we are able to\ndemonstrate the utility of our indexing approach in terms of accuracy and\nefficiency. Thus, perhaps for the first time, there is a robust and practical\nindexing mechanism for an important class of database queries.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 23:14:25 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2011 13:50:00 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Tsang", "Dominic", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1108.1956", "submitter": "George Beskales", "authors": "George Beskales, Marcus Fontoura, Maxim Gurevich, Sergei\n  Vassilvitskii, Vanja Josifovski", "title": "Factorization-based Lossless Compression of Inverted Indices", "comments": "To Appear as a short paper in CIKM'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale Web applications that require ranked top-k retrieval such as\nWeb search and online advertising are implemented using inverted indices. An\ninverted index represents a sparse term-document matrix, where non-zero\nelements indicate the strength of term-document association. In this work, we\npresent an approach for lossless compression of inverted indices. Our approach\nmaps terms in a document corpus to a new term space in order to reduce the\nnumber of non-zero elements in the term-document matrix, resulting in a more\ncompact inverted index. We formulate the problem of selecting a new term space\nthat minimizes the resulting index size as a matrix factorization problem, and\nprove that finding the optimal factorization is an NP-hard problem. We develop\na greedy algorithm for finding an approximate solution. A side effect of our\napproach is increasing the number of terms in the index, which may negatively\naffect query evaluation performance. To eliminate such effect, we develop a\nmethodology for modifying query evaluation algorithms by exploiting specific\nproperties of our compression approach. Our experimental evaluation\ndemonstrates that our approach achieves an index size reduction of 20%, while\nmaintaining the same query response times. Higher compression ratios up to 35%\nare achievable, however at the cost of slightly longer query response times.\nFurthermore, combining our approach with other lossless compression techniques,\nnamely variable-byte encoding, leads to index size reduction of up to 50%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 15:25:17 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Beskales", "George", ""], ["Fontoura", "Marcus", ""], ["Gurevich", "Maxim", ""], ["Vassilvitskii", "Sergei", ""], ["Josifovski", "Vanja", ""]]}, {"id": "1108.1986", "submitter": "Ezhil Arasi M", "authors": "D. P. Acharjya, and L. Ezhilarasi", "title": "A Knowledge Mining Model for Ranking Institutions using Rough Computing\n  with Ordering Rules and Formal Concept analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Emergences of computers and information technological revolution made\ntremendous changes in the real world and provides a different dimension for the\nintelligent data analysis. Well formed fact, the information at right time and\nat right place deploy a better knowledge.However, the challenge arises when\nlarger volume of inconsistent data is given for decision making and knowledge\nextraction. To handle such imprecise data certain mathematical tools of greater\nimportance has developed by researches in recent past namely fuzzy set,\nintuitionistic fuzzy set, rough Set, formal concept analysis and ordering\nrules. It is also observed that many information system contains numerical\nattribute values and therefore they are almost similar instead of exact\nsimilar. To handle such type of information system, in this paper we use two\nprocesses such as pre process and post process. In pre process we use rough set\non intuitionistic fuzzy approximation space with ordering rules for finding the\nknowledge whereas in post process we use formal concept analysis to explore\nbetter knowledge and vital factors affecting decisions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2011 17:11:02 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Acharjya", "D. P.", ""], ["Ezhilarasi", "L.", ""]]}, {"id": "1108.2685", "submitter": "Sreenivas Gollapudi", "authors": "Sreenivas Gollapudi, Samuel Ieong, Alexandros Ntoulas, Stelios\n  Paparizos", "title": "Efficient Query Rewrite for Structured Web Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search engines and specialized online verticals are increasingly\nincorporating results from structured data sources to answer semantically rich\nuser queries. For example, the query \\WebQuery{Samsung 50 inch led tv} can be\nanswered using information from a table of television data. However, the users\nare not domain experts and quite often enter values that do not match precisely\nthe underlying data. Samsung makes 46- or 55- inch led tvs, but not 50-inch\nones. So a literal execution of the above mentioned query will return zero\nresults. For optimal user experience, a search engine would prefer to return at\nleast a minimum number of results as close to the original query as possible.\nFurthermore, due to typical fast retrieval speeds in web-search, a search\nengine query execution is time-bound.\n  In this paper, we address these challenges by proposing algorithms that\nrewrite the user query in a principled manner, surfacing at least the required\nnumber of results while satisfying the low-latency constraint. We formalize\nthese requirements and introduce a general formulation of the problem. We show\nthat under a natural formulation, the problem is NP-Hard to solve optimally,\nand present approximation algorithms that produce good rewrites. We empirically\nvalidate our algorithms on large-scale data obtained from a commercial search\nengine's shopping vertical.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2011 18:37:11 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Gollapudi", "Sreenivas", ""], ["Ieong", "Samuel", ""], ["Ntoulas", "Alexandros", ""], ["Paparizos", "Stelios", ""]]}, {"id": "1108.2754", "submitter": "Karthik Raman", "authors": "Karthik Raman, Thorsten Joachims and Pannaga Shivaswamy", "title": "Structured Learning of Two-Level Dynamic Rankings", "comments": "10 Pages (Longer Version of CIKM 2011 paper containing more details\n  and experiments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For ambiguous queries, conventional retrieval systems are bound by two\nconflicting goals. On the one hand, they should diversify and strive to present\nresults for as many query intents as possible. On the other hand, they should\nprovide depth for each intent by displaying more than a single result. Since\nboth diversity and depth cannot be achieved simultaneously in the conventional\nstatic retrieval model, we propose a new dynamic ranking approach. Dynamic\nranking models allow users to adapt the ranking through interaction, thus\novercoming the constraints of presenting a one-size-fits-all static ranking. In\nparticular, we propose a new two-level dynamic ranking model for presenting\nsearch results to the user. In this model, a user's interactions with the\nfirst-level ranking are used to infer this user's intent, so that second-level\nrankings can be inserted to provide more results relevant for this intent.\nUnlike for previous dynamic ranking models, we provide an algorithm to\nefficiently compute dynamic rankings with provable approximation guarantees for\na large family of performance measures. We also propose the first principled\nalgorithm for learning dynamic ranking functions from training data. In\naddition to the theoretical results, we provide empirical evidence\ndemonstrating the gains in retrieval quality that our method achieves over\nconventional approaches.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2011 03:22:48 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Raman", "Karthik", ""], ["Joachims", "Thorsten", ""], ["Shivaswamy", "Pannaga", ""]]}, {"id": "1108.3298", "submitter": "Nando de Freitas", "authors": "Byron Knoll, Nando de Freitas", "title": "A Machine Learning Perspective on Predictive Coding with PAQ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAQ8 is an open source lossless data compression algorithm that currently\nachieves the best compression rates on many benchmarks. This report presents a\ndetailed description of PAQ8 from a statistical machine learning perspective.\nIt shows that it is possible to understand some of the modules of PAQ8 and use\nthis understanding to improve the method. However, intuitive statistical\nexplanations of the behavior of other modules remain elusive. We hope the\ndescription in this report will be a starting point for discussions that will\nincrease our understanding, lead to improvements to PAQ8, and facilitate a\ntransfer of knowledge from PAQ8 to other machine learning methods, such a\nrecurrent neural networks and stochastic memoizers. Finally, the report\npresents a broad range of new applications of PAQ to machine learning tasks\nincluding language modeling and adaptive text prediction, adaptive game\nplaying, classification, and compression using features from the field of deep\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 18:06:29 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Knoll", "Byron", ""], ["de Freitas", "Nando", ""]]}, {"id": "1108.4516", "submitter": "Yanwei Xu", "authors": "Yanwei XU", "title": "Scalable Continual Top-k Keyword Search in Relational Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword search in relational databases has been widely studied in recent\nyears because it does not require users neither to master a certain structured\nquery language nor to know the complex underlying database schemas. Most of\nexisting methods focus on answering snapshot keyword queries in static\ndatabases. In practice, however, databases are updated frequently, and users\nmay have long-term interests on specific topics. To deal with such a situation,\nit is necessary to build effective and efficient facility in a database system\nto support continual keyword queries.\n  In this paper, we propose an efficient method for answering continual top-$k$\nkeyword queries over relational databases. The proposed method is built on an\nexisting scheme of keyword search on relational data streams, but incorporates\nthe ranking mechanisms into the query processing methods and makes two\nimprovements to support efficient top-$k$ keyword search in relational\ndatabases. Compared to the existing methods, our method is more efficient both\nin computing the top-$k$ results in a static database and in maintaining the\ntop-$k$ results when the database continually being updated. Experimental\nresults validate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2011 07:52:52 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["XU", "Yanwei", ""]]}, {"id": "1108.4801", "submitter": "Karthik Subbian", "authors": "Karthik Subbian and Prem Melville", "title": "Supervised Rank Aggregation for Predicting Influence in Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GT cs.IR physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Much work in Social Network Analysis has focused on the identification of the\nmost important actors in a social network. This has resulted in several\nmeasures of influence and authority. While most of such sociometrics (e.g.,\nPageRank) are driven by intuitions based on an actors location in a network,\nasking for the \"most influential\" actors in itself is an ill-posed question,\nunless it is put in context with a specific measurable task. Constructing a\npredictive task of interest in a given domain provides a mechanism to\nquantitatively compare different measures of influence. Furthermore, when we\nknow what type of actionable insight to gather, we need not rely on a single\nnetwork centrality measure. A combination of measures is more likely to capture\nvarious aspects of the social network that are predictive and beneficial for\nthe task. Towards this end, we propose an approach to supervised rank\naggregation, driven by techniques from Social Choice Theory. We illustrate the\neffectiveness of this method through experiments on Twitter and citation\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 10:32:35 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Subbian", "Karthik", ""], ["Melville", "Prem", ""]]}, {"id": "1108.5460", "submitter": "Mohamed Quafafou", "authors": "Zahi Jarir, Mohamed Quafafou, Mahammed Erradi", "title": "Personalized Web Services for Web Information Extraction", "comments": null, "journal-ref": "International Journal of Web Services Practices, Vol. 5, No.1\n  (2010), pp. 22-31", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of information extraction from the Web emerged with the growth of\nthe Web and the multiplication of online data sources. This paper is an\nanalysis of information extraction methods. It presents a service oriented\napproach for web information extraction considering both web data management\nand extraction services. Then we propose an SOA based architecture to enhance\nflexibility and on-the-fly modification of web extraction services. An\nimplementation of the proposed architecture is proposed on the middleware level\nof Java Enterprise Edition (JEE) servers.\n", "versions": [{"version": "v1", "created": "Sat, 27 Aug 2011 15:49:28 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Jarir", "Zahi", ""], ["Quafafou", "Mohamed", ""], ["Erradi", "Mahammed", ""]]}, {"id": "1108.5491", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Improving Ranking Using Quantum Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.ET cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper shows that ranking information units by quantum probability differs\nfrom ranking them by classical probability provided the same data used for\nparameter estimation. As probability of detection (also known as recall or\npower) and probability of false alarm (also known as fallout or size) measure\nthe quality of ranking, we point out and show that ranking by quantum\nprobability yields higher probability of detection than ranking by classical\nprobability provided a given probability of false alarm and the same parameter\nestimation data. As quantum probability provided more effective detectors than\nclassical probability within other domains that data management, we conjecture\nthat, the system that can implement subspace-based detectors shall be more\neffective than a system which implements a set-based detectors, the\neffectiveness being calculated as expected recall estimated over the\nprobability of detection and expected fallout estimated over the probability of\nfalse alarm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2011 02:55:18 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.5575", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Getting Beyond the State of the Art of Information Retrieval with\n  Quantum Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  According to the probability ranking principle, the document set with the\nhighest values of probability of relevance optimizes information retrieval\neffectiveness given the probabilities are estimated as accurately as possible.\nThe key point of this principle is the separation of the document set into two\nsubsets with a given level of fallout and with the highest recall. If subsets\nof set measures are replaced by subspaces and space measures, we obtain an\nalternative theory stemming from Quantum Theory. That theory is named after\nvector probability because vectors represent event like sets do in classical\nprobability. The paper shows that the separation into vector subspaces is more\neffective than the separation into subsets with the same available evidence.\nThe result is proved mathematically and verified experimentally. In general,\nthe paper suggests that quantum theory is not only a source of rhetoric\ninspiration, but is a sufficient condition to improve retrieval effectiveness\nin a principled way.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 14:37:39 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.5703", "submitter": "Sivakumar Madesan", "authors": "Jeevan H E, Prashanth P P, Punith Kumar S N, Vinay Hegde", "title": "Web Pages Clustering: A New Approach", "comments": "Clustering, concept mining, information retrieval, metasearch engine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of web has resulted in vast volume of information.\nInformation availability at a rapid speed to the user is vital. English\nlanguage (or any for that matter) has lot of ambiguity in the usage of words.\nSo there is no guarantee that a keyword based search engine will provide the\nrequired results. This paper introduces the use of dictionary (standardised) to\nobtain the context with which a keyword is used and in turn cluster the results\nbased on this context. These ideas can be merged with a metasearch engine to\nenhance the search efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 07:02:35 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["E", "Jeevan H", ""], ["P", "Prashanth P", ""], ["N", "Punith Kumar S", ""], ["Hegde", "Vinay", ""]]}, {"id": "1108.5784", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Probability Ranking in Vector Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Probability Ranking Principle states that the document set with the\nhighest values of probability of relevance optimizes information retrieval\neffectiveness given the probabilities are estimated as accurately as possible.\nThe key point of the principle is the separation of the document set into two\nsubsets with a given level of fallout and with the highest recall. The paper\nintroduces the separation between two vector subspaces and shows that the\nseparation yields a more effective performance than the optimal separation into\nsubsets with the same available evidence, the performance being measured with\nrecall and fallout. The result is proved mathematically and exemplified\nexperimentally.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 00:31:44 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1108.6003", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Massimiliano Zanin, Perfecto Herrera and Xavier Serra", "title": "Characterization and exploitation of community structure in cover song\n  networks", "comments": null, "journal-ref": "Pattern Recognition Letters 33(9): 1032-1041, 2012", "doi": "10.1016/j.patrec.2012.02.013", "report-no": null, "categories": "cs.IR cs.MM cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of community detection algorithms is explored within the framework of\ncover song identification, i.e. the automatic detection of different audio\nrenditions of the same underlying musical piece. Until now, this task has been\nposed as a typical query-by-example task, where one submits a query song and\nthe system retrieves a list of possible matches ranked by their similarity to\nthe query. In this work, we propose a new approach which uses song communities\nto provide more relevant answers to a given query. Starting from the output of\na state-of-the-art system, songs are embedded in a complex weighted network\nwhose links represent similarity (related musical content). Communities inside\nthe network are then recognized as groups of covers and this information is\nused to enhance the results of the system. In particular, we show that this\napproach increases both the coherence and the accuracy of the system.\nFurthermore, we provide insight into the internal organization of individual\ncover song communities, showing that there is a tendency for the original song\nto be central within the community. We postulate that the methods and results\npresented here could be relevant to other query-by-example tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2011 15:25:20 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2011 10:46:30 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Zanin", "Massimiliano", ""], ["Herrera", "Perfecto", ""], ["Serra", "Xavier", ""]]}, {"id": "1108.6016", "submitter": "Benjamin Rubinstein", "authors": "Jim Gemmell and Benjamin I. P. Rubinstein and Ashok K. Chandra", "title": "Improving Entity Resolution with Global Constraints", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2011-100", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the greatest advances in web search have come from leveraging\nsocio-economic properties of online user behavior. Past advances include\nPageRank, anchor text, hubs-authorities, and TF-IDF. In this paper, we\ninvestigate another socio-economic property that, to our knowledge, has not yet\nbeen exploited: sites that create lists of entities, such as IMDB and Netflix,\nhave an incentive to avoid gratuitous duplicates. We leverage this property to\nresolve entities across the different web sites, and find that we can obtain\nsubstantial improvements in resolution accuracy. This improvement in accuracy\nalso translates into robustness, which often reduces the amount of training\ndata that must be labeled for comparing entities across many sites.\nFurthermore, the technique provides robustness when resolving sites that have\nsome duplicates, even without first removing these duplicates. We present\nalgorithms with very strong precision and recall, and show that max weight\nmatching, while appearing to be a natural choice turns out to have poor\nperformance in some situations. The presented techniques are now being used in\nthe back-end entity resolution system at a major Internet search engine.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 17:30:54 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Gemmell", "Jim", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Chandra", "Ashok K.", ""]]}, {"id": "1108.6198", "submitter": "Gadda Koteswara Rao", "authors": "G.Koteswara Rao, Shubhamoy Dey", "title": "Decision Support for e-Governance: A Text Mining Approach", "comments": "19 Pages, 7 Figures", "journal-ref": "International Journal of Managing Information Technology (IJMIT)\n  Vol.3, No.3, 2011, 73-91", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Information and communication technology has the capability to improve the\nprocess by which governments involve citizens in formulating public policy and\npublic projects. Even though much of government regulations may now be in\ndigital form (and often available online), due to their complexity and\ndiversity, identifying the ones relevant to a particular context is a\nnon-trivial task. Similarly, with the advent of a number of electronic online\nforums, social networking sites and blogs, the opportunity of gathering\ncitizens' petitions and stakeholders' views on government policy and proposals\nhas increased greatly, but the volume and the complexity of analyzing\nunstructured data makes this difficult. On the other hand, text mining has come\na long way from simple keyword search, and matured into a discipline capable of\ndealing with much more complex tasks. In this paper we discuss how text-mining\ntechniques can help in retrieval of information and relationships from textual\ndata sources, thereby assisting policy makers in discovering associations\nbetween policies and citizens' opinions expressed in electronic public forums\nand blogs etc. We also present here, an integrated text mining based\narchitecture for e-governance decision support along with a discussion on the\nIndian scenario.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2011 11:48:57 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["Rao", "G. Koteswara", ""], ["Dey", "Shubhamoy", ""]]}]