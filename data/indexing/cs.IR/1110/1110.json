[{"id": "1110.0289", "submitter": "Gerald Kembellec", "authors": "G\\'erald Kembellec", "title": "Repr\\'esentation de donn\\'ees et m\\'etadonn\\'ees dans une biblioth\\`eque\n  virtuelle pour une ad\\'equation avec l'usager et les outils de glanage ou\n  moissonnage scientifique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vehicles for official knowledge, as well as university libraries, suffer\nfrom an increasingly visible lack of interest. This is due to the advent of\nfully digital practices. By studying the psychological and cognitive models in\ninformation retrieval initiated in the 1980s, it is possible to use these\ntheories and apply them practically to the Information Retrieval System, taking\ninto account the requirements of virtual libraries. New metadata standards\nalong with modern tools that help managing references should help automating\nthe process of scientific research. We offer a practical implementation of the\ngiven theories to test them when they are applied to the information retrieval\nin computer sciences. This case under study will highlight good practices in\ngleaning and harvesting scientific literature.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 08:07:11 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Kembellec", "G\u00e9rald", ""]]}, {"id": "1110.0336", "submitter": "Gerald Kembellec", "authors": "G\\'erald Kembellec, Imad Saleh, Catherine Sauvaget (LIASD)", "title": "OntologyNavigator: WEB 2.0 scalable ontology based CLIR portal to IT\n  scientific corpus for researchers", "comments": "International Journal of Design Sciences and Technology 16, 2 (2009)\n  http://europia.org/IJDST/vol16/IJDST_V16N2_2009_paper%203.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the architecture used in the ongoing OntologyNavigator\nproject. It is a research tool to help advanced learners to find adapted IT\npapers to create scientific bibliographies. The purpose is the use of an IT\nrepresentation as educational research software for researchers. We use an\nontology based on the ACM's Computing Classification System in order to find\nscientific papers directly related to the new researcher's domain without any\nformal request. An ontology translation in French is automatically proposed and\ncan be based on Web 2.0 enhanced by a community of users. A visualization and\nnavigation model is proposed to make it more accessible and examples are given\nto show the interface of the tool. This model offers the possibility of cross\nlanguage query. Users deeply interact with the translation by providing\nalternative translation of the node label. Customers also enrich the ontology\nnode labels with implicit descriptors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2011 12:10:38 GMT"}], "update_date": "2012-02-07", "authors_parsed": [["Kembellec", "G\u00e9rald", "", "LIASD"], ["Saleh", "Imad", "", "LIASD"], ["Sauvaget", "Catherine", "", "LIASD"]]}, {"id": "1110.0704", "submitter": "Oren Somekh", "authors": "Ronen Barenboim, Edward Bortnikov, Nadav Golbandi, Amit Kagian, Liran\n  Katzir, Ronny Lempel, Hayim Makabee, Scott Roy, and Oren Somekh", "title": "Hierarchical Composable Optimization of Web Pages", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of creating modern Web media experiences is challenged by the\nneed to adapt the content and presentation choices to dynamic real-time\nfluctuations of user interest across multiple audiences. We introduce FAME - a\nFramework for Agile Media Experiences - which addresses this scalability\nproblem. FAME allows media creators to define abstract page models that are\nsubsequently transformed into real experiences through algorithmic\nexperimentation. FAME's page models are hierarchically composed of simple\nbuilding blocks, mirroring the structure of most Web pages. They are resolved\ninto concrete page instances by pluggable algorithms which optimize the pages\nfor specific business goals. Our framework allows retrieving dynamic content\nfrom multiple sources, defining the experimentation's degrees of freedom, and\nconstraining the algorithmic choices. It offers an effective separation of\nconcerns in the media creation process, enabling multiple stakeholders with\nprofoundly different skills to apply their crafts and perform their duties\nindependently, composing and reusing each other's work in modular ways.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 14:33:26 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Barenboim", "Ronen", ""], ["Bortnikov", "Edward", ""], ["Golbandi", "Nadav", ""], ["Kagian", "Amit", ""], ["Katzir", "Liran", ""], ["Lempel", "Ronny", ""], ["Makabee", "Hayim", ""], ["Roy", "Scott", ""], ["Somekh", "Oren", ""]]}, {"id": "1110.0725", "submitter": "Paulo Jesus", "authors": "Paulo Jesus, Carlos Baquero, Paulo S\\'ergio Almeida", "title": "A Survey of Distributed Data Aggregation Algorithms", "comments": "45 pages, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data aggregation is an important task, allowing the decentralized\ndetermination of meaningful global properties, that can then be used to direct\nthe execution of other applications. The resulting values result from the\ndistributed computation of functions like COUNT, SUM and AVERAGE. Some\napplication examples can found to determine the network size, total storage\ncapacity, average load, majorities and many others.\n  In the last decade, many different approaches have been proposed, with\ndifferent trade-offs in terms of accuracy, reliability, message and time\ncomplexity. Due to the considerable amount and variety of aggregation\nalgorithms, it can be difficult and time consuming to determine which\ntechniques will be more appropriate to use in specific settings, justifying the\nexistence of a survey to aid in this task.\n  This work reviews the state of the art on distributed data aggregation\nalgorithms, providing three main contributions. First, it formally defines the\nconcept of aggregation, characterizing the different types of aggregation\nfunctions. Second, it succinctly describes the main aggregation techniques,\norganizing them in a taxonomy. Finally, it provides some guidelines toward the\nselection and use of the most relevant techniques, summarizing their principal\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 15:24:25 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Jesus", "Paulo", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "1110.1112", "submitter": "Xuanhui Wang", "authors": "Changsung Kang and Xiaotong Lin and Xuanhui Wang and Yi Chang and\n  Belle Tseng", "title": "Modeling Perceived Relevance for Tail Queries without Click-Through Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through data has been used in various ways in Web search such as\nestimating relevance between documents and queries. Since only search snippets\nare perceived by users before issuing any clicks, the relevance induced by\nclicks are usually called \\emph{perceived relevance} which has proven to be\nquite useful for Web search. While there is plenty of click data for popular\nqueries, very little information is available for unpopular tail ones. These\ntail queries take a large portion of the search volume but search accuracy for\nthese queries is usually unsatisfactory due to data sparseness such as limited\nclick information. In this paper, we study the problem of modeling perceived\nrelevance for queries without click-through data. Instead of relying on users'\nclick data, we carefully design a set of snippet features and use them to\napproximately capture the perceived relevance. We study the effectiveness of\nthis set of snippet features in two settings: (1) predicting perceived\nrelevance and (2) enhancing search engine ranking. Experimental results show\nthat our proposed model is effective to predict the relative perceived\nrelevance of Web search results. Furthermore, our proposed snippet features are\neffective to improve search accuracy for longer tail queries without\nclick-through data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 21:48:53 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Kang", "Changsung", ""], ["Lin", "Xiaotong", ""], ["Wang", "Xuanhui", ""], ["Chang", "Yi", ""], ["Tseng", "Belle", ""]]}, {"id": "1110.1328", "submitter": "Venu Satuluri", "authors": "Venu Satuluri and Srinivasan Parthasarathy", "title": "Bayesian Locality Sensitive Hashing for Fast Similarity Search", "comments": "13 pages, 5 Tables, 21 figures. Added acknowledgments in v3. A\n  slightly shorter version of this paper without the appendix has been\n  published in the PVLDB journal, 5(5):430-441, 2012.\n  http://vldb.org/pvldb/vol5/p430_venusatuluri_vldb2012.pdf", "journal-ref": "PVLDB 5(5):430-441, 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of objects and an associated similarity measure, the\nall-pairs similarity search problem asks us to find all pairs of objects with\nsimilarity greater than a certain user-specified threshold. Locality-sensitive\nhashing (LSH) based methods have become a very popular approach for this\nproblem. However, most such methods only use LSH for the first phase of\nsimilarity search - i.e. efficient indexing for candidate generation. In this\npaper, we present BayesLSH, a principled Bayesian algorithm for the subsequent\nphase of similarity search - performing candidate pruning and similarity\nestimation using LSH. A simpler variant, BayesLSH-Lite, which calculates\nsimilarities exactly, is also presented. BayesLSH is able to quickly prune away\na large majority of the false positive candidate pairs, leading to significant\nspeedups over baseline approaches. For BayesLSH, we also provide probabilistic\nguarantees on the quality of the output, both in terms of accuracy and recall.\nFinally, the quality of BayesLSH's output can be easily tuned and does not\nrequire any manual setting of the number of hashes to use for similarity\nestimation, unlike standard approaches. For two state-of-the-art candidate\ngeneration algorithms, AllPairs and LSH, BayesLSH enables significant speedups,\ntypically in the range 2x-20x for a wide variety of datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2011 17:13:48 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2011 17:46:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2012 19:34:39 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Satuluri", "Venu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1110.2610", "submitter": "Parul Agarwal", "authors": "Parul Agarwal, M.Afshar Alam, Ranjit Biswas", "title": "Issues,Challenges and Tools of Clustering Algorithms", "comments": "6 PAGES", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 2, May 2011 ISSN (Online): 1694-0814 page numbers 523-528", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an unsupervised technique of Data Mining. It means grouping\nsimilar objects together and separating the dissimilar ones. Each object in the\ndata set is assigned a class label in the clustering process using a distance\nmeasure. This paper has captured the problems that are faced in real when\nclustering algorithms are implemented .It also considers the most extensively\nused tools which are readily available and support functions which ease the\nprogramming. Once algorithms have been implemented, they also need to be tested\nfor its validity. There exist several validation indexes for testing the\nperformance and accuracy which have also been discussed here.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 09:27:58 GMT"}], "update_date": "2011-10-13", "authors_parsed": [["Agarwal", "Parul", ""], ["Alam", "M. Afshar", ""], ["Biswas", "Ranjit", ""]]}, {"id": "1110.3088", "submitter": "Nigel Collier", "authors": "Nigel Collier", "title": "Towards cross-lingual alerting for bursty epidemic events", "comments": null, "journal-ref": "Journal of Biomedical Semantics 2011, 2(Suppl 5):S10", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Background: Online news reports are increasingly becoming a source for event\nbased early warning systems that detect natural disasters. Harnessing the\nmassive volume of information available from multilingual newswire presents as\nmany challenges as opportunities due to the patterns of reporting complex\nspatiotemporal events. Results: In this article we study the problem of\nutilising correlated event reports across languages. We track the evolution of\n16 disease outbreaks using 5 temporal aberration detection algorithms on\ntext-mined events classified according to disease and outbreak country. Using\nProMED reports as a silver standard, comparative analysis of news data for 13\nlanguages over a 129 day trial period showed improved sensitivity, F1 and\ntimeliness across most models using cross-lingual events. We report a detailed\ncase study analysis for Cholera in Angola 2010 which highlights the challenges\nfaced in correlating news events with the silver standard. Conclusions: The\nresults show that automated health surveillance using multilingual text mining\nhas the potential to turn low value news into high value alerts if informed\nchoices are used to govern the selection of models and data sources. An\nimplementation of the C2 alerting algorithm using multilingual news is\navailable at the BioCaster portal http://born.nii.ac.jp/?page=globalroundup.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 23:05:48 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Collier", "Nigel", ""]]}, {"id": "1110.3089", "submitter": "Nigel Collier", "authors": "Nigel Collier, Nguyen Truong Son, Ngoc Mai Nguyen", "title": "OMG U got flu? Analysis of shared health messages for bio-surveillance", "comments": null, "journal-ref": "Journal of Biomedical Semantics 2011, 2(Suppl 5):S9", "doi": "10.1186/2041-1480-2-S5-S9", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Background: Micro-blogging services such as Twitter offer the potential to\ncrowdsource epidemics in real-time. However, Twitter posts ('tweets') are often\nambiguous and reactive to media trends. In order to ground user messages in\nepidemic response we focused on tracking reports of self-protective behaviour\nsuch as avoiding public gatherings or increased sanitation as the basis for\nfurther risk analysis. Results: We created guidelines for tagging self\nprotective behaviour based on Jones and Salath\\'e (2009)'s behaviour response\nsurvey. Applying the guidelines to a corpus of 5283 Twitter messages related to\ninfluenza like illness showed a high level of inter-annotator agreement (kappa\n0.86). We employed supervised learning using unigrams, bigrams and regular\nexpressions as features with two supervised classifiers (SVM and Naive Bayes)\nto classify tweets into 4 self-reported protective behaviour categories plus a\nself-reported diagnosis. In addition to classification performance we report\nmoderately strong Spearman's Rho correlation by comparing classifier output\nagainst WHO/NREVSS laboratory data for A(H1N1) in the USA during the 2009-2010\ninfluenza season. Conclusions: The study adds to evidence supporting a high\ndegree of correlation between pre-diagnostic social media signals and\ndiagnostic influenza case data, pointing the way towards low cost sensor\nnetworks. We believe that the signals we have modelled may be applicable to a\nwide range of diseases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 23:15:44 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Collier", "Nigel", ""], ["Son", "Nguyen Truong", ""], ["Nguyen", "Ngoc Mai", ""]]}, {"id": "1110.3091", "submitter": "Nigel Collier", "authors": "Nigel Collier", "title": "What's unusual in online disease outbreak news?", "comments": null, "journal-ref": "Journal of Biomedical Semantics 2010, 1:2", "doi": "10.1186/2041-1480-1-2", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Background: Accurate and timely detection of public health events of\ninternational concern is necessary to help support risk assessment and response\nand save lives. Novel event-based methods that use the World Wide Web as a\nsignal source offer potential to extend health surveillance into areas where\ntraditional indicator networks are lacking. In this paper we address the issue\nof systematically evaluating online health news to support automatic alerting\nusing daily disease-country counts text mined from real world data using\nBioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration\ndetection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance\nagainst expert moderated ProMED-mail postings. Results: We report sensitivity,\nspecificity, positive predictive value (PPV), negative predictive value (NPV),\nmean alerts/100 days and F1, at 95% confidence interval (CI) for 287\nProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period.\nResults indicate that W2 had the best F1 with a slight benefit for day of week\neffect over C2. In drill down analysis we indicate issues arising from the\ngranular choice of country-level modeling, sudden drops in reporting due to day\nof week effects and reporting bias. Automatic alerting has been implemented in\nBioCaster available from http://born.nii.ac.jp. Conclusions: Online health news\nalerts have the potential to enhance manual analytical methods by increasing\nthroughput, timeliness and detection rates. Systematic evaluation of health\nnews aberrations is necessary to push forward our understanding of the complex\nrelationship between news report volumes and case numbers and to select the\nbest performing features and algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 23:22:43 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Collier", "Nigel", ""]]}, {"id": "1110.3094", "submitter": "Nigel Collier", "authors": "Nigel Collier, Son Doan", "title": "Syndromic classification of Twitter messages", "comments": "10 pages, 2 figures, eHealth 2011 conference, Malaga (Spain)\n  (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recent studies have shown strong correlation between social networking data\nand national influenza rates. We expanded upon this success to develop an\nautomated text mining system that classifies Twitter messages in real time into\nsix syndromic categories based on key terms from a public health ontology.\n10-fold cross validation tests were used to compare Naive Bayes (NB) and\nSupport Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM\nperformed better than NB on 4 out of 6 syndromes. The best performing\nclassifiers showed moderately strong F1 scores: respiratory = 86.2 (NB);\ngastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6\n(SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1);\nconstitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9\n(NB). The resulting classifiers were deployed together with an EARS C2\naberration detection algorithm in an experimental online system.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 23:42:32 GMT"}], "update_date": "2011-10-17", "authors_parsed": [["Collier", "Nigel", ""], ["Doan", "Son", ""]]}, {"id": "1110.3767", "submitter": "Herve Jegou", "authors": "Herv\\'e J\\'egou (INRIA - IRISA), Teddy Furon (INRIA - IRISA),\n  Jean-Jacques Fuchs (INRIA - IRISA)", "title": "Anti-sparse coding for approximate nearest neighbor search", "comments": "submitted to ICASSP'2012; RR-7771 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a binarization scheme for vectors of high dimension based\non the recent concept of anti-sparse coding, and shows its excellent\nperformance for approximate nearest neighbor search. Unlike other binarization\nschemes, this framework allows, up to a scaling factor, the explicit\nreconstruction from the binary representation of the original vector. The paper\nalso shows that random projections which are used in Locality Sensitive Hashing\nalgorithms, are significantly outperformed by regular frames for both synthetic\nand real data if the number of bits exceeds the vector dimensionality, i.e.,\nwhen high precision is required.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 19:16:16 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2011 06:13:45 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["J\u00e9gou", "Herv\u00e9", "", "INRIA - IRISA"], ["Furon", "Teddy", "", "INRIA - IRISA"], ["Fuchs", "Jean-Jacques", "", "INRIA - IRISA"]]}, {"id": "1110.3917", "submitter": "Bassam Mokbel", "authors": "Wouter Lueks, Bassam Mokbel, Michael Biehl, Barbara Hammer", "title": "How to Evaluate Dimensionality Reduction? - Improving the Co-ranking\n  Matrix", "comments": "This is an article for the Dagstuhl Preprint Archive, belonging to\n  Dagstuhl Seminar No. 11341 \"Learning in the context of very high dimensional\n  data\"", "journal-ref": null, "doi": null, "report-no": "DPA-11341", "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing number of dimensionality reduction methods available for data\nvisualization has recently inspired the development of quality assessment\nmeasures, in order to evaluate the resulting low-dimensional representation\nindependently from a methods' inherent criteria. Several (existing) quality\nmeasures can be (re)formulated based on the so-called co-ranking matrix, which\nsubsumes all rank errors (i.e. differences between the ranking of distances\nfrom every point to all others, comparing the low-dimensional representation to\nthe original data). The measures are often based on the partioning of the\nco-ranking matrix into 4 submatrices, divided at the K-th row and column,\ncalculating a weighted combination of the sums of each submatrix. Hence, the\nevaluation process typically involves plotting a graph over several (or even\nall possible) settings of the parameter K. Considering simple artificial\nexamples, we argue that this parameter controls two notions at once, that need\nnot necessarily be combined, and that the rectangular shape of submatrices is\ndisadvantageous for an intuitive interpretation of the parameter. We debate\nthat quality measures, as general and flexible evaluation tools, should have\nparameters with a direct and intuitive interpretation as to which specific\nerror types are tolerated or penalized. Therefore, we propose to replace K with\ntwo parameters to control these notions separately, and introduce a differently\nshaped weighting on the co-ranking matrix. The two new parameters can then\ndirectly be interpreted as a threshold up to which rank errors are tolerated,\nand a threshold up to which the rank-distances are significant for the\nevaluation. Moreover, we propose a color representation of local quality to\nvisually support the evaluation process for a given mapping, where every point\nin the mapping is colored according to its local contribution to the overall\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 09:17:29 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Lueks", "Wouter", ""], ["Mokbel", "Bassam", ""], ["Biehl", "Michael", ""], ["Hammer", "Barbara", ""]]}, {"id": "1110.4123", "submitter": "Antonios Garas", "authors": "David Garcia, Antonios Garas, and Frank Schweitzer", "title": "Positive words carry less information than negative words", "comments": "16 pages, 3 figures, 3 tables", "journal-ref": "EPJ Data Science 2012, 1:3", "doi": "10.1140/epjds3", "report-no": null, "categories": "cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the frequency of word use is not only determined by the word\nlength \\cite{Zipf1935} and the average information content\n\\cite{Piantadosi2011}, but also by its emotional content. We have analyzed\nthree established lexica of affective word usage in English, German, and\nSpanish, to verify that these lexica have a neutral, unbiased, emotional\ncontent. Taking into account the frequency of word usage, we find that words\nwith a positive emotional content are more frequently used. This lends support\nto Pollyanna hypothesis \\cite{Boucher1969} that there should be a positive bias\nin human expression. We also find that negative words contain more information\nthan positive words, as the informativeness of a word increases uniformly with\nits valence decrease. Our findings support earlier conjectures about (i) the\nrelation between word frequency and information content, and (ii) the impact of\npositive emotions on communication and social links.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 20:54:21 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2011 10:19:34 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2012 12:29:23 GMT"}, {"version": "v4", "created": "Sun, 27 May 2012 14:55:40 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Garcia", "David", ""], ["Garas", "Antonios", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1110.4844", "submitter": "Jeon-Hyung Kang", "authors": "Jeon-Hyung Kang and Jihie Kim", "title": "Analyzing Answers in Threaded Discussions using a Role-Based Information\n  Network", "comments": "The Third IEEE International Conference on Social Computing\n  (SocialCom2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online discussion boards are an important medium for collaboration. The goal\nof our work is to understand how messages and individual discussants contribute\nto Q&A discussions. We present a novel network model for capturing in-formation\nroles of messages and discussants, and show how we identify useful answers to\nthe initial question. We first classify information seeking or information\nproviding roles of messages, such as question, answer or acknowledgement. We\nalso identify user intent in the discussion as an information seeker or a\nprovider. We capture such role information within a reply-to discussion\nnetwork, and identify messages that answer seeker questions and how answeres\nare acknowledged. Message influences are analyzed using B-centrality measures.\nUser influences across different threads are combined with message influences.\nWe use the combined score in identifying the most useful answer in the thread.\nThe resulting ranks correlate with human provided ranks with an MRR score of\n0.67.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 17:15:48 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Kang", "Jeon-Hyung", ""], ["Kim", "Jihie", ""]]}, {"id": "1110.4851", "submitter": "Jeon-Hyung Kang", "authors": "Jeon-Hyung Kang and Kristina Lerman", "title": "Leveraging User Diversity to Harvest Knowledge on the Social Web", "comments": "The Third IEEE International Conference on Social Computing\n  (SocialCom2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social web users are a very diverse group with varying interests, levels of\nexpertise, enthusiasm, and expressiveness. As a result, the quality of content\nand annotations they create to organize content is also highly variable. While\nseveral approaches have been proposed to mine social annotations, for example,\nto learn folksonomies that reflect how people relate narrower concepts to\nbroader ones, these methods treat all users and the annotations they create\nuniformly. We propose a framework to automatically identify experts, i.e.,\nknowledgeable users who create high quality annotations, and use their\nknowledge to guide folksonomy learning. We evaluate the approach on a large\nbody of social annotations extracted from the photosharing site Flickr. We show\nthat using expert knowledge leads to more detailed and accurate folksonomies.\nMoreover, we show that including annotations from non-expert, or novice, users\nleads to more comprehensive folksonomies than experts' knowledge alone.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 17:34:55 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Kang", "Jeon-Hyung", ""], ["Lerman", "Kristina", ""]]}, {"id": "1110.5722", "submitter": "Fidelia Ibekwe-Sanjuan", "authors": "Fidelia Ibekwe-Sanjuan (ELICO), Fernandez Silvia (LIA), Sanjuan Eric\n  (LIA), Charton Eric (LIA)", "title": "Annotation of Scientific Summaries for Information Retrieval", "comments": "ECIR'08 Workshop on: Exploiting Semantic Annotations for Information\n  Retrieval, Glasgow : United Kingdom (2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology combining surface NLP and Machine Learning\ntechniques for ranking asbtracts and generating summaries based on annotated\ncorpora. The corpora were annotated with meta-semantic tags indicating the\ncategory of information a sentence is bearing (objective, findings, newthing,\nhypothesis, conclusion, future work, related work). The annotated corpus is fed\ninto an automatic summarizer for query-oriented abstract ranking and multi-\nabstract summarization. To adapt the summarizer to these two tasks, two novel\nweighting functions were devised in order to take into account the distribution\nof the tags in the corpus. Results, although still preliminary, are encouraging\nus to pursue this line of work and find better ways of building IR systems that\ncan take into account semantic annotations in a corpus.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2011 07:26:22 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Ibekwe-Sanjuan", "Fidelia", "", "ELICO"], ["Silvia", "Fernandez", "", "LIA"], ["Eric", "Sanjuan", "", "LIA"], ["Eric", "Charton", "", "LIA"]]}, {"id": "1110.5863", "submitter": "Owen Martin", "authors": "Owen S. Martin", "title": "A Wikipedia Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was originally designed as a literature review for a doctoral\ndissertation focusing on Wikipedia. This exposition gives the structure of\nWikipedia and the latest trends in Wikipedia research.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 21:19:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Martin", "Owen S.", ""]]}, {"id": "1110.6097", "submitter": "Lingfei Wu", "authors": "Lingfei Wu, Jiang Zhang", "title": "The Decentralized Structure of Collective Attention on the Web", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The collective browsing behavior of users gives rise to a flow\nnetwork transporting attention between websites. By analyzing the structure of\nthis network we uncovered a nontrivial scaling regularity concerning the impact\nof websites.\n  Methodology: We constructed three clickstreams networks, whose nodes were\nwebsites and edges were formed by the users switching between sites. We\ndeveloped an indicator Ci as a measure of the impact of site i and investigated\nits correlation with the traffic of the site Ai both on the three networks and\nacross the language communities within the networks.\n  Conclusions: We found that the impact of websites increased slower than their\ntraffic. Specifically, there existed a scaling relationship between Ci and Ai\nwith an exponent gamma smaller than 1. We suggested that this scaling\nrelationship characterized the decentralized structure of the clickstream\ncirculation: the World Wide Web is a system that favors small sites in\nreassigning the collective attention of users.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2011 14:49:36 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 12:56:10 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Wu", "Lingfei", ""], ["Zhang", "Jiang", ""]]}]