[{"id": "1008.0441", "submitter": "Yibei Ling", "authors": "Yibei Ling and Jie Mi", "title": "An Optimal Trade-off between Content Freshness and Refresh Cost", "comments": null, "journal-ref": "J. Appl. Probab. 41 (2004) 721-734", "doi": "10.1239/jap/1091543421", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching is an effective mechanism for reducing bandwidth usage and\nalleviating server load. However, the use of caching entails a compromise\nbetween content freshness and refresh cost. An excessive refresh allows a high\ndegree of content freshness at a greater cost of system resource. Conversely, a\ndeficient refresh inhibits content freshness but saves the cost of resource\nusages. To address the freshness-cost problem, we formulate the refresh\nscheduling problem with a generic cost model and use this cost model to\ndetermine an optimal refresh frequency that gives the best tradeoff between\nrefresh cost and content freshness. We prove the existence and uniqueness of an\noptimal refresh frequency under the assumptions that the arrival of content\nupdate is Poisson and the age-related cost monotonically increases with\ndecreasing freshness. In addition, we provide an analytic comparison of system\nperformance under fixed refresh scheduling and random refresh scheduling,\nshowing that with the same average refresh frequency two refresh schedulings\nare mathematically equivalent in terms of the long-run average cost.\n", "versions": [{"version": "v1", "created": "Tue, 3 Aug 2010 02:40:10 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ling", "Yibei", ""], ["Mi", "Jie", ""]]}, {"id": "1008.0716", "submitter": "Peter Prettenhofer", "authors": "Peter Prettenhofer and Benno Stein", "title": "Cross-Lingual Adaptation using Structural Correspondence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual adaptation, a special case of domain adaptation, refers to the\ntransfer of classification knowledge between two languages. In this article we\ndescribe an extension of Structural Correspondence Learning (SCL), a recently\nproposed algorithm for domain adaptation, for cross-lingual adaptation. The\nproposed method uses unlabeled documents from both languages, along with a word\ntranslation oracle, to induce cross-lingual feature correspondences. From these\ncorrespondences a cross-lingual representation is created that enables the\ntransfer of classification knowledge from the source to the target language.\nThe main advantages of this approach over other approaches are its resource\nefficiency and task specificity.\n  We conduct experiments in the area of cross-language topic and sentiment\nclassification involving English as source language and German, French, and\nJapanese as target languages. The results show a significant improvement of the\nproposed method over a machine translation baseline, reducing the relative\nerror due to cross-lingual adaptation by an average of 30% (topic\nclassification) and 59% (sentiment classification). We further report on\nempirical analyses that reveal insights into the use of unlabeled data, the\nsensitivity with respect to important hyperparameters, and the nature of the\ninduced cross-lingual correspondences.\n", "versions": [{"version": "v1", "created": "Wed, 4 Aug 2010 08:42:07 GMT"}, {"version": "v2", "created": "Wed, 25 Aug 2010 15:52:09 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Prettenhofer", "Peter", ""], ["Stein", "Benno", ""]]}, {"id": "1008.0826", "submitter": "Michael J. Kurtz", "authors": "Michael J. Kurtz", "title": "The Emerging Scholarly Brain", "comments": "to appear in Future Professional Communication in Astronomy-II\n  (FPCA-II) editors A. Heck and A. Accomazzi", "journal-ref": null, "doi": "10.1007/978-1-4419-8369-5_3", "report-no": null, "categories": "physics.soc-ph astro-ph.IM cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now a commonplace observation that human society is becoming a coherent\nsuper-organism, and that the information infrastructure forms its emerging\nbrain. Perhaps, as the underlying technologies are likely to become billions of\ntimes more powerful than those we have today, we could say that we are now\nbuilding the lizard brain for the future organism.\n", "versions": [{"version": "v1", "created": "Wed, 4 Aug 2010 17:11:18 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Kurtz", "Michael J.", ""]]}, {"id": "1008.1191", "submitter": "Dennis Luxen", "authors": "Daniel Karch, Dennis Luxen, Peter Sanders", "title": "Improved Fast Similarity Search in Dictionaries", "comments": "Full version of a short paper accepted for Spire 2010, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer an algorithm to solve the approximate dictionary matching\nproblem. Given a list of words $\\mathcal{W}$, maximum distance $d$ fixed at\npreprocessing time and a query word $q$, we would like to retrieve all words\nfrom $\\mathcal{W}$ that can be transformed into $q$ with $d$ or less edit\noperations. We present data structures that support fault tolerant queries by\ngenerating an index. On top of that, we present a generalization of the method\nthat eases memory consumption and preprocessing time significantly. At the same\ntime, running times of queries are virtually unaffected. We are able to match\nin lists of hundreds of thousands of words and beyond within microseconds for\nreasonable distances.\n", "versions": [{"version": "v1", "created": "Fri, 6 Aug 2010 13:30:31 GMT"}, {"version": "v2", "created": "Wed, 18 Aug 2010 12:38:24 GMT"}], "update_date": "2010-08-19", "authors_parsed": [["Karch", "Daniel", ""], ["Luxen", "Dennis", ""], ["Sanders", "Peter", ""]]}, {"id": "1008.1335", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed and Detlef Gerhard", "title": "Designing a Dynamic Components and Agent based Approach for Semantic\n  Information Retrieval", "comments": "In the proceedings of 6th CIIT Workshop on Research in Computing,\n  (CWRC 07) , P8, Pakistan 27 October, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper based on agent and semantic web technologies we propose an\napproach .i.e., Semantic Oriented Agent Based Search (SOAS), to cope with\ncurrently existing challenges of Meta data extraction, modeling and information\nretrieval over the web. SOAS is designed by keeping four major requirements\n.i.e., Automatic user request handling, Dynamic unstructured full text reading,\nAnalysing and modeling, Semantic query generation and optimized result\nclassifier. The architecture of SOAS is consisting of an agent called Personal\nAgent (PA) and five dynamic components .i.e., Request Processing Unit (RPU),\nAgent Locator (AL), Agent Communicator (AC), List Builder (LB) and Result\nGenerator (RG). Furthermore, in this paper we briefly discuss Semantic Web and\nsome already existing in time proposed and implemented semantic based\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 7 Aug 2010 12:41:26 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Ahmed", "Zeeshan", ""], ["Gerhard", "Detlef", ""]]}, {"id": "1008.1394", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed, Saman Majeed, Thomas Dandekar", "title": "Towards Design and Implementation of a Language Technology based\n  Information Processor for PDM Systems", "comments": null, "journal-ref": "IST Transactions of Information Technology- Theory and\n  Applications, Vol. 1, No. 1 (2),ISSN 1913-8822, pp. 1-7, 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product Data Management (PDM) aims to provide 'Systems' contributing in\nindustries by electronically maintaining organizational data, improving data\nrepository system, facilitating with easy access to CAD and providing\nadditional information engineering and management modules to access, store,\nintegrate, secure, recover and manage information. Targeting one of the\nunresolved issues i.e., provision of natural language based processor for the\nimplementation of an intelligent record search mechanism, an approach is\nproposed and discussed in detail in this manuscript. Designing an intelligent\napplication capable of reading and analyzing user's structured and unstructured\nnatural language based text requests and then extracting desired concrete and\noptimized results from knowledge base is still a challenging task for the\ndesigners because it is still very difficult to completely extract Meta data\nout of raw data. Residing within the limited scope of current research and\ndevelopment; we present an approach capable of reading user's natural language\nbased input text, understanding the semantic and extracting results from\nrepositories. To evaluate the effectiveness of implemented prototyped version\nof proposed approach, it is compared with some existing PDM Systems, in the end\nthe discussion is concluded with an abstract presentation of resultant\ncomparison amongst implemented prototype and some existing PDM Systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Aug 2010 09:38:38 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Ahmed", "Zeeshan", ""], ["Majeed", "Saman", ""], ["Dandekar", "Thomas", ""]]}, {"id": "1008.3282", "submitter": "Md. Saiful Islam", "authors": "Md. Saiful Islam, Shah Mostafa Khaled, Khalid Farhan, Md. Abdur Rahman\n  and Joy Rahman", "title": "Modeling Spammer Behavior: Na\\\"ive Bayes vs. Artificial Neural Networks", "comments": "4 pages, 1 figure, 3 tables", "journal-ref": "Proc. of IEEE ICIMT, Jeju Island, South Korea, December 16-18,\n  2009, pp. 52-55", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing the problem of spam emails in the Internet, this paper presents a\ncomparative study on Na\\\"ive Bayes and Artificial Neural Networks (ANN) based\nmodeling of spammer behavior. Keyword-based spam email filtering techniques\nfall short to model spammer behavior as the spammer constantly changes tactics\nto circumvent these filters. The evasive tactics that the spammer uses are\nthemselves patterns that can be modeled to combat spam. It has been observed\nthat both Na\\\"ive Bayes and ANN are best suitable for modeling spammer common\npatterns. Experimental results demonstrate that both of them achieve a\npromising detection rate of around 92%, which is considerably an improvement of\nperformance compared to the keyword-based contemporary filtering approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Aug 2010 12:03:39 GMT"}], "update_date": "2010-08-20", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Khaled", "Shah Mostafa", ""], ["Farhan", "Khalid", ""], ["Rahman", "Md. Abdur", ""], ["Rahman", "Joy", ""]]}, {"id": "1008.3795", "submitter": "Tom Kelsey", "authors": "T W Kelsey and W H B Wallace", "title": "Machine Science in Biomedicine: Practicalities, Pitfalls and Potential", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CE physics.data-an physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Science, or Data-driven Research, is a new and interesting scientific\nmethodology that uses advanced computational techniques to identify, retrieve,\nclassify and analyse data in order to generate hypotheses and develop models.\nIn this paper we describe three recent biomedical Machine Science studies, and\nuse these to assess the current state of the art with specific emphasis on data\nmining, data assessment, costs, limitations, skills and tool support.\n", "versions": [{"version": "v1", "created": "Mon, 23 Aug 2010 11:30:15 GMT"}], "update_date": "2010-08-24", "authors_parsed": [["Kelsey", "T W", ""], ["Wallace", "W H B", ""]]}, {"id": "1008.4249", "submitter": "Md. Saiful Islam", "authors": "Md. Saiful Islam, Abdullah Al Mahmud, Md. Rafiqul Islam", "title": "Machine Learning Approaches for Modeling Spammer Behavior", "comments": "12 pages, 3 figures, 5 tables, Submitted to AIRS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spam is commonly known as unsolicited or unwanted email messages in the\nInternet causing potential threat to Internet Security. Users spend a valuable\namount of time deleting spam emails. More importantly, ever increasing spam\nemails occupy server storage space and consume network bandwidth. Keyword-based\nspam email filtering strategies will eventually be less successful to model\nspammer behavior as the spammer constantly changes their tricks to circumvent\nthese filters. The evasive tactics that the spammer uses are patterns and these\npatterns can be modeled to combat spam. This paper investigates the\npossibilities of modeling spammer behavioral patterns by well-known\nclassification algorithms such as Na\\\"ive Bayesian classifier (Na\\\"ive Bayes),\nDecision Tree Induction (DTI) and Support Vector Machines (SVMs). Preliminary\nexperimental results demonstrate a promising detection rate of around 92%,\nwhich is considerably an enhancement of performance compared to similar spammer\nbehavior modeling research.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 10:36:40 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Mahmud", "Abdullah Al", ""], ["Islam", "Md. Rafiqul", ""]]}, {"id": "1008.4310", "submitter": "Nada Matta", "authors": "Nada Matta (UTT), Karima Sidoumou (UTT), Goritsa Ninova (UTT, LIPN),\n  Hassan Atifi (UTT)", "title": "Mod\\'elisation d'une analyse pragma-linguistique d'un forum de\n  discussion", "comments": null, "journal-ref": "Intelligence collective et organisation des connaissances (ISKO),\n  Lyon : France (2009)", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper, a modelling of an expertise in pragmatics. We\nfollow knowledge engineering techniques and observe the expert when he analyses\na social discussion forum. Then a number of models are defined. These models\nemphasises the process followed by the expert and a number of criteria used in\nhis analysis. Results can be used as guides that help to understand and\nannotate discussion forum. We aim at modelling other pragmatics analysis in\norder to complete the base of guides; criteria, process, etc. of discussion\nanalysis\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 16:23:03 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Matta", "Nada", "", "UTT"], ["Sidoumou", "Karima", "", "UTT"], ["Ninova", "Goritsa", "", "UTT, LIPN"], ["Atifi", "Hassan", "", "UTT"]]}, {"id": "1008.4658", "submitter": "Konstantin Biatov M.", "authors": "Konstantin Biatov", "title": "A high speed unsupervised speaker retrieval using vector quantization\n  and second-order statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an effective unsupervised method for query-by-example\nspeaker retrieval. We suppose that only one speaker is in each audio file or in\naudio segment. The audio data are modeled using a common universal codebook.\nThe codebook is based on bag-of-frames (BOF). The features corresponding to the\naudio frames are extracted from all audio files. These features are grouped\ninto clusters using the K-means algorithm. The individual audio files are\nmodeled by the normalized distribution of the numbers of cluster bins\ncorresponding to this file. In the first level the k-nearest to the query files\nare retrieved using vector space representation. In the second level the\nsecond-order statistical measure is applied to obtained k-nearest files to find\nthe final result of the retrieval. The described method is evaluated on the\nsubset of Ester corpus of French broadcast news.\n", "versions": [{"version": "v1", "created": "Fri, 27 Aug 2010 08:29:45 GMT"}, {"version": "v2", "created": "Thu, 9 Sep 2010 21:32:48 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Biatov", "Konstantin", ""]]}, {"id": "1008.4669", "submitter": "Md. Saiful Islam", "authors": "Md. Saiful Islam and Md. Iftekharul Amin", "title": "An Architecture of Active Learning SVMs with Relevance Feedback for\n  Classifying E-mail", "comments": "7 pages, 2 figures", "journal-ref": "Journal of Computer Science (IBAIS University), Vol. 1, No. 1, pp.\n  15-18, 2007", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have proposed an architecture of active learning SVMs with\nrelevance feedback (RF)for classifying e-mail. This architecture combines both\nactive learning strategies where instead of using a randomly selected training\nset, the learner has access to a pool of unlabeled instances and can request\nthe labels of some number of them and relevance feedback where if any mail\nmisclassified then the next set of support vectors will be different from the\npresent set otherwise the next set will not change. Our proposed architecture\nwill ensure that a legitimate e-mail will not be dropped in the event of\noverflowing mailbox. The proposed architecture also exhibits dynamic updating\ncharacteristics making life as difficult for the spammer as possible.\n", "versions": [{"version": "v1", "created": "Fri, 27 Aug 2010 09:06:29 GMT"}], "update_date": "2010-08-30", "authors_parsed": [["Islam", "Md. Saiful", ""], ["Amin", "Md. Iftekharul", ""]]}, {"id": "1008.4815", "submitter": "Alberto Costa", "authors": "Alberto Costa, Fabio Roda", "title": "Recommender Systems by means of Information Retrieval", "comments": null, "journal-ref": null, "doi": "10.1145/1988688.1988755", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we present a method for reformulating the Recommender Systems\nproblem in an Information Retrieval one. In our tests we have a dataset of\nusers who give ratings for some movies; we hide some values from the dataset,\nand we try to predict them again using its remaining portion (the so-called\n\"leave-n-out approach\"). In order to use an Information Retrieval algorithm, we\nreformulate this Recommender Systems problem in this way: a user corresponds to\na document, a movie corresponds to a term, the active user (whose rating we\nwant to predict) plays the role of the query, and the ratings are used as\nweigths, in place of the weighting schema of the original IR algorithm. The\noutput is the ranking list of the documents (\"users\") relevant for the query\n(\"active user\"). We use the ratings of these users, weighted according to the\nrank, to predict the rating of the active user. We carry out the comparison by\nmeans of a typical metric, namely the accuracy of the predictions returned by\nthe algorithm, and we compare this to the real ratings from users. In our first\ntests, we use two different Information Retrieval algorithms: LSPR, a recently\nproposed model based on Discrete Fourier Transform, and a simple vector space\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 27 Aug 2010 22:24:25 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Costa", "Alberto", ""], ["Roda", "Fabio", ""]]}, {"id": "1008.5057", "submitter": "Antti Ukkonen", "authors": "Antti Ukkonen", "title": "Approximate Top-k Retrieval from Hidden Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": "TKK-ICS-R36", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the evaluation of approximate top-k queries from relations with\na-priori unknown values. Such relations can arise for example in the context of\nexpensive predicates, or cloud-based data sources. The task is to find an\napproximate top-k set that is close to the exact one while keeping the total\nprocessing cost low. The cost of a query is the sum of the costs of the entries\nthat are read from the hidden relation. A novel aspect of this work is that we\nconsider prior information about the values in the hidden matrix. We propose an\nalgorithm that uses regression models at query time to assess whether a row of\nthe matrix can enter the top-k set given that only a subset of its values are\nknown. The regression models are trained with existing data that follows the\nsame distribution as the relation subjected to the query. To evaluate the\nalgorithm and to compare it with a method proposed previously in literature, we\nconduct experiments using data from a context sensitive Wikipedia search\nengine. The results indicate that the proposed method outperforms the baseline\nalgorithms in terms of the cost while maintaining a high accuracy of the\nreturned results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 11:30:32 GMT"}], "update_date": "2010-08-31", "authors_parsed": [["Ukkonen", "Antti", ""]]}, {"id": "1008.5287", "submitter": "Srivatsan Laxman", "authors": "Dipak Chaudhari, Om P. Damani, and Srivatsan Laxman", "title": "Lexical Co-occurrence, Statistical Significance, and Word Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical co-occurrence is an important cue for detecting word associations. We\npresent a theoretical framework for discovering statistically significant\nlexical co-occurrences from a given corpus. In contrast with the prevalent\npractice of giving weightage to unigram frequencies, we focus only on the\ndocuments containing both the terms (of a candidate bigram). We detect biases\nin span distributions of associated words, while being agnostic to variations\nin global unigram frequencies. Our framework has the fidelity to distinguish\ndifferent classes of lexical co-occurrences, based on strengths of the document\nand corpuslevel cues of co-occurrence in the data. We perform extensive\nexperiments on benchmark data sets to study the performance of various\nco-occurrence measures that are currently known in literature. We find that a\nrelatively obscure measure called Ochiai, and a newly introduced measure CSA\ncapture the notion of lexical co-occurrence best, followed next by LLR, Dice,\nand TTest, while another popular measure, PMI, suprisingly, performs poorly in\nthe context of lexical co-occurrence.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 11:37:32 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Chaudhari", "Dipak", ""], ["Damani", "Om P.", ""], ["Laxman", "Srivatsan", ""]]}]