[{"id": "1909.00057", "submitter": "Shaunak Mishra", "authors": "Shaunak Mishra, Jelena Gligorijevic, Narayan Bhamidipati", "title": "Learning from Multi-User Activity Trails for B2B Ad Targeting", "comments": "6 pages, accepted for AdKDD 2019 workshop held in conjunction with\n  KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online purchase decisions in organizations can go through a complex journey\nwith multiple agents involved in the decision making process. Depending on the\nproduct being purchased, and the organizational structure, the process may\ninvolve employees who first conduct market research, and then influence\ndecision makers who place the online purchase order. In such cases, the online\nactivity trail of a single individual in the organization may only provide\npartial information for predicting purchases (conversions). To refine\nconversion prediction for business-to-business (B2B) products using online\nactivity trails, we introduce the notion of relevant users in an organization\nwith respect to a given B2B advertiser, and leverage the collective activity\ntrails of such relevant users to predict conversions. In particular, our notion\nof relevant users is tied to a seed list of relevant activities for a B2B\nadvertiser, and we propose a method using distributed activity representations\nto build such a seed list. Experiments using data from Yahoo Gemini demonstrate\nthat the proposed methods can improve conversion prediction AUC by 8.8%, and\nprovide an interpretable advertiser specific list of activities useful for B2B\nad targeting.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:52:33 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Mishra", "Shaunak", ""], ["Gligorijevic", "Jelena", ""], ["Bhamidipati", "Narayan", ""]]}, {"id": "1909.00088", "submitter": "Steven Y. Feng", "authors": "Steven Y. Feng, Aaron W. Li and Jesse Hoey", "title": "Keep Calm and Switch On! Preserving Sentiment and Fluency in Semantic\n  Text Exchange", "comments": "EMNLP-IJCNLP 2019; Code available at\n  https://github.com/styfeng/SMERTI", "journal-ref": null, "doi": "10.18653/v1/D19-1272", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for measurably adjusting the\nsemantics of text while preserving its sentiment and fluency, a task we call\nsemantic text exchange. This is useful for text data augmentation and the\nsemantic correction of text generated by chatbots and virtual assistants. We\nintroduce a pipeline called SMERTI that combines entity replacement, similarity\nmasking, and text infilling. We measure our pipeline's success by its Semantic\nText Exchange Score (STES): the ability to preserve the original text's\nsentiment and fluency while adjusting semantic content. We propose to use\nmasking (replacement) rate threshold as an adjustable parameter to control the\namount of semantic change in the text. Our experiments demonstrate that SMERTI\ncan outperform baseline models on Yelp reviews, Amazon reviews, and news\nheadlines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 23:10:28 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 07:59:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Feng", "Steven Y.", ""], ["Li", "Aaron W.", ""], ["Hoey", "Jesse", ""]]}, {"id": "1909.00164", "submitter": "Ying Luo", "authors": "Ying Luo, Hai Zhao, Junlang Zhan", "title": "Named Entity Recognition Only from Word Embeddings", "comments": "Accepted by EMNLP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models have helped named entity (NE) recognition achieve\namazing performance without handcrafting features. However, existing systems\nrequire large amounts of human annotated training data. Efforts have been made\nto replace human annotations with external knowledge (e.g., NE dictionary,\npart-of-speech tags), while it is another challenge to obtain such effective\nresources. In this work, we propose a fully unsupervised NE recognition model\nwhich only needs to take informative clues from pre-trained word embeddings. We\nfirst apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture\nModel on word embeddings for entity span detection and type prediction, and\nthen further design an instance selector based on reinforcement learning to\ndistinguish positive sentences from noisy sentences and refine these\ncoarse-grained annotations through neural networks. Extensive experiments on\nCoNLL benchmark datasets demonstrate that our proposed light NE recognition\nmodel achieves remarkable performance without using any annotated lexicon or\ncorpus.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 08:22:13 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:22:32 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Luo", "Ying", ""], ["Zhao", "Hai", ""], ["Zhan", "Junlang", ""]]}, {"id": "1909.00170", "submitter": "Zhuosheng Zhang", "authors": "Ying Luo, Hai Zhao, Zhuosheng Zhang, Bingjie Tang", "title": "Open Named Entity Modeling from Embedding Distribution", "comments": "The early version accepted by IEEE Transactions on Knowledge and Data\n  Engineering (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report our discovery on named entity distribution in a\ngeneral word embedding space, which helps an open definition on multilingual\nnamed entity definition rather than previous closed and constraint definition\non named entities through a named entity dictionary, which is usually derived\nfrom human labor and replies on schedule update. Our initial visualization of\nmonolingual word embeddings indicates named entities tend to gather together\ndespite of named entity types and language difference, which enable us to model\nall named entities using a specific geometric structure inside embedding space,\nnamely, the named entity hypersphere. For monolingual cases, the proposed named\nentity model gives an open description of diverse named entity types and\ndifferent languages. For cross-lingual cases, mapping the proposed named entity\nmodel provides a novel way to build a named entity dataset for resource-poor\nlanguages. At last, the proposed named entity model may be shown as a handy\nclue to enhance state-of-the-art named entity recognition systems generally.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 08:56:46 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 15:01:25 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Luo", "Ying", ""], ["Zhao", "Hai", ""], ["Zhang", "Zhuosheng", ""], ["Tang", "Bingjie", ""]]}, {"id": "1909.00183", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Eloise Sorin, Joshua D. Symons, Erik Mayer, Sophia\n  N. Yaliraki, Francesca Toni, Mauricio Barahona", "title": "Extracting information from free text through unsupervised graph-based\n  clustering: an application to patient incident records", "comments": "To appear as a book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large volume of text in electronic healthcare records often remains\nunderused due to a lack of methodologies to extract interpretable content. Here\nwe present an unsupervised framework for the analysis of free text that\ncombines text-embedding with paragraph vectors and graph-theoretical multiscale\ncommunity detection. We analyse text from a corpus of patient incident reports\nfrom the National Health Service in England to find content-based clusters of\nreports in an unsupervised manner and at different levels of resolution. Our\nunsupervised method extracts groups with high intrinsic textual consistency and\ncompares well against categories hand-coded by healthcare personnel. We also\nshow how to use our content-driven clusters to improve the supervised\nprediction of the degree of harm of the incident based on the text of the\nreport. Finally, we discuss future directions to monitor reports over time, and\nto detect emerging trends outside pre-existing categories.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 10:03:11 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Sorin", "Eloise", ""], ["Symons", "Joshua D.", ""], ["Mayer", "Erik", ""], ["Yaliraki", "Sophia N.", ""], ["Toni", "Francesca", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1909.00302", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, Hadar Averbuch-Elor", "title": "READ: Recursive Autoencoders for Document Layout Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout is a fundamental component of any graphic design. Creating large\nvarieties of plausible document layouts can be a tedious task, requiring\nnumerous constraints to be satisfied, including local ones relating different\nsemantic elements and global constraints on the general appearance and spacing.\nIn this paper, we present a novel framework, coined READ, for REcursive\nAutoencoders for Document layout generation, to generate plausible 2D layouts\nof documents in large quantities and varieties. First, we devise an exploratory\nrecursive method to extract a structural decomposition of a single document.\nLeveraging a dataset of documents annotated with labeled bounding boxes, our\nrecursive neural network learns to map the structural representation, given in\nthe form of a simple hierarchy, to a compact code, the space of which is\napproximated by a Gaussian distribution. Novel hierarchies can be sampled from\nthis space, obtaining new document layouts. Moreover, we introduce a\ncombinatorial metric to measure structural similarity among document layouts.\nWe deploy it to show that our method is able to generate highly variable and\nrealistic layouts. We further demonstrate the utility of our generated layouts\nin the context of standard detection tasks on documents, showing that detection\nperformance improves when the training data is augmented with generated\ndocuments whose layouts are produced by READ.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 01:58:31 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:43:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 22:38:52 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 23:26:17 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Ben-Eliezer", "Omri", ""], ["Perel", "Or", ""], ["Averbuch-Elor", "Hadar", ""]]}, {"id": "1909.00344", "submitter": "EunJeong Hwang", "authors": "EunJeong Hwang and Yong-Hyuk Kim", "title": "Interdependency between the Stock Market and Financial News", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock prices are driven by various factors. In particular, many individual\ninvestors who have relatively little financial knowledge rely heavily on the\ninformation from news stories when making investment decisions in the stock\nmarket. However, these stories may not reflect future stock prices because of\nthe subjectivity in the news; stock prices may instead affect the news\ncontents. This study aims to discover whether it is news or stock prices that\nhave a greater impact on the other. To achieve this, we analyze the\nrelationship between news sentiment and stock prices based on time series\nanalysis using five different classification models. Our experimental results\nshow that stock prices have a bigger impact on the news contents than news does\non stock prices.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 07:42:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hwang", "EunJeong", ""], ["Kim", "Yong-Hyuk", ""]]}, {"id": "1909.00385", "submitter": "Fuyu Lv", "authors": "Fuyu Lv, Taiwei Jin, Changlong Yu, Fei Sun, Quan Lin, Keping Yang,\n  Wilfred Ng", "title": "SDM: Sequential Deep Matching Model for Online Large-scale Recommender\n  System", "comments": "9 pages, CIKM 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing users' precise preferences is a fundamental problem in large-scale\nrecommender system. Currently, item-based Collaborative Filtering (CF) methods\nare common matching approaches in industry. However, they are not effective to\nmodel dynamic and evolving preferences of users. In this paper, we propose a\nnew sequential deep matching (SDM) model to capture users' dynamic preferences\nby combining short-term sessions and long-term behaviors. Compared with\nexisting sequence-aware recommendation methods, we tackle the following two\ninherent problems in real-world applications: (1) there could exist multiple\ninterest tendencies in one session. (2) long-term preferences may not be\neffectively fused with current session interests. Long-term behaviors are\nvarious and complex, hence those highly related to the short-term session\nshould be kept for fusion. We propose to encode behavior sequences with two\ncorresponding components: multi-head self-attention module to capture multiple\ntypes of interests and long-short term gated fusion module to incorporate\nlong-term preferences. Successive items are recommended after matching between\nsequential user behavior vector and item embedding vectors. Offline experiments\non real-world datasets show the superior performance of the proposed SDM.\nMoreover, SDM has been successfully deployed on online large-scale recommender\nsystem at Taobao and achieves improvements in terms of a range of commercial\nmetrics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 11:59:13 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 16:04:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lv", "Fuyu", ""], ["Jin", "Taiwei", ""], ["Yu", "Changlong", ""], ["Sun", "Fei", ""], ["Lin", "Quan", ""], ["Yang", "Keping", ""], ["Ng", "Wilfred", ""]]}, {"id": "1909.00430", "submitter": "Matan Ben Noach", "authors": "Matan Ben Noach and Yoav Goldberg", "title": "Transfer Learning Between Related Tasks Using Expected Label Proportions", "comments": "EMNLP 2019", "journal-ref": "2019 Conference on Empirical Methods in Natural Language\n  Processing and 9th International Joint Conference on Natural Language\n  Processing", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning systems thrive on abundance of labeled training data but such\ndata is not always available, calling for alternative methods of supervision.\nOne such method is expectation regularization (XR) (Mann and McCallum, 2007),\nwhere models are trained based on expected label proportions. We propose a\nnovel application of the XR framework for transfer learning between related\ntasks, where knowing the labels of task A provides an estimation of the label\nproportion of task B. We then use a model trained for A to label a large\ncorpus, and use this corpus with an XR loss to train a model for task B. To\nmake the XR framework applicable to large-scale deep-learning setups, we\npropose a stochastic batched approximation procedure. We demonstrate the\napproach on the task of Aspect-based Sentiment classification, where we\neffectively use a sentence-level sentiment predictor to train accurate\naspect-based predictor. The method improves upon fully supervised neural system\ntrained on aspect-level data, and is also cumulative with LM-based pretraining,\nas we demonstrate by improving a BERT-based Aspect-based Sentiment model.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 17:11:35 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Noach", "Matan Ben", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1909.00554", "submitter": "Yoshifumi Seki", "authors": "Yoshifumi Seki, Mitsuo Yoshida", "title": "Analysis of Bias in Gathering Information Between User Attributes in\n  News Application", "comments": "8 pages, 13 figure, IEEE BigData 2018 Workshop : The 3rd\n  International Workshop on Application of Big Data for Computational Social\n  Science (ABCSS2018)", "journal-ref": null, "doi": "10.1109/bigdata.2018.8622482", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of information gathering on the web, confirmation bias is\nknown to exist, exemplified in phenomena such as echo chambers and filter\nbubbles. Our purpose is to reveal how people consume news and discuss these\nphenomena. In web services, we are able to use action logs of a service to\ninvestigate these phenomena. However, many existing studies about these\nphenomena are conducted via questionnaires, and there are few studies using\naction logs. In this paper, we attempt to discover biases of information\ngathering due to differences in user demographic attributes, such as age and\ngender, from the behavior log of the news distribution service. First, we\nsummarized the actions in the service for each user attribute and showed the\ndifference of user behavior depending on the attributes. Next, the degree of\ncorrelation between the attributes was measured using the correlation\ncoefficient, and a strong correlation was found to exist in the browsing\ntendency of the news articles between the attributes. Then, the bias of\nkeywords between attributes was discovered, keywords with bias in behavior\namong the attributes were found using parameters of regression analysis. Since\nthese discovered keywords are almost explainable by big news, our proposed\nmethod is effective in detecting biased keywords.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:44:59 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""]]}, {"id": "1909.00687", "submitter": "Diego Monti", "authors": "Diego Monti, Giuseppe Rizzo and Maurizio Morisio", "title": "All You Need is Ratings: A Clustering Approach to Synthetic Rating\n  Datasets Generation", "comments": "REVEAL 2019 Workshop on Reinforcement and Robust Estimators", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The public availability of collections containing user preferences is of\nvital importance for performing offline evaluations in the field of recommender\nsystems. However, the number of rating datasets is limited because of the costs\nrequired for their creation and the fear of violating the privacy of the users\nby sharing them. For this reason, numerous research attempts investigated the\ncreation of synthetic collections of ratings using generative approaches.\nNevertheless, these datasets are usually not reliable enough for conducting an\nevaluation campaign. In this paper, we propose a method for creating synthetic\ndatasets with a configurable number of users that mimic the characteristics of\nalready existing ones. We empirically validated the proposed approach by\nexploiting the synthetic datasets for evaluating different recommenders and by\ncomparing the results with the ones obtained using real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:20:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Monti", "Diego", ""], ["Rizzo", "Giuseppe", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1909.00741", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Niket Tandon, Hakan Ferhatosmanoglu, Gerhard\n  Weikum", "title": "VISIR: Visual and Semantic Image Label Refinement", "comments": "Published in WSDM 2018", "journal-ref": "ACM ISBN 978-1-4503-5581-0/18/02 2018", "doi": "10.1145/3159652.3159693", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media explosion has populated the Internet with a wealth of\nimages. There are two existing paradigms for image retrieval: 1) content-based\nimage retrieval (CBIR), which has traditionally used visual features for\nsimilarity search (e.g., SIFT features), and 2) tag-based image retrieval\n(TBIR), which has relied on user tagging (e.g., Flickr tags). CBIR now gains\nsemantic expressiveness by advances in deep-learning-based detection of visual\nlabels. TBIR benefits from query-and-click logs to automatically infer more\ninformative labels. However, learning-based tagging still yields noisy labels\nand is restricted to concrete objects, missing out on generalizations and\nabstractions. Click-based tagging is limited to terms that appear in the\ntextual context of an image or in queries that lead to a click. This paper\naddresses the above limitations by semantically refining and expanding the\nlabels suggested by learning-based object detection. We consider the semantic\ncoherence between the labels for different objects, leverage lexical and\ncommonsense knowledge, and cast the label assignment into a constrained\noptimization problem solved by an integer linear program. Experiments show that\nour method, called VISIR, improves the quality of the state-of-the-art visual\nlabeling tools like LSDA and YOLO.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:41:44 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Tandon", "Niket", ""], ["Ferhatosmanoglu", "Hakan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1909.00749", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Niket Tandon, Gerhard Weikum", "title": "Know2Look: Commonsense Knowledge for Visual Search", "comments": "Published in AKBC 2016", "journal-ref": "5th Workshop on Automated Knowledge Base Construction (AKBC) 2016", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise in popularity of social media, images accompanied by contextual\ntext form a huge section of the web. However, search and retrieval of documents\nare still largely dependent on solely textual cues. Although visual cues have\nstarted to gain focus, the imperfection in object/scene detection do not lead\nto significantly improved results. We hypothesize that the use of background\ncommonsense knowledge on query terms can significantly aid in retrieval of\ndocuments with associated images. To this end we deploy three different\nmodalities - text, visual cues, and commonsense knowledge pertaining to the\nquery - as a recipe for efficient search and retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 14:55:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Tandon", "Niket", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1909.00764", "submitter": "Michihiro Yasunaga", "authors": "Kokil Jaidka, Michihiro Yasunaga, Muthu Kumar Chandrasekaran, Dragomir\n  Radev, and Min-Yen Kan", "title": "The CL-SciSumm Shared Task 2018: Results and Key Insights", "comments": "BIRNDL @ SIGIR 2018. arXiv admin note: substantial text overlap with\n  arXiv:1907.09854", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This overview describes the official results of the CL-SciSumm Shared Task\n2018 -- the first medium-scale shared task on scientific document summarization\nin the computational linguistics (CL) domain. This year, the dataset comprised\n60 annotated sets of citing and reference papers from the open access research\npapers in the CL domain. The Shared Task was organized as a part of the 41st\nAnnual Conference of the Special Interest Group in Information Retrieval\n(SIGIR), held in Ann Arbor, USA in July 2018. We compare the participating\nsystems in terms of two evaluation metrics. The annotated dataset and\nevaluation scripts can be accessed and used by the community from:\n\\url{https://github.com/WING-NUS/scisumm-corpus}.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 15:23:55 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Jaidka", "Kokil", ""], ["Yasunaga", "Michihiro", ""], ["Chandrasekaran", "Muthu Kumar", ""], ["Radev", "Dragomir", ""], ["Kan", "Min-Yen", ""]]}, {"id": "1909.00900", "submitter": "Chengzhi Mao", "authors": "Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, Baishakhi Ray", "title": "Metric Learning for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are well-known to be fragile to adversarial attacks. We conduct\nan empirical analysis of deep representations under the state-of-the-art attack\nmethod called PGD, and find that the attack causes the internal representation\nto shift closer to the \"false\" class. Motivated by this observation, we propose\nto regularize the representation space under attack with metric learning to\nproduce more robust classifiers. By carefully sampling examples for metric\nlearning, our learned representation not only increases robustness, but also\ndetects previously unseen adversarial samples. Quantitative experiments show\nimprovement of robustness accuracy by up to 4% and detection efficiency by up\nto 6% according to Area Under Curve score over prior work. The code of our work\nis available at\nhttps://github.com/columbia/Metric_Learning_Adversarial_Robustness.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 00:39:40 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 00:43:15 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mao", "Chengzhi", ""], ["Zhong", "Ziyuan", ""], ["Yang", "Junfeng", ""], ["Vondrick", "Carl", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1909.01005", "submitter": "Takeshi Yoneda", "authors": "Takeshi Yoneda, Shunsuke Kozawa, Keisuke Osone, Yukinori Koide, Yosuke\n  Abe, Yoshifumi Seki", "title": "Algorithms and System Architecture for Immediate Personalized News\n  Recommendations", "comments": "WI '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization plays an important role in many services, just as news does.\nMany studies have examined news personalization algorithms, but few have\nconsidered practical environments. This paper provides algorithms and system\narchitecture for generating immediate personalized news in a practical\nenvironment. Immediacy means changes in news trends and user interests are\nreflected in recommended news lists quickly. Since news trends and user\ninterests rapidly change, immediacy is critical in news personalization\napplications. We develop algorithms and system architecture to realize\nimmediacy. Our algorithms are based on collaborative filtering of user clusters\nand evaluate news articles using click-through rate and decay scores based on\nthe time elapsed since the user's last access. Existing studies have not fully\ndiscussed system architecture, so a major contribution of this paper is that we\ndemonstrate a system architecture and realize our algorithms and a\nconfiguration example implemented on top of Amazon Web Services. We evaluate\nthe proposed method both offline and online. The offline experiments are\nconducted through a real-world dataset from a commercial news delivery service,\nand online experiments are conducted via A/B testing on production\nenvironments. We confirm the effectiveness of our proposed method and also that\nour system architecture can operate in large-scale production environments.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:44:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yoneda", "Takeshi", ""], ["Kozawa", "Shunsuke", ""], ["Osone", "Keisuke", ""], ["Koide", "Yukinori", ""], ["Abe", "Yosuke", ""], ["Seki", "Yoshifumi", ""]]}, {"id": "1909.01079", "submitter": "Lin Li", "authors": "Peipei Wang, Lin Li, Yi Yu, Guandong Xu", "title": "Social Influence-based Attentive Mavens Mining and Aggregative\n  Representation Learning for Group Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent group activities of human beings have become an indispensable part\nin their daily life. Group recommendation can recommend satisfactory activities\nto group members in the recommender systems, and the key issue is how to\naggregate preferences in different group members. Most existing group\nrecommendation employed the predefined static aggregation strategies to\naggregate the preferences of different group members, but these static\nstrategies cannot simulate the dynamic group decision-making. Meanwhile, most\nof these methods depend on intuitions or assumptions to analyze the influence\nof group members and lack of convincing theoretical support. We argue that the\ninfluence of group members plays a particularly important role in group\ndecision-making and it can better assist group profile modeling and perform\nmore accurate group recommendation. To tackle the issue of preference\naggregation for group recommendation, we propose a novel attentive aggregation\nrepresentation learning method based on sociological theory for group\nrecommendation, namely SIAGR (short for \"Social Influence-based Attentive Group\nRecommendation\"), which takes attention mechanisms and the popular method\n(BERT) as the aggregation representation for group profile modeling.\nSpecifically, we analyze the influence of group members based on social\nidentity theory and two-step flow theory and exploit an attentive mavens mining\nmethod. In addition, we develop a BERT-based representation method to learn the\ninteraction of group members. Lastly, we complete the group recommendation\nunder the neural collaborative filtering framework and verify the effectiveness\nof the proposed method by experimenting.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:29:48 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Peipei", ""], ["Li", "Lin", ""], ["Yu", "Yi", ""], ["Xu", "Guandong", ""]]}, {"id": "1909.01093", "submitter": "Zhiqiang Ma", "authors": "Azadeh Nematzadeh, Grace Bang, Xiaomo Liu, Zhiqiang Ma", "title": "Empirical Study on Detecting Controversy in Social Media", "comments": "The work is accepted by the 2nd KDD Workshop on Anomaly Detection in\n  Finance, 2019. The authors contributed equally to this work, listed in the\n  alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies and financial investors are paying increasing attention to social\nconsciousness in developing their corporate strategies and making investment\ndecisions to support a sustainable economy for the future. Public discussion on\nincidents and events -- controversies -- of companies can provide valuable\ninsights on how well the company operates with regards to social consciousness\nand indicate the company's overall operational capability. However, there are\nchallenges in evaluating the degree of a company's social consciousness and\nenvironmental sustainability due to the lack of systematic data. We introduce a\nsystem that utilizes Twitter data to detect and monitor controversial events\nand show their impact on market volatility. In our study, controversial events\nare identified from clustered tweets that share the same 5W terms and sentiment\npolarities of these clusters. Credible news links inside the event tweets are\nused to validate the truth of the event. A case study on the Starbucks\nPhiladelphia arrests shows that this method can provide the desired\nfunctionality.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 18:36:55 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Nematzadeh", "Azadeh", ""], ["Bang", "Grace", ""], ["Liu", "Xiaomo", ""], ["Ma", "Zhiqiang", ""]]}, {"id": "1909.01165", "submitter": "Yuanyuan Qi", "authors": "Yuanyuan Qi, Jiayue Zhang, Weiran Xu and Jun Guo", "title": "Finding Salient Context based on Semantic Matching for Relevance Ranking", "comments": "2019 IEEE International Conference on Visual Communications and Image\n  Processing (VCIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a salient-context based semantic matching method to\nimprove relevance ranking in information retrieval. We first propose a new\nnotion of salient context and then define how to measure it. Then we show how\nthe most salient context can be located with a sliding window technique.\nFinally, we use the semantic similarity between a query term and the most\nsalient context terms in a corpus of documents to rank those documents.\nExperiments on various collections from TREC show the effectiveness of our\nmodel compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:29:48 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Qi", "Yuanyuan", ""], ["Zhang", "Jiayue", ""], ["Xu", "Weiran", ""], ["Guo", "Jun", ""]]}, {"id": "1909.01436", "submitter": "Lucas Theis", "authors": "Iryna Korshunova, Hanchen Xiong, Mateusz Fedoryszak, Lucas Theis", "title": "Discriminative Topic Modeling with Logistic LDA", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 32, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite many years of research into latent Dirichlet allocation (LDA),\napplying LDA to collections of non-categorical items is still challenging. Yet\nmany problems with much richer data share a similar structure and could benefit\nfrom the vast literature on LDA. We propose logistic LDA, a novel\ndiscriminative variant of latent Dirichlet allocation which is easy to apply to\narbitrary inputs. In particular, our model can easily be applied to groups of\nimages, arbitrary text embeddings, and integrates well with deep neural\nnetworks. Although it is a discriminative model, we show that logistic LDA can\nlearn from unlabeled data in an unsupervised manner by exploiting the group\nstructure present in the data. In contrast to other recent topic models\ndesigned to handle arbitrary inputs, our model does not sacrifice the\ninterpretability and principled motivation of LDA.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 20:25:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 11:12:31 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Korshunova", "Iryna", ""], ["Xiong", "Hanchen", ""], ["Fedoryszak", "Mateusz", ""], ["Theis", "Lucas", ""]]}, {"id": "1909.01495", "submitter": "Bibek Paudel", "authors": "Bibek Paudel, Abraham Bernstein", "title": "Cross-Cutting Political Awareness through Diverse News Recommendations", "comments": "European Symposium Series on Societal Challenges in Computational\n  Social Science, Zurich, Switzerland, September 2nd-4th, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suggestions generated by most existing recommender systems are known to\nsuffer from a lack of diversity, and other issues like popularity bias. As a\nresult, they have been observed to promote well-known \"blockbuster\" items, and\nto present users with \"more of the same\" choices that entrench their existing\nbeliefs and biases. This limits users' exposure to diverse viewpoints and\npotentially increases political polarization. To promote the diversity of\nviews, we developed a novel computational framework that can identify the\npolitical leanings of users and the news items they share on online social\nnetworks. Based on such information, our system can recommend news items that\npurposefully expose users to different viewpoints and increase the diversity of\ntheir information \"diet.\" Our research on recommendation diversity and\npolitical polarization helps us to develop algorithms that measure each user's\nreaction %to diverse viewpoints and adjust the recommendation accordingly. The\nresult is an approach that exposes users to a variety of political views and\nwill, hopefully, broaden their acceptance (not necessarily the agreement) of\nvarious opinions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 23:09:25 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Paudel", "Bibek", ""], ["Bernstein", "Abraham", ""]]}, {"id": "1909.01601", "submitter": "Noemi Mauro", "authors": "Liliana Ardissono and Noemi Mauro", "title": "A Compositional Model of Multi-faceted Trust for Personalized Item\n  Recommendation", "comments": null, "journal-ref": "Expert Systems with Applications, Volume 140, 2020. ISSN 0957-4174", "doi": "10.1016/j.eswa.2019.112880", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trust-based recommender systems improve rating prediction with respect to\nCollaborative Filtering by leveraging the additional information provided by a\ntrust network among users to deal with the cold start problem. However, they\nare challenged by recent studies according to which people generally perceive\nthe usage of data about social relations as a violation of their own privacy.\nIn order to address this issue, we extend trust-based recommender systems with\nadditional evidence about trust, based on public anonymous information, and we\nmake them configurable with respect to the data that can be used in the given\napplication domain: 1 - We propose the Multi-faceted Trust Model (MTM) to\ndefine trust among users in a compositional way, possibly including or\nexcluding the types of information it contains. MTM flexibly integrates social\nlinks with public anonymous feedback received by user profiles and user\ncontributions in social networks. 2 - We propose LOCABAL+, based on MTM, which\nextends the LOCABAL trust-based recommender system with multi-faceted trust and\ntrust-based social regularization. Experiments carried out on two public\ndatasets of item reviews show that, with a minor loss of user coverage,\nLOCABAL+ outperforms state-of-the art trust-based recommender systems and\nCollaborative Filtering in accuracy, ranking of items and error minimization\nboth when it uses complete information about trust and when it ignores social\nrelations. The combination of MTM with LOCABAL+ thus represents a promising\nalternative to state-of-the-art trust-based recommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 07:56:04 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Ardissono", "Liliana", ""], ["Mauro", "Noemi", ""]]}, {"id": "1909.01610", "submitter": "Jacopo Staiano", "authors": "Thomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano", "title": "Answers Unite! Unsupervised Metrics for Reinforced Summarization Models", "comments": "Accepted at EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstractive summarization approaches based on Reinforcement Learning (RL)\nhave recently been proposed to overcome classical likelihood maximization. RL\nenables to consider complex, possibly non-differentiable, metrics that globally\nassess the quality and relevance of the generated outputs. ROUGE, the most used\nsummarization metric, is known to suffer from bias towards lexical similarity\nas well as from suboptimal accounting for fluency and readability of the\ngenerated abstracts. We thus explore and propose alternative evaluation\nmeasures: the reported human-evaluation analysis shows that the proposed\nmetrics, based on Question Answering, favorably compares to ROUGE -- with the\nadditional property of not requiring reference summaries. Training a RL-based\nmodel on these metrics leads to improvements (both in terms of human or\nautomated metrics) over current approaches that use ROUGE as a reward.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:20:31 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Scialom", "Thomas", ""], ["Lamprier", "Sylvain", ""], ["Piwowarski", "Benjamin", ""], ["Staiano", "Jacopo", ""]]}, {"id": "1909.01716", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R. Fabbri, Irene\n  Li, Dan Friedman, Dragomir R. Radev", "title": "ScisummNet: A Large Annotated Corpus and Content-Impact Models for\n  Scientific Paper Summarization with Citation Networks", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific article summarization is challenging: large, annotated corpora are\nnot available, and the summary should ideally include the article's impacts on\nresearch community. This paper provides novel solutions to these two\nchallenges. We 1) develop and release the first large-scale manually-annotated\ncorpus for scientific papers (on computational linguistics) by enabling faster\nannotation, and 2) propose summarization methods that integrate the authors'\noriginal highlights (abstract) and the article's actual impacts on the\ncommunity (citations), to create comprehensive, hybrid summaries. We conduct\nexperiments to demonstrate the efficacy of our corpus in training data-driven\nmodels for scientific paper summarization and the advantage of our hybrid\nsummaries over abstracts and traditional citation-based summaries. Our large\nannotated corpus and hybrid methods provide a new framework for scientific\npaper summarization research.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:04:48 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 02:51:44 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 01:32:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Kasai", "Jungo", ""], ["Zhang", "Rui", ""], ["Fabbri", "Alexander R.", ""], ["Li", "Irene", ""], ["Friedman", "Dan", ""], ["Radev", "Dragomir R.", ""]]}, {"id": "1909.01727", "submitter": "Yifang  Liu", "authors": "Yifang Liu, Zhentao Xu, Cong Hui, Yi Xuan, Jessie Chen, Yuanming Shan", "title": "Heterogeneous Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system is important to a content sharing/creating social\nnetwork. Collaborative filtering is a widely-adopted technology in conventional\nrecommenders, which is based on similarity between positively engaged content\nitems involving the same users. Conventional collaborative filtering (CCF)\nsuffers from cold start problem and narrow content diversity. We propose a new\nrecommendation approach, heterogeneous collaborative filtering (HCF) to tackle\nthese challenges at the root, while keeping the strength of collaborative\nfiltering. We present two implementation algorithms of HCF for content\nrecommendation and content dissemination. Experiment results demonstrate that\nour approach improve the recommendation quality in a real world social network\nfor content creating and sharing.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 07:56:58 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Liu", "Yifang", ""], ["Xu", "Zhentao", ""], ["Hui", "Cong", ""], ["Xuan", "Yi", ""], ["Chen", "Jessie", ""], ["Shan", "Yuanming", ""]]}, {"id": "1909.01756", "submitter": "Srinivas Kota Reddy", "authors": "Kota Srinivas Reddy and Nikhil Karamchandani", "title": "Rate-Memory Trade-off for Multi-access Coded Caching with Uncoded\n  Placement", "comments": "Accepted in Transactions on Communications. Preliminary works\n  appeared and presented in SPCOM and ISIT", "journal-ref": null, "doi": "10.1109/TCOMM.2020.2980817", "report-no": null, "categories": "cs.IT cs.DC cs.IR math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-access variant of the popular coded caching framework, which\nconsists of a central server with a catalog of $N$ files, $K$ caches with\nlimited memory $M$, and $K$ users such that each user has access to $L$\nconsecutive caches with a cyclic wrap-around and requests one file from the\ncentral server's catalog. The server assists in file delivery by transmitting a\nmessage of size $R$ over a shared error-free link and the goal is to\ncharacterize the optimal rate-memory trade-off. This setup was studied\npreviously by Hachem et al., where an achievable rate and an\ninformation-theoretic lower bound were derived. However, the multiplicative gap\nbetween them was shown to scale linearly with the access degree $L$ and thus\norder-optimality could not be established.\n  A series of recent works have used a natural mapping of the coded caching\nproblem to the well-known index coding problem to derive tighter\ncharacterizations of the optimal rate-memory trade-off under the additional\nassumption that the caches store uncoded content. We follow a similar strategy\nfor the multi-access framework and provide new bounds for the optimal\nrate-memory trade-off $R^*(M)$ over all uncoded placement policies. In\nparticular, we derive a new achievable rate for any $L \\ge 1$ and a new lower\nbound, which works for any uncoded placement policy and $L \\ge K/2$. We then\nestablish that the (multiplicative) gap between the new achievable rate and the\nlower bound is at most $2$ independent of all parameters, thus establishing an\norder-optimal characterization of $R^*(M)$ for any $L\\ge K/2$. This is a\nsignificant improvement over the previously known gap result, albeit under the\nrestriction of uncoded placement policies. Finally, we also characterize\n$R^*(M)$ exactly for a few special cases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:56:40 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 18:55:17 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Reddy", "Kota Srinivas", ""], ["Karamchandani", "Nikhil", ""]]}, {"id": "1909.01772", "submitter": "Tommaso Teofili", "authors": "Tommaso Teofili, Niyati Chhaya", "title": "Affect Enriched Word Embeddings for News Information Retrieval", "comments": null, "journal-ref": "NewsIR@SIGIR 2019: 63-68", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of words have shown to be useful to improve the\neffectiveness of IR systems in many sub-tasks like query expansion, retrieval\nand ranking. Algorithms like word2vec, GloVe and others are also key factors in\nmany improvements in different NLP tasks. One common issue with such embedding\nmodels is that words like happy and sad appear in similar contexts and hence\nare wrongly clustered close in the embedding space. In this paper we leverage\nAff2Vec, a set of word embeddings models which include affect information, in\norder to better capture the affect aspect in news text to achieve better\nresults in information retrieval tasks, also such embeddings are less hit by\nthe synonym/antonym issue. We evaluate their effectiveness on two IR related\ntasks (query expansion and ranking) over the New York Times dataset (TREC-core\n'17) comparing them against other word embeddings based models and classic\nranking models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 13:12:30 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Teofili", "Tommaso", ""], ["Chhaya", "Niyati", ""]]}, {"id": "1909.01793", "submitter": "Eric Du", "authors": "Eric Du, Xiaoyong Li", "title": "Employ Multimodal Machine Learning for Content quality analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of identifying high-quality content becomes increasingly important,\nand it can improve overall reading time and CTR(click-through rate estimates).\nGeneralizes quality analysis only focused on single Modal,such as image or\ntext,but in today's mainstream media sites a lot of information is presented in\ngraphic form.In this paper we propose a MultiModal quality recognition approach\nfor the quality score. First we use two feature extractors,one for image and\nanother for the text. After that we use an Siamese Network with the rank loss\nas the optimization objective.Compare with other approach,our approach get a\nmore accuracy result.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 11:29:37 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Du", "Eric", ""], ["Li", "Xiaoyong", ""]]}, {"id": "1909.01811", "submitter": "Ruomu Zou", "authors": "Ruomu Zou", "title": "A Deep, Forgetful Novelty-Seeking Movie Recommender Model", "comments": "19 pages, 14 figures, submitted as a contest entry to the S.-T. Yau\n  High School Science Award (Computer Award)", "journal-ref": null, "doi": "10.13140/RG.2.2.35255.27043", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As more and more people shift their movie watching online, competition\nbetween movie viewing websites are getting more and more intense. Therefore, it\nhas become incredibly important to accurately predict a given user's watching\nlist to maximize the chances of keeping the user on the platform. Recent\nstudies have suggested that the novelty-seeking propensity of users can impact\ntheir viewing behavior. In this paper, we aim to accurately model and describe\nthis novelty-seeking trait across many users and timestamps driven by data,\ntaking into consideration user forgetfulness. Compared to previous studies, we\npropose a more robust measure for novelty. Our model, termed Deep Forgetful\nNovelty-Seeking Model (DFNSM), leverages demographic information about users,\ngenre information about movies, and novelty-seeking traits to predict the most\nlikely next actions of a user. To evaluate the performance of our model, we\nconducted extensive experiments on a large movie rating dataset. The results\nreveal that DFNSM is very effective for movie recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:49:38 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Zou", "Ruomu", ""]]}, {"id": "1909.02065", "submitter": "Keping Bi", "authors": "Keping Bi, Choon Hui Teo, Yesh Dattatreya, Vijai Mohan, W. Bruce Croft", "title": "Leverage Implicit Feedback for Context-aware Product Search", "comments": "Presented at 2019 SIGIR Workshop on eCommerce (ECOM'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search serves as an important entry point for online shopping. In\ncontrast to web search, the retrieved results in product search not only need\nto be relevant but also should satisfy customers' preferences in order to\nelicit purchases. Previous work has shown the efficacy of purchase history in\npersonalized product search. However, customers with little or no purchase\nhistory do not benefit from personalized product search. Furthermore,\npreferences extracted from a customer's purchase history are usually long-term\nand may not always align with her short-term interests. Hence, in this paper,\nwe leverage clicks within a query session, as implicit feedback, to represent\nusers' hidden intents, which further act as the basis for re-ranking subsequent\nresult pages for the query. It has been studied extensively to model user\npreference with implicit feedback in recommendation tasks. However, there has\nbeen little research on modeling users' short-term interest in product search.\nWe study whether short-term context could help promote users' ideal item in the\nfollowing result pages for a query. Furthermore, we propose an end-to-end\ncontext-aware embedding model which can capture long-term and short-term\ncontext dependencies. Our experimental results on the datasets collected from\nthe search log of a commercial product search engine show that short-term\ncontext leads to much better performance compared with long-term and no\ncontext. Our results also show that our proposed model is more effective than\nword-based context-aware models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:13:44 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 17:33:01 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Bi", "Keping", ""], ["Teo", "Choon Hui", ""], ["Dattatreya", "Yesh", ""], ["Mohan", "Vijai", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1909.02071", "submitter": "Keping Bi", "authors": "Keping Bi, Qingyao Ai, Yongfeng Zhang, W. Bruce Croft", "title": "Conversational Product Search Based on Negative Feedback", "comments": "Accepted as a long paper in CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357939", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent assistants change the way people interact with computers and make\nit possible for people to search for products through conversations when they\nhave purchase needs. During the interactions, the system could ask questions on\ncertain aspects of the ideal products to clarify the users' needs. For example,\nprevious work proposed to ask users the exact characteristics of their ideal\nitems before showing results. However, users may not have clear ideas about\nwhat an ideal item looks like, especially when they have not seen any item. So\nit is more feasible to facilitate the conversational search by showing example\nitems and asking for feedback instead. In addition, when the users provide\nnegative feedback for the presented items, it is easier to collect their\ndetailed feedback on certain properties (aspect-value pairs) of the\nnon-relevant items. By breaking down the item-level negative feedback to\nfine-grained feedback on aspect-value pairs, more information is available to\nhelp clarify users' intents. So in this paper, we propose a conversational\nparadigm for product search driven by non-relevant items, based on which\nfine-grained feedback is collected and utilized to show better results in the\nnext iteration. We then propose an aspect-value likelihood model to incorporate\nboth positive and negative feedback on fine-grained aspect-value pairs of the\nnon-relevant items. Experimental results show that our model is significantly\nbetter than state-of-the-art product search baselines without using feedback\nand those baselines using item-level negative feedback.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:47:17 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bi", "Keping", ""], ["Ai", "Qingyao", ""], ["Zhang", "Yongfeng", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1909.02107", "submitter": "Hao-Jun Shi", "authors": "Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang", "title": "Compositional Embeddings Using Complementary Partitions for\n  Memory-Efficient Recommendation Systems", "comments": "11 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1145/3394486.3403059", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning-based recommendation systems exploit hundreds to\nthousands of different categorical features, each with millions of different\ncategories ranging from clicks to posts. To respect the natural diversity\nwithin the categorical data, embeddings map each category to a unique dense\nrepresentation within an embedded space. Since each categorical feature could\ntake on as many as tens of millions of different possible categories, the\nembedding tables form the primary memory bottleneck during both training and\ninference. We propose a novel approach for reducing the embedding size in an\nend-to-end fashion by exploiting complementary partitions of the category set\nto produce a unique embedding vector for each category without explicit\ndefinition. By storing multiple smaller embedding tables based on each\ncomplementary partition and combining embeddings from each table, we define a\nunique embedding for each category at smaller memory cost. This approach may be\ninterpreted as using a specific fixed codebook to ensure uniqueness of each\ncategory's representation. Our experimental results demonstrate the\neffectiveness of our approach over the hashing trick for reducing the size of\nthe embedding tables in terms of model loss and accuracy, while retaining a\nsimilar reduction in the number of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:47:08 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 04:10:00 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Shi", "Hao-Jun Michael", ""], ["Mudigere", "Dheevatsa", ""], ["Naumov", "Maxim", ""], ["Yang", "Jiyan", ""]]}, {"id": "1909.02423", "submitter": "Vincent Labatut", "authors": "Xavier Bost (LIA), Serigne Gueye (LIA), Vincent Labatut (LIA), Martha\n  Larson (DMIR), Georges Linar\\`es (LIA), Damien Malinas (CNELIAS), Rapha\\\"el\n  Roth (CNELIAS)", "title": "Remembering Winter Was Coming: Character-Oriented Video Summaries of TV\n  Series", "comments": null, "journal-ref": "Multimedia Tools and Applications, Springer, 2019,\n  78(24):35373-35399", "doi": "10.1007/s11042-019-07969-4", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's popular TV series tend to develop continuous, complex plots spanning\nseveral seasons, but are often viewed in controlled and discontinuous\nconditions. Consequently, most viewers need to be re-immersed in the story\nbefore watching a new season. Although discussions with friends and family can\nhelp, we observe that most viewers make extensive use of summaries to re-engage\nwith the plot. Automatic generation of video summaries of TV series' complex\nstories requires, first, modeling the dynamics of the plot and, second,\nextracting relevant sequences. In this paper, we tackle plot modeling by\nconsidering the social network of interactions between the characters involved\nin the narrative: substantial, durable changes in a major character's social\nenvironment suggest a new development relevant for the summary. Once\nidentified, these major stages in each character's storyline can be used as a\nbasis for completing the summary with related sequences. Our algorithm combines\nsuch social network analysis with filmmaking grammar to automatically generate\ncharacter-oriented video summaries of TV series from partially annotated data.\nWe carry out evaluation with a user study in a real-world scenario: a large\nsample of viewers were asked to rank video summaries centered on five\ncharacters of the popular TV series Game of Thrones, a few weeks before the\nnew, sixth season was released. Our results reveal the ability of\ncharacter-oriented summaries to re-engage viewers in television series and\nconfirm the contributions of modeling the plot content and exploiting stylistic\npatterns to identify salient sequences.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:00:45 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:24:57 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 07:10:52 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Gueye", "Serigne", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Larson", "Martha", "", "DMIR"], ["Linar\u00e8s", "Georges", "", "LIA"], ["Malinas", "Damien", "", "CNELIAS"], ["Roth", "Rapha\u00ebl", "", "CNELIAS"]]}, {"id": "1909.02523", "submitter": "Vito Walter Anelli", "authors": "Vito Walter Anelli and Tommaso Di Noia and Eugenio Di Sciascio and\n  Claudio Pomo and Azzurra Ragone", "title": "On the discriminative power of Hyper-parameters in Cross-Validation and\n  how to choose them", "comments": "5 pages RecSys 2019", "journal-ref": null, "doi": "10.1145/3298689.3347010", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-parameters tuning is a crucial task to make a model perform at its\nbest. However, despite the well-established methodologies, some aspects of the\ntuning remain unexplored. As an example, it may affect not just accuracy but\nalso novelty as well as it may depend on the adopted dataset. Moreover,\nsometimes it could be sufficient to concentrate on a single parameter only (or\na few of them) instead of their overall set. In this paper we report on our\ninvestigation on hyper-parameters tuning by performing an extensive 10-Folds\nCross-Validation on MovieLens and Amazon Movies for three well-known baselines:\nUser-kNN, Item-kNN, BPR-MF. We adopted a grid search strategy considering\napproximately 15 values for each parameter, and we then evaluated each\ncombination of parameters in terms of accuracy and novelty. We investigated the\ndiscriminative power of nDCG, Precision, Recall, MRR, EFD, EPC, and, finally,\nwe analyzed the role of parameters on model evaluation for Cross-Validation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:49:53 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Di Sciascio", "Eugenio", ""], ["Pomo", "Claudio", ""], ["Ragone", "Azzurra", ""]]}, {"id": "1909.02768", "submitter": "Marius K\\\"oppel", "authors": "Marius K\\\"oppel, Alexander Segner, Martin Wagener, Lukas Pensel,\n  Andreas Karwath, Stefan Kramer", "title": "Pairwise Learning to Rank by Neural Networks Revisited: Reconstruction,\n  Theoretical Analysis and Practical Performance", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pairwise learning to rank approach based on a neural net, called\nDirectRanker, that generalizes the RankNet architecture. We show mathematically\nthat our model is reflexive, antisymmetric, and transitive allowing for\nsimplified training and improved performance. Experimental results on the LETOR\nMSLR-WEB10K, MQ2007 and MQ2008 datasets show that our model outperforms\nnumerous state-of-the-art methods, while being inherently simpler in structure\nand using a pairwise approach only.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:42:58 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["K\u00f6ppel", "Marius", ""], ["Segner", "Alexander", ""], ["Wagener", "Martin", ""], ["Pensel", "Lukas", ""], ["Karwath", "Andreas", ""], ["Kramer", "Stefan", ""]]}, {"id": "1909.02776", "submitter": "Hosein Rezaei", "authors": "Hosein Rezaei, Seyed Amid Moeinzadeh, Azar Shahgholian and Mohamad\n  Saraee", "title": "Features in Extractive Supervised Single-document Summarization: Case of\n  Persian News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization has been one of the most challenging areas of research in\nNLP. Much effort has been made to overcome this challenge by using either the\nabstractive or extractive methods. Extractive methods are more popular, due to\ntheir simplicity compared with the more elaborate abstractive methods. In\nextractive approaches, the system will not generate sentences. Instead, it\nlearns how to score sentences within the text by using some textual features\nand subsequently selecting those with the highest-rank. Therefore, the core\nobjective is ranking and it highly depends on the document. This dependency has\nbeen unnoticed by many state-of-the-art solutions. In this work, the features\nof the document are integrated into vectors of every sentence. In this way, the\nsystem becomes informed about the context, increases the precision of the\nlearned model and consequently produces comprehensive and brief summaries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 09:04:14 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 03:10:27 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Rezaei", "Hosein", ""], ["Moeinzadeh", "Seyed Amid", ""], ["Shahgholian", "Azar", ""], ["Saraee", "Mohamad", ""]]}, {"id": "1909.03242", "submitter": "Isabelle Augenstein", "authors": "Isabelle Augenstein and Christina Lioma and Dongsheng Wang and Lucas\n  Chaves Lima and Casper Hansen and Christian Hansen and Jakob Grue Simonsen", "title": "MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact\n  Checking of Claims", "comments": "Proceedings of EMNLP 2019, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute the largest publicly available dataset of naturally occurring\nfactual claims for the purpose of automatic claim verification. It is collected\nfrom 26 fact checking websites in English, paired with textual sources and rich\nmetadata, and labelled for veracity by human expert journalists. We present an\nin-depth analysis of the dataset, highlighting characteristics and challenges.\nFurther, we present results for automatic veracity prediction, both with\nestablished baselines and with a novel method for joint ranking of evidence\npages and predicting veracity that outperforms all baselines. Significant\nperformance increases are achieved by encoding evidence, and by modelling\nmetadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that\nthis is a challenging testbed for claim veracity prediction.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 10:57:29 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 15:51:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Augenstein", "Isabelle", ""], ["Lioma", "Christina", ""], ["Wang", "Dongsheng", ""], ["Lima", "Lucas Chaves", ""], ["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Simonsen", "Jakob Grue", ""]]}, {"id": "1909.03276", "submitter": "Weiyu Cheng", "authors": "Weiyu Cheng, Yanyan Shen, Linpeng Huang", "title": "Adaptive Factorization Network: Learning Adaptive-Order Feature\n  Interactions", "comments": "Accepted by AAAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various factorization-based methods have been proposed to leverage\nsecond-order, or higher-order cross features for boosting the performance of\npredictive models. They generally enumerate all the cross features under a\npredefined maximum order, and then identify useful feature interactions through\nmodel training, which suffer from two drawbacks. First, they have to make a\ntrade-off between the expressiveness of higher-order cross features and the\ncomputational cost, resulting in suboptimal predictions. Second, enumerating\nall the cross features, including irrelevant ones, may introduce noisy feature\ncombinations that degrade model performance. In this work, we propose the\nAdaptive Factorization Network (AFN), a new model that learns arbitrary-order\ncross features adaptively from data. The core of AFN is a logarithmic\ntransformation layer to convert the power of each feature in a feature\ncombination into the coefficient to be learned. The experimental results on\nfour real datasets demonstrate the superior predictive performance of AFN\nagainst the start-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 14:30:43 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 02:05:51 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Cheng", "Weiyu", ""], ["Shen", "Yanyan", ""], ["Huang", "Linpeng", ""]]}, {"id": "1909.03443", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog", "title": "Auto-completion for Data Cells in Relational Tables", "comments": "In Proceedings of the 28th ACM International Conference on\n  Information and Knowledge Management (CIKM '19), 2019", "journal-ref": null, "doi": "10.1145/3357384.3357932", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of auto-completing data cells in relational tables. Such\ntables describe entities (in rows) with their attributes (in columns). We\npresent the CellAutoComplete framework to tackle several novel aspects of this\nproblem, including: (i) enabling a cell to have multiple, possibly conflicting\nvalues, (ii) supplementing the predicted values with supporting evidence, (iii)\ncombining evidence from multiple sources, and (iv) handling the case where a\ncell should be left empty. Our framework makes use of a large table corpus and\na knowledge base as data sources, and consists of preprocessing, candidate\nvalue finding, and value ranking components. Using a purpose-built test\ncollection, we show that our approach is 40\\% more effective than the best\nbaseline.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 12:17:15 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:54:45 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1909.03508", "submitter": "Martin Andrews", "authors": "Yew Ken Chia, Sam Witteveen, Martin Andrews", "title": "Transformer to CNN: Label-scarce distillation for efficient text\n  classification", "comments": "Accepted paper for CDNNRIA workshop at NeurIPS 2018. (3 pages +\n  references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant advances have been made in Natural Language Processing (NLP)\nmodelling since the beginning of 2018. The new approaches allow for accurate\nresults, even when there is little labelled data, because these NLP models can\nbenefit from training on both task-agnostic and task-specific unlabelled data.\nHowever, these advantages come with significant size and computational costs.\nThis workshop paper outlines how our proposed convolutional student\narchitecture, having been trained by a distillation process from a large-scale\nmodel, can achieve 300x inference speedup and 39x reduction in parameter count.\nIn some cases, the student model performance surpasses its teacher on the\nstudied tasks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 16:57:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Chia", "Yew Ken", ""], ["Witteveen", "Sam", ""], ["Andrews", "Martin", ""]]}, {"id": "1909.03527", "submitter": "Danish Contractor", "authors": "Danish Contractor and Krunal Shah and Aditi Partap and Mausam and\n  Parag Singla", "title": "Large Scale Question Answering using Tourism Data", "comments": "20 pages with supplementary notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel task of answering entity-seeking recommendation\nquestions using a collection of reviews that describe candidate answer\nentities. We harvest a QA dataset that contains 47,124 paragraph-sized real\nuser questions from travelers seeking recommendations for hotels, attractions\nand restaurants. Each question can have thousands of candidate answers to\nchoose from and each candidate is associated with a collection of unstructured\nreviews. This dataset is especially challenging because commonly used neural\narchitectures for reasoning and QA are prohibitively expensive for a task of\nthis scale. As a solution, we design a scalable cluster-select-rerank approach.\nIt first clusters text for each entity to identify exemplar sentences\ndescribing an entity. It then uses a scalable neural information retrieval (IR)\nmodule to select a set of potential entities from the large candidate set. A\nreranker uses a deeper attention-based architecture to pick the best answers\nfrom the selected entities. This strategy performs better than a pure IR or a\npure attention-based reasoning approach yielding nearly 25% relative\nimprovement in Accuracy@3 over both approaches.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 18:35:03 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 17:17:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Contractor", "Danish", ""], ["Shah", "Krunal", ""], ["Partap", "Aditi", ""], ["Mausam", "", ""], ["Singla", "Parag", ""]]}, {"id": "1909.03529", "submitter": "Junliang Yu", "authors": "Junliang Yu, Min Gao, Hongzhi Yin, Jundong Li, Chongming Gao, Qinyong\n  Wang", "title": "Generating Reliable Friends via Adversarial Training to Improve Social\n  Recommendation", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2019.00087", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent studies of social recommendation assume that people share\nsimilar preferences with their friends and the online social relations are\nhelpful in improving traditional recommender systems. However, this assumption\nis often untenable as the online social networks are quite sparse and a\nmajority of users only have a small number of friends. Besides, explicit\nfriends may not share similar interests because of the randomness in the\nprocess of building social networks. Therefore, discovering a number of\nreliable friends for each user plays an important role in advancing social\nrecommendation. Unlike other studies which focus on extracting valuable\nexplicit social links, our work pays attention to identifying reliable friends\nin both the observed and unobserved social networks. Concretely, in this paper,\nwe propose an end-to-end social recommendation framework based on Generative\nAdversarial Nets (GAN). The framework is composed of two blocks: a generator\nthat is used to produce friends that can possibly enhance the social\nrecommendation model, and a discriminator that is responsible for assessing\nthese generated friends and ranking the items according to both the current\nuser and her friends' preferences. With the competition between the generator\nand the discriminator, our framework can dynamically and adaptively generate\nreliable friends who can perfectly predict the current user' preference at a\nspecific time. As a result, the sparsity and unreliability problems of explicit\nsocial relations can be mitigated and the social recommendation performance is\nsignificantly improved. Experimental studies on real-world datasets demonstrate\nthe superiority of our framework and verify the positive effects of the\ngenerated reliable friends.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 19:03:33 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yu", "Junliang", ""], ["Gao", "Min", ""], ["Yin", "Hongzhi", ""], ["Li", "Jundong", ""], ["Gao", "Chongming", ""], ["Wang", "Qinyong", ""]]}, {"id": "1909.03579", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos, George Karypis", "title": "Boosting Item-based Collaborative Filtering via Nearly Uncoupled Random\n  Walks", "comments": "26 pages, complete version of the RecWalk conference paper that\n  appeared in ACM WSDM 2019", "journal-ref": null, "doi": "10.1145/3406241", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Item-based models are among the most popular collaborative filtering\napproaches for building recommender systems. Random walks can provide a\npowerful tool for harvesting the rich network of interactions captured within\nthese models. They can exploit indirect relations between the items, mitigate\nthe effects of sparsity, ensure wider itemspace coverage, as well as increase\nthe diversity of recommendation lists. Their potential, however, can be\nhindered by the tendency of the walks to rapidly concentrate towards the\ncentral nodes of the graph, thereby significantly restricting the range of\nK-step distributions that can be exploited for personalized recommendations. In\nthis work we introduce RecWalk; a novel random walk-based method that leverages\nthe spectral properties of nearly uncoupled Markov chains to provably lift this\nlimitation and prolong the influence of users' past preferences on the\nsuccessive steps of the walk---allowing the walker to explore the underlying\nnetwork more fruitfully. A comprehensive set of experiments on real-world\ndatasets verify the theoretically predicted properties of the proposed approach\nand indicate that they are directly linked to significant improvements in top-n\nrecommendation accuracy. They also highlight RecWalk's potential in providing a\nframework for boosting the performance of item-based models. RecWalk achieves\nstate-of-the-art top-n recommendation quality outperforming several competing\napproaches, including recently proposed methods that rely on deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 01:00:11 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:48:13 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Karypis", "George", ""]]}, {"id": "1909.03602", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing\n  Liu, Jiliang Tang, Hui Liu", "title": "DEAR: Deep Reinforcement Learning for Online Advertising Impression in\n  Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent prevalence of Reinforcement Learning (RL), there have been\ntremendous interests in utilizing RL for online advertising in recommendation\nplatforms (e.g., e-commerce and news feed sites). However, most RL-based\nadvertising algorithms focus on optimizing ads' revenue while ignoring the\npossible negative influence of ads on user experience of recommended items\n(products, articles and videos). Developing an optimal advertising algorithm in\nrecommendations faces immense challenges because interpolating ads improperly\nor too frequently may decrease user experience, while interpolating fewer ads\nwill reduce the advertising revenue. Thus, in this paper, we propose a novel\nadvertising strategy for the rec/ads trade-off. To be specific, we develop an\nRL-based framework that can continuously update its advertising strategies and\nmaximize reward in the long run. Given a recommendation list, we design a novel\nDeep Q-network architecture that can determine three internally related tasks\njointly, i.e., (i) whether to interpolate an ad or not in the recommendation\nlist, and if yes, (ii) the optimal ad and (iii) the optimal location to\ninterpolate. The experimental results based on real-world data demonstrate the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 02:56:03 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 02:54:37 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 01:39:14 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Gu", "Changsheng", ""], ["Zhang", "Haoshenglun", ""], ["Yang", "Xiwang", ""], ["Liu", "Xiaobing", ""], ["Tang", "Jiliang", ""], ["Liu", "Hui", ""]]}, {"id": "1909.03653", "submitter": "Svitlana Vakulenko", "authors": "Sophia Keyner, Vadim Savenkov, Svitlana Vakulenko", "title": "Open Data Chatbot", "comments": null, "journal-ref": "The Semantic Web - 16th International Conference, ESWC 2019,\n  Portoroz, Slovenia, June 2-6, 2019", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, chatbots received an increased attention from industry and diverse\nresearch communities as a dialogue-based interface providing advanced\nhuman-computer interactions. On the other hand, Open Data continues to be an\nimportant trend and a potential enabler for government transparency and citizen\nparticipation. This paper shows how these two paradigms can be combined to help\nnon-expert users find and discover open government datasets through dialogue.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:42:46 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Keyner", "Sophia", ""], ["Savenkov", "Vadim", ""], ["Vakulenko", "Svitlana", ""]]}, {"id": "1909.03654", "submitter": "Bin Guo", "authors": "Bin Guo, Yasan Ding, Lina Yao, Yunji Liang, Zhiwen Yu", "title": "The Future of Misinformation Detection: New Perspectives and Trends", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive spread of misinformation in social networks has become a global\nrisk, implicitly influencing public opinion and threatening social/political\ndevelopment. Misinformation detection (MID) has thus become a surging research\ntopic in recent years. As a promising and rapid developing research field, we\nfind that many efforts have been paid to new research problems and approaches\nof MID. Therefore, it is necessary to give a comprehensive review of the new\nresearch trends of MID. We first give a brief review of the literature history\nof MID, based on which we present several new research challenges and\ntechniques of it, including early detection, detection by multimodal data\nfusion, and explanatory detection. We further investigate the extraction and\nusage of various crowd intelligence in MID, which paves a promising way to\ntackle MID challenges. Finally, we give our own views on the open issues and\nfuture research directions of MID, such as model adaptivity/generality to new\nevents, embracing of novel machine learning models, explanatory detection\nmodels, and so on.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:45:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Guo", "Bin", ""], ["Ding", "Yasan", ""], ["Yao", "Lina", ""], ["Liang", "Yunji", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1909.03733", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih, Hamad Alsawalqah, Basima Elshqeirat, Tomayess Issa,\n  Pornpit Wongthongtham", "title": "Toward a Knowledge-based Personalised Recommender System for Mobile App\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the arena of mobile application development has\nexpanded considerably beyond the balance of the world\\'s software markets. With\nthe growing number of mobile software companies, and the mounting\nsophistication of smartphones\\' technology, developers have been building\nseveral categories of applications on dissimilar platforms. However, developers\nconfront several challenges through the implementation of mobile application\nprojects. In particular, there is a lack of consolidated systems that are\ncompetent to provide developers with personalised services promptly and\nefficiently. Hence, it is essential to develop tailored systems which can\nrecommend appropriate tools, IDEs, platforms, software components and other\ncorrelated artifacts to mobile application developers. This paper proposes a\nnew recommender system framework comprising a fortified set of techniques that\nare designed to provide mobile app developers with a distinctive platform to\nbrowse and search for the personalised artifacts. The proposed system make use\nof ontology and semantic web technology as well as machine learning techniques.\nIn particular, the new RS framework comprises the following components; (i)\ndomain knowledge inference module: including various semantic web technologies\nand lightweight ontologies; (ii) profiling and preferencing: a new proposed\ntime-aware multidimensional user modelling; (iii) query expansion: to improve\nand enhance the retrieved results by semantically augmenting users\\' query; and\n(iv) recommendation and information filtration: to make use of the\naforementioned components to provide personalised services to the designated\nusers and to answer a user\\'s query with the minimum mismatches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:55:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 20:25:57 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 21:02:37 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Abu-Salih", "Bilal", ""], ["Alsawalqah", "Hamad", ""], ["Elshqeirat", "Basima", ""], ["Issa", "Tomayess", ""], ["Wongthongtham", "Pornpit", ""]]}, {"id": "1909.04031", "submitter": "Keping Bi", "authors": "Keping Bi, Choon Hui Teo, Yesh Dattatreya, Vijai Mohan, W. Bruce Croft", "title": "A Study of Context Dependencies in Multi-page Product Search", "comments": "Accepted by CIKM 2019. arXiv admin note: substantial text overlap\n  with arXiv:1909.02065", "journal-ref": null, "doi": "10.1145/3357384.3358095", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In product search, users tend to browse results on multiple search result\npages (SERPs) (e.g., for queries on clothing and shoes) before deciding which\nitem to purchase. Users' clicks can be considered as implicit feedback which\nindicates their preferences and used to re-rank subsequent SERPs. Relevance\nfeedback (RF) techniques are usually involved to deal with such scenarios.\nHowever, these methods are designed for document retrieval, where relevance is\nthe most important criterion. In contrast, product search engines need to\nretrieve items that are not only relevant but also satisfactory in terms of\ncustomers' preferences. Personalization based on users' purchase history has\nbeen shown to be effective in product search. However, this method captures\nusers' long-term interest, which does not always align with their short-term\ninterest, and does not benefit customers with little or no purchase history. In\nthis paper, we study RF techniques based on both long-term and short-term\ncontext dependencies in multi-page product search. We also propose an\nend-to-end context-aware embedding model which can capture both types of\ncontext. Our experimental results show that short-term context leads to much\nbetter performance compared with long-term and no context. Moreover, our\nproposed model is more effective than state-of-art word-based RF models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 19:00:49 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 18:26:28 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Bi", "Keping", ""], ["Teo", "Choon Hui", ""], ["Dattatreya", "Yesh", ""], ["Mohan", "Vijai", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1909.04190", "submitter": "Nhan Nguyen-Thanh", "authors": "Nhan Nguyen-Thanh, Dana Marinca, Kinda Khawam, David Rohde, Flavian\n  Vasile, Elena Simona Lohan, Steven Martin, Dominique Quadri", "title": "Recommendation System-based Upper Confidence Bound for Online\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the method UCB-RS, which resorts to recommendation system (RS)\nfor enhancing the upper-confidence bound algorithm UCB, is presented. The\nproposed method is used for dealing with non-stationary and large-state spaces\nmulti-armed bandit problems. The proposed method has been targeted to the\nproblem of the product recommendation in the online advertising. Through\nextensive testing with RecoGym, an OpenAI Gym-based reinforcement learning\nenvironment for the product recommendation in online advertising, the proposed\nmethod outperforms the widespread reinforcement learning schemes such as\n$\\epsilon$-Greedy, Upper Confidence (UCB1) and Exponential Weights for\nExploration and Exploitation (EXP3).\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 22:43:33 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Nguyen-Thanh", "Nhan", ""], ["Marinca", "Dana", ""], ["Khawam", "Kinda", ""], ["Rohde", "David", ""], ["Vasile", "Flavian", ""], ["Lohan", "Elena Simona", ""], ["Martin", "Steven", ""], ["Quadri", "Dominique", ""]]}, {"id": "1909.04239", "submitter": "Yitong Meng", "authors": "Yitong Meng, Xinyan Dai, Xiao Yan, James Cheng, Weiwen Liu, Benben\n  Liao, Jun Guo, Guangyong Chen", "title": "PMD: An Optimal Transportation-based User Distance for Recommender\n  Systems", "comments": "This paper is accepted by European Conference on Information\n  Retrieval (ECIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering, a widely-used recommendation technique, predicts a\nuser's preference by aggregating the ratings from similar users. As a result,\nthese measures cannot fully utilize the rating information and are not suitable\nfor real world sparse data. To solve these issues, we propose a novel user\ndistance measure named Preference Mover's Distance (PMD) which makes full use\nof all ratings made by each user. Our proposed PMD can properly measure the\ndistance between a pair of users even if they have no co-rated items. We show\nthat this measure can be cast as an instance of the Earth Mover's Distance, a\nwell-studied transportation problem for which several highly efficient solvers\nhave been developed. Experimental results show that PMD can help achieve\nsuperior recommendation accuracy than state-of-the-art methods, especially when\ntraining data is very sparse.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 02:06:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 07:05:41 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Meng", "Yitong", ""], ["Dai", "Xinyan", ""], ["Yan", "Xiao", ""], ["Cheng", "James", ""], ["Liu", "Weiwen", ""], ["Liao", "Benben", ""], ["Guo", "Jun", ""], ["Chen", "Guangyong", ""]]}, {"id": "1909.04266", "submitter": "Yitong Meng", "authors": "Yitong Meng, Guangyong Chen, Benben Liao, Jun Guo, Weiwen Liu", "title": "Wasserstein Collaborative Filtering for Item Cold-start Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The item cold-start problem seriously limits the recommendation performance\nof Collaborative Filtering (CF) methods when new items have either none or very\nlittle interactions. To solve this issue, many modern Internet applications\npropose to predict a new item's interaction from the possessing contents.\nHowever, it is difficult to design and learn a map between the item's\ninteraction history and the corresponding contents. In this paper, we apply the\nWasserstein distance to address the item cold-start problem. Given item content\ninformation, we can calculate the similarity between the interacted items and\ncold-start ones, so that a user's preference on cold-start items can be\ninferred by minimizing the Wasserstein distance between the distributions over\nthese two types of items. We further adopt the idea of CF and propose\nWasserstein CF (WCF) to improve the recommendation performance on cold-start\nitems. Experimental results demonstrate the superiority of WCF over\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 03:32:05 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Meng", "Yitong", ""], ["Chen", "Guangyong", ""], ["Liao", "Benben", ""], ["Guo", "Jun", ""], ["Liu", "Weiwen", ""]]}, {"id": "1909.04276", "submitter": "Pankaj Malhotra", "authors": "Priyanka Gupta, Diksha Garg, Pankaj Malhotra, Lovekesh Vig, Gautam\n  Shroff", "title": "NISER: Normalized Item and Session Representations to Handle Popularity\n  Bias", "comments": "Presented at 1st International Workshop on Graph Representation\n  Learning and its Applications, CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of session-based recommendation (SR) models is to utilize the\ninformation from past actions (e.g. item/product clicks) in a session to\nrecommend items that a user is likely to click next. Recently it has been shown\nthat the sequence of item interactions in a session can be modeled as\ngraph-structured data to better account for complex item transitions. Graph\nneural networks (GNNs) can learn useful representations for such\nsession-graphs, and have been shown to improve over sequential models such as\nrecurrent neural networks [14]. However, we note that these GNN-based\nrecommendation models suffer from popularity bias: the models are biased\ntowards recommending popular items, and fail to recommend relevant long-tail\nitems (less popular or less frequent items). Therefore, these models perform\npoorly for the less popular new items arriving daily in a practical online\nsetting. We demonstrate that this issue is, in part, related to the magnitude\nor norm of the learned item and session-graph representations (embedding\nvectors). We propose a training procedure that mitigates this issue by using\nnormalized representations. The models using normalized item and session-graph\nrepresentations perform significantly better: i. for the less popular long-tail\nitems in the offline setting, and ii. for the less popular newly introduced\nitems in the online setting. Furthermore, our approach significantly improves\nupon existing state-of-the-art on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 04:24:35 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 05:29:42 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 13:28:59 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 13:04:37 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Gupta", "Priyanka", ""], ["Garg", "Diksha", ""], ["Malhotra", "Pankaj", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "1909.04465", "submitter": "Chun Yuan Yuan", "authors": "Chunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong Han, Songlin Hu", "title": "Jointly embedding the local and global relations of heterogeneous graph\n  for rumor detection", "comments": "10 pages, Accepted to the IEEE International Conference on Data\n  Mining 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of social media has revolutionized the way people\ncommunicate, share information and make decisions, but it also provides an\nideal platform for publishing and spreading rumors. Existing rumor detection\nmethods focus on finding clues from text content, user profiles, and\npropagation patterns. However, the local semantic relation and global\nstructural information in the message propagation graph have not been well\nutilized by previous works.\n  In this paper, we present a novel global-local attention network (GLAN) for\nrumor detection, which jointly encodes the local semantic and global structural\ninformation. We first generate a better integrated representation for each\nsource tweet by fusing the semantic information of related retweets with the\nattention mechanism. Then, we model the global relationships among all source\ntweets, retweets, and users as a heterogeneous graph to capture the rich\nstructural information for rumor detection. We conduct experiments on three\nreal-world datasets, and the results demonstrate that GLAN significantly\noutperforms the state-of-the-art models in both rumor detection and early\ndetection scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:18:59 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 12:42:08 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Yuan", "Chunyuan", ""], ["Ma", "Qianwen", ""], ["Zhou", "Wei", ""], ["Han", "Jizhong", ""], ["Hu", "Songlin", ""]]}, {"id": "1909.04491", "submitter": "Zi-Jing Liu", "authors": "Zijing Liu, Mauricio Barahona", "title": "Graph-based data clustering via multiscale community detection", "comments": "16 pages, 5 figures", "journal-ref": "Appl Netw Sci (2020) 5: 3", "doi": "10.1007/s41109-019-0248-7", "report-no": null, "categories": "cs.IR cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graph-theoretical approach to data clustering, which combines\nthe creation of a graph from the data with Markov Stability, a multiscale\ncommunity detection framework. We show how the multiscale capabilities of the\nmethod allow the estimation of the number of clusters, as well as alleviating\nthe sensitivity to the parameters in graph construction. We use both synthetic\nand benchmark real datasets to compare and evaluate several graph construction\nmethods and clustering algorithms, and show that multiscale graph-based\nclustering achieves improved performance compared to popular clustering methods\nwithout the need to set externally the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:44:26 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 10:21:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Liu", "Zijing", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1909.04493", "submitter": "Ningyu Zhang", "authors": "Qianghuai Jia, Ningyu Zhang, Nengwei Hua", "title": "Context-aware Deep Model for Entity Recommendation in Search Engine at\n  Alibaba", "comments": "CIKM2019 International Workshop on Entity Retrieval. arXiv admin\n  note: text overlap with arXiv:1511.08996 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity recommendation, providing search users with an improved experience via\nassisting them in finding related entities for a given query, has become an\nindispensable feature of today's search engines. Existing studies typically\nonly consider the queries with explicit entities. They usually fail to handle\ncomplex queries that without entities, such as \"what food is good for cold\nweather\", because their models could not infer the underlying meaning of the\ninput text. In this work, we believe that contexts convey valuable evidence\nthat could facilitate the semantic modeling of queries, and take them into\nconsideration for entity recommendation. In order to better model the semantics\nof queries and entities, we learn the representation of queries and entities\njointly with attentive deep neural networks. We evaluate our approach using\nlarge-scale, real-world search logs from a widely used commercial Chinese\nsearch engine. Our system has been deployed in ShenMa Search Engine and you can\nfetch it in UC Browser of Alibaba. Results from online A/B test suggest that\nthe impression efficiency of click-through rate increased by 5.1% and page view\nincreased by 5.5%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:47:20 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Jia", "Qianghuai", ""], ["Zhang", "Ningyu", ""], ["Hua", "Nengwei", ""]]}, {"id": "1909.04495", "submitter": "Yulun Hsieh", "authors": "Yu-Lun Hsieh and Minhao Cheng and Da-Cheng Juan and Wei Wei and\n  Wen-Lian Hsu and Cho-Jui Hsieh", "title": "Natural Adversarial Sentence Generation with Gradient-based Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work proposes a novel algorithm to generate natural language adversarial\ninput for text classification models, in order to investigate the robustness of\nthese models. It involves applying gradient-based perturbation on the sentence\nembeddings that are used as the features for the classifier, and learning a\ndecoder for generation. We employ this method to a sentiment analysis model and\nverify its effectiveness in inducing incorrect predictions by the model. We\nalso conduct quantitative and qualitative analysis on these examples and\ndemonstrate that our approach can generate more natural adversaries. In\naddition, it can be used to successfully perform black-box attacks, which\ninvolves attacking other existing models whose parameters are not known. On a\npublic sentiment analysis API, the proposed method introduces a 20% relative\ndecrease in average accuracy and 74% relative increase in absolute error.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 07:22:01 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Hsieh", "Yu-Lun", ""], ["Cheng", "Minhao", ""], ["Juan", "Da-Cheng", ""], ["Wei", "Wei", ""], ["Hsu", "Wen-Lian", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1909.04496", "submitter": "Chinmay Shukla", "authors": "Jake Sherman, Chinmay Shukla, Rhonda Textor, Su Zhang, Amy A. Winecoff", "title": "Assessing Fashion Recommendations: A Multifaceted Offline Evaluation\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is a unique domain for developing recommender systems (RS).\nPersonalization is critical to fashion users. As a result, highly accurate\nrecommendations are not sufficient unless they are also specific to users.\nMoreover, fashion data is characterized by a large majority of new users, so a\nrecommendation strategy that performs well only for users with prior\ninteraction history is a poor fit to the fashion problem. Critical to\naddressing these issues in fashion recommendation is an evaluation strategy\nthat: 1) includes multiple metrics that are relevant to fashion, and 2) is\nperformed within segments of users with different interaction histories. Here,\nwe present our multifaceted offline strategy for evaluating fashion RS. Using\nour proposed evaluation methodology, we compare the performance of three\ndifferent algorithms, a most popular (MP) items strategy, a collaborative\nfiltering (CF) strategy, and a content-based (CB) strategy. We demonstrate that\nonly by considering the performance of these algorithms across multiple metrics\nand user segments can we determine the extent to which each algorithm is likely\nto fulfill fashion users' needs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:04:53 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Sherman", "Jake", ""], ["Shukla", "Chinmay", ""], ["Textor", "Rhonda", ""], ["Zhang", "Su", ""], ["Winecoff", "Amy A.", ""]]}, {"id": "1909.04702", "submitter": "James Foulds", "authors": "Kamrun Naher Keya, Yannis Papanikolaou, James R. Foulds", "title": "Neural Embedding Allocation: Distributed Representations of Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models such as the skip-gram learn vector representations of\nwords' semantic relationships, and document embedding models learn similar\nrepresentations for documents. On the other hand, topic models provide latent\nrepresentations of the documents' topical themes. To get the benefits of these\nrepresentations simultaneously, we propose a unifying algorithm, called neural\nembedding allocation (NEA), which deconstructs topic models into interpretable\nvector-space embeddings of words, topics, documents, authors, and so on, by\nlearning neural embeddings to mimic the topic models. We showcase NEA's\neffectiveness and generality on LDA, author-topic models and the recently\nproposed mixed membership skip gram topic model and achieve better performance\nwith the embeddings compared to several state-of-the-art models. Furthermore,\nwe demonstrate that using NEA to smooth out the topics improves coherence\nscores over the original topic models when the number of topics is large.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:39:26 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Keya", "Kamrun Naher", ""], ["Papanikolaou", "Yannis", ""], ["Foulds", "James R.", ""]]}, {"id": "1909.04749", "submitter": "Meng Xia", "authors": "Meng Xia, Huan Wei, Min Xu, Leo Yu Ho Lo, Yong Wang, Rong Zhang,\n  Huamin Qu", "title": "Visual Analytics of Student Learning Behaviors on K-12 Mathematics\n  E-learning Platforms", "comments": "2 pages, 6 figures, 2019 VAST conference, Best Poster, Learning\n  Analytics, Visual Analytic System for Education, Online Learning, Learning\n  Data Analysis, Learning Trajectories Analysis, Mouse Movement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing popularity in online learning, a surge of E-learning\nplatforms have emerged to facilitate education opportunities for k-12 (from\nkindergarten to 12th grade) students and with this, a wealth of information on\ntheir learning logs are getting recorded. However, it remains unclear how to\nmake use of these detailed learning behavior data to improve the design of\nlearning materials and gain deeper insight into students' thinking and learning\nstyles. In this work, we propose a visual analytics system to analyze student\nlearning behaviors on a K-12 mathematics E-learning platform. It supports both\ncorrelation analysis between different attributes and a detailed visualization\nof user mouse-movement logs. Our case studies on a real dataset show that our\nsystem can better guide the design of learning resources (e.g., math questions)\nand facilitate quick interpretation of students' problem-solving and learning\nstyles.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 06:34:57 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 11:47:32 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Xia", "Meng", ""], ["Wei", "Huan", ""], ["Xu", "Min", ""], ["Lo", "Leo Yu Ho", ""], ["Wang", "Yong", ""], ["Zhang", "Rong", ""], ["Qu", "Huamin", ""]]}, {"id": "1909.04822", "submitter": "Elaheh ShafieiBavani", "authors": "Elaheh ShafieiBavani, Antonio Jimeno Yepes, Xu Zhong, David Martinez\n  Iraola", "title": "Global Locality in Biomedical Relation and Event Extraction", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1802.10569,\n  arXiv:1710.08312 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the exponential growth of biomedical literature, event and relation\nextraction are important tasks in biomedical text mining. Most work only focus\non relation extraction, and detect a single entity pair mention on a short span\nof text, which is not ideal due to long sentences that appear in biomedical\ncontexts. We propose an approach to both relation and event extraction, for\nsimultaneously predicting relationships between all mention pairs in a text. We\nalso perform an empirical study to discuss different network setups for this\npurpose. The best performing model includes a set of multi-head attentions and\nconvolutions, an adaptation of the transformer architecture, which offers\nself-attention the ability to strengthen dependencies among related elements,\nand models the interaction between features extracted by multiple attention\nheads. Experiment results demonstrate that our approach outperforms the state\nof the art on a set of benchmark biomedical corpora including BioNLP 2009,\n2011, 2013 and BioCreative 2017 shared tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 02:20:57 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 07:03:07 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["ShafieiBavani", "Elaheh", ""], ["Yepes", "Antonio Jimeno", ""], ["Zhong", "Xu", ""], ["Iraola", "David Martinez", ""]]}, {"id": "1909.04823", "submitter": "Haidong Rong", "authors": "Haidong Rong, Yangzihao Wang, Feihu Zhou, Junjie Zhai, Haiyang Wu, Rui\n  Lan, Fan Li, Han Zhang, Yuekui Yang, Zhenyu Guo, Di Wang", "title": "Distributed Equivalent Substitution Training for Large-Scale Recommender\n  Systems", "comments": "Accepted by SIGIR '2020. Proceedings of the 43rd International ACM\n  SIGIR Conference on Research and Development in Information Retrieval. 2020", "journal-ref": null, "doi": "10.1145/3397271.3401113", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Distributed Equivalent Substitution (DES) training, a novel\ndistributed training framework for large-scale recommender systems with dynamic\nsparse features. DES introduces fully synchronous training to large-scale\nrecommendation system for the first time by reducing communication, thus making\nthe training of commercial recommender systems converge faster and reach better\nCTR. DES requires much less communication by substituting the weights-rich\noperators with the computationally equivalent sub-operators and aggregating\npartial results instead of transmitting the huge sparse weights directly\nthrough the network. Due to the use of synchronous training on large-scale Deep\nLearning Recommendation Models (DLRMs), DES achieves higher AUC(Area Under\nROC). We successfully apply DES training on multiple popular DLRMs of\nindustrial scenarios. Experiments show that our implementation outperforms the\nstate-of-the-art PS-based training framework, achieving up to 68.7%\ncommunication savings and higher throughput compared to other PS-based\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 13:16:26 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 13:38:06 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 12:17:32 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 10:19:28 GMT"}, {"version": "v5", "created": "Sat, 30 May 2020 07:28:00 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Rong", "Haidong", ""], ["Wang", "Yangzihao", ""], ["Zhou", "Feihu", ""], ["Zhai", "Junjie", ""], ["Wu", "Haiyang", ""], ["Lan", "Rui", ""], ["Li", "Fan", ""], ["Zhang", "Han", ""], ["Yang", "Yuekui", ""], ["Guo", "Zhenyu", ""], ["Wang", "Di", ""]]}, {"id": "1909.04826", "submitter": "Pratik Ratadiya", "authors": "Pratik Ratadiya, Rahul Moorthy", "title": "Spam filtering on forums: A synthetic oversampling based approach for\n  imbalanced data classification", "comments": "Presented at SciPy India Conference 2018, IIT Bombay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Forums play an important role in providing a platform for community\ninteraction. The introduction of irrelevant content or spam by individuals for\ncommercial and social gains tends to degrade the professional experience\npresented to the forum users. Automated moderation of the relevancy of posted\ncontent is desired. Machine learning is used for text classification and finds\napplications in spam email detection, fraudulent transaction detection etc. The\nbalance of classes in training data is essential in the case of classification\nalgorithms to make the learning efficient and accurate. However, in the case of\nforums, the spam content is sparse compared to the relevant content giving rise\nto a bias towards the latter while training. A model trained on such biased\ndata will fail to classify a spam sample. An approach based on Synthetic\nMinority Over-sampling Technique(SMOTE) is presented in this paper to tackle\nimbalanced training data. It involves synthetically creating new minority class\nsamples from the existing ones until balance in data is achieved. The enhanced\ndata is then passed through various classifiers for which the performance is\nrecorded. The results were analyzed on the data of forums of Spoken Tutorial,\nIIT Bombay over standard performance metrics and revealed that models trained\nafter Synthetic Minority oversampling outperform the ones trained on imbalanced\ndata by substantial margins. An empirical comparison of the results obtained by\nboth SMOTE and without SMOTE for various supervised classification algorithms\nhave been presented in this paper. Synthetic oversampling proves to be a\ncritical technique for achieving uniform class distribution which in turn\nyields commendable results in text classification. The presented approach can\nbe further extended to content categorization on educational websites thus\nhelping to improve the overall digital learning experience.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:22:37 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ratadiya", "Pratik", ""], ["Moorthy", "Rahul", ""]]}, {"id": "1909.04847", "submitter": "Eugene Ie", "authors": "Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar,\n  Jing Wang, Rui Wu, Craig Boutilier", "title": "RecSim: A Configurable Simulation Platform for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose RecSim, a configurable platform for authoring simulation\nenvironments for recommender systems (RSs) that naturally supports sequential\ninteraction with users. RecSim allows the creation of new environments that\nreflect particular aspects of user behavior and item structure at a level of\nabstraction well-suited to pushing the limits of current reinforcement learning\n(RL) and RS techniques in sequential interactive recommendation problems.\nEnvironments can be easily configured that vary assumptions about: user\npreferences and item familiarity; user latent state and its dynamics; and\nchoice models and other user response behavior. We outline how RecSim offers\nvalue to RL and RS researchers and practitioners, and how it can serve as a\nvehicle for academic-industrial collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:43:45 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 13:30:53 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ie", "Eugene", ""], ["Hsu", "Chih-wei", ""], ["Mladenov", "Martin", ""], ["Jain", "Vihan", ""], ["Narvekar", "Sanmit", ""], ["Wang", "Jing", ""], ["Wu", "Rui", ""], ["Boutilier", "Craig", ""]]}, {"id": "1909.04925", "submitter": "Betty van Aken", "authors": "Betty van Aken, Benjamin Winter, Alexander L\\\"oser, Felix A. Gers", "title": "How Does BERT Answer Questions? A Layer-Wise Analysis of Transformer\n  Representations", "comments": "Accepted at CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3358028", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional Encoder Representations from Transformers (BERT) reach\nstate-of-the-art results in a variety of Natural Language Processing tasks.\nHowever, understanding of their internal functioning is still insufficient and\nunsatisfactory. In order to better understand BERT and other Transformer-based\nmodels, we present a layer-wise analysis of BERT's hidden states. Unlike\nprevious research, which mainly focuses on explaining Transformer models by\ntheir attention weights, we argue that hidden states contain equally valuable\ninformation. Specifically, our analysis focuses on models fine-tuned on the\ntask of Question Answering (QA) as an example of a complex downstream task. We\ninspect how QA models transform token vectors in order to find the correct\nanswer. To this end, we apply a set of general and QA-specific probing tasks\nthat reveal the information stored in each representation layer. Our\nqualitative analysis of hidden state visualizations provides additional\ninsights into BERT's reasoning process. Our results show that the\ntransformations within BERT go through phases that are related to traditional\npipeline tasks. The system can therefore implicitly incorporate task-specific\ninformation into its token representations. Furthermore, our analysis reveals\nthat fine-tuning has little impact on the models' semantic abilities and that\nprediction errors can be recognized in the vector representations of even early\nlayers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 08:55:09 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["van Aken", "Betty", ""], ["Winter", "Benjamin", ""], ["L\u00f6ser", "Alexander", ""], ["Gers", "Felix A.", ""]]}, {"id": "1909.04954", "submitter": "Philipp Mayr", "authors": "Guillaume Cabanac, Ingo Frommholz, Philipp Mayr", "title": "Report on the 8th International Workshop on Bibliometric-enhanced\n  Information Retrieval (BIR 2019)", "comments": "8 pages, report to appear in ACM SIGIR Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bibliometric-enhanced Information Retrieval workshop series (BIR) at ECIR\ntackled issues related to academic search, at the crossroads between\nInformation Retrieval and Bibliometrics. BIR is a hot topic investigated by\nboth academia (e.g., ArnetMiner, CiteSeerx, DocEar) and the industry (e.g.,\nGoogle Scholar, Microsoft Academic Search, Semantic Scholar). This report\npresents the 8th iteration of the one-day BIR workshop held at ECIR 2019 in\nCologne, Germany.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 10:07:59 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Cabanac", "Guillaume", ""], ["Frommholz", "Ingo", ""], ["Mayr", "Philipp", ""]]}, {"id": "1909.05038", "submitter": "Vito Walter Anelli", "authors": "Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio, Azzurra\n  Ragone, Joseph Trotta", "title": "How to make latent factors interpretable by feeding Factorization\n  machines with knowledge graphs", "comments": "Accepted as full paper at ISWC 2019, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based approaches to recommendation can recommend items with a very high\nlevel of accuracy. Unfortunately, even when the model embeds content-based\ninformation, if we move to a latent space we miss references to the actual\nsemantics of recommended items. Consequently, this makes non-trivial the\ninterpretation of a recommendation process. In this paper, we show how to\ninitialize latent factors in Factorization Machines by using semantic features\ncoming from a knowledge graph in order to train an interpretable model. With\nour model, semantic features are injected into the learning process to retain\nthe original informativeness of the items available in the dataset. The\naccuracy and effectiveness of the trained model have been tested using two\nwell-known recommender systems datasets. By relying on the information encoded\nin the original knowledge graph, we have also evaluated the semantic accuracy\nand robustness for the knowledge-aware interpretability of the final model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:25:38 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Di Sciascio", "Eugenio", ""], ["Ragone", "Azzurra", ""], ["Trotta", "Joseph", ""]]}, {"id": "1909.05099", "submitter": "Cl\\'ement Christophe", "authors": "Cl\\'ement Christophe, Julien Velcin, Jairo Cugliari, Philippe\n  Suignard, Manel Boumghar", "title": "How to detect novelty in textual data streams? A comparative study of\n  existing methods", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since datasets with annotation for novelty at the document and/or word level\nare not easily available, we present a simulation framework that allows us to\ncreate different textual datasets in which we control the way novelty occurs.\nWe also present a benchmark of existing methods for novelty detection in\ntextual data streams. We define a few tasks to solve and compare several\nstate-of-the-art methods. The simulation framework allows us to evaluate their\nperformances according to a set of limited scenarios and test their sensitivity\nto some parameters. Finally, we experiment with the same methods on different\nkinds of novelty in the New York Times Annotated Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 14:55:02 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Christophe", "Cl\u00e9ment", ""], ["Velcin", "Julien", ""], ["Cugliari", "Jairo", ""], ["Suignard", "Philippe", ""], ["Boumghar", "Manel", ""]]}, {"id": "1909.05314", "submitter": "Xueyuan She", "authors": "Xueyuan She, Yun Long, Daehyun Kim, Saibal Mukhopadhyay", "title": "ScieNet: Deep Learning with Spike-assisted Contextual Information\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) provide high image classification accuracy, but\nexperience significant performance degradation when perturbation from various\nsources are present in the input. The lack of resilience to input perturbations\nmakes DNN less reliable for systems interacting with physical world such as\nautonomous vehicles, robotics, to name a few, where imperfect input is the\nnormal condition. We present a hybrid deep network architecture with\nspike-assisted contextual information extraction (ScieNet). ScieNet integrates\nunsupervised learning using spiking neural network (SNN) for unsupervised\ncontextual informationextraction with a back-end DNN trained for\nclassification. The integrated network demonstrates high resilience to input\nperturbations without relying on prior training on perturbed inputs. We\ndemonstrate ScieNet with different back-end DNNs for image classification using\nCIFAR dataset considering stochastic (noise) and structured (rain) input\nperturbations. Experimental results demonstrate significant improvement in\naccuracy on noisy and rainy images without prior training, while maintaining\nstate-of-the-art accuracy on clean images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 19:10:07 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["She", "Xueyuan", ""], ["Long", "Yun", ""], ["Kim", "Daehyun", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1909.05414", "submitter": "Mei Wang", "authors": "Mei Wang, Weizhi Li, Yan Yan", "title": "Time-weighted Attentional Session-Aware Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Session-based Recurrent Neural Networks (RNNs) are gaining increasing\npopularity for recommendation task, due to the high autocorrelation of user's\nbehavior on the latest session and the effectiveness of RNN to capture the\nsequence order information. However, most existing session-based RNN\nrecommender systems still solely focus on the short-term interactions within a\nsingle session and completely discard all the other long-term data across\ndifferent sessions. While traditional Collaborative Filtering (CF) methods have\nmany advanced research works on exploring long-term dependency, which show\ngreat value to be explored and exploited in deep learning models. Therefore, in\nthis paper, we propose ASARS, a novel framework that effectively imports the\ntemporal dynamics methodology in CF into session-based RNN system in DL, such\nthat the temporal info can act as scalable weights by a parallel attentional\nnetwork. Specifically, we first conduct an extensive data analysis to show the\ndistribution and importance of such temporal interactions data both within\nsessions and across sessions. And then, our ASARS framework promotes two novel\nmodels: (1) an inter-session temporal dynamic model that captures the long-term\nuser interaction for RNN recommender system. We integrate the time changes in\nsession RNN and add user preferences as model drifting; and (2) a novel\ntriangle parallel attention network that enhances the original RNN model by\nincorporating time information. Such triangle parallel network is also\nspecially designed for realizing data argumentation in sequence-to-scalar RNN\narchitecture, and thus it can be trained very efficiently. Our extensive\nexperiments on four real datasets from different domains demonstrate the\neffectiveness and large improvement of ASARS for personalized recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 00:31:23 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Wang", "Mei", ""], ["Li", "Weizhi", ""], ["Yan", "Yan", ""]]}, {"id": "1909.05475", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang and Julian McAuley", "title": "Candidate Generation with Binary Codes for Large-Scale Top-N\n  Recommendation", "comments": "accepted to CIKM'19 as long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating the Top-N recommendations from a large corpus is computationally\nexpensive to perform at scale. Candidate generation and re-ranking based\napproaches are often adopted in industrial settings to alleviate efficiency\nproblems. However it remains to be fully studied how well such schemes\napproximate complete rankings (or how many candidates are required to achieve a\ngood approximation), or to develop systematic approaches to generate\nhigh-quality candidates efficiently. In this paper, we seek to investigate\nthese questions via proposing a candidate generation and re-ranking based\nframework (CIGAR), which first learns a preference-preserving binary embedding\nfor building a hash table to retrieve candidates, and then learns to re-rank\nthe candidates using real-valued ranking models with a candidate-oriented\nobjective. We perform a comprehensive study on several large-scale real-world\ndatasets consisting of millions of users/items and hundreds of millions of\ninteractions. Our results show that CIGAR significantly boosts the Top-N\naccuracy against state-of-the-art recommendation models, while reducing the\nquery time by orders of magnitude. We hope that this work could draw more\nattention to the candidate generation problem in recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:37:01 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["McAuley", "Julian", ""]]}, {"id": "1909.05478", "submitter": "Muhammad Nabeel Asim", "authors": "Muhammad Nabeel Asim, Muhammad Usman Ghani Khan, Muhammad Imran Malik,\n  Andreas Dengel, Sheraz Ahmed", "title": "A Robust Hybrid Approach for Textual Document Classification", "comments": "ICDAR Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text document classification is an important task for diverse natural\nlanguage processing based applications. Traditional machine learning approaches\nmainly focused on reducing dimensionality of textual data to perform\nclassification. This although improved the overall classification accuracy, the\nclassifiers still faced sparsity problem due to lack of better data\nrepresentation techniques. Deep learning based text document classification, on\nthe other hand, benefitted greatly from the invention of word embeddings that\nhave solved the sparsity problem and researchers focus mainly remained on the\ndevelopment of deep architectures. Deeper architectures, however, learn some\nredundant features that limit the performance of deep learning based solutions.\nIn this paper, we propose a two stage text document classification methodology\nwhich combines traditional feature engineering with automatic feature\nengineering (using deep learning). The proposed methodology comprises a filter\nbased feature selection (FSE) algorithm followed by a deep convolutional neural\nnetwork. This methodology is evaluated on the two most commonly used public\ndatasets, i.e., 20 Newsgroups data and BBC news data. Evaluation results reveal\nthat the proposed methodology outperforms the state-of-the-art of both the\n(traditional) machine learning and deep learning based text document\nclassification methodologies with a significant margin of 7.7% on 20 Newsgroups\nand 6.6% on BBC news datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:39:07 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Asim", "Muhammad Nabeel", ""], ["Khan", "Muhammad Usman Ghani", ""], ["Malik", "Muhammad Imran", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1909.05746", "submitter": "Tingle Li", "authors": "Tingle Li, Jiawei Chen, Haowen Hou, Ming Li", "title": "Sams-Net: A Sliced Attention-based Neural Network for Music Source\n  Separation", "comments": "Submitted to Interspeech 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) or Long short-term memory (LSTM) based\nmodels with the input of spectrogram or waveforms are commonly used for deep\nlearning based audio source separation. In this paper, we propose a Sliced\nAttention-based neural network (Sams-Net) in the spectrogram domain for the\nmusic source separation task. It enables spectral feature interactions with\nmulti-head attention mechanism, achieves easier parallel computing and has a\nlarger receptive field compared with LSTMs and CNNs respectively. Experimental\nresults on the MUSDB18 dataset show that the proposed method, with fewer\nparameters, outperforms most of the state-of-the-art DNN-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:21:36 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 06:46:27 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 13:46:41 GMT"}, {"version": "v4", "created": "Tue, 19 May 2020 03:37:22 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Li", "Tingle", ""], ["Chen", "Jiawei", ""], ["Hou", "Haowen", ""], ["Li", "Ming", ""]]}, {"id": "1909.05819", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Tomoya Machide, Ken-ichi Kawarabayashi", "title": "Anonymising Queries by Semantic Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting the privacy of search engine users is an important requirement in\nmany information retrieval scenarios. A user might not want a search engine to\nguess his or her information need despite requesting relevant results. We\npropose a method to protect the privacy of search engine users by decomposing\nthe queries using semantically \\emph{related} and unrelated \\emph{distractor}\nterms. Instead of a single query, the search engine receives multiple\ndecomposed query terms. Next, we reconstruct the search results relevant to the\noriginal query term by aggregating the search results retrieved for the\ndecomposed query terms. We show that the word embeddings learnt using a\ndistributed representation learning method can be used to find semantically\nrelated and distractor query terms. We derive the relationship between the\n\\emph{anonymity} achieved through the proposed query anonymisation method and\nthe \\emph{reconstructability} of the original search results using the\ndecomposed queries. We analytically study the risk of discovering the search\nengine users' information intents under the proposed query anonymisation\nmethod, and empirically evaluate its robustness against clustering-based\nattacks. Our experimental results show that the proposed method can accurately\nreconstruct the search results for user queries, without compromising the\nprivacy of the search engine users.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:27:46 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bollegala", "Danushka", ""], ["Machide", "Tomoya", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1909.05863", "submitter": "Ethan Perez", "authors": "Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe\n  Kiela, Kyunghyun Cho", "title": "Finding Generalizable Evidence by Learning to Convince Q&A Models", "comments": "EMNLP 2019. Code available at https://github.com/ethanjperez/convince", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system that finds the strongest supporting evidence for a given\nanswer to a question, using passage-based question-answering (QA) as a testbed.\nWe train evidence agents to select the passage sentences that most convince a\npretrained QA model of a given answer, if the QA model received those sentences\ninstead of the full passage. Rather than finding evidence that convinces one\nmodel alone, we find that agents select evidence that generalizes; agent-chosen\nevidence increases the plausibility of the supported answer, as judged by other\nQA models and humans. Given its general nature, this approach improves QA in a\nrobust manner: using agent-selected evidence (i) humans can correctly answer\nquestions with only ~20% of the full passage and (ii) QA models can generalize\nto longer passages and harder questions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:00:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Perez", "Ethan", ""], ["Karamcheti", "Siddharth", ""], ["Fergus", "Rob", ""], ["Weston", "Jason", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1909.05890", "submitter": "Chi Zhang", "authors": "Chi Zhang, Bryan Wilkinson, Ashwinkumar Ganesan, Tim Oates", "title": "Determining the Scale of Impact from Denial-of-Service Attacks in Real\n  Time Using Twitter", "comments": "DYnamic and Novel Advances in Machine Learning and Intelligent Cyber\n  Security Workshop, December 3--4, 2018, San Juan, PR, USA", "journal-ref": null, "doi": "10.1145/3306195.3306199", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denial of Service (DoS) attacks are common in on-line and mobile services\nsuch as Twitter, Facebook and banking. As the scale and frequency of\nDistributed Denial of Service (DDoS) attacks increase, there is an urgent need\nfor determining the impact of the attack. Two central challenges of the task\nare to get feedback from a large number of users and to get it in a timely\nmanner. In this paper, we present a weakly-supervised model that does not need\nannotated data to measure the impact of DoS issues by applying Latent Dirichlet\nAllocation and symmetric Kullback-Leibler divergence on tweets. There is a\nlimitation to the weakly-supervised module. It assumes that the event detected\nin a time window is a DoS attack event. This will become less of a problem,\nwhen more non-attack events twitter got collected and become less likely to be\nidentified as a new event. Another way to remove that limitation, an optional\nclassification layer, trained on manually annotated DoS attack tweets, to\nfilter out non-attack tweets can be used to increase precision at the expense\nof recall. Experimental results show that we can learn weakly-supervised models\nthat can achieve comparable precision to supervised ones and can be generalized\nacross entities in the same industry.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:11:56 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zhang", "Chi", ""], ["Wilkinson", "Bryan", ""], ["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""]]}, {"id": "1909.05965", "submitter": "Tian Xia", "authors": "Tian Xia, Shaodan Zhai, Shaojun Wang", "title": "Analysis of Regression Tree Fitting Algorithms in Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In learning to rank area, industry-level applications have been dominated by\ngradient boosting framework, which fits a tree using least square error\nprinciple. While in classification area, another tree fitting principle,\nweighted least square error, has been widely used, such as LogitBoost and its\nvariants. However, there is a lack of analysis on the relationship between the\ntwo principles in the scenario of learning to rank. We propose a new principle\nnamed least objective loss based error that enables us to analyze the issue\nabove as well as several important learning to rank models. We also implement\ntwo typical and strong systems and conduct our experiments in two real-world\ndatasets. Experimental results show that our proposed method brings moderate\nimprovements over least square error principle.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:07:48 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Xia", "Tian", ""], ["Zhai", "Shaodan", ""], ["Wang", "Shaojun", ""]]}, {"id": "1909.06058", "submitter": "Mengdi Zhu", "authors": "Mengdi Zhu, Zheye Deng, Wenhan Xiong, Mo Yu, Ming Zhang, William Yang\n  Wang", "title": "Neural Correction Model for Open-Domain Named Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) plays an important role in a wide range of\nnatural language processing tasks, such as relation extraction, question\nanswering, etc. However, previous studies on NER are limited to particular\ngenres, using small manually-annotated or large but low-quality datasets.\nMeanwhile, previous datasets for open-domain NER, built using distant\nsupervision, suffer from low precision, recall and ratio of annotated tokens\n(RAT). In this work, to address the low precision and recall problems, we first\nutilize DBpedia as the source of distant supervision to annotate abstracts from\nWikipedia and design a neural correction model trained with a human-annotated\nNER dataset, DocRED, to correct the false entity labels. In this way, we build\na large and high-quality dataset called AnchorNER and then train various models\nwith it. To address the low RAT problem of previous datasets, we introduce a\nmulti-task learning method to exploit the context information. We evaluate our\nmethods on five NER datasets and our experimental results show that models\ntrained with AnchorNER and our multi-task learning method obtain\nstate-of-the-art performances in the open-domain setting.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 06:44:30 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 10:14:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhu", "Mengdi", ""], ["Deng", "Zheye", ""], ["Xiong", "Wenhan", ""], ["Yu", "Mo", ""], ["Zhang", "Ming", ""], ["Wang", "William Yang", ""]]}, {"id": "1909.06076", "submitter": "Miklas S. Kristoffersen", "authors": "Miklas S. Kristoffersen, Jacob L. Wieland, Sven E. Shepstone,\n  Zheng-Hua Tan, Vinoba Vinayagamoorthy", "title": "Deep Joint Embeddings of Context and Content for Recommendation", "comments": "Accepted for CARS 2.0 - Context-Aware Recommender Systems Workshop @\n  RecSys'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning-based method for learning joint\ncontext-content embeddings (JCCE) with a view to context-aware recommendations,\nand demonstrate its application in the television domain. JCCE builds on recent\nprogress within latent representations for recommendation and deep metric\nlearning. The model effectively groups viewing situations and associated\nconsumed content, based on supervision from 2.7 million viewing events.\nExperiments confirm the recommendation ability of JCCE, achieving improvements\nwhen compared to state-of-the-art methods. Furthermore, the approach shows\nmeaningful structures in the learned representations that can be used to gain\nvaluable insights of underlying factors in the relationship between contextual\nsettings and content properties.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:14:22 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 05:09:18 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kristoffersen", "Miklas S.", ""], ["Wieland", "Jacob L.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""], ["Vinayagamoorthy", "Vinoba", ""]]}, {"id": "1909.06133", "submitter": "Andrea Barraza", "authors": "Andrea Barraza-Urbina and Mathieu d'Aquin", "title": "Towards Sharing Task Environments to Support Reproducible Evaluations of\n  Interactive Recommender Systems", "comments": "Included in the Offline Evaluation for Recommender Systems Workshop\n  (REVEAL'19), collocated with ACM RecSys 2019. REVEAL'19, September 20th,\n  2019, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond sharing datasets or simulations, we believe the Recommender Systems\n(RS) community should share Task Environments. In this work, we propose a\nhigh-level logical architecture that will help to reason about the core\ncomponents of a RS Task Environment, identify the differences between\nEnvironments, datasets and simulations; and most importantly, understand what\nneeds to be shared about Environments to achieve reproducible experiments. The\nwork presents itself as valuable initial groundwork, open to discussion and\nextensions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:52:30 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:57:55 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Barraza-Urbina", "Andrea", ""], ["d'Aquin", "Mathieu", ""]]}, {"id": "1909.06159", "submitter": "Kamala Balasubramaniam", "authors": "B. Kamala", "title": "BAGH -- Comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a new emerging research trend over the last decade which\nfocuses on analyzing the processes using event log and data. The raising\nintegration of information systems for the operation of business processes\nprovides the basis for innovative data analysis approaches. Process mining has\nthe strong relationship between with data mining so that it enables the bond\nbetween business intelligence approach and business process management. It\nfocuses on end to end processes and is possible because of the growing\navailability of event data and new process discovery and conformance checking\ntechniques. Process mining aims to discover, monitor and improve real processes\nby extracting knowledge from event logs readily available in todays information\nsystems. The discovered process models can be used for a variety of analysis\npurposes. Many companies have adopted Process aware Information Systems for\nsupporting their business processes in some form. These systems typically have\ntheir log events related to the actual business process executions. Proper\nanalysis of Process Aware Information Systems execution logs can yield\nimportant knowledge and help organizations improve the quality of their\nservices. This paper reviews and compares various process mining algorithms\nbased on their input parameters, the techniques used and the output generated\nby them.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:22:17 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Kamala", "B.", ""]]}, {"id": "1909.06162", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta, Khushbu Saxena, Usama Yaseen, Thomas Runkler, Hinrich\n  Sch\\\"utze", "title": "Neural Architectures for Fine-Grained Propaganda Detection in News", "comments": "EMNLP2019: Fine-grained propaganda detection shared task at NLP4IF\n  workshop (EMNLP2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system (MIC-CIS) details and results of\nparticipation in the fine-grained propaganda detection shared task 2019. To\naddress the tasks of sentence (SLC) and fragment level (FLC) propaganda\ndetection, we explore different neural architectures (e.g., CNN, LSTM-CRF and\nBERT) and extract linguistic (e.g., part-of-speech, named entity, readability,\nsentiment, emotion, etc.), layout and topical features. Specifically, we have\ndesigned multi-granularity and multi-tasking neural architectures to jointly\nperform both the sentence and fragment level propaganda detection.\nAdditionally, we investigate different ensemble schemes such as\nmajority-voting, relax-voting, etc. to boost overall system performance.\nCompared to the other participating systems, our submissions are ranked 3rd and\n4th in FLC and SLC tasks, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:11:47 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Gupta", "Pankaj", ""], ["Saxena", "Khushbu", ""], ["Yaseen", "Usama", ""], ["Runkler", "Thomas", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1909.06239", "submitter": "Alison Sneyd", "authors": "Alison Sneyd and Mark Stevenson", "title": "Modelling Stopping Criteria for Search Results using Poisson Processes", "comments": "Accepted to EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text retrieval systems often return large sets of documents, particularly\nwhen applied to large collections. Stopping criteria can reduce the number of\nthese documents that need to be manually evaluated for relevance by predicting\nwhen a suitable level of recall has been achieved. In this work, a novel method\nfor determining a stopping criterion is proposed that models the rate at which\nrelevant documents occur using a Poisson process. This method allows a user to\nspecify both a minimum desired level of recall to achieve and a desired\nprobability of having achieved it. We evaluate our method on a public dataset\nand compare it with previous techniques for determining stopping criteria.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:05:04 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Sneyd", "Alison", ""], ["Stevenson", "Mark", ""]]}, {"id": "1909.06362", "submitter": "Nasim Sonboli", "authors": "Kun Lin, Nasim Sonboli, Bamshad Mobasher, Robin Burke", "title": "Crank up the volume: preference bias amplification in collaborative\n  recommendation", "comments": "Presented at the RMSE workshop held in conjunction with the 13th ACM\n  Conference on Recommender Systems (RecSys), 2019, in Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are personalized: we expect the results given to a\nparticular user to reflect that user's preferences. Some researchers have\nstudied the notion of calibration, how well recommendations match users' stated\npreferences, and bias disparity the extent to which mis-calibration affects\ndifferent user groups. In this paper, we examine bias disparity over a range of\ndifferent algorithms and for different item categories and demonstrate\nsignificant differences between model-based and memory-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 03:23:02 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Lin", "Kun", ""], ["Sonboli", "Nasim", ""], ["Mobasher", "Bamshad", ""], ["Burke", "Robin", ""]]}, {"id": "1909.06429", "submitter": "Rinat Khaziev", "authors": "Rinat Khaziev, Bryce Casavant, Pearce Washabaugh, Amy A. Winecoff, and\n  Matthew Graham", "title": "Recommendation or Discrimination?: Quantifying Distribution Parity in\n  Information Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval (IR) systems often leverage query data to suggest\nrelevant items to users. This introduces the possibility of unfairness if the\nquery (i.e., input) and the resulting recommendations unintentionally correlate\nwith latent factors that are protected variables (e.g., race, gender, and age).\nFor instance, a visual search system for fashion recommendations may pick up on\nfeatures of the human models rather than fashion garments when generating\nrecommendations. In this work, we introduce a statistical test for\n\"distribution parity\" in the top-K IR results, which assesses whether a given\nset of recommendations is fair with respect to a specific protected variable.\nWe evaluate our test using both simulated and empirical results. First, using\nartificially biased recommendations, we demonstrate the trade-off between\nstatistically detectable bias and the size of the search catalog. Second, we\napply our test to a visual search system for fashion garments, specifically\ntesting for recommendation bias based on the skin tone of fashion models. Our\ndistribution parity test can help ensure that IR systems' results are fair and\nproduce a good experience for all users.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:17:19 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Khaziev", "Rinat", ""], ["Casavant", "Bryce", ""], ["Washabaugh", "Pearce", ""], ["Winecoff", "Amy A.", ""], ["Graham", "Matthew", ""]]}, {"id": "1909.06563", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta, Yatin Chaudhary, Hinrich Sch\\\"utze", "title": "Multi-view and Multi-source Transfers in Neural Topic Modeling with\n  Pretrained Topic and Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though word embeddings and topics are complementary representations, several\npast works have only used pre-trained word embeddings in (neural) topic\nmodeling to address data sparsity problem in short text or small collection of\ndocuments. However, no prior work has employed (pre-trained latent) topics in\ntransfer learning paradigm. In this paper, we propose an approach to (1)\nperform knowledge transfer using latent topics obtained from a large source\ncorpus, and (2) jointly transfer knowledge via the two representations (or\nviews) in neural topic modeling to improve topic quality, better deal with\npolysemy and data sparsity issues in a target corpus. In doing so, we first\naccumulate topics and word representations from one or many source corpora to\nbuild a pool of topics and word vectors. Then, we identify one or multiple\nrelevant source domain(s) and take advantage of corresponding topics and word\nfeatures via the respective pools to guide meaningful learning in the sparse\ntarget domain. We quantify the quality of topic and document representations\nvia generalization (perplexity), interpretability (topic coherence) and\ninformation retrieval (IR) using short-text, long-text, small and large\ndocument collections from news and medical domains. We have demonstrated the\nstate-of-the-art results on topic modeling with the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 09:16:05 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:05:34 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Gupta", "Pankaj", ""], ["Chaudhary", "Yatin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1909.06627", "submitter": "Xiaotian Han", "authors": "Chuan Shi, Xiaotian Han, Li Song, Xiao Wang, Senzhang Wang, Junping\n  Du, Philip S. Yu", "title": "Deep Collaborative Filtering with Multi-Aspect Information in\n  Heterogeneous Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recommender systems play a pivotal role in alleviating the problem\nof information overload. Latent factor models have been widely used for\nrecommendation. Most existing latent factor models mainly utilize the\ninteraction information between users and items, although some recently\nextended models utilize some auxiliary information to learn a unified latent\nfactor for users and items. The unified latent factor only represents the\ncharacteristics of users and the properties of items from the aspect of\npurchase history. However, the characteristics of users and the properties of\nitems may stem from different aspects, e.g., the brand-aspect and\ncategory-aspect of items. Moreover, the latent factor models usually use the\nshallow projection, which cannot capture the characteristics of users and items\nwell. In this paper, we propose a Neural network based Aspect-level\nCollaborative Filtering model (NeuACF) to exploit different aspect latent\nfactors. Through modelling the rich object properties and relations in\nrecommender system as a heterogeneous information network, NeuACF first\nextracts different aspect-level similarity matrices of users and items\nrespectively through different meta-paths, and then feeds an elaborately\ndesigned deep neural network with these matrices to learn aspect-level latent\nfactors. Finally, the aspect-level latent factors are fused for the top-N\nrecommendation. Moreover, to fuse information from different aspects more\neffectively, we further propose NeuACF++ to fuse aspect-level latent factors\nwith self-attention mechanism. Extensive experiments on three real world\ndatasets show that NeuACF and NeuACF++ significantly outperform both existing\nlatent factor models and recent neural network models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 16:33:16 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Shi", "Chuan", ""], ["Han", "Xiaotian", ""], ["Song", "Li", ""], ["Wang", "Xiao", ""], ["Wang", "Senzhang", ""], ["Du", "Junping", ""], ["Yu", "Philip S.", ""]]}, {"id": "1909.06667", "submitter": "Hossein A. Rahmani", "authors": "Hossein A. Rahmani, Mohammad Aliannejadi, Sajad Ahmadian, Mitra\n  Baratchi, Mohsen Afsharchi, Fabio Crestani", "title": "LGLMF: Local Geographical based Logistic Matrix Factorization Model for\n  POI Recommendation", "comments": "13 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Location-Based Social Networks, personalized Points\nof Interest (POIs) recommendation has become a critical task to help users\nexplore their surroundings. Due to the scarcity of check-in data, the\navailability of geographical information offers an opportunity to improve the\naccuracy of POI recommendation. Moreover, matrix factorization methods provide\neffective models which can be used in POI recommendation. However, there are\ntwo main challenges which should be addressed to improve the performance of POI\nrecommendation methods. First, leveraging geographical information to capture\nboth the user's personal, geographic profile and a location's geographic\npopularity. Second, incorporating the geographical model into the matrix\nfactorization approaches. To address these problems, a POI recommendation\nmethod is proposed in this paper based on a Local Geographical Model, which\nconsiders both users' and locations' points of view. To this end, an effective\ngeographical model is proposed by considering the user's main region of\nactivity and the relevance of each location within that region. Then, the\nproposed local geographical model is fused into the Logistic Matrix\nFactorization to improve the accuracy of POI recommendation. Experimental\nresults on two well-known datasets demonstrate that the proposed approach\noutperforms other state-of-the-art POI recommendation methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 20:23:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rahmani", "Hossein A.", ""], ["Aliannejadi", "Mohammad", ""], ["Ahmadian", "Sajad", ""], ["Baratchi", "Mitra", ""], ["Afsharchi", "Mohsen", ""], ["Crestani", "Fabio", ""]]}, {"id": "1909.06722", "submitter": "Tian Xia", "authors": "Tian Xia, Shaodan Zhai, Shaojun Wang", "title": "Plackett-Luce model for learning-to-rank task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  List-wise based learning to rank methods are generally supposed to have\nbetter performance than point- and pair-wise based. However, in real-world\napplications, state-of-the-art systems are not from list-wise based camp. In\nthis paper, we propose a new non-linear algorithm in the list-wise based\nframework called ListMLE, which uses the Plackett-Luce (PL) loss. Our\nexperiments are conducted on the two largest publicly available real-world\ndatasets, Yahoo challenge 2010 and Microsoft 30K. This is the first time in the\nsingle model level for a list-wise based system to match or overpass\nstate-of-the-art systems in real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 03:23:49 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Xia", "Tian", ""], ["Zhai", "Shaodan", ""], ["Wang", "Shaojun", ""]]}, {"id": "1909.06859", "submitter": "Shihao Zou", "authors": "Shihao Zou, Zhonghua Li, Mohammad Akbari, Jun Wang, Peng Zhang", "title": "MarlRank: Multi-agent Reinforced Learning to Rank", "comments": "CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating the relevancy between a query and a document, ranking models\nlargely neglect the mutual information among documents. A common wisdom is that\nif two documents are similar in terms of the same query, they are more likely\nto have similar relevance score. To mitigate this problem, in this paper, we\npropose a multi-agent reinforced ranking model, named MarlRank. In particular,\nby considering each document as an agent, we formulate the ranking process as a\nmulti-agent Markov Decision Process (MDP), where the mutual interactions among\ndocuments are incorporated in the ranking process. To compute the ranking list,\neach document predicts its relevance to a query considering not only its own\nquery-document features but also its similar documents features and actions. By\ndefining reward as a function of NDCG, we can optimize our model directly on\nthe ranking performance measure. Our experimental results on two LETOR\nbenchmark datasets show that our model has significant performance gains over\nthe state-of-art baselines. We also find that the NDCG shows an overall\nincreasing trend along with the step of interactions, which demonstrates that\nthe mutual information among documents helps improve the ranking performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 19:08:08 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zou", "Shihao", ""], ["Li", "Zhonghua", ""], ["Akbari", "Mohammad", ""], ["Wang", "Jun", ""], ["Zhang", "Peng", ""]]}, {"id": "1909.07131", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Dimitrios Rafailidis and Fabio Crestani", "title": "A Joint Two-Phase Time-Sensitive Regularized Collaborative Ranking Model\n  for Point of Interest Recommendation", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of location-based social networks (LBSNs) has led to a\ntremendous amount of user check-in data. Recommending points of interest (POIs)\nplays a key role in satisfying users' needs in LBSNs. While recent work has\nexplored the idea of adopting collaborative ranking (CR) for recommendation,\nthere have been few attempts to incorporate temporal information for POI\nrecommendation using CR. In this article, we propose a two-phase CR algorithm\nthat incorporates the geographical influence of POIs and is regularized based\non the variance of POIs popularity and users' activities over time. The\ntime-sensitive regularizer penalizes user and POIs that have been more\ntime-sensitive in the past, helping the model to account for their long-term\nbehavioral patterns while learning from user-POI interactions. Moreover, in the\nfirst phase, it attempts to rank visited POIs higher than the unvisited ones,\nand at the same time, apply the geographical influence. In the second phase,\nour algorithm tries to rank users' favorite POIs higher on the recommendation\nlist. Both phases employ a collaborative learning strategy that enables the\nmodel to capture complex latent associations from two different perspectives.\nExperiments on real-world datasets show that our proposed time-sensitive\ncollaborative ranking model beats state-of-the-art POI recommendation methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 11:33:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Rafailidis", "Dimitrios", ""], ["Crestani", "Fabio", ""]]}, {"id": "1909.07151", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Asmit Kumar Singh, Arun Balaji Buduru and Ponnurangam\n  Kumaraguru", "title": "Hashtags are (not) judgemental: The untold story of Lok Sabha elections\n  2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtags in online social media have become a way for users to build\ncommunities around topics, promote opinions, and categorize messages. In the\npolitical context, hashtags on Twitter are used by users to campaign for their\nparties, spread news, or to get followers and get a general idea by following a\ndiscussion built around a hashtag. In the past, researchers have studied\ncertain types and specific properties of hashtags by utilizing a lot of data\ncollected around hashtags. In this paper, we perform a large-scale empirical\nanalysis of elections using only the hashtags shared on Twitter during the 2019\nLok Sabha elections in India. We study the trends and events unfolded on the\nground, the latent topics to uncover representative hashtags and semantic\nsimilarity to relate hashtags with the election outcomes. We collect over 24\nmillion hashtags to perform extensive experiments. First, we find the trending\nhashtags to cross-reference them with the tweets in our dataset to list down\nnotable events. Second, we use Latent Dirichlet Allocation to find topic\npatterns in the dataset. In the end, we use skip-gram word embedding model to\nfind semantically similar hashtags. We propose popularity and an influence\nmetric to predict election outcomes using just the hashtags. Empirical results\nshow that influence is a good measure to predict the election outcome.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 12:30:16 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:55:30 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Gupta", "Saurabh", ""], ["Singh", "Asmit Kumar", ""], ["Buduru", "Arun Balaji", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "1909.07181", "submitter": "Praboda Rajapaksha", "authors": "Praboda Rajapaksha, Reza Farahbakhsh, Noel Crespi, Bruno Defude", "title": "Uncovering Flaming Events on News Media in Social Media", "comments": "This paper has been accepted in 38th IEEE International Performance\n  Computing and Communications Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networking sites (SNSs) facilitate the sharing of ideas and\ninformation through different types of feedback including publishing posts,\nleaving comments and other type of reactions. However, some comments or\nfeedback on SNSs are inconsiderate and offensive, and sometimes this type of\nfeedback has a very negative effect on a target user. The phenomenon known as\nflaming goes hand-in-hand with this type of posting that can trigger almost\ninstantly on SNSs. Most popular users such as celebrities, politicians and news\nmedia are the major victims of the flaming behaviors and so detecting these\ntypes of events will be useful and appreciated. Flaming event can be monitored\nand identified by analyzing negative comments received on a post. Thus, our\nmain objective of this study is to identify a way to detect flaming events in\nSNS using a sentiment prediction method. We use a deep Neural Network (NN)\nmodel that can identity sentiments of variable length sentences and classifies\nthe sentiment of SNSs content (both comments and posts) to discover flaming\nevents. Our deep NN model uses Word2Vec and FastText word embedding methods as\nits training to explore which method is the most appropriate. The labeled\ndataset for training the deep NN is generated using an enhanced lexicon based\napproach. Our deep NN model classifies the sentiment of a sentence into five\nclasses: Very Positive, Positive, Neutral, Negative and Very Negative. To\ndetect flaming incidents, we focus only on the comments classified into the\nNegative and Very Negative classes. As a use-case, we try to explore the\nflaming phenomena in the news media domain and therefore we focused on news\nitems posted by three popular news media on Facebook (BBCNews, CNN and FoxNews)\nto train and test the model.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:19:07 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rajapaksha", "Praboda", ""], ["Farahbakhsh", "Reza", ""], ["Crespi", "Noel", ""], ["Defude", "Bruno", ""]]}, {"id": "1909.07212", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Yongfeng Zhang, Keping Bi, W. Bruce Croft", "title": "Explainable Product Search with a Dynamic Relation Embedding Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search is one of the most popular methods for customers to discover\nproducts online. Most existing studies on product search focus on developing\neffective retrieval models that rank items by their likelihood to be purchased.\nThey, however, ignore the problem that there is a gap between how systems and\ncustomers perceive the relevance of items. Without explanations, users may not\nunderstand why product search engines retrieve certain items for them, which\nconsequentially leads to imperfect user experience and suboptimal system\nperformance in practice. In this work, we tackle this problem by constructing\nexplainable retrieval models for product search. Specifically, we propose to\nmodel the \"search and purchase\" behavior as a dynamic relation between users\nand items, and create a dynamic knowledge graph based on both the\nmulti-relational product data and the context of the search session. Ranking is\nconducted based on the relationship between users and items in the latent\nspace, and explanations are generated with logic inferences and entity soft\nmatching on the knowledge graph. Empirical experiments show that our model,\nwhich we refer to as the Dynamic Relation Embedding Model (DREM), significantly\noutperforms the state-of-the-art baselines and has the ability to produce\nreasonable explanations for search results.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:06:11 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ai", "Qingyao", ""], ["Zhang", "Yongfeng", ""], ["Bi", "Keping", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1909.07368", "submitter": "Madjid Khalilian", "authors": "Madjid Khalilian, Shiva Hassanzadeh", "title": "Document classification methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on different fields which are collected by users requires\nappropriate management and organization to be structured in a standard way and\nretrieved fast and more easily. Document classification is a conventional\nmethod to separate text based on their subjects among scientific text, web\npages and digital library. Different methods and techniques are proposed for\ndocument classifications that have advantages and deficiencies. In this paper,\nseveral unsupervised and supervised document classification methods are studied\nand compared.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:42:57 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Khalilian", "Madjid", ""], ["Hassanzadeh", "Shiva", ""]]}, {"id": "1909.07683", "submitter": "Palakorn Achananuparp", "authors": "Yue Liu, Helena Lee, Palakorn Achananuparp, Ee-Peng Lim, Tzu-Ling\n  Cheng, Shou-De Lin", "title": "Characterizing and Predicting Repeat Food Consumption Behavior for\n  Just-in-Time Interventions", "comments": "To appear in the Proceedings of Digital Public Health 2019", "journal-ref": null, "doi": "10.1145/3357729.3357736", "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are creatures of habit. In their daily life, people tend to\nrepeatedly consume similar types of food items over several days and\noccasionally switch to consuming different types of items when the consumptions\nbecome overly monotonous. However, the novel and repeat consumption behaviors\nhave not been studied in food recommendation research. More importantly, the\nability to predict daily eating habits of individuals is crucial to improve the\neffectiveness of food recommender systems in facilitating healthy lifestyle\nchange. In this study, we analyze the patterns of repeat food consumptions\nusing large-scale consumption data from a popular online fitness community\ncalled MyFitnessPal (MFP), conduct an offline evaluation of various\nstate-of-the-art algorithms in predicting the next-day food consumption, and\nanalyze their performance across different demographic groups and contexts. The\nexperiment results show that algorithms incorporating the\nexploration-and-exploitation and temporal dynamics are more effective in the\nnext-day recommendation task than most state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:51:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Yue", ""], ["Lee", "Helena", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-Peng", ""], ["Cheng", "Tzu-Ling", ""], ["Lin", "Shou-De", ""]]}, {"id": "1909.07705", "submitter": "Zaiqiao Meng", "authors": "Zaiqiao Meng, Richard McCreadie, Craig Macdonald and Iadh Ounis", "title": "Variational Bayesian Context-aware Representation for Grocery\n  Recommendation", "comments": "Accepted for CARS 2.0 - Context-Aware Recommender Systems Workshop @\n  RecSys'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grocery recommendation is an important recommendation use-case, which aims to\npredict which items a user might choose to buy in the future, based on their\nshopping history. However, existing methods only represent each user and item\nby single deterministic points in a low-dimensional continuous space. In\naddition, most of these methods are trained by maximizing the co-occurrence\nlikelihood with a simple Skip-gram-based formulation, which limits the\nexpressive ability of their embeddings and the resulting recommendation\nperformance. In this paper, we propose the Variational Bayesian Context-Aware\nRepresentation (VBCAR) model for grocery recommendation, which is a novel\nvariational Bayesian model that learns the user and item latent vectors by\nleveraging basket context information from past user-item interactions. We\ntrain our VBCAR model based on the Bayesian Skip-gram framework coupled with\nthe amortized variational inference so that it can learn more expressive latent\nrepresentations that integrate both the non-linearity and Bayesian behaviour.\nExperiments conducted on a large real-world grocery recommendation dataset show\nthat our proposed VBCAR model can significantly outperform existing\nstate-of-the-art grocery recommendation methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 10:38:19 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 13:07:07 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Meng", "Zaiqiao", ""], ["McCreadie", "Richard", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1909.07746", "submitter": "Shobhit Jain", "authors": "Shobhit Jain, Sravan Babu Bodapati, Ramesh Nallapati, Anima Anandkumar", "title": "Multi Sense Embeddings from Topic Models", "comments": "Accepted at ACL supported conference for Natural Language & Speech\n  Processing. https://www.aclweb.org/anthology/W19-74, Year: 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed word embeddings have yielded state-of-the-art performance in many\nNLP tasks, mainly due to their success in capturing useful semantic\ninformation. These representations assign only a single vector to each word\nwhereas a large number of words are polysemous (i.e., have multiple meanings).\nIn this work, we approach this critical problem in lexical semantics, namely\nthat of representing various senses of polysemous words in vector spaces. We\npropose a topic modeling based skip-gram approach for learning multi-prototype\nword embeddings. We also introduce a method to prune the embeddings determined\nby the probabilistic representation of the word in each topic. We use our\nembeddings to show that they can capture the context and word similarity\nstrongly and outperform various state-of-the-art implementations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:23:33 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 17:14:17 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Jain", "Shobhit", ""], ["Bodapati", "Sravan Babu", ""], ["Nallapati", "Ramesh", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1909.07873", "submitter": "Prashanth Vijayaraghavan", "authors": "Prashanth Vijayaraghavan, Deb Roy", "title": "Generating Black-Box Adversarial Examples for Text Classifiers Using a\n  Deep Reinforced Model", "comments": "16 pages, 3 figures, ECML PKDD 2019", "journal-ref": "Joint European Conference on Machine Learning and Knowledge\n  Discovery in Databases. Springer, Cham, 2019", "doi": "10.1007/978-3-030-46147-8_43", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, generating adversarial examples has become an important means of\nmeasuring robustness of a deep learning model. Adversarial examples help us\nidentify the susceptibilities of the model and further counter those\nvulnerabilities by applying adversarial training techniques. In natural\nlanguage domain, small perturbations in the form of misspellings or paraphrases\ncan drastically change the semantics of the text. We propose a reinforcement\nlearning based approach towards generating adversarial examples in black-box\nsettings. We demonstrate that our method is able to fool well-trained models\nfor (a) IMDB sentiment classification task and (b) AG's news corpus news\ncategorization task with significantly high success rates. We find that the\nadversarial examples generated are semantics-preserving perturbations to the\noriginal text.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:05:31 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "1909.07899", "submitter": "Taivanbat Badamdorj", "authors": "Taivanbat Badamdorj, Adiel Ben-Shalom, Nachum Dershowitz, Lior Wolf", "title": "Fast Search with Poor OCR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The indexing and searching of historical documents have garnered attention in\nrecent years due to massive digitization efforts of important collections\nworldwide. Pure textual search in these corpora is a problem since optical\ncharacter recognition (OCR) is infamous for performing poorly on such\nhistorical material, which often suffer from poor preservation. We propose a\nnovel text-based method for searching through noisy text. Our system represents\nwords as vectors, projects queries and candidates obtained from the OCR into a\ncommon space, and ranks the candidates using a metric suited to\nnearest-neighbor search. We demonstrate the practicality of our method on\ntypewritten German documents from the WWII era.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:40:38 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 16:09:59 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 19:59:51 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Badamdorj", "Taivanbat", ""], ["Ben-Shalom", "Adiel", ""], ["Dershowitz", "Nachum", ""], ["Wolf", "Lior", ""]]}, {"id": "1909.08041", "submitter": "Yixin Nie", "authors": "Yixin Nie, Songhe Wang, Mohit Bansal", "title": "Revealing the Importance of Semantic Retrieval for Machine Reading at\n  Scale", "comments": "14 pages (EMNLP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Reading at Scale (MRS) is a challenging task in which a system is\ngiven an input query and is asked to produce a precise output by \"reading\"\ninformation from a large knowledge base. The task has gained popularity with\nits natural combination of information retrieval (IR) and machine comprehension\n(MC). Advancements in representation learning have led to separated progress in\nboth IR and MC; however, very few studies have examined the relationship and\ncombined design of retrieval and comprehension at different levels of\ngranularity, for development of MRS systems. In this work, we give general\nguidelines on system design for MRS by proposing a simple yet effective\npipeline system with special consideration on hierarchical semantic retrieval\nat both paragraph and sentence level, and their potential effects on the\ndownstream task. The system is evaluated on both fact verification and\nopen-domain multihop QA, achieving state-of-the-art results on the leaderboard\ntest sets of both FEVER and HOTPOTQA. To further demonstrate the importance of\nsemantic retrieval, we present ablation and analysis studies to quantify the\ncontribution of neural retrieval modules at both paragraph-level and\nsentence-level, and illustrate that intermediate semantic retrieval modules are\nvital for not only effectively filtering upstream information and thus saving\ndownstream computation, but also for shaping upstream data distribution and\nproviding better data for downstream modeling. Code/data made publicly\navailable at: https://github.com/easonnie/semanticRetrievalMRS\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 19:21:11 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nie", "Yixin", ""], ["Wang", "Songhe", ""], ["Bansal", "Mohit", ""]]}, {"id": "1909.08203", "submitter": "Yuhong Guo", "authors": "Yuan Wu, Yuhong Guo", "title": "Dual Adversarial Co-Learning for Multi-Domain Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel dual adversarial co-learning approach for\nmulti-domain text classification (MDTC). The approach learns shared-private\nnetworks for feature extraction and deploys dual adversarial regularizations to\nalign features across different domains and between labeled and unlabeled data\nsimultaneously under a discrepancy based co-learning framework, aiming to\nimprove the classifiers' generalization capacity with the learned features. We\nconduct experiments on multi-domain sentiment classification datasets. The\nresults show the proposed approach achieves the state-of-the-art MDTC\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 04:15:43 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wu", "Yuan", ""], ["Guo", "Yuhong", ""]]}, {"id": "1909.08402", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Peter Bourgonje, Maria Berger, Julian\n  Moreno-Schneider, Georg Rehm, Bela Gipp", "title": "Enriching BERT with Knowledge Graph Embeddings for Document\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on the classification of books using short\ndescriptive texts (cover blurbs) and additional metadata. Building upon BERT, a\ndeep neural language model, we demonstrate how to combine text representations\nwith metadata and knowledge graph embeddings, which encode author information.\nCompared to the standard BERT approach we achieve considerably better results\nfor the classification task. For a more coarse-grained classification using\neight labels we achieve an F1- score of 87.20, while a detailed classification\nusing 343 labels yields an F1-score of 64.70. We make the source code and\ntrained models of our experiments publicly available\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 12:40:39 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ostendorff", "Malte", ""], ["Bourgonje", "Peter", ""], ["Berger", "Maria", ""], ["Moreno-Schneider", "Julian", ""], ["Rehm", "Georg", ""], ["Gipp", "Bela", ""]]}, {"id": "1909.08471", "submitter": "Olivier Jeunen", "authors": "Olivier Jeunen, Dmytro Mykhaylov, David Rohde, Flavian Vasile,\n  Alexandre Gilotte, Martin Bompaire", "title": "Learning from Bandit Feedback: An Overview of the State-of-the-art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning we often try to optimise a decision rule that would have\nworked well over a historical dataset; this is the so called empirical risk\nminimisation principle. In the context of learning from recommender system\nlogs, applying this principle becomes a problem because we do not have\navailable the reward of decisions we did not do. In order to handle this\n\"bandit-feedback\" setting, several Counterfactual Risk Minimisation (CRM)\nmethods have been proposed in recent years, that attempt to estimate the\nperformance of different policies on historical data. Through importance\nsampling and various variance reduction techniques, these methods allow more\nrobust learning and inference than classical approaches. It is difficult to\naccurately estimate the performance of policies that frequently perform actions\nthat were infrequently done in the past and a number of different types of\nestimators have been proposed.\n  In this paper, we review several methods, based on different off-policy\nestimators, for learning from bandit feedback. We discuss key differences and\ncommonalities among existing approaches, and compare their empirical\nperformance on the RecoGym simulation environment. To the best of our\nknowledge, this work is the first comparison study for bandit algorithms in a\nrecommender system setting.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:26:28 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Jeunen", "Olivier", ""], ["Mykhaylov", "Dmytro", ""], ["Rohde", "David", ""], ["Vasile", "Flavian", ""], ["Gilotte", "Alexandre", ""], ["Bompaire", "Martin", ""]]}, {"id": "1909.08737", "submitter": "Nan Wang", "authors": "Nan Wang, Hongning Wang", "title": "BPMR: Bayesian Probabilistic Multivariate Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-aspect user preferences are attracting wider attention in recommender\nsystems, as they enable more detailed understanding of users' evaluations of\nitems. Previous studies show that incorporating multi-aspect preferences can\ngreatly improve the performance and explainability of recommendation. However,\nas recommendation is essentially a ranking problem, there is no principled\nsolution for ranking multiple aspects collectively to enhance the\nrecommendation.\n  In this work, we derive a multi-aspect ranking criterion. To maintain the\ndependency among different aspects, we propose to use a vectorized\nrepresentation of multi-aspect ratings and develop a probabilistic multivariate\ntensor factorization framework (PMTF). The framework naturally leads to a\nprobabilistic multi-aspect ranking criterion, which generalizes the\nsingle-aspect ranking to a multivariate fashion. Experiment results on a large\nmulti-aspect review rating dataset confirmed the effectiveness of our solution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:40:59 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Wang", "Nan", ""], ["Wang", "Hongning", ""]]}, {"id": "1909.08752", "submitter": "Sanghwan Bae", "authors": "Sanghwan Bae, Taeuk Kim, Jihoon Kim, Sang-goo Lee", "title": "Summary Level Training of Sentence Rewriting for Abstractive\n  Summarization", "comments": "EMNLP 2019 Workshop on New Frontiers in Summarization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an attempt to combine extractive and abstractive summarization, Sentence\nRewriting models adopt the strategy of extracting salient sentences from a\ndocument first and then paraphrasing the selected ones to generate a summary.\nHowever, the existing models in this framework mostly rely on sentence-level\nrewards or suboptimal labels, causing a mismatch between a training objective\nand evaluation metric. In this paper, we present a novel training signal that\ndirectly maximizes summary-level ROUGE scores through reinforcement learning.\nIn addition, we incorporate BERT into our model, making good use of its ability\non natural language understanding. In extensive experiments, we show that a\ncombination of our proposed model and training procedure obtains new\nstate-of-the-art performance on both CNN/Daily Mail and New York Times\ndatasets. We also demonstrate that it generalizes better on DUC-2002 test set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 00:47:13 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 09:20:10 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2019 07:07:03 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Bae", "Sanghwan", ""], ["Kim", "Taeuk", ""], ["Kim", "Jihoon", ""], ["Lee", "Sang-goo", ""]]}, {"id": "1909.08756", "submitter": "Benoit Soubeyran", "authors": "Benoit Soubeyran", "title": "Biblioth\\`eque de la communaut\\'e assomptionniste : saisie informatique\n  et classement Dewey", "comments": "24 pages, in French, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Library of Saint Peter in Gallicantu has had an eventful history and\ndifferent phases of classification. It was constituted by the contribution of\nvarious private libraries of Religious of the Holy Land. Its history is\nintimately linked to the Assumptionist presence in Jerusalem. The\ncomputerization work carried out from 2018 onwards made it possible to clarify\nthe classification framework based on Dewey's decimal classification and to use\nthe databases - Wikidata, VIAF - to improve the BNF catalogue.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:53:53 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Soubeyran", "Benoit", ""]]}, {"id": "1909.08855", "submitter": "Pratyay Banerjee", "authors": "Arindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal, Swaroop Mishra and\n  Chitta Baral", "title": "How Additional Knowledge can Improve Natural Language Commonsense\n  Question Answering?", "comments": "14 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently several datasets have been proposed to encourage research in\nQuestion Answering domains where commonsense knowledge is expected to play an\nimportant role. Recent language models such as ROBERTA, BERT and GPT that have\nbeen pre-trained on Wikipedia articles and books have shown reasonable\nperformance with little fine-tuning on several such Multiple Choice\nQuestion-Answering (MCQ) datasets. Our goal in this work is to develop methods\nto incorporate additional (commonsense) knowledge into language model-based\napproaches for better question-answering in such domains. In this work, we\nfirst categorize external knowledge sources, and show performance does improve\non using such sources. We then explore three different strategies for knowledge\nincorporation and four different models for question-answering using external\ncommonsense knowledge. We analyze our predictions to explore the scope of\nfurther improvements.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:25:47 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 03:14:58 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:26:45 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Mitra", "Arindam", ""], ["Banerjee", "Pratyay", ""], ["Pal", "Kuntal Kumar", ""], ["Mishra", "Swaroop", ""], ["Baral", "Chitta", ""]]}, {"id": "1909.08863", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee", "title": "ASU at TextGraphs 2019 Shared Task: Explanation ReGeneration using\n  Language Models and Iterative Re-Ranking", "comments": "6+1 pages, 1 figures, Shared Task on Proceedings of the Thirteenth\n  Workshop on Graph-Based Methods for Natural Language Processing\n  (TextGraphs-13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we describe the system from Natural Language Processing group at\nArizona State University for the TextGraphs 2019 Shared Task. The task focuses\non Explanation Regeneration, an intermediate step towards general multi-hop\ninference on large graphs. Our approach consists of modeling the explanation\nregeneration task as a \\textit{learning to rank} problem, for which we use\nstate-of-the-art language models and explore dataset preparation techniques. We\nutilize an iterative re-ranking based approach to further improve the rankings.\nOur system secured 2nd rank in the task with a mean average precision (MAP) of\n41.3\\% on the test set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:47:16 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Banerjee", "Pratyay", ""]]}, {"id": "1909.08876", "submitter": "Daniel Hienert", "authors": "Dagmar Kern and Daniel Hienert", "title": "Understanding the Information needs of Social Scientists in Germany", "comments": null, "journal-ref": "In Proceedings of Association for Information Science and\n  Technology (ASIS&T 2018), 234-243", "doi": "10.1002/pra2.2018.14505501026", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information needs of social science researchers are manifold and almost\nstudied in every decade since the 1950s. With this paper, we contribute to this\nseries and present the results of three studies. We asked 367 social science\nresearchers in Germany for their information needs and identified needs in\ndifferent categories: literature, research data, measurement instruments,\nsupport for data analysis, support for data collection, variables in research\ndata, software support, networking/cooperation, and illustrative material.\nThereby, the search for literature and research data is still the main\ninformation need with more than three-quarter of our participants expressing\nneeds in these categories. With comprehensive lists of altogether 154 concrete\ninformation needs, even those that are only expressed by one participant, we\ncontribute to the holistic understanding of the information needs of social\nscience researchers of today.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 09:10:59 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Kern", "Dagmar", ""], ["Hienert", "Daniel", ""]]}, {"id": "1909.09420", "submitter": "Konstantin Schall", "authors": "Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung", "title": "Deep Aggregation of Regional Convolutional Activations for Content Based\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of deep learning based image retrieval remains in\naggregating convolutional activations into one highly representative feature\nvector. Ideally, this descriptor should encode semantic, spatial and low level\ninformation. Even though off-the-shelf pre-trained neural networks can already\nproduce good representations in combination with aggregation methods,\nappropriate fine tuning for the task of image retrieval has shown to\nsignificantly boost retrieval performance. In this paper, we present a simple\nyet effective supervised aggregation method built on top of existing regional\npooling approaches. In addition to the maximum activation of a given region, we\ncalculate regional average activations of extracted feature maps. Subsequently,\nweights for each of the pooled feature vectors are learned to perform a\nweighted aggregation to a single feature vector. Furthermore, we apply our\nnewly proposed NRA loss function for deep metric learning to fine tune the\nbackbone neural network and to learn the aggregation weights. Our method\nachieves state-of-the-art results for the INRIA Holidays data set and\ncompetitive results for the Oxford Buildings and Paris data sets while reducing\nthe training time significantly.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:43:00 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 06:53:16 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Schall", "Konstantin", ""], ["Barthel", "Kai Uwe", ""], ["Hezel", "Nico", ""], ["Jung", "Klaus", ""]]}, {"id": "1909.09427", "submitter": "Konstantin Schall", "authors": "Konstantin Schall, Kai Uwe Barthel, Nico Hezel, Klaus Jung", "title": "Deep Metric Learning using Similarities from Nonlinear Rank\n  Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep metric learning has achieved promising results in\nlearning high dimensional semantic feature embeddings where the spatial\nrelationships of the feature vectors match the visual similarities of the\nimages. Similarity search for images is performed by determining the vectors\nwith the smallest distances to a query vector. However, high retrieval quality\ndoes not depend on the actual distances of the feature vectors, but rather on\nthe ranking order of the feature vectors from similar images. In this paper, we\nintroduce a metric learning algorithm that focuses on identifying and modifying\nthose feature vectors that most strongly affect the retrieval quality. We\ncompute normalized approximated ranks and convert them to similarities by\napplying a nonlinear transfer function. These similarities are used in a newly\nproposed loss function that better contracts similar and disperses dissimilar\nsamples. Experiments demonstrate significant improvement over existing deep\nfeature embedding methods on the CUB-200-2011, Cars196, and Stanford Online\nProducts data sets for all embedding sizes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:07:15 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 06:49:42 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Schall", "Konstantin", ""], ["Barthel", "Kai Uwe", ""], ["Hezel", "Nico", ""], ["Jung", "Klaus", ""]]}, {"id": "1909.09436", "submitter": "Miltiadis Allamanis", "authors": "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, Marc\n  Brockschmidt", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "comments": "Updated evaluation numbers after fixing indexing bug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic code search is the task of retrieving relevant code given a natural\nlanguage query. While related to other information retrieval tasks, it requires\nbridging the gap between the language used in code (often abbreviated and\nhighly technical) and natural language more suitable to describe vague concepts\nand ideas.\n  To enable evaluation of progress on code search, we are releasing the\nCodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which\nconsists of 99 natural language queries with about 4k expert relevance\nannotations of likely results from CodeSearchNet Corpus. The corpus contains\nabout 6 million functions from open-source code spanning six programming\nlanguages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet\nCorpus also contains automatically generated query-like natural language for 2\nmillion functions, obtained from mechanically scraping and preprocessing\nassociated function documentation. In this article, we describe the methodology\nused to obtain the corpus and expert labels, as well as a number of simple\nbaseline solutions for the task.\n  We hope that CodeSearchNet Challenge encourages researchers and practitioners\nto study this interesting task further and will host a competition and\nleaderboard to track the progress on the challenge. We are also keen on\nextending CodeSearchNet Challenge to more queries and programming languages in\nthe future.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:52:45 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 09:21:21 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 09:09:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Husain", "Hamel", ""], ["Wu", "Ho-Hsiang", ""], ["Gazit", "Tiferet", ""], ["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""]]}, {"id": "1909.09551", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Mahdi Rabbani, SeyedValyAllah Ayobi", "title": "Natural Language Processing via LDA Topic Model in Recommendation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, Internet is one of the widest available media worldwide.\nRecommendation systems are increasingly being used in various applications such\nas movie recommendation, mobile recommendation, article recommendation and etc.\nCollaborative Filtering (CF) and Content-Based (CB) are Well-known techniques\nfor building recommendation systems. Topic modeling based on LDA, is a powerful\ntechnique for semantic mining and perform topic extraction. In the past few\nyears, many articles have been published based on LDA technique for building\nrecommendation systems. In this paper, we present taxonomy of recommendation\nsystems and applications based on LDA. In addition, we utilize LDA and Gibbs\nsampling algorithms to evaluate ISWC and WWW conference publications in\ncomputer science. Our study suggest that the recommendation systems based on\nLDA could be effective in building smart recommendation system in online\ncommunities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:08:51 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Rabbani", "Mahdi", ""], ["Ayobi", "SeyedValyAllah", ""]]}, {"id": "1909.09565", "submitter": "Bortik Bandyopadhyay", "authors": "Bortik Bandyopadhyay, Xiang Deng, Goonmeet Bajaj, Huan Sun, Srinivasan\n  Parthasarathy", "title": "Automatic Table completion using Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table is a popular data format to organize and present relational\ninformation. Users often have to manually compose tables when gathering their\ndesiderate information (e.g., entities and their attributes) for decision\nmaking. In this work, we propose to resolve a new type of heterogeneous query\nviz: tabular query, which contains a natural language query description, column\nnames of the desired table, and an example row. We aim to acquire more entity\ntuples (rows) and automatically fill the table specified by the tabular query.\nWe design a novel framework AutoTableComplete which aims to integrate schema\nspecific structural information with the natural language contextual\ninformation provided by the user, to complete tables automatically, using a\nheterogeneous knowledge base (KB) as the main information source. Given a\ntabular query as input, our framework first constructs a set of candidate\nchains that connect the given example entities in KB. We learn to select the\nbest matching chain from these candidates using the semantic context from\ntabular query. The selected chain is then converted into a SPARQL query,\nexecuted against KB to gather a set of candidate rows, that are then ranked in\norder of their relevance to the tabular query, to complete the desired table.\nWe construct a new dataset based on tables in Wikipedia pages and Freebase,\nusing which we perform a wide range of experiments to demonstrate the\neffectiveness of AutoTableComplete as well as present a detailed error analysis\nof our method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:39:55 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Bandyopadhyay", "Bortik", ""], ["Deng", "Xiang", ""], ["Bajaj", "Goonmeet", ""], ["Sun", "Huan", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1909.09922", "submitter": "Chan Hee Song", "authors": "Arijit Sehanobish, Chan Hee Song", "title": "Using Chinese Glyphs for Named Entity Recognition", "comments": "Extended abstract accepted to AAAI-2020, student track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Named Entity Recognition (NER) systems use additional features like\npart-of-speech (POS) tags, shallow parsing, gazetteers, etc. Such kind of\ninformation requires external knowledge like unlabeled texts and trained\ntaggers. Adding these features to NER systems have been shown to have a\npositive impact. However, sometimes creating gazetteers or taggers can take a\nlot of time and may require extensive data cleaning. In this paper for Chinese\nNER systems, we do not use these traditional features but we use lexicographic\nfeatures of Chinese characters. Chinese characters are composed of graphical\ncomponents called radicals and these components often have some semantic\nindicators. We propose CNN based models that incorporate this semantic\ninformation and use them for NER. Our models show an improvement over the\nbaseline BERT-BiLSTM-CRF model. We set a new baseline score for Chinese\nOntoNotes v5.0 and show an improvement of +.64 F1 score. We present a\nstate-of-the-art F1 score on Weibo dataset of 71.81 and show a competitive\nimprovement of +0.72 over baseline on ResumeNER dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:12:18 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 03:41:14 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sehanobish", "Arijit", ""], ["Song", "Chan Hee", ""]]}, {"id": "1909.10266", "submitter": "Felix Hamborg", "authors": "Felix Hamborg, Philipp Meschenmoser, Moritz Schubotz, Bela Gipp", "title": "NewsDeps: Visualizing the Origin of Information in News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In scientific publications, citations allow readers to assess the\nauthenticity of the presented information and verify it in the original\ncontext. News articles, however, do not contain citations and only rarely refer\nreaders to further sources. Readers often cannot assess the authenticity of the\npresented information as its origin is unclear. We present NewsDeps, the first\napproach that analyzes and visualizes where information in news articles stems\nfrom. NewsDeps employs methods from natural language processing and plagiarism\ndetection to measure article similarity. We devise a temporal-force-directed\ngraph that places articles as nodes chronologically. The graph connects\narticles by edges varying in width depending on the articles' similarity. We\ndemonstrate our approach in a case study with two real-world scenarios. We find\nthat NewsDeps increases efficiency and transparency in news consumption by\nrevealing which previously published articles are the primary sources of each\ngiven article.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 10:25:24 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Hamborg", "Felix", ""], ["Meschenmoser", "Philipp", ""], ["Schubotz", "Moritz", ""], ["Gipp", "Bela", ""]]}, {"id": "1909.10506", "submitter": "Daniel Gillick", "authors": "Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta,\n  Jason Baldridge, Eugene Ie, Diego Garcia-Olano", "title": "Learning Dense Representations for Entity Retrieval", "comments": "CoNLL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that it is feasible to perform entity linking by training a dual\nencoder (two-tower) model that encodes mentions and entities in the same dense\nvector space, where candidate entities are retrieved by approximate nearest\nneighbor search. Unlike prior work, this setup does not rely on an alias table\nfollowed by a re-ranker, and is thus the first fully learned entity retrieval\nmodel. We show that our dual encoder, trained using only anchor-text links in\nWikipedia, outperforms discrete alias table and BM25 baselines, and is\ncompetitive with the best comparable results on the standard TACKBP-2010\ndataset. In addition, it can retrieve candidates extremely fast, and\ngeneralizes well to a new dataset derived from Wikinews. On the modeling side,\nwe demonstrate the dramatic value of an unsupervised negative mining algorithm\nfor this task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 17:52:34 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Gillick", "Daniel", ""], ["Kulkarni", "Sayali", ""], ["Lansing", "Larry", ""], ["Presta", "Alessandro", ""], ["Baldridge", "Jason", ""], ["Ie", "Eugene", ""], ["Garcia-Olano", "Diego", ""]]}, {"id": "1909.10649", "submitter": "F\\'abio Souza", "authors": "F\\'abio Souza, Rodrigo Nogueira, Roberto Lotufo", "title": "Portuguese Named Entity Recognition using BERT-CRF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in language representation using neural networks have made it\nviable to transfer the learned internal states of a trained model to downstream\nnatural language processing tasks, such as named entity recognition (NER) and\nquestion answering. It has been shown that the leverage of pre-trained language\nmodels improves the overall performance on many tasks and is highly beneficial\nwhen labeled data is scarce. In this work, we train Portuguese BERT models and\nemploy a BERT-CRF architecture to the NER task on the Portuguese language,\ncombining the transfer capabilities of BERT with the structured predictions of\nCRF. We explore feature-based and fine-tuning training strategies for the BERT\nmodel. Our fine-tuning approach obtains new state-of-the-art results on the\nHAREM I dataset, improving the F1-score by 1 point on the selective scenario (5\nNE classes) and by 4 points on the total scenario (10 NE classes).\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:21:42 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 15:06:49 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Souza", "F\u00e1bio", ""], ["Nogueira", "Rodrigo", ""], ["Lotufo", "Roberto", ""]]}, {"id": "1909.10666", "submitter": "Yiming Cui", "authors": "Wentao Ma, Yiming Cui, Nan Shao, Su He, Wei-Nan Zhang, Ting Liu,\n  Shijin Wang, Guoping Hu", "title": "TripleNet: Triple Attention Network for Multi-Turn Response Selection in\n  Retrieval-based Chatbots", "comments": "10 pages, accepted as a conference paper at CoNLL 2019", "journal-ref": "CoNLL 2019 737-746", "doi": "10.18653/v1/K19-1069", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the importance of different utterances in the context for\nselecting the response usually depends on the current query. In this paper, we\npropose the model TripleNet to fully model the task with the triple <context,\nquery, response> instead of <context, response> in previous works. The heart of\nTripleNet is a novel attention mechanism named triple attention to model the\nrelationships within the triple at four levels. The new mechanism updates the\nrepresentation for each element based on the attention with the other two\nconcurrently and symmetrically. We match the triple <C, Q, R> centered on the\nresponse from char to context level for prediction. Experimental results on two\nlarge-scale multi-turn response selection datasets show that the proposed model\ncan significantly outperform the state-of-the-art methods. TripleNet source\ncode is available at https://github.com/wtma/TripleNet\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 00:45:32 GMT"}, {"version": "v2", "created": "Sun, 29 Sep 2019 06:31:38 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ma", "Wentao", ""], ["Cui", "Yiming", ""], ["Shao", "Nan", ""], ["He", "Su", ""], ["Zhang", "Wei-Nan", ""], ["Liu", "Ting", ""], ["Wang", "Shijin", ""], ["Hu", "Guoping", ""]]}, {"id": "1909.10699", "submitter": "Allen Nie", "authors": "Allen Nie, Arturo L. Pineda, Matt W. Wright Hannah Wand, Bryan Wulf,\n  Helio A. Costa, Ronak Y. Patel, Carlos D. Bustamante, James Zou", "title": "LitGen: Genetic Literature Recommendation Guided by Human Explanations", "comments": "12 pages; 5 figures. Accepted by PSB 2020 (Pacific Symposium on\n  Biocomputing) track: Artificial Intelligence for Enhancing Clinical Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As genetic sequencing costs decrease, the lack of clinical interpretation of\nvariants has become the bottleneck in using genetics data. A major rate\nlimiting step in clinical interpretation is the manual curation of evidence in\nthe genetic literature by highly trained biocurators. What makes curation\nparticularly time-consuming is that the curator needs to identify papers that\nstudy variant pathogenicity using different types of approaches and\nevidences---e.g. biochemical assays or case control analysis. In collaboration\nwith the Clinical Genomic Resource (ClinGen)---the flagship NIH program for\nclinical curation---we propose the first machine learning system, LitGen, that\ncan retrieve papers for a particular variant and filter them by specific\nevidence types used by curators to assess for pathogenicity. LitGen uses\nsemi-supervised deep learning to predict the type of evidence provided by each\npaper. It is trained on papers annotated by ClinGen curators and systematically\nevaluated on new test data collected by ClinGen. LitGen further leverages rich\nhuman explanations and unlabeled data to gain 7.9%-12.6% relative performance\nimprovement over models learned only on the annotated papers. It is a useful\nframework to improve clinical variant curation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:56:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Nie", "Allen", ""], ["Pineda", "Arturo L.", ""], ["Wand", "Matt W. Wright Hannah", ""], ["Wulf", "Bryan", ""], ["Costa", "Helio A.", ""], ["Patel", "Ronak Y.", ""], ["Bustamante", "Carlos D.", ""], ["Zou", "James", ""]]}, {"id": "1909.10736", "submitter": "Daniel Hienert", "authors": "Daniel Hienert and Dagmar Kern", "title": "Recognizing Topic Change in Search Sessions of Digital Libraries based\n  on Thesaurus and Classification System", "comments": null, "journal-ref": "In Proceedings of 2019 ACM/IEEE Joint Conference on Digital\n  Libraries (JCDL), 297-300", "doi": "10.1109/JCDL.2019.00049", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Log analysis in Web search showed that user sessions often contain several\ndifferent topics. This means sessions need to be segmented into parts which\nhandle the same topic in order to give appropriate user support based on the\ntopic, and not on a mixture of topics. Different methods have been proposed to\nsegment a user session to different topics based on timeouts, lexical analysis,\nquery similarity or external knowledge sources. In this paper, we study the\nproblem in a digital library for the social sciences. We present a method based\non a thesaurus and a classification system which are typical knowledge\norganization systems in digital libraries. Five experts evaluated our approach\nand rated it as good for the segmentation of search sessions into parts that\ntreat the same topic.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 06:57:25 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Hienert", "Daniel", ""], ["Kern", "Dagmar", ""]]}, {"id": "1909.10766", "submitter": "Rasmus Pagh", "authors": "Rasmus Pagh, Johan Sivertsen", "title": "The space complexity of inner product filters", "comments": "To appear at ICDT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of filtering candidate pairs in inner product\nsimilarity joins we study the following inner product estimation problem: Given\nparameters $d\\in {\\bf N}$, $\\alpha>\\beta\\geq 0$ and unit vectors $x,y\\in {\\bf\nR}^{d}$ consider the task of distinguishing between the cases $\\langle x,\ny\\rangle\\leq\\beta$ and $\\langle x, y\\rangle\\geq \\alpha$ where $\\langle x,\ny\\rangle = \\sum_{i=1}^d x_i y_i$ is the inner product of vectors $x$ and $y$.\nThe goal is to distinguish these cases based on information on each vector\nencoded independently in a bit string of the shortest length possible. In\ncontrast to much work on compressing vectors using randomized dimensionality\nreduction, we seek to solve the problem deterministically, with no probability\nof error. Inner product estimation can be solved in general via estimating\n$\\langle x, y\\rangle$ with an additive error bounded by $\\varepsilon = \\alpha -\n\\beta$. We show that $d \\log_2 \\left(\\tfrac{\\sqrt{1-\\beta}}{\\varepsilon}\\right)\n\\pm \\Theta(d)$ bits of information about each vector is necessary and\nsufficient. Our upper bound is constructive and improves a known upper bound of\n$d \\log_2(1/\\varepsilon) + O(d)$ by up to a factor of 2 when $\\beta$ is close\nto $1$. The lower bound holds even in a stronger model where one of the vectors\nis known exactly, and an arbitrary estimation function is allowed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:02:28 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 11:45:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Pagh", "Rasmus", ""], ["Sivertsen", "Johan", ""]]}, {"id": "1909.10779", "submitter": "Lisa Graziani", "authors": "Lisa Graziani, Stefano Melacci, Marco Gori", "title": "Jointly Learning to Detect Emotions and Predict Facebook Reactions", "comments": "International Conference on Artificial Neural Networks. Springer,\n  Cham, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing ubiquity of Social Media data offers an attractive perspective\nfor improving the quality of machine learning-based models in several fields,\nranging from Computer Vision to Natural Language Processing. In this paper we\nfocus on Facebook posts paired with reactions of multiple users, and we\ninvestigate their relationships with classes of emotions that are typically\nconsidered in the task of emotion detection. We are inspired by the idea of\nintroducing a connection between reactions and emotions by means of First-Order\nLogic formulas, and we propose an end-to-end neural model that is able to\njointly learn to detect emotions and predict Facebook reactions in a multi-task\nenvironment, where the logic formulas are converted into polynomial\nconstraints. Our model is trained using a large collection of unsupervised\ntexts together with data labeled with emotion classes and Facebook posts that\ninclude reactions. An extended experimental analysis that leverages a large\ncollection of Facebook posts shows that the tasks of emotion classification and\nreaction prediction can both benefit from their interaction.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 09:45:48 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Graziani", "Lisa", ""], ["Melacci", "Stefano", ""], ["Gori", "Marco", ""]]}, {"id": "1909.10812", "submitter": "Kim Hammar", "authors": "Kim Hammar, Shatha Jaradat, Nima Dokoohaki and Mihhail Matskin", "title": "Deep Text Mining of Instagram Data Without Strong Supervision", "comments": "8 pages, 5 figures. Pre-print for paper to appear in conference\n  proceedings for the Web Intelligence Conference", "journal-ref": null, "doi": "10.1109/WI.2018.00-94", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of social media, our online feeds increasingly consist of\nshort, informal, and unstructured text. This textual data can be analyzed for\nthe purpose of improving user recommendations and detecting trends. Instagram\nis one of the largest social media platforms, containing both text and images.\nHowever, most of the prior research on text processing in social media is\nfocused on analyzing Twitter data, and little attention has been paid to text\nmining of Instagram data. Moreover, many text mining methods rely on annotated\ntraining data, which in practice is both difficult and expensive to obtain. In\nthis paper, we present methods for unsupervised mining of fashion attributes\nfrom Instagram text, which can enable a new kind of user recommendation in the\nfashion domain. In this context, we analyze a corpora of Instagram posts from\nthe fashion domain, introduce a system for extracting fashion attributes from\nInstagram, and train a deep clothing classifier with weak supervision to\nclassify Instagram posts based on the associated text.\n  With our experiments, we confirm that word embeddings are a useful asset for\ninformation extraction. Experimental results show that information extraction\nusing word embeddings outperforms a baseline that uses Levenshtein distance.\nThe results also show the benefit of combining weak supervision signals using\ngenerative models instead of majority voting. Using weak supervision and\ngenerative modeling, an F1 score of 0.61 is achieved on the task of classifying\nthe image contents of Instagram posts based solely on the associated text,\nwhich is on level with human performance. Finally, our empirical study provides\none of the few available studies on Instagram text and shows that the text is\nnoisy, that the text distribution exhibits the long-tail phenomenon, and that\ncomment sections on Instagram are multi-lingual.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:04:02 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Hammar", "Kim", ""], ["Jaradat", "Shatha", ""], ["Dokoohaki", "Nima", ""], ["Matskin", "Mihhail", ""]]}, {"id": "1909.10912", "submitter": "Viet Anh Tran", "authors": "Viet-Anh Tran, Romain Hennequin, Jimena Royo-Letelier, Manuel\n  Moussallam", "title": "Improving Collaborative Metric Learning with Efficient Negative Sampling", "comments": "SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331337", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning based on triplet loss has been applied with success\nin a wide range of applications such as face recognition, image retrieval,\nspeaker change detection and recently recommendation with the CML model.\nHowever, as we show in this article, CML requires large batches to work\nreasonably well because of a too simplistic uniform negative sampling strategy\nfor selecting triplets. Due to memory limitations, this makes it difficult to\nscale in high-dimensional scenarios. To alleviate this problem, we propose here\na 2-stage negative sampling strategy which finds triplets that are highly\ninformative for learning. Our strategy allows CML to work effectively in terms\nof accuracy and popularity bias, even when the batch size is an order of\nmagnitude smaller than what would be needed with the default uniform sampling.\nWe demonstrate the suitability of the proposed strategy for recommendation and\nexhibit consistent positive results across various datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 13:37:49 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Tran", "Viet-Anh", ""], ["Hennequin", "Romain", ""], ["Royo-Letelier", "Jimena", ""], ["Moussallam", "Manuel", ""]]}, {"id": "1909.11057", "submitter": "Mustafa Atay", "authors": "Ali Alwehaibi and Mustafa Atay", "title": "A Rule-Based Relational XML Access Control Model in the Presence of\n  Authorization Conflicts", "comments": "14th International Conference on Information Technology - New\n  Generations, Las Vegas, NV, April 10-12, 2017, Published by Springer, Cham; 6\n  pages, 5 figures, 2 tables", "journal-ref": "Advances in Intelligent Systems and Computing Information\n  Technology - New Generations, 311-319 (2017)", "doi": "10.1007/978-3-319-54978-1_43", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is considerable amount of sensitive XML data stored in relational\ndatabases. It is a challenge to enforce node level fine-grained authorization\npolicies for XML data stored in relational databases which typically support\ntable and column level access control. Moreover, it is common to have\nconflicting authorization policies over the hierarchical nested structure of\nXML data. There are a couple of XML access control models for relational XML\ndatabases proposed in the literature. However, to our best knowledge, none of\nthem discussed handling authorization conflicts with conditions in the domain\nof relational XML databases. Therefore, we believe that there is a need to\ndefine and incorporate effective fine-grained XML authorization models with\nconflict handling mechanisms in the presence of conditions into relational XML\ndatabases. We address this issue in this study.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 17:16:40 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Alwehaibi", "Ali", ""], ["Atay", "Mustafa", ""]]}, {"id": "1909.11117", "submitter": "Alexis Arnaudon Dr", "authors": "Robert L. Peach, Alexis Arnaudon, Mauricio Barahona", "title": "Semi-supervised classification on graphs using explicit diffusion\n  dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification tasks based on feature vectors can be significantly improved\nby including within deep learning a graph that summarises pairwise\nrelationships between the samples. Intuitively, the graph acts as a conduit to\nchannel and bias the inference of class labels. Here, we study classification\nmethods that consider the graph as the originator of an explicit graph\ndiffusion. We show that appending graph diffusion to feature-based learning as\nan \\textit{a posteriori} refinement achieves state-of-the-art classification\naccuracy. This method, which we call Graph Diffusion Reclassification (GDR),\nuses overshooting events of a diffusive graph dynamics to reclassify individual\nnodes. The method uses intrinsic measures of node influence, which are distinct\nfor each node, and allows the evaluation of the relationship and importance of\nfeatures and graph for classification. We also present diff-GCN, a simple\nextension of Graph Convolutional Neural Network (GCN) architectures that\nleverages explicit diffusion dynamics, and allows the natural use of directed\ngraphs. To showcase our methods, we use benchmark datasets of documents with\nassociated citation data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 18:38:52 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Peach", "Robert L.", ""], ["Arnaudon", "Alexis", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1909.11189", "submitter": "Thomas Haider", "authors": "Thomas N. Haider", "title": "Diachronic Topics in New High German Poetry", "comments": null, "journal-ref": "In Proceedings of the International Digital Humanities Conference\n  DH2019, Utrecht, Link: https://dev.clariah.nl/files/dh2019/boa/1031.html", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical topic models are increasingly and popularly used by Digital\nHumanities scholars to perform distant reading tasks on literary data. It\nallows us to estimate what people talk about. Especially Latent Dirichlet\nAllocation (LDA) has shown its usefulness, as it is unsupervised, robust, easy\nto use, scalable, and it offers interpretable results. In a preliminary study,\nwe apply LDA to a corpus of New High German poetry (textgrid, with 51k poems,\n8m token), and use the distribution of topics over documents for a\nclassification of poems into time periods and for authorship attribution.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 21:19:01 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Haider", "Thomas N.", ""]]}, {"id": "1909.11252", "submitter": "Yang Lv", "authors": "Yang Lv, Liangsheng Zhuang, Pengyu Luo", "title": "Neighborhood-Enhanced and Time-Aware Model for Session-based\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session based recommendation has become one of the research hotpots in the\nfield of recommendation systems due to its highly practical value.Previous deep\nlearning methods mostly focus on the sequential characteristics within the\ncurrent session,and neglect the context similarity and temporal similarity\nbetween sessions which contain abundant collaborative information.In this\npaper,we propose a novel neural networks framework,namely Neighborhood Enhanced\nand Time Aware Recommendation Machine(NETA) for session based recommendation.\nFirstly,we introduce an efficient neighborhood retrieve mechanism to find out\nsimilar sessions which includes collaborative information.Then we design a\nguided attention with time-aware mechanism to extract collaborative\nrepresentation from neighborhood sessions.Especially,temporal recency between\nsessions is considered separately.Finally, we design a simple co-attention\nmechanism to determine the importance of complementary collaborative\nrepresentation when predicting the next item.Extensive experiments conducted on\ntwo real-world datasets demonstrate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 01:50:32 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:33:07 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Lv", "Yang", ""], ["Zhuang", "Liangsheng", ""], ["Luo", "Pengyu", ""]]}, {"id": "1909.11258", "submitter": "Omer Anjum", "authors": "Omer Anjum, Hongyu Gong, Suma Bhat, Wen-Mei Hwu, Jinjun Xiong", "title": "PaRe: A Paper-Reviewer Matching Approach Using a Common Topic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the right reviewers to assess the quality of conference submissions\nis a time consuming process for conference organizers. Given the importance of\nthis step, various automated reviewer-paper matching solutions have been\nproposed to alleviate the burden. Prior approaches, including bag-of-words\nmodels and probabilistic topic models have been inadequate to deal with the\nvocabulary mismatch and partial topic overlap between a paper submission and\nthe reviewer's expertise. Our approach, the common topic model, jointly models\nthe topics common to the submission and the reviewer's profile while relying on\nabstract topic vectors. Experiments and insightful evaluations on two datasets\ndemonstrate that the proposed method achieves consistent improvements compared\nto available state-of-the-art implementations of paper-reviewer matching.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 02:25:23 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Anjum", "Omer", ""], ["Gong", "Hongyu", ""], ["Bhat", "Suma", ""], ["Hwu", "Wen-Mei", ""], ["Xiong", "Jinjun", ""]]}, {"id": "1909.11288", "submitter": "Nway Han Nway", "authors": "Nway Nway Han, Aye Thida", "title": "Annotated Guidelines and Building Reference Corpus for Myanmar-English\n  Word Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference corpus for word alignment is an important resource for developing\nand evaluating word alignment methods. For Myanmar-English language pairs,\nthere is no reference corpus to evaluate the word alignment tasks. Therefore,\nwe created the guidelines for Myanmar-English word alignment annotation between\ntwo languages over contrastive learning and built the Myanmar-English reference\ncorpus consisting of verified alignments from Myanmar ALT of the Asian Language\nTreebank (ALT). This reference corpus contains confident labels sure (S) and\npossible (P) for word alignments which are used to test for the purpose of\nevaluation of the word alignments tasks. We discuss the most linking\nambiguities to define consistent and systematic instructions to align manual\nwords. We evaluated the results of annotators agreement using our reference\ncorpus in terms of alignment error rate (AER) in word alignment tasks and\ndiscuss the words relationships in terms of BLEU scores.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 04:47:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Han", "Nway Nway", ""], ["Thida", "Aye", ""]]}, {"id": "1909.11605", "submitter": "Tao Guo", "authors": "Tao Guo, Ruida Zhou, Chao Tian", "title": "On the Information Leakage in Private Information Retrieval Systems", "comments": "14 double-column pages, 5 figures, submitted to IEEE Transactions on\n  Information Forensics & Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider information leakage to the user in private information retrieval\n(PIR) systems. Information leakage can be measured in terms of individual\nmessage leakage or total leakage. Individual message leakage, or simply\nindividual leakage, is defined as the amount of information that the user can\nobtain on any individual message that is not being requested, and the total\nleakage is defined as the amount of information that the user can obtain about\nall the other messages except the one being requested. In this work, we\ncharacterize the tradeoff between the minimum download cost and the individual\nleakage, and that for the total leakage, respectively. New codes are proposed\nto achieve these optimal tradeoffs, which are also shown to be optimal in terms\nof the message size. We further characterize the optimal tradeoff between the\nminimum amount of common randomness and the total leakage. Moreover, we show\nthat under individual leakage, common randomness is in fact unnecessary when\nthere are more than two messages.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 16:39:42 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 04:37:30 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Guo", "Tao", ""], ["Zhou", "Ruida", ""], ["Tian", "Chao", ""]]}, {"id": "1909.11706", "submitter": "Minjun Kim", "authors": "Minjun Kim and Hiroki Sayama", "title": "The Power of Communities: A Text Classification Model with Automated\n  Labeling Process Using Network Community Detection", "comments": "13 pages, 5 figures, 1 table. Accepted by NetSci-X 2020 Tokyo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is one of the most critical areas in machine learning and\nartificial intelligence research. It has been actively adopted in many business\napplications such as conversational intelligence systems, news articles\ncategorizations, sentiment analysis, emotion detection systems, and many other\nrecommendation systems in our daily life. One of the problems in supervised\ntext classification models is that the models' performance depends heavily on\nthe quality of data labeling that is typically done by humans. In this study,\nwe propose a new network community detection-based approach to automatically\nlabel and classify text data into multiclass value spaces. Specifically, we\nbuild networks with sentences as the network nodes and pairwise cosine\nsimilarities between the Term Frequency-Inversed Document Frequency (TFIDF)\nvector representations of the sentences as the network link weights. We use the\nLouvain method to detect the communities in the sentence networks. We train and\ntest the Support Vector Machine and the Random Forest models on both the\nhuman-labeled data and network community detection labeled data. Results showed\nthat models with the data labeled by the network community detection\noutperformed the models with the human-labeled data by 2.68-3.75% of\nclassification accuracy. Our method may help developments of more accurate\nconversational intelligence and other text classification systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:43:22 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 03:18:20 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 16:41:43 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Kim", "Minjun", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1909.11824", "submitter": "Jianming Zheng", "authors": "Jianming Zheng, Fei Cai, Honghui Chen, Maarten de Rijke", "title": "Pre-train, Interact, Fine-tune: A Novel Interaction Representation for\n  Text Classification", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text representation can aid machines in understanding text. Previous work on\ntext representation often focuses on the so-called forward implication, i.e.,\npreceding words are taken as the context of later words for creating\nrepresentations, thus ignoring the fact that the semantics of a text segment is\na product of the mutual implication of words in the text: later words\ncontribute to the meaning of preceding words. We introduce the concept of\ninteraction and propose a two-perspective interaction representation, that\nencapsulates a local and a global interaction representation. Here, a local\ninteraction representation is one that interacts among words with\nparent-children relationships on the syntactic trees and a global interaction\ninterpretation is one that interacts among all the words in a sentence. We\ncombine the two interaction representations to develop a Hybrid Interaction\nRepresentation (HIR).\n  Inspired by existing feature-based and fine-tuning-based pretrain-finetuning\napproaches to language models, we integrate the advantages of feature-based and\nfine-tuning-based methods to propose the Pre-train, Interact, Fine-tune (PIF)\narchitecture.\n  We evaluate our proposed models on five widely-used datasets for text\nclassification tasks. Our ensemble method, outperforms state-of-the-art\nbaselines with improvements ranging from 2.03% to 3.15% in terms of error rate.\nIn addition, we find that, the improvements of PIF against most\nstate-of-the-art methods is not affected by increasing of the length of the\ntext.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:14:22 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zheng", "Jianming", ""], ["Cai", "Fei", ""], ["Chen", "Honghui", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1909.11913", "submitter": "Tong Wu", "authors": "Yue Wang, Tong Wu, Yunlong Wang, Gao Wang", "title": "Enhancing Model Interpretability and Accuracy for Disease Progression\n  Prediction via Phenotype-Based Patient Similarity Learning", "comments": "12 pages, accepted by Pacific Symposium on Biocomputing (PSB) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models have been proposed to extract temporal patterns from longitudinal\nelectronic health records (EHR) for clinical predictive models. However, the\ncommon relations among patients (e.g., receiving the same medical treatments)\nwere rarely considered. In this paper, we propose to learn patient similarity\nfeatures as phenotypes from the aggregated patient-medical service matrix using\nnon-negative matrix factorization. On real-world medical claim data, we show\nthat the learned phenotypes are coherent within each group, and also\nexplanatory and indicative of targeted diseases. We conducted experiments to\npredict the diagnoses for Chronic Lymphocytic Leukemia (CLL) patients. Results\nshow that the phenotype-based similarity features can improve prediction over\nmultiple baselines, including logistic regression, random forest, convolutional\nneural network, and more.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 05:56:33 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wang", "Yue", ""], ["Wu", "Tong", ""], ["Wang", "Yunlong", ""], ["Wang", "Gao", ""]]}, {"id": "1909.11974", "submitter": "Ze Yang", "authors": "Ze Yang, Can Xu, Wei Wu, Zhoujun Li", "title": "Read, Attend and Comment: A Deep Architecture for Automatic News Comment\n  Generation", "comments": "Accepted by EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic news comment generation is a new testbed for techniques of natural\nlanguage generation. In this paper, we propose a \"read-attend-comment\"\nprocedure for news comment generation and formalize the procedure with a\nreading network and a generation network. The reading network comprehends a\nnews article and distills some important points from it, then the generation\nnetwork creates a comment by attending to the extracted discrete points and the\nnews title. We optimize the model in an end-to-end manner by maximizing a\nvariational lower bound of the true objective using the back-propagation\nalgorithm. Experimental results on two datasets indicate that our model can\nsignificantly outperform existing methods in terms of both automatic evaluation\nand human judgment.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:34:05 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 06:33:29 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 17:55:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Yang", "Ze", ""], ["Xu", "Can", ""], ["Wu", "Wei", ""], ["Li", "Zhoujun", ""]]}, {"id": "1909.12009", "submitter": "Swagata Duari", "authors": "Swagata Duari and Vasudha Bhatnagar", "title": "Complex Network based Supervised Keyword Extractor", "comments": null, "journal-ref": "Expert Systems with Applications, 140, 112876 (2020)", "doi": "10.1016/j.eswa.2019.112876", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a supervised framework for automatic keyword\nextraction from single document. We model the text as complex network, and\nconstruct the feature set by extracting select node properties from it. Several\nnode properties have been exploited by unsupervised, graph-based keyword\nextraction methods to discriminate keywords from non-keywords. We exploit the\ncomplex interplay of node properties to design a supervised keyword extraction\nmethod. The training set is created from the feature set by assigning a label\nto each candidate keyword depending on whether the candidate is listed as a\ngold-standard keyword or not. Since the number of keywords in a document is\nmuch less than non-keywords, the curated training set is naturally imbalanced.\nWe train a binary classifier to predict keywords after balancing the training\nset. The model is trained using two public datasets from scientific domain and\ntested using three unseen scientific corpora and one news corpus. Comparative\nstudy of the results with several recent keyword and keyphrase extraction\nmethods establishes that the proposed method performs better in most cases.\nThis substantiates our claim that graph-theoretic properties of words are\neffective discriminators between keywords and non-keywords. We support our\nargument by showing that the improved performance of the proposed method is\nstatistically significant for all datasets. We also evaluate the effectiveness\nof the pre-trained model on Hindi and Assamese language documents. We observe\nthat the model performs equally well for the cross-language text even though it\nwas trained only on English language documents. This shows that the proposed\nmethod is independent of the domain, collection, and language of the training\ncorpora.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 10:02:07 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Duari", "Swagata", ""], ["Bhatnagar", "Vasudha", ""]]}, {"id": "1909.12229", "submitter": "Avinash Swaminathan", "authors": "Avinash Swaminathan, Raj Kuwar Gupta, Haimin Zhang, Debanjan Mahata,\n  Rakesh Gosangi, Rajiv Ratn Shah", "title": "Keyphrase Generation for Scientific Articles using GANs", "comments": "2 pages, 1 fig, 8 references, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a keyphrase generation approach using conditional\nGenerative Adversarial Networks (GAN). In our GAN model, the generator outputs\na sequence of keyphrases based on the title and abstract of a scientific\narticle. The discriminator learns to distinguish between machine-generated and\nhuman-curated keyphrases. We evaluate this approach on standard benchmark\ndatasets. Our model achieves state-of-the-art performance in generation of\nabstractive keyphrases and is also comparable to the best performing extractive\ntechniques. We also demonstrate that our method generates more diverse\nkeyphrases and make our implementation publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 02:46:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Swaminathan", "Avinash", ""], ["Gupta", "Raj Kuwar", ""], ["Zhang", "Haimin", ""], ["Mahata", "Debanjan", ""], ["Gosangi", "Rakesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1909.12301", "submitter": "Jingwei Ma", "authors": "Jingwei Ma, Jiahui Wen, Mingyang Zhong, Liangchen Liu, Chaojie Li,\n  Weitong Chen, Yin Yang, Honghui Tu, Xue Li", "title": "DBRec: Dual-Bridging Recommendation via Discovering Latent Groups", "comments": "10 pages, 16 figures, The 28th ACM International Conference on\n  Information and Knowledge Management (CIKM '19)", "journal-ref": null, "doi": "10.1145/3357384.3357892", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recommender systems, the user-item interaction data is usually sparse and\nnot sufficient for learning comprehensive user/item representations for\nrecommendation. To address this problem, we propose a novel dual-bridging\nrecommendation model (DBRec). DBRec performs latent user/item group discovery\nsimultaneously with collaborative filtering, and interacts group information\nwith users/items for bridging similar users/items. Therefore, a user's\npreference over an unobserved item, in DBRec, can be bridged by the users\nwithin the same group who have rated the item, or the user-rated items that\nshare the same group with the unobserved item. In addition, we propose to\njointly learn user-user group (item-item group) hierarchies, so that we can\neffectively discover latent groups and learn compact user/item representations.\nWe jointly integrate collaborative filtering, latent group discovering and\nhierarchical modelling into a unified framework, so that all the model\nparameters can be learned toward the optimization of the objective function. We\nvalidate the effectiveness of the proposed model with two real datasets, and\ndemonstrate its advantage over the state-of-the-art recommendation models with\nextensive experiments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 03:58:03 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 14:19:23 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ma", "Jingwei", ""], ["Wen", "Jiahui", ""], ["Zhong", "Mingyang", ""], ["Liu", "Liangchen", ""], ["Li", "Chaojie", ""], ["Chen", "Weitong", ""], ["Yang", "Yin", ""], ["Tu", "Honghui", ""], ["Li", "Xue", ""]]}, {"id": "1909.12425", "submitter": "Zhiwen Tang", "authors": "Zhiwen Tang, Grace Hui Yang", "title": "Dynamic Search -- Optimizing the Game of Information Seeking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the emerging topic of dynamic search (DS). To position\ndynamic search in a larger research landscape, the article discusses in detail\nits relationship to related research topics and disciplines. The article\nreviews approaches to modeling dynamics during information seeking, with an\nemphasis on Reinforcement Learning (RL)-enabled methods. Details are given for\nhow different approaches are used to model interactions among the human user,\nthe search system, and the environment. The paper ends with a review of\nevaluations of dynamic search systems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 22:50:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:40:34 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Tang", "Zhiwen", ""], ["Yang", "Grace Hui", ""]]}, {"id": "1909.12526", "submitter": "Ralph Gasser", "authors": "Luca Rossetto, Ralph Gasser, Heiko Schuldt", "title": "Query by Semantic Sketch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based query formulation is very common in image and video retrieval as\nthese techniques often complement textual retrieval methods that are based on\neither manual or machine generated annotations. In this paper, we present a\nretrieval approach that allows to query visual media collections by sketching\nconcept maps, thereby merging sketch-based retrieval with the search for\nsemantic labels. Users can draw a spatial distribution of different concept\nlabels, such as \"sky\", \"sea\" or \"person\" and then use these sketches to find\nimages or video scenes that exhibit a similar distribution of these concepts.\nHence, this approach does not only take the semantic concepts themselves into\naccount, but also their semantic relations as well as their spatial context.\nThe efficient vector representation enables efficient retrieval even in large\nmultimedia collections. We have integrated the semantic sketch query mode into\nour retrieval engine vitrivr and demonstrated its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:27:04 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Rossetto", "Luca", ""], ["Gasser", "Ralph", ""], ["Schuldt", "Heiko", ""]]}, {"id": "1909.12601", "submitter": "Kashif Ahmad Dr", "authors": "Naina Said, Kashif Ahmad, Nicola Conci, Ala Al-Fuqaha", "title": "Active Learning for Event Detection in Support of Disaster Analysis\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaster analysis in social media content is one of the interesting research\ndomains having abundance of data. However, there is a lack of labeled data that\ncan be used to train machine learning models for disaster analysis\napplications. Active learning is one of the possible solutions to such problem.\nTo this aim, in this paper we propose and assess the efficacy of an active\nlearning based framework for disaster analysis using images shared on social\nmedia outlets. Specifically, we analyze the performance of different active\nlearning techniques employing several sampling and disagreement strategies.\nMoreover, we collect a large-scale dataset covering images from eight common\ntypes of natural disasters. The experimental results show that the use of\nactive learning techniques for disaster analysis using images results in a\nperformance comparable to that obtained using human annotated images, and could\nbe used in frameworks for disaster analysis in images without tedious job of\nmanual annotation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 10:28:10 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Said", "Naina", ""], ["Ahmad", "Kashif", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1909.12746", "submitter": "Seyed Mohammad Hashemi", "authors": "Seyed Mohammad Hashemi, Mohammad Rahmati", "title": "Cross-domain recommender system using Generalized Canonical Correlation\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems provide personalized recommendations to the users from a\nlarge number of possible options in online stores. Matrix factorization is a\nwell-known and accurate collaborative filtering approach for recommender\nsystem, which suffers from cold-start problem for new users and items. Whenever\na new user participate with the system there is not enough interactions with\nthe system, therefore there are not enough ratings in the user-item matrix to\nlearn the matrix factorization model. Using auxiliary data such as users\ndemographic, ratings and reviews in relevant domains, is an effective solution\nto reduce the new user problem. In this paper, we used data of users from other\ndomains and build a common space to represent the latent factors of users from\ndifferent domains. In this representation we proposed an iterative method which\napplied MAX-VAR generalized canonical correlation analysis (GCCA) on users\nlatent factors learned from matrix factorization on each domain. Also, to\nimprove the capability of GCCA to learn latent factors for new users, we\npropose generalized canonical correlation analysis by inverse sum of selection\nmatrices (GCCA-ISSM) approach, which provides better recommendations in\ncold-start scenarios. The proposed approach is extended using content-based\nfeatures from topic modeling extracted from users reviews. We demonstrate the\naccuracy and effectiveness of the proposed approaches on cross-domain ratings\npredictions using comprehensive experiments on Amazon and MovieLens datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 22:27:53 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Hashemi", "Seyed Mohammad", ""], ["Rahmati", "Mohammad", ""]]}, {"id": "1909.12749", "submitter": "Mojdeh Saadati", "authors": "Mojdeh Saadati, Syed Shihab, Mohammed Shaiqur Rahman", "title": "Movie Recommender Systems: Implementation and Performance Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, explosive growth in the number of items in the catalog of\ne-commerce businesses, such as Amazon, Netflix, Pandora, etc., have warranted\nthe development of recommender systems to guide consumers towards their desired\nproducts based on their preferences and tastes. Some of the popular approaches\nfor building recommender systems, for mining user, derived input datasets, are:\ncontent-based systems, collaborative filtering, latent-factor systems using\nSingular Value Decomposition (SVD), and Restricted Boltzmann Machines (RBM). In\nthis project, user-user collaborative filtering, item-item collaborative\nfiltering, content-based recommendation, SVD, and neural networks were chosen\nfor implementation in Python to predict the user ratings of unwatched movies\nfor each user, and their performances were evaluated and compared.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 03:59:47 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Saadati", "Mojdeh", ""], ["Shihab", "Syed", ""], ["Rahman", "Mohammed Shaiqur", ""]]}, {"id": "1909.12756", "submitter": "Debi Prasanna Mohanty Mr", "authors": "Benu Madhab Changmai, Divija Nagaraju, Debi Prasanna Mohanty, Kriti\n  Singh, Kunal Bansal, Sukumar Moharana", "title": "On-Device User Intent Prediction for Context and Sequence Aware\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pursuit of improved accuracy in recommender systems has led to the\nincorporation of user context. Context-aware recommender systems typically\nhandle large amounts of data which must be uploaded and stored on the cloud,\nputting the user's personal information at risk. While there have been previous\nstudies on privacy-sensitive and context-aware recommender systems, there has\nnot been a full-fledged system deployed in an isolated mobile environment. We\npropose a secure and efficient on-device mechanism to predict a user's next\nintention. The knowledge of the user's real-time intention can help recommender\nsystems to provide more relevant recommendations at the right moment. Our\nproposed algorithm is both context and sequence aware. We embed user intentions\nas weighted nodes in an n-dimensional vector space where each dimension\nrepresents a specific user context factor. Through a neighborhood searching\nmethod followed by a sequence matching algorithm, we search for the most\nrelevant node to make the prediction. An evaluation of our methodology was done\non a diverse real-world dataset where it was able to address practical\nscenarios like behavior drifts and sequential patterns efficiently and\nrobustly. Our system also outperformed most of the state-of-the-art methods\nwhen evaluated for a similar problem domain on standard datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:59:03 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Changmai", "Benu Madhab", ""], ["Nagaraju", "Divija", ""], ["Mohanty", "Debi Prasanna", ""], ["Singh", "Kriti", ""], ["Bansal", "Kunal", ""], ["Moharana", "Sukumar", ""]]}, {"id": "1909.12798", "submitter": "Hao Wang", "authors": "Hao Wang, Zonghu Wang, Weishi Zhang", "title": "Quantitative analysis of Matthew effect and sparsity problem of\n  recommender systems", "comments": null, "journal-ref": "2018 IEEE 3rd International Conference on Cloud Computing and Big\n  Data Analysis (ICCCBDA)", "doi": "10.1109/ICCCBDA.2018.8386490", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have received great commercial success. Recommendation\nhas been used widely in areas such as e-commerce, online music FM, online news\nportal, etc. However, several problems related to input data structure pose\nserious challenge to recommender system performance. Two of these problems are\nMatthew effect and sparsity problem. Matthew effect heavily skews recommender\nsystem output towards popular items. Data sparsity problem directly affects the\ncoverage of recommendation result. Collaborative filtering is a simple\nbenchmark ubiquitously adopted in the industry as the baseline for recommender\nsystem design. Understanding the underlying mechanism of collaborative\nfiltering is crucial for further optimization. In this paper, we do a thorough\nquantitative analysis on Matthew effect and sparsity problem in the particular\ncontext setting of collaborative filtering. We compare the underlying mechanism\nof user-based and item-based collaborative filtering and give insight to\nindustrial recommender system builders.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:50:47 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Zonghu", ""], ["Zhang", "Weishi", ""]]}, {"id": "1909.12799", "submitter": "Anne-Marie Tousch", "authors": "Anne-Marie Tousch", "title": "How robust is MovieLens? A dataset analysis for recommender systems", "comments": "2 pages ; accepted at REVEAL workshop, RecSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research publication requires public datasets. In recommender systems, some\ndatasets are largely used to compare algorithms against a --supposedly-- common\nbenchmark. Problem: for various reasons, these datasets are heavily\npreprocessed, making the comparison of results across papers difficult. This\npaper makes explicit the variety of preprocessing and evaluation protocols to\ntest the robustness of a dataset (or lack of flexibility). While robustness is\ngood to compare results across papers, for flexible datasets we propose a\nmethod to select a preprocessing protocol and share results more transparently.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 09:36:31 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Tousch", "Anne-Marie", ""]]}, {"id": "1909.12807", "submitter": "Zhu Sun", "authors": "Zhu Sun, Qing Guo, Jie Yang, Hui Fang, Guibing Guo, Jie Zhang, Robin\n  Burke", "title": "Research Commentary on Recommendations with Side Information: A Survey\n  and Research Directions", "comments": null, "journal-ref": null, "doi": "10.1016/j.elerap.2019.100879", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have become an essential tool to help resolve the\ninformation overload problem in recent decades. Traditional recommender\nsystems, however, suffer from data sparsity and cold start problems. To address\nthese issues, a great number of recommendation algorithms have been proposed to\nleverage side information of users or items (e.g., social network and item\ncategory), demonstrating a high degree of effectiveness in improving\nrecommendation performance. This Research Commentary aims to provide a\ncomprehensive and systematic survey of the recent research on recommender\nsystems with side information. Specifically, we provide an overview of\nstate-of-the-art recommendation algorithms with side information from two\northogonal perspectives. One involves the different methodologies of\nrecommendation: the memory-based methods, latent factor, representation\nlearning, and deep learning models. The others cover different representations\nof side information, including structural data (flat, network, and hierarchical\nfeatures, and knowledge graphs); and non-structural data (text, image and video\nfeatures). Finally, we discuss challenges and provide new potential directions\nin recommendation, along with the conclusion of this survey.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 03:43:47 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 05:21:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sun", "Zhu", ""], ["Guo", "Qing", ""], ["Yang", "Jie", ""], ["Fang", "Hui", ""], ["Guo", "Guibing", ""], ["Zhang", "Jie", ""], ["Burke", "Robin", ""]]}, {"id": "1909.12902", "submitter": "Denys Dutykh", "authors": "Beno\\^it Colange and Laurent Vuillon and Sylvain Lespinats and Denys\n  Dutykh", "title": "Interpreting Distortions in Dimensionality Reduction by Superimposing\n  Neighbourhood Graphs", "comments": "5 pages, 6 figures, 22 references. Paper presented at IEEE VIS 2019\n  Conference. Other author's papers can be downloaded at\n  http://www.denys-dutykh.com/", "journal-ref": "Paper presented at IEEE Vis 2019 conference at Vancouver, Canada", "doi": "10.1109/VISUAL.2019.8933568", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To perform visual data exploration, many dimensionality reduction methods\nhave been developed. These tools allow data analysts to represent\nmultidimensional data in a 2D or 3D space, while preserving as much relevant\ninformation as possible. Yet, they cannot preserve all structures\nsimultaneously and they induce some unavoidable distortions. Hence, many\ncriteria have been introduced to evaluate a map's overall quality, mostly based\non the preservation of neighbourhoods. Such global indicators are currently\nused to compare several maps, which helps to choose the most appropriate\nmapping method and its hyperparameters. However, those aggregated indicators\ntend to hide the local repartition of distortions. Thereby, they need to be\nsupplemented by local evaluation to ensure correct interpretation of maps. In\nthis paper, we describe a new method, called MING, for `Map Interpretation\nusing Neighbourhood Graphs'. It offers a graphical interpretation of pairs of\nmap quality indicators, as well as local evaluation of the distortions. This is\ndone by displaying on the map the nearest neighbours graphs computed in the\ndata space and in the embedding. Shared and unshared edges exhibit reliable and\nunreliable neighbourhood information conveyed by the mapping. By this mean,\nanalysts may determine whether proximity (or remoteness) of points on the map\nfaithfully represents similarity (or dissimilarity) of original data, within\nthe meaning of a chosen map quality criteria. We apply this approach to two\npairs of widespread indicators: precision/recall and\ntrustworthiness/continuity, chosen for their wide use in the community, which\nwill allow an easy handling by users.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:48:26 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Colange", "Beno\u00eet", ""], ["Vuillon", "Laurent", ""], ["Lespinats", "Sylvain", ""], ["Dutykh", "Denys", ""]]}, {"id": "1909.12932", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka\n  Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka", "title": "BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for\n  Buddha Statues", "comments": "Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BUDA.ART, a system designed to assist researchers in Art\nHistory, to explore and analyze an archive of pictures of Buddha statues. The\nsystem combines different CBIR and classical retrieval techniques to assemble\n2D pictures, 3D statue scans and meta-data, that is focused on the Buddha\nfacial characteristics. We build the system from an archive of 50,000 Buddhism\npictures, identify unique Buddha statues, extract contextual information, and\nprovide specific facial embedding to first index the archive. The system allows\nfor mobile, on-site search, and to explore similarities of statues in the\narchive. In addition, we provide search visualization and 3D analysis of the\nstatues\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:35:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.13077", "submitter": "Dan Wang", "authors": "Dan Wang and Jibing Gong and Yaxi Song", "title": "W-RNN: News text classification based on a Weighted RNN", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the information is stored as text, so text mining is regarded as\nhaving high commercial potential. Aiming at the semantic constraint problem of\nclassification methods based on sparse representation, we propose a weighted\nrecurrent neural network (W-RNN), which can fully extract text serialization\nsemantic information. For the problem that the feature high dimensionality and\nunclear semantic relationship in text data representation, we first utilize the\nword vector to represent the vocabulary in the text and use Recurrent Neural\nNetwork (RNN) to extract features of the serialized text data. The word vector\nis then automatically weighted and summed using the intermediate output of the\nword vector to form the text representation vector. Finally, the neural network\nis used for classification. W-RNN is verified on the news dataset and proves\nthat W-RNN is superior to other four baseline methods in Precision, Recall, F1\nand loss values, which is suitable for text classification.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 11:54:15 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wang", "Dan", ""], ["Gong", "Jibing", ""], ["Song", "Yaxi", ""]]}, {"id": "1909.13183", "submitter": "Carl Yang", "authors": "Carl Yang, Lingrui Gan, Zongyi Wang, Jiaming Shen, Jinfeng Xiao,\n  Jiawei Han", "title": "Query-Specific Knowledge Summarization with Entity Evolutionary Networks", "comments": "published in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query, unlike traditional IR that finds relevant documents or\nentities, in this work, we focus on retrieving both entities and their\nconnections for insightful knowledge summarization. For example, given a query\n\"computer vision\" on a CS literature corpus, rather than returning a list of\nrelevant entities like \"cnn\", \"imagenet\" and \"svm\", we are interested in the\nconnections among them, and furthermore, the evolution patterns of such\nconnections along particular ordinal dimensions such as time. Particularly, we\nhope to provide structural knowledge relevant to the query, such as \"svm\" is\nrelated to \"imagenet\" but not \"cnn\". Moreover, we aim to model the changing\ntrends of the connections, such as \"cnn\" becomes highly related to \"imagenet\"\nafter 2010, which enables the tracking of knowledge evolutions. In this work,\nto facilitate such a novel insightful search system, we propose\n\\textsc{SetEvolve}, which is a unified framework based on nonparanomal\ngraphical models for evolutionary network construction from large text corpora.\nSystematic experiments on synthetic data and insightful case studies on\nreal-world corpora demonstrate the utility of \\textsc{SetEvolve}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 01:48:32 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Yang", "Carl", ""], ["Gan", "Lingrui", ""], ["Wang", "Zongyi", ""], ["Shen", "Jiaming", ""], ["Xiao", "Jinfeng", ""], ["Han", "Jiawei", ""]]}, {"id": "1909.13315", "submitter": "Yatin Chaudhary", "authors": "Yatin Chaudhary, Pankaj Gupta, Thomas Runkler", "title": "Lifelong Neural Topic Learning in Contextualized Autoregressive Topic\n  Models of Language via Informative Transfers", "comments": "94 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as LDA, DocNADE, iDocNADEe have been popular in document\nanalysis. However, the traditional topic models have several limitations\nincluding: (1) Bag-of-words (BoW) assumption, where they ignore word ordering,\n(2) Data sparsity, where the application of topic models is challenging due to\nlimited word co-occurrences, leading to incoherent topics and (3) No Continuous\nLearning framework for topic learning in lifelong fashion, exploiting\nhistorical knowledge (or latent topics) and minimizing catastrophic forgetting.\nThis thesis focuses on addressing the above challenges within neural topic\nmodeling framework. We propose: (1) Contextualized topic model that combines a\ntopic and a language model and introduces linguistic structures (such as word\nordering, syntactic and semantic features, etc.) in topic modeling, (2) A novel\nlifelong learning mechanism into neural topic modeling framework to demonstrate\ncontinuous learning in sequential document collections and minimizing\ncatastrophic forgetting. Additionally, we perform a selective data augmentation\nto alleviate the need for complete historical corpora during data hallucination\nor replay.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 16:43:30 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chaudhary", "Yatin", ""], ["Gupta", "Pankaj", ""], ["Runkler", "Thomas", ""]]}, {"id": "1909.13330", "submitter": "Ezgi Yildirim", "authors": "Ezgi Y{\\i}ld{\\i}r{\\i}m, Payam Azad, \\c{S}ule G\\\"und\\\"uz\n  \\\"O\\u{g}\\\"ud\\\"uc\\\"u", "title": "Neural Hybrid Recommender: Recommendation needs collaboration", "comments": "Accepted for ECML PKDD 2019 International Workshop on New Frontiers\n  in Mining Complex Patterns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has gained an indisputable success in computer\nvision, speech recognition, and natural language processing. After its rising\nsuccess on these challenging areas, it has been studied on recommender systems\nas well, but mostly to include content features into traditional methods. In\nthis paper, we introduce a generalized neural network-based recommender\nframework that is easily extendable by additional networks. This framework\nnamed NHR, short for Neural Hybrid Recommender allows us to include more\nelaborate information from the same and different data sources. We have worked\non item prediction problems, but the framework can be used for rating\nprediction problems as well with a single change on the loss function. To\nevaluate the effect of such a framework, we have tested our approach on\nbenchmark and not yet experimented datasets. The results in these real-world\ndatasets show the superior performance of our approach in comparison with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 17:51:52 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Y\u0131ld\u0131r\u0131m", "Ezgi", ""], ["Azad", "Payam", ""], ["\u00d6\u011f\u00fcd\u00fcc\u00fc", "\u015eule G\u00fcnd\u00fcz", ""]]}, {"id": "1909.13459", "submitter": "Jie Liu", "authors": "Jie Liu, Xiao Yan, Xinyan Dai, Zhirong Li, James Cheng, Ming-Chang\n  Yang", "title": "Understanding and Improving Proximity Graph based Maximum Inner Product\n  Search", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inner-product navigable small world graph (ip-NSW) represents the\nstate-of-the-art method for approximate maximum inner product search (MIPS) and\nit can achieve an order of magnitude speedup over the fastest baseline.\nHowever, to date it is still unclear where its exceptional performance comes\nfrom. In this paper, we show that there is a strong norm bias in the MIPS\nproblem, which means that the large norm items are very likely to become the\nresult of MIPS. Then we explain the good performance of ip-NSW as matching the\nnorm bias of the MIPS problem - large norm items have big in-degrees in the\nip-NSW proximity graph and a walk on the graph spends the majority of\ncomputation on these items, thus effectively avoids unnecessary computation on\nsmall norm items. Furthermore, we propose the ip-NSW+ algorithm, which improves\nip-NSW by introducing an additional angular proximity graph. Search is first\nconducted on the angular graph to find the angular neighbors of a query and\nthen the MIPS neighbors of these angular neighbors are used to initialize the\ncandidate pool for search on the inner-product proximity graph. Experiment\nresults show that ip-NSW+ consistently and significantly outperforms ip-NSW and\nprovides more robust performance under different data distributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 05:12:49 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 14:19:53 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Liu", "Jie", ""], ["Yan", "Xiao", ""], ["Dai", "Xinyan", ""], ["Li", "Zhirong", ""], ["Cheng", "James", ""], ["Yang", "Ming-Chang", ""]]}, {"id": "1909.13762", "submitter": "Maliheh Heydarpour", "authors": "Maryam Alizadeh, Maliheh Heydarpour Shahrezaei, Farajollah\n  Tahernezhad-Javazm", "title": "Ontology Based Information Integration: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ontology makes a special vocabulary which describes the domain of interest\nand the meaning of the term on that vocabulary. Based on the precision of the\nspecification, the concept of the ontology contains several data and conceptual\nmodels. The notion of ontology has emerged into wide ranges of applications\nincluding database integration, peer-to-peer systems, e-commerce, semantic web,\netc. It can be considered as a practical tool for conceptualizing things which\nare expressed in computer format. This paper is devoted to ontology matching as\na mean or information integration. Several matching solutions have been\npresented from various areas such as databases, information systems and\nartificial intelligence. All of them take advantages of different attributes of\nontology like, structures, data instances, semantics and labels and its other\nvaluable properties. The solutions have some common techniques and cope with\nsimilar problems, but use different methods for combining and exploiting their\nresults. Information integration is among the first classes of applications at\nwhich matching was considered as a probable solution. Information integration\ncontains many fields including, data integration, schema integration, catalogue\nintegration and semantic integration. We cover these notions in term of\nontology in our proposed paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 23:19:51 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Alizadeh", "Maryam", ""], ["Shahrezaei", "Maliheh Heydarpour", ""], ["Tahernezhad-Javazm", "Farajollah", ""]]}, {"id": "1909.13765", "submitter": "Mostafa Khalaji", "authors": "Mostafa Khalaji and Chitra Dadkhah", "title": "FNHSM_HRS: Hybrid recommender system using fuzzy clustering and\n  heuristic similarity measure", "comments": "6 pages, Conference: 7th Iranian Joint Congress on Fuzzy and\n  Intelligent Systems, 18th Conference on Fuzzy Systems and 17th Conference on\n  Intelligent Systems At: Bojnord, Iran, University of Bojnord, p.p 562-568,\n  January 2019. Persian format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Recommender Systems have become a comprehensive system for helping\nand guiding users in a huge amount of data on the Internet. Collaborative\nFiltering offers to active users based on the rating of a set of users. One of\nthe simplest and most comprehensible and successful models is to find users\nwith a taste in recommender systems. In this model, with increasing number of\nusers and items, the system is faced to scalability problem. On the other hand,\nimproving system performance when there is little information available from\nratings, that is important. In this paper, a hybrid recommender system called\nFNHSM_HRS which is based on the new heuristic similarity measure (NHSM) along\nwith a fuzzy clustering is presented. Using the fuzzy clustering method in the\nproposed system improves the scalability problem and increases the accuracy of\nsystem recommendations. The proposed system is based on the collaborative\nfiltering model and is partnered with the heuristic similarity measure to\nimprove the system's performance and accuracy. The evaluation of the proposed\nsystem based results on the MovieLens dataset carried out the results using\nMAE, Recall, Precision and Accuracy measures Indicating improvement in system\nperformance and increasing the accuracy of recommendation to collaborative\nfiltering methods which use other measures to find similarities.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:27:29 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Khalaji", "Mostafa", ""], ["Dadkhah", "Chitra", ""]]}]