[{"id": "1308.0661", "submitter": "Vincent Labatut", "authors": "Samet Atda\\u{g} and Vincent Labatut", "title": "A Comparison of Named Entity Recognition Tools Applied to Biographical\n  Texts", "comments": null, "journal-ref": "2nd International Conference on Systems and Computer Science,\n  Villeneuve d'Ascq (FR), 228-233, 2013", "doi": "10.1109/IcConSCS.2013.6632052", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity recognition (NER) is a popular domain of natural language\nprocessing. For this reason, many tools exist to perform this task. Amongst\nother points, they differ in the processing method they rely upon, the entity\ntypes they can detect, the nature of the text they can handle, and their\ninput/output formats. This makes it difficult for a user to select an\nappropriate NER tool for a specific situation. In this article, we try to\nanswer this question in the context of biographic texts. For this matter, we\nfirst constitute a new corpus by annotating Wikipedia articles. We then select\npublicly available, well known and free for research NER tools for comparison:\nStanford NER, Illinois NET, OpenCalais NER WS and Alias-i LingPipe. We apply\nthem to our corpus, assess their performances and compare them. When\nconsidering overall performances, a clear hierarchy emerges: Stanford has the\nbest results, followed by LingPipe, Illionois and OpenCalais. However, a more\ndetailed evaluation performed relatively to entity types and article categories\nhighlights the fact their performances are diversely influenced by those\nfactors. This complementarity opens an interesting perspective regarding the\ncombination of these individual tools in order to improve performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 05:57:48 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Atda\u011f", "Samet", ""], ["Labatut", "Vincent", ""]]}, {"id": "1308.0701", "submitter": "Meisam Booshehri", "authors": "Meisam Booshehri, Abbas Malekpour, Peter Luksch, Kamran Zamanifar,\n  Shahdad Shariatmadari", "title": "Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text", "comments": "9 pages, International Journal of Computer Science and Information\n  Security", "journal-ref": "IJCSIS, 11(5), 64-72", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this position paper we present a new approach for discovering some special\nclasses of assertional knowledge in the text by using large RDF repositories,\nresulting in the extraction of new non-taxonomic ontological relations. Also we\nuse inductive reasoning beside our approach to make it outperform. Then, we\nprepare a case study by applying our approach on sample data and illustrate the\nsoundness of our proposed approach. Moreover in our point of view current LOD\ncloud is not a suitable base for our proposal in all informational domains.\nTherefore we figure out some directions based on prior works to enrich datasets\nof Linked Data by using web mining. The result of such enrichment can be reused\nfor further relation extraction and ontology enrichment from unstructured free\ntext documents.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 14:30:55 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Booshehri", "Meisam", ""], ["Malekpour", "Abbas", ""], ["Luksch", "Peter", ""], ["Zamanifar", "Kamran", ""], ["Shariatmadari", "Shahdad", ""]]}, {"id": "1308.0897", "submitter": "Kowcika A", "authors": "Kowcika A, Uma Maheswari, Geetha T V", "title": "Context Specific Event Model For News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new context based event indexing and event ranking model for\nNews Articles. The context event clusters formed from the UNL Graphs uses the\nmodified scoring scheme for segmenting events which is followed by clustering\nof events. From the context clusters obtained three models are developed-\nIdentification of Main and Sub events; Event Indexing and Event Ranking. Based\non the properties considered from the UNL Graphs for the modified scoring main\nevents and sub events associated with main-events are identified. The temporal\ndetails obtained from the context cluster are stored using hashmap data\nstructure. The temporal details are place-where the event took; person-who\ninvolved in that event; time-when the event took place. Based on the\ninformation collected from the context clusters three indices are generated-\nTime index, Person index, and Place index. This index gives complete details\nabout every event obtained from context clusters. A new scoring scheme is\nintroduced for ranking the events. The scoring scheme for event ranking gives\nweight-age based on the priority level of the events. The priority level\nincludes the occurrence of the event in the title of the document, event\nfrequency, and inverse document frequency of the events.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 07:22:07 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["A", "Kowcika", ""], ["Maheswari", "Uma", ""], ["T", "Geetha", "V"]]}, {"id": "1308.0971", "submitter": "Fragkiskos  Malliaros", "authors": "Fragkiskos D. Malliaros and Michalis Vazirgiannis", "title": "Clustering and Community Detection in Directed Networks: A Survey", "comments": "86 pages, 17 figures. Physics Reports Journal (To Appear)", "journal-ref": null, "doi": "10.1016/j.physrep.2013.08.002", "report-no": null, "categories": "cs.SI cs.IR physics.bio-ph physics.comp-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks (or graphs) appear as dominant structures in diverse domains,\nincluding sociology, biology, neuroscience and computer science. In most of the\naforementioned cases graphs are directed - in the sense that there is\ndirectionality on the edges, making the semantics of the edges non symmetric.\nAn interesting feature that real networks present is the clustering or\ncommunity structure property, under which the graph topology is organized into\nmodules commonly called communities or clusters. The essence here is that nodes\nof the same community are highly similar while on the contrary, nodes across\ncommunities present low similarity. Revealing the underlying community\nstructure of directed complex networks has become a crucial and\ninterdisciplinary topic with a plethora of applications. Therefore, naturally\nthere is a recent wealth of research production in the area of mining directed\ngraphs - with clustering being the primary method and tool for community\ndetection and evaluation. The goal of this paper is to offer an in-depth review\nof the methods presented so far for clustering directed networks along with the\nrelevant necessary methodological background and also related applications. The\nsurvey commences by offering a concise review of the fundamental concepts and\nmethodological base on which graph clustering algorithms capitalize on. Then we\npresent the relevant work along two orthogonal classifications. The first one\nis mostly concerned with the methodological principles of the clustering\nalgorithms, while the second one approaches the methods from the viewpoint\nregarding the properties of a good cluster in a directed network. Further, we\npresent methods and metrics for evaluating graph clustering results,\ndemonstrate interesting application domains and provide promising future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 13:12:13 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Malliaros", "Fragkiskos D.", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1308.1009", "submitter": "Ping Li", "authors": "Ping Li, Gennady Samorodnitsky, John Hopcroft", "title": "Sign Stable Projections, Sign Cauchy Projections and Chi-Square Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of stable random projections is popular for efficiently computing\nthe Lp distances in high dimension (where 0<p<=2), using small space. Because\nit adopts nonadaptive linear projections, this method is naturally suitable\nwhen the data are collected in a dynamic streaming fashion (i.e., turnstile\ndata streams). In this paper, we propose to use only the signs of the projected\ndata and analyze the probability of collision (i.e., when the two signs\ndiffer). We derive a bound of the collision probability which is exact when p=2\nand becomes less sharp when p moves away from 2. Interestingly, when p=1 (i.e.,\nCauchy random projections), we show that the probability of collision can be\naccurately approximated as functions of the chi-square similarity. For example,\nwhen the (un-normalized) data are binary, the maximum approximation error of\nthe collision probability is smaller than 0.0192. In text and vision\napplications, the chi-square similarity is a popular measure for nonnegative\ndata when the features are generated from histograms. Our experiments confirm\nthat the proposed method is promising for large-scale learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2013 15:25:51 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Li", "Ping", ""], ["Samorodnitsky", "Gennady", ""], ["Hopcroft", "John", ""]]}, {"id": "1308.1166", "submitter": "Peter Gloor", "authors": "Tobias Futterer, Peter A. Gloor, Tushar Malhotra, Harrison Mfula,\n  Karsten Packmohr, Stefan Schultheiss", "title": "WikiPulse - A News-Portal Based on Wikipedia", "comments": "Presented at COINs13 Conference, Chile, 2013 (arxiv:1308.1028)", "journal-ref": null, "doi": null, "report-no": "coins13/2013/11", "categories": "cs.IR cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more user-generated content is complementing conventional\njournalism. While we don't think that CNN or New York Times and its\nprofessional journalists will disappear anytime soon, formidable competition is\nemerging through humble Wikipedia editors. In earlier work (Becker 2012), we\nfound that entertainment and sports news appeared on average about two hours\nearlier on Wikipedia than on CNN and Reuters online. In this project we build a\nnews-reader that automatically identifies late-breaking news among the most\nrecent Wikipedia articles and then displays it on a dedicated Web site.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 03:07:02 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Futterer", "Tobias", ""], ["Gloor", "Peter A.", ""], ["Malhotra", "Tushar", ""], ["Mfula", "Harrison", ""], ["Packmohr", "Karsten", ""], ["Schultheiss", "Stefan", ""]]}, {"id": "1308.1224", "submitter": "Aleksandar Stupar", "authors": "Aleksandar Stupar and Sebastian Michel", "title": "Benchmarking Soundtrack Recommendation Systems with SRBench", "comments": "Extended version of the CIKM 2013 paper: SRbench-A Benchmark for\n  Soundtrack Recommendation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a benchmark to evaluate the retrieval performance of soundtrack\nrecommendation systems is proposed. Such systems aim at finding songs that are\nplayed as background music for a given set of images. The proposed benchmark is\nbased on preference judgments, where relevance is considered a continuous\nordinal variable and judgments are collected for pairs of songs with respect to\na query (i.e., set of images). To capture a wide variety of songs and images,\nwe use a large space of possible music genres, different emotions expressed\nthrough music, and various query-image themes. The benchmark consists of two\ntypes of relevance assessments: (i) judgments obtained from a user study, that\nserve as a \"gold standard\" for (ii) relevance judgments gathered through\nAmazon's Mechanical Turk. We report on an analysis of relevance judgments based\non different levels of user agreement and investigate the performance of two\nstate-of-the-art soundtrack recommendation systems using the proposed\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 10:10:22 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Stupar", "Aleksandar", ""], ["Michel", "Sebastian", ""]]}, {"id": "1308.1792", "submitter": "Michal Aharon", "authors": "Michal Aharon, Natalie Aizenberg, Edward Bortnikov, Ronny Lempel, Roi\n  Adadi, Tomer Benyamini, Liron Levin, Ran Roth, Ohad Serfaty", "title": "OFF-Set: One-pass Factorization of Feature Sets for Online\n  Recommendation in Persistent Cold Start Settings", "comments": "8 pages, 3 figures, a shorter version is supposed to be published in\n  RecSys13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging recommendation tasks is recommending to a new,\npreviously unseen user. This is known as the 'user cold start' problem.\nAssuming certain features or attributes of users are known, one approach for\nhandling new users is to initially model them based on their features.\n  Motivated by an ad targeting application, this paper describes an extreme\nonline recommendation setting where the cold start problem is perpetual. Every\nuser is encountered by the system just once, receives a recommendation, and\neither consumes or ignores it, registering a binary reward.\n  We introduce One-pass Factorization of Feature Sets, OFF-Set, a novel\nrecommendation algorithm based on Latent Factor analysis, which models users by\nmapping their features to a latent space. Furthermore, OFF-Set is able to model\nnon-linear interactions between pairs of features. OFF-Set is designed for\npurely online recommendation, performing lightweight updates of its model per\neach recommendation-reward observation. We evaluate OFF-Set against several\nstate of the art baselines, and demonstrate its superiority on real\nad-targeting data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 09:24:24 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Aharon", "Michal", ""], ["Aizenberg", "Natalie", ""], ["Bortnikov", "Edward", ""], ["Lempel", "Ronny", ""], ["Adadi", "Roi", ""], ["Benyamini", "Tomer", ""], ["Levin", "Liron", ""], ["Roth", "Ran", ""], ["Serfaty", "Ohad", ""]]}, {"id": "1308.1817", "submitter": "Pasi Rikhard Saari", "authors": "Pasi Saari, Tuomas Eerola", "title": "Semantic Computing of Moods Based on Tags in Social Media of Music", "comments": "Preprint, 14 pages", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, 2013", "doi": "10.1109/TKDE.2013.128", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social tags inherent in online music services such as Last.fm provide a rich\nsource of information on musical moods. The abundance of social tags makes this\ndata highly beneficial for developing techniques to manage and retrieve mood\ninformation, and enables study of the relationships between music content and\nmood representations with data substantially larger than that available for\nconventional emotion research. However, no systematic assessment has been done\non the accuracy of social tags and derived semantic models at capturing mood\ninformation in music. We propose a novel technique called Affective Circumplex\nTransformation (ACT) for representing the moods of music tracks in an\ninterpretable and robust fashion based on semantic computing of social tags and\nresearch in emotion modeling. We validate the technique by predicting listener\nratings of moods in music tracks, and compare the results to prediction with\nthe Vector Space Model (VSM), Singular Value Decomposition (SVD), Nonnegative\nMatrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA).\nThe results show that ACT consistently outperforms the baseline techniques, and\nits performance is robust against a low number of track-level mood tags. The\nresults give validity and analytical insights for harnessing millions of music\ntracks and associated mood data available through social tags in application\ndevelopment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 11:29:24 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Saari", "Pasi", ""], ["Eerola", "Tuomas", ""]]}, {"id": "1308.1847", "submitter": "Blesson Varghese", "authors": "Vu Dung Nguyen, Blesson Varghese, Adam Barker", "title": "The Royal Birth of 2013: Analysing and Visualising Public Sentiment in\n  the UK Using Twitter", "comments": "http://www.blessonv.com/research/publicsentiment/ 9 pages. Submitted\n  to IEEE BigData 2013: Workshop on Big Humanities, October 2013", "journal-ref": null, "doi": "10.1109/BigData.2013.6691669", "report-no": null, "categories": "cs.CL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of information retrieved from microblogging services such as Twitter\ncan provide valuable insight into public sentiment in a geographic region. This\ninsight can be enriched by visualising information in its geographic context.\nTwo underlying approaches for sentiment analysis are dictionary-based and\nmachine learning. The former is popular for public sentiment analysis, and the\nlatter has found limited use for aggregating public sentiment from Twitter\ndata. The research presented in this paper aims to extend the machine learning\napproach for aggregating public sentiment. To this end, a framework for\nanalysing and visualising public sentiment from a Twitter corpus is developed.\nA dictionary-based approach and a machine learning approach are implemented\nwithin the framework and compared using one UK case study, namely the royal\nbirth of 2013. The case study validates the feasibility of the framework for\nanalysis and rapid visualisation. One observation is that there is good\ncorrelation between the results produced by the popular dictionary-based\napproach and the machine learning approach when large volumes of tweets are\nanalysed. However, for rapid analysis to be possible faster methods need to be\ndeveloped using big data techniques and parallel methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 13:31:15 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2013 06:53:19 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Nguyen", "Vu Dung", ""], ["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1308.2354", "submitter": "Srijith Ravikumar", "authors": "Srijith Ravikumar, Kartik Talamadupula, Raju Balakrishnan, Subbarao\n  Kambhampati", "title": "RAProp: Ranking Tweets by Exploiting the Tweet/User/Web Ecosystem and\n  Inter-Tweet Agreement", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of Twitter renders improved trustworthiness and\nrelevance assessment of tweets much more important for search. However, given\nthe limitations on the size of tweets, it is hard to extract measures for\nranking from the tweets' content alone. We present a novel ranking method,\ncalled RAProp, which combines two orthogonal measures of relevance and\ntrustworthiness of a tweet. The first, called Feature Score, measures the\ntrustworthiness of the source of the tweet. This is done by extracting features\nfrom a 3-layer twitter ecosystem, consisting of users, tweets and the pages\nreferred to in the tweets. The second measure, called agreement analysis,\nestimates the trustworthiness of the content of the tweet, by analyzing how and\nwhether the content is independently corroborated by other tweets. We view the\ncandidate result set of tweets as the vertices of a graph, with the edges\nmeasuring the estimated agreement between each pair of tweets. The feature\nscore is propagated over this agreement graph to compute the top-k tweets that\nhave both trustworthy sources and independent corroboration. The evaluation of\nour method on 16 million tweets from the TREC 2011 Microblog Dataset shows that\nfor top-30 precision we achieve 53% higher than current best performing method\non the Dataset and over 300% over current Twitter Search. We also present a\ndetailed internal empirical evaluation of RAProp in comparison to several\nalternative approaches proposed by us.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 00:56:59 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Ravikumar", "Srijith", ""], ["Talamadupula", "Kartik", ""], ["Balakrishnan", "Raju", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1308.2359", "submitter": "Arun Maiya", "authors": "Arun S. Maiya, John P. Thompson, Francisco Loaiza-Lemos, Robert M.\n  Rolfe", "title": "Exploratory Analysis of Highly Heterogeneous Document Collections", "comments": "9 pages; KDD 2013: 19th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an effective multifaceted system for exploratory analysis of\nhighly heterogeneous document collections. Our system is based on intelligently\ntagging individual documents in a purely automated fashion and exploiting these\ntags in a powerful faceted browsing framework. Tagging strategies employed\ninclude both unsupervised and supervised approaches based on machine learning\nand natural language processing. As one of our key tagging strategies, we\nintroduce the KERA algorithm (Keyword Extraction for Reports and Articles).\nKERA extracts topic-representative terms from individual documents in a purely\nunsupervised fashion and is revealed to be significantly more effective than\nstate-of-the-art methods. Finally, we evaluate our system in its ability to\nhelp users locate documents pertaining to military critical technologies buried\ndeep in a large heterogeneous sea of information.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2013 02:28:11 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Maiya", "Arun S.", ""], ["Thompson", "John P.", ""], ["Loaiza-Lemos", "Francisco", ""], ["Rolfe", "Robert M.", ""]]}, {"id": "1308.2853", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Daniel Hsu, Majid Janzamin, Sham Kakade", "title": "When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor\n  Tucker Decompositions with Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcomplete latent representations have been very popular for unsupervised\nfeature learning in recent years. In this paper, we specify which overcomplete\nmodels can be identified given observable moments of a certain order. We\nconsider probabilistic admixture or topic models in the overcomplete regime,\nwhere the number of latent topics can greatly exceed the size of the observed\nword vocabulary. While general overcomplete topic models are not identifiable,\nwe establish generic identifiability under a constraint, referred to as topic\npersistence. Our sufficient conditions for identifiability involve a novel set\nof \"higher order\" expansion conditions on the topic-word matrix or the\npopulation structure of the model. This set of higher-order expansion\nconditions allow for overcomplete models, and require the existence of a\nperfect matching from latent topics to higher order observed words. We\nestablish that random structured topic models are identifiable w.h.p. in the\novercomplete regime. Our identifiability results allows for general\n(non-degenerate) distributions for modeling the topic proportions, and thus, we\ncan handle arbitrarily correlated topics in our framework. Our identifiability\nresults imply uniqueness of a class of tensor decompositions with structured\nsparsity which is contained in the class of Tucker decompositions, but is more\ngeneral than the Candecomp/Parafac (CP) decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 13:16:10 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Janzamin", "Majid", ""], ["Kakade", "Sham", ""]]}, {"id": "1308.3059", "submitter": "Wei Zeng", "authors": "Wei Zeng, An Zeng, Ming-Sheng Shang and Yi-Cheng Zhang", "title": "Membership in social networks and the application in information\n  filtering", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": "10.1140/epjb/e2013-40258-1", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past a few years, users' membership in the online system (i.e. the\nsocial groups that online users joined) are wildly investigated. Most of these\nworks focus on the detection, formulation and growth of online communities. In\nthis paper, we study users' membership in a coupled system which contains\nuser-group and user-object bipartite networks. By linking users' membership\ninformation and their object selection, we find that the users who have\ncollected only a few objects are more likely to be \"influenced\" by the\nmembership when choosing objects. Moreover, we observe that some users may join\nmany online communities though they collected few objects. Based on these\nfindings, we design a social diffusion recommendation algorithm which can\neffectively solve the user cold-start problem. Finally, we propose a\npersonalized combination of our method and the hybrid method in [PNAS 107, 4511\n(2010)], which leads to a further improvement in the overall recommendation\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 08:29:34 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Zeng", "Wei", ""], ["Zeng", "An", ""], ["Shang", "Ming-Sheng", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1308.3060", "submitter": "Wei Zeng", "authors": "Wei Zeng, An Zeng, Ming-Sheng Shang and Yi-Cheng Zhang", "title": "Information filtering in sparse online systems: recommendation via\n  semi-local diffusion", "comments": "8 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0079354", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of the Internet and overwhelming amount of information\nand choices that people are confronted with, recommender systems have been\ndeveloped to effectively support users' decision-making process in the online\nsystems. However, many recommendation algorithms suffer from the data sparsity\nproblem, i.e. the user-object bipartite networks are so sparse that algorithms\ncannot accurately recommend objects for users. This data sparsity problem makes\nmany well-known recommendation algorithms perform poorly. To solve the problem,\nwe propose a recommendation algorithm based on the semi-local diffusion process\non a user-object bipartite network. The numerical simulation on two sparse\ndatasets, Amazon and Bookcross, show that our method significantly outperforms\nthe state-of-the-art methods especially for those small-degree users. Two\npersonalized semi-local diffusion methods are proposed which further improve\nthe recommendation accuracy. Finally, our work indicates that sparse online\nsystems are essentially different from the dense online systems, all the\nalgorithms and conclusions based on dense data should be rechecked again in\nsparse data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 08:29:41 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Zeng", "Wei", ""], ["Zeng", "An", ""], ["Shang", "Ming-Sheng", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1308.3177", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Dept Electrical and Comput. Engin., Drexel Univ.),\n  P.M.B. Vitanyi (CWI and Comput. Sci., Univ. Amsterdam)", "title": "Normalized Google Distance of Multisets with Applications", "comments": "25 pages, LaTeX, 3 figures/tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized Google distance (NGD) is a relative semantic distance based on the\nWorld Wide Web (or any other large electronic database, for instance Wikipedia)\nand a search engine that returns aggregate page counts. The earlier NGD between\npairs of search terms (including phrases) is not sufficient for all\napplications. We propose an NGD of finite multisets of search terms that is\nbetter for many applications. This gives a relative semantics shared by a\nmultiset of search terms. We give applications and compare the results with\nthose obtained using the pairwise NGD. The derivation of NGD method is based on\nKolmogorov complexity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 17:04:15 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Cohen", "Andrew R.", "", "Dept Electrical and Comput. Engin., Drexel Univ."], ["Vitanyi", "P. M. B.", "", "CWI and Comput. Sci., Univ. Amsterdam"]]}, {"id": "1308.3225", "submitter": "Mohamed Ben Halima", "authors": "M. Ben Halima, M. Hamroun, S. Ben Moussa and A. M. Alimi", "title": "An interactive engine for multilingual video browsing using semantic\n  content", "comments": "4 pages, IGS 2013 Conference; IGS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of audio-visual information has increased dramatically with the\nadvent of High Speed Internet. Furthermore, technological advances in recent\nyears in the field of information technology, have simplified the use of video\ndata in various fields by the general public. This made it possible to store\nlarge collections of video documents into computer systems. To enable efficient\nuse of these collections, it is necessary to develop tools to facilitate access\nto these documents and handling them. In this paper we propose a method for\nindexing and retrieval of video sequences in a video database of large\ndimension, based on a weighting technique to calculate the degree of membership\nof a concept in a video also a structuring of the data of the audio-visual\n(context / concept / video) and a relevance feedback mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 19:54:11 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Halima", "M. Ben", ""], ["Hamroun", "M.", ""], ["Moussa", "S. Ben", ""], ["Alimi", "A. M.", ""]]}, {"id": "1308.3554", "submitter": "Yoshihisa Udagawa Dr", "authors": "Yoshihisa Udagawa", "title": "Source Code Retrieval Using Sequence Based Similarity", "comments": "16 pages, 6 figures, 3 tables", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.4, July 2013, pp.57-74", "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplicated code has a negative impact on the quality of software systems and\nshould be detected at least. In this paper, we discuss an approach that\nimproves source code retrieval using the structural information about the\nprograms. We developed a lexical parser to extract control statements and\nmethod identifiers from Java programs. We propose a similarity measure that is\ndefined by the ratio of the number of sequentially full matching statements to\nthe number of sequentially partial matching ones. The similarity measure is\nconsidered to be an extension of a set based similarity index, e.g.,\nSorensen-Dice index. Our key contribution of this research is the development\nof a similarity retrieval algorithm that derives meaningful search conditions\nfrom a given sequence, and then performs retrieval using all of the derived\nconditions. Experiments show that our retrieval model outperforms the other\nretrieval models up to 90.9% in the number of retrieved methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 05:07:44 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Udagawa", "Yoshihisa", ""]]}, {"id": "1308.3876", "submitter": "Sanjay Singh", "authors": "Jnanamurthy HK and Sanjay Singh", "title": "Detection and Filtering of Collaborative Malicious Users in Reputation\n  System using Quality Repository Approach", "comments": "14 pages, 5 figures, 5 tables, submitted to ICACCI 2013, Mysore,\n  india", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reputation system is gaining popularity as it helps a user to be sure\nabout the quality of a product/service he wants to buy. Nonetheless online\nreputation system is not immune from attack. Dealing with malicious ratings in\nreputation systems has been recognized as an important but difficult task. This\nproblem is challenging when the number of true user's ratings is relatively\nsmall and unfair ratings plays majority in rated values. In this paper, we have\nproposed a new method to find malicious users in online reputation systems\nusing Quality Repository Approach (QRA). We mainly concentrated on anomaly\ndetection in both rating values and the malicious users. QRA is very efficient\nto detect malicious user ratings and aggregate true ratings. The proposed\nreputation system has been evaluated through simulations and it is concluded\nthat the QRA based system significantly reduces the impact of unfair ratings\nand improve trust on reputation score with lower false positive as compared to\nother method used for the purpose.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2013 17:34:46 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["HK", "Jnanamurthy", ""], ["Singh", "Sanjay", ""]]}, {"id": "1308.4648", "submitter": "Robert Bridges", "authors": "Nikki McNeil, Robert A. Bridges, Michael D. Iannacone, Bogdan Czejdo,\n  Nicolas Perez, John R. Goodall", "title": "PACE: Pattern Accurate Computationally Efficient Bootstrapping for\n  Timely Discovery of Cyber-Security Concepts", "comments": "6 pages, 3 figures, ieeeTran conference. International Conference on\n  Machine Learning and Applications 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public disclosure of important security information, such as knowledge of\nvulnerabilities or exploits, often occurs in blogs, tweets, mailing lists, and\nother online sources months before proper classification into structured\ndatabases. In order to facilitate timely discovery of such knowledge, we\npropose a novel semi-supervised learning algorithm, PACE, for identifying and\nclassifying relevant entities in text sources. The main contribution of this\npaper is an enhancement of the traditional bootstrapping method for entity\nextraction by employing a time-memory trade-off that simultaneously circumvents\na costly corpus search while strengthening pattern nomination, which should\nincrease accuracy. An implementation in the cyber-security domain is discussed\nas well as challenges to Natural Language Processing imposed by the security\ndomain.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 18:01:42 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 18:07:22 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 14:13:26 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["McNeil", "Nikki", ""], ["Bridges", "Robert A.", ""], ["Iannacone", "Michael D.", ""], ["Czejdo", "Bogdan", ""], ["Perez", "Nicolas", ""], ["Goodall", "John R.", ""]]}, {"id": "1308.4839", "submitter": "Benjamin Piwowarski", "authors": "Zeynep Pehlivan and Benjamin Piwowarski and St\\'ephane Gan\\c{c}arski", "title": "Diversification Based Static Index Pruning - Application to Temporal\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, web archives preserve the history of large portions of the web. As\nmedias are shifting from printed to digital editions, accessing these huge\ninformation sources is drawing increasingly more attention from national and\ninternational institutions, as well as from the research community. These\ncollections are intrinsically big, leading to index files that do not fit into\nthe memory and an increase query response time. Decreasing the index size is a\ndirect way to decrease this query response time.\n  Static index pruning methods reduce the size of indexes by removing a part of\nthe postings. In the context of web archives, it is necessary to remove\npostings while preserving the temporal diversity of the archive. None of the\nexisting pruning approaches take (temporal) diversification into account.\n  In this paper, we propose a diversification-based static index pruning\nmethod. It differs from the existing pruning approaches by integrating\ndiversification within the pruning context. We aim at pruning the index while\npreserving retrieval effectiveness and diversity by pruning while maximizing a\ngiven IR evaluation metric like DCG. We show how to apply this approach in the\ncontext of web archives. Finally, we show on two collections that search\neffectiveness in temporal collections after pruning can be improved using our\napproach rather than diversity oblivious approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 12:09:54 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Pehlivan", "Zeynep", ""], ["Piwowarski", "Benjamin", ""], ["Gan\u00e7arski", "St\u00e9phane", ""]]}, {"id": "1308.4941", "submitter": "Robert Bridges", "authors": "Robert A. Bridges, Corinne L. Jones, Michael D. Iannacone, Kelly M.\n  Testa, John R. Goodall", "title": "Automatic Labeling for Entity Extraction in Cyber Security", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely analysis of cyber-security information necessitates automated\ninformation extraction from unstructured text. While state-of-the-art\nextraction methods produce extremely accurate results, they require ample\ntraining data, which is generally unavailable for specialized applications,\nsuch as detecting security related entities; moreover, manual annotation of\ncorpora is very costly and often not a viable solution. In response, we develop\na very precise method to automatically label text from several data sources by\nleveraging related, domain-specific, structured data and provide public access\nto a corpus annotated with cyber-security entities. Next, we implement a\nMaximum Entropy Model trained with the average perceptron on a portion of our\ncorpus ($\\sim$750,000 words) and achieve near perfect precision, recall, and\naccuracy, with training times under 17 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 18:23:25 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 18:45:43 GMT"}, {"version": "v3", "created": "Mon, 9 Jun 2014 23:51:25 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Bridges", "Robert A.", ""], ["Jones", "Corinne L.", ""], ["Iannacone", "Michael D.", ""], ["Testa", "Kelly M.", ""], ["Goodall", "John R.", ""]]}, {"id": "1308.5273", "submitter": "Luca de Alfaro", "authors": "Luca de Alfaro and Michael Shavlovsky", "title": "CrowdGrader: Crowdsourcing the Evaluation of Homework Assignments", "comments": "Technical Report UCSC-SOE-13-11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing offers a practical method for ranking and scoring large amounts\nof items. To investigate the algorithms and incentives that can be used in\ncrowdsourcing quality evaluations, we built CrowdGrader, a tool that lets\nstudents submit and collaboratively grade solutions to homework assignments. We\npresent the algorithms and techniques used in CrowdGrader, and we describe our\nresults and experience in using the tool for several computer-science\nassignments.\n  CrowdGrader combines the student-provided grades into a consensus grade for\neach submission using a novel crowdsourcing algorithm that relies on a\nreputation system. The algorithm iterativerly refines inter-dependent estimates\nof the consensus grades, and of the grading accuracy of each student. On\nsynthetic data, the algorithm performs better than alternatives not based on\nreputation. On our preliminary experimental data, the performance seems\ndependent on the nature of review errors, with errors that can be ascribed to\nthe reviewer being more tractable than those arising from random external\nevents. To provide an incentive for reviewers, the grade each student receives\nin an assignment is a combination of the consensus grade received by their\nsubmissions, and of a reviewing grade capturing their reviewing effort and\naccuracy. This incentive worked well in practice.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 01:23:21 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["de Alfaro", "Luca", ""], ["Shavlovsky", "Michael", ""]]}, {"id": "1308.5275", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "The Lovasz-Bregman Divergence and connections to rank aggregation,\n  clustering, and web ranking", "comments": "18 pages. A shorter version appeared in Proc. Uncertainty in\n  Artificial Intelligence (UAI)-2013, Bellevue, WA", "journal-ref": "UAI-2013", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the recently introduced theory of Lovasz-Bregman (LB) divergences\n(Iyer & Bilmes, 2012) in several ways. We show that they represent a distortion\nbetween a 'score' and an 'ordering', thus providing a new view of rank\naggregation and order based clustering with interesting connections to web\nranking. We show how the LB divergences have a number of properties akin to\nmany permutation based metrics, and in fact have as special cases forms very\nsimilar to the Kendall-$\\tau$ metric. We also show how the LB divergences\nsubsume a number of commonly used ranking measures in information retrieval,\nlike the NDCG and AUC. Unlike the traditional permutation based metrics,\nhowever, the LB divergence naturally captures a notion of \"confidence\" in the\norderings, thus providing a new representation to applications involving\naggregating scores as opposed to just orderings. We show how a number of\nrecently used web ranking models are forms of Lovasz-Bregman rank aggregation\nand also observe that a natural form of Mallow's model using the LB divergence\nhas been used as conditional ranking models for the 'Learning to Rank' problem.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 01:31:22 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1308.5286", "submitter": "Sabir Ribas", "authors": "Sabir Ribas, Berthier Ribeiro-Neto, Edmundo de Souza e Silva, Nivio\n  Ziviani", "title": "R-Score: Reputation-based Scoring of Research Groups", "comments": "23 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To manage the problem of having a higher demand for resources than\navailability of funds, research funding agencies usually rank the major\nresearch groups in their area of knowledge. This ranking relies on a careful\nanalysis of the research groups in terms of their size, number of PhDs\ngraduated, research results and their impact, among other variables. While\nresearch results are not the only variable to consider, they are frequently\ngiven special attention because of the notoriety they confer to the researchers\nand the programs they are affiliated with. In here we introduce a new metric\nfor quantifying publication output, called R-Score for reputation-based score,\nwhich can be used in support to the ranking of research groups or programs. The\nnovelty is that the metric depends solely on the listings of the publications\nof the members of a group, with no dependency on citation counts. R-Score has\nsome interesting properties: (a) it does not require access to the contents of\npublished material, (b) it can be curated to produce highly accurate results,\nand (c) it can be naturally used to compare publication output of research\ngroups (e.g., graduate programs) inside a same country, geographical area, or\nacross the world. An experiment comparing the publication output of 25 CS\ngraduate programs from Brazil suggests that R-Score can be quite useful for\nproviding early insights into the publication patterns of the various research\ngroups one wants to compare.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 03:22:03 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 21:51:03 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Ribas", "Sabir", ""], ["Ribeiro-Neto", "Berthier", ""], ["Silva", "Edmundo de Souza e", ""], ["Ziviani", "Nivio", ""]]}, {"id": "1308.6074", "submitter": "Sohan Seth", "authors": "Sohan Seth, Niko V\\\"alim\\\"aki, Samuel Kaski, Antti Honkela", "title": "Exploration and retrieval of whole-metagenome sequencing samples", "comments": "16 pages; additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Over the recent years, the field of whole metagenome shotgun sequencing has\nwitnessed significant growth due to the high-throughput sequencing technologies\nthat allow sequencing genomic samples cheaper, faster, and with better coverage\nthan before. This technical advancement has initiated the trend of sequencing\nmultiple samples in different conditions or environments to explore the\nsimilarities and dissimilarities of the microbial communities. Examples include\nthe human microbiome project and various studies of the human intestinal tract.\nWith the availability of ever larger databases of such measurements, finding\nsamples similar to a given query sample is becoming a central operation. In\nthis paper, we develop a content-based exploration and retrieval method for\nwhole metagenome sequencing samples. We apply a distributed string mining\nframework to efficiently extract all informative sequence $k$-mers from a pool\nof metagenomic samples and use them to measure the dissimilarity between two\nsamples. We evaluate the performance of the proposed approach on two human gut\nmetagenome data sets as well as human microbiome project metagenomic samples.\nWe observe significant enrichment for diseased gut samples in results of\nqueries with another diseased sample and very high accuracy in discriminating\nbetween different body sites even though the method is unsupervised. A software\nimplementation of the DSM framework is available at\nhttps://github.com/HIITMetagenomics/dsm-framework\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 07:28:35 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 09:57:56 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Seth", "Sohan", ""], ["V\u00e4lim\u00e4ki", "Niko", ""], ["Kaski", "Samuel", ""], ["Honkela", "Antti", ""]]}, {"id": "1308.6118", "submitter": "Sorin Alupoaie", "authors": "Sorin Alupoaie, P\\'adraig Cunningham", "title": "Using tf-idf as an edge weighting scheme in user-object bipartite\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite user-object networks are becoming increasingly popular in\nrepresenting user interaction data in a web or e-commerce environment. They\nhave certain characteristics and challenges that differentiates them from other\nbipartite networks. This paper analyzes the properties of five real world\nuser-object networks. In all cases we found a heavy tail object degree\ndistribution with popular objects connecting together a large part of the users\ncausing significant edge inflation in the projected users network. We propose a\nnovel edge weighting strategy based on tf-idf and show that the new scheme\nimproves both the density and the quality of the community structure in the\nprojections. The improvement is also noticed when comparing to partially random\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 10:25:02 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Alupoaie", "Sorin", ""], ["Cunningham", "P\u00e1draig", ""]]}]