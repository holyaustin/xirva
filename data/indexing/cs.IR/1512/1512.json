[{"id": "1512.00126", "submitter": "Alex Zhavoronkov", "authors": "Yuri Nikolsky, Roman Gurinovich, Oleg Kuryan, Aleksandr Pashuk, Alexej\n  Scherbakov, Konstantin Romantsov, Leslie C. Jellen, Alex Zhavoronkov", "title": "GrantMed: a new, international system for tracking grants and funding\n  trends in the life sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of PubMed and other search engines in managing the\nmassive volume of biomedical literature and the retrieval of individual\npublications, grant-related data remains scattered and relatively inaccessible.\nThis is problematic, as project and funding data has significant analytical\nvalue and could be integral to publication retrieval. Here, we introduce\nGrantMed, a searchable international database of biomedical grants that\nintegrates some 20 million publications with the nearly 1.4 million research\nprojects and 650 billion dollars of funding that made them possible. For any\ngiven topic in the life sciences, Grantmed provides instantaneous visualization\nof the past 30 years of dollars spent and projects awarded, along with detailed\nindividual project descriptions, funding amounts, and links to investigators,\nresearch organizations, and resulting publications. It summarizes trends in\nfunding and publication rates for areas of interest and merges data from\nvarious national grant databases to create one international grant tracking\nsystem. This information will benefit the research community and funding\nentities alike. Users can view trends over time or current projects underway\nand use this information to navigate the decision-making process in moving\nforward. They can view projects prior to publication and records of previous\nprojects. Convenient access to this data for analytical purposes will be\nbeneficial in many ways, helping to prevent project overlap, reduce funding\nredundancy, identify areas of success, accelerate dissemination of ideas, and\nexpose knowledge gaps in moving forward. It is our hope that this will be a\ncentral resource for international life sciences research communities and the\nfunding organizations that support them, ultimately streamlining progress.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 03:06:47 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Nikolsky", "Yuri", ""], ["Gurinovich", "Roman", ""], ["Kuryan", "Oleg", ""], ["Pashuk", "Aleksandr", ""], ["Scherbakov", "Alexej", ""], ["Romantsov", "Konstantin", ""], ["Jellen", "Leslie C.", ""], ["Zhavoronkov", "Alex", ""]]}, {"id": "1512.00198", "submitter": "Sylvain Peyronnet", "authors": "Thomas Largillier, Guillaume Peyronnet, Sylvain Peyronnet", "title": "Efficient filtering of adult content using textual information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays adult content represents a non negligible proportion of the Web\ncontent. It is of the utmost importance to protect children from this content.\nSearch engines, as an entry point for Web navigation are ideally placed to deal\nwith this issue.\n  In this paper, we propose a method that builds a safe index i.e.\nadult-content free for search engines. This method is based on a filter that\nuses only textual information from the web page and the associated URL.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 09:57:03 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 10:12:53 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Largillier", "Thomas", ""], ["Peyronnet", "Guillaume", ""], ["Peyronnet", "Sylvain", ""]]}, {"id": "1512.00228", "submitter": "Yunior Ram\\'irez-Cruz", "authors": "Henry Rosales-M\\'endez, Yunior Ram\\'irez-Cruz", "title": "MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering\n  Algorithms", "comments": null, "journal-ref": "Pattern Recognition Letters 84 (2016) 142-148", "doi": "10.1016/j.patrec.2016.09.002", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validation of biclustering algorithms remains a challenging task, even\nthough a number of measures have been proposed for evaluating the quality of\nthese algorithms. Although no criterion is universally accepted as the overall\nbest, a number of meta-evaluation conditions to be satisfied by biclustering\nalgorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$,\na new external measure for evaluating biclusterings, in the scenario where gold\nstandard annotations are available for both the object clusters and the\nassociated feature subspaces. Our proposal relies on the so-called\nmicro-objects transformation and satisfies the most comprehensive set of\nmeta-evaluation conditions so far enunciated for biclusterings. Additionally,\nthe proposed measure adequately handles the occurrence of overlapping in both\nthe object and feature spaces. Moreover, when used for evaluating traditional\nclusterings, which are viewed as a particular case of biclustering, the\nproposed measure also satisfies the most comprehensive set of meta-evaluation\nconditions so far enunciated for this task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 11:26:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 11:35:22 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Rosales-M\u00e9ndez", "Henry", ""], ["Ram\u00edrez-Cruz", "Yunior", ""]]}, {"id": "1512.00442", "submitter": "Ke Li", "authors": "Ke Li, Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "comments": "13 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2016. This version corrects a typo in the pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:53:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:47:10 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 06:51:49 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1512.00576", "submitter": "Derwin Suhartono", "authors": "Derwin Suhartono", "title": "Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen\n  Teks Berbahasa Indonesia", "comments": "17 pages, 6 figures, 3 tables, Technical Report Program Studi Doktor\n  Ilmu Komputer Universitas Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One task that is included in managing documents is how to find substantial\ninformation inside. Topic modeling is a technique that has been developed to\nproduce document representation in form of keywords. The keywords will be used\nin the indexing process and document retrieval as needed by users. In this\nresearch, we will discuss specifically about Probabilistic Latent Semantic\nAnalysis (PLSA). It will cover PLSA mechanism which involves Expectation\nMaximization (EM) as the training algorithm, how to conduct testing, and obtain\nthe accuracy result.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:41:58 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Suhartono", "Derwin", ""]]}, {"id": "1512.00578", "submitter": "Derwin Suhartono", "authors": "Derwin Suhartono", "title": "Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk\n  Esai Argumentatif", "comments": "16 pages, 3 figures, 2 tables, Technical Report Program Studi Doktor\n  Ilmu Komputer Universitas Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By automatically recognize argument component, essay writers can do some\ninspections to texts that they have written. It will assist essay scoring\nprocess objectively and precisely because essay grader is able to see how well\nthe argument components are constructed. Some reseachers have tried to do\nargument detection and classification along with its implementation in some\ndomains. The common approach is by doing feature extraction to the text.\nGenerally, the features are structural, lexical, syntactic, indicator, and\ncontextual. In this research, we add new feature to the existing features. It\nadopts keywords list by Knott and Dale (1993). The experiment result shows the\nargument classification achieves 72.45% accuracy. Moreover, we still get the\nsame accuracy without the keyword lists. This concludes that the keyword lists\ndo not affect significantly to the features. All features are still weak to\nclassify major claim and claim, so we need other features which are useful to\ndifferentiate those two kind of argument components.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:58:38 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Suhartono", "Derwin", ""]]}, {"id": "1512.00765", "submitter": "Cedric De Boom", "authors": "Cedric De Boom, Steven Van Canneyt, Steven Bohez, Thomas Demeester,\n  Bart Dhoedt", "title": "Learning Semantic Similarity for Very Short Texts", "comments": "6 pages, 5 figures, 3 tables, ReLSD workshop at ICDM 15", "journal-ref": null, "doi": "10.1109/ICDMW.2015.86", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levering data on social media, such as Twitter and Facebook, requires\ninformation retrieval algorithms to become able to relate very short text\nfragments to each other. Traditional text similarity methods such as tf-idf\ncosine-similarity, based on word overlap, mostly fail to produce good results\nin this case, since word overlap is little or non-existent. Recently,\ndistributed word representations, or word embeddings, have been shown to\nsuccessfully allow words to match on the semantic level. In order to pair short\ntext fragments - as a concatenation of separate words - an adequate distributed\nsentence representation is needed, in existing literature often obtained by\nnaively combining the individual word representations. We therefore\ninvestigated several text representations as a combination of word embeddings\nin the context of semantic pair matching. This paper investigates the\neffectiveness of several such naive techniques, as well as traditional tf-idf\nsimilarity, for fragments of different lengths. Our main contribution is a\nfirst step towards a hybrid method that combines the strength of dense\ndistributed representations - as opposed to sparse term matching - with the\nstrength of tf-idf based methods to automatically reduce the impact of less\ninformative terms. Our new approach outperforms the existing techniques in a\ntoy experimental set-up, leading to the conclusion that the combination of word\nembeddings and tf-idf information might lead to a better model for semantic\ncontent within very short text fragments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 16:31:20 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["De Boom", "Cedric", ""], ["Van Canneyt", "Steven", ""], ["Bohez", "Steven", ""], ["Demeester", "Thomas", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1512.00907", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Innovation Pursuit: A New Approach to Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749206", "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In subspace clustering, a group of data points belonging to a union of\nsubspaces are assigned membership to their respective subspaces. This paper\npresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of\nsubspace clustering using a new geometrical idea whereby subspaces are\nidentified based on their relative novelties. We present two frameworks in\nwhich the idea of innovation pursuit is used to distinguish the subspaces.\nUnderlying the first framework is an iterative method that finds the subspaces\nconsecutively by solving a series of simple linear optimization problems, each\nsearching for a direction of innovation in the span of the data potentially\northogonal to all subspaces except for the one to be identified in one step of\nthe algorithm. A detailed mathematical analysis is provided establishing\nsufficient conditions for iPursuit to correctly cluster the data. The proposed\napproach can provably yield exact clustering even when the subspaces have\nsignificant intersections. It is shown that the complexity of the iterative\napproach scales only linearly in the number of data points and subspaces, and\nquadratically in the dimension of the subspaces. The second framework\nintegrates iPursuit with spectral clustering to yield a new variant of\nspectral-clustering-based algorithms. The numerical simulations with both real\nand synthetic data demonstrate that iPursuit can often outperform the\nstate-of-the-art subspace clustering algorithms, more so for subspaces with\nsignificant intersections, and that it significantly improves the\nstate-of-the-art result for subspace-segmentation-based face clustering.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:52:43 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 05:26:58 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 23:12:17 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 03:29:06 GMT"}, {"version": "v5", "created": "Sun, 26 Nov 2017 15:24:33 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1512.01043", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar, Dhiren Patel", "title": "Approaches for Sentiment Analysis on Twitter: A State-of-Art study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Microbloging is an extremely prevalent broadcast medium amidst the Internet\nfraternity these days. People share their opinions and sentiments about variety\nof subjects like products, news, institutions, etc., every day on microbloging\nwebsites. Sentiment analysis plays a key role in prediction systems, opinion\nmining systems, etc. Twitter, one of the microbloging platforms allows a limit\nof 140 characters to its users. This restriction stimulates users to be very\nconcise about their opinion and twitter an ocean of sentiments to analyze.\nTwitter also provides developer friendly streaming API for data retrieval\npurpose allowing the analyst to search real time tweets from various users. In\nthis paper, we discuss the state-of-art of the works which are focused on\nTwitter, the online social network platform, for sentiment analysis. We survey\nvarious lexical, machine learning and hybrid approaches for sentiment analysis\non Twitter.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 11:29:36 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Thakkar", "Harsh", ""], ["Patel", "Dhiren", ""]]}, {"id": "1512.01587", "submitter": "Sahil Garg", "authors": "Sahil Garg, Aram Galstyan, Ulf Hermjakob, and Daniel Marcu", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of\n  Biomedical Text", "comments": "Appearing in Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence (AAAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance the state of the art in biomolecular interaction extraction with\nthree contributions: (i) We show that deep, Abstract Meaning Representations\n(AMR) significantly improve the accuracy of a biomolecular interaction\nextraction system when compared to a baseline that relies solely on surface-\nand syntax-based features; (ii) In contrast with previous approaches that infer\nrelations on a sentence-by-sentence basis, we expand our framework to enable\nconsistent predictions over sets of sentences (documents); (iii) We further\nmodify and expand a graph kernel learning framework to enable concurrent\nexploitation of automatically induced AMR (semantic) and dependency structure\n(syntactic) representations. Our experiments show that our approach yields\ninteraction extraction systems that are more robust in environments where there\nis a significant mismatch between training and test conditions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 22:58:29 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Hermjakob", "Ulf", ""], ["Marcu", "Daniel", ""]]}, {"id": "1512.02009", "submitter": "Bei Chen", "authors": "Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling document structure is of great importance for discourse analysis and\nrelated applications. The goal of this research is to capture the document\nintent structure by modeling documents as a mixture of topic words and\nrhetorical words. While the topics are relatively unchanged through one\ndocument, the rhetorical functions of sentences usually change following\ncertain orders in discourse. We propose GMM-LDA, a topic modeling based\nBayesian unsupervised model, to analyze the document intent structure\ncooperated with order information. Our model is flexible that has the ability\nto combine the annotations and do supervised learning. Additionally, entropic\nregularization can be introduced to model the significant divergence between\ntopics and intents. We perform experiments in both unsupervised and supervised\nsettings, results show the superiority of our model over several\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:16:58 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Bei", ""], ["Zhu", "Jun", ""], ["Yang", "Nan", ""], ["Tian", "Tian", ""], ["Zhou", "Ming", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.02573", "submitter": "Nour El-Mawass", "authors": "Nour El-Mawass, Saad Alaboodi", "title": "Hunting for Spammers: Detecting Evolved Spammers on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Once an email problem, spam has nowadays branched into new territories with\ndisruptive effects. In particular, spam has established itself over the recent\nyears as a ubiquitous, annoying, and sometimes threatening aspect of online\nsocial networks. Due to its prevalent existence, many works have tackled spam\non Twitter from different angles. Spam is, however, a moving target. The new\ngeneration of spammers on Twitter has evolved into online creatures that are\nnot easily recognizable by old detection systems. With the strong tangled\nspamming community, automatic tweeting scripts, and the ability to massively\ncreate Twitter accounts with a negligible cost, spam on Twitter is becoming\nsmarter, fuzzier and harder to detect. Our own analysis of spam content on\nArabic trending hashtags in Saudi Arabia results in an estimate of about three\nquarters of the total generated content. This alarming rate makes the\ndevelopment of adaptive spam detection techniques a very real and pressing\nneed. In this paper, we analyze the spam content of trending hashtags on Saudi\nTwitter, and assess the performance of previous spam detection systems on our\nrecently gathered dataset. Due to the escalating manipulation that\ncharacterizes newer spamming accounts, simple manual labeling currently leads\nto inaccurate results. In order to get reliable ground-truth data, we propose\nan updated manual classification algorithm that avoids the deficiencies of\nolder manual approaches. We also adapt the previously proposed features to\nrespond to spammers evading techniques, and use these features to build a new\ndata-driven detection system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 18:21:31 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 21:53:18 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["El-Mawass", "Nour", ""], ["Alaboodi", "Saad", ""]]}, {"id": "1512.03165", "submitter": "Eissa Alshari", "authors": "Eissa M. Alshari", "title": "Semantic Arabic Information Retrieval Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous increasing in the amount of the published and stored\ninformation requires a special Information Retrieval (IR) frameworks to search\nand get information accurately and speedily. Currently, keywords-based\ntechniques are commonly used in information retrieval. However, a major\ndrawback of the keywords approach is its inability of handling the polysemy and\nsynonymy phenomenon of the natural language. For instance, the meanings of\nwords and understanding of concepts differ in different communities. Same word\nuse for different concepts (polysemy) or use different words for the same\nconcept (synonymy). Most of information retrieval frameworks have a weakness to\ndeal with the semantics of the words in term of (indexing, Boolean model,\nLatent Semantic Analysis (LSA) , Latent semantic Index (LSI) and semantic\nranking, etc.). Traditional Arabic Information Retrieval (AIR) models\nperformance insufficient with semantic queries, which deal with not only the\nkeywords but also with the context of these keywords. Therefore, there is a\nneed for a semantic information retrieval model with a semantic index structure\nand ranking algorithm based on semantic index.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 08:10:49 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Alshari", "Eissa M.", ""]]}, {"id": "1512.03167", "submitter": "Eissa Alshari", "authors": "Emad Elabd, Eissa Alshari, and Hatem Abdulkader", "title": "Semantic Boolean Arabic Information Retrieval", "comments": "in The International Arab Journal of Information Technology, Vol. 12,\n  No. 3, May 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic language is one of the most widely spoken languages. This language has\na complex morphological structure and is considered as one of the most prolific\nlanguages in terms of article linguistic. Therefore, Arabic Information\nRetrieval (AIR) models need specific techniques to deal with this complex\nmorphological structure. This paper aims to develop an integrate AIR\nframeworks. It lists and analysis the different Information Retrieval (IR)\nmethods and techniques such as query processing, stemming and indexing which\nare used in AIR systems. We conclude that AIR frameworks have a weakness to\ndeal with semantic in term of indexing, Boolean model, Latent Semantic Analysis\n(LSA), Latent Semantic Index (LSI) and semantic ranking. Therefore, semantic\nBoolean IR framework is proposed in this paper. This model is implemented and\nthe precision, recall and run time are measured and compared with the\ntraditional IR model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 08:19:16 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Elabd", "Emad", ""], ["Alshari", "Eissa", ""], ["Abdulkader", "Hatem", ""]]}, {"id": "1512.04038", "submitter": "Shixia Liu", "authors": "Mengchen Liu, Shixia Liu, Xizhou Zhu, Qinying Liao, Furu Wei, and\n  Shimei Pan", "title": "An Uncertainty-Aware Approach for Exploratory Microblog Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there has been a great deal of interest in analyzing customer\nopinions and breaking news in microblogs, progress has been hampered by the\nlack of an effective mechanism to discover and retrieve data of interest from\nmicroblogs. To address this problem, we have developed an uncertainty-aware\nvisual analytics approach to retrieve salient posts, users, and hashtags. We\nextend an existing ranking technique to compute a multifaceted retrieval\nresult: the mutual reinforcement rank of a graph node, the uncertainty of each\nrank, and the propagation of uncertainty among different graph nodes. To\nillustrate the three facets, we have also designed a composite visualization\nwith three visual components: a graph visualization, an uncertainty glyph, and\na flow map. The graph visualization with glyphs, the flow map, and the\nuncertainty analysis together enable analysts to effectively find the most\nuncertain results and interactively refine them. We have applied our approach\nto several Twitter datasets. Qualitative evaluation and two real-world case\nstudies demonstrate the promise of our approach for retrieving high-quality\nmicroblog data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 11:56:09 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Liu", "Mengchen", ""], ["Liu", "Shixia", ""], ["Zhu", "Xizhou", ""], ["Liao", "Qinying", ""], ["Wei", "Furu", ""], ["Pan", "Shimei", ""]]}, {"id": "1512.04042", "submitter": "Shixia Liu", "authors": "Shixia Liu, Jialun Yin, Xiting Wang, Weiwei Cui, Kelei Cao, Jian Pei", "title": "Online Visual Analytics of Text Streams", "comments": "IEEE TVCG 2016", "journal-ref": null, "doi": "10.1109/TVCG.2015.2509990", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online visual analytics approach to helping users explore and\nunderstand hierarchical topic evolution in high-volume text streams. The key\nidea behind this approach is to identify representative topics in incoming\ndocuments and align them with the existing representative topics that they\nimmediately follow (in time). To this end, we learn a set of streaming tree\ncuts from topic trees based on user-selected focus nodes. A dynamic Bayesian\nnetwork model has been developed to derive the tree cuts in the incoming topic\ntrees to balance the fitness of each tree cut and the smoothness between\nadjacent tree cuts. By connecting the corresponding topics at different times,\nwe are able to provide an overview of the evolving hierarchical topics. A\nsedimentation-based visualization has been designed to enable the interactive\nanalysis of streaming text data from global patterns to local details. We\nevaluated our method on real-world datasets and the results are generally\nfavorable.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 12:22:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Shixia", ""], ["Yin", "Jialun", ""], ["Wang", "Xiting", ""], ["Cui", "Weiwei", ""], ["Cao", "Kelei", ""], ["Pei", "Jian", ""]]}, {"id": "1512.04133", "submitter": "George Cushen", "authors": "George Cushen", "title": "A Person Re-Identification System For Mobile Devices", "comments": "Appearing in Proceedings of the 11th IEEE/ACM International\n  Conference on Signal Image Technology & Internet Systems (SITIS 2015)", "journal-ref": null, "doi": "10.1109/SITIS.2015.96", "report-no": null, "categories": "cs.CV cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a critical security task for recognizing a person\nacross spatially disjoint sensors. Previous work can be computationally\nintensive and is mainly based on low-level cues extracted from RGB data and\nimplemented on a PC for a fixed sensor network (such as traditional CCTV). We\npresent a practical and efficient framework for mobile devices (such as smart\nphones and robots) where high-level semantic soft biometrics are extracted from\nRGB and depth data. By combining these cues, our approach attempts to provide\nrobustness to noise, illumination, and minor variations in clothing. This\nmobile approach may be particularly useful for the identification of persons in\nareas ill-served by fixed sensors or for tasks where the sensor position and\ndirection need to dynamically adapt to a target. Results on the BIWI dataset\nare preliminary but encouraging. Further evaluation and demonstration of the\nsystem will be available on our website.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 22:33:17 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cushen", "George", ""]]}, {"id": "1512.04633", "submitter": "Peter Lofgren", "authors": "Peter Lofgren", "title": "Efficient Algorithms for Personalized PageRank", "comments": "PhD Thesis (Stanford Computer Science). Based on joint work with Sid\n  Banerjee and Ashish Goel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new, more efficient algorithms for estimating random walk scores\nsuch as Personalized PageRank from a given source node to one or several target\nnodes. These scores are useful for personalized search and recommendations on\nnetworks including social networks, user-item networks, and the web. Past work\nhas proposed using Monte Carlo or using linear algebra to estimate scores from\na single source to every target, making them inefficient for a single pair. Our\ncontribution is a new bidirectional algorithm which combines linear algebra and\nMonte Carlo to achieve significant speed improvements. On a diverse set of six\ngraphs, our algorithm is 70x faster than past state-of-the-art algorithms. We\nalso present theoretical analysis: while past algorithms require $\\Omega(n)$\ntime to estimate a random walk score of typical size $\\frac{1}{n}$ on an\n$n$-node graph to a given constant accuracy, our algorithm requires only\n$O(\\sqrt{m})$ expected time for an average target, where $m$ is the number of\nedges, and is provably accurate.\n  In addition to our core bidirectional estimator for personalized PageRank, we\npresent an alternative algorithm for undirected graphs, a generalization to\narbitrary walk lengths and Markov Chains, an algorithm for personalized search\nranking, and an algorithm for sampling random paths from a given source to a\ngiven set of targets. We expect our bidirectional methods can be extended in\nother ways and will be useful subroutines in other graph analysis problems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 02:54:26 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lofgren", "Peter", ""]]}, {"id": "1512.04701", "submitter": "Weixin Li", "authors": "Weixin Li, Jungseock Joo, Hang Qi, and Song-Chun Zhu", "title": "Joint Image-Text News Topic Detection and Tracking with And-Or Graph\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop a method for automatically detecting and\ntracking topics in broadcast news. We present a hierarchical And-Or graph (AOG)\nto jointly represent the latent structure of both texts and visuals. The AOG\nembeds a context sensitive grammar that can describe the hierarchical\ncomposition of news topics by semantic elements about people involved, related\nplaces and what happened, and model contextual relationships between elements\nin the hierarchy. We detect news topics through a cluster sampling process\nwhich groups stories about closely related events. Swendsen-Wang Cuts (SWC), an\neffective cluster sampling algorithm, is adopted for traversing the solution\nspace and obtaining optimal clustering solutions by maximizing a Bayesian\nposterior probability. Topics are tracked to deal with the continuously updated\nnews streams. We generate topic trajectories to show how topics emerge, evolve\nand disappear over time. The experimental results show that our method can\nexplicitly describe the textual and visual data in news videos and produce\nmeaningful topic trajectories. Our method achieves superior performance\ncompared to state-of-the-art methods on both a public dataset Reuters-21578 and\na self-collected dataset named UCLA Broadcast News Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 10:01:37 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Li", "Weixin", ""], ["Joo", "Jungseock", ""], ["Qi", "Hang", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1512.05004", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Jiaan Zeng and Colin Allen", "title": "Towards Evaluation of Cultural-scale Claims in Light of Topic Model\n  Sampling Effects", "comments": "2016 International Conference on Computational Social Science\n  (IC2S2), June 23-26, 2016. 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cultural-scale models of full text documents are prone to over-interpretation\nby researchers making unintentionally strong socio-linguistic claims (Pechenick\net al., 2015) without recognizing that even large digital libraries are merely\nsamples of all the books ever produced. In this study, we test the sensitivity\nof the topic models to the sampling process by taking random samples of books\nin the Hathi Trust Digital Library from different areas of the Library of\nCongress Classification Outline. For each classification area, we train several\ntopic models over the entire class with different random seeds, generating a\nset of spanning models. Then, we train topic models on random samples of books\nfrom the classification area, generating a set of sample models. Finally, we\nperform a topic alignment between each pair of models by computing the\nJensen-Shannon distance (JSD) between the word probability distributions for\neach topic. We take two measures on each model alignment: alignment distance\nand topic overlap. We find that sample models with a large sample size\ntypically have an alignment distance that falls in the range of the alignment\ndistance between spanning models. Unsurprisingly, as sample size increases,\nalignment distance decreases. We also find that the topic overlap increases as\nsample size increases. However, the decomposition of these measures by sample\nsize differs by number of topics and by classification area. We speculate that\nthese measures could be used to find classes which have a common \"canon\"\ndiscussed among all books in the area, as shown by high topic overlap and low\nalignment distance even in small sample sizes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 23:07:58 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 21:12:17 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 15:48:16 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Murdock", "Jaimie", ""], ["Zeng", "Jiaan", ""], ["Allen", "Colin", ""]]}, {"id": "1512.05437", "submitter": "Hyok-Chol Choe", "authors": "Man-Hung Jong, Chong-Han Ri, Hyok-Chol Choe, Chol-Jun Hwang", "title": "A Method of Passage-Based Document Retrieval in Question Answering\n  System", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for using the scoring values of passages to effectively\nretrieve documents in a Question Answering system.\n  For this, we suggest evaluation function that considers proximity between\neach question terms in passage. And using this evaluation function , we extract\na documents which involves scoring values in the highest collection, as a\nsuitable document for question.\n  The proposed method is very effective in document retrieval of Korean\nquestion answering system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 01:48:37 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Jong", "Man-Hung", ""], ["Ri", "Chong-Han", ""], ["Choe", "Hyok-Chol", ""], ["Hwang", "Chol-Jun", ""]]}, {"id": "1512.05685", "submitter": "Johann Schaible", "authors": "Johann Schaible and Thomas Gottron and Ansgar Scherp", "title": "TermPicker: Enabling the Reuse of Vocabulary Terms by Exploiting Data\n  from the Linked Open Data Cloud - An Extended Technical Report", "comments": "17 pages, 3 figures, extended technical report for a Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding which vocabulary terms to use when modeling data as Linked Open Data\n(LOD) is far from trivial. Choosing too general vocabulary terms, or terms from\nvocabularies that are not used by other LOD datasets, is likely to lead to a\ndata representation, which will be harder to understand by humans and to be\nconsumed by Linked data applications. In this technical report, we propose\nTermPicker: a novel approach for vocabulary reuse by recommending RDF types and\nproperties based on exploiting the information on how other data providers on\nthe LOD cloud use RDF types and properties to describe their data. To this end,\nwe introduce the notion of so-called schema-level patterns (SLPs). They capture\nhow sets of RDF types are connected via sets of properties within some data\ncollection, e.g., within a dataset on the LOD cloud. TermPicker uses such SLPs\nand generates a ranked list of vocabulary terms for reuse. The lists of\nrecommended terms are ordered by a ranking model which is computed using the\nmachine learning approach Learning To Rank (L2R). TermPicker is evaluated based\non the recommendation quality that is measured using the Mean Average Precision\n(MAP) and the Mean Reciprocal Rank at the first five positions (MRR@5). Our\nresults illustrate an improvement of the recommendation quality by 29% - 36%\nwhen using SLPs compared to the beforehand investigated baselines of\nrecommending solely popular vocabulary terms or terms from the same vocabulary.\nThe overall best results are achieved using SLPs in conjunction with the\nLearning To Rank algorithm Random Forests.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 17:37:56 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 22:00:10 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Schaible", "Johann", ""], ["Gottron", "Thomas", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1512.06034", "submitter": "Marco Manna", "authors": "Weronika T. Adrian and Nicola Leone and Marco Manna", "title": "Ontology-driven Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogeneous unstructured data (HUD) are collections of unstructured documents\nthat share common properties, such as similar layout, common file format, or\ncommon domain of values. Building on such properties, it would be desirable to\nautomatically process HUD to access the main information through a semantic\nlayer -- typically an ontology -- called semantic view. Hence, we propose an\nontology-based approach for extracting semantically rich information from HUD,\nby integrating and extending recent technologies and results from the fields of\nclassical information extraction, table recognition, ontologies, text\nannotation, and logic programming. Moreover, we design and implement a system,\nnamed KnowRex, that has been successfully applied to curriculum vitae in the\nEuropass style to offer a semantic view of them, and be able, for example, to\nselect those which exhibit required skills.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 16:56:02 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Adrian", "Weronika T.", ""], ["Leone", "Nicola", ""], ["Manna", "Marco", ""]]}, {"id": "1512.06303", "submitter": "Andrew Elkouri", "authors": "Andrew Elkouri", "title": "Predicting the Sentiment Polarity and Rating of Yelp Reviews", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews of businesses have become increasingly important in recent\nyears, as customers and even competitors use them to judge the quality of a\nbusiness. Yelp is one of the most popular websites for users to write such\nreviews, and it would be useful for them to be able to predict the sentiment or\neven the star rating of a review. In this paper, we develop two classifiers to\nperform positive/negative classification and 5-star classification. We use\nNaive Bayes, Support Vector Machines, and Logistic Regression as models, and\nachieved the best accuracy with Logistic Regression: 92.90% for\npositive/negative classification, and 63.92% for 5-star classification. These\nresults demonstrate the quality of the Logistic Regression model using only the\ntext of the review, yet there is a promising opportunity for improvement with\nmore data, more features, and perhaps different models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 01:12:38 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Elkouri", "Andrew", ""]]}, {"id": "1512.06362", "submitter": "Nichola Abdo", "authors": "Nichola Abdo, Cyrill Stachniss, Luciano Spinello, Wolfram Burgard", "title": "Collaborative Filtering for Predicting User Preferences for Organizing\n  Objects", "comments": "Submission to The International Journal of Robotics Research.\n  Relevant material can be found at\n  http://www2.informatik.uni-freiburg.de/~abdon/task_preferences.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As service robots become more and more capable of performing useful tasks for\nus, there is a growing need to teach robots how we expect them to carry out\nthese tasks. However, different users typically have their own preferences, for\nexample with respect to arranging objects on different shelves. As many of\nthese preferences depend on a variety of factors including personal taste,\ncultural background, or common sense, it is challenging for an expert to\npre-program a robot in order to accommodate all potential users. At the same\ntime, it is impractical for robots to constantly query users about how they\nshould perform individual tasks. In this work, we present an approach to learn\npatterns in user preferences for the task of tidying up objects in containers,\ne.g., shelves or boxes. Our method builds upon the paradigm of collaborative\nfiltering for making personalized recommendations and relies on data from\ndifferent users that we gather using crowdsourcing. To deal with novel objects\nfor which we have no data, we propose a method that compliments standard\ncollaborative filtering by leveraging information mined from the Web. When\nsolving a tidy-up task, we first predict pairwise object preferences of the\nuser. Then, we subdivide the objects in containers by modeling a spectral\nclustering problem. Our solution is easy to update, does not require complex\nmodeling, and improves with the amount of user data. We evaluate our approach\nusing crowdsourcing data from over 1,200 users and demonstrate its\neffectiveness for two tidy-up scenarios. Additionally, we show that a real\nrobot can reliably predict user preferences using our approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 12:04:14 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Abdo", "Nichola", ""], ["Stachniss", "Cyrill", ""], ["Spinello", "Luciano", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1512.06785", "submitter": "Longqi Yang", "authors": "Longqi Yang, Cheng-Kang Hsieh, Deborah Estrin", "title": "Beyond Classification: Latent User Interests Profiling from Visual\n  Contents Analysis", "comments": "2015 IEEE 15th International Conference on Data Mining Workshops", "journal-ref": null, "doi": "10.1109/ICDMW.2015.160", "report-no": null, "categories": "cs.IR cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User preference profiling is an important task in modern online social\nnetworks (OSN). With the proliferation of image-centric social platforms, such\nas Pinterest, visual contents have become one of the most informative data\nstreams for understanding user preferences. Traditional approaches usually\ntreat visual content analysis as a general classification problem where one or\nmore labels are assigned to each image. Although such an approach simplifies\nthe process of image analysis, it misses the rich context and visual cues that\nplay an important role in people's perception of images. In this paper, we\nexplore the possibilities of learning a user's latent visual preferences\ndirectly from image contents. We propose a distance metric learning method\nbased on Deep Convolutional Neural Networks (CNN) to directly extract\nsimilarity information from visual contents and use the derived distance metric\nto mine individual users' fine-grained visual preferences. Through our\npreliminary experiments using data from 5,790 Pinterest users, we show that\neven for the images within the same category, each user possesses distinct and\nindividually-identifiable visual preferences that are consistent over their\nlifetime. Our results underscore the untapped potential of finer-grained visual\npreference profiling in understanding users' preferences.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 19:54:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Yang", "Longqi", ""], ["Hsieh", "Cheng-Kang", ""], ["Estrin", "Deborah", ""]]}, {"id": "1512.06863", "submitter": "Julian McAuley", "authors": "Julian McAuley and Alex Yang", "title": "Addressing Complex and Subjective Product-Related Queries with Customer\n  Reviews", "comments": "WWW 2016; 14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews are often our first port of call when considering products and\npurchases online. When evaluating a potential purchase, we may have a specific\nquery in mind, e.g. `will this baby seat fit in the overhead compartment of a\n747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer\nsuch questions we must either wade through huge volumes of consumer reviews\nhoping to find one that is relevant, or otherwise pose our question directly to\nthe community via a Q/A system.\n  In this paper we hope to fuse these two paradigms: given a large volume of\npreviously answered queries about products, we hope to automatically learn\nwhether a review of a product is relevant to a given query. We formulate this\nas a machine learning problem using a mixture-of-experts-type framework---here\neach review is an `expert' that gets to vote on the response to a particular\nquery; simultaneously we learn a relevance function such that `relevant'\nreviews are those that vote correctly. At test time this learned relevance\nfunction allows us to surface reviews that are relevant to new queries\non-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million\nquestions (and answers) and 13 million reviews. We show quantitatively that it\nis effective at addressing both binary and open-ended queries, and\nqualitatively that it surfaces reviews that human evaluators consider to be\nrelevant.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 21:01:07 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["McAuley", "Julian", ""], ["Yang", "Alex", ""]]}, {"id": "1512.07046", "submitter": "Jan Rupnik", "authors": "Jan Rupnik, Andrej Muhic, Gregor Leban, Primoz Skraba, Blaz Fortuna,\n  Marko Grobelnik", "title": "News Across Languages - Cross-Lingual Document Similarity and Event\n  Tracking", "comments": "Accepted for publication in Journal of Artificial Intelligence\n  Research, Special Track on Cross-language Algorithms and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, we follow news which is distributed globally. Significant\nevents are reported by different sources and in different languages. In this\nwork, we address the problem of tracking of events in a large multilingual\nstream. Within a recently developed system Event Registry we examine two\naspects of this problem: how to compare articles in different languages and how\nto link collections of articles in different languages which refer to the same\nevent. Taking a multilingual stream and clusters of articles from each\nlanguage, we compare different cross-lingual document similarity measures based\non Wikipedia. This allows us to compute the similarity of any two articles\nregardless of language. Building on previous work, we show there are methods\nwhich scale well and can compute a meaningful similarity between articles from\nlanguages with little or no direct overlap in the training data. Using this\ncapability, we then propose an approach to link clusters of articles across\nlanguages which represent the same event. We provide an extensive evaluation of\nthe system as a whole, as well as an evaluation of the quality and robustness\nof the similarity measure and the linking algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:11:32 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Rupnik", "Jan", ""], ["Muhic", "Andrej", ""], ["Leban", "Gregor", ""], ["Skraba", "Primoz", ""], ["Fortuna", "Blaz", ""], ["Grobelnik", "Marko", ""]]}, {"id": "1512.07051", "submitter": "Julia Kiseleva", "authors": "Julia Kiseleva and Alejandro Montes Garc\\'ia and Jaap Kamps and Nikita\n  Spirin", "title": "The Impact of Technical Domain Expertise on Search Behavior and Task\n  Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain expertise is regarded as one of the key factors impacting search\nsuccess: experts are known to write more effective queries, to select the right\nresults on the result page, and to find answers satisfying their information\nneeds. Search transaction logs play the crucial role in the result ranking. Yet\ndespite the variety in expertise levels of users, all prior interactions are\ntreated alike, suggesting that weighting in expertise can improve the ranking\nfor informational tasks. The main aim of this paper is to investigate the\nimpact of high levels of technical domain expertise on both search behavior and\ntask outcome. We conduct an online user study with searchers proficient in\nprogramming languages. We focus on Java and Javascript, yet we believe that our\nstudy and results are applicable for other expertise-sensitive search tasks.\nThe main findings are three-fold: First, we constructed expertise tests that\neffectively measure technical domain expertise and correlate well with the\nself-reported expertise. Second, we showed that there is a clear position bias,\nbut technical domain experts were less affected by position bias. Third, we\nfound that general expertise helped finding the correct answers, but the domain\nexperts were more successful as they managed to detect better answers. Our work\nis using explicit tests to determine user expertise levels, which is an\nimportant step toward fully automatic detection of expertise levels based on\ninteraction behavior. A deeper understanding of the impact of expertise on\nsearch behavior and task outcome can enable more effective use of expert\nbehavior in search logs - essentially make everyone search as an expert.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 12:31:51 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Kiseleva", "Julia", ""], ["Garc\u00eda", "Alejandro Montes", ""], ["Kamps", "Jaap", ""], ["Spirin", "Nikita", ""]]}, {"id": "1512.07370", "submitter": "Taejin Park", "authors": "Taejin Park, Taejin Lee", "title": "Musical instrument sound classification with deep convolutional neural\n  network using feature fusion approach", "comments": "14 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A new musical instrument classification method using convolutional neural\nnetworks (CNNs) is presented in this paper. Unlike the traditional methods, we\ninvestigated a scheme for classifying musical instruments using the learned\nfeatures from CNNs. To create the learned features from CNNs, we not only used\na conventional spectrogram image, but also proposed multiresolution recurrence\nplots (MRPs) that contain the phase information of a raw input signal.\nConsequently, we fed the characteristic timbre of the particular instrument\ninto a neural network, which cannot be extracted using a phase-blinded\nrepresentations such as a spectrogram. By combining our proposed MRPs and\nspectrogram images with a multi-column network, the performance of our proposed\nclassifier system improves over a system that uses only a spectrogram.\nFurthermore, the proposed classifier also outperforms the baseline result from\ntraditional handcrafted features and classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 07:09:13 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Park", "Taejin", ""], ["Lee", "Taejin", ""]]}, {"id": "1512.07444", "submitter": "Marianna Kouneli", "authors": "Marianna Kouneli", "title": "Exploiting Hierarchy for Ranking-based Recommendation", "comments": "81 pages, M.Sc. Thesis (in Greek), Department of Computer Engineering\n  and Informatics, University of Patras", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this master's thesis is to study and develop a new algorithmic\nframework for collaborative filtering (CF) to generate recommendations. The\nmethod we propose is based on the exploitation of the hierarchical structure of\nthe item space and intuitively \"stands\" on the property of Near Complete\nDecomposability (NCD) which is inherent in the structure of the majority of\nhierarchical systems. Building on the intuition behind the NCDawareRank\nalgorithm and its related concept of NCD proximity, we model our system in a\nway that illuminates its endemic characteristics and we propose a new\nalgorithmic framework for recommendations, called HIR. We focus on combining\nthe direct with the NCD \"neighborhoods\" of items to achieve better\ncharacterization of the inter-item relations, in order to improve the quality\nof recommendations and alleviate sparsity related problems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:02:11 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Kouneli", "Marianna", ""]]}, {"id": "1512.07454", "submitter": "Allan Hanbury", "authors": "Allan Hanbury, Henning M\\\"uller, Krisztian Balog, Torben Brodt, Gordon\n  V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree\n  Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer,\n  Martin Potthast", "title": "Evaluation-as-a-Service: Overview and Outlook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation in empirical computer science is essential to show progress and\nassess technologies developed. Several research domains such as information\nretrieval have long relied on systematic evaluation to measure progress: here,\nthe Cranfield paradigm of creating shared test collections, defining search\ntasks, and collecting ground truth for these tasks has persisted up until now.\nIn recent years, however, several new challenges have emerged that do not fit\nthis paradigm very well: extremely large data sets, confidential data sets as\nfound in the medical domain, and rapidly changing data sets as often\nencountered in industry. Also, crowdsourcing has changed the way that industry\napproaches problem-solving with companies now organizing challenges and handing\nout monetary awards to incentivize people to work on their challenges,\nparticularly in the field of machine learning.\n  This white paper is based on discussions at a workshop on\nEvaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets\nto participants and have them work on the data locally, but keeping the data\ncentral and allowing access via Application Programming Interfaces (API),\nVirtual Machines (VM) or other possibilities to ship executables. The objective\nof this white paper are to summarize and compare the current approaches and\nconsolidate the experiences of these approaches to outline the next steps of\nEaaS, particularly towards sustainable research infrastructures.\n  This white paper summarizes several existing approaches to EaaS and analyzes\ntheir usage scenarios and also the advantages and disadvantages. The many\nfactors influencing EaaS are overviewed, and the environment in terms of\nmotivations for the various stakeholders, from funding agencies to challenge\norganizers, researchers and participants, to industry interested in supplying\nreal-world problems for which they require solutions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:44:09 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Hanbury", "Allan", ""], ["M\u00fcller", "Henning", ""], ["Balog", "Krisztian", ""], ["Brodt", "Torben", ""], ["Cormack", "Gordon V.", ""], ["Eggel", "Ivan", ""], ["Gollub", "Tim", ""], ["Hopfgartner", "Frank", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Kando", "Noriko", ""], ["Krithara", "Anastasia", ""], ["Lin", "Jimmy", ""], ["Mercer", "Simon", ""], ["Potthast", "Martin", ""]]}, {"id": "1512.07636", "submitter": "Petros Boufounos", "authors": "Petros T Boufounos, Shantanu Rane, Hassan Mansour", "title": "Representation and Coding of Signal Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to signal representation and coding theory have traditionally\nfocused on how to best represent signals using parsimonious representations\nthat incur the lowest possible distortion. Classical examples include linear\nand non-linear approximations, sparse representations, and rate-distortion\ntheory. Very often, however, the goal of processing is to extract specific\ninformation from the signal, and the distortion should be measured on the\nextracted information. The corresponding representation should, therefore,\nrepresent that information as parsimoniously as possible, without necessarily\naccurately representing the signal itself.\n  In this paper, we examine the problem of encoding signals such that\nsufficient information is preserved about their pairwise distances and their\ninner products. For that goal, we consider randomized embeddings as an encoding\nmechanism and provide a framework to analyze their performance. We also\ndemonstrate that it is possible to design the embedding such that it represents\ndifferent ranges of distances with different precision. These embeddings also\nallow the computation of kernel inner products with control on their inner\nproduct-preserving properties. Our results provide a broad framework to design\nand analyze embeddins, and generalize existing results in this area, such as\nrandom Fourier kernels and universal embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 21:04:31 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Boufounos", "Petros T", ""], ["Rane", "Shantanu", ""], ["Mansour", "Hassan", ""]]}, {"id": "1512.07807", "submitter": "Homayun Afrabandpey", "authors": "Seppo Virtanen, Homayun Afrabandpey, Samuel Kaski", "title": "Visualizations Relevant to The User By Multi-View Latent Variable\n  Factorization", "comments": "IEEE International Conference on Acoustic, Speech and Signal\n  Processing 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472120", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main goal of data visualization is to find, from among all the available\nalternatives, mappings to the 2D/3D display which are relevant to the user.\nAssuming user interaction data, or other auxiliary data about the items or\ntheir relationships, the goal is to identify which aspects in the primary data\nsupport the user\\'s input and, equally importantly, which aspects of the\nuser\\'s potentially noisy input have support in the primary data. For solving\nthe problem, we introduce a multi-view embedding in which a latent\nfactorization identifies which aspects in the two data views (primary data and\nuser data) are related and which are specific to only one of them. The\nfactorization is a generative model in which the display is parameterized as a\npart of the factorization and the other factors explain away the aspects not\nexpressible in a two-dimensional display. Functioning of the model is\ndemonstrated on several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 12:53:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 12:12:10 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Virtanen", "Seppo", ""], ["Afrabandpey", "Homayun", ""], ["Kaski", "Samuel", ""]]}, {"id": "1512.08008", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Adham Beykikhoshk and Ognjen Arandjelovic and Dinh Phung and Svetha\n  Venkatesh", "title": "Discovering topic structures of a temporally evolving document corpus", "comments": "2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel framework for the discovery of the topical\ncontent of a data corpus, and the tracking of its complex structural changes\nacross the temporal dimension. In contrast to previous work our model does not\nimpose a prior on the rate at which documents are added to the corpus nor does\nit adopt the Markovian assumption which overly restricts the type of changes\nthat the model can capture. Our key technical contribution is a framework based\non (i) discretization of time into epochs, (ii) epoch-wise topic discovery\nusing a hierarchical Dirichlet process-based model, and (iii) a temporal\nsimilarity graph which allows for the modelling of complex topic changes:\nemergence and disappearance, evolution, splitting, and merging. The power of\nthe proposed framework is demonstrated on two medical literature corpora\nconcerned with the autism spectrum disorder (ASD) and the metabolic syndrome\n(MetS) -- both increasingly important research subjects with significant social\nand healthcare consequences. In addition to the collected ASD and metabolic\nsyndrome literature corpora which we made freely available, our contribution\nalso includes an extensive empirical analysis of the proposed framework. We\ndescribe a detailed and careful examination of the effects that our\nalgorithms's free parameters have on its output, and discuss the significance\nof the findings both in the context of the practical application of our\nalgorithm as well as in the context of the existing body of work on temporal\ntopic analysis. Our quantitative analysis is followed by several qualitative\ncase studies highly relevant to the current research on ASD and MetS, on which\nour algorithm is shown to capture well the actual developments in these fields.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 15:18:11 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Beykikhoshk", "Adham", ""], ["Arandjelovic", "Ognjen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1512.08325", "submitter": "Shimin Cai Dr", "authors": "Yao-Dong Zhao, Shi-Min Cai, Ming Tang, Ming-Sheng Shang", "title": "A Fast Recommendation Algorithm for Social Tagging Systems : A Delicious\n  Case", "comments": "20 pages, 7 figures", "journal-ref": "Physica A 483, 209 (2017)", "doi": "10.1016/j.physa.2017.04.131", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tripartite graph is one of the commonest topological structures in social\ntagging systems such as Delicious, which has three types of nodes (i.e., users,\nURLs and tags). Traditional recommender systems developed based on\ncollaborative filtering for the social tagging systems bring very high demands\non CPU time cost. In this paper, to overcome this drawback, we propose a novel\napproach that extracts non-overlapping user clusters and corresponding\noverlapping item clusters simultaneously through coarse clustering to\naccelerate the user-based collaborative filtering and develop a fast\nrecommendation algorithm for the social tagging systems. The experimental\nresults show that the proposed approach is able to dramatically reduce the\nprocessing time cost greater than $90\\%$ and relatively enhance the accuracy in\ncomparison with the ordinary user-based collaborative filtering algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 06:23:37 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Zhao", "Yao-Dong", ""], ["Cai", "Shi-Min", ""], ["Tang", "Ming", ""], ["Shang", "Ming-Sheng", ""]]}, {"id": "1512.08808", "submitter": "Kerstin Bunte", "authors": "Kerstin Bunte, Eemeli Lepp\\\"aaho, Inka Saarinen, Samuel Kaski", "title": "Sparse group factor analysis for biclustering of multiple data sources", "comments": "7 pages, 5 figures, 1 table in Bioinformatics 2016", "journal-ref": "Bioinformatics Volume 32, Issue 16 Pp. 2457-2463, 2016", "doi": "10.1093/bioinformatics/btw207", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Modelling methods that find structure in data are necessary with\nthe current large volumes of genomic data, and there have been various efforts\nto find subsets of genes exhibiting consistent patterns over subsets of\ntreatments. These biclustering techniques have focused on one data source,\noften gene expression data. We present a Bayesian approach for joint\nbiclustering of multiple data sources, extending a recent method Group Factor\nAnalysis (GFA) to have a biclustering interpretation with additional sparsity\nassumptions. The resulting method enables data-driven detection of linear\nstructure present in parts of the data sources. Results: Our simulation studies\nshow that the proposed method reliably infers bi-clusters from heterogeneous\ndata sources. We tested the method on data from the NCI-DREAM drug sensitivity\nprediction challenge, resulting in an excellent prediction accuracy. Moreover,\nthe predictions are based on several biclusters which provide insight into the\ndata sources, in this case on gene expression, DNA methylation, protein\nabundance, exome sequence, functional connectivity fingerprints and drug\nsensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:07:35 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 10:23:53 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Bunte", "Kerstin", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Saarinen", "Inka", ""], ["Kaski", "Samuel", ""]]}]