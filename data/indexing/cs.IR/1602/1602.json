[{"id": "1602.00104", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution", "title": "Extracting Keyword for Disambiguating Name Based on the Overlap\n  Principle", "comments": "7 pages, Proceeding of International Conference on Information\n  Technology and Engineering Application (4-th ICIBA), Book 1, 119-125,\n  February 20-21, 2015. arXiv admin note: text overlap with arXiv:1212.3023", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Name disambiguation has become one of the main themes in the Semantic Web\nagenda. The semantic web is an extension of the current Web in which\ninformation is not only given well-defined meaning, but also has many purposes\nthat contain the ambiguous naturally or a lot of thing came with the overlap,\nmainly deals with the persons name. Therefore, we develop an approach to\nextract keywords from web snippet with utilizing the overlap principle, a\nconcept to understand things with ambiguous, whereby features of person are\ngenerated for dealing with the variety of web, the web is steadily gaining\nground in the semantic research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 10:53:05 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""]]}, {"id": "1602.00251", "submitter": "Kaveh Bakhtiyari", "authors": "Kaveh Bakhtiyari", "title": "Do we have privacy in the digital world?", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.2492.5203/2", "report-no": null, "categories": "cs.CR cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not really.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 14:22:47 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 15:53:55 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Bakhtiyari", "Kaveh", ""]]}, {"id": "1602.01137", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Eric Nalisnick, Nick Craswell and Rich Caruana", "title": "A Dual Embedding Space Model for Document Ranking", "comments": "This paper is an extended evaluation and analysis of the model\n  proposed in a poster to appear in WWW'16, April 11 - 15, 2016, Montreal,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental goal of search engines is to identify, given a query, documents\nthat have relevant text. This is intrinsically difficult because the query and\nthe document may use different vocabulary, or the document may contain query\nwords without being relevant. We investigate neural word embeddings as a source\nof evidence in document ranking. We train a word2vec embedding model on a large\nunlabelled query corpus, but in contrast to how the model is commonly used, we\nretain both the input and the output projections, allowing us to leverage both\nthe embedding spaces to derive richer distributional relationships. During\nranking we map the query words into the input space and the document words into\nthe output space, and compute a query-document relevance score by aggregating\nthe cosine similarities across all the query-document word pairs.\n  We postulate that the proposed Dual Embedding Space Model (DESM) captures\nevidence on whether a document is about a query term in addition to what is\nmodelled by traditional term-frequency based approaches. Our experiments show\nthat the DESM can re-rank top documents returned by a commercial Web search\nengine, like Bing, better than a term-matching based signal like TF-IDF.\nHowever, when ranking a larger set of candidate documents, we find the\nembeddings-based approach is prone to false positives, retrieving documents\nthat are only loosely related to the query. We demonstrate that this problem\ncan be solved effectively by ranking based on a linear mixture of the DESM and\nthe word counting features.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 22:23:18 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Nalisnick", "Eric", ""], ["Craswell", "Nick", ""], ["Caruana", "Rich", ""]]}, {"id": "1602.01248", "submitter": "Spyros Sioutas SS", "authors": "Nikolaos Nodarakis, Spyros Sioutas, Athanasios Tsakalidis and Giannis\n  Tzimas", "title": "Using Hadoop for Large Scale Analysis on Twitter: A Technical Report", "comments": "8 pages, 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (or opinion mining) on Twitter data has attracted much\nattention recently. One of the system's key features, is the immediacy in\ncommunication with other users in an easy, user-friendly and fast way.\nConsequently, people tend to express their feelings freely, which makes Twitter\nan ideal source for accumulating a vast amount of opinions towards a wide\ndiversity of topics. This amount of information offers huge potential and can\nbe harnessed to receive the sentiment tendency towards these topics. However,\nsince none can invest an infinite amount of time to read through these tweets,\nan automated decision making approach is necessary. Nevertheless, most existing\nsolutions are limited in centralized environments only. Thus, they can only\nprocess at most a few thousand tweets. Such a sample, is not representative to\ndefine the sentiment polarity towards a topic due to the massive number of\ntweets published daily. In this paper, we go one step further and develop a\nnovel method for sentiment learning in the MapReduce framework. Our algorithm\nexploits the hashtags and emoticons inside a tweet, as sentiment labels, and\nproceeds to a classification procedure of diverse sentiment types in a parallel\nand distributed manner. Moreover, we utilize Bloom filters to compact the\nstorage size of intermediate data and boost the performance of our algorithm.\nThrough an extensive experimental evaluation, we prove that our solution is\nefficient, robust and scalable and confirm the quality of our sentiment\nidentification.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:19:19 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Nodarakis", "Nikolaos", ""], ["Sioutas", "Spyros", ""], ["Tsakalidis", "Athanasios", ""], ["Tzimas", "Giannis", ""]]}, {"id": "1602.01428", "submitter": "Jason Dou", "authors": "Jason Dou, Ni Sun, Xiaojun Zou", "title": "\"Draw My Topics\": Find Desired Topics fast from large scale of Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the \"Draw My Topics\" toolkit, which provides a fast way to\nincorporate social scientists' interest into standard topic modelling. Instead\nof using raw corpus with primitive processing as input, an algorithm based on\nVector Space Model and Conditional Entropy are used to connect social\nscientists' willingness and unsupervised topic models' output. Space for users'\nadjustment on specific corpus of their interest is also accommodated. We\ndemonstrate the toolkit's use on the Diachronic People's Daily Corpus in\nChinese.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 19:44:37 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Dou", "Jason", ""], ["Sun", "Ni", ""], ["Zou", "Xiaojun", ""]]}, {"id": "1602.01585", "submitter": "Ruining He", "authors": "Ruining He, Julian McAuley", "title": "Ups and Downs: Modeling the Visual Evolution of Fashion Trends with\n  One-Class Collaborative Filtering", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": "10.1145/2872427.2883037", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a successful recommender system depends on understanding both the\ndimensions of people's preferences as well as their dynamics. In certain\ndomains, such as fashion, modeling such preferences can be incredibly\ndifficult, due to the need to simultaneously model the visual appearance of\nproducts as well as their evolution over time. The subtle semantics and\nnon-linear dynamics of fashion evolution raise unique challenges especially\nconsidering the sparsity and large scale of the underlying datasets. In this\npaper we build novel models for the One-Class Collaborative Filtering setting,\nwhere our goal is to estimate users' fashion-aware personalized ranking\nfunctions based on their past feedback. To uncover the complex and evolving\nvisual factors that people consider when evaluating products, our method\ncombines high-level visual features extracted from a deep convolutional neural\nnetwork, users' past feedback, as well as evolving trends within the community.\nExperimentally we evaluate our method on two large real-world datasets from\nAmazon.com, where we show it to outperform state-of-the-art personalized\nranking measures, and also use it to visualize the high-level fashion trends\nacross the 11-year span of our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 08:31:05 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["He", "Ruining", ""], ["McAuley", "Julian", ""]]}, {"id": "1602.01665", "submitter": "Ronan Cummins", "authors": "Ronan Cummins", "title": "Improved Query Topic Models via Pseudo-Relevant P\\'olya Document Models", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-expansion via pseudo-relevance feedback is a popular method of\novercoming the problem of vocabulary mismatch and of increasing average\nretrieval effectiveness. In this paper, we develop a new method that estimates\na query topic model from a set of pseudo-relevant documents using a new\nlanguage modelling framework.\n  We assume that documents are generated via a mixture of multivariate Polya\ndistributions, and we show that by identifying the topical terms in each\ndocument, we can appropriately select terms that are likely to belong to the\nquery topic model. The results of experiments on several TREC collections show\nthat the new approach compares favourably to current state-of-the-art expansion\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 13:19:31 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Cummins", "Ronan", ""]]}, {"id": "1602.01792", "submitter": "Kunho Kim", "authors": "Kunho Kim, Madian Khabsa, C. Lee Giles", "title": "Random Forest DBSCAN for USPTO Inventor Name Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name disambiguation and the subsequent name conflation are essential for the\ncorrect processing of person name queries in a digital library or other\ndatabase. It distinguishes each unique person from all other records in the\ndatabase. We study inventor name disambiguation for a patent database using\nmethods and features from earlier work on author name disambiguation and\npropose a feature set appropriate for a patent database. A random forest was\nselected for the pairwise linking classifier since they outperform Naive Bayes,\nLogistic Regression, Support Vector Machines (SVM), Conditional Inference Tree,\nand Decision Trees. Blocking size, very important for scaling, was selected\nbased on experiments that determined feature importance and accuracy. The\nDBSCAN algorithm is used for clustering records, using a distance function\nderived from random forest classifier. For additional scalability clustering\nwas parallelized. Tests on the USPTO patent database show that our method\nsuccessfully disambiguated 12 million inventor mentions within 6.5 hours.\nEvaluation on datasets from USPTO PatentsView inventor name disambiguation\ncompetition shows our algorithm outperforms all algorithms in the competition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 19:00:30 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 20:22:34 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 16:50:33 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 14:25:22 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Kim", "Kunho", ""], ["Khabsa", "Madian", ""], ["Giles", "C. Lee", ""]]}, {"id": "1602.01937", "submitter": "Kambiz Ghazinour", "authors": "Kambiz Ghazinour, Stan Matwin, Marina Sokolova", "title": "YOURPRIVACYPROTECTOR, A recommender system for privacy settings in\n  social networks", "comments": "15 pages, International journal of security, privacy and trust\n  management. (IJSPTM) Volume 2, No 4, Aug. 2013", "journal-ref": "International journal of security, privacy and trust management.\n  (IJSPTM) Volume 2, No 4, Aug. 2013", "doi": "10.5121/ijsptm.2013.2402", "report-no": null, "categories": "cs.CR cs.CY cs.IR cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Ensuring privacy of users of social networks is probably an unsolvable\nconundrum. At the same time, an informed use of the existing privacy options by\nthe social network participants may alleviate - or even prevent - some of the\nmore drastic privacy-averse incidents. Unfortunately, recent surveys show that\nan average user is either not aware of these options or does not use them,\nprobably due to their perceived complexity. It is therefore reasonable to\nbelieve that tools assisting users with two tasks: 1) understanding their\nsocial net behavior in terms of their privacy settings and broad privacy\ncategories, and 2)recommending reasonable privacy options, will be a valuable\ntool for everyday privacy practice in a social network context. This paper\npresents YourPrivacyProtector, a recommender system that shows how simple\nmachine learning techniques may provide useful assistance in these two tasks to\nFacebook users. We support our claim with empirical results of application of\nYourPrivacyProtector to two groups of Facebook users.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 07:29:55 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Ghazinour", "Kambiz", ""], ["Matwin", "Stan", ""], ["Sokolova", "Marina", ""]]}, {"id": "1602.02047", "submitter": "Elvys Linhares Pontes", "authors": "Elvys Linhares Pontes", "title": "Utiliza\\c{c}\\~ao de Grafos e Matriz de Similaridade na Sumariza\\c{c}\\~ao\n  Autom\\'atica de Documentos Baseada em Extra\\c{c}\\~ao de Frases", "comments": "Dissertation, 83 pages, in Portuguese. in Disserta\\c{c}\\~ao de\n  Mestrado, Universidade Federal do Cear\\'a, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet increased the amount of information available. However, the\nreading and understanding of this information are costly tasks. In this\nscenario, the Natural Language Processing (NLP) applications enable very\nimportant solutions, highlighting the Automatic Text Summarization (ATS), which\nproduce a summary from one or more source texts. Automatically summarizing one\nor more texts, however, is a complex task because of the difficulties inherent\nto the analysis and generation of this summary. This master's thesis describes\nthe main techniques and methodologies (NLP and heuristics) to generate\nsummaries. We have also addressed and proposed some heuristics based on graphs\nand similarity matrix to measure the relevance of judgments and to generate\nsummaries by extracting sentences. We used the multiple languages (English,\nFrench and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA\n(French) corpus to evaluate the developped systems. The results obtained were\nquite interesting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 14:54:57 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Pontes", "Elvys Linhares", ""]]}, {"id": "1602.02133", "submitter": "Issa Atoum", "authors": "Issa Atoum and Ahmed Otoom", "title": "Mining Software Quality from Software Reviews: Research Trends and Open\n  Issues", "comments": "11 pages", "journal-ref": "International Journal of Computer Trends and Technology,Vol. 31,\n  No. 2, Jan 2016", "doi": "10.14445/22312803/IJCTT-V31P114", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software review text fragments have considerably valuable information about\nusers experience. It includes a huge set of properties including the software\nquality. Opinion mining or sentiment analysis is concerned with analyzing\ntextual user judgments. The application of sentiment analysis on software\nreviews can find a quantitative value that represents software quality.\nAlthough many software quality methods are proposed they are considered\ndifficult to customize and many of them are limited. This article investigates\nthe application of opinion mining as an approach to extract software quality\nproperties. We found that the major issues of software reviews mining using\nsentiment analysis are due to software lifecycle and the diverse users and\nteams.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:42:24 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Atoum", "Issa", ""], ["Otoom", "Ahmed", ""]]}, {"id": "1602.02255", "submitter": "Qing-Yuan Jiang", "authors": "Qing-Yuan Jiang, Wu-Jun Li", "title": "Deep Cross-Modal Hashing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, cross-modal hashing (CMH)\nhas been widely used for similarity search in multimedia retrieval\napplications. However, almost all existing CMH methods are based on\nhand-crafted features which might not be optimally compatible with the\nhash-code learning procedure. As a result, existing CMH methods with\nhandcrafted features may not achieve satisfactory performance. In this paper,\nwe propose a novel cross-modal hashing method, called deep crossmodal hashing\n(DCMH), by integrating feature learning and hash-code learning into the same\nframework. DCMH is an end-to-end learning framework with deep neural networks,\none for each modality, to perform feature learning from scratch. Experiments on\ntwo real datasets with text-image modalities show that DCMH can outperform\nother baselines to achieve the state-of-the-art performance in cross-modal\nretrieval applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 13:43:24 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 09:43:56 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Jiang", "Qing-Yuan", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1602.02332", "submitter": "Antti Puurula", "authors": "Antti Puurula", "title": "Scalable Text Mining with Sparse Generative Models", "comments": "PhD Thesis, Computer Science, University of Waikato, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The information age has brought a deluge of data. Much of this is in text\nform, insurmountable in scope for humans and incomprehensible in structure for\ncomputers. Text mining is an expanding field of research that seeks to utilize\nthe information contained in vast document collections. General data mining\nmethods based on machine learning face challenges with the scale of text data,\nposing a need for scalable text mining methods.\n  This thesis proposes a solution to scalable text mining: generative models\ncombined with sparse computation. A unifying formalization for generative text\nmodels is defined, bringing together research traditions that have used\nformally equivalent models, but ignored parallel developments. This framework\nallows the use of methods developed in different processing tasks such as\nretrieval and classification, yielding effective solutions across different\ntext mining tasks. Sparse computation using inverted indices is proposed for\ninference on probabilistic models. This reduces the computational complexity of\nthe common text mining operations according to sparsity, yielding probabilistic\nmodels with the scalability of modern search engines.\n  The proposed combination provides sparse generative models: a solution for\ntext mining that is general, effective, and scalable. Extensive experimentation\non text classification and ranked retrieval datasets are conducted, showing\nthat the proposed solution matches or outperforms the leading task-specific\nmethods in effectiveness, with a order of magnitude decrease in classification\ntimes for Wikipedia article categorization with a million classes. The\ndeveloped methods were further applied in two 2014 Kaggle data mining prize\ncompetitions with over a hundred competing teams, earning first and second\nplaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 02:49:27 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Puurula", "Antti", ""]]}, {"id": "1602.02506", "submitter": "Thomas Steiner", "authors": "Thomas Steiner", "title": "Wikipedia Tools for Google Spreadsheets", "comments": "4 pages, 3 Listings, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the Wikipedia Tools for Google Spreadsheets.\nGoogle Spreadsheets is part of a free, Web-based software office suite offered\nby Google within its Google Docs service. It allows users to create and edit\nspreadsheets online, while collaborating with other users in realtime.\nWikipedia is a free-access, free-content Internet encyclopedia, whose content\nand data is available, among other means, through an API. With the Wikipedia\nTools for Google Spreadsheets, we have created a toolkit that facilitates\nworking with Wikipedia data from within a spreadsheet context. We make these\ntools available as open-source on GitHub\n[https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released\nunder the permissive Apache 2.0 license.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 09:40:43 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Steiner", "Thomas", ""]]}, {"id": "1602.02620", "submitter": "Ninh Pham", "authors": "Ninh Pham, Rasmus Pagh", "title": "Scalability and Total Recall with Fast CoveringLSH", "comments": "Short version appears in Proceedings of CIKM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic\ntechnique for similarity search with strong performance guarantees in\nhigh-dimensional spaces. A drawback of traditional LSH schemes is that they may\nhave \\emph{false negatives}, i.e., the recall is less than 100\\%. This limits\nthe applicability of LSH in settings requiring precise performance guarantees.\nBuilding on the recent theoretical \"CoveringLSH\" construction that eliminates\nfalse negatives, we propose a fast and practical covering LSH scheme for\nHamming space called \\emph{Fast CoveringLSH (fcLSH)}. Inheriting the design\nbenefits of CoveringLSH our method avoids false negatives and always reports\nall near neighbors. Compared to CoveringLSH we achieve an asymptotic\nimprovement to the hash function computation time from $\\mathcal{O}(dL)$ to\n$\\mathcal{O}(d + L\\log{L})$, where $d$ is the dimensionality of data and $L$ is\nthe number of hash tables. Our experiments on synthetic and real-world data\nsets demonstrate that \\emph{fcLSH} is comparable (and often superior) to\ntraditional hashing-based approaches for search radius up to 20 in\nhigh-dimensional Hamming space.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 16:03:11 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 10:46:19 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pham", "Ninh", ""], ["Pagh", "Rasmus", ""]]}, {"id": "1602.02842", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Collaborative filtering via sparse Markov random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a central role in providing individualized access to\ninformation and services. This paper focuses on collaborative filtering, an\napproach that exploits the shared structure among mind-liked users and similar\nitems. In particular, we focus on a formal probabilistic framework known as\nMarkov random fields (MRF). We address the open problem of structure learning\nand introduce a sparsity-inducing algorithm to automatically estimate the\ninteraction structures between users and between items. Item-item and user-user\ncorrelation networks are obtained as a by-product. Large-scale experiments on\nmovie recommendation and date matching datasets demonstrate the power of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:30:27 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.02850", "submitter": "Bo Tang", "authors": "Bo Tang, Steven Kay, and Haibo He", "title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "comments": "This paper has been submitted to the IEEE Trans. Knowledge and Data\n  Engineering. 14 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TKDE.2016.2563436", "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 03:43:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tang", "Bo", ""], ["Kay", "Steven", ""], ["He", "Haibo", ""]]}, {"id": "1602.02911", "submitter": "Andrew Mcmurry", "authors": "Andrew J. McMurry", "title": "Searching PubMed for articles relevant to clinical interpretation of\n  rare human genetic variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous challenges persist that delay clinical interpretation of human\ngenetic variants, to name a few: (1) un- structured PubMed articles are the\nmost abundant source of evidence, yet their variant annotations are difficult\nto query uniformly, (2) variants can be reported many different ways, for\nexample as DNA sequence change or protein modification, (3) historical drift in\nannotations over time between various genome reference assemblies and\ntranscript alignments, (4) no single laboratory has sufficient numbers of human\nsamples, necessitating precompetitive efforts to share evidence for clinical\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 09:22:40 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["McMurry", "Andrew J.", ""]]}, {"id": "1602.03101", "submitter": "Fl\\'avio Martins", "authors": "Fl\\'avio Martins, Jo\\~ao Magalh\\~aes and Jamie Callan", "title": "Barbara Made the News: Mining the Behavior of Crowds for Time-Aware\n  Learning to Rank", "comments": "To appear in WSDM 2016", "journal-ref": null, "doi": "10.1145/2835776.2835825", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Twitter, and other microblogging services, the generation of new content\nby the crowd is often biased towards immediacy: what is happening now. Prompted\nby the propagation of commentary and information through multiple mediums,\nusers on the Web interact with and produce new posts about newsworthy topics\nand give rise to trending topics. This paper proposes to leverage on the\nbehavioral dynamics of users to estimate the most relevant time periods for a\ntopic. Our hypothesis stems from the fact that when a real-world event occurs\nit usually has peak times on the Web: a higher volume of tweets, new visits and\nedits to related Wikipedia articles, and news published about the event. In\nthis paper, we propose a novel time-aware ranking model that leverages on\nmultiple sources of crowd signals. Our approach builds on two major novelties.\nFirst, a unifying approach that given query q, mines and represents temporal\nevidence from multiple sources of crowd signals. This allows us to predict the\ntemporal relevance of documents for query q. Second, a principled retrieval\nmodel that integrates temporal signals in a learning to rank framework, to rank\nresults according to the predicted temporal relevance. Evaluation on the TREC\n2013 and 2014 Microblog track datasets demonstrates that the proposed model\nachieves a relative improvement of 13.2% over lexical retrieval models and 6.2%\nover a learning to rank baseline.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 18:01:57 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Martins", "Fl\u00e1vio", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Callan", "Jamie", ""]]}, {"id": "1602.03606", "submitter": "Federico Barrios", "authors": "Federico Barrios, Federico L\\'opez, Luis Argerich, Rosa Wachenchauzer", "title": "Variations of the Similarity Function of TextRank for Automated\n  Summarization", "comments": "8 pages, 2 figures. Presented at the Argentine Symposium on\n  Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)", "journal-ref": "44 JAIIO - ASAI 2015 - ISSN: 2451-7585, pages 65-72", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents new alternatives to the similarity function for the\nTextRank algorithm for automatic summarization of texts. We describe the\ngeneralities of the algorithm and the different functions we propose. Some of\nthese variants achieve a significative improvement using the same metrics and\ndataset as the original publication.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 02:39:21 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Barrios", "Federico", ""], ["L\u00f3pez", "Federico", ""], ["Argerich", "Luis", ""], ["Wachenchauzer", "Rosa", ""]]}, {"id": "1602.04375", "submitter": "Mrinmaya Sachan", "authors": "Mrinmaya Sachan, Avinava Dubey, Eric P. Xing", "title": "Science Question Answering using Instructional Materials", "comments": "Corrected that the science QA dataset is NOT freely available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a solution for elementary science test using instructional\nmaterials. We posit that there is a hidden structure that explains the\ncorrectness of an answer given the question and instructional materials and\npresent a unified max-margin framework that learns to find these hidden\nstructures (given a corpus of question-answer pairs and instructional\nmaterials), and uses what it learns to answer novel elementary science\nquestions. Our evaluation shows that our framework outperforms several strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 20:13:48 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 01:17:56 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Sachan", "Mrinmaya", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1602.04393", "submitter": "Abhinav Maurya", "authors": "Abhinav Maurya, Kenton Murray, Yandong Liu, Chris Dyer, William W.\n  Cohen, Daniel B. Neill", "title": "Semantic Scan: Detecting Subtle, Spatially Localized Events in Text\n  Streams", "comments": "10 pages, 4 figures, KDD 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection and precise characterization of emerging topics in text\nstreams can be highly useful in applications such as timely and targeted public\nhealth interventions and discovering evolving regional business trends. Many\nmethods have been proposed for detecting emerging events in text streams using\ntopic modeling. However, these methods have numerous shortcomings that make\nthem unsuitable for rapid detection of locally emerging events on massive text\nstreams. In this paper, we describe Semantic Scan (SS) that has been developed\nspecifically to overcome these shortcomings in detecting new spatially compact\nevents in text streams.\n  Semantic Scan integrates novel contrastive topic modeling with online\ndocument assignment and principled likelihood ratio-based spatial scanning to\nidentify emerging events with unexpected patterns of keywords hidden in text\nstreams. This enables more timely and accurate detection and characterization\nof anomalous, spatially localized emerging events. Semantic Scan does not\nrequire manual intervention or labeled training data, and is robust to noise in\nreal-world text data since it identifies anomalous text patterns that occur in\na cluster of new documents rather than an anomaly in a single new document.\n  We compare Semantic Scan to alternative state-of-the-art methods such as\nTopics over Time, Online LDA, and Labeled LDA on two real-world tasks: (i) a\ndisease surveillance task monitoring free-text Emergency Department chief\ncomplaints in Allegheny County, and (ii) an emerging business trend detection\ntask based on Yelp reviews. On both tasks, we find that Semantic Scan provides\nsignificantly better event detection and characterization accuracy than\ncompeting approaches, while providing up to an order of magnitude speedup.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 22:33:56 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Maurya", "Abhinav", ""], ["Murray", "Kenton", ""], ["Liu", "Yandong", ""], ["Dyer", "Chris", ""], ["Cohen", "William W.", ""], ["Neill", "Daniel B.", ""]]}, {"id": "1602.04567", "submitter": "Changho Suh", "authors": "Changho Suh, Vincent Y. F. Tan, Renbo Zhao", "title": "Adversarial Top-$K$ Ranking", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the top-$K$ ranking problem where the goal is to recover the set of\ntop-$K$ ranked items out of a large collection of items based on partially\nrevealed preferences. We consider an adversarial crowdsourced setting where\nthere are two population sets, and pairwise comparison samples drawn from one\nof the populations follow the standard Bradley-Terry-Luce model (i.e., the\nchance of item $i$ beating item $j$ is proportional to the relative score of\nitem $i$ to item $j$), while in the other population, the corresponding chance\nis inversely proportional to the relative score. When the relative size of the\ntwo populations is known, we characterize the minimax limit on the sample size\nrequired (up to a constant) for reliably identifying the top-$K$ items, and\ndemonstrate how it scales with the relative size. Moreover, by leveraging a\ntensor decomposition method for disambiguating mixture distributions, we extend\nour result to the more realistic scenario in which the relative population size\nis unknown, thus establishing an upper bound on the fundamental limit of the\nsample size for recovering the top-$K$ set.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:22:59 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Suh", "Changho", ""], ["Tan", "Vincent Y. F.", ""], ["Zhao", "Renbo", ""]]}, {"id": "1602.04572", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc and Ganesh Venkataraman and Mario Rodriguez and Shakti\n  Sinha and Senthil Sundaram and Lin Guo", "title": "Personalized Expertise Search at LinkedIn", "comments": "2015 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkedIn is the largest professional network with more than 350 million\nmembers. As the member base increases, searching for experts becomes more and\nmore challenging. In this paper, we propose an approach to address the problem\nof personalized expertise search on LinkedIn, particularly for exploratory\nsearch queries containing {\\it skills}. In the offline phase, we introduce a\ncollaborative filtering approach based on matrix factorization. Our approach\nestimates expertise scores for both the skills that members list on their\nprofiles as well as the skills they are likely to have but do not explicitly\nlist. In the online phase (at query time) we use expertise scores on these\nskills as a feature in combination with other features to rank the results. To\nlearn the personalized ranking function, we propose a heuristic to extract\ntraining data from search logs while handling position and sample selection\nbiases. We tested our models on two products - LinkedIn homepage and LinkedIn\nrecruiter. A/B tests showed significant improvements in click through rates -\n31% for CTR@1 for recruiter (18% for homepage) as well as downstream messages\nsent from search - 37% for recruiter (20% for homepage). As of writing this\npaper, these models serve nearly all live traffic for skills search on LinkedIn\nhomepage as well as LinkedIn recruiter.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:44:59 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Venkataraman", "Ganesh", ""], ["Rodriguez", "Mario", ""], ["Sinha", "Shakti", ""], ["Sundaram", "Senthil", ""], ["Guo", "Lin", ""]]}, {"id": "1602.04709", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti, Amir A. Delay, Fatemeh Seyedmendhi", "title": "Identifying Structures in Social Conversations in NSCLC Patients through\n  the Semi-Automatic extraction of Topical Taxonomies", "comments": "7 pages, 7 figures, 1 table", "journal-ref": "Journal of Engineering Research and Applications, Vol. 6, Issue 1,\n  January 2016, pp.20-26", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration of social conversations for addressing patient's needs is an\nimportant analytical task in which many scholarly publications are contributing\nto fill the knowledge gap in this area. The main difficulty remains the\ninability to turn such contributions into pragmatic processes the\npharmaceutical industry can leverage in order to generate insight from social\nmedia data, which can be considered as one of the most challenging source of\ninformation available today due to its sheer volume and noise. This study is\nbased on the work by Scott Spangler and Jeffrey Kreulen and applies it to\nidentify structure in social media through the extraction of a topical taxonomy\nable to capture the latent knowledge in social conversations in health-related\nsites. The mechanism for automatically identifying and generating a taxonomy\nfrom social conversations is developed and pressured tested using public data\nfrom media sites focused on the needs of cancer patients and their families.\nMoreover, a novel method for generating the category's label and the\ndetermination of an optimal number of categories is presented which extends\nScott and Jeffrey's research in a meaningful way. We assume the reader is\nfamiliar with taxonomies, what they are and how they are used.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 19:56:49 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Crocetti", "Giancarlo", ""], ["Delay", "Amir A.", ""], ["Seyedmendhi", "Fatemeh", ""]]}, {"id": "1602.04924", "submitter": "Viet Ha-Thuc", "authors": "Dhruv Arya and Viet Ha-Thuc and Shakti Sinha", "title": "Personalized Federated Search at LinkedIn", "comments": "in Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management (CIKM 2015)", "journal-ref": null, "doi": "10.1145/2806416.2806615", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkedIn has grown to become a platform hosting diverse sources of\ninformation ranging from member profiles, jobs, professional groups, slideshows\netc. Given the existence of multiple sources, when a member issues a query like\n\"software engineer\", the member could look for software engineer profiles, jobs\nor professional groups. To tackle this problem, we exploit a data-driven\napproach that extracts searcher intents from their profile data and recent\nactivities at a large scale. The intents such as job seeking, hiring, content\nconsuming are used to construct features to personalize federated search\nexperience. We tested the approach on the LinkedIn homepage and A/B tests show\nsignificant improvements in member engagement. As of writing this paper, the\napproach powers all of federated search on LinkedIn homepage.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 07:10:42 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Arya", "Dhruv", ""], ["Ha-Thuc", "Viet", ""], ["Sinha", "Shakti", ""]]}, {"id": "1602.04930", "submitter": "Hai-Jun Zhou", "authors": "Yi-Zhi Xu and Hai-Jun Zhou", "title": "Generalized minimum dominating set and application in automatic text\n  summarization", "comments": "11 pages, including 4 figures and 2 tables. To be published in\n  Journal of Physics: Conference Series", "journal-ref": null, "doi": "10.1088/1742-6596/699/1/012014", "report-no": null, "categories": "cs.IR cond-mat.stat-mech cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a graph formed by vertices and weighted edges, a generalized minimum\ndominating set (MDS) is a vertex set of smallest cardinality such that the\nsummed weight of edges from each outside vertex to vertices in this set is\nequal to or larger than certain threshold value. This generalized MDS problem\nreduces to the conventional MDS problem in the limiting case of all the edge\nweights being equal to the threshold value. We treat the generalized MDS\nproblem in the present paper by a replica-symmetric spin glass theory and\nderive a set of belief-propagation equations. As a practical application we\nconsider the problem of extracting a set of sentences that best summarize a\ngiven input text document. We carry out a preliminary test of the statistical\nphysics-inspired method to this automatic text summarization problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 07:43:29 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Xu", "Yi-Zhi", ""], ["Zhou", "Hai-Jun", ""]]}, {"id": "1602.04983", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario\n  Fritz", "title": "Contextual Media Retrieval Using Natural Language Queries", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread integration of cameras in hand-held and head-worn devices as\nwell as the ability to share content online enables a large and diverse visual\ncapture of the world that millions of users build up collectively every day. We\nenvision these images as well as associated meta information, such as GPS\ncoordinates and timestamps, to form a collective visual memory that can be\nqueried while automatically taking the ever-changing context of mobile users\ninto account. As a first step towards this vision, in this work we present\nXplore-M-Ego: a novel media retrieval system that allows users to query a\ndynamic database of images and videos using spatio-temporal natural language\nqueries. We evaluate our system using a new dataset of real user queries as\nwell as through a usability study. One key finding is that there is a\nconsiderable amount of inter-user variability, for example in the resolution of\nspatial relations in natural language utterances. We show that our retrieval\nsystem can cope with this variability using personalisation through an online\nlearning-based retrieval formulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:04:29 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Malinowski", "Mateusz", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1602.05157", "submitter": "Gangli Liu", "authors": "Gangli Liu", "title": "A Ranking Algorithm for Re-finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-finding files from a personal computer is a frequent demand to users. When\nencountered a difficult re-finding task, people may not recall the attributes\nused by conventional re-finding methods, such as a file's path, file name,\nkeywords etc., the re-finding would fail.\n  We proposed a method to support difficult re-finding tasks. By asking the\nuser a list of questions about the target, such as a document's pages, author\nnumbers, accumulated reading time, last reading location etc. Then use the\nuser's answers to filter out the target.\n  After the user answered a list of questions about the target file, we\nevaluate the user's familiar degree about the target file based on the answers.\nWe devise a ranking algorithm which sorts the candidates by comparing the\nuser's familiarity degree about the target and the candidates.\n  We also propose a method to generate re-finding tasks artificially based on\nthe user's own document corpus.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:01:25 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 08:30:06 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Liu", "Gangli", ""]]}, {"id": "1602.05183", "submitter": "A. I. M. Jakaria Rahman", "authors": "Ronald Rousseau, A.I.M. Jakaria Rahman, Raf Guns and Tim C.E. Engels", "title": "A note and a correction on measuring cognitive distance in multiple\n  dimensions", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous article (Rahman, Guns, Rousseau, and Engels, 2015) we described\nseveral approaches to determine the cognitive distance between two units. One\nof these approaches was based on what we called barycenters in N dimensions.\nThe present note corrects this terminology and introduces the more adequate\nterm 'similarity-adapted publication vectors'. Furthermore, we correct an error\nin normalization and explain the importance of scale invariance in determining\ncognitive distance. We also consider weighted cosine similarity as an\nalternative approach to determine cognitive (dis)similarity. Overall, we find\nthat the three approaches (distance between barycenters, distance between\nsimilarity-adapted publication vectors, and weighted cosine similarity) yield\nvery similar results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:55:52 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 22:13:48 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Rousseau", "Ronald", ""], ["Rahman", "A. I. M. Jakaria", ""], ["Guns", "Raf", ""], ["Engels", "Tim C. E.", ""]]}, {"id": "1602.05285", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Choice by Elimination via Deep Neural Networks", "comments": "PAKDD workshop on Biologically Inspired Techniques for Data Mining\n  (BDM'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Choice by Elimination, a new framework that integrates\ndeep neural networks into probabilistic sequential choice models for learning\nto rank. Given a set of items to chose from, the elimination strategy starts\nwith the whole item set and iteratively eliminates the least worthy item in the\nremaining subset. We prove that the choice by elimination is equivalent to\nmarginalizing out the random Gompertz latent utilities. Coupled with the choice\nmodel is the recently introduced Neural Highway Networks for approximating\narbitrarily complex rank functions. We evaluate the proposed framework on a\nlarge-scale public dataset with over 425K items, drawn from the Yahoo! learning\nto rank challenge. It is demonstrated that the proposed method is competitive\nagainst state-of-the-art learning to rank methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 03:17:10 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.05352", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak and\n  Thorsten Joachims", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation", "comments": "10 pages in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data for evaluating and training recommender systems is subject to\nselection biases, either through self-selection by the users or through the\nactions of the recommendation system itself. In this paper, we provide a\nprincipled approach to handling selection biases, adapting models and\nestimation techniques from causal inference. The approach leads to unbiased\nperformance estimators despite biased data, and to a matrix factorization\nmethod that provides substantially improved prediction performance on\nreal-world data. We theoretically and empirically characterize the robustness\nof the approach, finding that it is highly practical and scalable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 09:58:25 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 03:18:59 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Schnabel", "Tobias", ""], ["Swaminathan", "Adith", ""], ["Singh", "Ashudeep", ""], ["Chandak", "Navin", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1602.06025", "submitter": "Yong Ren", "authors": "Yong Ren, Yining Wang, Jun Zhu", "title": "Spectral Learning for Supervised Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:07:20 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ren", "Yong", ""], ["Wang", "Yining", ""], ["Zhu", "Jun", ""]]}, {"id": "1602.06136", "submitter": "Mazen Alsarem", "authors": "Mazen Alsarem (DRIM), Pierre-Edouard Portier (DRIM), Sylvie Calabretto\n  (DRIM), Harald Kosch", "title": "Ordonnancement d'entit\\'es pour la rencontre du web des documents et du\n  web des donn\\'ees", "comments": "in French, Revue des Sciences et Technologies de l'Information -\n  S{\\'e}rie Document Num\\'erique, Lavoisier, 2015, Nouvelles approches en\n  recherche d'information, 18 (2-3/2015 ), pp.123-154", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances of the Linked Open Data (LOD) initiative are giving rise to a\nmore structured web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)\nconnecting many other datasets. They also made possible new web services for\nentity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for\nnew applications that will benefit from a combination of the web of documents\nand the web of data. To ease the emergence of these new use-cases, we propose a\nquery-biased algorithm for the ranking of entities detected inside a web page.\nOur algorithm combine link analysis with dimensionality reduction. We use\ncrowdsourcing for building a publicly available and reusable dataset on which\nwe compare our algorithm to the state of the art. Finally, we use this\nalgorithm for the construction of semantic snippets for which we evaluate the\nusability and the usefulness with a crowdsourcing-based approach.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 13:05:42 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Alsarem", "Mazen", "", "DRIM"], ["Portier", "Pierre-Edouard", "", "DRIM"], ["Calabretto", "Sylvie", "", "DRIM"], ["Kosch", "Harald", ""]]}, {"id": "1602.06454", "submitter": "Azade Nazi", "authors": "Azade Nazi, Mahashweta Das, Gautam Das", "title": "Web Item Reviewing Made Easy By Leveraging Available User Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of online review sites over the past decade has motivated\nbusinesses of all types to possess an expansive arsenal of user feedback to\nmark their reputation. Though a significant proportion of purchasing decisions\nare driven by average rating, detailed reviews are critical for activities like\nbuying expensive digital SLR camera. Since writing a detailed review for an\nitem is usually time-consuming, the number of reviews available in the Web is\nfar from many. Given a user and an item our goal is to identify the top-$k$\nmeaningful phrases/tags to help her review the item easily. We propose\ngeneral-constrained optimization framework based on three measures - relevance\n(how well the result set of tags describes an item), coverage (how well the\nresult set of tags covers the different aspects of an item), and polarity (how\nwell sentiment is attached to the result set of tags). By adopting different\ndefinitions of coverage, we identify two concrete problem instances that enable\na wide range of real-world scenarios. We develop practical algorithms with\ntheoretical bounds to solve these problems efficiently. We conduct experiments\non synthetic and real data crawled from the web to validate the effectiveness\nof our solutions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 20:33:41 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Nazi", "Azade", ""], ["Das", "Mahashweta", ""], ["Das", "Gautam", ""]]}, {"id": "1602.06977", "submitter": "Ethan Fast", "authors": "Ethan Fast, William McGrath, Pranav Rajpurkar, Michael Bernstein", "title": "Augur: Mining Human Behaviors from Fiction to Power Interactive Systems", "comments": "CHI: ACM Conference on Human Factors in Computing Systems 2016", "journal-ref": null, "doi": "10.1145/2858036.2858528", "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From smart homes that prepare coffee when we wake, to phones that know not to\ninterrupt us during important conversations, our collective visions of HCI\nimagine a future in which computers understand a broad range of human\nbehaviors. Today our systems fall short of these visions, however, because this\nrange of behaviors is too large for designers or programmers to capture\nmanually. In this paper, we instead demonstrate it is possible to mine a broad\nknowledge base of human behavior by analyzing more than one billion words of\nmodern fiction. Our resulting knowledge base, Augur, trains vector models that\ncan predict many thousands of user activities from surrounding objects in\nmodern contexts: for example, whether a user may be eating food, meeting with a\nfriend, or taking a selfie. Augur uses these predictions to identify actions\nthat people commonly take on objects in the world and estimate a user's future\nactivities given their current situation. We demonstrate Augur-powered,\nactivity-based systems such as a phone that silences itself when the odds of\nyou answering it are low, and a dynamic music player that adjusts to your\npresent activity. A field deployment of an Augur-powered wearable camera\nresulted in 96% recall and 71% precision on its unsupervised predictions of\ncommon daily activities. A second evaluation where human judges rated the\nsystem's predictions over a broad set of input images found that 94% were rated\nsensible.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 21:44:05 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 20:54:28 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Fast", "Ethan", ""], ["McGrath", "William", ""], ["Rajpurkar", "Pranav", ""], ["Bernstein", "Michael", ""]]}, {"id": "1602.07217", "submitter": "Joan Guisado-G\\'amez", "authors": "Joan Guisado-G\\'amez and Arnau Prat-P\\'erez and Josep Llu\\'is\n  Larriba-Pey", "title": "Query Expansion via structural motifs in Wikipedia Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for relevant information can be very frustrating for users who,\nunintentionally, use too general or inappropriate keywords to express their\nrequests. To overcome this situation, query expansion techniques aim at\ntransforming the user request by adding new terms, referred as expansion\nfeatures, that better describe the real intent of the users. We propose a\nmethod that relies exclusively on relevant structures (as opposed to the use of\nsemantics) found in knowledge bases (KBs) to extract the expansion features. We\ncall our method Structural Query Expansion (SQE). The structural analysis of\nKBs takes us to propose a set of structural motifs that connect their strongly\nrelated entries, which can be used to extract expansion features. In this paper\nwe use Wikipedia as our KB, which is probably one of the largest sources of\ninformation. SQE is capable of achieving more than 150% improvement over non\nexpanded queries and is able to identify the expansion features in less than\n0.2 seconds in the worst case scenario. Most significantly, we believe that we\nare contributing to open new research directions in query expansion, proposing\na method that is orthogonal to many current systems. For example, SQE improves\npseudo-relevance feedback techniques up to 13%\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 16:09:49 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 10:00:56 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Guisado-G\u00e1mez", "Joan", ""], ["Prat-P\u00e9rez", "Arnau", ""], ["Larriba-Pey", "Josep Llu\u00eds", ""]]}, {"id": "1602.07783", "submitter": "Zhao Kang", "authors": "Zhao Kang and Qiang Cheng", "title": "Top-N Recommendation with Novel Rank Approximation", "comments": "SDM 2016. arXiv admin note: text overlap with arXiv:1601.04800", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of accurate recommender systems has been widely recognized by\nacademia and industry. However, the recommendation quality is still rather low.\nRecently, a linear sparse and low-rank representation of the user-item matrix\nhas been applied to produce Top-N recommendations. This approach uses the\nnuclear norm as a convex relaxation for the rank function and has achieved\nbetter recommendation accuracy than the state-of-the-art methods. In the past\nseveral years, solving rank minimization problems by leveraging nonconvex\nrelaxations has received increasing attention. Some empirical results\ndemonstrate that it can provide a better approximation to original problems\nthan convex relaxation. In this paper, we propose a novel rank approximation to\nenhance the performance of Top-N recommendation systems, where the\napproximation error is controllable. Experimental results on real data show\nthat the proposed rank approximation improves the Top-$N$ recommendation\naccuracy substantially.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 03:33:44 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:58:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Kang", "Zhao", ""], ["Cheng", "Qiang", ""]]}, {"id": "1602.07799", "submitter": "R V Kanagavalli", "authors": "V.R. Kanagavalli, G. Maheeja", "title": "A Study on the usage of Data Structures in Information Retrieval", "comments": "National Conference on Innovations in Communication and Computing\n  Technologies Feb 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tries to throw light in the usage of data structures in the field\nof information retrieval. Information retrieval is an area of study which is\ngaining momentum as the need and urge for sharing and exploring information is\ngrowing day by day. Data structures have been the area of research for a long\nperiod in the arena of computer science. The need to have efficient data\nstructures has become even more important as the data grows in an exponential\nnature.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 05:12:55 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Kanagavalli", "V. R.", ""], ["Maheeja", "G.", ""]]}, {"id": "1602.07811", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Vincent Labatut (LIA), Serigne Gueye (LIA), Georges\n  Linar\\`es (LIA)", "title": "Narrative Smoothing: Dynamic Conversational Network for the Analysis of\n  TV Series Plots", "comments": null, "journal-ref": "DyNo: 2nd International Workshop on Dynamics in Networks, in\n  conjunction with the 2016 IEEE/ACM International Conference ASONAM, Aug 2016,\n  San Francisco, United States. pp.1111-1118,\n  \\&\\#x27E8;10.1109/ASONAM.2016.7752379\\&\\#x27E9", "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern popular TV series often develop complex storylines spanning several\nseasons, but are usually watched in quite a discontinuous way. As a result, the\nviewer generally needs a comprehensive summary of the previous season plot\nbefore the new one starts. The generation of such summaries requires first to\nidentify and characterize the dynamics of the series subplots. One way of doing\nso is to study the underlying social network of interactions between the\ncharacters involved in the narrative. The standard tools used in the Social\nNetworks Analysis field to extract such a network rely on an integration of\ntime, either over the whole considered period, or as a sequence of several\ntime-slices. However, they turn out to be inappropriate in the case of TV\nseries, due to the fact the scenes showed onscreen alternatively focus on\nparallel storylines, and do not necessarily respect a traditional chronology.\nThis makes existing extraction methods inefficient to describe the dynamics of\nrelationships between characters, or to get a relevant instantaneous view of\nthe current social state in the plot. This is especially true for characters\nshown as interacting with each other at some previous point in the plot but\ntemporarily neglected by the narrative. In this article, we introduce narrative\nsmoothing, a novel, still exploratory, network extraction method. It smooths\nthe relationship dynamics based on the plot properties, aiming at solving some\nof the limitations present in the standard approaches. In order to assess our\nmethod, we apply it to a new corpus of 3 popular TV series, and compare it to\nboth standard approaches. Our results are promising, showing narrative\nsmoothing leads to more relevant observations when it comes to the\ncharacterization of the protagonists and their relationships. It could be used\nas a basis for further modeling the intertwined storylines constituting TV\nseries plots.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 06:06:04 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 14:28:28 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2016 08:45:13 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 14:45:41 GMT"}, {"version": "v5", "created": "Thu, 30 Jan 2020 07:49:30 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Gueye", "Serigne", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"]]}, {"id": "1602.08186", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc, Ye Xu, Satya Pradeep Kanduri, Xianren Wu, Vijay Dialani,\n  Yan Yan, Abhishek Gupta, Shakti Sinha", "title": "Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn", "comments": "Demonstrated at WWW 2015, The 25th International World Wide Web\n  Conference (WWW 2015)", "journal-ref": null, "doi": "10.1145/2872518.2890549", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key challenge in talent search is how to translate complex criteria of a\nhiring position into a search query. This typically requires deep knowledge on\nwhich skills are typically needed for the position, what are their\nalternatives, which companies are likely to have such candidates, etc. However,\nlisting examples of suitable candidates for a given position is a relatively\neasy job. Therefore, in order to help searchers overcome this challenge, we\ndesign a next generation of talent search paradigm at LinkedIn: Search by Ideal\nCandidates. This new system only needs the searcher to input one or several\nexamples of suitable candidates for the position. The system will generate a\nquery based on the input candidates and then retrieve and rank results based on\nthe query as well as the input candidates. The query is also shown to the\nsearcher to make the system transparent and to allow the searcher to interact\nwith it. As the searcher modifies the initial query and makes it deviate from\nthe ideal candidates, the search ranking function dynamically adjusts an\nrefreshes the ranking results balancing between the roles of query and ideal\ncandidates. As of writing this paper, the new system is being launched to our\ncustomers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 03:22:40 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Xu", "Ye", ""], ["Kanduri", "Satya Pradeep", ""], ["Wu", "Xianren", ""], ["Dialani", "Vijay", ""], ["Yan", "Yan", ""], ["Gupta", "Abhishek", ""], ["Sinha", "Shakti", ""]]}, {"id": "1602.08393", "submitter": "Anshumali Shrivastava", "authors": "Anshumali Shrivastava", "title": "Exact Weighted Minwise Hashing in Constant Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted minwise hashing (WMH) is one of the fundamental subroutine, required\nby many celebrated approximation algorithms, commonly adopted in industrial\npractice for large scale-search and learning. The resource bottleneck of the\nalgorithms is the computation of multiple (typically a few hundreds to\nthousands) independent hashes of the data. The fastest hashing algorithm is by\nIoffe \\cite{Proc:Ioffe_ICDM10}, which requires one pass over the entire data\nvector, $O(d)$ ($d$ is the number of non-zeros), for computing one hash.\nHowever, the requirement of multiple hashes demands hundreds or thousands\npasses over the data. This is very costly for modern massive dataset.\n  In this work, we break this expensive barrier and show an expected constant\namortized time algorithm which computes $k$ independent and unbiased WMH in\ntime $O(k)$ instead of $O(dk)$ required by Ioffe's method. Moreover, our\nproposal only needs a few bits (5 - 9 bits) of storage per hash value compared\nto around $64$ bits required by the state-of-art-methodologies. Experimental\nevaluations, on real datasets, show that for computing 500 WMH, our proposal\ncan be 60000x faster than the Ioffe's method without losing any accuracy. Our\nmethod is also around 100x faster than approximate heuristics capitalizing on\nthe efficient \"densified\" one permutation hashing schemes\n\\cite{Proc:OneHashLSH_ICML14}. Given the simplicity of our approach and its\nsignificant advantages, we hope that it will replace existing implementations\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 16:55:08 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Shrivastava", "Anshumali", ""]]}, {"id": "1602.08581", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush\n  Ramsurat, Bhiksha Raj, Rita Singh", "title": "Content-based Video Indexing and Retrieval Using Corr-LDA", "comments": "8 Pages, Updated References, Added Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video indexing and retrieval methods on popular web-based multimedia\nsharing websites are based on user-provided sparse tagging. This paper proposes\na very specific way of searching for video clips, based on the content of the\nvideo. We present our work on Content-based Video Indexing and Retrieval using\nthe Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic\nframework. This is a model that provides for auto-annotation of videos in a\ndatabase with textual descriptors, and brings the added benefit of utilizing\nthe semantic relations between the content of the video and text. We use the\nconcept-level matching provided by corr-LDA to build correspondences between\ntext and multimedia, with the objective of retrieving content with increased\naccuracy. In our experiments, we employ only the audio components of the\nindividual recordings and compare our results with an SVM-based approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 10:27:49 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 22:47:51 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Parekh", "Sanjeel", ""], ["Mohandoss", "Vikas", ""], ["Ramsurat", "Anush", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1602.08800", "submitter": "Vitaly Bulgakov", "authors": "Vitaly Bulgakov", "title": "Iterative Aggregation Method for Solving Principal Component Analysis\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motivated by the previously developed multilevel aggregation method for\nsolving structural analysis problems a novel two-level aggregation approach for\nefficient iterative solution of Principal Component Analysis (PCA) problems is\nproposed. The course aggregation model of the original covariance matrix is\nused in the iterative solution of the eigenvalue problem by a power iterations\nmethod. The method is tested on several data sets consisting of large number of\ntext documents.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 02:40:05 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Bulgakov", "Vitaly", ""]]}, {"id": "1602.09134", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "The Capacity of Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the private information retrieval (PIR) problem a user wishes to retrieve,\nas efficiently as possible, one out of $K$ messages from $N$ non-communicating\ndatabases (each holds all $K$ messages) while revealing nothing about the\nidentity of the desired message index to any individual database. The\ninformation theoretic capacity of PIR is the maximum number of bits of desired\ninformation that can be privately retrieved per bit of downloaded information.\nFor $K$ messages and $N$ databases, we show that the PIR capacity is\n$(1+1/N+1/N^2+\\cdots+1/N^{K-1})^{-1}$. A remarkable feature of the capacity\nachieving scheme is that if we eliminate any subset of messages (by setting the\nmessage symbols to zero), the resulting scheme also achieves the PIR capacity\nfor the remaining subset of messages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 20:51:37 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:29:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}]