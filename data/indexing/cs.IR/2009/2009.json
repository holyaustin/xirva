[{"id": "2009.00235", "submitter": "Tanmoy Chakraborty", "authors": "Shravika Mittal, Tanmoy Chakraborty, Siddharth Pal", "title": "Dynamics of node influence in network growth models", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classes of network growth models have been proposed in the literature\nfor capturing real-world complex networks. Existing research primarily focuses\non global characteristics of these models, e.g., degree distribution. We aim to\nshift the focus towards studying the network growth dynamics from the\nperspective of individual nodes. In this paper, we study how a metric for node\ninfluence in network growth models behaves over time as the network evolves.\nThis metric, which we call node visibility, captures the probability of the\nnode to form new connections. First, we conduct an investigation on three\npopular network growth models -- preferential attachment, additive, and\nmultiplicative fitness models; and primarily look into the \"influential nodes\"\nor \"leaders\" to understand how their visibility evolves over time.\nSubsequently, we consider a generic fitness model and observe that the\nmultiplicative model strikes a balance between allowing influential nodes to\nmaintain their visibility, while at the same time making it possible for new\nnodes to gain visibility in the network. Finally, we observe that a spatial\ngrowth model with multiplicative fitness can curtail the global reach of\ninfluential nodes, thereby allowing the emergence of a multiplicity of \"local\nleaders\" in the network.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 05:09:17 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Mittal", "Shravika", ""], ["Chakraborty", "Tanmoy", ""], ["Pal", "Siddharth", ""]]}, {"id": "2009.00497", "submitter": "Philomene Chagniot", "authors": "Philom\\`ene Chagniot, Flavian Vasile, David Rohde", "title": "From Clicks to Conversions: Recommendation for long-term reward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are often optimised for short-term reward: a\nrecommendation is considered successful if a reward (e.g. a click) can be\nobserved immediately after the recommendation. The advantage of this framework\nis that with some reasonable (although questionable) assumptions, it allows\nfamiliar supervised learning tools to be used for the recommendation task.\nHowever, it means that long-term business metrics, e.g. sales or retention are\nignored. In this paper we introduce a framework for modeling long-term rewards\nin the RecoGym simulation environment. We use this newly introduced\nfunctionality to showcase problems introduced by the last-click attribution\nscheme in the case of conversion-optimized recommendations and propose a simple\nextension that leads to state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 14:53:57 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Chagniot", "Philom\u00e8ne", ""], ["Vasile", "Flavian", ""], ["Rohde", "David", ""]]}, {"id": "2009.00611", "submitter": "Krutarth Patel", "authors": "Krutarth Patel, Cornelia Caragea, Mark Phillips, Nathaniel Fox", "title": "Identifying Documents In-Scope of a Collection from Web Archives", "comments": "10 pages", "journal-ref": "In Proceedings of the ACM/IEEE Joint Conference on Digital\n  Libraries in 2020 (JCDL 2020)", "doi": "10.1145/3383583.3398540", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archive data usually contains high-quality documents that are very useful\nfor creating specialized collections of documents, e.g., scientific digital\nlibraries and repositories of technical reports. In doing so, there is a\nsubstantial need for automatic approaches that can distinguish the documents of\ninterest for a collection out of the huge number of documents collected by web\narchiving institutions. In this paper, we explore different learning models and\nfeature representations to determine the best performing ones for identifying\nthe documents of interest from the web archived data. Specifically, we study\nboth machine learning and deep learning models and \"bag of words\" (BoW)\nfeatures extracted from the entire document or from specific portions of the\ndocument, as well as structural features that capture the structure of\ndocuments. We focus our evaluation on three datasets that we created from three\ndifferent Web archives. Our experimental results show that the BoW classifiers\nthat focus only on specific portions of the documents (rather than the full\ntext) outperform all compared methods on all three datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 16:22:23 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Patel", "Krutarth", ""], ["Caragea", "Cornelia", ""], ["Phillips", "Mark", ""], ["Fox", "Nathaniel", ""]]}, {"id": "2009.00799", "submitter": "Houye Ji", "authors": "Jinghan Shi, Houye Ji, Chuan Shi, Xiao Wang, Zhiqiang Zhang, Jun Zhou", "title": "Heterogeneous Graph Neural Network for Recommendation", "comments": null, "journal-ref": "2020ICML Workshop", "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prosperous development of e-commerce has spawned diverse recommendation\nsystems. As a matter of fact, there exist rich and complex interactions among\nvarious types of nodes in real-world recommendation systems, which can be\nconstructed as heterogeneous graphs. How learn representative node embedding is\nthe basis and core of the personalized recommendation system. Meta-path is a\nwidely used structure to capture the semantics beneath such interactions and\nshow potential ability in improving node embedding. In this paper, we propose\nHeterogeneous Graph neural network for Recommendation (HGRec) which injects\nhigh-order semantic into node embedding via aggregating multi-hops meta-path\nbased neighbors and fuses rich semantics via multiple meta-paths based on\nattention mechanism to get comprehensive node embedding. Experimental results\ndemonstrate the importance of rich high-order semantics and also show the\npotentially good interpretability of HGRec.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 03:16:48 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Shi", "Jinghan", ""], ["Ji", "Houye", ""], ["Shi", "Chuan", ""], ["Wang", "Xiao", ""], ["Zhang", "Zhiqiang", ""], ["Zhou", "Jun", ""]]}, {"id": "2009.01053", "submitter": "James-Andrew Sarmiento", "authors": "James-Andrew Sarmiento", "title": "Exploiting Latent Codes: Interactive Fashion Product Generation, Similar\n  Image Retrieval, and Cross-Category Recommendation using Variational\n  Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of deep learning applications in the fashion industry has fueled\nadvances in curating large-scale datasets to build applications for product\ndesign, image retrieval, and recommender systems. In this paper, the author\nproposes using Variational Autoencoder (VAE) to build an interactive fashion\nproduct application framework that allows the users to generate products with\nattributes according to their liking, retrieve similar styles for the same\nproduct category, and receive content-based recommendations from other\ncategories. Fashion product images dataset containing eyewear, footwear, and\nbags are appropriate to illustrate that this pipeline is applicable in the\nbooming industry of e-commerce enabling direct user interaction in specifying\ndesired products paired with new methods for data matching, and recommendation\nsystems by using VAE and exploiting its generated latent codes.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 13:27:30 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Sarmiento", "James-Andrew", ""]]}, {"id": "2009.01091", "submitter": "Marcelo Ponce", "authors": "Marcelo Ponce, Amit Sandhel", "title": "covid19.analytics: An R Package to Obtain, Analyze and Visualize Data\n  from the Coronavirus Disease Pandemic", "comments": "Version with updates matching ver 2.1 of the covi19.analytics package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of a new pandemic worldwide, a novel strategy to approach\nit has emerged. Several initiatives under the umbrella of \"open science\" are\ncontributing to tackle this unprecedented situation. In particular, the \"R\nLanguage and Environment for Statistical Computing\" offers an excellent tool\nand ecosystem for approaches focusing on open science and reproducible results.\nHence it is not surprising that with the onset of the pandemic, a large number\nof R packages and resources were made available for researches working in the\npandemic. In this paper, we present an R package that allows users to access\nand analyze worldwide data from resources publicly available. We will introduce\nthe covid19.analytics package, focusing in its capabilities and presenting a\nparticular study case where we describe how to deploy the \"COVID19.ANALYTICS\nDashboard Explorer\".\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 14:05:10 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 16:59:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ponce", "Marcelo", ""], ["Sandhel", "Amit", ""]]}, {"id": "2009.01210", "submitter": "Biswanath Dutta Dr.", "authors": "B. Dutta, M. DeBellis", "title": "CODO: An Ontology for Collection and Analysis of Covid-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COviD-19 Ontology for cases and patient information (CODO) provides a\nmodel for the collection and analysis of data about the COVID-19 pandemic. The\nontology provides a standards-based open-source model that facilitates the\nintegration of data from heterogeneous data sources. The ontology was designed\nby analysing disparate COVID-19 data sources such as datasets, literature,\nservices, etc. The ontology follows the best practices for vocabularies by\nre-using concepts from other leading vocabularies and by using the W3C\nstandards RDF, OWL, SWRL, and SPARQL. The ontology already has one independent\nuser and has incorporated real-world data from the government of India.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 17:32:37 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Dutta", "B.", ""], ["DeBellis", "M.", ""]]}, {"id": "2009.01311", "submitter": "Amifa Raj", "authors": "Amifa Raj, Connor Wood, Ananda Montoly, Michael D. Ekstrand", "title": "Comparing Fair Ranking Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Ranking is a fundamental aspect of recommender systems. However, ranked\noutputs can be susceptible to various biases; some of these may cause\ndisadvantages to members of protected groups. Several metrics have been\nproposed to quantify the (un)fairness of rankings, but there has not been to\ndate any direct comparison of these metrics. This complicates deciding what\nfairness metrics are applicable for specific scenarios, and assessing the\nextent to which metrics agree or disagree. In this paper, we describe several\nfair ranking metrics in a common notation, enabling direct comparison of their\napproaches and assumptions, and empirically compare them on the same\nexperimental setup and data set. Our work provides a direct comparative\nanalysis identifying similarities and differences of fair ranking metrics\nselected for our work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 19:28:09 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Raj", "Amifa", ""], ["Wood", "Connor", ""], ["Montoly", "Ananda", ""], ["Ekstrand", "Michael D.", ""]]}, {"id": "2009.01715", "submitter": "Lorenzo Porcaro", "authors": "Dougal Shakespeare, Lorenzo Porcaro, Emilia G\\'omez, Carlos Castillo", "title": "Exploring Artist Gender Bias in Music Recommendation", "comments": "Presented at the 2nd Workshop on the Impact of Recommender Systems\n  (ImpactRS), at the 14th ACM Conference on Recommender Systems (RecSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music Recommender Systems (mRS) are designed to give personalised and\nmeaningful recommendations of items (i.e. songs, playlists or artists) to a\nuser base, thereby reflecting and further complementing individual users'\nspecific music preferences. Whilst accuracy metrics have been widely applied to\nevaluate recommendations in mRS literature, evaluating a user's item utility\nfrom other impact-oriented perspectives, including their potential for\ndiscrimination, is still a novel evaluation practice in the music domain. In\nthis work, we center our attention on a specific phenomenon for which we want\nto estimate if mRS may exacerbate its impact: gender bias. Our work presents an\nexploratory study, analyzing the extent to which commonly deployed state of the\nart Collaborative Filtering(CF) algorithms may act to further increase or\ndecrease artist gender bias. To assess group biases introduced by CF, we deploy\na recently proposed metric of bias disparity on two listening event datasets:\nthe LFM-1b dataset, and the earlier constructed Celma's dataset. Our work\ntraces the causes of disparity to variations in input gender distributions and\nuser-item preferences, highlighting the effect such configurations can have on\nuser's gender bias after recommendation generation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 14:47:10 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:50:57 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Shakespeare", "Dougal", ""], ["Porcaro", "Lorenzo", ""], ["G\u00f3mez", "Emilia", ""], ["Castillo", "Carlos", ""]]}, {"id": "2009.01860", "submitter": "Zhenyu Gao", "authors": "J. Chen, Z. Gao", "title": "A Comprehensive Pipeline for Hotel Recommendation System", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a comprehensive pipeline to build a hotel recommendation\nsystem with the raw data collected by Apps in users' smartphones. The pipeline\nmainly consists of pre-processing of the raw data and training prediction\nmodels. We use two methods, Support Vector Machine (SVM) and Recurrent Neural\nNetwork (RNN). The results show that two methods achieved a reasonable accuracy\nwith the pre-processing of the raw data. Therefore, we conclude that this paper\nprovides a comprehensive pipeline, in which a hotel recommendation system was\nsuccessfully built from the raw data to specific applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:49:57 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Chen", "J.", ""], ["Gao", "Z.", ""]]}, {"id": "2009.01928", "submitter": "Quintino Francesco Lotito", "authors": "Quintino Francesco Lotito and Alberto Montresor", "title": "Efficient Algorithms to Mine Maximal Span-Trusses From Temporal Graphs", "comments": "Published at the 16th International Workshop on Mining and Learning\n  with Graphs (MLG '20)", "journal-ref": "International Workshop on Mining and Learning with Graphs 2020\n  (MLG '20)", "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, there has been an increasing interest in temporal\ngraphs, pushed by a growing availability of temporally-annotated network data\ncoming from social, biological and financial networks. Despite the importance\nof analyzing complex temporal networks, there is a huge gap between the set of\ndefinitions, algorithms and tools available to study large static graphs and\nthe ones available for temporal graphs. An important task in temporal graph\nanalysis is mining dense structures, i.e., identifying high-density subgraphs\ntogether with the span in which this high density is observed. In this paper,\nwe introduce the concept of $(k, \\Delta)$-truss (span-truss) in temporal\ngraphs, a temporal generalization of the $k$-truss, in which $k$ captures the\ninformation about the density and $\\Delta$ captures the time span in which this\ndensity holds. We then propose novel and efficient algorithms to identify\nmaximal span-trusses, namely the ones not dominated by any other span-truss\nneither in the order $k$ nor in the interval $\\Delta$, and evaluate them on a\nnumber of public available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:14:57 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 12:50:27 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lotito", "Quintino Francesco", ""], ["Montresor", "Alberto", ""]]}, {"id": "2009.01938", "submitter": "Samarth Rawal", "authors": "Samarth Rawal and Chitta Baral", "title": "Multi-Perspective Semantic Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) is the task of obtaining pieces of data (such as\ndocuments or snippets of text) that are relevant to a particular query or need\nfrom a large repository of information. While a combination of traditional\nkeyword- and modern BERT-based approaches have been shown to be effective in\nrecent work, there are often nuances in identifying what information is\n\"relevant\" to a particular query, which can be difficult to properly capture\nusing these systems. This work introduces the concept of a Multi-Perspective IR\nsystem, a novel methodology that combines multiple deep learning and\ntraditional IR models to better predict the relevance of a query-sentence pair,\nalong with a standardized framework for tuning this system. This work is\nevaluated on the BioASQ Biomedical IR + QA challenges.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 21:56:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Rawal", "Samarth", ""], ["Baral", "Chitta", ""]]}, {"id": "2009.01953", "submitter": "Gustavo Polleti", "authors": "Gustavo Padilha Polleti, Douglas Luan de Souza, Fabio Cozman", "title": "Why should I not follow you? Reasons For and Reasons Against in\n  Responsible Recommender Systems", "comments": "6 pages, 4 figures, ACM Recsys 2020, 3rd FAccTRec Workshop:\n  Responsible Recommendation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few Recommender Systems (RS) resort to explanations so as to enhance trust\nin recommendations. However, current techniques for explanation generation tend\nto strongly uphold the recommended products instead of presenting both reasons\nfor and reasons against them. We argue that an RS can better enhance overall\ntrust and transparency by frankly displaying both kinds of reasons to users.We\nhave developed such an RS by exploiting knowledge graphs and by applying\nSnedegar's theory of practical reasoning. We show that our implemented RS has\nexcellent performance and we report on an experiment with human subjects that\nshows the value of presenting both reasons for and against, with significant\nimprovements in trust, engagement, and persuasion.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 23:16:04 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 15:15:41 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Polleti", "Gustavo Padilha", ""], ["de Souza", "Douglas Luan", ""], ["Cozman", "Fabio", ""]]}, {"id": "2009.02051", "submitter": "Verena Haunschmid", "authors": "Verena Haunschmid, Ethan Manilow, Gerhard Widmer", "title": "Towards Musically Meaningful Explanations Using Source Separation", "comments": "6+2 pages, 4 figures; Submitted to International Society for Music\n  Information Retrieval Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are successfully applied in a wide variety of\nmusic information retrieval (MIR) tasks. Such models are usually considered\n\"black boxes\", meaning that their predictions are not interpretable. Prior work\non explainable models in MIR has generally used image processing tools to\nproduce explanations for DNN predictions, but these are not necessarily\nmusically meaningful, or can be listened to (which, arguably, is important in\nmusic). We propose audioLIME, a method based on Local Interpretable\nModel-agnostic Explanation (LIME), extended by a musical definition of\nlocality. LIME learns locally linear models on perturbations of an example that\nwe want to explain. Instead of extracting components of the spectrogram using\nimage segmentation as part of the LIME pipeline, we propose using source\nseparation. The perturbations are created by switching on/off sources which\nmakes our explanations listenable. We first validate audioLIME on a classifier\nthat was deliberately trained to confuse the true target with a spurious\nsignal, and show that this can easily be detected using our method. We then\nshow that it passes a sanity check that many available explanation methods\nfail. Finally, we demonstrate the general applicability of our (model-agnostic)\nmethod on a third-party music tagger.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 08:09:03 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Haunschmid", "Verena", ""], ["Manilow", "Ethan", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2009.02147", "submitter": "Yichao Wang", "authors": "Yichao Wang, Huifeng Guo, Ruiming Tang, Zhirong Liu, Xiuqiang He", "title": "A Practical Incremental Method to Train Deep CTR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models in recommender systems are usually trained in the batch\nmode, namely iteratively trained on a fixed-size window of training data. Such\nbatch mode training of deep learning models suffers from low training\nefficiency, which may lead to performance degradation when the model is not\nproduced on time. To tackle this issue, incremental learning is proposed and\nhas received much attention recently. Incremental learning has great potential\nin recommender systems, as two consecutive window of training data overlap most\nof the volume. It aims to update the model incrementally with only the newly\nincoming samples from the timestamp when the model is updated last time, which\nis much more efficient than the batch mode training. However, most of the\nincremental learning methods focus on the research area of image recognition\nwhere new tasks or classes are learned over time. In this work, we introduce a\npractical incremental method to train deep CTR models, which consists of three\ndecoupled modules (namely, data, feature and model module). Our method can\nachieve comparable performance to the conventional batch mode training with\nmuch better training efficiency. We conduct extensive experiments on a public\nbenchmark and a private dataset to demonstrate the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 12:35:42 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Wang", "Yichao", ""], ["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["Liu", "Zhirong", ""], ["He", "Xiuqiang", ""]]}, {"id": "2009.02251", "submitter": "Xiangyun Ding", "authors": "Xiangyun Ding, Wenjian Yu, Yuyang Xie, Shenghua Liu", "title": "Efficient Model-Based Collaborative Filtering with Fast Adaptive PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model-based collaborative filtering (CF) approach utilizing fast adaptive\nrandomized singular value decomposition (SVD) is proposed for the matrix\ncompletion problem in recommender system. Firstly, a fast adaptive PCA\nframeworkis presented which combines the fixed-precision randomized matrix\nfactorization algorithm [1] and accelerating skills for handling large sparse\ndata. Then, a novel termination mechanism for the adaptive PCA is proposed to\nautomatically determine a number of latent factors for achieving the near\noptimal prediction accuracy during the subsequent model-based CF. The resulted\nCF approach has good accuracy while inheriting high runtime efficiency.\nExperiments on real data show that, the proposed adaptive PCA is up to 2.7X and\n6.7X faster than the original fixed-precision SVD approach [1] and svds in\nMatlab repsectively, while preserving accuracy. The proposed model-based CF\napproach is able to efficiently process the MovieLens data with 20M ratings and\nexhibits more than 10X speedup over the regularized matrix factorization based\napproach [2] and the fast singular value thresholding approach [3] with\ncomparable or better accuracy. It also owns the advantage of parameter free.\nCompared with the deep-learning-based CF approach, the proposed approach is\nmuch more computationally efficient, with just marginal performance loss.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:32:14 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Ding", "Xiangyun", ""], ["Yu", "Wenjian", ""], ["Xie", "Yuyang", ""], ["Liu", "Shenghua", ""]]}, {"id": "2009.02252", "submitter": "Fabio Petroni", "authors": "Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid\n  Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin,\n  Jean Maillard, Vassilis Plachouras, Tim Rockt\\\"aschel, Sebastian Riedel", "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks", "comments": "accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Challenging problems such as open-domain question answering, fact checking,\nslot filling and entity linking require access to large, external knowledge\nsources. While some models do well on individual tasks, developing general\nmodels is difficult as each task might require computationally expensive\nindexing of custom knowledge sources, in addition to dedicated infrastructure.\nTo catalyze research on models that condition on specific information in large\ntextual resources, we present a benchmark for knowledge-intensive language\ntasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia,\nreducing engineering turnaround through the re-use of components, as well as\naccelerating research into task-agnostic memory architectures. We test both\ntask-specific and general baselines, evaluating downstream performance in\naddition to the ability of the models to provide provenance. We find that a\nshared dense vector index coupled with a seq2seq model is a strong baseline,\noutperforming more tailor-made approaches for fact checking, open-domain\nquestion answering and dialogue, and yielding competitive results on entity\nlinking and slot filling, by generating disambiguated text. KILT data and code\nare available at https://github.com/facebookresearch/KILT.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 15:32:19 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 08:59:41 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 09:27:43 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 15:20:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Petroni", "Fabio", ""], ["Piktus", "Aleksandra", ""], ["Fan", "Angela", ""], ["Lewis", "Patrick", ""], ["Yazdani", "Majid", ""], ["De Cao", "Nicola", ""], ["Thorne", "James", ""], ["Jernite", "Yacine", ""], ["Karpukhin", "Vladimir", ""], ["Maillard", "Jean", ""], ["Plachouras", "Vassilis", ""], ["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2009.02423", "submitter": "Harshal Chaudhari", "authors": "Harshal A. Chaudhari, Sangdi Lin, Ondrej Linda", "title": "A General Framework for Fairness in Multistakeholder Recommendations", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary recommender systems act as intermediaries on multi-sided\nplatforms serving high utility recommendations from sellers to buyers. Such\nsystems attempt to balance the objectives of multiple stakeholders including\nsellers, buyers, and the platform itself. The difficulty in providing\nrecommendations that maximize the utility for a buyer, while simultaneously\nrepresenting all the sellers on the platform has lead to many interesting\nresearch problems.Traditionally, they have been formulated as integer linear\nprograms which compute recommendations for all the buyers together in an\n\\emph{offline} fashion, by incorporating coverage constraints so that the\nindividual sellers are proportionally represented across all the recommended\nitems. Such approaches can lead to unforeseen biases wherein certain buyers\nconsistently receive low utility recommendations in order to meet the global\nseller coverage constraints. To remedy this situation, we propose a general\nformulation that incorporates seller coverage objectives alongside individual\nbuyer objectives in a real-time personalized recommender system. In addition,\nwe leverage highly scalable submodular optimization algorithms to provide\nrecommendations to each buyer with provable theoretical quality bounds.\nFurthermore, we empirically evaluate the efficacy of our approach using data\nfrom an online real-estate marketplace.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 23:54:06 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Chaudhari", "Harshal A.", ""], ["Lin", "Sangdi", ""], ["Linda", "Ondrej", ""]]}, {"id": "2009.02526", "submitter": "Riza Ozcelik", "authors": "Abdullatif K\\\"oksal, Hilal D\\\"onmez, R{\\i}za \\\"Oz\\c{c}elik, Elif\n  Ozkirimli, Arzucan \\\"Ozg\\\"ur", "title": "Vapur: A Search Engine to Find Related Protein-Compound Pairs in\n  COVID-19 Literature", "comments": "EMNLP 2020 - COVID-19 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG q-bio.MN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus Disease of 2019 (COVID-19) created dire consequences globally and\ntriggered an intense scientific effort from different domains. The resulting\npublications created a huge text collection in which finding the studies\nrelated to a biomolecule of interest is challenging for general purpose search\nengines because the publications are rich in domain specific terminology. Here,\nwe present Vapur: an online COVID-19 search engine specifically designed to\nfind related protein - chemical pairs. Vapur is empowered with a\nrelation-oriented inverted index that is able to retrieve and group studies for\na query biomolecule with respect to its related entities. The inverted index of\nVapur is automatically created with a BioNLP pipeline and integrated with an\nonline user interface. The online interface is designed for the smooth\ntraversal of the current literature by domain researchers and is publicly\navailable at https://tabilab.cmpe.boun.edu.tr/vapur/ .\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 12:50:54 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 21:45:27 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 06:34:24 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["K\u00f6ksal", "Abdullatif", ""], ["D\u00f6nmez", "Hilal", ""], ["\u00d6z\u00e7elik", "R\u0131za", ""], ["Ozkirimli", "Elif", ""], ["\u00d6zg\u00fcr", "Arzucan", ""]]}, {"id": "2009.02590", "submitter": "Nasim Sonboli", "authors": "Nasim Sonboli, Robin Burke, Nicholas Mattei, Farzad Eskandanian, Tian\n  Gao", "title": "\"And the Winner Is...\": Dynamic Lotteries for Multi-group Fairness-Aware\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As recommender systems are being designed and deployed for an increasing\nnumber of socially-consequential applications, it has become important to\nconsider what properties of fairness these systems exhibit. There has been\nconsiderable research on recommendation fairness. However, we argue that the\nprevious literature has been based on simple, uniform and often uni-dimensional\nnotions of fairness assumptions that do not recognize the real-world\ncomplexities of fairness-aware applications. In this paper, we explicitly\nrepresent the design decisions that enter into the trade-off between accuracy\nand fairness across multiply-defined and intersecting protected groups,\nsupporting multiple fairness metrics. The framework also allows the recommender\nto adjust its performance based on the historical view of recommendations that\nhave been delivered over a time horizon, dynamically rebalancing between\nfairness concerns. Within this framework, we formulate lottery-based mechanisms\nfor choosing between fairness concerns, and demonstrate their performance in\ntwo recommendation domains.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 20:15:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Sonboli", "Nasim", ""], ["Burke", "Robin", ""], ["Mattei", "Nicholas", ""], ["Eskandanian", "Farzad", ""], ["Gao", "Tian", ""]]}, {"id": "2009.02623", "submitter": "Zifeng Wang", "authors": "Zifeng Wang and Xi Chen and Rui Wen and Shao-Lun Huang and Ercan E.\n  Kuruoglu and Yefeng Zheng", "title": "Information Theoretic Counterfactual Learning from Missing-Not-At-Random\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual learning for dealing with missing-not-at-random data (MNAR) is\nan intriguing topic in the recommendation literature since MNAR data are\nubiquitous in modern recommender systems. Missing-at-random (MAR) data, namely\nrandomized controlled trials (RCTs), are usually required by most previous\ncounterfactual learning methods for debiasing learning. However, the execution\nof RCTs is extraordinarily expensive in practice. To circumvent the use of\nRCTs, we build an information-theoretic counterfactual variational information\nbottleneck (CVIB), as an alternative for debiasing learning without RCTs. By\nseparating the task-aware mutual information term in the original information\nbottleneck Lagrangian into factual and counterfactual parts, we derive a\ncontrastive information loss and an additional output confidence penalty, which\nfacilitates balanced learning between the factual and counterfactual domains.\nEmpirical evaluation on real-world datasets shows that our CVIB significantly\nenhances both shallow and deep models, which sheds light on counterfactual\nlearning in recommendation that goes beyond RCTs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 01:22:47 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 13:54:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Zifeng", ""], ["Chen", "Xi", ""], ["Wen", "Rui", ""], ["Huang", "Shao-Lun", ""], ["Kuruoglu", "Ercan E.", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2009.02625", "submitter": "Zifeng Wang", "authors": "Zifeng Wang and Rui Wen and Xi Chen and Shilei Cao and Shao-Lun Huang\n  and Buyue Qian and Yefeng Zheng", "title": "Online Disease Self-diagnosis with Inductive Heterogeneous Graph\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3449795", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Healthcare Graph Convolutional Network (HealGCN) to offer\ndisease self-diagnosis service for online users based on Electronic Healthcare\nRecords (EHRs). Two main challenges are focused in this paper for online\ndisease diagnosis: (1) serving cold-start users via graph convolutional\nnetworks and (2) handling scarce clinical description via a symptom retrieval\nsystem. To this end, we first organize the EHR data into a heterogeneous graph\nthat is capable of modeling complex interactions among users, symptoms and\ndiseases, and tailor the graph representation learning towards disease\ndiagnosis with an inductive learning paradigm. Then, we build a disease\nself-diagnosis system with a corresponding EHR Graph-based Symptom Retrieval\nSystem (GraphRet) that can search and provide a list of relevant alternative\nsymptoms by tracing the predefined meta-paths. GraphRet helps enrich the seed\nsymptom set through the EHR graph when confronting users with scarce\ndescriptions, hence yield better diagnosis accuracy. At last, we validate the\nsuperiority of our model on a large-scale EHR dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 01:32:14 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 01:47:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Zifeng", ""], ["Wen", "Rui", ""], ["Chen", "Xi", ""], ["Cao", "Shilei", ""], ["Huang", "Shao-Lun", ""], ["Qian", "Buyue", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2009.02637", "submitter": "Zheng Gao", "authors": "Zheng Gao, Hongsong Li, Zhuoren Jiang, Xiaozhong Liu", "title": "Detecting User Community in Sparse Domain via Cross-Graph Pairwise\n  Learning", "comments": "10 pages, 5 figures", "journal-ref": "SIGIR 2020", "doi": "10.1145/3397271.3401055", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberspace hosts abundant interactions between users and different kinds of\nobjects, and their relations are often encapsulated as bipartite graphs.\nDetecting user community in such heterogeneous graphs is an essential task to\nuncover user information needs and to further enhance recommendation\nperformance. While several main cyber domains carrying high-quality graphs,\nunfortunately, most others can be quite sparse. However, as users may appear in\nmultiple domains (graphs), their high-quality activities in the main domains\ncan supply community detection in the sparse ones, e.g., user behaviors on\nGoogle can help thousands of applications to locate his/her local community\nwhen s/he uses Google ID to login those applications. In this paper, our model,\nPairwise Cross-graph Community Detection (PCCD), is proposed to cope with the\nsparse graph problem by involving external graph knowledge to learn user\npairwise community closeness instead of detecting direct communities.\nParticularly in our model, to avoid taking excessive propagated information, a\ntwo-level filtering module is utilized to select the most informative\nconnections through both community and node level filters. Subsequently, a\nCommunity Recurrent Unit (CRU) is designed to estimate pairwise user community\ncloseness. Extensive experiments on two real-world graph datasets validate our\nmodel against several strong alternatives. Supplementary experiments also\nvalidate its robustness on graphs with varied sparsity scales.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 02:51:06 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gao", "Zheng", ""], ["Li", "Hongsong", ""], ["Jiang", "Zhuoren", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "2009.02657", "submitter": "Zheng Gao", "authors": "Zheng Gao, Chun Guo, Xiaozhong Liu", "title": "Efficient Personalized Community Detection via Genetic Evolution", "comments": "9 pages, 4 figures", "journal-ref": "GECCO 2020", "doi": "10.1145/3321707.3321711", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized community detection aims to generate communities associated with\nuser need on graphs, which benefits many downstream tasks such as node\nrecommendation and link prediction for users, etc. It is of great importance\nbut lack of enough attention in previous studies which are on topics of\nuser-independent, semi-supervised, or top-K user-centric community detection.\nMeanwhile, most of their models are time consuming due to the complex graph\nstructure. Different from these topics, personalized community detection\nrequires to provide higher-resolution partition on nodes that are more relevant\nto user need while coarser manner partition on the remaining less relevant\nnodes. In this paper, to solve this task in an efficient way, we propose a\ngenetic model including an offline and an online step. In the offline step, the\nuser-independent community structure is encoded as a binary tree. And\nsubsequently an online genetic pruning step is applied to partition the tree\ninto communities. To accelerate the speed, we also deploy a distributed version\nof our model to run under parallel environment. Extensive experiments on\nmultiple datasets show that our model outperforms the state-of-arts with\nsignificantly reduced running time.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 06:59:12 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gao", "Zheng", ""], ["Guo", "Chun", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "2009.02684", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "An Improved Algorithm for Fast K-Word Proximity Search Based on\n  Multi-Component Key Indexes", "comments": null, "journal-ref": "Intelligent Systems and Applications. IntelliSys 2020. Advances in\n  Intelligent Systems and Computing, vol 1251", "doi": "10.1007/978-3-030-55187-2_37", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search query consists of several words. In a proximity full-text search, we\nwant to find documents that contain these words near each other. This task\nrequires much time when the query consists of high-frequently occurring words.\nIf we cannot avoid this task by excluding high-frequently occurring words from\nconsideration by declaring them as stop words, then we can optimize our\nsolution by introducing additional indexes for faster execution. In a previous\nwork, we discussed how to decrease the search time with multi-component key\nindexes. We had shown that additional indexes can be used to improve the\naverage query execution time up to 130 times if queries consisted of\nhigh-frequently occurring words. In this paper, we present another search\nalgorithm that overcomes some limitations of our previous algorithm and\nprovides even more performance gain.\n  This is a pre-print of a contribution published in Arai K., Kapoor S., Bhatia\nR. (eds) Intelligent Systems and Applications. IntelliSys 2020. Advances in\nIntelligent Systems and Computing, vol 1251, published by Springer, Cham. The\nfinal authenticated version is available online at:\nhttps://doi.org/10.1007/978-3-030-55187-2_37\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 09:25:13 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "2009.02782", "submitter": "Nava Tintarev", "authors": "Boning Gong and Mesut Kaya and Nava Tintarev", "title": "Contextual Personalized Re-Ranking of Music Recommendations through\n  Audio Features", "comments": "RecSys 2020: CARS 2.0: Workshop on Context-Aware Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users are able to access millions of songs through music streaming services\nlike Spotify, Pandora, and Deezer. Access to such large catalogs, created a\nneed for relevant song recommendations. However, user preferences are highly\nsubjective in nature and change according to context (e.g., music that is\nsuitable in the morning is not as suitable in the evening). Moreover, the music\none user may prefer in a given context may be different from what another user\nprefers in the same context (i.e., what is considered good morning music\ndiffers across users). Accurately representing these preferences is essential\nto creating accurate and effective song recommendations. User preferences for\nsongs can be based on high level audio features, such as tempo and valence. In\nthis paper, we therefore propose a contextual re-ranking algorithm, based on\naudio feature representations of user preferences in specific contextual\nconditions. We evaluate the performance of our re-ranking algorithm using the\n#NowPlaying-RS dataset, which exists of user listening events crawled from\nTwitter and is enriched with song audio features. We compare a global (context\nfor all users) and personalized (context for each user) model based on these\naudio features. The global model creates an audio feature representation of\neach contextual condition based on the preferences of all users. Unlike the\nglobal model, the personalized model creates user-specific audio feature\nrepresentations of contextual conditions, and is measured across 333 distinct\nusers. We show that the personalized model outperforms the global model when\nevaluated using the precision and mean average precision metrics.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:27:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gong", "Boning", ""], ["Kaya", "Mesut", ""], ["Tintarev", "Nava", ""]]}, {"id": "2009.02865", "submitter": "Dylan Cashman", "authors": "Dylan Cashman, Shenyu Xu, Subhajit Das, Florian Heimerl, Cong Liu,\n  Shah Rukh Humayoun, Michael Gleicher, Alex Endert, Remco Chang", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data\n  Augmentation Using Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most visual analytics systems assume that all foraging for data happens\nbefore the analytics process; once analysis begins, the set of data attributes\nconsidered is fixed. Such separation of data construction from analysis\nprecludes iteration that can enable foraging informed by the needs that arise\nin-situ during the analysis. The separation of the foraging loop from the data\nanalysis tasks can limit the pace and scope of analysis. In this paper, we\npresent CAVA, a system that integrates data curation and data augmentation with\nthe traditional data exploration and analysis tasks, enabling information\nforaging in-situ during analysis. Identifying attributes to add to the dataset\nis difficult because it requires human knowledge to determine which available\nattributes will be helpful for the ensuing analytical tasks. CAVA crawls\nknowledge graphs to provide users with a a broad set of attributes drawn from\nexternal data to choose from. Users can then specify complex operations on\nknowledge graphs to construct additional attributes. CAVA shows how visual\nanalytics can help users forage for attributes by letting users visually\nexplore the set of available data, and by serving as an interface for query\nconstruction. It also provides visualizations of the knowledge graph itself to\nhelp users understand complex joins such as multi-hop aggregations. We assess\nthe ability of our system to enable users to perform complex data combinations\nwithout programming in a user study over two datasets. We then demonstrate the\ngeneralizability of CAVA through two additional usage scenarios. The results of\nthe evaluation confirm that CAVA is effective in helping the user perform data\nforaging that leads to improved analysis outcomes, and offer evidence in\nsupport of integrating data augmentation as a part of the visual analytics\npipeline.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 02:57:19 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Cashman", "Dylan", ""], ["Xu", "Shenyu", ""], ["Das", "Subhajit", ""], ["Heimerl", "Florian", ""], ["Liu", "Cong", ""], ["Humayoun", "Shah Rukh", ""], ["Gleicher", "Michael", ""], ["Endert", "Alex", ""], ["Chang", "Remco", ""]]}, {"id": "2009.02931", "submitter": "Preslav Nakov", "authors": "Alex Nikolov, Giovanni Da San Martino, Ivan Koychev, and Preslav Nakov", "title": "Team Alex at CLEF CheckThat! 2020: Identifying Check-Worthy Tweets With\n  Transformer Models", "comments": "Check-worthiness; Fact-Checking; Veracity", "journal-ref": "CLEF-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While misinformation and disinformation have been thriving in social media\nfor years, with the emergence of the COVID-19 pandemic, the political and the\nhealth misinformation merged, thus elevating the problem to a whole new level\nand giving rise to the first global infodemic. The fight against this infodemic\nhas many aspects, with fact-checking and debunking false and misleading claims\nbeing among the most important ones. Unfortunately, manual fact-checking is\ntime-consuming and automatic fact-checking is resource-intense, which means\nthat we need to pre-filter the input social media posts and to throw out those\nthat do not appear to be check-worthy. With this in mind, here we propose a\nmodel for detecting check-worthy tweets about COVID-19, which combines deep\ncontextualized text representations with modeling the social context of the\ntweet. We further describe a number of additional experiments and comparisons,\nwhich we believe should be useful for future research as they provide some\nindication about what techniques are effective for the task. Our official\nsubmission to the English version of CLEF-2020 CheckThat! Task 1, system\nTeam_Alex, was ranked second with a MAP score of 0.8034, which is almost tied\nwith the wining system, lagging behind by just 0.003 MAP points absolute.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 08:03:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Nikolov", "Alex", ""], ["Martino", "Giovanni Da San", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "2009.03014", "submitter": "Samudra Herath", "authors": "Samudra Herath, Matthew Roughan, Gary Glonek", "title": "Simulating Name-like Vectors for Testing Large-scale Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient entity resolution (ER) has been a problem in data\nanalysis and data mining projects for decades. In our work, we are interested\nin developing ER methods to handle big data. Good public datasets are\nrestricted in this area and usually small in size. Simulation is one technique\nfor generating datasets for testing. Existing simulation tools have problems of\ncomplexity, scalability and limitations of resampling. We address these\nproblems by introducing a better way of simulating testing data for big data\nER. Our proposed simulation model is simple, inexpensive and fast. We focus on\navoiding the detail-level simulation of records using a simple vector\nrepresentation. In this paper, we will discuss how to simulate simple vectors\nthat approximate the properties of names (commonly used as identification\nkeys).\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 11:00:17 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Herath", "Samudra", ""], ["Roughan", "Matthew", ""], ["Glonek", "Gary", ""]]}, {"id": "2009.03051", "submitter": "Kashif Ahmad Dr", "authors": "Syed Zohaib Hassan, Kashif Ahmad, Steven Hicks, Paal Halvorsen, Ala\n  Al-Fuqaha, Nicola Conci, Michael Riegler", "title": "Visual Sentiment Analysis from Disaster Images in Social Media", "comments": "10 pages, 6 figures, 6 tables. arXiv admin note: substantial text\n  overlap with arXiv:2002.03773", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social networks and users' tendency towards\nsharing their feelings, expressions, and opinions in text, visual, and audio\ncontent, have opened new opportunities and challenges in sentiment analysis.\nWhile sentiment analysis of text streams has been widely explored in\nliterature, sentiment analysis from images and videos is relatively new. This\narticle focuses on visual sentiment analysis in a societal important domain,\nnamely disaster analysis in social media. To this aim, we propose a deep visual\nsentiment analyzer for disaster related images, covering different aspects of\nvisual sentiment analysis starting from data collection, annotation, model\nselection, implementation, and evaluations. For data annotation, and analyzing\npeoples' sentiments towards natural disasters and associated images in social\nmedia, a crowd-sourcing study has been conducted with a large number of\nparticipants worldwide. The crowd-sourcing study resulted in a large-scale\nbenchmark dataset with four different sets of annotations, each aiming a\nseparate task. The presented analysis and the associated dataset will provide a\nbaseline/benchmark for future research in the domain. We believe the proposed\nsystem can contribute toward more livable communities by helping different\nstakeholders, such as news broadcasters, humanitarian organizations, as well as\nthe general public.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:29:52 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Hassan", "Syed Zohaib", ""], ["Ahmad", "Kashif", ""], ["Hicks", "Steven", ""], ["Halvorsen", "Paal", ""], ["Al-Fuqaha", "Ala", ""], ["Conci", "Nicola", ""], ["Riegler", "Michael", ""]]}, {"id": "2009.03087", "submitter": "Josimar Chire Saire", "authors": "Josimar Edinson Chire Saire, Honorio Apaza Alanoca", "title": "Text Mining over Curriculum Vitae of Peruvian Professionals using\n  Official Scientific Site DINA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade, Peruvian government started to invest and promote\nScience and Technology through Concytec(National Council of Science and\nTechnology). Many programs are oriented to support research projects, expenses\nfor paper presentation, organization of conferences/ events and more. Concytec\ncreated a National Directory of Researchers(DINA) where professionals can\ncreate and add curriculum vitae, Concytec can provide official title of\nResearcher following some criterion for the evaluation. The actual paper aims\nto conduct an exploratory analysis over the curriculum vitae of Peruvian\nProfessionals using Data Mining Approach to understand Peruvian context.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:16:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Saire", "Josimar Edinson Chire", ""], ["Alanoca", "Honorio Apaza", ""]]}, {"id": "2009.03091", "submitter": "Lenart Treven", "authors": "Luka Kolar, Rok \\v{S}ikonja, Lenart Treven", "title": "Iterative Correction of Sensor Degradation and a Bayesian Multi-Sensor\n  Data Fusion Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for inferring ground-truth signal from multiple\ndegraded signals, affected by different amounts of sensor exposure. The\nalgorithm learns a multiplicative degradation effect by performing iterative\ncorrections of two signals solely from the ratio between them. The degradation\nfunction d should be continuous, satisfy monotonicity, and d(0) = 1. We use\nsmoothed monotonic regression method, where we easily incorporate the\naforementioned criteria to the fitting part. We include theoretical analysis\nand prove convergence to the ground-truth signal for the noiseless measurement\nmodel. Lastly, we present an approach to fuse the noisy corrected signals using\nGaussian processes. We use sparse Gaussian processes that can be utilized for a\nlarge number of measurements together with a specialized kernel that enables\nthe estimation of noise values of all sensors. The data fusion framework\nnaturally handles data gaps and provides a simple and powerful method for\nobserving the signal trends on multiple timescales(long-term and short-term\nsignal properties). The viability of correction method is evaluated on a\nsynthetic dataset with known ground-truth signal.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:24:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kolar", "Luka", ""], ["\u0160ikonja", "Rok", ""], ["Treven", "Lenart", ""]]}, {"id": "2009.03207", "submitter": "James Grant", "authors": "James A. Grant, David S. Leslie", "title": "Learning to Rank under Multinomial Logit Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the optimal ordering of content is an important challenge in website\ndesign. The learning to rank (LTR) framework models this problem as a\nsequential problem of selecting lists of content and observing where users\ndecide to click. Most previous work on LTR assumes that the user considers each\nitem in the list in isolation, and makes binary choices to click or not on\neach. We introduce a multinomial logit (MNL) choice model to the LTR framework,\nwhich captures the behaviour of users who consider the ordered list of items as\na whole and make a single choice among all the items and a no-click option.\nUnder the MNL model, the user favours items which are either inherently more\nattractive, or placed in a preferable position within the list. We propose\nupper confidence bound algorithms to minimise regret in two settings - where\nthe position dependent parameters are known, and unknown. We present\ntheoretical analysis leading to an $\\Omega(\\sqrt{T})$ lower bound for the\nproblem, an $\\tilde{O}(\\sqrt{T})$ upper bound on regret for the known parameter\nversion. Our analyses are based on tight new concentration results for\nGeometric random variables, and novel functional inequalities for maximum\nlikelihood estimators computed on discrete data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 16:15:12 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Grant", "James A.", ""], ["Leslie", "David S.", ""]]}, {"id": "2009.03257", "submitter": "Leon Moonen", "authors": "Carl Martin Rosenberg and Leon Moonen", "title": "Improving Problem Identification via Automated Log Clustering using\n  Dimensionality Reduction", "comments": null, "journal-ref": "Published in ESEM'18, Proceedings of the 12th ACM/IEEE\n  International Symposium on Empirical Software Engineering and Measurement,\n  October 2018, Article: 16, pp. 1-10,", "doi": "10.1145/3239235.3239248", "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: We consider the problem of automatically grouping logs of runs that\nfailed for the same underlying reasons, so that they can be treated more\neffectively, and investigate the following questions: (1) Does an approach\ndeveloped to identify problems in system logs generalize to identifying\nproblems in continuous deployment logs? (2) How does dimensionality reduction\naffect the quality of automated log clustering? (3) How does the criterion used\nfor merging clusters in the clustering algorithm affect clustering quality?\n  Method: We replicate and extend earlier work on clustering system log files\nto assess its generalization to continuous deployment logs. We consider the\noptional inclusion of one of these dimensionality reduction techniques:\nPrincipal Component Analysis (PCA), Latent Semantic Indexing (LSI), and\nNon-negative Matrix Factorization (NMF). Moreover, we consider three\nalternative cluster merge criteria (Single Linkage, Average Linkage, and\nWeighted Linkage), in addition to the Complete Linkage criterion used in\nearlier work. We empirically evaluate the 16 resulting configurations on\ncontinuous deployment logs provided by our industrial collaborator.\n  Results: Our study shows that (1) identifying problems in continuous\ndeployment logs via clustering is feasible, (2) including NMF significantly\nimproves overall accuracy and robustness, and (3) Complete Linkage performs\nbest of all merge criteria analyzed.\n  Conclusions: We conclude that problem identification via automated log\nclustering is improved by including dimensionality reduction, as it decreases\nthe pipeline's sensitivity to parameter choice, thereby increasing its\nrobustness for handling different inputs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:26:18 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Rosenberg", "Carl Martin", ""], ["Moonen", "Leon", ""]]}, {"id": "2009.03258", "submitter": "Akhil Sai Peddireddy", "authors": "Akhil Sai Peddireddy", "title": "Personalized Review Ranking for Improving Shopper's Decision Making: A\n  Term Frequency based Approach", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-generated reviews serve as crucial references in shopper's\ndecision-making process. Moreover, they improve product sales and validate the\nreputation of the website as a whole. Thus, it becomes important to design\nreviews ranking methods that help shoppers make informed decisions quickly.\nHowever, reviews ranking has its unique challenges. First, there is no\nrelevance labels for reviews. A relevant review for shopper A might not be\nrelevant to shopper B. Second, since shoppers cannot click on reviews, we have\nno ways of getting relevance feedback. Eventually, reviews ranking suffers from\nthe lack of ground truth due to the variability in the standard of relevance\nfor different users. In this paper, we aim to address the challenges of helping\nusers to find information they might be interested in from the sea of customer\nreviews. Using the Amazon Customer Reviews Dataset collected and organized by\nUCSD, we first constructed user profiles based on user's personal web trails,\nrecent shopping history and previous reviews, incorporated user profiles into\nour ranking algorithm, and assigned higher ranks to reviews that address\nindividual shopper's concerns to the largest extent. Also, we leveraged user\nprofiles to recommend products based on reviews texts. We evaluated our model\nbased on both empirical evaluations and numerical evaluations of review scores.\nThe results from both evaluation methods reveal a significant increase in the\nquality of top reviews as well as user satisfaction for over 1000 products. Our\nreviews based recommendation system also suggests that there's a large chance\nof user viewing and liking the product we recommend. Our work shows the basic\nsteps of developing a ranking method that learns from a particular end-user's\npreferences.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:27:25 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Peddireddy", "Akhil Sai", ""]]}, {"id": "2009.03376", "submitter": "Jingtao Ding", "authors": "Jingtao Ding, Yuhan Quan, Quanming Yao, Yong Li, Depeng Jin", "title": "Simplify and Robustify Negative Sampling for Implicit Collaborative\n  Filtering", "comments": "20 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative sampling approaches are prevalent in implicit collaborative\nfiltering for obtaining negative labels from massive unlabeled data. As two\nmajor concerns in negative sampling, efficiency and effectiveness are still not\nfully achieved by recent works that use complicate structures and overlook risk\nof false negative instances. In this paper, we first provide a novel\nunderstanding of negative instances by empirically observing that only a few\ninstances are potentially important for model learning, and false negatives\ntend to have stable predictions over many training iterations. Above findings\nmotivate us to simplify the model by sampling from designed memory that only\nstores a few important candidates and, more importantly, tackle the untouched\nfalse negative problem by favouring high-variance samples stored in memory,\nwhich achieves efficient sampling of true negatives with high-quality.\nEmpirical results on two synthetic datasets and three real-world datasets\ndemonstrate both robustness and superiorities of our negative sampling method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 19:08:26 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ding", "Jingtao", ""], ["Quan", "Yuhan", ""], ["Yao", "Quanming", ""], ["Li", "Yong", ""], ["Jin", "Depeng", ""]]}, {"id": "2009.03668", "submitter": "Shuo Zhang", "authors": "Javeria Habib and Shuo Zhang and Krisztian Balog", "title": "IAI MovieBot: A Conversational Movie Recommender System", "comments": "Proceedings of the 29th ACM International Conference on Information\n  and Knowledge Management, Oct 2020", "journal-ref": null, "doi": "10.1145/3340531.3417433", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational recommender systems support users in accomplishing\nrecommendation-related goals via multi-turn conversations. To better model\ndynamically changing user preferences and provide the community with a reusable\ndevelopment framework, we introduce IAI MovieBot, a conversational recommender\nsystem for movies. It features a task-specific dialogue flow, a multi-modal\nchat interface, and an effective way to deal with dynamically changing user\npreferences. The system is made available open source and is operated as a\nchannel on Telegram.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:14:52 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Habib", "Javeria", ""], ["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "2009.03679", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "Proximity full-text searches of frequently occurring words with a\n  response time guarantee", "comments": "Text overlap with arXiv:1812.07640 in introduction and definitions,\n  the primary content is different", "journal-ref": "Mathematical Analysis With Applications. CONCORD-90 2018. Springer\n  Proceedings in Mathematics & Statistics, vol 318. Springer, Cham", "doi": "10.1007/978-3-030-42176-2_37", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full-text search engines are important tools for information retrieval. In a\nproximity full-text search, a document is relevant if it contains query terms\nnear each other, especially if the query terms are frequently occurring words.\nFor each word in the text, we use additional indexes to store information about\nnearby words at distances from the given word of less than or equal to\nMaxDistance, which is a parameter. A search algorithm for the case when the\nquery consists of high-frequently used words is discussed. In addition, we\npresent results of experiments with different values of MaxDistance to evaluate\nthe search speed dependence on the value of MaxDistance. These results show\nthat the average time of the query execution with our indexes is 94.7-45.9\ntimes (depending on the value of MaxDistance) less than that with standard\ninverted files when queries that contain high-frequently occurring words are\nevaluated.\n  This is a pre-print of a contribution published in Pinelas S., Kim A., Vlasov\nV. (eds) Mathematical Analysis With Applications. CONCORD-90 2018. Springer\nProceedings in Mathematics & Statistics, vol 318, published by Springer, Cham.\nThe final authenticated version is available online at:\nhttps://doi.org/10.1007/978-3-030-42176-2_37\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 09:14:59 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "2009.03779", "submitter": "Edward Raff", "authors": "Edward Raff, Richard Zak, Gary Lopez Munoz, William Fleming, Hyrum S.\n  Anderson, Bobby Filar, Charles Nicholas, James Holt", "title": "Automatic Yara Rule Generation Using Biclustering", "comments": "to be published in the 13th ACM Workshop on Artificial Intelligence\n  and Security (AISec)", "journal-ref": null, "doi": "10.1145/3411508.3421372", "report-no": null, "categories": "cs.CR cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yara rules are a ubiquitous tool among cybersecurity practitioners and\nanalysts. Developing high-quality Yara rules to detect a malware family of\ninterest can be labor- and time-intensive, even for expert users. Few tools\nexist and relatively little work has been done on how to automate the\ngeneration of Yara rules for specific families. In this paper, we leverage\nlarge n-grams ($n \\geq 8$) combined with a new biclustering algorithm to\nconstruct simple Yara rules more effectively than currently available software.\nOur method, AutoYara, is fast, allowing for deployment on low-resource\nequipment for teams that deploy to remote networks. Our results demonstrate\nthat AutoYara can help reduce analyst workload by producing rules with useful\ntrue-positive rates while maintaining low false-positive rates, sometimes\nmatching or even outperforming human analysts. In addition, real-world testing\nby malware analysts indicates AutoYara could reduce analyst time spent\nconstructing Yara rules by 44-86%, allowing them to spend their time on the\nmore advanced malware that current tools can't handle. Code will be made\navailable at https://github.com/NeuromorphicComputationResearchProgram .\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 02:02:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Raff", "Edward", ""], ["Zak", "Richard", ""], ["Munoz", "Gary Lopez", ""], ["Fleming", "William", ""], ["Anderson", "Hyrum S.", ""], ["Filar", "Bobby", ""], ["Nicholas", "Charles", ""], ["Holt", "James", ""]]}, {"id": "2009.04016", "submitter": "George Zerveas", "authors": "George Zerveas, Ruochen Zhang, Leila Kim, Carsten Eickhoff", "title": "Brown University at TREC Deep Learning 2019", "comments": null, "journal-ref": "Proceedings of the Twenty-Eighth Text REtrieval Conference, TREC\n  2019, Gaithersburg, Maryland, USA, November 13-15, 2019. NIST Special\n  Publication 1250, National Institute of Standards and Technology (NIST) 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Brown University's submission to the TREC 2019 Deep\nLearning track. We followed a 2-phase method for producing a ranking of\npassages for a given input query: In the the first phase, the user's query is\nexpanded by appending 3 queries generated by a transformer model which was\ntrained to rephrase an input query into semantically similar queries. The\nexpanded query can exhibit greater similarity in surface form and vocabulary\noverlap with the passages of interest and can therefore serve as enriched input\nto any downstream information retrieval method. In the second phase, we use a\nBERT-based model pre-trained for language modeling but fine-tuned for query -\ndocument relevance prediction to compute relevance scores for a set of 1000\ncandidate passages per query and subsequently obtain a ranking of passages by\nsorting them based on the predicted relevance scores. According to the results\npublished in the official Overview of the TREC Deep Learning Track 2019, our\nteam ranked 3rd in the passage retrieval task (including full ranking and\nre-ranking), and 2nd when considering only re-ranking submissions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 22:54:03 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zerveas", "George", ""], ["Zhang", "Ruochen", ""], ["Kim", "Leila", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2009.04104", "submitter": "Wei Hu", "authors": "Xinze Lyu and Guangyao Li and Jiacheng Huang and Wei Hu", "title": "Rule-Guided Graph Neural Networks for Recommender Systems", "comments": "Accepted in the 19th International Semantic Web Conference (ISWC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the cold start problem caused by collaborative filtering in\nrecommender systems, knowledge graphs (KGs) are increasingly employed by many\nmethods as auxiliary resources. However, existing work incorporated with KGs\ncannot capture the explicit long-range semantics between users and items\nmeanwhile consider various connectivity between items. In this paper, we\npropose RGRec, which combines rule learning and graph neural networks (GNNs)\nfor recommendation. RGRec first maps items to corresponding entities in KGs and\nadds users as new entities. Then, it automatically learns rules to model the\nexplicit long-range semantics, and captures the connectivity between entities\nby aggregation to better encode various information. We show the effectiveness\nof RGRec on three real-world datasets. Particularly, the combination of rule\nlearning and GNNs achieves substantial improvement compared to methods only\nusing either of them.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 05:00:02 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Lyu", "Xinze", ""], ["Li", "Guangyao", ""], ["Huang", "Jiacheng", ""], ["Hu", "Wei", ""]]}, {"id": "2009.04426", "submitter": "Felipe Del Rio", "authors": "Pablo Messina, Manuel Cartagena, Patricio Cerda-Mardini, Felipe del\n  Rio and Denis Parra", "title": "CuratorNet: Visually-aware Recommendation of Art Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are several visually-aware recommendation models in domains\nlike fashion or even movies, the art domain lacks thesame level of research\nattention, despite the recent growth of the online artwork market. To reduce\nthis gap, in this article we introduceCuratorNet, a neural network architecture\nfor visually-aware recommendation of art images. CuratorNet is designed at the\ncore withthe goal of maximizing generalization: the network has a fixed set of\nparameters that only need to be trained once, and thereafter themodel is able\nto generalize to new users or items never seen before, without further\ntraining. This is achieved by leveraging visualcontent: items are mapped to\nitem vectors through visual embeddings, and users are mapped to user vectors by\naggregating the visualcontent of items they have consumed. Besides the model\narchitecture, we also introduce novel triplet sampling strategies to build\natraining set for rank learning in the art domain, resulting in more effective\nlearning than naive random sampling. With an evaluationover a real-world\ndataset of physical paintings, we show that CuratorNet achieves the best\nperformance among several baselines,including the state-of-the-art model VBPR.\nCuratorNet is motivated and evaluated in the art domain, but its architecture\nand trainingscheme could be adapted to recommend images in other areas\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:22:17 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:35:08 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Messina", "Pablo", ""], ["Cartagena", "Manuel", ""], ["Cerda-Mardini", "Patricio", ""], ["del Rio", "Felipe", ""], ["Parra", "Denis", ""]]}, {"id": "2009.04441", "submitter": "Diego Antognini", "authors": "Kirtan Padh, Diego Antognini, Emma Lejal Glaude, Boi Faltings, Claudiu\n  Musat", "title": "Addressing Fairness in Classification with a Model-Agnostic\n  Multi-Objective Algorithm", "comments": "Accepted at UAI 2021. 14 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of fairness in classification is to learn a classifier that does not\ndiscriminate against groups of individuals based on sensitive attributes, such\nas race and gender. One approach to designing fair algorithms is to use\nrelaxations of fairness notions as regularization terms or in a constrained\noptimization problem. We observe that the hyperbolic tangent function can\napproximate the indicator function. We leverage this property to define a\ndifferentiable relaxation that approximates fairness notions provably better\nthan existing relaxations. In addition, we propose a model-agnostic\nmulti-objective architecture that can simultaneously optimize for multiple\nfairness notions and multiple sensitive attributes and supports all statistical\nparity-based notions of fairness. We use our relaxation with the\nmulti-objective architecture to learn fair classifiers. Experiments on public\ndatasets show that our method suffers a significantly lower loss of accuracy\nthan current debiasing algorithms relative to the unconstrained model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:40:24 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 17:17:00 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 12:39:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Padh", "Kirtan", ""], ["Antognini", "Diego", ""], ["Glaude", "Emma Lejal", ""], ["Faltings", "Boi", ""], ["Musat", "Claudiu", ""]]}, {"id": "2009.04485", "submitter": "Edward A. Fox", "authors": "Saurabh Chakravarty and Satvik Chekuri and Maanav Mehrotra and Edward\n  A. Fox", "title": "Aspect Classification for Legal Depositions", "comments": "19 pages, 3 figures, 11 tables, detailed version of shorter paper\n  being submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attorneys and others have a strong interest in having a digital library with\nsuitable services (e.g., summarizing, searching, and browsing) to help them\nwork with large corpora of legal depositions. Their needs often involve\nunderstanding the semantics of such documents. That depends in part on the role\nof the deponent, e.g., plaintiff, defendant, law enforcement personnel, expert,\netc. In the case of tort litigation associated with property and casualty\ninsurance claims, such as relating to an injury, it is important to know not\nonly about liability, but also about events, accidents, physical conditions,\nand treatments.\n  We hypothesize that a legal deposition consists of various aspects that are\ndiscussed as part of the deponent testimony. Accordingly, we developed an\nontology of aspects in a legal deposition for accident and injury cases. Using\nthat, we have developed a classifier that can identify portions of text for\neach of the aspects of interest. Doing so was complicated by the peculiarities\nof this genre, e.g., that deposition transcripts generally consist of data in\nthe form of question-answer (QA) pairs. Accordingly, our automated system\nstarts with pre-processing, and then transforms the QA pairs into a canonical\nform made up of declarative sentences. Classifying the declarative sentences\nthat are generated, according to the aspect, can then help with downstream\ntasks such as summarization, segmentation, question-answering, and information\nretrieval.\n  Our methods have achieved a classification F1 score of 0.83. Having the\naspects classified with a good accuracy will help in choosing QA pairs that can\nbe used as candidate summary sentences, and to generate an informative summary\nfor legal professionals or insurance claim agents. Our methodology could be\nextended to legal depositions of other kinds, and to aid services like\nsearching.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 18:00:15 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chakravarty", "Saurabh", ""], ["Chekuri", "Satvik", ""], ["Mehrotra", "Maanav", ""], ["Fox", "Edward A.", ""]]}, {"id": "2009.04695", "submitter": "Diego Antognini", "authors": "Blagoj Mitrevski, Milena Filipovic, Diego Antognini, Emma Lejal\n  Glaude, Boi Faltings, Claudiu Musat", "title": "Momentum-based Gradient Methods in Multi-objective Recommender Systems", "comments": "Under review. 8 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nuncorrelated objectives. Classic multi-gradient descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space.\n  In this work, we create a multi-objective Adamize method that leverage the\nbenefits of the Adam optimizer in single-objective problems. This corrects and\nstabilizes the gradients of every objective before calculating a common\ngradient descent vector that optimizes all the objectives simultaneously. We\nevaluate the benefits of Multi-objective Adamize on two multi-objective\nrecommender systems and for three different objective combinations, both\ncorrelated or uncorrelated. We report significant improvements, measured with\nthree different Pareto front metrics: hypervolume, coverage, and spacing.\nFinally, we show that the Adamized Pareto front strictly dominates the previous\none on multiple objective pairs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 07:12:21 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Mitrevski", "Blagoj", ""], ["Filipovic", "Milena", ""], ["Antognini", "Diego", ""], ["Glaude", "Emma Lejal", ""], ["Faltings", "Boi", ""], ["Musat", "Claudiu", ""]]}, {"id": "2009.04780", "submitter": "Krenare Pireva Nuci", "authors": "Denis Selimi, Krenare Pireva Nuci", "title": "The use of Recommender Systems in web technology and an in-depth\n  analysis of Cold State problem", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the WWW (World Wide Web), dynamic development and spread of data has\nresulted a tremendous amount of information available on the Internet, yet user\nis unable to find relevant information in a short span of time. Consequently, a\nsystem called recommendation system developed to help users find their\ninfromation with ease through their browsing activities. In other words,\nrecommender systems are tools for interacting with large amount of information\nthat provide personalized view for prioritizing items likely to be of keen for\nusers. They have developed over the years in artificial intelligence techniques\nthat include machine learning and data mining amongst many to mention.\nFurthermore, the recommendation systems have personalized on an e-commerce,\non-line applications such as Amazon.com, Netflix, and Booking.com. As a result,\nthis has inspired many researchers to extend the reach of recommendation\nsystems into new sets of challenges and problem areas that are yet to be truly\nsolved, primarily a problem with the case of making a recommendation to a new\nuser that is called cold-state (i.e. cold-start) user problem where the new\nuser might likely not yield much of information searched. Therfore, the purpose\nof this paper is to tackle the said cold-start problem with a few effecient\nmethods and challenges, as well as identify and overview the current state of\nrecommendation system as a whole\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:32:59 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Selimi", "Denis", ""], ["Nuci", "Krenare Pireva", ""]]}, {"id": "2009.04915", "submitter": "Trond Linjordet", "authors": "Trond Linjordet and Krisztian Balog", "title": "Sanitizing Synthetic Training Data Generation for Question Answering\n  over Knowledge Graphs", "comments": "Proceedings of the 2020 ACM SIGIR International Conference on Theory\n  of Information Retrieval (ICTIR '20), 2020. 6 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3409256.3409836", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data generation is important to training and evaluating neural\nmodels for question answering over knowledge graphs. The quality of the data\nand the partitioning of the datasets into training, validation and test splits\nimpact the performance of the models trained on this data. If the synthetic\ndata generation depends on templates, as is the predominant approach for this\ntask, there may be a leakage of information via a shared basis of templates\nacross data splits if the partitioning is not performed hygienically. This\npaper investigates the extent of such information leakage across data splits,\nand the ability of trained models to generalize to test data when the leakage\nis controlled. We find that information leakage indeed occurs and that it\naffects performance. At the same time, the trained models do generalize to test\ndata under the sanitized partitioning presented here. Importantly, these\nfindings extend beyond the particular flavor of question answering task we\nstudied and raise a series of difficult questions around template-based\nsynthetic data generation that will necessitate additional research.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:58:20 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Linjordet", "Trond", ""], ["Balog", "Krisztian", ""]]}, {"id": "2009.04964", "submitter": "Shaina Raza Ms", "authors": "Shaina Raza, Chen Ding", "title": "News Recommender System: A review of recent progress, challenges, and\n  opportunities", "comments": "Accepted in Artificial Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, more and more news readers tend to read news online where they have\naccess to millions of news articles from multiple sources. In order to help\nusers to find the right and relevant content, news recommender systems (NRS)\nare developed to relieve the information overload problem and suggest news\nitems that users might be interested in. In this paper, we highlight the major\nchallenges faced by the news recommendation domain and identify the possible\nsolutions from the state-of-the-art. Due to the rapid growth of building\nrecommender systems using deep learning models, we divide our discussion in two\nparts. In the first part, we present an overview of the conventional\nrecommendation solutions, datasets, evaluation criteria beyond accuracy and\nrecommendation platforms being used in NRS. In the second part, we explain the\ndeep learning-based recommendation solutions applied in NRS. Different from\nprevious surveys, we also study the effects of news recommendations on user\nbehavior and try to suggest the possible remedies to mitigate these effects. By\nproviding the state-of-the-art knowledge, this survey can help researchers and\npractical professionals in their understanding of developments in news\nrecommendation algorithms. It also sheds light on potential new directions\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 16:11:49 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 15:48:43 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 22:26:57 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 16:44:10 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Raza", "Shaina", ""], ["Ding", "Chen", ""]]}, {"id": "2009.05121", "submitter": "Sarvesh Soni", "authors": "Sarvesh Soni and Kirk Roberts", "title": "Patient Cohort Retrieval using Transformer Language Models", "comments": "Accepted at the AMIA Annual Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply deep learning-based language models to the task of patient cohort\nretrieval (CR) with the aim to assess their efficacy. The task of CR requires\nthe extraction of relevant documents from the electronic health records (EHRs)\non the basis of a given query. Given the recent advancements in the field of\ndocument retrieval, we map the task of CR to a document retrieval task and\napply various deep neural models implemented for the general domain tasks. In\nthis paper, we propose a framework for retrieving patient cohorts using neural\nlanguage models without the need of explicit feature engineering and domain\nexpertise. We find that a majority of our models outperform the BM25 baseline\nmethod on various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 19:40:41 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Soni", "Sarvesh", ""], ["Roberts", "Kirk", ""]]}, {"id": "2009.05138", "submitter": "Negin Golrezaei", "authors": "Negin Golrezaei, Vahideh Manshadi, Jon Schneider, Shreyas Sekar", "title": "Learning Product Rankings Robust to Fake Users", "comments": "65 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many online platforms, customers' decisions are substantially influenced\nby product rankings as most customers only examine a few top-ranked products.\nConcurrently, such platforms also use the same data corresponding to customers'\nactions to learn how these products must be ranked or ordered. These\ninteractions in the underlying learning process, however, may incentivize\nsellers to artificially inflate their position by employing fake users, as\nexemplified by the emergence of click farms. Motivated by such fraudulent\nbehavior, we study the ranking problem of a platform that faces a mixture of\nreal and fake users who are indistinguishable from one another. We first show\nthat existing learning algorithms---that are optimal in the absence of fake\nusers---may converge to highly sub-optimal rankings under manipulation by fake\nusers. To overcome this deficiency, we develop efficient learning algorithms\nunder two informational environments: in the first setting, the platform is\naware of the number of fake users, and in the second setting, it is agnostic to\nthe number of fake users. For both these environments, we prove that our\nalgorithms converge to the optimal ranking, while being robust to the\naforementioned fraudulent behavior; we also present worst-case performance\nguarantees for our methods, and show that they significantly outperform\nexisting algorithms. At a high level, our work employs several novel approaches\nto guarantee robustness such as: (i) constructing product-ordering graphs that\nencode the pairwise relationships between products inferred from the customers'\nactions; and (ii) implementing multiple levels of learning with a judicious\namount of bi-directional cross-learning between levels.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 20:26:02 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Golrezaei", "Negin", ""], ["Manshadi", "Vahideh", ""], ["Schneider", "Jon", ""], ["Sekar", "Shreyas", ""]]}, {"id": "2009.05183", "submitter": "Ye Tao", "authors": "Ye Tao, Can Wang, Lina Yao, Weimin Li, Yonghong Yu", "title": "TRec: Sequential Recommender Based On Latent Item Trend Information", "comments": "8 pages, accepted by IJCNN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system plays an important role in online web applications.\nSequential recommender further models user short-term preference through\nexploiting information from latest user-item interaction history. Most of the\nsequential recommendation methods neglect the importance of ever-changing item\npopularity. We propose the model from the intuition that items with most user\ninteractions may be popular in the past but could go out of fashion in recent\ndays. To this end, this paper proposes a novel sequential recommendation\napproach dubbed TRec, TRec learns item trend information from implicit user\ninteraction history and incorporates item trend information into next item\nrecommendation tasks. Then a self-attention mechanism is used to learn better\nnode representation. Our model is trained via pairwise rank-based optimization.\nWe conduct extensive experiments with seven baseline methods on four benchmark\ndatasets, The empirical result shows our approach outperforms other\nstateof-the-art methods while maintains a superiorly low runtime cost. Our\nstudy demonstrates the importance of item trend information in recommendation\nsystem designs, and our method also possesses great efficiency which enables it\nto be practical in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 00:31:39 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Tao", "Ye", ""], ["Wang", "Can", ""], ["Yao", "Lina", ""], ["Li", "Weimin", ""], ["Yu", "Yonghong", ""]]}, {"id": "2009.05381", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang Yang, Xun Wang,\n  Meng Wang", "title": "Dual Encoding for Video Retrieval by Text", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. Code and data will be available at\n  https://github.com/danieljf24/hybrid_space. Conference version:\n  arXiv:1809.06181", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3059295", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attacks the challenging problem of video retrieval by text. In\nsuch a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc\nqueries described exclusively in the form of a natural-language sentence, with\nno visual example provided. Given videos as sequences of frames and queries as\nsequences of words, an effective sequence-to-sequence cross-modal matching is\ncrucial. To that end, the two modalities need to be first encoded into\nreal-valued vectors and then projected into a common space. In this paper we\nachieve this by proposing a dual deep encoding network that encodes videos and\nqueries into powerful dense representations of their own. Our novelty is\ntwo-fold. First, different from prior art that resorts to a specific\nsingle-level encoder, the proposed network performs multi-level encoding that\nrepresents the rich content of both modalities in a coarse-to-fine fashion.\nSecond, different from a conventional common space learning algorithm which is\neither concept based or latent space based, we introduce hybrid space learning\nwhich combines the high performance of the latent space and the good\ninterpretability of the concept space. Dual encoding is conceptually simple,\npractically effective and end-to-end trained with hybrid space learning.\nExtensive experiments on four challenging video datasets show the viability of\nthe new method.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:49:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 09:26:20 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Dong", "Jianfeng", ""], ["Li", "Xirong", ""], ["Xu", "Chaoxi", ""], ["Yang", "Xun", ""], ["Yang", "Gang", ""], ["Wang", "Xun", ""], ["Wang", "Meng", ""]]}, {"id": "2009.05603", "submitter": "Dumitru-Clementin Cercel", "authors": "Andrei-Marius Avram, Dumitru-Clementin Cercel, Costin-Gabriel Chiru", "title": "UPB at SemEval-2020 Task 6: Pretrained Language Models for Definition\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents our contribution in the context of the 6th task of\nSemEval-2020: Extracting Definitions from Free Text in Textbooks (DeftEval).\nThis competition consists of three subtasks with different levels of\ngranularity: (1) classification of sentences as definitional or\nnon-definitional,(2) labeling of definitional sentences, and (3) relation\nclassification. We use various pretrained language models (i.e., BERT, XLNet,\nRoBERTa, SciBERT, and ALBERT) to solve each of the three subtasks of the\ncompetition. Specifically, for each language model variant, we experiment by\nboth freezing its weights and fine-tuning them. We also explore a multi-task\narchitecture that was trained to jointly predict the outputs for the second and\nthe third subtasks. Our best performing model evaluated on the DeftEval dataset\nobtains the 32nd place for the first subtask and the 37th place for the second\nsubtask. The code is available for further research at:\nhttps://github.com/avramandrei/DeftEval.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 18:36:22 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 19:33:05 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Avram", "Andrei-Marius", ""], ["Cercel", "Dumitru-Clementin", ""], ["Chiru", "Costin-Gabriel", ""]]}, {"id": "2009.05619", "submitter": "Josimar Chire Saire", "authors": "Josimar E. Chire-Saire", "title": "Characterizing Twitter Interaction during COVID-19 pandemic using\n  Complex Networks and Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of covid-19 started many months ago, the reported origin was in\nWuhan Market, China. Fastly, this virus was propagated to other countries\nbecause the access to international travels is affordable and many countries\nhave a distance of some flight hours, besides borders were a constant flow of\npeople. By the other hand, Internet users have the habits of sharing content\nusing Social Networks and issues, problems, thoughts about Covdid-19 were not\nan exception. Therefore, it is possible to analyze Social Network interaction\nfrom one city, country to understand the impact generated by this global issue.\nSouth America is one region with developing countries with challenges to face\nrelated to Politics, Economy, Public Health and other. Therefore, the scope of\nthis paper is to analyze the interaction on Twitter of South American countries\nand characterize the flow of data through the users using Complex Network\nrepresentation and Text Mining. The preliminary experiments introduces the idea\nof existence of patterns, similar to Complex Systems. Besides, the degree\ndistribution confirm the idea of having a System and visualization of Adjacency\nMatrices show the presence of users' group publishing and interacting together\nduring the time, there is a possibility of identification of robots sending\nposts constantly.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 19:12:44 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chire-Saire", "Josimar E.", ""]]}, {"id": "2009.05794", "submitter": "Jieming Zhu", "authors": "Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, Xiuqiang He", "title": "FuxiCTR: An Open Benchmark for Click-Through Rate Prediction", "comments": "Feebacks and comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as recommender systems, online advertising, and\nproduct search, click-through rate (CTR) prediction is a critical task, because\nits accuracy has a direct impact on both platform revenue and user experience.\nIn recent years, with the prevalence of deep learning, CTR prediction has been\nwidely studied in both academia and industry, resulting in an abundance of deep\nCTR models. Unfortunately, there is still a lack of a standardized benchmark\nand uniform evaluation protocols for CTR prediction. This leads to the\nnon-reproducible and even inconsistent experimental results among these\nstudies. In this paper, we present an open benchmark (namely FuxiCTR) for\nreproducible research and provide a rigorous comparison of different models for\nCTR prediction. Specifically, we ran over 4,600 experiments for a total of more\nthan 12,000 GPU hours in a uniform framework to re-evaluate 24 existing models\non two widely-used datasets, Criteo and Avazu. Surprisingly, our experiments\nshow that many models have smaller differences than expected and sometimes are\neven inconsistent with what reported in the literature. We believe that our\nbenchmark could not only allow researchers to gauge the effectiveness of new\nmodels conveniently, but also share some good practices to fairly compare with\nthe state of the arts. We will release all the code and benchmark settings.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 13:34:22 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhu", "Jieming", ""], ["Liu", "Jinyang", ""], ["Yang", "Shuai", ""], ["Zhang", "Qi", ""], ["He", "Xiuqiang", ""]]}, {"id": "2009.06054", "submitter": "Megan Ma", "authors": "Megan Ma, Dmitriy Podkopaev, Avalon Campbell-Cousins, Adam Nicholas", "title": "Deconstructing Legal Text_Object Oriented Design in Legal Adjudication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rules are pervasive in the law. In the context of computer engineering, the\ntranslation of legal text to algorithmic form is seemingly direct. In large\npart, law may be a ripe field for expert systems and machine learning. For\nengineers, existing law appears formulaic and logically reducible to \"if, then\"\nstatements. The underlying assumption is that the legal language is both\nself-referential and universal. Moreover, description is considered distinct\nfrom interpretation; that in describing the law, the language is seen as\nquantitative and objectifiable. Nevertheless, is descriptive formal language\npurely dissociative? From the logic machine of the 1970s to the modern fervor\nfor artificial intelligence (AI), governance by numbers is making a persuasive\nreturn. Could translation be possible? The project follows a fundamentally\nsemantic conundrum: what is the significance of \"meaning\" in legal language?\nThe project, therefore, tests translation by deconstructing sentences from\nexisting legal judgments to their constituent factors. Definitions are then\nextracted in accordance with the interpretations of the judges. The intent is\nto build an expert system predicated on alleged rules of legal reasoning. The\nauthors apply both linguistic modelling and natural language processing\ntechnology to parse the legal judgments. The project extends beyond prior\nresearch in the area, combining a broadly statistical model of context with the\nrelative precision of syntactic structure. The preliminary hypothesis is that,\nby analyzing the components of legal language with a variety of techniques, we\ncan begin to translate law to numerical form.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 18:04:01 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ma", "Megan", ""], ["Podkopaev", "Dmitriy", ""], ["Campbell-Cousins", "Avalon", ""], ["Nicholas", "Adam", ""]]}, {"id": "2009.06108", "submitter": "Tongxin Zhou", "authors": "Tongxin Zhou, Yingfei Wang, Lu (Lucy) Yan, Yong Tan", "title": "Spoiled for Choice? Personalized Recommendation for Healthcare\n  Decisions: A Multi-Armed Bandit Approach", "comments": "39 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online healthcare communities provide users with various healthcare\ninterventions to promote healthy behavior and improve adherence. When faced\nwith too many intervention choices, however, individuals may find it difficult\nto decide which option to take, especially when they lack the experience or\nknowledge to evaluate different options. The choice overload issue may\nnegatively affect users' engagement in health management. In this study, we\ntake a design-science perspective to propose a recommendation framework that\nhelps users to select healthcare interventions. Taking into account that users'\nhealth behaviors can be highly dynamic and diverse, we propose a multi-armed\nbandit (MAB)-driven recommendation framework, which enables us to adaptively\nlearn users' preference variations while promoting recommendation diversity in\nthe meantime. To better adapt an MAB to the healthcare context, we synthesize\ntwo innovative model components based on prominent health theories. The first\ncomponent is a deep-learning-based feature engineering procedure, which is\ndesigned to learn crucial recommendation contexts in regard to users'\nsequential health histories, health-management experiences, preferences, and\nintrinsic attributes of healthcare interventions. The second component is a\ndiversity constraint, which structurally diversifies recommendations in\ndifferent dimensions to provide users with well-rounded support. We apply our\napproach to an online weight management context and evaluate it rigorously\nthrough a series of experiments. Our results demonstrate that each of the\ndesign components is effective and that our recommendation design outperforms a\nwide range of state-of-the-art recommendation systems. Our study contributes to\nthe research on the application of business intelligence and has implications\nfor multiple stakeholders, including online healthcare platforms, policymakers,\nand users.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 22:55:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Tongxin", "", "Lucy"], ["Wang", "Yingfei", "", "Lucy"], ["Lu", "", "", "Lucy"], ["Yan", "", ""], ["Tan", "Yong", ""]]}, {"id": "2009.06141", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Yiqing Zhang, Hai Zhao, Xi Zhou, Xiang Zhou", "title": "Composing Answer from Multi-spans for Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to generate answers for non-extraction\nmachine reading comprehension (MRC) tasks whose answers cannot be simply\nextracted as one span from the given passages. Using a pointer network-style\nextractive decoder for such type of MRC may result in unsatisfactory\nperformance when the ground-truth answers are given by human annotators or\nhighly re-paraphrased from parts of the passages. On the other hand, using\ngenerative decoder cannot well guarantee the resulted answers with well-formed\nsyntax and semantics when encountering long sentences. Therefore, to alleviate\nthe obvious drawbacks of both sides, we propose an answer making-up method from\nextracted multi-spans that are learned by our model as highly confident\n$n$-gram candidates in the given passage. That is, the returned answers are\ncomposed of discontinuous multi-spans but not just one consecutive span in the\ngiven passages anymore. The proposed method is simple but effective: empirical\nexperiments on MS MARCO show that the proposed method has a better performance\non accurately generating long answers, and substantially outperforms two\ncompetitive typical one-span and Seq2Seq baseline decoders.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 01:44:42 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhang", "Yiqing", ""], ["Zhao", "Hai", ""], ["Zhou", "Xi", ""], ["Zhou", "Xiang", ""]]}, {"id": "2009.06327", "submitter": "Yan Zhao", "authors": "Yan Zhao, Shoujin Wang, Yan Wang, Hongwei Liu, and Weizhe Zhang", "title": "Double-Wing Mixture of Experts for Streaming Recommendations", "comments": "This paper was accepted by the 21st International Conference on Web\n  Information Systems Engineering (WISE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming Recommender Systems (SRSs) commonly train recommendation models on\nnewly received data only to address user preference drift, i.e., the changing\nuser preferences towards items. However, this practice overlooks the long-term\nuser preferences embedded in historical data. More importantly, the common\nheterogeneity in data stream greatly reduces the accuracy of streaming\nrecommendations. The reason is that different preferences (or characteristics)\nof different types of users (or items) cannot be well learned by a unified\nmodel. To address these two issues, we propose a Variational and\nReservoir-enhanced Sampling based Double-Wing Mixture of Experts framework,\ncalled VRS-DWMoE, to improve the accuracy of streaming recommendations. In\nVRS-DWMoE, we first devise variational and reservoir-enhanced sampling to\nwisely complement new data with historical data, and thus address the user\npreference drift issue while capturing long-term user preferences. After that,\nwe propose a Double-Wing Mixture of Experts (DWMoE) model to first effectively\nlearn heterogeneous user preferences and item characteristics, and then make\nrecommendations based on them. Specifically, DWMoE contains two Mixture of\nExperts (MoE, an effective ensemble learning model) to learn user preferences\nand item characteristics, respectively. Moreover, the multiple experts in each\nMoE learn the preferences (or characteristics) of different types of users (or\nitems) where each expert specializes in one underlying type. Extensive\nexperiments demonstrate that VRS-DWMoE consistently outperforms the\nstate-of-the-art SRSs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 11:09:22 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhao", "Yan", ""], ["Wang", "Shoujin", ""], ["Wang", "Yan", ""], ["Liu", "Hongwei", ""], ["Zhang", "Weizhe", ""]]}, {"id": "2009.06372", "submitter": "Thai Hoang", "authors": "Thai Quoc Hoang and Phuong Thu Vu", "title": "Not-NUTs at W-NUT 2020 Task 2: A BERT-based System in Identifying\n  Informative COVID-19 English Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of 2020 when the COVID-19 pandemic is full-blown on a global scale,\npeople's need to have access to legitimate information regarding COVID-19 is\nmore urgent than ever, especially via online media where the abundance of\nirrelevant information overshadows the more informative ones. In response to\nsuch, we proposed a model that, given an English tweet, automatically\nidentifies whether that tweet bears informative content regarding COVID-19 or\nnot. By ensembling different BERTweet model configurations, we have achieved\ncompetitive results that are only shy of those by top performing teams by\nroughly 1% in terms of F1 score on the informative class. In the\npost-competition period, we have also experimented with various other\napproaches that potentially boost generalization to a new dataset.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:49:16 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Hoang", "Thai Quoc", ""], ["Vu", "Phuong Thu", ""]]}, {"id": "2009.06375", "submitter": "Nickil Maveli", "authors": "Nickil Maveli", "title": "EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers with\n  Generalized Augmentation for Identifying Informativeness in COVID-19 Tweets", "comments": "Accepted at W-NUT workshop of EMNLP 2020 (7 pages, 6 figures, 3\n  tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter and, in general, social media has become an indispensable\ncommunication channel in times of emergency. The ubiquitousness of smartphone\ngadgets enables people to declare an emergency observed in real-time. As a\nresult, more agencies are interested in programmatically monitoring Twitter\n(disaster relief organizations and news agencies). Therefore, recognizing the\ninformativeness of a Tweet can help filter noise from the large volumes of\nTweets. In this paper, we present our submission for WNUT-2020 Task 2:\nIdentification of informative COVID-19 English Tweets. Our most successful\nmodel is an ensemble of transformers, including RoBERTa, XLNet, and BERTweet\ntrained in a Semi-Supervised Learning (SSL) setting. The proposed system\nachieves an F1 score of 0.9011 on the test set (ranking 7th on the leaderboard)\nand shows significant gains in performance compared to a baseline system using\nFastText embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 15:57:28 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 16:47:10 GMT"}, {"version": "v3", "created": "Sun, 18 Apr 2021 12:28:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Maveli", "Nickil", ""]]}, {"id": "2009.06504", "submitter": "Longxiang Liu", "authors": "Longxiang Liu, Zhuosheng Zhang, Hai Zhao, Xi Zhou, Xiang Zhou", "title": "Filling the Gap of Utterance-aware and Speaker-aware Representation for\n  Multi-turn Dialogue", "comments": "accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-turn dialogue is composed of multiple utterances from two or more\ndifferent speaker roles. Thus utterance- and speaker-aware clues are supposed\nto be well captured in models. However, in the existing retrieval-based\nmulti-turn dialogue modeling, the pre-trained language models (PrLMs) as\nencoder represent the dialogues coarsely by taking the pairwise dialogue\nhistory and candidate response as a whole, the hierarchical information on\neither utterance interrelation or speaker roles coupled in such representations\nis not well addressed. In this work, we propose a novel model to fill such a\ngap by modeling the effective utterance-aware and speaker-aware representations\nentailed in a dialogue history. In detail, we decouple the contextualized word\nrepresentations by masking mechanisms in Transformer-based PrLM, making each\nword only focus on the words in current utterance, other utterances, two\nspeaker roles (i.e., utterances of sender and utterances of receiver),\nrespectively. Experimental results show that our method boosts the strong\nELECTRA baseline substantially in four public benchmark datasets, and achieves\nvarious new state-of-the-art performance over previous methods. A series of\nablation studies are conducted to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:07:19 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 19:01:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Longxiang", ""], ["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""], ["Zhou", "Xi", ""], ["Zhou", "Xiang", ""]]}, {"id": "2009.06510", "submitter": "Sebastian Weigelt", "authors": "Sebastian Weigelt and Vanessa Steurer and Walter F. Tichy", "title": "At your Command! An Empirical Study on How LaypersonsTeach Robots New\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even though intelligent systems such as Siri or Google Assistant are\nenjoyable (and useful) dialog partners, users can only access predefined\nfunctionality. Enabling end-users to extend the functionality of intelligent\nsystems will be the next big thing. To promote research in this area we carried\nout an empirical study on how laypersons teach robots new functions by means of\nnatural language instructions. The result is a labeled corpus consisting of\n3168 submissions given by 870 subjects. The analysis of the dataset revealed\nthat many participants used certain wordings to express their wish to teach new\nfunctionality; two corresponding trigrams are among the most frequent. On the\ncontrary, more than one third (36.93%) did not verbalize the teaching intent at\nall. We labeled the semantic constituents in the utterances: declaration\n(including the name of the function) and intermediate steps. The full corpus is\npublicly available: http://dx.doi.org/10.21227/zecn-6c61\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 15:16:25 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Weigelt", "Sebastian", ""], ["Steurer", "Vanessa", ""], ["Tichy", "Walter F.", ""]]}, {"id": "2009.06732", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler", "title": "Efficient Transformers: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \"X-former\" models have been\nproposed - Reformer, Linformer, Performer, Longformer, to name a few - which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this paper characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \"X-former\" models,\nproviding an organized and comprehensive overview of existing work and models\nacross multiple domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:38:14 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 07:23:37 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Bahri", "Dara", ""], ["Metzler", "Donald", ""]]}, {"id": "2009.06824", "submitter": "Yan Zhao", "authors": "Yan Zhao, Shoujin Wang, Yan Wang, Hongwei Liu", "title": "Stratified and Time-aware Sampling based Adaptive Ensemble Learning for\n  Streaming Recommendations", "comments": "This paper was accepted by Applied Intelligence in July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have played an increasingly important role in providing\nusers with tailored suggestions based on their preferences. However, the\nconventional offline recommender systems cannot handle the ubiquitous data\nstream well. To address this issue, Streaming Recommender Systems (SRSs) have\nemerged in recent years, which incrementally train recommendation models on\nnewly received data for effective real-time recommendations. Focusing on new\ndata only benefits addressing concept drift, i.e., the changing user\npreferences towards items. However, it impedes capturing long-term user\npreferences. In addition, the commonly existing underload and overload problems\nshould be well tackled for higher accuracy of streaming recommendations. To\naddress these problems, we propose a Stratified and Time-aware Sampling based\nAdaptive Ensemble Learning framework, called STS-AEL, to improve the accuracy\nof streaming recommendations. In STS-AEL, we first devise stratified and\ntime-aware sampling to extract representative data from both new data and\nhistorical data to address concept drift while capturing long-term user\npreferences. Also, incorporating the historical data benefits utilizing the\nidle resources in the underload scenario more effectively. After that, we\npropose adaptive ensemble learning to efficiently process the overloaded data\nin parallel with multiple individual recommendation models, and then\neffectively fuse the results of these models with a sequential adaptive\nmechanism. Extensive experiments conducted on three real-world datasets\ndemonstrate that STS-AEL, in all the cases, significantly outperforms the\nstate-of-the-art SRSs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 02:00:42 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhao", "Yan", ""], ["Wang", "Shoujin", ""], ["Wang", "Yan", ""], ["Liu", "Hongwei", ""]]}, {"id": "2009.06884", "submitter": "Xu Chen", "authors": "Xu Chen and Ya Zhang and Ivor Tsang and Yuangang Pan and Jingchao Su", "title": "Towards Equivalent Transformation of User Preferences in Cross Domain\n  Recommendation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross domain recommendation (CDR) has been proposed to tackle the data\nsparsity problem in recommender systems. This paper focuses on a common\nscenario for CDR where different domains share the same set of users but no\noverlapping items. The majority of recent methods have explored shared-user\nrepresentation to transfer knowledge across different domains. However, the\nidea of shared-user representation resorts to learn the overlapped properties\nof user preferences across different domains and suppresses the domain-specific\nproperties of user preferences. In this paper, we attempt to learn both\nproperties of user preferences for CDR, i.e. capturing both the overlapped and\ndomain-specific properties. In particular, we assume that each user's\npreferences in one domain can be expressed by the other one, and these\npreferences can be mutually converted to each other with the so-called\nequivalent transformations. Based on this assumption, we propose an equivalent\ntransformation learner (ETL) which models the joint distribution of user\nbehaviors across different domains. The equivalent transformations in ETL relax\nthe idea of shared-user representation and allow the learned preferences in\ndifferent domains to have the capacity of preserving the domain-specific\nproperties as well as the overlapped properties. Extensive experiments on three\npublic benchmarks demonstrate the effectiveness of ETL compared with recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 06:37:01 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Tsang", "Ivor", ""], ["Pan", "Yuangang", ""], ["Su", "Jingchao", ""]]}, {"id": "2009.07258", "submitter": "Kai Hui", "authors": "Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates", "title": "BERT-QE: Contextualized Query Expansion for Document Re-ranking", "comments": "Accepted in EMNLP-Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion aims to mitigate the mismatch between the language used in a\nquery and in a document. However, query expansion methods can suffer from\nintroducing non-relevant information when expanding the query. To bridge this\ngap, inspired by recent advances in applying contextualized models like BERT to\nthe document retrieval task, this paper proposes a novel query expansion model\nthat leverages the strength of the BERT model to select relevant document\nchunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test\ncollections, the proposed BERT-QE model significantly outperforms BERT-Large\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 17:50:09 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:08:25 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zheng", "Zhi", ""], ["Hui", "Kai", ""], ["He", "Ben", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Yates", "Andrew", ""]]}, {"id": "2009.07346", "submitter": "Georgios Theocharous", "authors": "Georgios Theocharous, Yash Chandak, Philip S. Thomas, Frits de Nijs", "title": "Reinforcement Learning for Strategic Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic recommendations (SR) refer to the problem where an intelligent\nagent observes the sequential behaviors and activities of users and decides\nwhen and how to interact with them to optimize some long-term objectives, both\nfor the user and the business. These systems are in their infancy in the\nindustry and in need of practical solutions to some fundamental research\nchallenges. At Adobe research, we have been implementing such systems for\nvarious use-cases, including points of interest recommendations, tutorial\nrecommendations, next step guidance in multi-media editing software, and ad\nrecommendation for optimizing lifetime value. There are many research\nchallenges when building these systems, such as modeling the sequential\nbehavior of users, deciding when to intervene and offer recommendations without\nannoying the user, evaluating policies offline with high confidence, safe\ndeployment, non-stationarity, building systems from passive data that do not\ncontain past recommendations, resource constraint optimization in multi-user\nsystems, scaling to large and dynamic actions spaces, and handling and\nincorporating human cognitive biases. In this paper we cover various use-cases\nand research challenges we solved to make these systems practical.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 20:45:48 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Theocharous", "Georgios", ""], ["Chandak", "Yash", ""], ["Thomas", "Philip S.", ""], ["de Nijs", "Frits", ""]]}, {"id": "2009.07531", "submitter": "Xuanang Chen", "authors": "Xuanang Chen, Ben He, Kai Hui, Le Sun, Yingfei Sun", "title": "Simplified TinyBERT: Knowledge Distillation for Document Retrieval", "comments": "Accepted at ECIR 2021 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the effectiveness of utilizing the BERT model for document ranking,\nthe high computational cost of such approaches limits their uses. To this end,\nthis paper first empirically investigates the effectiveness of two knowledge\ndistillation models on the document ranking task. In addition, on top of the\nrecently proposed TinyBERT model, two simplifications are proposed. Evaluations\non two different and widely-used benchmarks demonstrate that Simplified\nTinyBERT with the proposed simplifications not only boosts TinyBERT, but also\nsignificantly outperforms BERT-Base when providing 15$\\times$ speedup.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:59:33 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 09:07:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Chen", "Xuanang", ""], ["He", "Ben", ""], ["Hui", "Kai", ""], ["Sun", "Le", ""], ["Sun", "Yingfei", ""]]}, {"id": "2009.07755", "submitter": "Guillaume Salha", "authors": "Elena V. Epure and Guillaume Salha and Romain Hennequin", "title": "Multilingual Music Genre Embeddings for Effective Cross-Lingual Music\n  Item Annotation", "comments": "21st International Society for Music Information Retrieval Conference\n  (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating music items with music genres is crucial for music recommendation\nand information retrieval, yet challenging given that music genres are\nsubjective concepts. Recently, in order to explicitly consider this\nsubjectivity, the annotation of music items was modeled as a translation task:\npredict for a music item its music genres within a target vocabulary or\ntaxonomy (tag system) from a set of music genre tags originating from other tag\nsystems. However, without a parallel corpus, previous solutions could not\nhandle tag systems in other languages, being limited to the English-language\nonly. Here, by learning multilingual music genre embeddings, we enable\ncross-lingual music genre translation without relying on a parallel corpus.\nFirst, we apply compositionality functions on pre-trained word embeddings to\nrepresent multi-word tags.Second, we adapt the tag representations to the music\ndomain by leveraging multilingual music genres graphs with a modified\nretrofitting algorithm. Experiments show that our method: 1) is effective in\ntranslating music genres across tag systems in multiple languages (English,\nFrench and Spanish); 2) outperforms the previous baseline in an\nEnglish-language multi-source translation task. We publicly release the new\nmultilingual data and code.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:39:04 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Epure", "Elena V.", ""], ["Salha", "Guillaume", ""], ["Hennequin", "Romain", ""]]}, {"id": "2009.07776", "submitter": "Jelena Te\\v{s}i\\'c", "authors": "Lucas Rusnak and Jelena Te\\v{s}i\\'c", "title": "Characterizing Attitudinal Network Graphs through Frustration Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR cs.SY eess.SY math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attitudinal Network Graphs are signed graphs where edges capture an expressed\nopinion: two vertices connected by an edge can be agreeable (positive) or\nantagonistic (negative). A signed graph is called balanced if each of its\ncycles includes an even number of negative edges. Balance is often\ncharacterized by frustration index or by finding a single convergent balanced\nstate i.e. network consensus. In this paper, we propose to expand the measures\nof consensus from a single balanced state associated to the frustration index\nto the set of nearest balanced states. We introduce the frustration cloud as a\nset of all nearest balanced states, and use a graph balancing algorithm to find\nall nearest balanced states in deterministic way. Computational concerns are\naddressed by measuring consensus probabilistically, and we introduce new vertex\nand edge metrics to quantify status, agreement, and influence. We introduce new\nglobal measure of controversy for a given signed graph, and show that vertex\nstatus is a zero-sum game in the signed network. We propose an efficient\nscalable algorithm for calculating frustration cloud based measures in social\nnetwork and survey data of up to 80,000 vertices and half-a-million edges, and\nwe demonstrate the power of the proposed approach to provide discriminant\nfeatures for community discovery when compared to spectral clustering and to\nautomatically identify dominant vertices and anomalous decisions in the\nnetwork.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:14:16 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 01:46:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Rusnak", "Lucas", ""], ["Te\u0161i\u0107", "Jelena", ""]]}, {"id": "2009.07780", "submitter": "Sicheng Zhou", "authors": "Yadan Fan, Sicheng Zhou, Yifan Li, Rui Zhang", "title": "Deep Learning Approaches for Extracting Adverse Events and Indications\n  of Dietary Supplements from Clinical Text", "comments": null, "journal-ref": "J Am Med Inform Assoc. 2020 Nov 5:ocaa218. PMID: 33150942", "doi": "10.1093/jamia/ocaa218", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of our work is to demonstrate the feasibility of utilizing deep\nlearning models to extract safety signals related to the use of dietary\nsupplements (DS) in clinical text. Two tasks were performed in this study. For\nthe named entity recognition (NER) task, Bi-LSTM-CRF (Bidirectional\nLong-Short-Term-Memory Conditional Random Fields) and BERT (Bidirectional\nEncoder Representations from Transformers) models were trained and compared\nwith CRF model as a baseline to recognize the named entities of DS and Events\nfrom clinical notes. In the relation extraction (RE) task, two deep learning\nmodels, including attention-based Bi-LSTM and CNN (Convolutional Neural\nNetwork), and a random forest model were trained to extract the relations\nbetween DS and Events, which were categorized into three classes: positive\n(i.e., indication), negative (i.e., adverse events), and not related. The best\nperformed NER and RE models were further applied on clinical notes mentioning\n88 DS for discovering DS adverse events and indications, which were compared\nwith a DS knowledge base. For the NER task, deep learning models achieved a\nbetter performance than CRF, with F1 scores above 0.860. The attention-based\nBi-LSTM model performed the best in the relation extraction task, with the F1\nscore of 0.893. When comparing DS event pairs generated by the deep learning\nmodels with the knowledge base for DS and Event, we found both known and\nunknown pairs. Deep learning models can detect adverse events and indication of\nDS in clinical notes, which hold great potential for monitoring the safety of\nDS use.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:18:04 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 06:00:02 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fan", "Yadan", ""], ["Zhou", "Sicheng", ""], ["Li", "Yifan", ""], ["Zhang", "Rui", ""]]}, {"id": "2009.07810", "submitter": "Tara Safavi", "authors": "Tara Safavi, Danai Koutra", "title": "CoDEx: A Comprehensive Knowledge Graph Completion Benchmark", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CoDEx, a set of knowledge graph completion datasets extracted from\nWikidata and Wikipedia that improve upon existing knowledge graph completion\nbenchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises\nthree knowledge graphs varying in size and structure, multilingual descriptions\nof entities and relations, and tens of thousands of hard negative triples that\nare plausible but verified to be false. To characterize CoDEx, we contribute\nthorough empirical analyses and benchmarking experiments. First, we analyze\neach CoDEx dataset in terms of logical relation patterns. Next, we report\nbaseline link prediction and triple classification results on CoDEx for five\nextensively tuned embedding models. Finally, we differentiate CoDEx from the\npopular FB15K-237 knowledge graph completion dataset by showing that CoDEx\ncovers more diverse and interpretable content, and is a more difficult link\nprediction benchmark. Data, code, and pretrained models are available at\nhttps://bit.ly/2EPbrJs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:08:23 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:10:10 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Safavi", "Tara", ""], ["Koutra", "Danai", ""]]}, {"id": "2009.07964", "submitter": "Zhijing Jin", "authors": "Xiaoyu Xing, Zhijing Jin, Di Jin, Bingning Wang, Qi Zhang, and\n  Xuanjing Huang", "title": "Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based\n  Sentiment Analysis", "comments": "EMNLP 2020, long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards\na specific aspect in the text. However, existing ABSA test sets cannot be used\nto probe whether a model can distinguish the sentiment of the target aspect\nfrom the non-target aspects. To solve this problem, we develop a simple but\neffective approach to enrich ABSA test sets. Specifically, we generate new\nexamples to disentangle the confounding sentiments of the non-target aspects\nfrom the target aspect's sentiment. Based on the SemEval 2014 dataset, we\nconstruct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the\naspect robustness of ABSA models. Over 92% data of ARTS show high fluency and\ndesired sentiment on all aspects by human evaluation. Using ARTS, we analyze\nthe robustness of nine ABSA models, and observe, surprisingly, that their\naccuracy drops by up to 69.73%. We explore several ways to improve aspect\nrobustness, and find that adversarial training can improve models' performance\non ARTS by up to 32.85%. Our code and new test set are available at\nhttps://github.com/zhijing-jin/ARTS_TestSet\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 22:38:18 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 05:36:10 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2020 15:35:36 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 08:19:36 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Xing", "Xiaoyu", ""], ["Jin", "Zhijing", ""], ["Jin", "Di", ""], ["Wang", "Bingning", ""], ["Zhang", "Qi", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2009.08018", "submitter": "Xiyan Fu", "authors": "Xiyan Fu and Jun Wang and Zhenglu Yang", "title": "Multi-modal Summarization for Video-containing Documents", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization of multimedia data becomes increasingly significant as it is\nthe basis for many real-world applications, such as question answering, Web\nsearch, and so forth. Most existing multi-modal summarization works however\nhave used visual complementary features extracted from images rather than\nvideos, thereby losing abundant information. Hence, we propose a novel\nmulti-modal summarization task to summarize from a document and its associated\nvideo. In this work, we also build a baseline general model with effective\nstrategies, i.e., bi-hop attention and improved late fusion mechanisms to\nbridge the gap between different modalities, and a bi-stream summarization\nstrategy to employ text and video summarization simultaneously. Comprehensive\nexperiments show that the proposed model is beneficial for multi-modal\nsummarization and superior to existing methods. Moreover, we collect a novel\ndataset and it provides a new resource for future study that results from\ndocuments and videos.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:13:14 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Fu", "Xiyan", ""], ["Wang", "Jun", ""], ["Yang", "Zhenglu", ""]]}, {"id": "2009.08114", "submitter": "Mariona Coll Ardanuy", "authors": "Mariona Coll Ardanuy, Kasra Hosseini, Katherine McDonough, Amrey\n  Krause, Daniel van Strien and Federico Nanni", "title": "A Deep Learning Approach to Geographical Candidate Selection through\n  Toponym Matching", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing toponyms and resolving them to their real-world referents is\nrequired for providing advanced semantic access to textual data. This process\nis often hindered by the high degree of variation in toponyms. Candidate\nselection is the task of identifying the potential entities that can be\nreferred to by a toponym previously recognized. While it has traditionally\nreceived little attention in the research community, it has been shown that\ncandidate selection has a significant impact on downstream tasks (i.e. entity\nresolution), especially in noisy or non-standard text. In this paper, we\nintroduce a flexible deep learning method for candidate selection through\ntoponym matching, using state-of-the-art neural network architectures. We\nperform an intrinsic toponym matching evaluation based on several new realistic\ndatasets, which cover various challenging scenarios (cross-lingual and regional\nvariations, as well as OCR errors). We report its performance on candidate\nselection in the context of the downstream task of toponym resolution, both on\nexisting datasets and on a new manually-annotated resource of\nnineteenth-century English OCR'd text.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 07:24:56 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 14:24:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ardanuy", "Mariona Coll", ""], ["Hosseini", "Kasra", ""], ["McDonough", "Katherine", ""], ["Krause", "Amrey", ""], ["van Strien", "Daniel", ""], ["Nanni", "Federico", ""]]}, {"id": "2009.08142", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov, Kishor Patil, Gugan Thoppe", "title": "Online Algorithms for Estimating Change Rates of Web Pages", "comments": "A significantly extended version of ValueTools 2020 conference paper\n  [arXiv:2004.02167]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI math.PR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For providing quick and accurate search results, a search engine maintains a\nlocal snapshot of the entire web. And, to keep this local cache fresh, it\nemploys a crawler for tracking changes across various web pages. It would have\nbeen ideal if the crawler managed to update the local snapshot as soon as a\npage changed on the web. However, finite bandwidth availability and server\nrestrictions mean that there is a bound on how frequently the different pages\ncan be crawled. This then brings forth the following optimisation problem:\nmaximise the freshness of the local cache subject to the crawling frequency\nbeing within the prescribed bounds. Recently, tractable algorithms have been\nproposed to solve this optimisation problem under different cost criteria.\nHowever, these assume the knowledge of exact page change rates, which is\nunrealistic in practice. We address this issue here. Specifically, we provide\nthree novel schemes for online estimation of page change rates. All these\nschemes only need partial information about the page change process, i.e., they\nonly need to know if the page has changed or not since the last crawl instance.\nOur first scheme is based on the law of large numbers, the second on the theory\nof stochastic approximation, while the third is an extension of the second and\ninvolves an additional momentum term. For all of these schemes, we prove\nconvergence and, also, provide their convergence rates. As far as we know, the\nresults concerning the third estimator is quite novel. Specifically, this is\nthe first convergence type result for a stochastic approximation algorithm with\nmomentum. Finally, we provide some numerical experiments (on real as well as\nsynthetic data) to compare the performance of our proposed estimators with the\nexisting ones (e.g., MLE).\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 08:25:02 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Patil", "Kishor", ""], ["Thoppe", "Gugan", ""]]}, {"id": "2009.08206", "submitter": "Saad Aloteibi", "authors": "Saad Aloteibi and Stephen Clark", "title": "Learning to Personalize for Web Search Sessions", "comments": "10 pages; Preprint of the full paper accepted at CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412050", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of session search focuses on using interaction data to improve\nrelevance for the user's next query at the session level. In this paper, we\nformulate session search as a personalization task under the framework of\nlearning to rank. Personalization approaches re-rank results to match a user\nmodel. Such user models are usually accumulated over time based on the user's\nbrowsing behaviour. We use a pre-computed and transparent set of user models\nbased on concepts from the social science literature. Interaction data are used\nto map each session to these user models. Novel features are then estimated\nbased on such models as well as sessions' interaction data. Extensive\nexperiments on test collections from the TREC session track show statistically\nsignificant improvements over current session search algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 10:54:07 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Aloteibi", "Saad", ""], ["Clark", "Stephen", ""]]}, {"id": "2009.08553", "submitter": "Yuning Mao", "authors": "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,\n  Jiawei Han, Weizhu Chen", "title": "Generation-Augmented Retrieval for Open-domain Question Answering", "comments": "ACL 2021 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Generation-Augmented Retrieval (GAR) for answering open-domain\nquestions, which augments a query through text generation of heuristically\ndiscovered relevant contexts without external resources as supervision. We\ndemonstrate that the generated contexts substantially enrich the semantics of\nthe queries and GAR with sparse representations (BM25) achieves comparable or\nbetter performance than state-of-the-art dense retrieval methods such as DPR.\nWe show that generating diverse contexts for a query is beneficial as fusing\ntheir results consistently yields better retrieval accuracy. Moreover, as\nsparse and dense representations are often complementary, GAR can be easily\ncombined with DPR to achieve even better performance. GAR achieves\nstate-of-the-art performance on Natural Questions and TriviaQA datasets under\nthe extractive QA setup when equipped with an extractive reader, and\nconsistently outperforms other retrieval methods when the same generative\nreader is used.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 23:08:01 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 03:23:27 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 22:08:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mao", "Yuning", ""], ["He", "Pengcheng", ""], ["Liu", "Xiaodong", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["Han", "Jiawei", ""], ["Chen", "Weizhu", ""]]}, {"id": "2009.08582", "submitter": "William Barnhart", "authors": "William Barnhart and Zhi Tian", "title": "The Capacity of Multi-user Private Information Retrieval for\n  Computationally Limited Databases", "comments": "$\\copyright$ 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a private information retrieval (PIR) scheme that allows a user to\nretrieve a single message from an arbitrary number of databases by colluding\nwith other users while hiding the desired message index. This scheme is of\nparticular significance when there is only one accessible database -- a special\ncase that turns out to be more challenging for PIR in the multi-database case.\nThe upper bound for privacy-preserving capacity for these scenarios is\n$C=(1+\\frac{1}{S}+\\cdots+\\frac{1}{S^{K-1}})^{-1}$, where $K$ is the number of\nmessages and $S$ represents the quantity of information sources such as\n$S=N+U-1$ for $U$ users and $N$ databases. We show that the proposed\ninformation retrieval scheme attains the capacity bound even when only one\ndatabase is present, which differs from most existing works that hinge on the\naccess to multiple databases in order to hide user privacy. Unlike the\nmulti-database case, this scheme capitalizes on the inability for a database to\ncross-reference queries made by multiple users due to computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 01:35:39 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:38:33 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 00:37:20 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Barnhart", "William", ""], ["Tian", "Zhi", ""]]}, {"id": "2009.08590", "submitter": "Kumud Chauhan", "authors": "Kumud Chauhan", "title": "NEU at WNUT-2020 Task 2: Data Augmentation To Tell BERT That Death Is\n  Not Necessarily Informative", "comments": "WNUT-2020 Task 2 System Description paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of people around the world are sharing COVID-19 related information\non social media platforms. Since not all the information shared on the social\nmedia is useful, a machine learning system to identify informative posts can\nhelp users in finding relevant information. In this paper, we present a BERT\nclassifier system for W-NUT2020 Shared Task 2: Identification of Informative\nCOVID-19 English Tweets. Further, we show that BERT exploits some easy signals\nto identify informative tweets, and adding simple patterns to uninformative\ntweets drastically degrades BERT performance. In particular, simply adding 10\ndeaths to tweets in dev set, reduces BERT F1- score from 92.63 to 7.28. We also\npropose a simple data augmentation technique that helps in improving the\nrobustness and generalization ability of the BERT classifier.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 02:16:49 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Chauhan", "Kumud", ""]]}, {"id": "2009.08621", "submitter": "Hai Dong", "authors": "Mingwei Zhang, Jiawei Zhao, Hai Dong, Ke Deng, and Ying Liu", "title": "A Knowledge Graph based Approach for Mobile Application Recommendation", "comments": "15 pages. The 18th International Conference on Service Oriented\n  Computing (ICSOC 2020) (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid prevalence of mobile devices and the dramatic proliferation of\nmobile applications (apps), app recommendation becomes an emergent task that\nwould benefit both app users and stockholders. How to effectively organize and\nmake full use of rich side information of users and apps is a key challenge to\naddress the sparsity issue for traditional approaches. To meet this challenge,\nwe proposed a novel end-to-end Knowledge Graph Convolutional Embedding\nPropagation Model (KGEP) for app recommendation. Specifically, we first\ndesigned a knowledge graph construction method to model the user and app side\ninformation, then adopted KG embedding techniques to capture the factual\ntriplet-focused semantics of the side information related to the first-order\nstructure of the KG, and finally proposed a relation-weighted convolutional\nembedding propagation model to capture the recommendation-focused semantics\nrelated to high-order structure of the KG. Extensive experiments conducted on a\nreal-world dataset validate the effectiveness of the proposed approach compared\nto the state-of-the-art recommendation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 04:17:23 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zhang", "Mingwei", ""], ["Zhao", "Jiawei", ""], ["Dong", "Hai", ""], ["Deng", "Ke", ""], ["Liu", "Ying", ""]]}, {"id": "2009.08947", "submitter": "Markus Viljanen", "authors": "Markus Viljanen, Jukka Vahlo, Aki Koponen, Tapio Pahikkala", "title": "Content Based Player and Game Interaction Model for Game Recommendation\n  in the Cold Start setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game recommendation is an important application of recommender systems.\nRecommendations are made possible by data sets of historical player and game\ninteractions, and sometimes the data sets include features that describe games\nor players. Collaborative filtering has been found to be the most accurate\npredictor of past interactions. However, it can only be applied to predict new\ninteractions for those games and players where a significant number of past\ninteractions are present. In other words, predictions for completely new games\nand players is not possible. In this paper, we use a survey data set of game\nlikes to present content based interaction models that generalize into new\ngames, new players, and both new games and players simultaneously. We find that\nthe models outperform collaborative filtering in these tasks, which makes them\nuseful for real world game recommendation. The content models also provide\ninterpretations of why certain games are liked by certain players for game\nanalytics purposes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 11:10:49 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Viljanen", "Markus", ""], ["Vahlo", "Jukka", ""], ["Koponen", "Aki", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "2009.08948", "submitter": "Haihua Chen", "authors": "Haihua Chen", "title": "A New Citation Recommendation Strategy Based on Term Functions in\n  Related Studies Section", "comments": "17 pages, 4 figures, and 3 tables. Journal of Data and Information\n  Science (2020)", "journal-ref": null, "doi": "10.2478/jdis-2021-0022", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Researchers frequently encounter the following problems when writing\nscientific articles: (1) Selecting appropriate citations to support the\nresearch idea is challenging. (2) The literature review is not conducted\nextensively, which leads to working on a research problem that others have well\naddressed. This study focuses on citation recommendation in the related studies\nsection by applying the term function of a citation context, potentially\nimproving the efficiency of writing a literature review.\nDesign/methodology/approach: We present nine term functions with three newly\ncreated and six identified from existing literature. Using these term functions\nas labels, we annotate 531 research papers in three topics to evaluate our\nproposed recommendation strategy. BM25 and Word2vec with VSM are implemented as\nthe baseline models for the recommendation. Then the term function information\nis applied to enhance the performance. Findings: The experiments show that the\nterm function-based methods outperform the baseline methods regarding the\nrecall, precision, and F1-score measurement, demonstrating that term functions\nare useful in identifying valuable citations. Research limitations: The dataset\nis insufficient due to the complexity of annotating citation functions for\nparagraphs in the related studies section. More recent deep learning models\nshould be performed to future validate the proposed approach. Practical\nimplications: The citation recommendation strategy can be helpful for valuable\ncitation discovery, semantic scientific retrieval, and automatic literature\nreview generation. Originality/value: The proposed citation function-based\ncitation recommendation can generate intuitive explanations of the results for\nusers, improving the transparency, persuasiveness, and effectiveness of\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 15:03:44 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 16:36:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Haihua", ""]]}, {"id": "2009.08949", "submitter": "Yafei Xu", "authors": "Yafei Xu and Tian Xie and Yu Zhang", "title": "Boosting Retailer Revenue by Generated Optimized Combined Multiple\n  Digital Marketing Campaigns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Campaign is a frequently employed instrument in lifting up the GMV (Gross\nMerchandise Volume) of retailer in traditional marketing. As its counterpart in\nonline context, digital-marketing-campaign (DMC) has being trending in recent\nyears with the rapid development of the e-commerce. However, how to empower\nmassive sellers on the online retailing platform the capacity of applying\ncombined multiple digital marketing campaigns to boost their shops' revenue, is\nstill a novel topic. In this work, a comprehensive solution of generating\noptimized combined multiple DMCs is presented. Firstly, a potential\npersonalized DMC pool is generated for every retailer by a newly proposed\nneural network model, i.e. the DMCNet (Digital-Marketing-Campaign Net).\nSecondly, based on the sub-modular optimization theory and the DMC pool by\nDMCNet, the generated combined multiple DMCs are ranked with respect to their\nrevenue generation strength then the top three ranked campaigns are returned to\nthe sellers' back-end management system, so that retailers can set combined\nmultiple DMCs for their online shops just in one-shot. Real online A/B-test\nshows that with the integrated solution, sellers of the online retailing\nplatform increase their shops' GMVs with approximately 6$\\%$.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 14:42:15 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Xu", "Yafei", ""], ["Xie", "Tian", ""], ["Zhang", "Yu", ""]]}, {"id": "2009.08950", "submitter": "Karthik Raja Kalaiselvi Bhaskar", "authors": "Karthik Raja Kalaiselvi Bhaskar, Deepa Kundur, Yuri Lawryshyn", "title": "Implicit Feedback Deep Collaborative Filtering Product Recommendation\n  System", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, several Collaborative Filtering (CF) approaches with latent\nvariable methods were studied using user-item interactions to capture important\nhidden variations of the sparse customer purchasing behaviours. The latent\nfactors are used to generalize the purchasing pattern of the customers and to\nprovide product recommendations. CF with Neural Collaborative Filtering(NCF)\nwas shown to produce the highest Normalized Discounted Cumulative Gain (NDCG)\nperformance on the real-world proprietary dataset provided by a large parts\nsupply company. Different hyperparameters were tested using Bayesian\nOptimization (BO) for applicability in the CF framework. External data sources\nlike click-data and metrics like Clickthrough Rate (CTR) were reviewed for\npotential extensions to the work presented. The work shown in this paper\nprovides techniques the Company can use to provide product recommendations to\nenhance revenues, attract new customers, and gain advantages over competitors.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 19:30:14 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 15:08:40 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Bhaskar", "Karthik Raja Kalaiselvi", ""], ["Kundur", "Deepa", ""], ["Lawryshyn", "Yuri", ""]]}, {"id": "2009.08952", "submitter": "Charles Dickens", "authors": "Charles Dickens, Rishika Singh, Lise Getoor", "title": "HyperFair: A Soft Approach to Integrating Fairness Criteria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are being employed across an increasingly diverse set of\ndomains that can potentially make a significant social and individual impact.\nFor this reason, considering fairness is a critical step in the design and\nevaluation of such systems. In this paper, we introduce HyperFair, a general\nframework for enforcing soft fairness constraints in a hybrid recommender\nsystem. HyperFair models integrate variations of fairness metrics as a\nregularization of a joint inference objective function. We implement our\napproach using probabilistic soft logic and show that it is particularly\nwell-suited for this task as it is expressive and structural constraints can be\nadded to the system in a concise and interpretable manner. We propose two ways\nto employ the methods we introduce: first as an extension of a probabilistic\nsoft logic recommender system template; second as a fair retrofitting technique\nthat can be used to improve the fairness of predictions from a black-box model.\nWe empirically validate our approach by implementing multiple HyperFair hybrid\nrecommenders and compare them to a state-of-the-art fair recommender. We also\nrun experiments showing the effectiveness of our methods for the task of\nretrofitting a black-box model and the trade-off between the amount of fairness\nenforced and the prediction performance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 05:00:06 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Dickens", "Charles", ""], ["Singh", "Rishika", ""], ["Getoor", "Lise", ""]]}, {"id": "2009.08955", "submitter": "Rashidul Islam", "authors": "Rashidul Islam, Kamrun Naher Keya, Ziqian Zeng, Shimei Pan, James\n  Foulds", "title": "Neural Fair Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing proportion of human interactions are digitized on social media\nplatforms and subjected to algorithmic decision-making, and it has become\nincreasingly important to ensure fair treatment from these algorithms. In this\nwork, we investigate gender bias in collaborative-filtering recommender systems\ntrained on social media data. We develop neural fair collaborative filtering\n(NFCF), a practical framework for mitigating gender bias in recommending\nsensitive items (e.g. jobs, academic concentrations, or courses of study) using\na pre-training and fine-tuning approach to neural collaborative filtering,\naugmented with bias correction techniques. We show the utility of our methods\nfor gender de-biased career and college major recommendations on the MovieLens\ndataset and a Facebook dataset, respectively, and achieve better performance\nand fairer behavior than several state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 05:11:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Islam", "Rashidul", ""], ["Keya", "Kamrun Naher", ""], ["Zeng", "Ziqian", ""], ["Pan", "Shimei", ""], ["Foulds", "James", ""]]}, {"id": "2009.08956", "submitter": "Jiri Hron", "authors": "Jiri Hron and Karl Krauth and Michael I. Jordan and Niki Kilbertus", "title": "Exploration in two-stage recommender systems", "comments": "Published at the REVEAL 2020 workshop (RecSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage recommender systems are widely adopted in industry due to their\nscalability and maintainability. These systems produce recommendations in two\nsteps: (i) multiple nominators preselect a small number of items from a large\npool using cheap-to-compute item embeddings; (ii) with a richer set of\nfeatures, a ranker rearranges the nominated items and serves them to the user.\nA key challenge of this setup is that optimal performance of each stage in\nisolation does not imply optimal global performance. In response to this issue,\nMa et al. (2020) proposed a nominator training objective importance weighted by\nthe ranker's probability of recommending each item. In this work, we focus on\nthe complementary issue of exploration. Modeled as a contextual bandit problem,\nwe find LinUCB (a near optimal exploration strategy for single-stage systems)\nmay lead to linear regret when deployed in two-stage recommenders. We therefore\npropose a method of synchronising the exploration strategies between the ranker\nand the nominators. Our algorithm only relies on quantities already computed by\nstandard LinUCB at each stage and can be implemented in three lines of\nadditional code. We end by demonstrating the effectiveness of our algorithm\nexperimentally.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 16:52:51 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Hron", "Jiri", ""], ["Krauth", "Karl", ""], ["Jordan", "Michael I.", ""], ["Kilbertus", "Niki", ""]]}, {"id": "2009.08957", "submitter": "Sheng-Chieh Lin", "authors": "Sheng-Chieh Lin, Ting-Wei Lin, Jing-Kai Lou, Ming-Feng Tsai, Chuan-Ju\n  Wang", "title": "Personalized TV Recommendation: Fusing User Behavior and Preferences", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-stage ranking approach for recommending\nlinear TV programs. The proposed approach first leverages user viewing patterns\nregarding time and TV channels to identify potential candidates for\nrecommendation and then further leverages user preferences to rank these\ncandidates given textual information about programs. To evaluate the method, we\nconduct empirical studies on a real-world TV dataset, the results of which\ndemonstrate the superior performance of our model in terms of both\nrecommendation accuracy and time efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 16:05:53 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Lin", "Ting-Wei", ""], ["Lou", "Jing-Kai", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "2009.08958", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Keyword Search Engine Enriched by Expert System Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword search engines are essential elements of large information spaces.\nThe largest information space is the Web, and keyword search engines play\ncrucial role there. The advent of keyword search engines has provided a quantum\nleap in the development of the Web. Since then, the Web has continued to\nevolve, and keyword search systems have proven inadequate. A new quantum leap\nin the development of keyword search engines is needed. This quantum leap can\nbe provided with more intellectual keyword search engines. The increased\nintelligence of such keyword search engines can be achieved through a\ncombination of keyword search engines and expert systems. The paper reveals how\nit can be done.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 20:23:23 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "2009.08962", "submitter": "Meimei Liu", "authors": "Meimei Liu, Hongxia Yang", "title": "DVE: Dynamic Variational Embeddings with Applications in Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding is a useful technique to project a high-dimensional feature into a\nlow-dimensional space, and it has many successful applications including link\nprediction, node classification and natural language processing. Current\napproaches mainly focus on static data, which usually lead to unsatisfactory\nperformance in applications involving large changes over time. How to\ndynamically characterize the variation of the embedded features is still\nlargely unexplored. In this paper, we introduce a dynamic variational embedding\n(DVE) approach for sequence-aware data based on recent advances in recurrent\nneural networks. DVE can model the node's intrinsic nature and temporal\nvariation explicitly and simultaneously, which are crucial for exploration. We\nfurther apply DVE to sequence-aware recommender systems, and develop an\nend-to-end neural architecture for link prediction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 20:05:56 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Liu", "Meimei", ""], ["Yang", "Hongxia", ""]]}, {"id": "2009.08978", "submitter": "Diego Antognini", "authors": "Milena Filipovic, Blagoj Mitrevski, Diego Antognini, Emma Lejal\n  Glaude, Boi Faltings, Claudiu Musat", "title": "Modeling Online Behavior in Recommender Systems: The Importance of\n  Temporal Context", "comments": "Under review. 8 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating online recommender system performance is notoriously difficult and\nthe discrepancy between the online and offline behaviors is typically not\naccounted for in offline evaluations. Recommender systems research tends to\nevaluate model performance on randomly sampled targets, yet the same systems\nare later used to predict user behavior sequentially from a fixed point in\ntime. This disparity permits weaknesses to go unnoticed until the model is\ndeployed in a production setting. We first demonstrate how omitting temporal\ncontext when evaluating recommender system performance leads to false\nconfidence. To overcome this, we propose an offline evaluation protocol\nmodeling the real-life use-case that simultaneously accounts for temporal\ncontext.\n  Next, we propose a training procedure to further embed the temporal context\nin existing models: we introduce it in a multi-objective approach to\ntraditionally time-unaware recommender systems. We confirm the advantage of\nadding a temporal objective via the proposed evaluation protocol. Finally, we\nvalidate that the Pareto Fronts obtained with the added objective dominate\nthose produced by state-of-the-art models that are only optimized for accuracy\non three real-world publicly available datasets. The results show that\nincluding our temporal objective can improve recall@20 by up to 20%.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:36:43 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Filipovic", "Milena", ""], ["Mitrevski", "Blagoj", ""], ["Antognini", "Diego", ""], ["Glaude", "Emma Lejal", ""], ["Faltings", "Boi", ""], ["Musat", "Claudiu", ""]]}, {"id": "2009.09074", "submitter": "Xia Li", "authors": "Rachel Grotheer, Yihuan Huang, Pengyu Li, Elizaveta Rebrova, Deanna\n  Needell, Longxiu Huang, Alona Kryshchenko, Xia Li, Kyung Ha, Oleksandr\n  Kryshchenko", "title": "COVID-19 Literature Topic-Based Search via Hierarchical NMF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dataset of COVID-19-related scientific literature is compiled, combining\nthe articles from several online libraries and selecting those with open access\nand full text available. Then, hierarchical nonnegative matrix factorization is\nused to organize literature related to the novel coronavirus into a tree\nstructure that allows researchers to search for relevant literature based on\ndetected topics. We discover eight major latent topics and 52 granular\nsubtopics in the body of literature, related to vaccines, genetic structure and\nmodeling of the disease and patient studies, as well as related diseases and\nvirology. In order that our tool may help current researchers, an interactive\nwebsite is created that organizes available literature using this hierarchical\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 05:45:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Grotheer", "Rachel", ""], ["Huang", "Yihuan", ""], ["Li", "Pengyu", ""], ["Rebrova", "Elizaveta", ""], ["Needell", "Deanna", ""], ["Huang", "Longxiu", ""], ["Kryshchenko", "Alona", ""], ["Li", "Xia", ""], ["Ha", "Kyung", ""], ["Kryshchenko", "Oleksandr", ""]]}, {"id": "2009.09076", "submitter": "Anis Zaman", "authors": "Anis Zaman, Boyu Zhang, Ehsan Hoque, Vincent Silenzio, Henry Kautz", "title": "The Relationship between Deteriorating Mental Health Conditions and\n  Longitudinal Behavioral Changes in Google and YouTube Usages among College\n  Students in the United States during COVID-19: Observational Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health problems among the global population are worsened during the\ncoronavirus disease (COVID-19). How individuals engage with online platforms\nsuch as Google Search and YouTube undergoes drastic shifts due to pandemic and\nsubsequent lockdowns. Such ubiquitous daily behaviors on online platforms have\nthe potential to capture and correlate with clinically alarming deteriorations\nin mental health profiles in a non-invasive manner. The goal of this study is\nto examine, among college students, the relationship between deteriorating\nmental health conditions and changes in user behaviors when engaging with\nGoogle Search and YouTube during COVID-19. This study recruited a cohort of 49\nstudents from a U.S. college campus during January 2020 (prior to the pandemic)\nand measured the anxiety and depression levels of each participant. This study\nfollowed up with the same cohort during May 2020 (during the pandemic), and the\nanxiety and depression levels were assessed again. The longitudinal Google\nSearch and YouTube history data were anonymized and collected. From\nindividual-level Google Search and YouTube histories, we developed 5 signals\nthat can quantify shifts in online behaviors during the pandemic. We then\nassessed the differences between groups with and without deteriorating mental\nhealth profiles in terms of these features. Significant features included\nlate-night online activities, continuous usages, and time away from the\ninternet, porn consumptions, and keywords associated with negative emotions,\nsocial activities, and personal affairs. Though further studies are required,\nour results demonstrated the feasibility of utilizing pervasive online data to\nestablish non-invasive surveillance systems for mental health conditions that\nbypasses many disadvantages of existing screening methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 00:54:57 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zaman", "Anis", ""], ["Zhang", "Boyu", ""], ["Hoque", "Ehsan", ""], ["Silenzio", "Vincent", ""], ["Kautz", "Henry", ""]]}, {"id": "2009.09107", "submitter": "Tian Shi", "authors": "Tian Shi and Liuqing Li and Ping Wang and Chandan K. Reddy", "title": "A Simple and Effective Self-Supervised Contrastive Learning Framework\n  for Aspect Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised aspect detection (UAD) aims at automatically extracting\ninterpretable aspects and identifying aspect-specific segments (such as\nsentences) from online reviews. However, recent deep learning-based topic\nmodels, specifically aspect-based autoencoder, suffer from several problems,\nsuch as extracting noisy aspects and poorly mapping aspects discovered by\nmodels to the aspects of interest. To tackle these challenges, in this paper,\nwe first propose a self-supervised contrastive learning framework and an\nattention-based model equipped with a novel smooth self-attention (SSA) module\nfor the UAD task in order to learn better representations for aspects and\nreview segments. Secondly, we introduce a high-resolution selective mapping\n(HRSMap) method to efficiently assign aspects discovered by the model to\naspects of interest. We also propose using a knowledge distilling technique to\nfurther improve the aspect detection performance. Our methods outperform\nseveral recent unsupervised and weakly supervised approaches on publicly\navailable benchmark user review datasets. Aspect interpretation results show\nthat extracted aspects are meaningful, have good coverage, and can be easily\nmapped to aspects of interest. Ablation studies and attention weight\nvisualization also demonstrate the effectiveness of SSA and the knowledge\ndistilling method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:13:49 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 04:57:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Shi", "Tian", ""], ["Li", "Liuqing", ""], ["Wang", "Ping", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2009.09112", "submitter": "Tian Shi", "authors": "Tian Shi and Ping Wang and Chandan K. Reddy", "title": "An Interpretable and Uncertainty Aware Multi-Task Framework for\n  Multi-Aspect Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several online platforms have seen a rapid increase in the\nnumber of review systems that request users to provide aspect-level feedback.\nDocument-level Multi-aspect Sentiment Classification (DMSC), where the goal is\nto predict the ratings/sentiment from a review at an individual aspect level,\nhas become a challenging and imminent problem. To tackle this challenge, we\npropose a deliberate self-attention-based deep neural network model, namely\nFEDAR, for the DMSC problem, which can achieve competitive performance while\nalso being able to interpret the predictions made. FEDAR is equipped with a\nhighway word embedding layer to transfer knowledge from pre-trained word\nembeddings, an RNN encoder layer with output features enriched by pooling and\nfactorization techniques, and a deliberate self-attention layer. In addition,\nwe also propose an Attention-driven Keywords Ranking (AKR) method, which can\nautomatically discover aspect keywords and aspect-level opinion keywords from\nthe review corpus based on the attention weights. These keywords are\nsignificant for rating predictions by FEDAR. Since crowdsourcing annotation can\nbe an alternate way to recover missing ratings of reviews, we propose a\nLEcture-AuDience (LEAD) strategy to estimate model uncertainty in the context\nof multi-task learning, so that valuable human resources can focus on the most\nuncertain predictions. Our extensive set of experiments on five different\nopen-domain DMSC datasets demonstrate the superiority of the proposed FEDAR and\nLEAD models. We further introduce two new DMSC datasets in the healthcare\ndomain and benchmark different baseline models and our models on them.\nAttention weights visualization results and visualization of aspect and opinion\nkeywords demonstrate the interpretability of our model and the effectiveness of\nour AKR method.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 22:32:39 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 03:44:49 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shi", "Tian", ""], ["Wang", "Ping", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2009.09226", "submitter": "Chaojun Xiao", "authors": "Zheni Zeng, Chaojun Xiao, Yuan Yao, Ruobing Xie, Zhiyuan Liu, Fen Lin,\n  Leyu Lin and Maosong Sun", "title": "Knowledge Transfer via Pre-training for Recommendation: A Review and\n  Prospect", "comments": "This paper is submitted to Frontiers in Big Data and is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems aim to provide item recommendations for users, and are\nusually faced with data sparsity problem (e.g., cold start) in real-world\nscenarios. Recently pre-trained models have shown their effectiveness in\nknowledge transfer between domains and tasks, which can potentially alleviate\nthe data sparsity problem in recommender systems. In this survey, we first\nprovide a review of recommender systems with pre-training. In addition, we show\nthe benefits of pre-training to recommender systems through experiments.\nFinally, we discuss several promising directions for future research for\nrecommender systems with pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 13:06:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zeng", "Zheni", ""], ["Xiao", "Chaojun", ""], ["Yao", "Yuan", ""], ["Xie", "Ruobing", ""], ["Liu", "Zhiyuan", ""], ["Lin", "Fen", ""], ["Lin", "Leyu", ""], ["Sun", "Maosong", ""]]}, {"id": "2009.09259", "submitter": "Shengjun Pan", "authors": "Shengjun Pan, Brendan Kitts, Tian Zhou, Hao He, Bharatbhushan Shetty,\n  Aaron Flores, Djordje Gligorijevic, Junwei Pan, Tingyu Mao, San Gultekin and\n  Jianlong Zhang", "title": "Bid Shading by Win-Rate Estimation and Surplus Maximization", "comments": "AdKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new win-rate based bid shading algorithm (WR) that\ndoes not rely on the minimum-bid-to-win feedback from a Sell-Side Platform\n(SSP). The method uses a modified logistic regression to predict the profit\nfrom each possible shaded bid price. The function form allows fast maximization\nat run-time, a key requirement for Real-Time Bidding (RTB) systems. We report\nproduction results from this method along with several other algorithms. We\nfound that bid shading, in general, can deliver significant value to\nadvertisers, reducing price per impression to about 55% of the unshaded cost.\nFurther, the particular approach described in this paper captures 7% more\nprofit for advertisers, than do benchmark methods of just bidding the most\nprobable winning price. We also report 4.3% higher surplus than an industry\nSell-Side Platform shading service. Furthermore, we observed 3% - 7% lower\neCPM, eCPC and eCPA when the algorithm was integrated with budget controllers.\nWe attribute the gains above as being mainly due to the explicit maximization\nof the surplus function, and note that other algorithms can take advantage of\nthis same approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 15:46:54 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Pan", "Shengjun", ""], ["Kitts", "Brendan", ""], ["Zhou", "Tian", ""], ["He", "Hao", ""], ["Shetty", "Bharatbhushan", ""], ["Flores", "Aaron", ""], ["Gligorijevic", "Djordje", ""], ["Pan", "Junwei", ""], ["Mao", "Tingyu", ""], ["Gultekin", "San", ""], ["Zhang", "Jianlong", ""]]}, {"id": "2009.09290", "submitter": "Gabriela Surita", "authors": "Gabriela Surita, Rodrigo Nogueira, Roberto Lotufo", "title": "Can questions summarize a corpus? Using question generation for\n  characterizing COVID-19 research", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the latent questions on some textual data? In this work, we\ninvestigate using question generation models for exploring a collection of\ndocuments. Our method, dubbed corpus2question, consists of applying a\npre-trained question generation model over a corpus and aggregating the\nresulting questions by frequency and time. This technique is an alternative to\nmethods such as topic modelling and word cloud for summarizing large amounts of\ntextual data. Results show that applying corpus2question on a corpus of\nscientific articles related to COVID-19 yields relevant questions about the\ntopic. The most frequent questions are \"what is covid 19\" and \"what is the\ntreatment for covid\". Among the 1000 most frequent questions are \"what is the\nthreshold for herd immunity\" and \"what is the role of ace2 in viral entry\". We\nshow that the proposed method generated similar questions for 13 of the 27\nexpert-made questions from the CovidQA question answering dataset.\n  The code to reproduce our experiments and the generated questions are\navailable at: https://github.com/unicamp-dl/corpus2question\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 19:57:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Surita", "Gabriela", ""], ["Nogueira", "Rodrigo", ""], ["Lotufo", "Roberto", ""]]}, {"id": "2009.09326", "submitter": "Nicolas Araque", "authors": "Nicolas Araque, Germano Rojas, Maria Vitali", "title": "UniNet: Next Term Course Recommendation using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICACSIS51025.2020.9263144", "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Course enrollment recommendation is a relevant task that helps university\nstudents decide what is the best combination of courses to enroll in the next\nterm. In particular, recommender system techniques like matrix factorization\nand collaborative filtering have been developed to try to solve this problem.\nAs these techniques fail to represent the time-dependent nature of academic\nperformance datasets we propose a deep learning approach using recurrent neural\nnetworks that aims to better represent how chronological order of course grades\naffects the probability of success. We have shown that it is possible to obtain\na performance of 81.10% on AUC metric using only grade information and that it\nis possible to develop a recommender system with academic student performance\nprediction. This is shown to be meaningful across different student GPA levels\nand course difficulties\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 00:07:45 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Araque", "Nicolas", ""], ["Rojas", "Germano", ""], ["Vitali", "Maria", ""]]}, {"id": "2009.09392", "submitter": "Ivan Sekulic", "authors": "Ivan Sekuli\\'c, Amir Soleimani, Mohammad Aliannejadi, Fabio Crestani", "title": "Longformer for MS MARCO Document Re-ranking Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two step document ranking, where the initial retrieval is done by a classical\ninformation retrieval method, followed by neural re-ranking model, is the new\nstandard. The best performance is achieved by using transformer-based models as\nre-rankers, e.g., BERT. We employ Longformer, a BERT-like model for long\ndocuments, on the MS MARCO document re-ranking task. The complete code used for\ntraining the model can be found on:\nhttps://github.com/isekulic/longformer-marco\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 09:22:24 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sekuli\u0107", "Ivan", ""], ["Soleimani", "Amir", ""], ["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}, {"id": "2009.09463", "submitter": "Yue Zhao", "authors": "Zheng Li, Yue Zhao, Nicola Botta, Cezar Ionescu, Xiyang Hu", "title": "COPOD: Copula-Based Outlier Detection", "comments": "Proceedings of the 2020 International Conference on Data Mining\n  (ICDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection refers to the identification of rare items that are deviant\nfrom the general data distribution. Existing approaches suffer from high\ncomputational complexity, low predictive capability, and limited\ninterpretability. As a remedy, we present a novel outlier detection algorithm\ncalled COPOD, which is inspired by copulas for modeling multivariate data\ndistribution. COPOD first constructs an empirical copula, and then uses it to\npredict tail probabilities of each given data point to determine its level of\n\"extremeness\". Intuitively, we think of this as calculating an anomalous\np-value. This makes COPOD both parameter-free, highly interpretable, and\ncomputationally efficient. In this work, we make three key contributions, 1)\npropose a novel, parameter-free outlier detection algorithm with both great\nperformance and interpretability, 2) perform extensive experiments on 30\nbenchmark datasets to show that COPOD outperforms in most cases and is also one\nof the fastest algorithms, and 3) release an easy-to-use Python implementation\nfor reproducibility.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 16:06:39 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Zheng", ""], ["Zhao", "Yue", ""], ["Botta", "Nicola", ""], ["Ionescu", "Cezar", ""], ["Hu", "Xiyang", ""]]}, {"id": "2009.09588", "submitter": "Jisu Jeong", "authors": "Jisu Jeong, Jeong-Min Yun, Hongi Keam, Young-Jin Park, Zimin Park,\n  Junki Cho", "title": "div2vec: Diversity-Emphasized Node Embedding", "comments": "To appear in the ImpactRS Workshop at ACM RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the interest of graph representation learning has been rapidly\nincreasing in recommender systems. However, most existing studies have focused\non improving accuracy, but in real-world systems, the recommendation diversity\nshould be considered as well to improve user experiences. In this paper, we\npropose the diversity-emphasized node embedding div2vec, which is a random\nwalk-based unsupervised learning method like DeepWalk and node2vec. When\ngenerating random walks, DeepWalk and node2vec sample nodes of higher degree\nmore and nodes of lower degree less. On the other hand, div2vec samples nodes\nwith the probability inversely proportional to its degree so that every node\ncan evenly belong to the collection of random walks. This strategy improves the\ndiversity of recommendation models. Offline experiments on the MovieLens\ndataset showed that our new method improves the recommendation performance in\nterms of both accuracy and diversity. Moreover, we evaluated the proposed model\non two real-world services, WATCHA and LINE Wallet Coupon, and observed the\ndiv2vec improves the recommendation quality by diversifying the system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 02:50:50 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jeong", "Jisu", ""], ["Yun", "Jeong-Min", ""], ["Keam", "Hongi", ""], ["Park", "Young-Jin", ""], ["Park", "Zimin", ""], ["Cho", "Junki", ""]]}, {"id": "2009.09671", "submitter": "Dimitrios Vasilas", "authors": "Dimitrios Vasilas (DELYS, SU), Marc Shapiro (DELYS, SU), Bradley King,\n  Sara Hamouda (DELYS, SU)", "title": "Towards application-specific query processing systems", "comments": null, "journal-ref": "36{\\`e}me Conf{\\'e}rence sur la Gestion de Donn{\\'e}es --\n  Principes, Technologies et Applications (BDA 2020), Oct 2020, Paris, France", "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database systems use query processing subsystems for enabling efficient\nquery-based data retrieval. An essential aspect of designing any\nquery-intensive application is tuning the query system to fit the application's\nrequirements and workload characteristics. However, the configuration\nparameters provided by traditional database systems do not cover the design\ndecisions and trade-offs that arise from the geo-distribution of users and\ndata. In this paper, we present a vision towards a new type of query system\narchitecture that addresses this challenge by enabling query systems to be\ndesigned and deployed in a per use case basis. We propose a distributed\nabstraction called Query Processing Unit that encapsulates primitive query\nprocessing tasks, and show how it can be used as a building block for\nassembling query systems. Using this approach, application architects can\nconstruct query systems specialized to their use cases, by controlling the\nquery system's architecture and the placement of its state. We demonstrate the\nexpressiveness of this approach by applying it to the design of a query system\nthat can flexibly place its state in the data center or at the edge, and show\nthat state placement decisions affect the trade-off between query response time\nand query result freshness.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 08:13:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Vasilas", "Dimitrios", "", "DELYS, SU"], ["Shapiro", "Marc", "", "DELYS, SU"], ["King", "Bradley", "", "DELYS, SU"], ["Hamouda", "Sara", "", "DELYS, SU"]]}, {"id": "2009.09926", "submitter": "Po Li", "authors": "Po Li, Lei Li, Yan Fu, Jun Rong, Yu Zhang", "title": "Cross-Modal Alignment with Mixture Experts Neural Network for\n  Intral-City Retail Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Cross-modal Alignment with mixture experts Neural\nNetwork (CameNN) recommendation model for intral-city retail industry, which\naims to provide fresh foods and groceries retailing within 5 hours delivery\nservice arising for the outbreak of Coronavirus disease (COVID-19) pandemic\naround the world. We propose CameNN, which is a multi-task model with three\ntasks including Image to Text Alignment (ITA) task, Text to Image Alignment\n(TIA) task and CVR prediction task. We use pre-trained BERT to generate the\ntext embedding and pre-trained InceptionV4 to generate image patch embedding\n(each image is split into small patches with the same pixels and treat each\npatch as an image token). Softmax gating networks follow to learn the weight of\neach transformer expert output and choose only a subset of experts conditioned\non the input. Then transformer encoder is applied as the share-bottom layer to\nlearn all input features' shared interaction. Next, mixture of transformer\nexperts (MoE) layer is implemented to model different aspects of tasks. At top\nof the MoE layer, we deploy a transformer layer for each task as task tower to\nlearn task-specific information. On the real word intra-city dataset,\nexperiments demonstrate CameNN outperform baselines and achieve significant\nimprovements on the image and text representation. In practice, we applied\nCameNN on CVR prediction in our intra-city recommender system which is one of\nthe leading intra-city platforms operated in China.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:36:52 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Li", "Po", ""], ["Li", "Lei", ""], ["Fu", "Yan", ""], ["Rong", "Jun", ""], ["Zhang", "Yu", ""]]}, {"id": "2009.09930", "submitter": "Mohammad Hadi", "authors": "Mohammad Abdul Hadi and Fatemeh H Fard", "title": "AOBTM: Adaptive Online Biterm Topic Modeling for Version Sensitive\n  Short-texts Analysis", "comments": "13 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of mobile app reviews has shown its important role in requirement\nengineering, software maintenance and evolution of mobile apps. Mobile app\ndevelopers check their users' reviews frequently to clarify the issues\nexperienced by users or capture the new issues that are introduced due to a\nrecent app update. App reviews have a dynamic nature and their discussed topics\nchange over time. The changes in the topics among collected reviews for\ndifferent versions of an app can reveal important issues about the app update.\nA main technique in this analysis is using topic modeling algorithms. However,\napp reviews are short texts and it is challenging to unveil their latent topics\nover time. Conventional topic models suffer from the sparsity of word\nco-occurrence patterns while inferring topics for short texts. Furthermore,\nthese algorithms cannot capture topics over numerous consecutive time-slices.\nOnline topic modeling algorithms speed up the inference of topic models for the\ntexts collected in the latest time-slice by saving a fraction of data from the\nprevious time-slice. But these algorithms do not analyze the statistical-data\nof all the previous time-slices, which can confer contributions to the topic\ndistribution of the current time-slice.\n  We propose Adaptive Online Biterm Topic Model (AOBTM) to model topics in\nshort texts adaptively. AOBTM alleviates the sparsity problem in short-texts\nand considers the statistical-data for an optimal number of previous\ntime-slices. We also propose parallel algorithms to automatically determine the\noptimal number of topics and the best number of previous versions that should\nbe considered in topic inference phase. Automatic evaluation on collections of\napp reviews and real-world short text datasets confirm that AOBTM can find more\ncoherent topics and outperforms the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 09:50:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hadi", "Mohammad Abdul", ""], ["Fard", "Fatemeh H", ""]]}, {"id": "2009.09931", "submitter": "Harshit Pande", "authors": "Harshit Pande", "title": "Field-Embedded Factorization Machines for Click-through rate prediction", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction models are common in many online\napplications such as digital advertising and recommender systems. Field-Aware\nFactorization Machine (FFM) and Field-weighted Factorization Machine (FwFM) are\nstate-of-the-art among the shallow models for CTR prediction. Recently, many\ndeep learning-based models have also been proposed. Among deeper models,\nDeepFM, xDeepFM, AutoInt+, and FiBiNet are state-of-the-art models. The deeper\nmodels combine a core architectural component, which learns explicit feature\ninteractions, with a deep neural network (DNN) component. We propose a novel\nshallow Field-Embedded Factorization Machine (FEFM) and its deep counterpart\nDeep Field-Embedded Factorization Machine (DeepFEFM). FEFM learns symmetric\nmatrix embeddings for each field pair along with the usual single vector\nembeddings for each feature. FEFM has significantly lower model complexity than\nFFM and roughly the same complexity as FwFM. FEFM also has insightful\nmathematical properties about important fields and field interactions. DeepFEFM\ncombines the FEFM interaction vectors learned by the FEFM component with a DNN\nand is thus able to learn higher order interactions. We conducted comprehensive\nexperiments over a wide range of hyperparameters on two large publicly\navailable real-world datasets. When comparing test AUC and log loss, the\nresults show that FEFM and DeepFEFM outperform the existing state-of-the-art\nshallow and deep models for CTR prediction tasks. We have made the code of FEFM\nand DeepFEFM available in the DeepCTR library\n(https://github.com/shenweichen/DeepCTR).\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 15:32:42 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 18:45:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pande", "Harshit", ""]]}, {"id": "2009.09935", "submitter": "Christine Bauer", "authors": "Markus Schedl, Christine Bauer, Wolfgang Reisinger, Dominik Kowald,\n  Elisabeth Lex", "title": "Listener Modeling and Context-aware Music Recommendation Based on\n  Country Archetypes", "comments": "30 pages, 3 tables, 12 figures", "journal-ref": null, "doi": "10.3389/frai.2020.508725", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music preferences are strongly shaped by the cultural and socio-economic\nbackground of the listener, which is reflected, to a considerable extent, in\ncountry-specific music listening profiles. Previous work has already identified\nseveral country-specific differences in the popularity distribution of music\nartists listened to. In particular, what constitutes the \"music mainstream\"\nstrongly varies between countries. To complement and extend these results, the\narticle at hand delivers the following major contributions: First, using\nstate-of-the-art unsupervised learning techniques, we identify and thoroughly\ninvestigate (1) country profiles of music preferences on the fine-grained level\nof music tracks (in contrast to earlier work that relied on music preferences\non the artist level) and (2) country archetypes that subsume countries sharing\nsimilar patterns of listening preferences. Second, we formulate four user\nmodels that leverage the user's country information on music preferences. Among\nothers, we propose a user modeling approach to describe a music listener as a\nvector of similarities over the identified country clusters or archetypes.\nThird, we propose a context-aware music recommendation system that leverages\nimplicit user feedback, where context is defined via the four user models. More\nprecisely, it is a multi-layer generative model based on a variational\nautoencoder, in which contextual features can influence recommendations through\na gating mechanism. Fourth, we thoroughly evaluate the proposed recommendation\nsystem and user models on a real-world corpus of more than one billion\nlistening records of users around the world (out of which we use 369 million in\nour experiments) and show its merits vis-a-vis state-of-the-art algorithms that\ndo not exploit this type of context information.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:59:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Schedl", "Markus", ""], ["Bauer", "Christine", ""], ["Reisinger", "Wolfgang", ""], ["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""]]}, {"id": "2009.09945", "submitter": "Wenjie Wang", "authors": "Wenjie Wang, Fuli Feng, Xiangnan He, Hanwang Zhang, Tat-Seng Chua", "title": "Clicks can be Cheating: Counterfactual Recommendation for Mitigating\n  Clickbait Issue", "comments": "Accepted by SIGIR 2021", "journal-ref": "Proceedings of the 44th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2021)", "doi": "10.1145/3404835.3462962", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation is a prevalent and critical service in information systems. To\nprovide personalized suggestions to users, industry players embrace machine\nlearning, more specifically, building predictive models based on the click\nbehavior data. This is known as the Click-Through Rate (CTR) prediction, which\nhas become the gold standard for building personalized recommendation service.\nHowever, we argue that there is a significant gap between clicks and user\nsatisfaction -- it is common that a user is \"cheated\" to click an item by the\nattractive title/cover of the item. This will severely hurt user's trust on the\nsystem if the user finds the actual content of the clicked item disappointing.\nWhat's even worse, optimizing CTR models on such flawed data will result in the\nMatthew Effect, making the seemingly attractive but actually low-quality items\nbe more frequently recommended.\n  In this paper, we formulate the recommendation models as a causal graph that\nreflects the cause-effect factors in recommendation, and address the clickbait\nissue by performing counterfactual inference on the causal graph. We imagine a\ncounterfactual world where each item has only exposure features (i.e., the\nfeatures that the user can see before making a click decision). By estimating\nthe click likelihood of a user in the counterfactual world, we are able to\nreduce the direct effect of exposure features and eliminate the clickbait\nissue. Experiments on real-world datasets demonstrate that our method\nsignificantly improves the post-click satisfaction of CTR models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 15:08:10 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 02:33:42 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 04:23:51 GMT"}, {"version": "v4", "created": "Sat, 22 May 2021 06:08:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Wenjie", ""], ["Feng", "Fuli", ""], ["He", "Xiangnan", ""], ["Zhang", "Hanwang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2009.10002", "submitter": "Yujia Zheng", "authors": "Yujia Zheng, Siyi Liu, Zekun Li, Shu Wu", "title": "DGTN: Dual-channel Graph Transition Network for Session-based\n  Recommendation", "comments": "Accepted at ICDMW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of session-based recommendation is to predict user actions based on\nanonymous sessions. Recent research mainly models the target session as a\nsequence or a graph to capture item transitions within it, ignoring complex\ntransitions between items in different sessions that have been generated by\nother users. These item transitions include potential collaborative information\nand reflect similar behavior patterns, which we assume may help with the\nrecommendation for the target session. In this paper, we propose a novel\nmethod, namely Dual-channel Graph Transition Network (DGTN), to model item\ntransitions within not only the target session but also the neighbor sessions.\nSpecifically, we integrate the target session and its neighbor (similar)\nsessions into a single graph. Then the transition signals are explicitly\ninjected into the embedding by channel-aware propagation. Experiments on\nreal-world datasets demonstrate that DGTN outperforms other state-of-the-art\nmethods. Further analysis verifies the rationality of dual-channel item\ntransition modeling, suggesting a potential future direction for session-based\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 16:29:29 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zheng", "Yujia", ""], ["Liu", "Siyi", ""], ["Li", "Zekun", ""], ["Wu", "Shu", ""]]}, {"id": "2009.10128", "submitter": "Mickael Arcos", "authors": "Micka\\\"el Arcos", "title": "Claraprint: a chord and melody based fingerprint for western classical\n  music cover detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cover song detection has been an active field in the Music Information\nRetrieval (MIR) community during the past decades. Most of the research\ncommunity focused in solving it for a wide range of music genres with diverse\ncharacteristics. Western classical music, a genre heavily based on the\nrecording of \"cover songs\", or musical works, represents a large heritage,\noffering immediate application for an efficient fingerprint algorithm. We\npropose an engineering approach for retrieving a cover song from a reference\ndatabase thanks to a fingerprint designed for classical musical works. We open\na new data set to encourage the scientific community to use it for further\nresearches regarding this genre.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 18:46:00 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Arcos", "Micka\u00ebl", ""]]}, {"id": "2009.10255", "submitter": "EPTCS", "authors": "Thomas Prokosch (Institute for Informatics, Ludwig-Maximilian\n  University of Munich, Germany)", "title": "A Low-Level Index for Distributed Logic Programming", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 303-312", "doi": "10.4204/EPTCS.325.40", "report-no": null, "categories": "cs.SC cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed logic programming language with support for meta-programming\nand stream processing offers a variety of interesting research problems, such\nas: How can a versatile and stable data structure for the indexing of a large\nnumber of expressions be implemented with simple low-level data structures? Can\nlow-level programming help to reduce the number of occur checks in Robinson's\nunification algorithm? This article gives the answers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:52:15 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Prokosch", "Thomas", "", "Institute for Informatics, Ludwig-Maximilian\n  University of Munich, Germany"]]}, {"id": "2009.10270", "submitter": "Davis Liang", "authors": "Davis Liang, Peng Xu, Siamak Shakeri, Cicero Nogueira dos Santos,\n  Ramesh Nallapati, Zhiheng Huang, Bing Xiang", "title": "Embedding-based Zero-shot Retrieval through Query Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passage retrieval addresses the problem of locating relevant passages,\nusually from a large corpus, given a query. In practice, lexical term-matching\nalgorithms like BM25 are popular choices for retrieval owing to their\nefficiency. However, term-based matching algorithms often miss relevant\npassages that have no lexical overlap with the query and cannot be finetuned to\ndownstream datasets. In this work, we consider the embedding-based two-tower\narchitecture as our neural retrieval model. Since labeled data can be scarce\nand because neural retrieval models require vast amounts of data to train, we\npropose a novel method for generating synthetic training data for retrieval.\nOur system produces remarkable results, significantly outperforming BM25 on 5\nout of 6 datasets tested, by an average of 2.45 points for Recall@1. In some\ncases, our model trained on synthetic data can even outperform the same model\ntrained on real data\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:54:27 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Liang", "Davis", ""], ["Xu", "Peng", ""], ["Shakeri", "Siamak", ""], ["Santos", "Cicero Nogueira dos", ""], ["Nallapati", "Ramesh", ""], ["Huang", "Zhiheng", ""], ["Xiang", "Bing", ""]]}, {"id": "2009.10295", "submitter": "Cheng Yan", "authors": "Cheng Yan, Guansong Pang, Xiao Bai, Jun Zhou, Lin Gu", "title": "Beyond Triplet Loss: Person Re-identification with Fine-grained\n  Difference-aware Pairwise Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-IDentification (ReID) aims at re-identifying persons from different\nviewpoints across multiple cameras. Capturing the fine-grained appearance\ndifferences is often the key to accurate person ReID, because many identities\ncan be differentiated only when looking into these fine-grained differences.\nHowever, most state-of-the-art person ReID approaches, typically driven by a\ntriplet loss, fail to effectively learn the fine-grained features as they are\nfocused more on differentiating large appearance differences. To address this\nissue, we introduce a novel pairwise loss function that enables ReID models to\nlearn the fine-grained features by adaptively enforcing an exponential\npenalization on the images of small differences and a bounded penalization on\nthe images of large differences. The proposed loss is generic and can be used\nas a plugin to replace the triplet loss to significantly enhance different\ntypes of state-of-the-art approaches. Experimental results on four benchmark\ndatasets show that the proposed loss substantially outperforms a number of\npopular loss functions by large margins; and it also enables significantly\nimproved data efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 03:04:12 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Yan", "Cheng", ""], ["Pang", "Guansong", ""], ["Bai", "Xiao", ""], ["Zhou", "Jun", ""], ["Gu", "Lin", ""]]}, {"id": "2009.10606", "submitter": "Yue Zhao", "authors": "Yue Zhao, Ryan A. Rossi, Leman Akoglu", "title": "Automating Outlier Detection via Meta-Learning", "comments": "21 pages. The code is available at http://github.com/yzhao062/MetaOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an unsupervised outlier detection (OD) task on a new dataset, how can\nwe automatically select a good outlier detection method and its\nhyperparameter(s) (collectively called a model)? Thus far, model selection for\nOD has been a \"black art\"; as any model evaluation is infeasible due to the\nlack of (i) hold-out data with labels, and (ii) a universal objective function.\nIn this work, we develop the first principled data-driven approach to model\nselection for OD, called MetaOD, based on meta-learning. MetaOD capitalizes on\nthe past performances of a large body of detection models on existing outlier\ndetection benchmark datasets, and carries over this prior experience to\nautomatically select an effective model to be employed on a new dataset without\nusing any labels. To capture task similarity, we introduce specialized\nmeta-features that quantify outlying characteristics of a dataset. Through\ncomprehensive experiments, we show the effectiveness of MetaOD in selecting a\ndetection model that significantly outperforms the most popular outlier\ndetectors (e.g., LOF and iForest) as well as various state-of-the-art\nunsupervised meta-learners while being extremely fast. To foster\nreproducibility and further research on this new problem, we open-source our\nentire meta-learning system, benchmark environment, and testbed datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:14:45 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:44:35 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhao", "Yue", ""], ["Rossi", "Ryan A.", ""], ["Akoglu", "Leman", ""]]}, {"id": "2009.10619", "submitter": "Chongshou Li Dr", "authors": "Chongshou Li, Brenda Cheang, Zhixing Luo and Andrew Lim", "title": "An Exponential Factorization Machine with Percentage Error Minimization\n  to Retail Sales Forecasting", "comments": "Accepted by ACM Transactions on Knowledge Discovery from Data (ACM\n  TKDD)", "journal-ref": "ACM Transactions on Knowledge Discovery from Data 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to sales forecasting for new products with\nlong lead time but short product life cycle. These SKUs are usually sold for\none season only, without any replenishments. An exponential factorization\nmachine (EFM) sales forecast model is developed to solve this problem which not\nonly considers SKU attributes, but also pairwise interactions. The EFM model is\nsignificantly different from the original Factorization Machines (FM) from\ntwo-fold: (1) the attribute-level formulation for explanatory variables and (2)\nexponential formulation for the positive response variable. The attribute-level\nformation excludes infeasible intra-attribute interactions and results in more\nefficient feature engineering comparing with the conventional one-hot encoding,\nwhile the exponential formulation is demonstrated more effective than the\nlog-transformation for the positive but not skewed distributed responses. In\norder to estimate the parameters, percentage error squares (PES) and error\nsquares (ES) are minimized by a proposed adaptive batch gradient descent method\nover the training set. Real-world data provided by a footwear retailer in\nSingapore is used for testing the proposed approach. The forecasting\nperformance in terms of both mean absolute percentage error (MAPE) and mean\nabsolute error (MAE) compares favourably with not only off-the-shelf models but\nalso results reported by extant sales and demand forecasting studies. The\neffectiveness of the proposed approach is also demonstrated by two external\npublic datasets. Moreover, we prove the theoretical relationships between PES\nand ES minimization, and present an important property of the PES minimization\nfor regression models; that it trains models to underestimate data. This\nproperty fits the situation of sales forecasting where unit-holding cost is\nmuch greater than the unit-shortage cost.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 15:21:38 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Li", "Chongshou", ""], ["Cheang", "Brenda", ""], ["Luo", "Zhixing", ""], ["Lim", "Andrew", ""]]}, {"id": "2009.10778", "submitter": "Danqing Zhang", "authors": "Danqing Zhang, Tao Li, Haiyang Zhang, Bing Yin", "title": "On Data Augmentation for Extreme Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on data augmentation for the extreme multi-label\nclassification (XMC) problem. One of the most challenging issues of XMC is the\nlong tail label distribution where even strong models suffer from insufficient\nsupervision. To mitigate such label bias, we propose a simple and effective\naugmentation framework and a new state-of-the-art classifier. Our augmentation\nframework takes advantage of the pre-trained GPT-2 model to generate\nlabel-invariant perturbations of the input texts to augment the existing\ntraining data. As a result, it present substantial improvements over baseline\nmodels. Our contributions are two-factored: (1) we introduce a new\nstate-of-the-art classifier that uses label attention with RoBERTa and combine\nit with our augmentation framework for further improvement; (2) we present a\nbroad study on how effective are different augmentation methods in the XMC\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 19:31:08 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Danqing", ""], ["Li", "Tao", ""], ["Zhang", "Haiyang", ""], ["Yin", "Bing", ""]]}, {"id": "2009.10791", "submitter": "Zhengzhong Liang", "authors": "Zhengzhong Liang, Yiyun Zhao, Mihai Surdeanu", "title": "Using the Hammer Only on Nails: A Hybrid Method for Evidence Retrieval\n  for Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence retrieval is a key component of explainable question answering (QA).\nWe argue that, despite recent progress, transformer network-based approaches\nsuch as universal sentence encoder (USE-QA) do not always outperform\ntraditional information retrieval (IR) methods such as BM25 for evidence\nretrieval for QA. We introduce a lexical probing task that validates this\nobservation: we demonstrate that neural IR methods have the capacity to capture\nlexical differences between questions and answers, but miss obvious lexical\noverlap signal. Learning from this probing analysis, we introduce a hybrid\napproach for evidence retrieval that combines the advantages of both IR\ndirections. Our approach uses a routing classifier that learns when to direct\nincoming questions to BM25 vs. USE-QA for evidence retrieval using very simple\nstatistics, which can be efficiently extracted from the top candidate evidence\nsentences produced by a BM25 model. We demonstrate that this hybrid evidence\nretrieval generally performs better than either individual retrieval strategy\non three QA datasets: OpenBookQA, ReQA SQuAD, and ReQA NQ. Furthermore, we show\nthat the proposed routing strategy is considerably faster than neural methods,\nwith a runtime that is up to 5 times faster than USE-QA.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 20:08:14 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Liang", "Zhengzhong", ""], ["Zhao", "Yiyun", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "2009.10989", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh, Dhruv Gelda, Zhongfang Zhuang, Yan Zheng, Liang\n  Gou, Wei Zhang", "title": "Towards a Flexible Embedding Learning Framework", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is a fundamental building block for analyzing\nentities in a database. While the existing embedding learning methods are\neffective in various data mining problems, their applicability is often limited\nbecause these methods have pre-determined assumptions on the type of semantics\ncaptured by the learned embeddings, and the assumptions may not well align with\nspecific downstream tasks. In this work, we propose an embedding learning\nframework that 1) uses an input format that is agnostic to input data type, 2)\nis flexible in terms of the relationships that can be embedded into the learned\nrepresentations, and 3) provides an intuitive pathway to incorporate domain\nknowledge into the embedding learning process. Our proposed framework utilizes\na set of entity-relation-matrices as the input, which quantifies the affinities\namong different entities in the database. Moreover, a sampling mechanism is\ncarefully designed to establish a direct connection between the input and the\ninformation captured by the output embeddings. To complete the representation\nlearning toolbox, we also outline a simple yet effective post-processing\ntechnique to properly visualize the learned embeddings. Our empirical results\ndemonstrate that the proposed framework, in conjunction with a set of relevant\nentity-relation-matrices, outperforms the existing state-of-the-art approaches\nin various data mining tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 08:00:56 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""], ["Gelda", "Dhruv", ""], ["Zhuang", "Zhongfang", ""], ["Zheng", "Yan", ""], ["Gou", "Liang", ""], ["Zhang", "Wei", ""]]}, {"id": "2009.11129", "submitter": "Saba Nazir", "authors": "Saba Nazir, Taner Cagali, Chris Newell, Mehrnoosh Sadrzadeh", "title": "Cosine Similarity of Multimodal Content Vectors for TV Programmes", "comments": "3 pages, 1 figure, Machine Learning for Media Discovery (ML4MD)\n  Workshop at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal information originates from a variety of sources: audiovisual\nfiles, textual descriptions, and metadata. We show how one can represent the\ncontent encoded by each individual source using vectors, how to combine the\nvectors via middle and late fusion techniques, and how to compute the semantic\nsimilarities between the contents. Our vectorial representations are built from\nspectral features and Bags of Audio Words, for audio, LSI topics and Doc2vec\nembeddings for subtitles, and the categorical features, for metadata. We\nimplement our model on a dataset of BBC TV programmes and evaluate the fused\nrepresentations to provide recommendations. The late fused similarity matrices\nsignificantly improve the precision and diversity of recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:12:30 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Nazir", "Saba", ""], ["Cagali", "Taner", ""], ["Newell", "Chris", ""], ["Sadrzadeh", "Mehrnoosh", ""]]}, {"id": "2009.11221", "submitter": "Florian Wirthm\\\"uller", "authors": "Florian Wirthm\\\"uller, Marvin Klimke, Julian Schlechtriemen, Jochen\n  Hipp and Manfred Reichert", "title": "A Fleet Learning Architecture for Enhanced Behavior Predictions during\n  Challenging External Conditions", "comments": "the article has been accepted for publication during the 2020 IEEE\n  Symposium Series on Computational Intelligence (SSCI) within the IEEE\n  Symposium on Computational Intelligence in Vehicles and Transportation\n  Systems (CIVTS), 7 pages, 6 figures", "journal-ref": null, "doi": "10.1109/SSCI47803.2020.9308542", "report-no": null, "categories": "cs.RO cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Already today, driver assistance systems help to make daily traffic more\ncomfortable and safer. However, there are still situations that are quite rare\nbut are hard to handle at the same time. In order to cope with these situations\nand to bridge the gap towards fully automated driving, it becomes necessary to\nnot only collect enormous amounts of data but rather the right ones. This data\ncan be used to develop and validate the systems through machine learning and\nsimulation pipelines. Along this line this paper presents a fleet\nlearning-based architecture that enables continuous improvements of systems\npredicting the movement of surrounding traffic participants. Moreover, the\npresented architecture is applied to a testing vehicle in order to prove the\nfundamental feasibility of the system. Finally, it is shown that the system\ncollects meaningful data which are helpful to improve the underlying prediction\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 15:33:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 06:31:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wirthm\u00fcller", "Florian", ""], ["Klimke", "Marvin", ""], ["Schlechtriemen", "Julian", ""], ["Hipp", "Jochen", ""], ["Reichert", "Manfred", ""]]}, {"id": "2009.11352", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Julia Kiseleva and Aleksandr Chuklin and Jeff\n  Dalton and Mikhail Burtsev", "title": "ConvAI3: Generating Clarifying Questions for Open-Domain Dialogue\n  Systems (ClariQ)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document presents a detailed description of the challenge on clarifying\nquestions for dialogue systems (ClariQ). The challenge is organized as part of\nthe Conversational AI challenge series (ConvAI3) at Search Oriented\nConversational AI (SCAI) EMNLP workshop in 2020. The main aim of the\nconversational systems is to return an appropriate answer in response to the\nuser requests. However, some user requests might be ambiguous. In IR settings\nsuch a situation is handled mainly thought the diversification of the search\nresult page. It is however much more challenging in dialogue settings with\nlimited bandwidth. Therefore, in this challenge, we provide a common evaluation\nframework to evaluate mixed-initiative conversations. Participants are asked to\nrank clarifying questions in an information-seeking conversations. The\nchallenge is organized in two stages where in Stage 1 we evaluate the\nsubmissions in an offline setting and single-turn conversations. Top\nparticipants of Stage 1 get the chance to have their model tested by human\nannotators.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 19:48:02 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Kiseleva", "Julia", ""], ["Chuklin", "Aleksandr", ""], ["Dalton", "Jeff", ""], ["Burtsev", "Mikhail", ""]]}, {"id": "2009.11559", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda and Yasuo Tabei", "title": "Dynamic Similarity Search on Integer Sketches", "comments": "Accepted by IEEE ICDM 2020 as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-preserving hashing is a core technique for fast similarity\nsearches, and it randomly maps data points in a metric space to strings of\ndiscrete symbols (i.e., sketches) in the Hamming space. While traditional\nhashing techniques produce binary sketches, recent ones produce integer\nsketches for preserving various similarity measures. However, most similarity\nsearch methods are designed for binary sketches and inefficient for integer\nsketches. Moreover, most methods are either inapplicable or inefficient for\ndynamic datasets, although modern real-world datasets are updated over time. We\npropose dynamic filter trie (DyFT), a dynamic similarity search method for both\nbinary and integer sketches. An extensive experimental analysis using large\nreal-world datasets shows that DyFT performs superiorly with respect to\nscalability, time performance, and memory efficiency. For example, on a huge\ndataset of 216 million data points, DyFT performs a similarity search 6,000\ntimes faster than a state-of-the-art method while reducing to one-thirteenth in\nmemory.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:13:17 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Kanda", "Shunsuke", ""], ["Tabei", "Yasuo", ""]]}, {"id": "2009.11576", "submitter": "Krisztian Balog", "authors": "Kristian Gingstad and {\\O}yvind Jekteberg and Krisztian Balog", "title": "ArXivDigest: A Living Lab for Personalized Scientific Literature\n  Recommendation", "comments": "Proceedings of the 29th ACM International Conference on Information\n  and Knowledge Management (CIKM'20), Oct 2020", "journal-ref": null, "doi": "10.1145/3340531.3417417", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing personalized recommendations that are also accompanied by\nexplanations as to why an item is recommended is a research area of growing\nimportance. At the same time, progress is limited by the availability of open\nevaluation resources. In this work, we address the task of scientific\nliterature recommendation. We present arXivDigest, which is an online service\nproviding personalized arXiv recommendations to end users and operates as a\nliving lab for researchers wishing to work on explainable scientific literature\nrecommendations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 09:53:25 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Gingstad", "Kristian", ""], ["Jekteberg", "\u00d8yvind", ""], ["Balog", "Krisztian", ""]]}, {"id": "2009.11771", "submitter": "Diego Saez-Trumper", "authors": "Oleksii Moskalenko, Denis Parra, and Diego Saez-Trumper", "title": "Scalable Recommendation of Wikipedia Articles to Editors Using\n  Representation Learning", "comments": null, "journal-ref": "ComplexRec 2020, Workshop on Recommendation in Complex Scenarios\n  at the ACM RecSys Conference on Recommender Systems (RecSys 2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is edited by volunteer editors around the world. Considering the\nlarge amount of existing content (e.g. over 5M articles in English Wikipedia),\ndeciding what to edit next can be difficult, both for experienced users that\nusually have a huge backlog of articles to prioritize, as well as for newcomers\nwho that might need guidance in selecting the next article to contribute.\nTherefore, helping editors to find relevant articles should improve their\nperformance and help in the retention of new editors. In this paper, we address\nthe problem of recommending relevant articles to editors. To do this, we\ndevelop a scalable system on top of Graph Convolutional Networks and Doc2Vec,\nlearning how to represent Wikipedia articles and deliver personalized\nrecommendations for editors. We test our model on editors' histories,\npredicting their most recent edits based on their prior edits. We outperform\ncompetitive implicit-feedback collaborative-filtering methods such as WMRF\nbased on ALS, as well as a traditional IR-method such as content-based\nfiltering based on BM25. All of the data used on this paper is publicly\navailable, including graph embeddings for Wikipedia articles, and we release\nour code to support replication of our experiments. Moreover, we contribute\nwith a scalable implementation of a state-of-art graph embedding algorithm as\ncurrent ones cannot efficiently handle the sheer size of the Wikipedia graph.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:56:02 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Moskalenko", "Oleksii", ""], ["Parra", "Denis", ""], ["Saez-Trumper", "Diego", ""]]}, {"id": "2009.11796", "submitter": "Neha Kaushik", "authors": "Niladri Chatterjee, Neha Kaushik", "title": "Automatic Extraction of Agriculture Terms from Domain Text: A Survey of\n  Tools and Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agriculture is a key component in any country's development. Domain-specific\nknowledge resources serve to gain insight into the domain. Existing knowledge\nresources such as AGROVOC and NAL Thesaurus are developed and maintained by the\ndomain experts. Population of terms into these knowledge resources can be\nautomated by using automatic term extraction tools for processing unstructured\nagricultural text. Automatic term extraction is also a key component in many\nsemantic web applications, such as ontology creation, recommendation systems,\nsentiment classification, query expansion among others. The primary goal of an\nautomatic term extraction system is to maximize the number of valid terms and\nminimize the number of invalid terms extracted from the input set of documents.\nDespite its importance in various applications, the availability of online\ntools for the said purpose is rather limited. Moreover, the performance of the\nmost popular ones among them varies significantly. As a consequence, selection\nof the right term extraction tool is perceived as a serious problem for\ndifferent knowledge-based applications. This paper presents an analysis of\nthree commonly used term extraction tools, viz. RAKE, TerMine, TermRaider and\ncompares their performance in terms of precision and recall, vis-a-vis RENT, a\nmore recent term extractor developed by these authors for agriculture domain.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 16:38:44 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Chatterjee", "Niladri", ""], ["Kaushik", "Neha", ""]]}, {"id": "2009.11898", "submitter": "Anna Glazkova", "authors": "Anna Glazkova, Yury Egorov, Maksim Glazkov", "title": "A Comparative Study of Feature Types for Age-Based Text Classification", "comments": "Accepted to AIST-2020 (The 9th International Conference on Analysis\n  of Images, Social Networks and Texts)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to automatically determine the age audience of a novel provides\nmany opportunities for the development of information retrieval tools. Firstly,\ndevelopers of book recommendation systems and electronic libraries may be\ninterested in filtering texts by the age of the most likely readers. Further,\nparents may want to select literature for children. Finally, it will be useful\nfor writers and publishers to determine which features influence whether the\ntexts are suitable for children. In this article, we compare the empirical\neffectiveness of various types of linguistic features for the task of age-based\nclassification of fiction texts. For this purpose, we collected a text corpus\nof book previews labeled with one of two categories -- children's or adult. We\nevaluated the following types of features: readability indices, sentiment,\nlexical, grammatical and general features, and publishing attributes. The\nresults obtained show that the features describing the text at the document\nlevel can significantly increase the quality of machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 18:41:10 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Glazkova", "Anna", ""], ["Egorov", "Yury", ""], ["Glazkov", "Maksim", ""]]}, {"id": "2009.12097", "submitter": "Amin Heydari Alashti", "authors": "Amin Heydari Alashti, Ahmad Asgharian Rezaei, Alireza Elahi, Sobhan\n  Sayyaran, Mohammad Ghodsi", "title": "Parsisanj: a semi-automatic component-based approach towards search\n  engine evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessing to required data on the internet is wide via search engines in the\nlast two decades owing to the huge amount of available data and the high rate\nof new data is generating daily. Accordingly, search engines are encouraged to\nmake the most valuable existing data on the web searchable. Knowing how to\nhandle a large amount of data in each step of a search engines' procedure from\ncrawling to indexing and ranking is just one of the challenges that a\nprofessional search engine should solve. Moreover, it should also have the best\npractices in handling users' traffics, state-of-the-art natural language\nprocessing tools, and should also address many other challenges on the edge of\nscience and technology. As a result, evaluating these systems is too\nchallenging due to the level of internal complexity they have, and is crucial\nfor finding the improvement path of the existing system. Therefore, an\nevaluation procedure is a normal subsystem of a search engine that has the role\nof building its roadmap. Recently, several countries have developed national\nsearch engine programs to build an infrastructure to provide special services\nbased on their needs on the available data of their language on the web. This\nresearch is conducted accordingly to enlighten the advancement path of two\nIranian national search engines: Yooz and Parsijoo in comparison with two\ninternational ones, Google and Bing. Unlike related work, it is a\nsemi-automatic method to evaluate the search engines at the first pace.\nEventually, we obtained some interesting results which based on them the\ncomponent-based improvement roadmap of national search engines could be\nillustrated concretely.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 09:17:57 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Alashti", "Amin Heydari", ""], ["Rezaei", "Ahmad Asgharian", ""], ["Elahi", "Alireza", ""], ["Sayyaran", "Sobhan", ""], ["Ghodsi", "Mohammad", ""]]}, {"id": "2009.12192", "submitter": "Benjamin Chamberlain", "authors": "Benjamin P. Chamberlain, Emanuele Rossi, Dan Shiebler, Suvash Sedhain,\n  Michael M. Bronstein", "title": "Tuning Word2vec for Large Scale Recommendation Systems", "comments": "11 pages, 4 figures, Fourteenth ACM Conference on Recommender Systems", "journal-ref": "Fourteenth ACM Conference on Recommender Systems (RecSys '20),\n  September 22--26, 2020, Virtual Event, Brazil", "doi": "10.1145/3383313.3418486", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec is a powerful machine learning tool that emerged from Natural\nLan-guage Processing (NLP) and is now applied in multiple domains, including\nrecom-mender systems, forecasting, and network analysis. As Word2vec is often\nused offthe shelf, we address the question of whether the default\nhyperparameters are suit-able for recommender systems. The answer is\nemphatically no. In this paper, wefirst elucidate the importance of\nhyperparameter optimization and show that un-constrained optimization yields an\naverage 221% improvement in hit rate over thedefault parameters. However,\nunconstrained optimization leads to hyperparametersettings that are very\nexpensive and not feasible for large scale recommendationtasks. To this end, we\ndemonstrate 138% average improvement in hit rate with aruntime\nbudget-constrained hyperparameter optimization. Furthermore, to\nmakehyperparameter optimization applicable for large scale recommendation\nproblemswhere the target dataset is too large to search over, we investigate\ngeneralizinghyperparameters settings from samples. We show that applying\nconstrained hy-perparameter optimization using only a 10% sample of the data\nstill yields a 91%average improvement in hit rate over the default parameters\nwhen applied to thefull datasets. Finally, we apply hyperparameters learned\nusing our method of con-strained optimization on a sample to the Who To Follow\nrecommendation serviceat Twitter and are able to increase follow rates by 15%.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 10:50:19 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chamberlain", "Benjamin P.", ""], ["Rossi", "Emanuele", ""], ["Shiebler", "Dan", ""], ["Sedhain", "Suvash", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "2009.12298", "submitter": "Matt Sharpe", "authors": "Matthew Sharpe", "title": "A review of metadata fields associated with podcast RSS feeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Podcasts are traditionally shared through RSS feeds. As well as pointing to\nthe audio files, RSS gives a creator a way of providing metadata about the\npodcast shows and episodes. We investigate how certain metadata fields\nassociated with podcasts are currently being used and comment on their\napplicability to recommendations. Specifically, we find that many creators are\nnot using the itunes:type field in the expected fashion, and that using this\nfield for recommendations might not lead to an optimal user experience. We\nperform similar explorations for the season number and the category associated\nwith a podcast, and also find that the fields aren't being used in the expected\nfashion. Finally, we examine the notion that a single podcast show is the same\nas a single RSS feed. This also turns out to not be strictly true in all cases.\nIn short, the metadata associated with many podcasts isn't always reflective of\nthe show and should be used with caution.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 15:41:37 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 09:39:08 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Sharpe", "Matthew", ""]]}, {"id": "2009.12316", "submitter": "Xin Qian", "authors": "Xin Qian, Ryan A. Rossi, Fan Du, Sungchul Kim, Eunyee Koh, Sana Malik,\n  Tak Yeon Lee, Joel Chan", "title": "ML-based Visualization Recommendation: Learning to Recommend\n  Visualizations from Data", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation seeks to generate, score, and recommend to users\nuseful visualizations automatically, and are fundamentally important for\nexploring and gaining insights into a new or existing dataset quickly. In this\nwork, we propose the first end-to-end ML-based visualization recommendation\nsystem that takes as input a large corpus of datasets and visualizations,\nlearns a model based on this data. Then, given a new unseen dataset from an\narbitrary user, the model automatically generates visualizations for that new\ndataset, derive scores for the visualizations, and output a list of recommended\nvisualizations to the user ordered by effectiveness. We also describe an\nevaluation framework to quantitatively evaluate visualization recommendation\nmodels learned from a large corpus of visualizations and datasets. Through\nquantitative experiments, a user study, and qualitative analysis, we show that\nour end-to-end ML-based system recommends more effective and useful\nvisualizations compared to existing state-of-the-art rule-based systems.\nFinally, we observed a strong preference by the human experts in our user study\ntowards the visualizations recommended by our ML-based system as opposed to the\nrule-based system (5.92 from a 7-point Likert scale compared to only 3.45).\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:13:29 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Qian", "Xin", ""], ["Rossi", "Ryan A.", ""], ["Du", "Fan", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""], ["Malik", "Sana", ""], ["Lee", "Tak Yeon", ""], ["Chan", "Joel", ""]]}, {"id": "2009.12414", "submitter": "Haruna Isah", "authors": "Chantal Montgomery, Haruna Isah, Farhana Zulkernine", "title": "Towards a Natural Language Query Processing System", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling the information retrieval gap between non-technical database\nend-users and those with the knowledge of formal query languages has been an\ninteresting area of data management and analytics research. The use of natural\nlanguage interfaces to query information from databases offers the opportunity\nto bridge the communication challenges between end-users and systems that use\nformal query languages. Previous research efforts mainly focused on developing\nstructured query interfaces to relational databases. However, the evolution of\nunstructured big data such as text, images, and video has exposed the\nlimitations of traditional structured query interfaces. While the existing web\nsearch tools prove the popularity and usability of natural language query, they\nreturn complete documents and web pages instead of focused query responses and\nare not applicable to database systems. This paper reports our study on the\ndesign and development of a natural language query interface to a backend\nrelational database. The novelty in the study lies in defining a graph database\nas a middle layer to store necessary metadata needed to transform a natural\nlanguage query into structured query language that can be executed on backend\ndatabases. We implemented and evaluated our approach using a restaurant\ndataset. The translation results for some sample queries yielded a 90% accuracy\nrate.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 19:52:20 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Montgomery", "Chantal", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2009.12417", "submitter": "Mohammad Javad Shayegan", "authors": "Mohammad Javad Shayegan and Maasoumeh Kouhzadi", "title": "An Analysis of the Impact of SEO on University Website Ranking", "comments": "A Persian version of this paper has been published", "journal-ref": "Seyfabad, M. K., & Fard, M. J. S. (2019). An Analysis of the\n  Impact of SEO on University Website Ranking. Iranian Journal of Information\n  Processing and Management, 34(4), 1787-1810", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, ranking systems in universities have been considered by the academic\ncommunity, and there is a tight competition between world universities to\nachieve higher ranks. In the meantime, the ranking of university websites is\nalso in the spotlight, and the Webometric research center announces the ranks\nof university websites twice a year. Examining university rankings indicators\nand the Webometric ranks of the university indicates that some of these\nindicators, directly and indirectly, affect each other. On the other hand, a\npreliminary study of Webometric indicators shows that some Search Engine\nOptimization (SEO) indicators can affect Webometric ranks. The purpose of this\nresearch is to show how far the SEO metrics can affect the website rank of the\nuniversity. To do this, after extracting 38 points of the significant SEO\nmetrics of the extracted universities using various tools, data analysis was\nconducted along with applying association rules on the data. The results of the\nresearch show that some of the SEO metrics, such as the number of backlinks,\nAlexa Rank, and Page Rank have a direct and significant impact on the website\nrank of universities, and in this regard, interesting rules have been\nextracted.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 20:03:00 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:46:39 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Shayegan", "Mohammad Javad", ""], ["Kouhzadi", "Maasoumeh", ""]]}, {"id": "2009.12468", "submitter": "Hoda Eldardiry", "authors": "Eslam Hussein and Hoda Eldardiry", "title": "Investigating Misinformation in Online Marketplaces: An Audit Study on\n  Amazon", "comments": "8 pages, 9 figures, submitted to ASONAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search and recommendation systems are ubiquitous and irreplaceable tools in\nour daily lives. Despite their critical role in selecting and ranking the most\nrelevant information, they typically do not consider the veracity of\ninformation presented to the user. In this paper, we introduce an audit\nmethodology to investigate the extent of misinformation presented in search\nresults and recommendations on online marketplaces. We investigate the factors\nand personalization attributes that influence the amount of misinformation in\nsearches and recommendations. Recently, several media reports criticized Amazon\nfor hosting and recommending items that promote misinformation on topics such\nas vaccines. Motivated by those reports, we apply our algorithmic auditing\nmethodology on Amazon to verify those claims. Our audit study investigates (a)\nfactors that might influence the search algorithms of Amazon and (b)\npersonalization attributes that contribute to amplifying the amount of\nmisinformation recommended to users in their search results and\nrecommendations. Our audit study collected ~526k search results and ~182k\nhomepage recommendations, with ~8.5k unique items. Each item is annotated for\nits stance on vaccines' misinformation (pro, neutral, or anti). Our study\nreveals that (1) the selection and ranking by the default Featured search\nalgorithm of search results that have misinformation stances are positively\ncorrelated with the stance of search queries and customers' evaluation of items\n(ratings and reviews), (2) misinformation stances of search results are neither\naffected by users' activities nor by interacting (browsing, wish-listing,\nshopping) with items that have a misinformation stance, and (3) a filter bubble\nbuilt-in users' homepages have a misinformation stance positively correlated\nwith the misinformation stance of items that a user interacts with.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 22:48:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Hussein", "Eslam", ""], ["Eldardiry", "Hoda", ""]]}, {"id": "2009.12496", "submitter": "Qiang Liu", "authors": "Qiang Liu", "title": "Modeling Dyadic Conversations for Personality Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, automatical personality inference is drawing extensive attention\nfrom both academia and industry. Conventional methods are mainly based on user\ngenerated contents, e.g., profiles, likes, and texts of an individual, on\nsocial media, which are actually not very reliable. In contrast, dyadic\nconversations between individuals can not only capture how one expresses\noneself, but also reflect how one reacts to different situations. Rich\ncontextual information in dyadic conversation can explain an individual's\nresponse during his or her conversation. In this paper, we propose a novel\naugmented Gated Recurrent Unit (GRU) model for learning unsupervised Personal\nConversational Embeddings (PCE) based on dyadic conversations between\nindividuals. We adjust the formulation of each layer of a conventional GRU with\nsequence to sequence learning and personal information of both sides of the\nconversation. Based on the learned PCE, we can infer the personality of each\nindividual. We conduct experiments on the Movie Script dataset, which is\ncollected from conversations between characters in movie scripts. We find that\nmodeling dyadic conversations between individuals can significantly improve\npersonality inference accuracy. Experimental results illustrate the successful\nperformance of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 01:25:42 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Qiang", ""]]}, {"id": "2009.12765", "submitter": "Damai Dai", "authors": "Damai Dai, Hua Zheng, Fuli Luo, Pengcheng Yang, Baobao Chang, Zhifang\n  Sui", "title": "Inductively Representing Out-of-Knowledge-Graph Entities by Optimal\n  Estimation Under Translational Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Knowledge Graph Completion (KGC) assumes that all test entities\nappear during training. However, in real-world scenarios, Knowledge Graphs (KG)\nevolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and\nwe need to represent these entities efficiently. Most existing Knowledge Graph\nEmbedding (KGE) methods cannot represent OOKG entities without costly\nretraining on the whole KG. To enhance efficiency, we propose a simple and\neffective method that inductively represents OOKG entities by their optimal\nestimation under translational assumptions. Given pretrained embeddings of the\nin-knowledge-graph (IKG) entities, our method needs no additional learning.\nExperimental results show that our method outperforms the state-of-the-art\nmethods with higher efficiency on two KGC tasks with OOKG entities.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 07:12:18 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Dai", "Damai", ""], ["Zheng", "Hua", ""], ["Luo", "Fuli", ""], ["Yang", "Pengcheng", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "2009.12914", "submitter": "Yingjie Hu", "authors": "Yingjie Hu and Jimin Wang", "title": "How do people describe locations during a natural disaster: an analysis\n  of tweets from Hurricane Harvey", "comments": "11th International Conference on Geographic Information Science\n  (GIScience 2021)", "journal-ref": null, "doi": "10.4230/LIPIcs.GIScience.2021.I.6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms, such as Twitter, have been increasingly used by\npeople during natural disasters to share information and request for help.\nHurricane Harvey was a category 4 hurricane that devastated Houston, Texas, USA\nin August 2017 and caused catastrophic flooding in the Houston metropolitan\narea. Hurricane Harvey also witnessed the widespread use of social media by the\ngeneral public in response to this major disaster, and geographic locations are\nkey information pieces described in many of the social media messages. A\ngeoparsing system, or a geoparser, can be utilized to automatically extract and\nlocate the described locations, which can help first responders reach the\npeople in need. While a number of geoparsers have already been developed, it is\nunclear how effective they are in recognizing and geo-locating the locations\ndescribed by people during natural disasters. To fill this gap, this work seeks\nto understand how people describe locations during a natural disaster by\nanalyzing a sample of tweets posted during Hurricane Harvey. We then identify\nthe limitations of existing geoparsers in processing these tweets, and discuss\npossible approaches to overcoming these limitations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 18:17:09 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Hu", "Yingjie", ""], ["Wang", "Jimin", ""]]}, {"id": "2009.12969", "submitter": "Yifang Liu", "authors": "Yifang Liu, Zhentao Xu, Qiyuan An, Yang Yi, Yanzhi Wang, Trevor Hastie", "title": "Simultaneous Relevance and Diversity: A New Recommendation Inference\n  Approach", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance and diversity are both important to the success of recommender\nsystems, as they help users to discover from a large pool of items a compact\nset of candidates that are not only interesting but exploratory as well. The\nchallenge is that relevance and diversity usually act as two competing\nobjectives in conventional recommender systems, which necessities the classic\ntrade-off between exploitation and exploration. Traditionally, higher diversity\noften means sacrifice on relevance and vice versa. We propose a new approach,\nheterogeneous inference, which extends the general collaborative filtering (CF)\nby introducing a new way of CF inference, negative-to-positive. Heterogeneous\ninference achieves divergent relevance, where relevance and diversity support\neach other as two collaborating objectives in one recommendation model, and\nwhere recommendation diversity is an inherent outcome of the relevance\ninference process. Benefiting from its succinctness and flexibility, our\napproach is applicable to a wide range of recommendation scenarios/use-cases at\nvarious sophistication levels. Our analysis and experiments on public datasets\nand real-world production data show that our approach outperforms existing\nmethods on relevance and diversity simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:20:12 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Yifang", ""], ["Xu", "Zhentao", ""], ["An", "Qiyuan", ""], ["Yi", "Yang", ""], ["Wang", "Yanzhi", ""], ["Hastie", "Trevor", ""]]}, {"id": "2009.13059", "submitter": "Shashwat Aggarwal", "authors": "Shashwat Aggarwal, Ramesh Singh", "title": "Visual Exploration and Knowledge Discovery from Biomedical Dark Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization techniques proffer efficient means to organize and present\ndata in graphically appealing formats, which not only speeds up the process of\ndecision making and pattern recognition but also enables decision-makers to\nfully understand data insights and make informed decisions. Over time, with the\nrise in technological and computational resources, there has been an\nexponential increase in the world's scientific knowledge. However, most of it\nlacks structure and cannot be easily categorized and imported into regular\ndatabases. This type of data is often termed as Dark Data. Data visualization\ntechniques provide a promising solution to explore such data by allowing quick\ncomprehension of information, the discovery of emerging trends, identification\nof relationships and patterns, etc. In this empirical research study, we use\nthe rich corpus of PubMed comprising of more than 30 million citations from\nbiomedical literature to visually explore and understand the underlying\nkey-insights using various information visualization techniques. We employ a\nnatural language processing based pipeline to discover knowledge out of the\nbiomedical dark data. The pipeline comprises of different lexical analysis\ntechniques like Topic Modeling to extract inherent topics and major focus\nareas, Network Graphs to study the relationships between various entities like\nscientific documents and journals, researchers, and, keywords and terms, etc.\nWith this analytical research, we aim to proffer a potential solution to\novercome the problem of analyzing overwhelming amounts of information and\ndiminish the limitation of human cognition and perception in handling and\nexamining such large volumes of data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 04:27:05 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Aggarwal", "Shashwat", ""], ["Singh", "Ramesh", ""]]}, {"id": "2009.13167", "submitter": "Qian Li", "authors": "Qian Li, Nan Guo, Xiaochun Ye, Dongrui Fan, and Zhimin Tang", "title": "Video Face Recognition System: RetinaFace-mnet-faster and Secondary\n  Search", "comments": "Accepted by FICC(Future of Information and Communication Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is widely used in the scene. However, different visual\nenvironments require different methods, and face recognition has a difficulty\nin complex environments. Therefore, this paper mainly experiments complex faces\nin the video. First, we design an image pre-processing module for fuzzy scene\nor under-exposed faces to enhance images. Our experimental results demonstrate\nthat effective images pre-processing improves the accuracy of 0.11%, 0.2% and\n1.4% on LFW, WIDER FACE and our datasets, respectively. Second, we propose\nRetinacFace-mnet-faster for detection and a confidence threshold specification\nfor face recognition, reducing the lost rate. Our experimental results show\nthat our RetinaFace-mnet-faster for 640*480 resolution on the Tesla P40 and\nsingle-thread improve speed of 16.7% and 70.2%, respectively. Finally, we\ndesign secondary search mechanism with HNSW to improve performance. Ours is\nsuitable for large-scale datasets, and experimental results show that our\nmethod is 82% faster than the violent retrieval for the single-frame detection.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:31:38 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 01:47:49 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Li", "Qian", ""], ["Guo", "Nan", ""], ["Ye", "Xiaochun", ""], ["Fan", "Dongrui", ""], ["Tang", "Zhimin", ""]]}, {"id": "2009.13181", "submitter": "Camille-Sovanneary Gauthier", "authors": "Camille-Sovanneary Gauthier, Romaric Gaudel and Elisa Fromont", "title": "Position-Based Multiple-Play Bandits with Thompson Sampling", "comments": "Accepted at IDA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-play bandits aim at displaying relevant items at relevant positions\non a web page. We introduce a new bandit-based algorithm, PB-MHB, for online\nrecommender systems which uses the Thompson sampling framework. This algorithm\nhandles a display setting governed by the position-based model. Our sampling\nmethod does not require as input the probability of a user to look at a given\nposition in the web page which is, in practice, very difficult to obtain.\nExperiments on simulated and real datasets show that our method, with fewer\nprior information, deliver better recommendations than state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 09:50:53 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 10:06:13 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 16:28:21 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gauthier", "Camille-Sovanneary", ""], ["Gaudel", "Romaric", ""], ["Fromont", "Elisa", ""]]}, {"id": "2009.13249", "submitter": "Qianliang Wu", "authors": "Qianliang Wu and Tong Zhang and Zhen Cui and Jian Yang", "title": "Interest-Behaviour Multiplicative Network for Resource-limited\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource constraints, e.g. limited product inventory or financial strength,\nmay affect consumers' choices or preferences in some recommendation tasks but\nare usually ignored in previous recommendation methods. In this paper, we aim\nto mine the cue of user preferences in resource-limited recommendation tasks,\nfor which purpose we specifically build a large used car transaction dataset\npossessing resource-limitation characteristics. Accordingly, we propose an\ninterest-behavior multiplicative network to predict the user's future\ninteraction based on dynamic connections between users and items. To describe\nthe user-item connection dynamically, mutually-recursive recurrent neural\nnetworks (MRRNNs) are introduced to capture interactive long-term dependencies,\nand meantime effective representations of users and items are obtained. To\nfurther take the resource limitation into consideration, a resource-limited\nbranch is built to specifically explore the influence of resource variation on\nuser preferences. Finally, mutual information is introduced to measure the\nsimilarity between the user action and fused features to predict future\ninteraction, where the fused features come from both MRRNNs and\nresource-limited branches. We test the performance on the built used car\ntransaction dataset as well as the Tmall dataset, and the experimental results\nverify the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:11:13 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 12:14:35 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 11:59:18 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 03:08:00 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Wu", "Qianliang", ""], ["Zhang", "Tong", ""], ["Cui", "Zhen", ""], ["Yang", "Jian", ""]]}, {"id": "2009.13292", "submitter": "Itzik Malkiel", "authors": "Itzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin, Ori Katz and\n  Noam Koenigstein", "title": "RecoBERT: A Catalog Language Model for Text-Based Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models that utilize extensive self-supervised pre-training from\nunlabeled text, have recently shown to significantly advance the\nstate-of-the-art performance in a variety of language understanding tasks.\nHowever, it is yet unclear if and how these recent models can be harnessed for\nconducting text-based recommendations. In this work, we introduce RecoBERT, a\nBERT-based approach for learning catalog-specialized language models for\ntext-based item recommendations. We suggest novel training and inference\nprocedures for scoring similarities between pairs of items, that don't require\nitem similarity labels. Both the training and the inference techniques were\ndesigned to utilize the unlabeled structure of textual catalogs, and minimize\nthe discrepancy between them. By incorporating four scores during inference,\nRecoBERT can infer text-based item-to-item similarities more accurately than\nother techniques. In addition, we introduce a new language understanding task\nfor wine recommendations using similarities based on professional wine reviews.\nAs an additional contribution, we publish annotated recommendations dataset\ncrafted by human wine experts. Finally, we evaluate RecoBERT and compare it to\nvarious state-of-the-art NLP models on wine and fashion recommendations tasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:23:38 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Malkiel", "Itzik", ""], ["Barkan", "Oren", ""], ["Caciularu", "Avi", ""], ["Razin", "Noam", ""], ["Katz", "Ori", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2009.13294", "submitter": "Rohit Rawat", "authors": "Rohit Rawat", "title": "Virtual Proximity Citation (VCP): A Supervised Deep Learning Method to\n  Relate Uncited Papers On Grounds of Citation Proximity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Citation based approaches have seen good progress for recommending research\npapers using citations in the paper. Citation proximity analysis which uses the\nin-text citation proximity to find relatedness between two research papers is\nbetter than co-citation analysis and bibliographic analysis. However, one\ncommon problem which exists in each approach is that paper should be well\ncited. If documents are not cited properly or not cited at all, then using\nthese approaches will not be helpful. To overcome the problem, this paper\ndiscusses the approach Virtual Citation Proximity (VCP) which uses Siamese\nNeural Network along with the notion of citation proximity analysis and\ncontent-based filtering. To train this model, the actual distance between the\ntwo citations in a document is used as ground truth, this distance is the word\ncount between the two citations. VCP is trained on Wikipedia articles for which\nthe actual word count is available which is used to calculate the similarity\nbetween the documents. This can be used to calculate relatedness between two\ndocuments in a way they would have been cited in the proximity even if the\ndocuments are uncited. This approach has shown a great improvement in\npredicting proximity with basic neural networks over the approach which uses\nthe Average Citation Proximity index value as the ground truth. This can be\nimproved by using a complex neural network and proper hyper tuning of\nparameters.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 12:24:00 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Rawat", "Rohit", ""]]}, {"id": "2009.13299", "submitter": "Shuqing Bian", "authors": "Shuqing Bian, Xu Chen, Wayne Xin Zhao, Kun Zhou, Yupeng Hou, Yang\n  Song, Tao Zhang and Ji-Rong Wen", "title": "Learning to Match Jobs with Resumes from Sparse Interaction Data using\n  Multi-View Co-Teaching Network", "comments": null, "journal-ref": "CIKM 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing growth of online recruitment data, job-resume\nmatching has become an important task to automatically match jobs with suitable\nresumes. This task is typically casted as a supervised text matching problem.\nSupervised learning is powerful when the labeled data is sufficient. However,\non online recruitment platforms, job-resume interaction data is sparse and\nnoisy, which affects the performance of job-resume match algorithms. To\nalleviate these problems, in this paper, we propose a novel multi-view\nco-teaching network from sparse interaction data for job-resume matching. Our\nnetwork consists of two major components, namely text-based matching model and\nrelation-based matching model. The two parts capture semantic compatibility in\ntwo different views, and complement each other. In order to address the\nchallenges from sparse and noisy data, we design two specific strategies to\ncombine the two components. First, two components share the learned parameters\nor representations, so that the original representations of each component can\nbe enhanced. More importantly, we adopt a co-teaching mechanism to reduce the\ninfluence of noise in training data. The core idea is to let the two components\nhelp each other by selecting more reliable training instances. The two\nstrategies focus on representation enhancement and data enhancement,\nrespectively. Compared with pure text-based matching models, the proposed\napproach is able to learn better data representations from limited or even\nsparse interaction data, which is more resistible to noise in training data.\nExperiment results have demonstrated that our model is able to outperform\nstate-of-the-art methods for job-resume matching.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 03:09:54 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bian", "Shuqing", ""], ["Chen", "Xu", ""], ["Zhao", "Wayne Xin", ""], ["Zhou", "Kun", ""], ["Hou", "Yupeng", ""], ["Song", "Yang", ""], ["Zhang", "Tao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2009.13367", "submitter": "Inna Vogel", "authors": "Inna Vogel, Jeong-Eun Choi, Meghana Meghana", "title": "Similarity Detection Pipeline for Crawling a Topic Related Fake News\n  Corpus", "comments": "Further development done", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news detection is a challenging task aiming to reduce human time and\neffort to check the truthfulness of news. Automated approaches to combat fake\nnews, however, are limited by the lack of labeled benchmark datasets,\nespecially in languages other than English. Moreover, many publicly available\ncorpora have specific limitations that make them difficult to use. To address\nthis problem, our contribution is threefold. First, we propose a new, publicly\navailable German topic related corpus for fake news detection. To the best of\nour knowledge, this is the first corpus of its kind. In this regard, we\ndeveloped a pipeline for crawling similar news articles. As our third\ncontribution, we conduct different learning experiments to detect fake news.\nThe best performance was achieved using sentence level embeddings from SBERT in\ncombination with a Bi-LSTM (k=0.88).\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:35:31 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 12:32:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Vogel", "Inna", ""], ["Choi", "Jeong-Eun", ""], ["Meghana", "Meghana", ""]]}, {"id": "2009.13685", "submitter": "Yash Goyal", "authors": "Aayush Surana, Yash Goyal, Vinoo Alluri", "title": "Static and Dynamic Measures of Active Music Listening as Indicators of\n  Depression Risk", "comments": "Appearing in the proceedings of the Speech, Music and Mind Workshop\n  2020, a satellite workshop of INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music, an integral part of our lives, which is not only a source of\nentertainment but plays an important role in mental well-being by impacting\nmoods, emotions and other affective states. Music preferences and listening\nstrategies have been shown to be associated with the psychological well-being\nof listeners including internalized symptomatology and depression. However,\ntill date no studies exist that examine time-varying music consumption, in\nterms of acoustic content, and its association with users' well-being. In the\ncurrent study, we aim at unearthing static and dynamic patterns prevalent in\nactive listening behavior of individuals which may be used as indicators of\nrisk for depression. Mental well-being scores and listening histories of 541\nLast.fm users were examined. Static and dynamic acoustic and emotion-related\nfeatures were extracted from each user's listening history and correlated with\ntheir mental well-being scores. Results revealed that individuals with greater\ndepression risk resort to higher dependency on music with greater\nrepetitiveness in their listening activity. Furthermore, the affinity of\ndepressed individuals towards music that can be perceived as sad was found to\nbe resistant to change over time. This study has large implications for future\nwork in the area of assessing mental illness risk by exploiting digital\nfootprints of users via online music streaming platforms.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 23:29:53 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Surana", "Aayush", ""], ["Goyal", "Yash", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2009.13724", "submitter": "Fajie Yuan", "authors": "Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Joemon Jose, Beibei\n  Kong, Yudong Li", "title": "One Person, One Model, One World: Learning Continual User Representation\n  without Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning user representations is a vital technique toward effective user\nmodeling and personalized recommender systems. Existing approaches often derive\nan individual set of model parameters for each task by training on separate\ndata. However, the representation of the same user potentially has some\ncommonalities, such as preference and personality, even in different tasks. As\nsuch, these separately trained representations could be suboptimal in\nperformance as well as inefficient in terms of parameter sharing.\n  In this paper, we delve on research to continually learn user representations\ntask by task, whereby new tasks are learned while using partial parameters from\nold ones. A new problem arises since when new tasks are trained, previously\nlearned parameters are very likely to be modified, and as a result, an\nartificial neural network (ANN)-based model may lose its capacity to serve for\nwell-trained previous tasks forever, this issue is termed catastrophic\nforgetting. To address this issue, we present \\emph{Conure} the first\n\\underline{con}tinual, or lifelong, \\underline{u}ser \\underline{re}presentation\nlearner -- i.e., learning new tasks over time without forgetting old ones.\nSpecifically, we propose iteratively removing less important weights of old\ntasks in a deep user representation model, motivated by the fact that neural\nnetwork models are usually over-parameterized. In this way, we could learn many\ntasks with a single model by reusing the important weights, and modifying the\nless important weights to adapt to new tasks. We conduct extensive experiments\non two real-world datasets with nine tasks and show that \\emph{Conure} largely\nexceeds the standard model that does not purposely preserve such old\n\"knowledge\", and performs competitively or sometimes better than models which\nare trained either individually for each task or simultaneously by merging all\ntask data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 01:49:14 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:37:16 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 10:07:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yuan", "Fajie", ""], ["Zhang", "Guoxiao", ""], ["Karatzoglou", "Alexandros", ""], ["Jose", "Joemon", ""], ["Kong", "Beibei", ""], ["Li", "Yudong", ""]]}, {"id": "2009.13836", "submitter": "Abon Chaudhuri", "authors": "Theban Stanley, Nihar Vanjara, Yanxin Pan, Ekaterina Pirogova, Swagata\n  Chakraborty, Abon Chaudhuri", "title": "SIR: Similar Image Retrieval for Product Search in E-Commerce", "comments": "Accepted in 13th International Conference on Similarity Search and\n  Applications, SISAP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a similar image retrieval (SIR) platform that is used to quickly\ndiscover visually similar products in a catalog of millions. Given the size,\ndiversity, and dynamism of our catalog, product search poses many challenges.\nIt can be addressed by building supervised models to tagging product images\nwith labels representing themes and later retrieving them by labels. This\napproach suffices for common and perennial themes like \"white shirt\" or\n\"lifestyle image of TV\". It does not work for new themes such as\n\"e-cigarettes\", hard-to-define ones such as \"image with a promotional badge\",\nor the ones with short relevance span such as \"Halloween costumes\". SIR is\nideal for such cases because it allows us to search by an example, not a\npre-defined theme. We describe the steps - embedding computation, encoding, and\nindexing - that power the approximate nearest neighbor search back-end. We also\nhighlight two applications of SIR. The first one is related to the detection of\nproducts with various types of potentially objectionable themes. This\napplication is run with a sense of urgency, hence the typical time frame to\ntrain and bootstrap a model is not permitted. Also, these themes are often\nshort-lived based on current trends, hence spending resources to build a\nlasting model is not justified. The second application is a variant item\ndetection system where SIR helps discover visual variants that are hard to find\nthrough text search. We analyze the performance of SIR in the context of these\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 07:53:03 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Stanley", "Theban", ""], ["Vanjara", "Nihar", ""], ["Pan", "Yanxin", ""], ["Pirogova", "Ekaterina", ""], ["Chakraborty", "Swagata", ""], ["Chaudhuri", "Abon", ""]]}, {"id": "2009.13929", "submitter": "Shahabodin Khadivizand", "authors": "Shahabodin Khadivi Zand", "title": "Towards Intelligent Risk-based Customer Segmentation in Banking", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Processes, i.e., a set of coordinated tasks and activities to\nachieve a business goal, and their continuous improvements are key to the\noperation of any organization. In banking, business processes are increasingly\ndynamic as various technologies have made dynamic processes more prevalent. For\nexample, customer segmentation, i.e., the process of grouping related customers\nbased on common activities and behaviors, could be a data-driven and\nknowledge-intensive process. In this paper, we present an intelligent\ndata-driven pipeline composed of a set of processing elements to move\ncustomers' data from one system to another, transforming the data into the\ncontextualized data and knowledge along the way. The goal is to present a novel\nintelligent customer segmentation process which automates the feature\nengineering, i.e., the process of using (banking) domain knowledge to extract\nfeatures from raw data via data mining techniques, in the banking domain. We\nadopt a typical scenario for analyzing customer transaction records, to\nhighlight how the presented approach can significantly improve the quality of\nrisk-based customer segmentation in the absence of feature engineering.As\nresult, our proposed method is able to achieve accuracy of 91% compared to\nclassical approaches in terms of detecting, identifying and classifying\ntransaction to the right classification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 11:22:04 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Zand", "Shahabodin Khadivi", ""]]}, {"id": "2009.14045", "submitter": "Resul Tugay", "authors": "Bekir Berker T\\\"urker, Resul Tugay, \\c{S}ule \\\"O\\u{g}\\\"ud\\\"uc\\\"u,\n  \\.Ipek K{\\i}z{\\i}l", "title": "Hotel Recommendation System Based on User Profiles and Collaborative\n  Filtering", "comments": "in Turkish language, UBMK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, people start to use online reservation systems to plan their\nvacations since they have vast amount of choices available. Selecting when and\nwhere to go from this large-scale options is getting harder. In addition,\nsometimes consumers can miss the better options due to the wealth of\ninformation to be found on the online reservation systems. In this sense,\npersonalized services such as recommender systems play a crucial role in\ndecision making. Two traditional recommendation techniques are content-based\nand collaborative filtering. While both methods have their advantages, they\nalso have certain disadvantages, some of which can be solved by combining both\ntechniques to improve the quality of the recommendation. The resulting system\nis known as a hybrid recommender system. This paper presents a new hybrid hotel\nrecommendation system that has been developed by combining content-based and\ncollaborative filtering approaches that recommends customer the hotel they need\nand save them from time loss.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 09:57:54 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["T\u00fcrker", "Bekir Berker", ""], ["Tugay", "Resul", ""], ["\u00d6\u011f\u00fcd\u00fcc\u00fc", "\u015eule", ""], ["K\u0131z\u0131l", "\u0130pek", ""]]}, {"id": "2009.14261", "submitter": "Remesh Babu K R", "authors": "Dincy Davis, Reena Murali, Remesh Babu", "title": "Abusive Language Detection and Characterization of Twitter Behavior", "comments": "7 pages, 7 figures and 8 tables", "journal-ref": "International Journal of Computer Sciences and Engineering, Vol.8,\n  Issue.7, July 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, abusive language detection in online content is performed using\nBidirectional Recurrent Neural Network (BiRNN) method. Here the main objective\nis to focus on various forms of abusive behaviors on Twitter and to detect\nwhether a speech is abusive or not. The results are compared for various\nabusive behaviors in social media, with Convolutional Neural Netwrok (CNN) and\nRecurrent Neural Network (RNN) methods and proved that the proposed BiRNN is a\nbetter deep learning model for automatic abusive speech detection.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 07:38:11 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Davis", "Dincy", ""], ["Murali", "Reena", ""], ["Babu", "Remesh", ""]]}, {"id": "2009.14474", "submitter": "Qiang Dong", "authors": "Qiang Dong, Shuang-Shuang Xie, Xiaofan Yang, Yuan Yan Tang", "title": "User-item matching for recommendation fairness: a view from\n  item-providers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we all know, users and item-providers are two main groups of participants\nin recommender systems. The main task of this paper is to significantly improve\nthe coverage fairness (item-provider oriented objective), and simultaneously\nkeep the recommendation accuracy in a high level (user oriented objective).\nFirst, an effective and totally robust approach of improving the coverage\nfairness is proposed, that is to constrain the allowed recommendation times of\nan item to be proportional to the frequency of its being purchased in the past.\nSecond, in this constrained recommendation scenario, a serial of heuristic\nstrategies of user-item matching priority are proposed to minimize the loss of\nrecommendation accuracy. The parameterized strategy among them is validated to\nachieve better recommendation accuracy than the baseline algorithm in regular\nrecommendation scenario, and it has an overwhelming superiority in coverage\nfairness over the regular algorithm. Third, to get the optimal solution of this\nuser-item matching problem, we design a Minimum Cost Maximum Flow model, which\nachieves almost the same value of coverage fairness and even better accuracy\nperformance than the parameterized heuristic strategy. Finally, we empirically\ndemonstrate that, even compared with several state-of-the-art enhanced versions\nof the baseline algorithm, our framework of the constrained recommendation\nscenario coupled with the MCMF user-item matching priority strategy still has a\nseveral-to-one advantage in the coverage fairness, while its recommendation\nprecision is more than 90% of the best value of all the enhanced algorithms.\nWhat is more, our proposed framework is parameter-free and thus achieves this\nsuperior performance without the time cost of parameter optimization, while all\nthe above existing enhanced algorithms have to traverse their intrinsic\nparameter to get the best performance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 07:20:45 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Dong", "Qiang", ""], ["Xie", "Shuang-Shuang", ""], ["Yang", "Xiaofan", ""], ["Tang", "Yuan Yan", ""]]}, {"id": "2009.14578", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji, Erik Cambria and Pekka Marttinen", "title": "Dilated Convolutional Attention Network for Medical Code Assignment from\n  Clinical Text", "comments": "The 3rd Clinical Natural Language Processing Workshop at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical code assignment, which predicts medical codes from clinical texts, is\na fundamental task of intelligent medical information systems. The emergence of\ndeep models in natural language processing has boosted the development of\nautomatic assignment methods. However, recent advanced neural architectures\nwith flat convolutions or multi-channel feature concatenation ignore the\nsequential causal constraint within a text sequence and may not learn\nmeaningful clinical text representations, especially for lengthy clinical notes\nwith long-term sequential dependency. This paper proposes a Dilated\nConvolutional Attention Network (DCAN), integrating dilated convolutions,\nresidual connections, and label attention, for medical code assignment. It\nadopts dilated convolutions to capture complex medical patterns with a\nreceptive field which increases exponentially with dilation size. Experiments\non a real-world clinical dataset empirically show that our model improves the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 11:55:58 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Cambria", "Erik", ""], ["Marttinen", "Pekka", ""]]}]