[{"id": "1705.00105", "submitter": "Yury Maximov", "authors": "Sumit Sidana, Mikhail Trofimov, Oleg Horodnitskii, Charlotte Laclau,\n  Yury Maximov, Massih-Reza Amini", "title": "Representation Learning and Pairwise Ranking for Implicit Feedback in\n  Recommendation Systems", "comments": "12 pages, 4 figures, 5 tables, Modified version contains updated\n  results of all the models and rectifies some of the earlier issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel ranking framework for collaborative\nfiltering with the overall aim of learning user preferences over items by\nminimizing a pairwise ranking loss. We show the minimization problem involves\ndependent random variables and provide a theoretical analysis by proving the\nconsistency of the empirical risk minimization in the worst case where all\nusers choose a minimal number of positive and negative items. We further derive\na Neural-Network model that jointly learns a new representation of users and\nitems in an embedded space as well as the preference relation of users over the\npairs of items. The learning objective is based on three scenarios of ranking\nlosses that control the ability of the model to maintain the ordering over the\nitems induced from the users' preferences, as well as, the capacity of the\ndot-product defined in the learned embedded space to produce the ordering. The\nproposed model is by nature suitable for implicit feedback and involves the\nestimation of only very few parameters. Through extensive experiments on\nseveral real-world benchmarks on implicit data, we show the interest of\nlearning the preference and the embedding simultaneously when compared to\nlearning those separately. We also demonstrate that our approach is very\ncompetitive with the best state-of-the-art collaborative filtering techniques\nproposed for implicit feedback.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:03:40 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 09:47:02 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 14:35:37 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 09:31:35 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Sidana", "Sumit", ""], ["Trofimov", "Mikhail", ""], ["Horodnitskii", "Oleg", ""], ["Laclau", "Charlotte", ""], ["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1705.00217", "submitter": "Mikhail Khodak", "authors": "Mikhail Khodak, Andrej Risteski, Christiane Fellbaum, Sanjeev Arora", "title": "Extending and Improving Wordnet via Unsupervised Word Embeddings", "comments": "17 pages, 3 figures, In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an unsupervised approach for improving WordNet that builds\nupon recent advances in document and sense representation via distributional\nsemantics. We apply our methods to construct Wordnets in French and Russian,\nlanguages which both lack good manual constructions.1 These are evaluated on\ntwo new 600-word test sets for word-to-synset matching and found to improve\ngreatly upon synset recall, outperforming the best automated Wordnets in\nF-score. Our methods require very few linguistic resources, thus being\napplicable for Wordnet construction in low-resources languages, and may further\nbe applied to sense clustering and other Wordnet improvements.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 17:50:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Khodak", "Mikhail", ""], ["Risteski", "Andrej", ""], ["Fellbaum", "Christiane", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1705.00561", "submitter": "Richard Oentaryo", "authors": "Ferdian Thung, Richard J. Oentaryo, David Lo, Yuan Tian", "title": "WebAPIRec: Recommending Web APIs to Software Projects via Personalized\n  Ranking", "comments": "IEEE Transactions on Emerging Topics in Computational Intelligence,\n  2017", "journal-ref": "IEEE Transactions on Emerging Topics in Computational Intelligence\n  2017", "doi": "10.1109/TETCI.2017.2699222", "report-no": null, "categories": "cs.IR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application programming interfaces (APIs) offer a plethora of functionalities\nfor developers to reuse without reinventing the wheel. Identifying the\nappropriate APIs given a project requirement is critical for the success of a\nproject, as many functionalities can be reused to achieve faster development.\nHowever, the massive number of APIs would often hinder the developers' ability\nto quickly find the right APIs. In this light, we propose a new, automated\napproach called WebAPIRec that takes as input a project profile and outputs a\nranked list of {web} APIs that can be used to implement the project. At its\nheart, WebAPIRec employs a personalized ranking model that ranks web APIs\nspecific (personalized) to a project. Based on the historical data of {web} API\nusages, WebAPIRec learns a model that minimizes the incorrect ordering of web\nAPIs, i.e., when a used {web} API is ranked lower than an unused (or a\nnot-yet-used) web API. We have evaluated our approach on a dataset comprising\n9,883 web APIs and 4,315 web application projects from ProgrammableWeb with\npromising results. For 84.0% of the projects, WebAPIRec is able to successfully\nreturn correct APIs that are used to implement the projects in the top-5\npositions. This is substantially better than the recommendations provided by\nProgrammableWeb's native search functionality. WebAPIRec also outperforms\nMcMillan et al.'s application search engine and popularity-based\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 15:28:20 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Thung", "Ferdian", ""], ["Oentaryo", "Richard J.", ""], ["Lo", "David", ""], ["Tian", "Yuan", ""]]}, {"id": "1705.00578", "submitter": "Petr Knoth", "authors": "Petr Knoth, Lucas Anastasiou, Aristotelis Charalampous, Matteo\n  Cancellieri, Samuel Pearce, Nancy Pontika, Vaclav Bayer", "title": "Towards effective research recommender systems for repositories", "comments": "In proceedings of Open Repositories 2017, Brisbane, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we argue why and how the integration of recommender systems\nfor research can enhance the functionality and user experience in repositories.\nWe present the latest technical innovations in the CORE Recommender, which\nprovides research article recommendations across the global network of\nrepositories and journals. The CORE Recommender has been recently redeveloped\nand released into production in the CORE system and has also been deployed in\nseveral third-party repositories. We explain the design choices of this unique\nsystem and the evaluation processes we have in place to continue raising the\nquality of the provided recommendations. By drawing on our experience, we\ndiscuss the main challenges in offering a state-of-the-art recommender solution\nfor repositories. We highlight two of the key limitations of the current\nrepository infrastructure with respect to developing research recommender\nsystems: 1) the lack of a standardised protocol and capabilities for exposing\nanonymised user-interaction logs, which represent critically important input\ndata for recommender systems based on collaborative filtering and 2) the lack\nof a voluntary global sign-on capability in repositories, which would enable\nthe creation of personalised recommendation and notification solutions based on\npast user interactions.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:08:08 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Knoth", "Petr", ""], ["Anastasiou", "Lucas", ""], ["Charalampous", "Aristotelis", ""], ["Cancellieri", "Matteo", ""], ["Pearce", "Samuel", ""], ["Pontika", "Nancy", ""], ["Bayer", "Vaclav", ""]]}, {"id": "1705.00604", "submitter": "Joel Brogan Joel R Brogan", "authors": "Joel Brogan, Paolo Bestagini, Aparna Bharati, Allan Pinto, Daniel\n  Moreira, Kevin Bowyer, Patrick Flynn, Anderson Rocha, and Walter Scheirer", "title": "Spotting the Difference: Context Retrieval and Analysis for Improved\n  Forgery Detection and Localization", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As image tampering becomes ever more sophisticated and commonplace, the need\nfor image forensics algorithms that can accurately and quickly detect forgeries\ngrows. In this paper, we revisit the ideas of image querying and retrieval to\nprovide clues to better localize forgeries. We propose a method to perform\nlarge-scale image forensics on the order of one million images using the help\nof an image search algorithm and database to gather contextual clues as to\nwhere tampering may have taken place. In this vein, we introduce five new\nstrongly invariant image comparison methods and test their effectiveness under\nheavy noise, rotation, and color space changes. Lastly, we show the\neffectiveness of these methods compared to passive image forensics using Nimble\n[https://www.nist.gov/itl/iad/mig/nimble-challenge], a new, state-of-the-art\ndataset from the National Institute of Standards and Technology (NIST).\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:43:49 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Brogan", "Joel", ""], ["Bestagini", "Paolo", ""], ["Bharati", "Aparna", ""], ["Pinto", "Allan", ""], ["Moreira", "Daniel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter", ""]]}, {"id": "1705.00831", "submitter": "Brendan Avent", "authors": "Brendan Avent, Aleksandra Korolova, David Zeber, Torgeir Hovden,\n  Benjamin Livshits", "title": "BLENDER: Enabling Local Search with a Hybrid Differential Privacy Model", "comments": "Proceedings of the 26th USENIX Security Symposium (USENIX Security\n  17). August 16-18, 2017, Vancouver, BC. ISBN 978-1-931971-40-9", "journal-ref": "Journal of Privacy and Confidentiality, Vol. 9 (2) 2019", "doi": "10.29012/jpc.680", "report-no": null, "categories": "cs.CR cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid model of differential privacy that considers a\ncombination of regular and opt-in users who desire the differential privacy\nguarantees of the local privacy model and the trusted curator model,\nrespectively. We demonstrate that within this model, it is possible to design a\nnew type of blended algorithm for the task of privately computing the head of a\nsearch log. This blended approach provides significant improvements in the\nutility of obtained data compared to related work while providing users with\ntheir desired privacy guarantees. Specifically, on two large search click data\nsets, comprising 1.75 and 16 GB respectively, our approach attains NDCG values\nexceeding 95% across a range of privacy budget values.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:36:35 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 17:28:55 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 10:42:39 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 23:11:29 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Avent", "Brendan", ""], ["Korolova", "Aleksandra", ""], ["Zeber", "David", ""], ["Hovden", "Torgeir", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1705.00894", "submitter": "Svitlana Vakulenko", "authors": "Sebastian Neumaier, Vadim Savenkov and Svitlana Vakulenko", "title": "Talking Open Data", "comments": "Accepted at ESWC2017 demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enticing users into exploring Open Data remains an important challenge for\nthe whole Open Data paradigm. Standard stock interfaces often used by Open Data\nportals are anything but inspiring even for tech-savvy users, let alone those\nwithout an articulated interest in data science. To address a broader range of\ncitizens, we designed an open data search interface supporting natural language\ninteractions via popular platforms like Facebook and Skype. Our data-aware\nchatbot answers search requests and suggests relevant open datasets, bringing\nfun factor and a potential of viral dissemination into Open Data exploration.\nThe current system prototype is available for Facebook\n(https://m.me/OpenDataAssistant) and Skype\n(https://join.skype.com/bot/6db830ca-b365-44c4-9f4d-d423f728e741) users.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:35:12 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Neumaier", "Sebastian", ""], ["Savenkov", "Vadim", ""], ["Vakulenko", "Svitlana", ""]]}, {"id": "1705.00947", "submitter": "Guilherme Ramos", "authors": "Jo\\~ao Sa\\'ude, Guilherme Ramos, Carlos Caleiro, Soummya Kar", "title": "Robust reputation-based ranking on multipartite rating networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of online reviews, ratings and opinions and its growing influence\non people's behavior and decisions boosted the interest to extract meaningful\ninformation from this data deluge. Hence, crowdsourced ratings of products and\nservices gained a critical role in business, governments, and others. We\npropose a new reputation-based ranking system utilizing multipartite rating\nsubnetworks, that clusters users by their similarities, using Kolmogorov\ncomplexity. Our system is novel in that it reflects a diversity of\nopinions/preferences by assigning possibly distinct rankings, for the same\nitem, for different groups of users. We prove the convergence and efficiency of\nthe system and show that it copes better with spamming/spurious users, and it\nis more robust to attacks than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:05:45 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Sa\u00fade", "Jo\u00e3o", ""], ["Ramos", "Guilherme", ""], ["Caleiro", "Carlos", ""], ["Kar", "Soummya", ""]]}, {"id": "1705.00995", "submitter": "Amir Karami", "authors": "Amir Karami and Aryya Gangopadhyay and Bin Zhou and Hadi Kharrazi", "title": "Fuzzy Approach Topic Discovery in Health and Medical Corpora", "comments": "12 Pages, International Journal of Fuzzy Systems, 2017", "journal-ref": null, "doi": "10.1007/s40815-017-0327-9", "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of medical documents and electronic health records (EHRs) are in\ntext format that poses a challenge for data processing and finding relevant\ndocuments. Looking for ways to automatically retrieve the enormous amount of\nhealth and medical knowledge has always been an intriguing topic. Powerful\nmethods have been developed in recent years to make the text processing\nautomatic. One of the popular approaches to retrieve information based on\ndiscovering the themes in health & medical corpora is topic modeling, however,\nthis approach still needs new perspectives. In this research we describe fuzzy\nlatent semantic analysis (FLSA), a novel approach in topic modeling using fuzzy\nperspective. FLSA can handle health & medical corpora redundancy issue and\nprovides a new method to estimate the number of topics. The quantitative\nevaluations show that FLSA produces superior performance and features to latent\nDirichlet allocation (LDA), the most popular topic model.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 14:29:14 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 02:41:00 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Karami", "Amir", ""], ["Gangopadhyay", "Aryya", ""], ["Zhou", "Bin", ""], ["Kharrazi", "Hadi", ""]]}, {"id": "1705.01410", "submitter": "A H M Forhadul Islam", "authors": "Mohamed Aboeleinen, A H M Forhadul Islam", "title": "Social Network Analysis of yahoo web-search engine query logs", "comments": "14 pages, 5 figures, project work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web is now the undisputed warehouse for information. It can now provide most\nof the answers for modern problems. Search engines do a great job by combining\nand ranking the best results when the users try to search for any particular\ninformation. However, as we know 'with great power comes great responsibility',\nit is not an easy task for data analysts to find the most relevant information\nfor the queries. One major challenge is that web search engines face\ndifficulties in recognizing users' specific search interests given his initial\nquery. In this project, we have tried to build query networks from web search\nengine query logs, with the nodes representing queries and the edges exhibiting\nthe semantic relatedness between Queries.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 11:29:49 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Aboeleinen", "Mohamed", ""], ["Islam", "A H M Forhadul", ""]]}, {"id": "1705.01509", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra and Nick Craswell", "title": "Neural Models for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ranking models for information retrieval (IR) use shallow or deep\nneural networks to rank search results in response to a query. Traditional\nlearning to rank models employ machine learning techniques over hand-crafted IR\nfeatures. By contrast, neural models learn representations of language from raw\ntext that can bridge the gap between query and document vocabulary. Unlike\nclassical IR models, these new machine learning based approaches are\ndata-hungry, requiring large scale training data before they can be deployed.\nThis tutorial introduces basic concepts and intuitions behind neural IR models,\nand places them in the context of traditional retrieval models. We begin by\nintroducing fundamental concepts of IR and different neural and non-neural\napproaches to learning vector representations of text. We then review shallow\nneural IR methods that employ pre-trained neural term embeddings without\nlearning the IR task end-to-end. We introduce deep neural networks next,\ndiscussing popular deep architectures. Finally, we review the current DNN\nmodels for information retrieval. We conclude with a discussion on potential\nfuture directions for neural IR.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 17:08:05 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1705.02009", "submitter": "Hien To", "authors": "Hien To, Sumeet Agrawal, Seon Ho Kim, Cyrus Shahabi", "title": "On Identifying Disaster-Related Tweets: Matching-based or\n  Learning-based?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media such as tweets are emerging as platforms contributing to\nsituational awareness during disasters. Information shared on Twitter by both\naffected population (e.g., requesting assistance, warning) and those outside\nthe impact zone (e.g., providing assistance) would help first responders,\ndecision makers, and the public to understand the situation first-hand.\nEffective use of such information requires timely selection and analysis of\ntweets that are relevant to a particular disaster. Even though abundant tweets\nare promising as a data source, it is challenging to automatically identify\nrelevant messages since tweet are short and unstructured, resulting to\nunsatisfactory classification performance of conventional learning-based\napproaches. Thus, we propose a simple yet effective algorithm to identify\nrelevant messages based on matching keywords and hashtags, and provide a\ncomparison between matching-based and learning-based approaches. To evaluate\nthe two approaches, we put them into a framework specifically proposed for\nanalyzing disaster-related tweets. Analysis results on eleven datasets with\nvarious disaster types show that our technique provides relevant tweets of\nhigher quality and more interpretable results of sentiment analysis tasks when\ncompared to learning approach.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 20:42:23 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["To", "Hien", ""], ["Agrawal", "Sumeet", ""], ["Kim", "Seon Ho", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1705.02085", "submitter": "ThaiBinh Nguyen", "authors": "ThaiBinh Nguyen, Atsuhiro Takasu", "title": "A Probabilistic Model for the Cold-Start Problem in Rating Prediction\n  using Click Data", "comments": "ICONIP 2017", "journal-ref": "In Proceedings of ICONIP 2017, pp 196-205", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most efficient methods in collaborative filtering is matrix\nfactorization, which finds the latent vector representations of users and items\nbased on the ratings of users to items. However, a matrix factorization based\nalgorithm suffers from the cold-start problem: it cannot find latent vectors\nfor items to which previous ratings are not available. This paper utilizes\nclick data, which can be collected in abundance, to address the cold-start\nproblem. We propose a probabilistic item embedding model that learns item\nrepresentations from click data, and a model named EMB-MF, that connects it\nwith a probabilistic matrix factorization for rating prediction. The\nexperiments on three real-world datasets demonstrate that the proposed model is\nnot only effective in recommending items with no previous ratings, but also\noutperforms competing methods, especially when the data is very sparse.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:14:49 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 03:48:27 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Nguyen", "ThaiBinh", ""], ["Takasu", "Atsuhiro", ""]]}, {"id": "1705.02146", "submitter": "Avikalp Srivastava", "authors": "Avikalp Srivastava, Madhav Datt, Jaikrishna Chaparala, Shubham Mangla,\n  Priyadarshi Patnaik", "title": "Social Media Advertisement Outreach: Learning the Role of Aesthetics", "comments": "Accepted to SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080759", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporations spend millions of dollars on developing creative image-based\npromotional content to advertise to their user-base on platforms like Twitter.\nOur paper is an initial study, where we propose a novel method to evaluate and\nimprove outreach of promotional images from corporations on Twitter, based\npurely on their describable aesthetic attributes. Existing works in aesthetic\nbased image analysis exclusively focus on the attributes of digital\nphotographs, and are not applicable to advertisements due to the influences of\ninherent content and context based biases on outreach.\n  Our paper identifies broad categories of biases affecting such images,\ndescribes a method for normalization to eliminate effects of those biases and\nscore images based on their outreach, and examines the effects of certain\nhandcrafted describable aesthetic features on image outreach. Optimizing on the\ndescribable aesthetic features resulting from this research is a simple method\nfor corporations to complement their existing marketing strategy to gain\nsignificant improvement in user engagement on social media for promotional\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 09:25:00 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 06:20:05 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Srivastava", "Avikalp", ""], ["Datt", "Madhav", ""], ["Chaparala", "Jaikrishna", ""], ["Mangla", "Shubham", ""], ["Patnaik", "Priyadarshi", ""]]}, {"id": "1705.02203", "submitter": "Tesfamariam Mulugeta Abuhay", "authors": "Tesfamariam M. Abuhay, Sergey V. Kovalchuk, Klavdiya O. Bochenina,\n  George Kampis, Valeria V. Krzhizhanovskaya, Michael H. Lees", "title": "Analysis of Computational Science Papers from ICCS 2001-2016 using Topic\n  Modeling and Graph Theory", "comments": "Accepted by International Conference on Computational Science (ICCS)\n  2017 which will be held in Zurich, Switzerland from June 11-June 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents results of topic modeling and network models of topics\nusing the International Conference on Computational Science corpus, which\ncontains domain-specific (computational science) papers over sixteen years (a\ntotal of 5695 papers). We discuss topical structures of International\nConference on Computational Science, how these topics evolve over time in\nresponse to the topicality of various problems, technologies and methods, and\nhow all these topics relate to one another. This analysis illustrates\nmultidisciplinary research and collaborations among scientific communities, by\nconstructing static and dynamic networks from the topic modeling results and\nthe keywords of authors. The results of this study give insights about the past\nand future trends of core discussion topics in computational science. We used\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\ntopics and labeled and grouped results hierarchically.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:24:41 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Abuhay", "Tesfamariam M.", ""], ["Kovalchuk", "Sergey V.", ""], ["Bochenina", "Klavdiya O.", ""], ["Kampis", "George", ""], ["Krzhizhanovskaya", "Valeria V.", ""], ["Lees", "Michael H.", ""]]}, {"id": "1705.02518", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Kashyap Popat, Gerhard Weikum", "title": "Exploring Latent Semantic Factors to Find Useful Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provided by consumers are a valuable asset for e-Commerce\nplatforms, influencing potential consumers in making purchasing decisions.\nHowever, these reviews are of varying quality, with the useful ones buried deep\nwithin a heap of non-informative reviews. In this work, we attempt to\nautomatically identify review quality in terms of its helpfulness to the end\nconsumers. In contrast to previous works in this domain exploiting a variety of\nsyntactic and community-level features, we delve deep into the semantics of\nreviews as to what makes them useful, providing interpretable explanation for\nthe same. We identify a set of consistency and semantic factors, all from the\ntext, ratings, and timestamps of user-generated reviews, making our approach\ngeneralizable across all communities and domains. We explore review semantics\nin terms of several latent factors like the expertise of its author, his\njudgment about the fine-grained facets of the underlying product, and his\nwriting style. These are cast into a Hidden Markov Model -- Latent Dirichlet\nAllocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii)\nitem facets, and (iii) review helpfulness. Large-scale experiments on five\nreal-world datasets from Amazon show significant improvement over\nstate-of-the-art baselines in predicting and ranking useful reviews.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:21:48 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Popat", "Kashyap", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02519", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Hemank Lamba, Gerhard Weikum", "title": "Item Recommendation with Evolving User Preferences and Experience", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2015.111", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current recommender systems exploit user and item similarities by\ncollaborative filtering. Some advanced methods also consider the temporal\nevolution of item ratings as a global background process. However, all prior\nmethods disregard the individual evolution of a user's experience level and how\nthis is expressed in the user's writing in a review community. In this paper,\nwe model the joint evolution of user experience, interest in specific item\nfacets, writing style, and rating behavior. This way we can generate individual\nrecommendations that take into account the user's maturity level (e.g.,\nrecommending art movies rather than blockbusters for a cinematography expert).\nAs only item ratings and review texts are observables, we capture the user's\nexperience and interests in a latent model learned from her reviews, vocabulary\nand writing style. We develop a generative HMM-LDA model to trace user\nevolution, where the Hidden Markov Model (HMM) traces her latent experience\nprogressing over time -- with solely user reviews and ratings as observables\nover time. The facets of a user's interest are drawn from a Latent Dirichlet\nAllocation (LDA) model derived from her reviews, as a function of her (again\nlatent) experience level. In experiments with five real-world datasets, we show\nthat our model improves the rating prediction over state-of-the-art baselines,\nby a substantial margin. We also show, in a use-case study, that our model\nperforms well in the assessment of user experience levels.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:22:41 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Lamba", "Hemank", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02522", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum, Cristian Danescu-Niculescu-Mizil", "title": "People on Drugs: Credibility of User Statements in Health Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online health communities are a valuable source of information for patients\nand physicians. However, such user-generated resources are often plagued by\ninaccuracies and misinformation. In this work we propose a method for\nautomatically establishing the credibility of user-generated medical statements\nand the trustworthiness of their authors by exploiting linguistic cues and\ndistant supervision from expert sources. To this end we introduce a\nprobabilistic graphical model that jointly learns user trustworthiness,\nstatement credibility, and language objectivity. We apply this methodology to\nthe task of extracting rare or unknown side-effects of medical drugs --- this\nbeing one of the problems where large scale non-expert data has the potential\nto complement expert medical knowledge. We show that our method can reliably\nextract side-effects and filter out false statements, while identifying\ntrustworthy users that are likely to contribute valuable medical information.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:38:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1705.02667", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum", "title": "People on Media: Jointly Identifying Credible News and Trustworthy\n  Citizen Journalists in Online Communities", "comments": null, "journal-ref": null, "doi": "10.1145/2806416.2806537", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media seems to have become more partisan, often providing a biased coverage\nof news catering to the interest of specific groups. It is therefore essential\nto identify credible information content that provides an objective narrative\nof an event. News communities such as digg, reddit, or newstrust offer\nrecommendations, reviews, quality ratings, and further insights on journalistic\nworks. However, there is a complex interaction between different factors in\nsuch online communities: fairness and style of reporting, language clarity and\nobjectivity, topical perspectives (like political viewpoint), expertise and\nbias of community members, and more. This paper presents a model to\nsystematically analyze the different interactions in a news community between\nusers, news, and sources. We develop a probabilistic graphical model that\nleverages this joint interaction to identify 1) highly credible news articles,\n2) trustworthy news sources, and 3) expert users who perform the role of\n\"citizen journalists\" in the community. Our method extends CRF models to\nincorporate real-valued ratings, as some communities have very fine-grained\nscales that cannot be easily discretized without losing information. To the\nbest of our knowledge, this paper is the first full-fledged analysis of\ncredibility, trust, and expertise in news communities.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:41:31 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:40:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02668", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Sourav Dutta, Gerhard Weikum", "title": "Credible Review Detection with Limited Information using Consistency\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provide viewpoints on the strengths and shortcomings of\nproducts/services, influencing potential customers' purchasing decisions.\nHowever, the proliferation of non-credible reviews -- either fake (promoting/\ndemoting an item), incompetent (involving irrelevant aspects), or biased --\nentails the problem of identifying credible reviews. Prior works involve\nclassifiers harnessing rich information about items/users -- which might not be\nreadily available in several domains -- that provide only limited\ninterpretability as to why a review is deemed non-credible. This paper presents\na novel approach to address the above issues. We utilize latent topic models\nleveraging review texts, item ratings, and timestamps to derive consistency\nfeatures without relying on item/user histories, unavailable for \"long-tail\"\nitems/users. We develop models, for computing review credibility scores to\nprovide interpretable evidence for non-credible reviews, that are also\ntransferable to other domains -- addressing the scarcity of labeled data.\nExperiments on real-world datasets demonstrate improvements over\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:43:01 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Dutta", "Sourav", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02669", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Stephan Guennemann, Gerhard Weikum", "title": "Item Recommendation with Continuous Experience Evolution of Users using\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939780", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online review communities are dynamic as users join and leave, adopt new\nvocabulary, and adapt to evolving trends. Recent work has shown that\nrecommender systems benefit from explicit consideration of user experience.\nHowever, prior work assumes a fixed number of discrete experience levels,\nwhereas in reality users gain experience and mature continuously over time.\nThis paper presents a new model that captures the continuous evolution of user\nexperience, and the resulting language model in reviews and other posts. Our\nmodel is unsupervised and combines principles of Geometric Brownian Motion,\nBrownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal\nprogression of user experience and language model respectively. We develop\npractical algorithms for estimating the model parameters from data and for\ninference with our model (e.g., to recommend items). Extensive experiments with\nfive real-world datasets show that our model not only fits data better than\ndiscrete-model baselines, but also outperforms state-of-the-art methods for\npredicting item ratings.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:46:43 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 06:47:51 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 17:56:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Guennemann", "Stephan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.03067", "submitter": "Daniele Ramazzotti", "authors": "Lucrezia Patruno, Edoardo Galimberti, Daniele Ramazzotti, Giulio\n  Caravagna, Luca De Sano, Marco Antoniotti, and Alex Graudenzi", "title": "cyTRON and cyTRON/JS: two Cytoscape-based applications for the inference\n  of cancer evolution models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of sequencing data of cancer samples is fueling\nthe development of algorithmic strategies to investigate tumor heterogeneity\nand infer reliable models of cancer evolution. We here build up on previous\nworks on cancer progression inference from genomic alteration data, to deliver\ntwo distinct Cytoscape-based applications, which allow to produce, visualize\nand manipulate cancer evolution models, also by interacting with public genomic\nand proteomics databases. In particular, we here introduce cyTRON, a\nstand-alone Cytoscape app, and cyTRON/JS, a web application which employs the\nfunctionalities of Cytoscape/JS.\n  cyTRON was developed in Java; the code is available at\nhttps://github.com/BIMIB-DISCo/cyTRON and on the Cytoscape App Store\nhttp://apps.cytoscape.org/apps/cytron. cyTRON/JS was developed in JavaScript\nand R; the source code of the tool is available at\nhttps://github.com/BIMIB-DISCo/cyTRON-js and the tool is accessible from\nhttps://bimib.disco.unimib.it/cytronjs/welcome.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:02:52 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 10:20:52 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Patruno", "Lucrezia", ""], ["Galimberti", "Edoardo", ""], ["Ramazzotti", "Daniele", ""], ["Caravagna", "Giulio", ""], ["De Sano", "Luca", ""], ["Antoniotti", "Marco", ""], ["Graudenzi", "Alex", ""]]}, {"id": "1705.03172", "submitter": "Jialong Han", "authors": "Xin Zheng, Jialong Han, Aixin Sun", "title": "A Survey of Location Prediction on Twitter", "comments": "Accepted to TKDE. 30 pages, 1 figure", "journal-ref": null, "doi": "10.1109/TKDE.2018.2807840", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locations, e.g., countries, states, cities, and point-of-interests, are\ncentral to news, emergency events, and people's daily lives. Automatic\nidentification of locations associated with or mentioned in documents has been\nexplored for decades. As one of the most popular online social network\nplatforms, Twitter has attracted a large number of users who send millions of\ntweets on daily basis. Due to the world-wide coverage of its users and\nreal-time freshness of tweets, location prediction on Twitter has gained\nsignificant attention in recent years. Research efforts are spent on dealing\nwith new challenges and opportunities brought by the noisy, short, and\ncontext-rich nature of tweets. In this survey, we aim at offering an overall\npicture of location prediction on Twitter. Specifically, we concentrate on the\nprediction of user home locations, tweet locations, and mentioned locations. We\nfirst define the three tasks and review the evaluation metrics. By summarizing\nTwitter network, tweet content, and tweet context as potential inputs, we then\nstructurally highlight how the problems depend on these inputs. Each dependency\nis illustrated by a comprehensive review of the corresponding strategies\nadopted in state-of-the-art approaches. In addition, we also briefly review two\nrelated problems, i.e., semantic location prediction and point-of-interest\nrecommendation. Finally, we list future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 04:14:57 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 08:29:45 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zheng", "Xin", ""], ["Han", "Jialong", ""], ["Sun", "Aixin", ""]]}, {"id": "1705.03214", "submitter": "Juergen Mueller", "authors": "Juergen Mueller and Gerd Stumme", "title": "Predicting Rising Follower Counts on Twitter Using Profile Information", "comments": "10 pages, 3 figures, 8 tables, WebSci '17, June 25--28, 2017, Troy,\n  NY, USA", "journal-ref": null, "doi": "10.1145/3091478.3091490", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When evaluating the cause of one's popularity on Twitter, one thing is\nconsidered to be the main driver: Many tweets. There is debate about the kind\nof tweet one should publish, but little beyond tweets. Of particular interest\nis the information provided by each Twitter user's profile page. One of the\nfeatures are the given names on those profiles. Studies on psychology and\neconomics identified correlations of the first name to, e.g., one's school\nmarks or chances of getting a job interview in the US. Therefore, we are\ninterested in the influence of those profile information on the follower count.\nWe addressed this question by analyzing the profiles of about 6 Million Twitter\nusers. All profiles are separated into three groups: Users that have a first\nname, English words, or neither of both in their name field. The assumption is\nthat names and words influence the discoverability of a user and subsequently\nhis/her follower count. We propose a classifier that labels users who will\nincrease their follower count within a month by applying different models based\non the user's group. The classifiers are evaluated with the area under the\nreceiver operator curve score and achieves a score above 0.800.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 07:30:19 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Mueller", "Juergen", ""], ["Stumme", "Gerd", ""]]}, {"id": "1705.03264", "submitter": "Abhik Jana", "authors": "Abhik Jana, Sruthi Mooriyath, Animesh Mukherjee, Pawan Goyal", "title": "WikiM: Metapaths based Wikification of Scientific Abstracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to disseminate the exponential extent of knowledge being produced in\nthe form of scientific publications, it would be best to design mechanisms that\nconnect it with already existing rich repository of concepts -- the Wikipedia.\nNot only does it make scientific reading simple and easy (by connecting the\ninvolved concepts used in the scientific articles to their Wikipedia\nexplanations) but also improves the overall quality of the article. In this\npaper, we present a novel metapath based method, WikiM, to efficiently wikify\nscientific abstracts -- a topic that has been rarely investigated in the\nliterature. One of the prime motivations for this work comes from the\nobservation that, wikified abstracts of scientific documents help a reader to\ndecide better, in comparison to the plain abstracts, whether (s)he would be\ninterested to read the full article. We perform mention extraction mostly\nthrough traditional tf-idf measures coupled with a set of smart filters. The\nentity linking heavily leverages on the rich citation and author publication\nnetworks. Our observation is that various metapaths defined over these networks\ncan significantly enhance the overall performance of the system. For mention\nextraction and entity linking, we outperform most of the competing\nstate-of-the-art techniques by a large margin arriving at precision values of\n72.42% and 73.8% respectively over a dataset from the ACL Anthology Network. In\norder to establish the robustness of our scheme, we wikify three other datasets\nand get precision values of 63.41%-94.03% and 67.67%-73.29% respectively for\nthe mention extraction and the entity linking phase.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:35:15 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Jana", "Abhik", ""], ["Mooriyath", "Sruthi", ""], ["Mukherjee", "Animesh", ""], ["Goyal", "Pawan", ""]]}, {"id": "1705.03556", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, W. Bruce Croft", "title": "Relevance-based Word Embedding", "comments": "to appear in the proceedings of The 40th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 22:11:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1705.04009", "submitter": "Barun Patra", "authors": "Barun Patra", "title": "A survey of Community Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of numerous community forums, tasks associated with the same\nhave gained importance in the recent past. With the influx of new questions\nevery day on these forums, the issues of identifying methods to find answers to\nsaid questions, or even trying to detect duplicate questions, are of practical\nimportance and are challenging in their own right. This paper aims at surveying\nsome of the aforementioned issues, and methods proposed for tackling the same.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 04:22:58 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Patra", "Barun", ""]]}, {"id": "1705.04286", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Yibo Zhang, Harun Gunaydin, Da Teng, Aydogan Ozcan", "title": "Phase recovery and holographic image reconstruction using deep learning\n  in neural networks", "comments": null, "journal-ref": "Light: Science and Applications, 7, e17141 (2018)", "doi": "10.1038/lsa.2017.141", "report-no": null, "categories": "cs.CV cs.IR cs.LG physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase recovery from intensity-only measurements forms the heart of coherent\nimaging techniques and holography. Here we demonstrate that a neural network\ncan learn to perform phase recovery and holographic image reconstruction after\nappropriate training. This deep learning-based approach provides an entirely\nnew framework to conduct holographic imaging by rapidly eliminating twin-image\nand self-interference related spatial artifacts. Compared to existing\napproaches, this neural network based method is significantly faster to\ncompute, and reconstructs improved phase and amplitude images of the objects\nusing only one hologram, i.e., requires less number of measurements in addition\nto being computationally faster. We validated this method by reconstructing\nphase and amplitude images of various samples, including blood and Pap smears,\nand tissue sections. These results are broadly applicable to any phase recovery\nproblem, and highlight that through machine learning challenging problems in\nimaging science can be overcome, providing new avenues to design powerful\ncomputational imaging systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:26:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Rivenson", "Yair", ""], ["Zhang", "Yibo", ""], ["Gunaydin", "Harun", ""], ["Teng", "Da", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1705.04803", "submitter": "Bhaskar Mitra", "authors": "Federico Nanni, Bhaskar Mitra, Matt Magnusson and Laura Dietz", "title": "Benchmark for Complex Answer Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving paragraphs to populate a Wikipedia article is a challenging task.\nThe new TREC Complex Answer Retrieval (TREC CAR) track introduces a\ncomprehensive dataset that targets this retrieval scenario. We present early\nresults from a variety of approaches -- from standard information retrieval\nmethods (e.g., tf-idf) to complex systems that using query expansion using\nknowledge bases and deep neural networks. The goal is to offer future\nparticipants of this track an overview of some promising approaches to tackle\nthis problem.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 09:06:52 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Nanni", "Federico", ""], ["Mitra", "Bhaskar", ""], ["Magnusson", "Matt", ""], ["Dietz", "Laura", ""]]}, {"id": "1705.04892", "submitter": "Jimmy Lin", "authors": "Jinfeng Rao, Ferhan Ture, Hua He, Oliver Jojic, and Jimmy Lin", "title": "Talking to Your TV: Context-Aware Voice Search with Hierarchical\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the novel problem of navigational voice queries posed against an\nentertainment system, where viewers interact with a voice-enabled remote\ncontroller to specify the program to watch. This is a difficult problem for\nseveral reasons: such queries are short, even shorter than comparable voice\nqueries in other domains, which offers fewer opportunities for deciphering user\nintent. Furthermore, ambiguity is exacerbated by underlying speech recognition\nerrors. We address these challenges by integrating word- and character-level\nrepresentations of the queries and by modeling voice search sessions to capture\nthe contextual dependencies in query sequences. Both are accomplished with a\nprobabilistic framework in which recurrent and feedforward neural network\nmodules are organized in a hierarchical manner. From a raw dataset of 32M voice\nqueries from 2.5M viewers on the Comcast Xfinity X1 entertainment system, we\nextracted data to train and test our models. We demonstrate the benefits of our\nhybrid representation and context-aware model, which significantly outperforms\nmodels without context as well as the current deployed product.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 22:24:26 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Rao", "Jinfeng", ""], ["Ture", "Ferhan", ""], ["He", "Hua", ""], ["Jojic", "Oliver", ""], ["Lin", "Jimmy", ""]]}, {"id": "1705.05103", "submitter": "Vedran Vukoti\\'c", "authors": "Vedran Vukotic, Christian Raymond, Guillaume Gravier", "title": "Generative Adversarial Networks for Multimodal Representation Learning\n  in Video Hyperlinking", "comments": "4 pages, 1 figure, 2 tables, published at ACM International\n  Conference in Multimedia Retrieval (ICMR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous multimodal representations suitable for multimodal information\nretrieval are usually obtained with methods that heavily rely on multimodal\nautoencoders. In video hyperlinking, a task that aims at retrieving video\nsegments, the state of the art is a variation of two interlocked networks\nworking in opposing directions. These systems provide good multimodal\nembeddings and are also capable of translating from one representation space to\nthe other. Operating on representation spaces, these networks lack the ability\nto operate in the original spaces (text or image), which makes it difficult to\nvisualize the crossmodal function, and do not generalize well to unseen data.\nRecently, generative adversarial networks have gained popularity and have been\nused for generating realistic synthetic data and for obtaining high-level,\nsingle-modal latent representation spaces. In this work, we evaluate the\nfeasibility of using GANs to obtain multimodal representations. We show that\nGANs can be used for multimodal representation learning and that they provide\nmultimodal representations that are superior to representations obtained with\nmultimodal autoencoders. Additionally, we illustrate the ability of visualizing\ncrossmodal translations that can provide human-interpretable insights on\nlearned GAN-based video hyperlinking models.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:52:07 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Vukotic", "Vedran", ""], ["Raymond", "Christian", ""], ["Gravier", "Guillaume", ""]]}, {"id": "1705.05765", "submitter": "Akshay Soni", "authors": "Jeya Balaji Balasubramanian, Akshay Soni, Yashar Mehdad, Nikolay\n  Laptev", "title": "Online Article Ranking as a Constrained, Dynamic, Multi-Objective\n  Optimization Problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The content ranking problem in a social news website, is typically a function\nthat maximizes a scalar metric of interest like dwell-time. However, like in\nmost real-world applications we are interested in more than one metric---for\ninstance simultaneously maximizing click-through rate, monetization metrics,\ndwell-time---and also satisfy the traffic requirements promised to different\npublishers. All this needs to be done on online data and under the settings\nwhere the objective function and the constraints can dynamically change; this\ncould happen if for instance new publishers are added, some contracts are\nadjusted, or if some contracts are over.\n  In this paper, we formulate this problem as a constrained, dynamic,\nmulti-objective optimization problem. We propose a novel framework that extends\na successful genetic optimization algorithm, NSGA-II, to solve this online,\ndata-driven problem. We design the modules of NSGA-II to suit our problem. We\nevaluate optimization performance using Hypervolume and introduce a confidence\ninterval metric for assessing the practicality of a solution. We demonstrate\nthe application of this framework on a real-world Article Ranking problem. We\nobserve that we make considerable improvements in both time and performance\nover a brute-force baseline technique that is currently in production.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:27:57 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Balasubramanian", "Jeya Balaji", ""], ["Soni", "Akshay", ""], ["Mehdad", "Yashar", ""], ["Laptev", "Nikolay", ""]]}, {"id": "1705.06056", "submitter": "Krisztian Balog", "authors": "Dar\\'io Garigliotti, Faegheh Hasibi, Krisztian Balog", "title": "Target Type Identification for Entity-Bearing Queries", "comments": "Extended version of SIGIR'17 short paper, 5 pages", "journal-ref": null, "doi": "10.1145/3077136.3080659", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the target types of entity-bearing queries can help improve\nretrieval performance as well as the overall search experience. In this work,\nwe address the problem of automatically detecting the target types of a query\nwith respect to a type taxonomy. We propose a supervised learning approach with\na rich variety of features. Using a purpose-built test collection, we show that\nour approach outperforms existing methods by a remarkable margin. This is an\nextended version of the article published with the same title in the\nProceedings of SIGIR'17.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 09:06:42 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 09:52:55 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Garigliotti", "Dar\u00edo", ""], ["Hasibi", "Faegheh", ""], ["Balog", "Krisztian", ""]]}, {"id": "1705.06123", "submitter": "Chongyang Gu", "authors": "Haoyu Xu (1 and 2), Chongyang Gu (1 and 3), Han Zhou (1), Sengpan Kou\n  (4), Junjie Zhang (3) ((1) Shanghai Advanced Research Institute, Chinese\n  Academy of Sciences, China, (2) University of Chinese Academy of Sciences,\n  China,(3) Department of Communication and Information Engineering, Shanghai\n  University, China,(4) Department of Mathematics, The Chinese University of\n  Hong Kong, Hong Kong SAR)", "title": "JCTC: A Large Job posting Corpus for Text Classification", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The absence of an appropriate text classification corpus makes the massive\namount of online job information unusable for labor market analysis. This paper\npresents JCTC, a large job posting corpus for text classification. In JCTC\nconstruction framework, a formal specification issued by the Chinese central\ngovernment is chosen as the classification standard. The unsupervised learning\n(WE-cos), supervised learning algorithm (SVM) and human judgements are all used\nin the construction process. JCTC has 102581 online job postings distributed in\n465 categories. The method proposed here can not only ameliorate the high\ndemands on people's skill and knowledge, but reduce the subjective influences\nas well. Besides, the method is not limited in Chinese. We benchmark five\nstate-of-the-art deep learning approaches on JCTC providing baseline results\nfor future studies. JCTC might be the first job posting corpus for text\nclassification and the largest one in Chinese. With the help of JCTC, related\norganizations are able to monitor, analyze and predict the labor market in a\ncomprehensive, accurate and timely manner.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 12:32:05 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 02:33:08 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Xu", "Haoyu", "", "1 and 2"], ["Gu", "Chongyang", "", "1 and 3"], ["Zhou", "Han", ""], ["Kou", "Sengpan", ""], ["Zhang", "Junjie", ""]]}, {"id": "1705.06338", "submitter": "Bibek Behera", "authors": "Bibek Behera, Manoj Joshi, Abhilash KK, Mohammad Ansari Ismail", "title": "Distributed Vector Representation Of Shopping Items, The Customer And\n  Shopping Cart To Build A Three Fold Recommendation System", "comments": "Cicling 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main idea of this paper is to represent shopping items through vectors\nbecause these vectors act as the base for building em- beddings for customers\nand shopping carts. Also, these vectors are input to the mathematical models\nthat act as either a recommendation engine or help in targeting potential\ncustomers. We have used exponential family embeddings as the tool to construct\ntwo basic vectors - product embeddings and context vectors. Using the basic\nvectors, we build combined embeddings, trip embeddings and customer embeddings.\nCombined embeddings mix linguistic properties of product names with their\nshopping patterns. The customer embeddings establish an understand- ing of the\nbuying pattern of customers in a group and help in building customer profile.\nFor example a customer profile can represent customers frequently buying\npet-food. Identifying such profiles can help us bring out offers and discounts.\nSimilarly, trip embeddings are used to build trip profiles. People happen to\nbuy similar set of products in a trip and hence their trip embeddings can be\nused to predict the next product they would like to buy. This is a novel\ntechnique and the first of its kind to make recommendation using product, trip\nand customer embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 20:28:14 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Behera", "Bibek", ""], ["Joshi", "Manoj", ""], ["KK", "Abhilash", ""], ["Ismail", "Mohammad Ansari", ""]]}, {"id": "1705.06504", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Vadim Savenkov", "title": "TableQA: Question Answering on Tabular Data", "comments": "Full version of the demo paper accepted at SEMANTiCS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular data is difficult to analyze and to search through, yielding for new\ntools and interfaces that would allow even non tech-savvy users to gain\ninsights from open datasets without resorting to specialized data analysis\ntools or even without having to fully understand the dataset structure. The\ngoal of our demonstration is to showcase answering natural language questions\nfrom tabular data, and to discuss related system configuration and model\ntraining aspects. Our prototype is publicly available and open-sourced (see\nhttps://svakulenko.ai.wu.ac.at/tableqa).\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 10:08:38 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 11:47:45 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Savenkov", "Vadim", ""]]}, {"id": "1705.06979", "submitter": "Matthias Dorfer", "authors": "Matthias Dorfer and Jan Schl\\\"uter and Andreu Vall and Filip\n  Korzeniowski and Gerhard Widmer", "title": "End-to-End Cross-Modality Retrieval with CCA Projections and Pairwise\n  Ranking Loss", "comments": "Preliminary version of a paper published in the International Journal\n  of Multimedia Information Retrieval", "journal-ref": null, "doi": "10.1007/s13735-018-0151-5\"", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modality retrieval encompasses retrieval tasks where the fetched items\nare of a different type than the search query, e.g., retrieving pictures\nrelevant to a given text query. The state-of-the-art approach to cross-modality\nretrieval relies on learning a joint embedding space of the two modalities,\nwhere items from either modality are retrieved using nearest-neighbor search.\nIn this work, we introduce a neural network layer based on Canonical\nCorrelation Analysis (CCA) that learns better embedding spaces by analytically\ncomputing projections that maximize correlation. In contrast to previous\napproaches, the CCA Layer (CCAL) allows us to combine existing objectives for\nembedding space learning, such as pairwise ranking losses, with the optimal\nprojections of CCA. We show the effectiveness of our approach for\ncross-modality retrieval on three different scenarios (text-to-image,\naudio-sheet-music and zero-shot retrieval), surpassing both Deep CCA and a\nmulti-view network using freely learned projections optimized by a pairwise\nranking loss, especially when little training data is available (the code for\nall three methods is released at: https://github.com/CPJKU/cca_layer).\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 13:23:46 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 14:03:05 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Dorfer", "Matthias", ""], ["Schl\u00fcter", "Jan", ""], ["Vall", "Andreu", ""], ["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1705.07047", "submitter": "Aida Slavic", "authors": "Aida Slavic", "title": "Faceted classification: management and use", "comments": null, "journal-ref": "Axiomathes, 18 (2), pp. 257-71 (2008)", "doi": "10.1007/s10516-007-9030-z", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses issues related to the use of faceted classifications in\nan online environment. The author argues that knowledge organization systems\ncan be fully utilized in information retrieval only if they are exposed and\nmade available for machine processing. The experience with classification\nautomation to date may be used to speed up and ease the conversion of existing\nfaceted schemes or the creation of management tools for new systems. The author\nsuggests that it is possible to agree on a set of functional requirements for\nsupporting faceted classifications online that are equally relevant for the\nmaintenance of classifications, the creation of classification indexing tools,\nor the management of classifications in an authority file. It is suggested that\na set of requirements for analytico-synthetic classifications may be put\nforward to improve standards for the use and exchange of knowledge organization\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:22:19 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Slavic", "Aida", ""]]}, {"id": "1705.07051", "submitter": "Gustavo Lima", "authors": "Gustavo R. Lima, Carlos E. Mello, Geraldo Zimbrao", "title": "Speeding up Memory-based Collaborative Filtering with Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play an important role in many scenarios where users are\noverwhelmed with too many choices to make. In this context, Collaborative\nFiltering (CF) arises by providing a simple and widely used approach for\npersonalized recommendation. Memory-based CF algorithms mostly rely on\nsimilarities between pairs of users or items, which are posteriorly employed in\nclassifiers like k-Nearest Neighbor (kNN) to generalize for unknown ratings. A\nmajor issue regarding this approach is to build the similarity matrix.\nDepending on the dimensionality of the rating matrix, the similarity\ncomputations may become computationally intractable. To overcome this issue, we\npropose to represent users by their distances to preselected users, namely\nlandmarks. This procedure allows to drastically reduce the computational cost\nassociated with the similarity matrix. We evaluated our proposal on two\ndistinct distinguishing databases, and the results showed our method has\nconsistently and considerably outperformed eight CF algorithms (including both\nmemory-based and model-based) in terms of computational performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:31:09 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Lima", "Gustavo R.", ""], ["Mello", "Carlos E.", ""], ["Zimbrao", "Geraldo", ""]]}, {"id": "1705.07058", "submitter": "Aida Slavic", "authors": "Aida Slavic", "title": "Classification revisited: a web of knowledge", "comments": "Aida Slavic (2011) Classification revisited: a web of knowledge. In:\n  Innovations in information retrieval: perspectives for theory and practice.\n  Eds. Allen Foster and Pauline Rafferty. London: Facet, pp. 23-48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of the Semantic Web (SW) is gradually unfolding and taking shape\nthrough a web of linked data, a part of which is built by capturing semantics\nstored in existing knowledge organization systems (KOS), subject metadata and\nresource metadata. The content of vast bibliographic collections is currently\ncategorized by some widely used bibliographic classification and we may soon\nsee them being mined for information and linked in a meaningful way across the\nWeb. Bibliographic classifications are designed for knowledge mediation which\noffers both a rich terminology and different ways in which concepts can be\ncategorized and related to each other in the universe of knowledge. From\n1990-2010 they have been used in various resource discovery services on the Web\nand continue to be used to support information integration in a number of\ninternational digital library projects. In this chapter we will revisit some of\nthe ways in which universal classifications, as language independent concept\nschemes, can assist humans and computers in structuring and presenting\ninformation and formulating queries. Most importantly, we highlight issues\nimportant to understanding bibliographic classifications, both in terms of\ntheir unused potential and technical limitations.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:43:10 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Slavic", "Aida", ""]]}, {"id": "1705.07311", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Ida Mele, and Fabio Crestani", "title": "Personalized Ranking for Context-Aware Venue Suggestion", "comments": "The 32nd ACM SIGAPP Symposium On Applied Computing (SAC), Marrakech,\n  Morocco, April 4-6, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making personalized and context-aware suggestions of venues to the users is\nvery crucial in venue recommendation. These suggestions are often based on\nmatching the venues' features with the users' preferences, which can be\ncollected from previously visited locations. In this paper we present a novel\nuser-modeling approach which relies on a set of scoring functions for making\npersonalized suggestions of venues based on venues content and reviews as well\nas users context. Our experiments, conducted on the dataset of the TREC\nContextual Suggestion Track, prove that our methodology outperforms\nstate-of-the-art approaches by a significant margin.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 14:21:02 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Mele", "Ida", ""], ["Crestani", "Fabio", ""]]}, {"id": "1705.07563", "submitter": "Yuxin Su", "authors": "Yuxin Su, Irwin King, Michael Lyu", "title": "Learning to Rank Using Localized Geometric Mean Metrics", "comments": "To appear in SIGIR'17", "journal-ref": null, "doi": "10.1145/3077136.3080828", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning-to-rank (LtR) algorithms focus on query-independent model, in\nwhich query and document do not lie in the same feature space, and the rankers\nrely on the feature ensemble about query-document pair instead of the\nsimilarity between query instance and documents. However, existing algorithms\ndo not consider local structures in query-document feature space, and are\nfragile to irrelevant noise features. In this paper, we propose a novel\nRiemannian metric learning algorithm to capture the local structures and\ndevelop a robust LtR algorithm. First, we design a concept called \\textit{ideal\ncandidate document} to introduce metric learning algorithm to query-independent\nmodel. Previous metric learning algorithms aiming to find an optimal metric\nspace are only suitable for query-dependent model, in which query instance and\ndocuments belong to the same feature space and the similarity is directly\ncomputed from the metric space. Then we extend the new and extremely fast\nglobal Geometric Mean Metric Learning (GMML) algorithm to develop a localized\nGMML, namely L-GMML. Based on the combination of local learned metrics, we\nemploy the popular Normalized Discounted Cumulative Gain~(NDCG) scorer and\nWeighted Approximate Rank Pairwise (WARP) loss to optimize the \\textit{ideal\ncandidate document} for each query candidate set. Finally, we can quickly\nevaluate all candidates via the similarity between the \\textit{ideal candidate\ndocument} and other candidates. By leveraging the ability of metric learning\nalgorithms to describe the complex structural information, our approach gives\nus a principled and efficient way to perform LtR tasks. The experiments on\nreal-world datasets demonstrate that our proposed L-GMML algorithm outperforms\nthe state-of-the-art metric learning to rank methods and the stylish\nquery-independent LtR algorithms regarding accuracy and computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:46:44 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Su", "Yuxin", ""], ["King", "Irwin", ""], ["Lyu", "Michael", ""]]}, {"id": "1705.08063", "submitter": "Arman Cohan", "authors": "Arman Cohan, Nazli Goharian", "title": "Contextualizing Citations for Scientific Summarization using Word\n  Embeddings and Domain Knowledge", "comments": "SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080740", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation texts are sometimes not very informative or in some cases inaccurate\nby themselves; they need the appropriate context from the referenced paper to\nreflect its exact contributions. To address this problem, we propose an\nunsupervised model that uses distributed representation of words as well as\ndomain knowledge to extract the appropriate context from the reference paper.\nEvaluation results show the effectiveness of our model by significantly\noutperforming the state-of-the-art. We furthermore demonstrate how an effective\ncontextualization method results in improving citation-based summarization of\nthe scientific articles.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 02:55:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1705.08094", "submitter": "Guangdong Bai", "authors": "Zhengkui Wang, Guangdong Bai, Soumyadeb Chowdhury, Quanqing Xu, Zhi\n  Lin Seow", "title": "TwiInsight: Discovering Topics and Sentiments from Social Media Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms contain a great wealth of information which provides\nopportunities for us to explore hidden patterns or unknown correlations, and\nunderstand people's satisfaction with what they are discussing. As one\nshowcase, in this paper, we present a system, TwiInsight which explores the\ninsight of Twitter data. Different from other Twitter analysis systems,\nTwiInsight automatically extracts the popular topics under different categories\n(e.g., healthcare, food, technology, sports and transport) discussed in Twitter\nvia topic modeling and also identifies the correlated topics across different\ncategories. Additionally, it also discovers the people's opinions on the tweets\nand topics via the sentiment analysis. The system also employs an intuitive and\ninformative visualization to show the uncovered insight. Furthermore, we also\ndevelop and compare six most popular algorithms - three for sentiment analysis\nand three for topic modeling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 06:49:12 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Zhengkui", ""], ["Bai", "Guangdong", ""], ["Chowdhury", "Soumyadeb", ""], ["Xu", "Quanqing", ""], ["Seow", "Zhi Lin", ""]]}, {"id": "1705.08154", "submitter": "Martin K\\\"orner", "authors": "Martin K\\\"orner", "title": "Reference String Extraction Using Line-Based Conditional Random Fields", "comments": "5 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The extraction of individual reference strings from the reference section of\nscientific publications is an important step in the citation extraction\npipeline. Current approaches divide this task into two steps by first detecting\nthe reference section areas and then grouping the text lines in such areas into\nreference strings. We propose a classification model that considers every line\nin a publication as a potential part of a reference string. By applying\nline-based conditional random fields rather than constructing the graphical\nmodel based on the individual words, dependencies and patterns that are typical\nin reference sections provide strong features while the overall complexity of\nthe model is reduced.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 09:36:41 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["K\u00f6rner", "Martin", ""]]}, {"id": "1705.08283", "submitter": "Andreu Vall", "authors": "Andreu Vall, Hamid Eghbal-zadeh, Matthias Dorfer, Markus Schedl,\n  Gerhard Widmer", "title": "Music Playlist Continuation by Learning from Hand-Curated Examples and\n  Song Features: Alleviating the Cold-Start Problem for Rare and Out-of-Set\n  Songs", "comments": null, "journal-ref": null, "doi": "10.1145/3125486.3125494", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated music playlist generation is a specific form of music\nrecommendation. Generally stated, the user receives a set of song suggestions\ndefining a coherent listening session. We hypothesize that the best way to\nconvey such playlist coherence to new recommendations is by learning it from\nactual curated examples, in contrast to imposing ad hoc constraints.\nCollaborative filtering methods can be used to capture underlying patterns in\nhand-curated playlists. However, the scarcity of thoroughly curated playlists\nand the bias towards popular songs result in the vast majority of songs\noccurring in very few playlists and thus being poorly recommended. To overcome\nthis issue, we propose an alternative model based on a song-to-playlist\nclassifier, which learns the underlying structure from actual playlists while\nleveraging song features derived from audio, social tags and independent\nlistening logs. Experiments on two datasets of hand-curated playlists show\ncompetitive performance compared to collaborative filtering when sufficient\ntraining data is available and more robust performance when recommending rare\nand out-of-set songs. For example, both approaches achieve a recall@100 of\nroughly 35% for songs occurring in 5 or more training playists, whereas the\nproposed model achieves a recall@100 of roughly 15% for songs occurring in 4 or\nless training playlists, compared to the 3% achieved by collaborative\nfiltering.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:05:11 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 09:52:36 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 09:45:31 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Vall", "Andreu", ""], ["Eghbal-zadeh", "Hamid", ""], ["Dorfer", "Matthias", ""], ["Schedl", "Markus", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1705.08321", "submitter": "Roman Gurinovich", "authors": "Roman Gurinovich, Alexander Pashuk, Yuriy Petrovskiy, Alex\n  Dmitrievskij, Oleg Kuryan, Alexei Scerbacov, Antonia Tiggre, Elena Moroz,\n  Yuri Nikolsky", "title": "Increasing Papers' Discoverability with Precise Semantic Labeling: the\n  sci.AI Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of published findings in biomedicine increases continually. At the\nsame time, specifics of the domain's terminology complicates the task of\nrelevant publications retrieval. In the current research, we investigate\ninfluence of terms' variability and ambiguity on a paper's likelihood of being\nretrieved. We obtained statistics that demonstrate significance of the issue\nand its challenges, followed by presenting the sci.AI platform, which allows\nprecise terms labeling as a resolution.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 17:04:42 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gurinovich", "Roman", ""], ["Pashuk", "Alexander", ""], ["Petrovskiy", "Yuriy", ""], ["Dmitrievskij", "Alex", ""], ["Kuryan", "Oleg", ""], ["Scerbacov", "Alexei", ""], ["Tiggre", "Antonia", ""], ["Moroz", "Elena", ""], ["Nikolsky", "Yuri", ""]]}, {"id": "1705.08598", "submitter": "Mark Sanderson", "authors": "Omid Aghili, Mark Sanderson", "title": "Journalists' information needs, seeking behavior, and its determinants\n  on social media", "comments": "This is a very rushed version of the work conducted here. It does a\n  bad job of explaining the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the results of a qualitative study on journalists' information\nseeking behavior on social media. Based on interviews with eleven journalists\nalong with a study of a set of university level journalism modules, we\ndetermined the categories of information need types that lead journalists to\nsocial media. We also determined the ways that social media is exploited as a\ntool to satisfy information needs and to define influential factors, which\nimpacted on journalists' information seeking behavior. We find that not only is\nsocial media used as an information source, but it can also be a supplier of\nstories found serendipitously. We find seven information need types that expand\nthe types found in previous work. We also find five categories of influential\nfactors that affect the way journalists seek information.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 04:03:55 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 06:13:34 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 09:15:37 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Aghili", "Omid", ""], ["Sanderson", "Mark", ""]]}, {"id": "1705.08804", "submitter": "Sirui Yao", "authors": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative-filtering methods to make unfair predictions for users from\nminority groups. We identify the insufficiency of existing fairness metrics and\npropose four new metrics that address different forms of unfairness. These\nfairness metrics can be optimized by adding fairness terms to the learning\nobjective. Experiments on synthetic and real data show that our new metrics can\nbetter measure fairness than the baseline, and that the fairness objectives\neffectively help reduce unfairness.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:52:06 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:11:25 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yao", "Sirui", ""], ["Huang", "Bert", ""]]}, {"id": "1705.08844", "submitter": "Rodrigo Toro Icarte", "authors": "Rodrigo Toro Icarte, Jorge A. Baier, Cristian Ruz, Alvaro Soto", "title": "How a General-Purpose Commonsense Ontology can Improve Performance of\n  Learning-Based Image Retrieval", "comments": "Accepted in IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge representation community has built general-purpose ontologies\nwhich contain large amounts of commonsense knowledge over relevant aspects of\nthe world, including useful visual information, e.g.: \"a ball is used by a\nfootball player\", \"a tennis player is located at a tennis court\". Current\nstate-of-the-art approaches for visual recognition do not exploit these\nrule-based knowledge sources. Instead, they learn recognition models directly\nfrom training examples. In this paper, we study how general-purpose\nontologies---specifically, MIT's ConceptNet ontology---can improve the\nperformance of state-of-the-art vision systems. As a testbed, we tackle the\nproblem of sentence-based image retrieval. Our retrieval approach incorporates\nknowledge from ConceptNet on top of a large pool of object detectors derived\nfrom a deep learning technique. In our experiments, we show that ConceptNet can\nimprove performance on a common benchmark dataset. Key to our performance is\nthe use of the ESPGAME dataset to select visually relevant relations from\nConceptNet. Consequently, a main conclusion of this work is that\ngeneral-purpose commonsense ontologies improve performance on visual reasoning\ntasks when properly filtered to select meaningful visual relations.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:22:53 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Icarte", "Rodrigo Toro", ""], ["Baier", "Jorge A.", ""], ["Ruz", "Cristian", ""], ["Soto", "Alvaro", ""]]}, {"id": "1705.09656", "submitter": "Claudia Orellana-Rodriguez", "authors": "Terrence Szymanski, Claudia Orellana-Rodriguez and Mark T. Keane", "title": "Helping News Editors Write Better Headlines: A Recommender to Improve\n  the Keyword Contents & Shareability of News Headlines", "comments": null, "journal-ref": "Natural Language Processing meets Journalism. IJCAI-16 workshop.\n  Pages 30-34. 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a software tool that employs state-of-the-art natural language\nprocessing (NLP) and machine learning techniques to help newspaper editors\ncompose effective headlines for online publication. The system identifies the\nmost salient keywords in a news article and ranks them based on both their\noverall popularity and their direct relevance to the article. The system also\nuses a supervised regression model to identify headlines that are likely to be\nwidely shared on social media. The user interface is designed to simplify and\nspeed the editor's decision process on the composition of the headline. As\nsuch, the tool provides an efficient way to combine the benefits of automated\npredictors of engagement and search-engine optimization (SEO) with human\njudgments of overall headline quality.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 17:40:58 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Szymanski", "Terrence", ""], ["Orellana-Rodriguez", "Claudia", ""], ["Keane", "Mark T.", ""]]}, {"id": "1705.09808", "submitter": "Madhulika Mohanty", "authors": "Madhulika Mohanty and Maya Ramanath", "title": "KlusTree: Clustering Answer Trees from Keyword Search on Graphs", "comments": "16 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph structured data on the web is now massive as well as diverse, ranging\nfrom social networks, web graphs to knowledge-bases. Effectively querying this\ngraph structured data is non-trivial and has led to research in a variety of\ndirections -- structured queries, keyword and natural language queries,\nautomatic translation of these queries to structured queries, etc. We are\nconcerned with a class of queries called relationship queries, which are\nusually expressed as a set of keywords (each keyword denoting a named entity).\nThe results returned are a set of ranked trees, each of which denotes\nrelationships among the various keywords. The result list could consist of\nhundreds of answers. The problem of keyword search on graphs has been explored\nfor over a decade now, but an important aspect that is not as extensively\nstudied is that of user experience. We propose KlusTree, which presents\nclustered results to the users instead of a list of all the results. In our\napproach, the result trees are represented using language models and are\nclustered using JS divergence as a distance measure. We compare KlusTree with\nthe well-known approaches based on isomorphism and tree-edit distance based\nclustering. The user evaluations show that KlusTree outperforms the other two\nin providing better clustering, thereby enriching user experience, revealing\ninteresting patterns and improving result interpretation by the user.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 11:24:53 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Mohanty", "Madhulika", ""], ["Ramanath", "Maya", ""]]}, {"id": "1705.10351", "submitter": "Eric Tellez Dr.", "authors": "Eric S. Tellez, Guillermo Ruiz, Edgar Chavez, Mario Graff", "title": "A scalable solution to the nearest neighbor search problem through\n  local-search methods on neighbor graphs", "comments": null, "journal-ref": "Pattern Analysis and Applications 24 763--777 2021", "doi": "10.1007/s10044-020-00946-w", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near neighbor search (NNS) is a powerful abstraction for data access;\nhowever, data indexing is troublesome even for approximate indexes. For\nintrinsically high-dimensional data, high-quality fast searches demand either\nindexes with impractically large memory usage or preprocessing time.\n  In this paper, we introduce an algorithm to solve a nearest-neighbor query\n$q$ by minimizing a kernel function defined by the distance from $q$ to each\nobject in the database. The minimization is performed using metaheuristics to\nsolve the problem rapidly; even when some methods in the literature use this\nstrategy behind the scenes, our approach is the first one using it explicitly.\nWe also provide two approaches to select edges in the graph's construction\nstage that limit memory footprint and reduce the number of free parameters\nsimultaneously.\n  We carry out a thorough experimental comparison with state-of-the-art indexes\nthrough synthetic and real-world datasets; we found out that our contributions\nachieve competitive performances regarding speed, accuracy, and memory in\nalmost any of our benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:32:00 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 11:49:06 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 00:26:50 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 14:46:02 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Tellez", "Eric S.", ""], ["Ruiz", "Guillermo", ""], ["Chavez", "Edgar", ""], ["Graff", "Mario", ""]]}, {"id": "1705.10453", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari", "title": "Twitter Hashtag Recommendation using Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter, one of the biggest and most popular microblogging Websites, has\nevolved into a powerful communication platform which allows millions of active\nusers to generate huge volume of microposts and queries on a daily basis. To\naccommodate effective categorization and easy search, users are allowed to make\nuse of hashtags, keywords or phrases prefixed by hash character, to categorize\nand summarize their posts. However, valid hashtags are not restricted and thus\nare created in a free and heterogeneous style, increasing difficulty of the\ntask of tweet categorization. In this paper, we propose a low-rank weighted\nmatrix factorization based method to recommend hashtags to the users solely\nbased on their hashtag usage history and independent from their tweets'\ncontents. We confirm using two-sample t-test that users are more likely to\nadopt new hashtags similar to the ones they have previously adopted. In\nparticular, we formulate the problem of hashtag recommendation into an\noptimization problem and incorporate hashtag correlation weight matrix into it\nto account for the similarity between different hashtags. We finally leverage\nwidely used matrix factorization from recommender systems to solve the\noptimization problem by capturing the latent factors of users and hashtags.\nEmpirical experiments demonstrate that our method is capable to properly\nrecommend hashtags.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 04:28:28 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Alvari", "Hamidreza", ""]]}, {"id": "1705.10513", "submitter": "Weinan Zhang", "authors": "Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang,\n  Peng Zhang, Dell Zhang", "title": "IRGAN: A Minimax Game for Unifying Generative and Discriminative\n  Information Retrieval Models", "comments": "12 pages; appendix added", "journal-ref": null, "doi": "10.1145/3077136.3080786", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper provides a unified account of two schools of thinking in\ninformation retrieval modelling: the generative retrieval focusing on\npredicting relevant documents given a query, and the discriminative retrieval\nfocusing on predicting relevancy given a query-document pair. We propose a game\ntheoretical minimax game to iteratively optimise both models. On one hand, the\ndiscriminative model, aiming to mine signals from labelled and unlabelled data,\nprovides guidance to train the generative model towards fitting the underlying\nrelevance distribution over documents given the query. On the other hand, the\ngenerative model, acting as an attacker to the current discriminative model,\ngenerates difficult examples for the discriminative model in an adversarial way\nby minimising its discrimination objective. With the competition between these\ntwo models, we show that the unified framework takes advantage of both schools\nof thinking: (i) the generative model learns to fit the relevance distribution\nover documents via the signals from the discriminative model, and (ii) the\ndiscriminative model is able to exploit the unlabelled data selected by the\ngenerative model to achieve a better estimation for document ranking. Our\nexperimental results have demonstrated significant performance gains as much as\n23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of\napplications including web search, item recommendation, and question answering.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 09:03:31 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 09:00:19 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Lantao", ""], ["Zhang", "Weinan", ""], ["Gong", "Yu", ""], ["Xu", "Yinghui", ""], ["Wang", "Benyou", ""], ["Zhang", "Peng", ""], ["Zhang", "Dell", ""]]}, {"id": "1705.10689", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra, Ashton Anderson, Fernando Diaz, Amit Sharma, Hanna\n  Wallach, Emine Yilmaz", "title": "Auditing Search Engines for Differential Satisfaction Across\n  Demographics", "comments": "8 pages Accepted at WWW 2017", "journal-ref": null, "doi": "10.1145/3041021.3054197", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online services, such as search engines, social media platforms, and\ndigital marketplaces, are advertised as being available to any user, regardless\nof their age, gender, or other demographic factors. However, there are growing\nconcerns that these services may systematically underserve some groups of\nusers. In this paper, we present a framework for internally auditing such\nservices for differences in user satisfaction across demographic groups, using\nsearch engines as a case study. We first explain the pitfalls of na\\\"ively\ncomparing the behavioral metrics that are commonly used to evaluate search\nengines. We then propose three methods for measuring latent differences in user\nsatisfaction from observed differences in evaluation metrics. To develop these\nmethods, we drew on ideas from the causal inference literature and the\nmultilevel modeling literature. Our framework is broadly applicable to other\nonline services, and provides general insight into interpreting their\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:47:00 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Anderson", "Ashton", ""], ["Diaz", "Fernando", ""], ["Sharma", "Amit", ""], ["Wallach", "Hanna", ""], ["Yilmaz", "Emine", ""]]}, {"id": "1705.11056", "submitter": "Ruihui Zhao", "authors": "Ruihui Zhao, Yuanliang Sun, Mizuho Iwaihara", "title": "LRSE: A Lightweight Efficient Searchable Encryption Scheme using Local\n  and Global Representations", "comments": "9 pages, submitted to CIKM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is emerging as a revolutionary computing paradigm, while\nsecurity and privacy become major concerns in the cloud scenario. For which\nSearchable Encryption (SE) technology is proposed to support efficient\nretrieval of encrypted data. However, the absence of lightweight ranked search\nwith higher search quality in a harsh adversary model is still a typical\nshortage in existing SE schemes. In this paper, we propose a novel SE scheme\ncalled LRSE which firstly integrates machine learning methods into the\nframework of SE and combines local and global representations of encrypted\ncloud data to achieve the above design goals. In LRSE, we employ an improved\nsecure kNN scheme to guarantee sufficient privacy protection. Our detailed\nsecurity analysis shows that LRSE satisfies our formulated privacy\nrequirements. Extensive experiments performed on benchmark datasets demonstrate\nthat LRSE indeed achieves state-of-the-art search quality with lowest system\ncost.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 07:33:54 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Zhao", "Ruihui", ""], ["Sun", "Yuanliang", ""], ["Iwaihara", "Mizuho", ""]]}]