[{"id": "1612.00148", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Yashar Mehdad, Troy Chevalier", "title": "Domain Adaptation for Named Entity Recognition in Online Media with Word\n  Embeddings", "comments": "12 pages, 3 figures, 8 tables arxiv preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content on the Internet is heterogeneous and arises from various domains like\nNews, Entertainment, Finance and Technology. Understanding such content\nrequires identifying named entities (persons, places and organizations) as one\nof the key steps. Traditionally Named Entity Recognition (NER) systems have\nbeen built using available annotated datasets (like CoNLL, MUC) and demonstrate\nexcellent performance. However, these models fail to generalize onto other\ndomains like Sports and Finance where conventions and language use can differ\nsignificantly. Furthermore, several domains do not have large amounts of\nannotated labeled data for training robust Named Entity Recognition models. A\nkey step towards this challenge is to adapt models learned on domains where\nlarge amounts of annotated training data are available to domains with scarce\nannotated data.\n  In this paper, we propose methods to effectively adapt models learned on one\ndomain onto other domains using distributed word representations. First we\nanalyze the linguistic variation present across domains to identify key\nlinguistic insights that can boost performance across domains. We propose\nmethods to capture domain specific semantics of word usage in addition to\nglobal semantics. We then demonstrate how to effectively use such domain\nspecific knowledge to learn NER models that outperform previous baselines in\nthe domain adaptation setting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:08:53 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Mehdad", "Yashar", ""], ["Chevalier", "Troy", ""]]}, {"id": "1612.00260", "submitter": "Rom\\`an R. Zapatrin", "authors": "Roman Zapatrin", "title": "Factory of realities: on the emergence of virtual spatiotemporal\n  structures", "comments": "21 pp", "journal-ref": "\"Beyond Peaceful Coexistence The Emergence of Space, Time and\n  Quantum\", Ignazio Licata, Ed., IMPERIAL COLLEGE PRESS (2016), pp. 201-220", "doi": "10.1142/9781783268320_0008", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous nature of modern Information Retrieval and Virtual World give\nrise to new realities. To what extent are these \"realities\" real? Which\n\"physics\" should be applied to quantitatively describe them? In this essay I\ndwell on few examples. The first is Adaptive neural networks, which are not\nnetworks and not neural, but still provide service similar to classical ANNs in\nextended fashion. The second is the emergence of objects looking like\nEinsteinian spacetime, which describe the behavior of an Internet surfer like\ngeodesic motion. The third is the demonstration of nonclassical and even\nstronger-than-quantum probabilities in Information Retrieval, their use.\n  Immense operable datasets provide new operationalistic environments, which\nbecome to greater and greater extent \"realities\". In this essay, I consider the\noverall Information Retrieval process as an objective physical process,\nrepresenting it according to Melucci metaphor in terms of physical-like\nexperiments. Various semantic environments are treated as analogs of various\nrealities. The readers' attention is drawn to topos approach to physical\ntheories, which provides a natural conceptual and technical framework to cope\nwith the new emerging realities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 08:41:04 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zapatrin", "Roman", ""]]}, {"id": "1612.00959", "submitter": "Karol W\\k{e}grzycki", "authors": "Andrzej Pacuk and Piotr Sankowski and Karol W\\k{e}grzycki and Adam\n  Witkowski and Piotr Wygocki", "title": "RecSys Challenge 2016: job recommendations based on preselection of\n  offers and gradient boosting", "comments": "6 pages, 1 figure, 2 tables, Description of 2nd place winning\n  solution of RecSys 2016 Challange. To be published in RecSys'16 Challange\n  Proceedings", "journal-ref": "Proceedings of the Recommender Systems Challenge, RecSys Challenge\n  '16, Boston, Massachusetts - September 15 - 15, 2016, pages 10:1--10:4", "doi": "10.1145/2987538.2987544", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Mim-Solution's approach to the RecSys Challenge 2016, which\nranked 2nd. The goal of the competition was to prepare job recommendations for\nthe users of the website Xing.com.\n  Our two phase algorithm consists of candidate selection followed by the\ncandidate ranking. We ranked the candidates by the predicted probability that\nthe user will positively interact with the job offer. We have used Gradient\nBoosting Decision Trees as the regression tool.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 11:35:37 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Pacuk", "Andrzej", ""], ["Sankowski", "Piotr", ""], ["W\u0119grzycki", "Karol", ""], ["Witkowski", "Adam", ""], ["Wygocki", "Piotr", ""]]}, {"id": "1612.00992", "submitter": "Mark Howison", "authors": "David Berenbaum, Dwyer Deighan, Thomas Marlow, Ashley Lee, Scott\n  Frickel, Mark Howison", "title": "Mining Spatio-temporal Data on Industrialization from Historical\n  Registries", "comments": null, "journal-ref": "Journal of Environmental Informatics 34(1): 28-34 (2019)", "doi": "10.3808/jei.201700381", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing availability of big data in many fields, historical data\non socioevironmental phenomena are often not available due to a lack of\nautomated and scalable approaches for collecting, digitizing, and assembling\nthem. We have developed a data-mining method for extracting tabulated, geocoded\ndata from printed directories. While scanning and optical character recognition\n(OCR) can digitize printed text, these methods alone do not capture the\nstructure of the underlying data. Our pipeline integrates both page layout\nanalysis and OCR to extract tabular, geocoded data from structured text. We\ndemonstrate the utility of this method by applying it to scanned manufacturing\nregistries from Rhode Island that record 41 years of industrial land use. The\nresulting spatio-temporal data can be used for socioenvironmental analyses of\nindustrialization at a resolution that was not previously possible. In\nparticular, we find strong evidence for the dispersion of manufacturing from\nthe urban core of Providence, the state's capital, along the Interstate 95\ncorridor to the north and south.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 17:54:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Berenbaum", "David", ""], ["Deighan", "Dwyer", ""], ["Marlow", "Thomas", ""], ["Lee", "Ashley", ""], ["Frickel", "Scott", ""], ["Howison", "Mark", ""]]}, {"id": "1612.01340", "submitter": "Ankesh Anand", "authors": "Ankesh Anand, Tanmoy Chakraborty, Noseong Park", "title": "We used Neural Networks to Detect Clickbaits: You won't believe what\n  happened Next!", "comments": "Accepted to the European Conference on Information Retrieval (ECIR),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online content publishers often use catchy headlines for their articles in\norder to attract users to their websites. These headlines, popularly known as\nclickbaits, exploit a user's curiosity gap and lure them to click on links that\noften disappoint them. Existing methods for automatically detecting clickbaits\nrely on heavy feature engineering and domain knowledge. Here, we introduce a\nneural network architecture based on Recurrent Neural Networks for detecting\nclickbaits. Our model relies on distributed word representations learned from a\nlarge unannotated corpora, and character embeddings learned via Convolutional\nNeural Networks. Experimental results on a dataset of news headlines show that\nour model outperforms existing techniques for clickbait detection with an\naccuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 13:18:48 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 02:44:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Anand", "Ankesh", ""], ["Chakraborty", "Tanmoy", ""], ["Park", "Noseong", ""]]}, {"id": "1612.01834", "submitter": "Beidi Chen", "authors": "Beidi Chen, Anshumali Shrivastava", "title": "Revisiting Winner Take All (WTA) Hashing for Sparse Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WTA (Winner Take All) hashing has been successfully applied in many large\nscale vision applications. This hashing scheme was tailored to take advantage\nof the comparative reasoning (or order based information), which showed\nsignificant accuracy improvements. In this paper, we identify a subtle issue\nwith WTA, which grows with the sparsity of the datasets. This issue limits the\ndiscriminative power of WTA. We then propose a solution for this problem based\non the idea of Densification which provably fixes the issue. Our experiments\nshow that Densified WTA Hashing outperforms Vanilla WTA both in image\nclassification and retrieval tasks consistently and significantly.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:51:37 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 08:50:26 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1612.01835", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi, Beidi Chen, Anshumali Shrivastava, Dan Wallach,\n  Farinaz Koushanfar", "title": "Sub-Linear Privacy-Preserving Near-Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Near-Neighbor Search (NNS), a new client queries a database (held by a\nserver) for the most similar data (near-neighbors) given a certain similarity\nmetric. The Privacy-Preserving variant (PP-NNS) requires that neither server\nnor the client shall learn information about the other party's data except what\ncan be inferred from the outcome of NNS. The overwhelming growth in the size of\ncurrent datasets and the lack of a truly secure server in the online world\nrender the existing solutions impractical; either due to their high\ncomputational requirements or non-realistic assumptions which potentially\ncompromise privacy. PP-NNS having query time {\\it sub-linear} in the size of\nthe database has been suggested as an open research direction by Li et al.\n(CCSW'15). In this paper, we provide the first such algorithm, called Secure\nLocality Sensitive Indexing (SLSI) which has a sub-linear query time and the\nability to handle honest-but-curious parties. At the heart of our proposal lies\na secure binary embedding scheme generated from a novel probabilistic\ntransformation over locality sensitive hashing family. We provide information\ntheoretic bound for the privacy guarantees and support our theoretical claims\nusing substantial empirical evidence on real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:53:06 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 08:41:48 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 16:46:39 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 17:36:02 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""], ["Wallach", "Dan", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1612.01840", "submitter": "Micha\\\"el Defferrard", "authors": "Micha\\\"el Defferrard, Kirell Benzi, Pierre Vandergheynst and Xavier\n  Bresson", "title": "FMA: A Dataset For Music Analysis", "comments": "ISMIR 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Free Music Archive (FMA), an open and easily accessible\ndataset suitable for evaluating several tasks in MIR, a field concerned with\nbrowsing, searching, and organizing large music collections. The community's\ngrowing interest in feature and end-to-end learning is however restrained by\nthe limited availability of large audio datasets. The FMA aims to overcome this\nhurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio\nfrom 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a\nhierarchical taxonomy of 161 genres. It provides full-length and high-quality\naudio, pre-computed features, together with track- and user-level metadata,\ntags, and free-form text such as biographies. We here describe the dataset and\nhow it was created, propose a train/validation/test split and three subsets,\ndiscuss some suitable MIR tasks, and evaluate some baselines for genre\nrecognition. Code, data, and usage examples are available at\nhttps://github.com/mdeff/fma\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 14:58:59 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 23:14:19 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 18:38:33 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Defferrard", "Micha\u00ebl", ""], ["Benzi", "Kirell", ""], ["Vandergheynst", "Pierre", ""], ["Bresson", "Xavier", ""]]}, {"id": "1612.02350", "submitter": "Francisco Raposo", "authors": "Francisco Raposo, David Martins de Matos, Ricardo Ribeiro", "title": "An Information-theoretic Approach to Machine-oriented Music\n  Summarization", "comments": "7 pages, 1 algorithm, 7 figures, 1 table, submitted to Pattern\n  Recognition Letters (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music summarization allows for higher efficiency in processing, storage, and\nsharing of datasets. Machine-oriented approaches, being agnostic to human\nconsumption, optimize these aspects even further. Such summaries have already\nbeen successfully validated in some MIR tasks. We now generalize previous\nconclusions by evaluating the impact of generic summarization of music from a\nprobabilistic perspective. We estimate Gaussian distributions for original and\nsummarized songs and compute their relative entropy, in order to measure\ninformation loss incurred by summarization. Our results suggest that relative\nentropy is a good predictor of summarization performance in the context of\ntasks relying on a bag-of-features model. Based on this observation, we further\npropose a straightforward yet expressive summarizer, which minimizes relative\nentropy with respect to the original song, that objectively outperforms\nprevious methods and is better suited to avoid potential copyright issues.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 18:02:09 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 14:47:27 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 16:31:23 GMT"}, {"version": "v4", "created": "Sat, 12 Aug 2017 14:16:35 GMT"}, {"version": "v5", "created": "Mon, 25 Sep 2017 09:35:58 GMT"}, {"version": "v6", "created": "Fri, 21 Sep 2018 16:26:36 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Raposo", "Francisco", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1612.02696", "submitter": "Sven Kosub", "authors": "Sven Kosub", "title": "A note on the triangle inequality for the Jaccard distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two simple proofs of the triangle inequality for the Jaccard distance in\nterms of nonnegative, monotone, submodular functions are given and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 15:25:35 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kosub", "Sven", ""]]}, {"id": "1612.02814", "submitter": "Ting Chen Ting Chen", "authors": "Ting Chen and Yizhou Sun", "title": "Task-Guided and Path-Augmented Heterogeneous Network Embedding for\n  Author Identification", "comments": "Accepted by WSDM 2017. This is an extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of author identification under\ndouble-blind review setting, which is to identify potential authors given\ninformation of an anonymized paper. Different from existing approaches that\nrely heavily on feature engineering, we propose to use network embedding\napproach to address the problem, which can automatically represent nodes into\nlower dimensional feature vectors. However, there are two major limitations in\nrecent studies on network embedding: (1) they are usually general-purpose\nembedding methods, which are independent of the specific tasks; and (2) most of\nthese approaches can only deal with homogeneous networks, where the\nheterogeneity of the network is ignored. Hence, challenges faced here are two\nfolds: (1) how to embed the network under the guidance of the author\nidentification task, and (2) how to select the best type of information due to\nthe heterogeneity of the network.\n  To address the challenges, we propose a task-guided and path-augmented\nheterogeneous network embedding model. In our model, nodes are first embedded\nas vectors in latent feature space. Embeddings are then shared and jointly\ntrained according to task-specific and network-general objectives. We extend\nthe existing unsupervised network embedding to incorporate meta paths in\nheterogeneous networks, and select paths according to the specific task. The\nguidance from author identification task for network embedding is provided both\nexplicitly in joint training and implicitly during meta path selection. Our\nexperiments demonstrate that by using path-augmented network embedding with\ntask guidance, our model can obtain significantly better accuracy at\nidentifying the true authors comparing to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:56:48 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 04:20:03 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""]]}, {"id": "1612.03231", "submitter": "Yongjun Zhu", "authors": "Yongjun Zhu, Erjia Yan, Il-Yeol Song", "title": "A natural language interface to a graph-based bibliographic information\n  retrieval system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing scientific literature, there is a need on a natural\nlanguage interface to bibliographic information retrieval systems to retrieve\nrelated information effectively. In this paper, we propose a natural language\ninterface, NLI-GIBIR, to a graph-based bibliographic information retrieval\nsystem. In designing NLI-GIBIR, we developed a novel framework that can be\napplicable to graph-based bibliographic information retrieval systems. Our\nframework integrates algorithms/heuristics for interpreting and analyzing\nnatural language bibliographic queries. NLI-GIBIR allows users to search for a\nvariety of bibliographic data through natural language. A series of text- and\nlinguistic-based techniques are used to analyze and answer natural language\nqueries, including tokenization, named entity recognition, and syntactic\nanalysis. We find that our framework can effectively represents and addresses\ncomplex bibliographic information needs. Thus, the contributions of this paper\nare as follows: First, to our knowledge, it is the first attempt to propose a\nnatural language interface to graph-based bibliographic information retrieval.\nSecond, we propose a novel customized natural language processing framework\nthat integrates a few original algorithms/heuristics for interpreting and\nanalyzing natural language bibliographic queries. Third, we show that the\nproposed framework and natural language interface provide a practical solution\nin building real-world natural language interface-based bibliographic\ninformation retrieval systems. Our experimental results show that the presented\nsystem can correctly answer 39 out of 40 example natural language queries with\nvarying lengths and complexities.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 00:32:28 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Zhu", "Yongjun", ""], ["Yan", "Erjia", ""], ["Song", "Il-Yeol", ""]]}, {"id": "1612.03277", "submitter": "Seyed-Mehdi-Reza Beheshti", "authors": "Seyed-Mehdi-Reza Beheshti and Alireza Tabebordbar and Boualem\n  Benatallah and Reza Nouri", "title": "Data Curation APIs", "comments": null, "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201617", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and analyzing big data is firmly recognized as a powerful and\nstrategic priority. For deeper interpretation of and better intelligence with\nbig data, it is important to transform raw data (unstructured, semi-structured\nand structured data sources, e.g., text, video, image data sets) into curated\ndata: contextualized data and knowledge that is maintained and made available\nfor use by end-users and applications. In particular, data curation acts as the\nglue between raw data and analytics, providing an abstraction layer that\nrelieves users from time consuming, tedious and error prone curation tasks. In\nthis context, the data curation process becomes a vital analytics asset for\nincreasing added value and insights.\n  In this paper, we identify and implement a set of curation APIs and make them\navailable (on GitHub) to researchers and developers to assist them transforming\ntheir raw data into curated data. The curation APIs enable developers to easily\nadd features - such as extracting keyword, part of speech, and named entities\nsuch as Persons, Locations, Organizations, Companies, Products, Diseases,\nDrugs, etc.; providing synonyms and stems for extracted information items\nleveraging lexical knowledge bases for the English language such as WordNet;\nlinking extracted entities to external knowledge bases such as Google Knowledge\nGraph and Wikidata; discovering similarity among the extracted information\nitems, such as calculating similarity between string, number, date and time\ndata; classifying, sorting and categorizing data into various types, forms or\nany other distinct class; and indexing structured and unstructured data - into\ntheir applications.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 10:54:45 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Beheshti", "Seyed-Mehdi-Reza", ""], ["Tabebordbar", "Alireza", ""], ["Benatallah", "Boualem", ""], ["Nouri", "Reza", ""]]}, {"id": "1612.03316", "submitter": "Omar Alonso", "authors": "Omar Alonso", "title": "Label Visualization and Exploration in IR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a renaissance in visual analytics systems for data analysis and\nsharing, in particular, in the current wave of big data applications. We\nintroduce RAVE, a prototype that automates the generation of an interface that\nuses facets and visualization techniques for exploring and analyzing relevance\nassessments data sets collected via crowdsourcing. We present a technical\ndescription of the main components and demonstrate its use.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 16:33:06 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Alonso", "Omar", ""]]}, {"id": "1612.03365", "submitter": "Marc-Andr\\'e Carbonneau", "authors": "Marc-Andr\\'e Carbonneau, Veronika Cheplygina, Eric Granger and\n  Ghyslain Gagnon", "title": "Multiple Instance Learning: A Survey of Problem Characteristics and\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.10.009", "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is a form of weakly supervised learning\nwhere training instances are arranged in sets, called bags, and a label is\nprovided for the entire bag. This formulation is gaining interest because it\nnaturally fits various problems and allows to leverage weakly labeled data.\nConsequently, it has been used in diverse application fields such as computer\nvision and document classification. However, learning from bags raises\nimportant challenges that are unique to MIL. This paper provides a\ncomprehensive survey of the characteristics which define and differentiate the\ntypes of MIL problems. Until now, these problem characteristics have not been\nformally identified and described. As a result, the variations in performance\nof MIL algorithms from one data set to another are difficult to explain. In\nthis paper, MIL problem characteristics are grouped into four broad categories:\nthe composition of the bags, the types of data distribution, the ambiguity of\ninstance labels, and the task to be performed. Methods specialized to address\neach category are reviewed. Then, the extent to which these characteristics\nmanifest themselves in key MIL application areas are described. Finally,\nexperiments are conducted to compare the performance of 16 state-of-the-art MIL\nmethods on selected problem characteristics. This paper provides insight on how\nthe problem characteristics affect MIL algorithms, recommendations for future\nbenchmarking and promising avenues for research.\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 02:19:22 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carbonneau", "Marc-Andr\u00e9", ""], ["Cheplygina", "Veronika", ""], ["Granger", "Eric", ""], ["Gagnon", "Ghyslain", ""]]}, {"id": "1612.03597", "submitter": "Dat Quoc Nguyen", "authors": "Thanh Vu, Dat Quoc Nguyen, Mark Johnson, Dawei Song, Alistair Willis", "title": "Search Personalization with Embeddings", "comments": "In Proceedings of the 39th European Conference on Information\n  Retrieval, ECIR 2017, to appear", "journal-ref": null, "doi": "10.1007/978-3-319-56608-5_54", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that the performance of search personalization\ndepends on the richness of user profiles which normally represent the user's\ntopical interests. In this paper, we propose a new embedding approach to\nlearning user profiles, where users are embedded on a topical interest space.\nWe then directly utilize the user profiles for search personalization.\nExperiments on query logs from a major commercial web search engine demonstrate\nthat our embedding approach improves the performance of the search engine and\nalso achieves better search performance than other strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 10:27:31 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Vu", "Thanh", ""], ["Nguyen", "Dat Quoc", ""], ["Johnson", "Mark", ""], ["Song", "Dawei", ""], ["Willis", "Alistair", ""]]}, {"id": "1612.03639", "submitter": "Xiaopeng Li", "authors": "Xiaopeng Li, Ming Cheung and James She", "title": "Connection Discovery using Shared Images by Gaussian Relational Topic\n  Model", "comments": "IEEE International Conference on Big Data 2016", "journal-ref": null, "doi": "10.1109/BigData.2016.7840689", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social graphs, representing online friendships among users, are one of the\nfundamental types of data for many applications, such as recommendation,\nvirality prediction and marketing in social media. However, this data may be\nunavailable due to the privacy concerns of users, or kept private by social\nnetwork operators, which makes such applications difficult. Inferring user\ninterests and discovering user connections through their shared multimedia\ncontent has attracted more and more attention in recent years. This paper\nproposes a Gaussian relational topic model for connection discovery using user\nshared images in social media. The proposed model not only models user\ninterests as latent variables through their shared images, but also considers\nthe connections between users as a result of their shared images. It explicitly\nrelates user shared images to user connections in a hierarchical, systematic\nand supervisory way and provides an end-to-end solution for the problem. This\npaper also derives efficient variational inference and learning algorithms for\nthe posterior of the latent variables and model parameters. It is demonstrated\nthrough experiments with over 200k images from Flickr that the proposed method\nsignificantly outperforms the methods in previous works.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 12:10:28 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Li", "Xiaopeng", ""], ["Cheung", "Ming", ""], ["She", "James", ""]]}, {"id": "1612.03896", "submitter": "David Tsurel", "authors": "David Tsurel, Dan Pelleg, Ido Guy, Dafna Shahaf", "title": "Fun Facts: Automatic Trivia Fact Extraction from Wikipedia", "comments": "To appear in Proceedings of tenth ACM International Conference on Web\n  Search and Data Mining, WSDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant portion of web search queries directly refers to named\nentities. Search engines explore various ways to improve the user experience\nfor such queries. We suggest augmenting search results with {\\em trivia facts}\nabout the searched entity. Trivia is widely played throughout the world, and\nwas shown to increase users' engagement and retention.\n  Most random facts are not suitable for the trivia section. There is skill\n(and art) to curating good trivia. In this paper, we formalize a notion of\n\\emph{trivia-worthiness} and propose an algorithm that automatically mines\ntrivia facts from Wikipedia. We take advantage of Wikipedia's category\nstructure, and rank an entity's categories by their trivia-quality.\n  Our algorithm is capable of finding interesting facts, such as Obama's Grammy\nor Elvis' stint as a tank gunner. In user studies, our algorithm captures the\nintuitive notion of \"good trivia\" 45\\% higher than prior work. Search-page\ntests show a 22\\% decrease in bounce rates and a 12\\% increase in dwell time,\nproving our facts hold users' attention.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 20:51:21 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Tsurel", "David", ""], ["Pelleg", "Dan", ""], ["Guy", "Ido", ""], ["Shahaf", "Dafna", ""]]}, {"id": "1612.04118", "submitter": "pmeerkamp", "authors": "Philipp Meerkamp (Bloomberg LP) and Zhengyi Zhou (AT&T Labs Research)", "title": "Information Extraction with Character-level Neural Networks and Free\n  Noisy Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture for information extraction from text that augments\nan existing parser with a character-level neural network. The network is\ntrained using a measure of consistency of extracted data with existing\ndatabases as a form of noisy supervision. Our architecture combines the ability\nof constraint-based information extraction systems to easily incorporate domain\nknowledge and constraints with the ability of deep neural networks to leverage\nlarge amounts of data to learn complex features. Boosting the existing parser's\nprecision, the system led to large improvements over a mature and highly tuned\nconstraint-based production information extraction system used at Bloomberg for\nfinancial language text.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 12:12:20 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 01:01:28 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Meerkamp", "Philipp", "", "Bloomberg LP"], ["Zhou", "Zhengyi", "", "AT&T Labs Research"]]}, {"id": "1612.04403", "submitter": "Mason Bretan", "authors": "Mason Bretan", "title": "You Are What You Eat... Listen to, Watch, and Read", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a data driven method for deriving the relationship\nbetween personality and media preferences. A qunatifiable representation of\nsuch a relationship can be leveraged for use in recommendation systems and\nameliorate the \"cold start\" problem. Here, the data is comprised of an original\ncollection of 1,316 Okcupid dating profiles. Of these profiles, 800 are labeled\nwith one of 16 possible Myers-Briggs Type Indicators (MBTI). A personality\nspecific topic model describing a person's favorite books, movies, shows,\nmusic, and food was generated using latent Dirichlet allocation (LDA). There\nwere several significant findings, for example, intuitive thinking types\npreferred sci-fi/fantasy entertainment, extraversion correlated positively with\nupbeat dance music, and jazz, folk, and international cuisine correlated\npositively with those characterized by openness to experience. Many other\ncorrelations confirmed previous findings describing the relationship among\npersonality, writing style, and personal preferences. (For complete\nword/personality type assocations see the Appendix).\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 21:29:05 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Bretan", "Mason", ""]]}, {"id": "1612.04418", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Andrey Shutovich (Yandex,\n  Moscow, Russia), Philipp Pushnyakov (Yandex, Moscow, Russia), Evgeniy\n  Krokhalyov (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow, Russia),\n  Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "User Model-Based Intent-Aware Metrics for Multilingual Search Evaluation", "comments": "7 pages, 1 figure, 3 tables", "journal-ref": "NIPS 2016 Workshop \"What If? Inference and Learning of\n  Hypothetical and Counterfactual Interventions in Complex Systems\" (What If\n  2016) pre-print", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing importance of multilingual aspect of web search, no\nappropriate offline metrics to evaluate its quality are proposed so far. At the\nsame time, personal language preferences can be regarded as intents of a query.\nThis approach translates the multilingual search problem into a particular task\nof search diversification. Furthermore, the standard intent-aware approach\ncould be adopted to build a diversified metric for multilingual search on the\nbasis of a classical IR metric such as ERR. The intent-aware approach estimates\nuser satisfaction under a user behavior model. We show however that the\nunderlying user behavior models is not realistic in the multilingual case, and\nthe produced intent-aware metric do not appropriately estimate the user\nsatisfaction. We develop a novel approach to build intent-aware user behavior\nmodels, which overcome these limitations and convert to quality metrics that\nbetter correlate with standard online metrics of user satisfaction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 22:09:24 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Shutovich", "Andrey", "", "Yandex,\n  Moscow, Russia"], ["Pushnyakov", "Philipp", "", "Yandex, Moscow, Russia"], ["Krokhalyov", "Evgeniy", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow, Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1612.04883", "submitter": "Tara Safavi", "authors": "Yike Liu, Tara Safavi, Abhilash Dighe, Danai Koutra", "title": "Graph Summarization Methods and Applications: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While advances in computing resources have made processing enormous amounts\nof data possible, human ability to identify patterns in such data has not\nscaled accordingly. Efficient computational methods for condensing and\nsimplifying data are thus becoming vital for extracting actionable insights. In\nparticular, while data summarization techniques have been studied extensively,\nonly recently has summarizing interconnected data, or graphs, become popular.\nThis survey is a structured, comprehensive overview of the state-of-the-art\nmethods for summarizing graph data. We first broach the motivation behind, and\nthe challenges of, graph summarization. We then categorize summarization\napproaches by the type of graphs taken as input and further organize each\ncategory by core methodology. Finally, we discuss applications of summarization\non real-world graphs and conclude by describing some open problems in the\nfield.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 23:39:45 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 03:32:02 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 18:10:26 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Liu", "Yike", ""], ["Safavi", "Tara", ""], ["Dighe", "Abhilash", ""], ["Koutra", "Danai", ""]]}, {"id": "1612.04978", "submitter": "EPTCS", "authors": "Ladislav Peska (Charles University in Prague, Faculty of Mathematics\n  and Physics)", "title": "Using the Context of User Feedback in Recommender Systems", "comments": "In Proceedings MEMICS 2016, arXiv:1612.04037", "journal-ref": "EPTCS 233, 2016, pp. 1-12", "doi": "10.4204/EPTCS.233.1", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work is generally focused on recommending for small or medium-sized\ne-commerce portals, where explicit feedback is absent and thus the usage of\nimplicit feedback is necessary. Nonetheless, for some implicit feedback\nfeatures, the presentation context may be of high importance. In this paper, we\npresent a model of relevant contextual features affecting user feedback,\npropose methods leveraging those features, publish a dataset of real e-commerce\nusers containing multiple user feedback indicators as well as its context and\nfinally present results of purchase prediction and recommendation experiments.\nOff-line experiments with real users of a Czech travel agency website\ncorroborated the importance of leveraging presentation context in both purchase\nprediction and recommendation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 08:49:50 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Peska", "Ladislav", "", "Charles University in Prague, Faculty of Mathematics\n  and Physics"]]}, {"id": "1612.05070", "submitter": "Matthias Dorfer", "authors": "Matthias Dorfer, Andreas Arzt, Gerhard Widmer", "title": "Towards End-to-End Audio-Sheet-Music Retrieval", "comments": "In NIPS 2016 End-to-end Learning for Speech and Audio Processing\n  Workshop, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the feasibility of learning to retrieve short\nsnippets of sheet music (images) when given a short query excerpt of music\n(audio) -- and vice versa --, without any symbolic representation of music or\nscores. This would be highly useful in many content-based musical retrieval\nscenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)\nand learns correlated latent spaces allowing for cross-modality retrieval in\nboth directions. Initial experiments with relatively simple monophonic music\nshow promising results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:07:51 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dorfer", "Matthias", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1612.05413", "submitter": "Gerhard Gossen", "authors": "Gerhard Gossen and Elena Demidova and Thomas Risse", "title": "Analyzing Web Archives Through Topic and Event Focused Sub-collections", "comments": "Published in the proceedings of the 8th ACM Conference on Web Science\n  2016", "journal-ref": "Proceedings of the 8th ACM Conference on Web Science (2016, pp.\n  291--295)", "doi": "10.1145/2908131.2908175", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives capture the history of the Web and are therefore an important\nsource to study how societal developments have been reflected on the Web.\nHowever, the large size of Web archives and their temporal nature pose many\nchallenges to researchers interested in working with these collections. In this\nwork, we describe the challenges of working with Web archives and propose the\nresearch methodology of extracting and studying sub-collections of the archive\nfocused on specific topics and events. We discuss the opportunities and\nchallenges of this approach and suggest a framework for creating\nsub-collections.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 10:10:16 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gossen", "Gerhard", ""], ["Demidova", "Elena", ""], ["Risse", "Thomas", ""]]}, {"id": "1612.05729", "submitter": "Mirko Polato", "authors": "Mirko Polato and Fabio Aiolli", "title": "Exploiting sparsity to build efficient kernel based collaborative\n  filtering for top-N item recommendation", "comments": "Under revision for Neurocomputing (Elsevier Journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of implicit feedback datasets has raised the\ninterest in developing effective collaborative filtering techniques able to\ndeal asymmetrically with unambiguous positive feedback and ambiguous negative\nfeedback. In this paper, we propose a principled kernel-based collaborative\nfiltering method for top-N item recommendation with implicit feedback. We\npresent an efficient implementation using the linear kernel, and we show how to\ngeneralize it to kernels of the dot product family preserving the efficiency.\nWe also investigate on the elements which influence the sparsity of a standard\ncosine kernel. This analysis shows that the sparsity of the kernel strongly\ndepends on the properties of the dataset, in particular on the long tail\ndistribution. We compare our method with state-of-the-art algorithms achieving\ngood results both in terms of efficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 10:50:41 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "1612.06136", "submitter": "Nuno Moniz", "authors": "Nuno Moniz, Lu\\'is Torgo, Jo\\~ao Vinagre", "title": "Data-Driven Relevance Judgments for Ranking Evaluation", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ranking evaluation metrics are a fundamental element of design and\nimprovement efforts in information retrieval. We observe that most popular\nmetrics disregard information portrayed in the scores used to derive rankings,\nwhen available. This may pose a numerical scaling problem, causing an under- or\nover-estimation of the evaluation depending on the degree of divergence between\nthe scores of ranked items. The purpose of this work is to propose a principled\nway of quantifying multi-graded relevance judgments of items and enable a more\naccurate penalization of ordering errors in rankings. We propose a data-driven\ngeneration of relevance functions based on the degree of the divergence amongst\na set of items' scores and its application in the evaluation metric Normalized\nDiscounted Cumulative Gain (nDCG). We use synthetic data to demonstrate the\ninterest of our proposal and a combination of data on news items from Google\nNews and their respective popularity in Twitter to show its performance in\ncomparison to the standard nDCG. Results show that our proposal is capable of\nproviding a more fine-grained evaluation of rankings when compared to the\nstandard nDCG, and that the latter frequently under- or over-estimates its\nevaluation scores in light of the divergence of items' scores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 11:48:53 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Moniz", "Nuno", ""], ["Torgo", "Lu\u00eds", ""], ["Vinagre", "Jo\u00e3o", ""]]}, {"id": "1612.06162", "submitter": "Gerhard Gossen", "authors": "Gerhard Gossen, Elena Demidova and Thomas Risse", "title": "The iCrawl Wizard -- Supporting Interactive Focused Crawl Specification", "comments": "Published in the Proceedings of the European Conference on\n  Information Retrieval (ECIR) 2015", "journal-ref": null, "doi": "10.1007/978-3-319-16354-3_88", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collections of Web documents about specific topics are needed for many areas\nof current research. Focused crawling enables the creation of such collections\non demand. Current focused crawlers require the user to manually specify\nstarting points for the crawl (seed URLs). These are also used to describe the\nexpected topic of the collection. The choice of seed URLs influences the\nquality of the resulting collection and requires a lot of expertise. In this\ndemonstration we present the iCrawl Wizard, a tool that assists users in\ndefining focused crawls efficiently and semi-automatically. Our tool uses major\nsearch engines and Social Media APIs as well as information extraction\ntechniques to find seed URLs and a semantic description of the crawl intent.\nUsing the iCrawl Wizard even non-expert users can create semantic\nspecifications for focused crawlers interactively and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 13:09:15 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gossen", "Gerhard", ""], ["Demidova", "Elena", ""], ["Risse", "Thomas", ""]]}, {"id": "1612.06195", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truic\\u{a}, J\\'er\\^ome Darmont (ERIC), Julien Velcin\n  (ERIC)", "title": "A Scalable Document-based Architecture for Text Analysis", "comments": null, "journal-ref": "12th International Conference on Advanced Data Mining and\n  Applications (ADMA 2016), Dec 2016, Gold Coast, Australia. Springer, 10086,\n  pp.481-494, 2016, Lecture Notes in Artificial Intelligence", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing textual data is a very challenging task because of the huge volume\nof data generated daily. Fundamental issues in text analysis include the lack\nof structure in document datasets, the need for various preprocessing steps\n%(e.g., stem or lemma extraction, part-of-speech tagging, named entities\nrecognition...), and performance and scaling issues. Existing text analysis\narchitectures partly solve these issues, providing restrictive data schemas,\naddressing only one aspect of text preprocessing and focusing on one single\ntask when dealing with performance optimization. %As a result, no definite\nsolution is currently available. Thus, we propose in this paper a new generic\ntext analysis architecture, where document structure is flexible, many\npreprocessing techniques are integrated and textual datasets are indexed for\nefficient access. We implement our conceptual architecture using both a\nrelational and a document-oriented database. Our experiments demonstrate the\nfeasibility of our approach and the superiority of the document-oriented\nlogical and physical implementation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:24:23 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Velcin", "Julien", "", "ERIC"]]}, {"id": "1612.06202", "submitter": "Gerhard Gossen", "authors": "Gerhard Gossen, Elena Demidova, Thomas Risse", "title": "iCrawl: Improving the Freshness of Web Collections by Integrating Social\n  Web and Focused Web Crawling", "comments": "Published in the Proceedings of the 15th ACM/IEEE-CS Joint Conference\n  on Digital Libraries 2015", "journal-ref": "Proceedings of the 15th ACM/IEEE-CS Joint Conference on Digital\n  Libraries (pp. 75--84) (2015)", "doi": "10.1145/2756406.2756925", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the Digital Humanities and journalists need to monitor,\ncollect and analyze fresh online content regarding current events such as the\nEbola outbreak or the Ukraine crisis on demand. However, existing focused\ncrawling approaches only consider topical aspects while ignoring temporal\naspects and therefore cannot achieve thematically coherent and fresh Web\ncollections. Especially Social Media provide a rich source of fresh content,\nwhich is not used by state-of-the-art focused crawlers. In this paper we\naddress the issues of enabling the collection of fresh and relevant Web and\nSocial Web content for a topic of interest through seamless integration of Web\nand Social Media in a novel integrated focused crawler. The crawler collects\nWeb and Social Media content in a single system and exploits the stream of\nfresh Social Media content for guiding the crawler.\n", "versions": [{"version": "v1", "created": "Mon, 19 Dec 2016 14:33:21 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Gossen", "Gerhard", ""], ["Demidova", "Elena", ""], ["Risse", "Thomas", ""]]}, {"id": "1612.06738", "submitter": "Sujit Kumar Sahoo Ph.D.", "authors": "Sujit Kumar Sahoo", "title": "Local Sparse Approximation for Image Restoration with Adaptive Block\n  Size Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of image restoration (denoising and inpainting) is\napproached using sparse approximation of local image blocks. The local image\nblocks are extracted by sliding square windows over the image. An adaptive\nblock size selection procedure for local sparse approximation is proposed,\nwhich affects the global recovery of underlying image. Ideally the adaptive\nlocal block selection yields the minimum mean square error (MMSE) in recovered\nimage. This framework gives us a clustered image based on the selected block\nsize, then each cluster is restored separately using sparse approximation. The\nresults obtained using the proposed framework are very much comparable with the\nrecently proposed image restoration techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:28:48 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Sahoo", "Sujit Kumar", ""]]}, {"id": "1612.06753", "submitter": "Spencer Cappallo", "authors": "Spencer Cappallo, Thomas Mensink, Cees G. M. Snoek", "title": "Video Stream Retrieval of Unseen Queries using Semantic Memory", "comments": "Presented at BMVC 2016, British Machine Vision Conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval of live, user-broadcast video streams is an under-addressed and\nincreasingly relevant challenge. The on-line nature of the problem requires\ntemporal evaluation and the unforeseeable scope of potential queries motivates\nan approach which can accommodate arbitrary search queries. To account for the\nbreadth of possible queries, we adopt a no-example approach to query retrieval,\nwhich uses a query's semantic relatedness to pre-trained concept classifiers.\nTo adapt to shifting video content, we propose memory pooling and memory\nwelling methods that favor recent information over long past content. We\nidentify two stream retrieval tasks, instantaneous retrieval at any particular\ntime and continuous retrieval over a prolonged duration, and propose means for\nevaluating them. Three large scale video datasets are adapted to the challenge\nof stream retrieval. We report results for our search methods on the new stream\nretrieval tasks, as well as demonstrate their efficacy in a traditional,\nnon-streaming video task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 16:59:24 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Cappallo", "Spencer", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1612.06935", "submitter": "Xingzhong Du", "authors": "Xingzhong Du, Hongzhi Yin, Ling Chen, Yang Wang, Yi Yang, Xiaofang\n  Zhou", "title": "Personalized Video Recommendation Using Rich Contents from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video recommendation has become an essential way of helping people explore\nthe massive videos and discover the ones that may be of interest to them. In\nthe existing video recommender systems, the models make the recommendations\nbased on the user-video interactions and single specific content features. When\nthe specific content features are unavailable, the performance of the existing\nmodels will seriously deteriorate. Inspired by the fact that rich contents\n(e.g., text, audio, motion, and so on) exist in videos, in this paper, we\nexplore how to use these rich contents to overcome the limitations caused by\nthe unavailability of the specific ones. Specifically, we propose a novel\ngeneral framework that incorporates arbitrary single content feature with\nuser-video interactions, named as collaborative embedding regression (CER)\nmodel, to make effective video recommendation in both in-matrix and\nout-of-matrix scenarios. Our extensive experiments on two real-world\nlarge-scale datasets show that CER beats the existing recommender models with\nany single content feature and is more time efficient. In addition, we propose\na priority-based late fusion (PRI) method to gain the benefit brought by the\nintegrating the multiple content features. The corresponding experiment shows\nthat PRI brings real performance improvement to the baseline and outperforms\nthe existing fusion methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 01:01:49 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 00:55:16 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 12:05:27 GMT"}, {"version": "v4", "created": "Tue, 27 Jun 2017 01:42:29 GMT"}, {"version": "v5", "created": "Mon, 24 Jul 2017 08:12:58 GMT"}, {"version": "v6", "created": "Wed, 5 Dec 2018 03:56:00 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Du", "Xingzhong", ""], ["Yin", "Hongzhi", ""], ["Chen", "Ling", ""], ["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1612.07025", "submitter": "Mirko Polato", "authors": "Mirko Polato and Fabio Aiolli", "title": "Boolean kernels for collaborative filtering in top-N item recommendation", "comments": "24 pages, 28 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many personalized recommendation problems available data consists only of\npositive interactions (implicit feedback) between users and items. This problem\nis also known as One-Class Collaborative Filtering (OC-CF). Linear models\nusually achieve state-of-the-art performances on OC-CF problems and many\nefforts have been devoted to build more expressive and complex representations\nable to improve the recommendations. Recent analysis show that collaborative\nfiltering (CF) datasets have peculiar characteristics such as high sparsity and\na long tailed distribution of the ratings. In this paper we propose a boolean\nkernel, called Disjunctive kernel, which is less expressive than the linear one\nbut it is able to alleviate the sparsity issue in CF contexts. The embedding of\nthis kernel is composed by all the combinations of a certain arity d of the\ninput variables, and these combined features are semantically interpreted as\ndisjunctions of the input variables. Experiments on several CF datasets show\nthe effectiveness and the efficiency of the proposed kernel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 09:29:09 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 10:16:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "1612.07040", "submitter": "Ze Hu", "authors": "Ze Hu, Zhan Zhang, Qing Chen, Haiqin Yang, Decheng Zuo", "title": "A deep learning approach for predicting the quality of online health\n  expert question-answering services", "comments": "Submitted to Journal of Biomedical Informatics journal on Dec 10,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, a growing number of health consumers are asking health-related\nquestions online, at any time and from anywhere, which effectively lowers the\ncost of health care. The most common approach is using online health expert\nquestion-answering (HQA) services, as health consumers are more willing to\ntrust answers from professional physicians. However, these answers can be of\nvarying quality depending on circumstance. In addition, as the available HQA\nservices grow, how to predict the answer quality of HQA services via machine\nlearning becomes increasingly important and challenging. In an HQA service,\nanswers are normally short texts, which are severely affected by the data\nsparsity problem. Furthermore, HQA services lack community features such as\nbest answer and user votes. Therefore, the wisdom of the crowd is not available\nto rate answer quality. To address these problems, in this paper, the\nprediction of HQA answer quality is defined as a classification task. First,\nbased on the characteristics of HQA services and feedback from medical experts,\na standard for HQA service answer quality evaluation is defined. Next, based on\nthe characteristics of HQA services, several novel non-textual features are\nproposed, including surface linguistic features and social features. Finally, a\ndeep belief network (DBN)-based HQA answer quality prediction framework is\nproposed to predict the quality of answers by learning the high-level hidden\nsemantic representation from the physicians' answers. Our results prove that\nthe proposed framework overcomes the problem of overly sparse textual features\nin short text answers and effectively identifies high-quality answers.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 10:09:30 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Hu", "Ze", ""], ["Zhang", "Zhan", ""], ["Chen", "Qing", ""], ["Yang", "Haiqin", ""], ["Zuo", "Decheng", ""]]}, {"id": "1612.07117", "submitter": "Nam Khanh Tran", "authors": "Nam Khanh Tran", "title": "Classification and Learning-to-rank Approaches for Cross-Device Matching\n  at CIKM Cup 2016", "comments": "CIKM Cup 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two methods for tackling the problem of\ncross-device matching for online advertising at CIKM Cup 2016. The first method\nconsiders the matching problem as a binary classification task and solve it by\nutilizing ensemble learning techniques. The second method defines the matching\nproblem as a ranking task and effectively solve it with using learning-to-rank\nalgorithms. The results show that the proposed methods obtain promising\nresults, in which the ranking-based method outperforms the classification-based\nmethod for the task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 15:02:41 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Tran", "Nam Khanh", ""]]}, {"id": "1612.07688", "submitter": "Safeeullah Soomro", "authors": "Nareena Soomro, Safeeulah Soomro, Zainab Alansari, Suhni Abbasi,\n  Mohammad Riyaz Belgaum, Abdul Baqi Khakwani", "title": "Development of UMLS Based Health Care Web Services for Android Platform", "comments": null, "journal-ref": "Sindh Univ. Res. Jour. (Sci. Ser.) Vol. 48 (4D) 05-08 (2016)", "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this fast developing world of information, the amount of medical knowledge\nis rising at an exponential level. The UMLS (Unified Medical Language Systems),\nis rich knowledge base consisting files and software that provides many health\nand biomedical vocabularies and standards. A Web service is a web solution to\nfacilitate machine-to-machine interaction over a network. Few UMLS web services\nare currently available for portable devices, but most of them lack in\nefficiency and performance. It is proposed to develop Android-based web\nservices for healthcare systems underlying rich knowledge source of UMLS. The\nexperimental evaluation was made to analyse the efficiency and performance\neffect with and without using the designed prototype. The understand-ability\nand interaction with the prototype were greater than those who used the\nalternate sources to obtain the answers to their questions. The overall\nperformance indicates that the system is convenient and easy to use. The result\nof the evaluation clearly proved that designed system retrieves all the\npertinent information better than syntactic searches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 16:38:43 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Soomro", "Nareena", ""], ["Soomro", "Safeeulah", ""], ["Alansari", "Zainab", ""], ["Abbasi", "Suhni", ""], ["Belgaum", "Mohammad Riyaz", ""], ["Khakwani", "Abdul Baqi", ""]]}, {"id": "1612.07843", "submitter": "Wojciech Samek", "authors": "Leila Arras, Franziska Horn, Gr\\'egoire Montavon, Klaus-Robert\n  M\\\"uller, Wojciech Samek", "title": "\"What is Relevant in a Text Document?\": An Interpretable Machine\n  Learning Approach", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0181142", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text documents can be described by a number of abstract concepts such as\nsemantic category, writing style, or sentiment. Machine learning (ML) models\nhave been trained to automatically map documents to these abstract concepts,\nallowing to annotate very large text collections, more than could be processed\nby a human in a lifetime. Besides predicting the text's category very\naccurately, it is also highly desirable to understand how and why the\ncategorization process takes place. In this paper, we demonstrate that such\nunderstanding can be achieved by tracing the classification decision back to\nindividual words using layer-wise relevance propagation (LRP), a recently\ndeveloped technique for explaining predictions of complex non-linear\nclassifiers. We train two word-based ML models, a convolutional neural network\n(CNN) and a bag-of-words SVM classifier, on a topic categorization task and\nadapt the LRP method to decompose the predictions of these models onto words.\nResulting scores indicate how much individual words contribute to the overall\nclassification decision. This enables one to distill relevant information from\ntext documents without an explicit semantic information extraction step. We\nfurther use the word-wise relevance scores for generating novel vector-based\ndocument representations which capture semantic information. Based on these\ndocument vectors, we introduce a measure of model explanatory power and show\nthat, although the SVM and CNN models perform similarly in terms of\nclassification accuracy, the latter exhibits a higher level of explainability\nwhich makes it more comprehensible for humans and potentially more useful for\nother applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 00:31:30 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1612.08178", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar, Debanjan Das, Indra Banerjee, Mamta Kumari and Prasenjit\n  Biswas", "title": "JU_KS_Group@FIRE 2016: Consumer Health Information Search", "comments": "8th meeting of Forum for Information Retrieval Evaluation 2016, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the methodology used and the results obtained by\nus for completing the tasks given under the shared task on Consumer Health\nInformation Search (CHIS) collocated with the Forum for Information Retrieval\nEvaluation (FIRE) 2016, ISI Kolkata. The shared task consists of two sub-tasks\n- (1) task1: given a query and a document/set of documents associated with that\nquery, the task is to classify the sentences in the document as relevant to the\nquery or not and (2) task 2: the relevant sentences need to be further\nclassified as supporting the claim made in the query, or opposing the claim\nmade in the query. We have participated in both the sub-tasks. The percentage\naccuracy obtained by our developed system for task1 was 73.39 which is third\nhighest among the 9 teams participated in the shared task.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 13:30:23 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Sarkar", "Kamal", ""], ["Das", "Debanjan", ""], ["Banerjee", "Indra", ""], ["Kumari", "Mamta", ""], ["Biswas", "Prasenjit", ""]]}, {"id": "1612.08391", "submitter": "Aggelos Pikrakis", "authors": "Giannis Karamanolakis, Elias Iosif, Athanasia Zlatintsi, Aggelos\n  Pikrakis and Alexandros Potamianos", "title": "Audio-based Distributional Semantic Models for Music Auto-tagging and\n  Similarity Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of Audio-based Distributional Semantic Models (ADSMs)\nenables the computation of audio and lexical vector representations in a joint\nacoustic-semantic space. In this work, these joint representations are applied\nto the problem of automatic tag generation. The predicted tags together with\ntheir corresponding acoustic representation are exploited for the construction\nof acoustic-semantic clip embeddings. The proposed algorithms are evaluated on\nthe task of similarity measurement between music clips. Acoustic-semantic\nmodels are shown to outperform the state-of-the-art for this task and produce\nhigh quality tags for audio/music clips.\n", "versions": [{"version": "v1", "created": "Mon, 26 Dec 2016 14:35:51 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Iosif", "Elias", ""], ["Zlatintsi", "Athanasia", ""], ["Pikrakis", "Aggelos", ""], ["Potamianos", "Alexandros", ""]]}, {"id": "1612.08543", "submitter": "Amir Hossein Akhavan Rahnama", "authors": "Amir Hossein Akhavan Rahnama", "title": "Distributed Real-Time Sentiment Analysis for Big Data Social Streams", "comments": null, "journal-ref": "IEEE 2014 International Conference on Control, Decision and\n  Information Technologies (CoDIT)", "doi": "10.1109/CoDIT.2014.6996998", "report-no": null, "categories": "stat.ML cs.CL cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data trend has enforced the data-centric systems to have continuous fast\ndata streams. In recent years, real-time analytics on stream data has formed\ninto a new research field, which aims to answer queries about\nwhat-is-happening-now with a negligible delay. The real challenge with\nreal-time stream data processing is that it is impossible to store instances of\ndata, and therefore online analytical algorithms are utilized. To perform\nreal-time analytics, pre-processing of data should be performed in a way that\nonly a short summary of stream is stored in main memory. In addition, due to\nhigh speed of arrival, average processing time for each instance of data should\nbe in such a way that incoming instances are not lost without being captured.\nLastly, the learner needs to provide high analytical accuracy measures.\nSentinel is a distributed system written in Java that aims to solve this\nchallenge by enforcing both the processing and learning process to be done in\ndistributed form. Sentinel is built on top of Apache Storm, a distributed\ncomputing platform. Sentinels learner, Vertical Hoeffding Tree, is a parallel\ndecision tree-learning algorithm based on the VFDT, with ability of enabling\nparallel classification in distributed environments. Sentinel also uses\nSpaceSaving to keep a summary of the data stream and stores its summary in a\nsynopsis data structure. Application of Sentinel on Twitter Public Stream API\nis shown and the results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 09:10:18 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Rahnama", "Amir Hossein Akhavan", ""]]}, {"id": "1612.08644", "submitter": "Anubhav Gupta", "authors": "Anubhav Gupta, M. Narasimha Murty", "title": "Finding Influential Institutions in Bibliographic Information Networks", "comments": "KDD Cup Workshop, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking in bibliographic information networks is a widely studied problem due\nto its many applications such as advertisement industry, funding, search\nengines, etc. Most of the existing works on ranking in bibliographic\ninformation network are based on ranking of research papers and their authors.\nBut the bibliographic information network can be used for solving other\nimportant problems as well. The KDD Cup $2016$ competition considers one such\nproblem, which is to measure the impact of research institutions, i.e. to\nperform ranking of research institutions. The competition took place in three\nphases. In this paper, we discuss our solutions for ranking institutions in\neach phase. We participated under team name \"anu@TASL\" and our solutions\nachieved the average NDCG@$20$ score of $0.7483$, ranking in eleventh place in\nthe contest.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 14:39:23 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Gupta", "Anubhav", ""], ["Murty", "M. Narasimha", ""]]}, {"id": "1612.09062", "submitter": "Chao-Hsuan Ke", "authors": "Chao-Hsuan Ke, Tsung-Lu Michael Lee and Jung-Hsien Chiang", "title": "Condensedly: comprehending article contents through condensed texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: Abstracts in biomedical articles can provide a quick overview of the\narticles but detailed information cannot be obtained without reading full-text\ncontents. Full-text articles certainly generate more information and contents;\nhowever, accessing full-text documents is usually time consuming. Condensedly\nis a web-based application, which provides readers an easy and efficient way to\naccess full-text paragraphs using sentences in abstracts as fishing bait to\nretrieve the big fish reside in full-text. Condensedly is based on the\nparagraph ranking algorithm, which evaluates and ranks full-text paragraphs\nbased on their association scores with sentences in abstracts.\n  Availability: http://140.116.247.185/~research/Condensedly\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 07:56:09 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ke", "Chao-Hsuan", ""], ["Lee", "Tsung-Lu Michael", ""], ["Chiang", "Jung-Hsien", ""]]}, {"id": "1612.09535", "submitter": "Concei\\c{c}\\~ao Rocha", "authors": "Concei\\c{c}\\~ao Rocha, Al\\'ipio Jorge, Roberta Sionara, Paula Brito,\n  Carlos Pimenta and Solange Rezende", "title": "PAMPO: using pattern matching and pos-tagging for effective Named\n  Entities recognition in Portuguese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the entity extraction task (named entity recognition)\nof a text mining process that aims at unveiling non-trivial semantic\nstructures, such as relationships and interaction between entities or\ncommunities. In this paper we present a simple and efficient named entity\nextraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging\nbased algorithm for NER), relies on flexible pattern matching, part-of-speech\ntagging and lexical-based rules. It was developed to process texts written in\nPortuguese, however it is potentially applicable to other languages as well.\n  We compare our approach with current alternatives that support Named Entity\nRecognition (NER) for content written in Portuguese. These are Alchemy, Zemanta\nand Rembrandt. Evaluation of the efficacy of the entity extraction method on\nseveral texts written in Portuguese indicates a considerable improvement on\n$recall$ and $F_1$ measures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 17:10:29 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Rocha", "Concei\u00e7\u00e3o", ""], ["Jorge", "Al\u00edpio", ""], ["Sionara", "Roberta", ""], ["Brito", "Paula", ""], ["Pimenta", "Carlos", ""], ["Rezende", "Solange", ""]]}, {"id": "1612.09572", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "FolksoDrivenCloud: an annotation and process application for social\n  collaborative networking", "comments": "9 pages, 3 figures; for details see: http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the FolksoDriven Cloud (FDC) built on Cloud and on\nSemantic technologies. Cloud computing has emerged in these recent years as the\nnew paradigm for the provision of on-demand distributed computing resources.\nSemantic Web can be used for relationship between different data and\ndescriptions of services to annotate provenance of repositories on ontologies.\nThe FDC service is composed of a back-end which submits and monitors the\ndocuments, and a user front-end which allows users to schedule on-demand\noperations and to watch the progress of running processes. The impact of the\nproposed method is illustrated on a user since its inception.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 19:47:38 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1612.09574", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Automatic Data Deformation Analysis on Evolving Folksonomy Driven\n  Environment", "comments": "8 pages, 3 figures; 2 tables; for details see:\n  http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Folksodriven framework makes it possible for data scientists to define an\nontology environment where searching for buried patterns that have some kind of\npredictive power to build predictive models more effectively. It accomplishes\nthis through an abstractions that isolate parameters of the predictive modeling\nprocess searching for patterns and designing the feature set, too. To reflect\nthe evolving knowledge, this paper considers ontologies based on folksonomies\naccording to a new concept structure called \"Folksodriven\" to represent\nfolksonomies. So, the studies on the transformational regulation of the\nFolksodriven tags are regarded to be important for adaptive folksonomies\nclassifications in an evolving environment used by Intelligent Systems to\nrepresent the knowledge sharing. Folksodriven tags are used to categorize\nsalient data points so they can be fed to a machine-learning system and\n\"featurizing\" the data.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 19:52:09 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}]