[{"id": "1808.00004", "submitter": "Kaige Yang Mr", "authors": "Kaige Yang and Laura Toni", "title": "Graph-Based Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study recommendation systems modelled as contextual\nmulti-armed bandit (MAB) problems. We propose a graph-based recommendation\nsystem that learns and exploits the geometry of the user space to create\nmeaningful clusters in the user domain. This reduces the dimensionality of the\nrecommendation problem while preserving the accuracy of MAB. We then study the\neffect of graph sparsity and clusters size on the MAB performance and provide\nexhaustive simulation results both in synthetic and in real-case datasets.\nSimulation results show improvements with respect to state-of-the-art MAB\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:16:54 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Yang", "Kaige", ""], ["Toni", "Laura", ""]]}, {"id": "1808.00076", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza P. Moreira, Felipe Ferreira, Adilson Marques da Cunha", "title": "News Session-Based Recommendations using Deep Neural Networks", "comments": "Accepted for the Third Workshop on Deep Learning for Recommender\n  Systems - DLRS 2018, October 02-07, 2018, Vancouver, Canada.\n  https://recsys.acm.org/recsys18/dlrs/", "journal-ref": null, "doi": "10.1145/3270323.3270328", "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommender systems are aimed to personalize users experiences and help\nthem to discover relevant articles from a large and dynamic search space.\nTherefore, news domain is a challenging scenario for recommendations, due to\nits sparse user profiling, fast growing number of items, accelerated item's\nvalue decay, and users preferences dynamic shift. Some promising results have\nbeen recently achieved by the usage of Deep Learning techniques on Recommender\nSystems, specially for item's feature extraction and for session-based\nrecommendations with Recurrent Neural Networks. In this paper, it is proposed\nan instantiation of the CHAMELEON -- a Deep Learning Meta-Architecture for News\nRecommender Systems. This architecture is composed of two modules, the first\nresponsible to learn news articles representations, based on their text and\nmetadata, and the second module aimed to provide session-based recommendations\nusing Recurrent Neural Networks. The recommendation task addressed in this work\nis next-item prediction for users sessions: \"what is the next most likely\narticle a user might read in a session?\" Users sessions context is leveraged by\nthe architecture to provide additional information in such extreme cold-start\nscenario of news recommendation. Users' behavior and item features are both\nmerged in an hybrid recommendation approach. A temporal offline evaluation\nmethod is also proposed as a complementary contribution, for a more realistic\nevaluation of such task, considering dynamic factors that affect global\nreadership interests like popularity, recency, and seasonality. Experiments\nwith an extensive number of session-based recommendation methods were performed\nand the proposed instantiation of CHAMELEON meta-architecture obtained a\nsignificant relative improvement in top-n accuracy and ranking metrics (10% on\nHit Rate and 13% on MRR) over the best benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 21:15:54 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 01:02:00 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 03:09:58 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Moreira", "Gabriel de Souza P.", ""], ["Ferreira", "Felipe", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "1808.00103", "submitter": "Paul Sheridan", "authors": "Paul Sheridan, Mikael Onsj\\\"o, Claudia Becerra, Sergio Jimenez, and\n  George Due\\~nas", "title": "An Ontology-Based Recommender System with an Application to the Star\n  Trek Television Franchise", "comments": "25 pages, 6 figures, 5 tables, minor revisions", "journal-ref": "Future Internet 2019, 11(9), 182", "doi": "10.3390/fi11090182", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering based recommender systems have proven to be extremely\nsuccessful in settings where user preference data on items is abundant.\nHowever, collaborative filtering algorithms are hindered by their weakness\nagainst the item cold-start problem and general lack of interpretability.\nOntology-based recommender systems exploit hierarchical organizations of users\nand items to enhance browsing, recommendation, and profile construction. While\nontology-based approaches address the shortcomings of their collaborative\nfiltering counterparts, ontological organizations of items can be difficult to\nobtain for items that mostly belong to the same category (e.g., television\nseries episodes). In this paper, we present an ontology-based recommender\nsystem that integrates the knowledge represented in a large ontology of\nliterary themes to produce fiction content recommendations. The main novelty of\nthis work is an ontology-based method for computing similarities between items\nand its integration with the classical Item-KNN (K-nearest neighbors)\nalgorithm. As a study case, we evaluated the proposed method against other\napproaches by performing the classical rating prediction task on a collection\nof Star Trek television series episodes in an item cold-start scenario. This\ntransverse evaluation provides insights into the utility of different\ninformation resources and methods for the initial stages of recommender system\ndevelopment. We found our proposed method to be a convenient alternative to\ncollaborative filtering approaches for collections of mostly similar items,\nparticularly when other content-based approaches are not applicable or\notherwise unavailable. Aside from the new methods, this paper contributes a\ntestbed for future research and an online framework to collaboratively extend\nthe ontology of literary themes to cover other narrative content.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 22:53:30 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 00:34:39 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 00:20:35 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Sheridan", "Paul", ""], ["Onsj\u00f6", "Mikael", ""], ["Becerra", "Claudia", ""], ["Jimenez", "Sergio", ""], ["Due\u00f1as", "George", ""]]}, {"id": "1808.00239", "submitter": "Rohan Kumar", "authors": "Rohan Kumar, Mohit Kumar, Neil Shah, Christos Faloutsos", "title": "Did We Get It Right? Predicting Query Performance in E-commerce Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of evaluating whether results served by\nan e-commerce search engine for a query are good or not. This is a critical\nquestion in evaluating any e-commerce search engine. While this question is\ntraditionally answered using simple metrics like query click-through rate\n(CTR), we observe that in e-commerce search, such metrics can be misleading.\nUpon inspection, we find cases where CTR is high but the results are poor and\nvice versa. Similar cases exist for other metrics like time to click which are\noften also used for evaluating search engines. We aim to learn the quality of\nthe results served by the search engine based on users' interactions with the\nresults. Although this problem has been studied in the web search context, this\nis the first study for e-commerce search, to the best of our knowledge. Despite\ncertain commonalities with evaluating web search engines, there are several\nmajor differences such as underlying reasons for search failure, and\navailability of rich user interaction data with products (e.g. adding a product\nto the cart). We study large-scale user interaction logs from Flipkart's search\nengine, analyze behavioral patterns and build models to classify queries based\non user behavior signals. We demonstrate the feasibility and efficacy of such\nmodels in accurately predicting query performance. Our classifier is able to\nachieve an average AUC of 0.75 on a held-out test set.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 09:18:36 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kumar", "Rohan", ""], ["Kumar", "Mohit", ""], ["Shah", "Neil", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1808.00337", "submitter": "Miklas S. Kristoffersen", "authors": "Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan", "title": "The Importance of Context When Recommending TV Content: Dataset and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TMM.2019.2944214", "report-no": null, "categories": "cs.IR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home entertainment systems feature in a variety of usage scenarios with one\nor more simultaneous users, for whom the complexity of choosing media to\nconsume has increased rapidly over the last decade. Users' decision processes\nare complex and highly influenced by contextual settings, but data supporting\nthe development and evaluation of context-aware recommender systems are scarce.\nIn this paper we present a dataset of self-reported TV consumption enriched\nwith contextual information of viewing situations. We show how choice of genre\nassociates with, among others, the number of present users and users' attention\nlevels. Furthermore, we evaluate the performance of predicting chosen genres\ngiven different configurations of contextual information, and compare the\nresults to contextless predictions. The results suggest that including\ncontextual features in the prediction cause notable improvements, and both\ntemporal and social context show significant contributions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 11:17:43 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 10:44:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1808.00525", "submitter": "Jinseok Kim", "authors": "Jinseok Kim and Jenna Kim", "title": "The impact of imbalanced training data on machine learning for author\n  name disambiguation", "comments": "17 pages, 3 figures, and 3 tables", "journal-ref": "Kim, J. & Kim, J. (2018). The impact of imbalanced training data\n  on machine learning for author name disambiguation. Scientometrics", "doi": "10.1007/s11192-018-2865-9", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised machine learning for author name disambiguation, negative\ntraining data are often dominantly larger than positive training data. This\npaper examines how the ratios of negative to positive training data can affect\nthe performance of machine learning algorithms to disambiguate author names in\nbibliographic records. On multiple labeled datasets, three classifiers -\nLogistic Regression, Na\\\"ive Bayes, and Random Forest - are trained through\nrepresentative features such as coauthor names, and title words extracted from\nthe same training data but with various positive-negative training data ratios.\nResults show that increasing negative training data can improve disambiguation\nperformance but with a few percent of performance gains and sometimes degrade\nit. Logistic Regression and Na\\\"ive Bayes learn optimal disambiguation models\neven with a base ratio (1:1) of positive and negative training data. Also, the\nperformance improvement by Random Forest tends to quickly saturate roughly\nafter 1:10 ~ 1:15. These findings imply that contrary to the common practice\nusing all training data, name disambiguation algorithms can be trained using\npart of negative training data without degrading much disambiguation\nperformance while increasing computational efficiency. This study calls for\nmore attention from author name disambiguation scholars to methods for machine\nlearning from imbalanced data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 14:29:27 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 02:59:12 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kim", "Jinseok", ""], ["Kim", "Jenna", ""]]}, {"id": "1808.00720", "submitter": "Stephen Bonner", "authors": "David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, Alexandros\n  Karatzoglou", "title": "RecoGym: A Reinforcement Learning Environment for the problem of Product\n  Recommendation in Online Advertising", "comments": "Accepted at the REVEAL workshop at the Twelfth ACM Conference on\n  Recommender Systems (RecSys '18), October 2--7, 2018, Vancouver, BC, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are becoming ubiquitous in many settings and take many\nforms, from product recommendation in e-commerce stores, to query suggestions\nin search engines, to friend recommendation in social networks. Current\nresearch directions which are largely based upon supervised learning from\nhistorical data appear to be showing diminishing returns with a lot of\npractitioners report a discrepancy between improvements in offline metrics for\nsupervised learning and the online performance of the newly proposed models.\nOne possible reason is that we are using the wrong paradigm: when looking at\nthe long-term cycle of collecting historical performance data, creating a new\nversion of the recommendation model, A/B testing it and then rolling it out. We\nsee that there a lot of commonalities with the reinforcement learning (RL)\nsetup, where the agent observes the environment and acts upon it in order to\nchange its state towards better states (states with higher rewards). To this\nend we introduce RecoGym, an RL environment for recommendation, which is\ndefined by a model of user traffic patterns on e-commerce and the users\nresponse to recommendations on the publisher websites. We believe that this is\nan important step forward for the field of recommendation systems research,\nthat could open up an avenue of collaboration between the recommender systems\nand reinforcement learning communities and lead to better alignment between\noffline and online performance metrics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:13:18 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 11:58:09 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Rohde", "David", ""], ["Bonner", "Stephen", ""], ["Dunlop", "Travis", ""], ["Vasile", "Flavian", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1808.00957", "submitter": "Yash Kumar Lal", "authors": "Vaibhav Kumar, Mrinal Dhar, Dhruv Khattar, Yash Kumar Lal, Abhimanshu\n  Mishra, Manish Shrivastava, Vasudeva Varma", "title": "SWDE : A Sub-Word And Document Embedding Based Engine for Clickbait\n  Detection", "comments": "Accepted at SIGIR 2018 as Computational Surprise in Information\n  Retrieval (CompS) Workshop Paper. arXiv admin note: substantial text overlap\n  with arXiv:1710.01507", "journal-ref": "\"SWDE : A Sub-Word And Document Embedding Based Engine for\n  Clickbait Detection\". In Proceedings of SIGIR 2018 Workshop on Computational\n  Surprise in Information Retrieval, Ann Arbor, MI, USA, July 8-12 (CompS'18,\n  SIGIR), 4 pages", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to expand their reach and increase website ad revenue, media outlets\nhave started using clickbait techniques to lure readers to click on articles on\ntheir digital platform. Having successfully enticed the user to open the\narticle, the article fails to satiate his curiosity serving only to boost\nclick-through rates. Initial methods for this task were dependent on feature\nengineering, which varies with each dataset. Industry systems have relied on an\nexhaustive set of rules to get the job done. Neural networks have barely been\nexplored to perform this task. We propose a novel approach considering\ndifferent textual embeddings of a news headline and the related article. We\ngenerate sub-word level embeddings of the title using Convolutional Neural\nNetworks and use them to train a bidirectional LSTM architecture. An attention\nlayer allows for calculation of significance of each term towards the nature of\nthe post. We also generate Doc2Vec embeddings of the title and article text and\nmodel how they interact, following which it is concatenated with the output of\nthe previous component. Finally, this representation is passed through a neural\nnetwork to obtain a score for the headline. We test our model over 2538 posts\n(having trained it on 17000 records) and achieve an accuracy of 83.49%\noutscoring previous state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:02:00 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kumar", "Vaibhav", ""], ["Dhar", "Mrinal", ""], ["Khattar", "Dhruv", ""], ["Lal", "Yash Kumar", ""], ["Mishra", "Abhimanshu", ""], ["Shrivastava", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1808.00958", "submitter": "Bruno Tuffin", "authors": "Ahmed Kamoun (IMT Atlantique), Patrick Maill\\'e (RSM), Bruno Tuffin\n  (DIONYSOS)", "title": "Evaluating search engines and defining a consensus implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different search engines provide different outputs for the same keyword. This\nmay be due to different definitions of relevance, and/or to different\nknowledge/anticipation of users' preferences, but rankings are also suspected\nto be biased towards own content, which may prejudicial to other content\nproviders. In this paper, we make some initial steps toward a rigorous\ncomparison and analysis of search engines, by proposing a definition for a\nconsensual relevance of a page with respect to a keyword, from a set of search\nengines. More specifically, we look at the results of several search engines\nfor a sample of keywords, and define for each keyword the visibility of a page\nbased on its ranking over all search engines. This allows to define a score of\nthe search engine for a keyword, and then its average score over all keywords.\nBased on the pages visibility, we can also define the consensus search engine\nas the one showing the most visible results for each keyword. We have\nimplemented this model and present an analysis of the results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 11:28:28 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Kamoun", "Ahmed", "", "IMT Atlantique"], ["Maill\u00e9", "Patrick", "", "RSM"], ["Tuffin", "Bruno", "", "DIONYSOS"]]}, {"id": "1808.01006", "submitter": "Kilol Gupta", "authors": "Kilol Gupta, Mukund Yelahanka Raghuprasad, Pankhuri Kumar", "title": "A Hybrid Variational Autoencoder for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's day and age when almost every industry has an online presence with\nusers interacting in online marketplaces, personalized recommendations have\nbecome quite important. Traditionally, the problem of collaborative filtering\nhas been tackled using Matrix Factorization which is linear in nature. We\nextend the work of [11] on using variational autoencoders (VAEs) for\ncollaborative filtering with implicit feedback by proposing a hybrid,\nmulti-modal approach. Our approach combines movie embeddings (learned from a\nsibling VAE network) with user ratings from the Movielens 20M dataset and\napplies it to the task of movie recommendation. We empirically show how the VAE\nnetwork is empowered by incorporating movie embeddings. We also visualize movie\nand user embeddings by clustering their latent representations obtained from a\nVAE.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 06:57:11 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 23:31:19 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Gupta", "Kilol", ""], ["Raghuprasad", "Mukund Yelahanka", ""], ["Kumar", "Pankhuri", ""]]}, {"id": "1808.01075", "submitter": "Zhi Li", "authors": "Zhi Li, Hongke Zhao, Qi Liu, Zhenya Huang, Tao Mei, Enhong Chen", "title": "Learning from History and Present: Next-item Recommendation via\n  Discriminatively Exploiting User Behaviors", "comments": "10 pages, 7 figures, KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3220014", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern e-commerce, the behaviors of customers contain rich\ninformation, e.g., consumption habits, the dynamics of preferences. Recently,\nsession-based recommendations are becoming popular to explore the temporal\ncharacteristics of customers' interactive behaviors. However, existing works\nmainly exploit the short-term behaviors without fully taking the customers'\nlong-term stable preferences and evolutions into account. In this paper, we\npropose a novel Behavior-Intensive Neural Network (BINN) for next-item\nrecommendation by incorporating both users' historical stable preferences and\npresent consumption motivations. Specifically, BINN contains two main\ncomponents, i.e., Neural Item Embedding, and Discriminative Behaviors Learning.\nFirstly, a novel item embedding method based on user interactions is developed\nfor obtaining an unified representation for each item. Then, with the embedded\nitems and the interactive behaviors over item sequences, BINN discriminatively\nlearns the historical preferences and present motivations of the target users.\nThus, BINN could better perform recommendations of the next items for the\ntarget users. Finally, for evaluating the performances of BINN, we conduct\nextensive experiments on two real-world datasets, i.e., Tianchi and JD. The\nexperimental results clearly demonstrate the effectiveness of BINN compared\nwith several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 03:16:49 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Li", "Zhi", ""], ["Zhao", "Hongke", ""], ["Liu", "Qi", ""], ["Huang", "Zhenya", ""], ["Mei", "Tao", ""], ["Chen", "Enhong", ""]]}, {"id": "1808.01092", "submitter": "Chaoran Huang", "authors": "Chaoran Huang, Lina Yao, Xianzhi Wang, Boualem Benatallah, Shuai\n  Zhang, and Manqing Dong", "title": "Expert Recommendation via Tensor Factorization with Regularizing\n  Hierarchical Topical Relationships", "comments": "16th International Conference on Service Oriented Computing (ICSOC\n  2018). Hanzhou, China, Nov 12 - Nov. 15, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge acquisition and exchange are generally crucial yet costly for both\nbusinesses and individuals, especially when the knowledge concerns various\nareas. Question Answering Communities offer an opportunity for sharing\nknowledge at a low cost, where communities users, many of whom are domain\nexperts, can potentially provide high-quality solutions to a given problem. In\nthis paper, we propose a framework for finding experts across multiple\ncollaborative networks. We employ the recent techniques of tree-guided learning\n(via tensor decomposition), and matrix factorization to explore user expertise\nfrom past voted posts. Tensor decomposition enables to leverage the latent\nexpertise of users, and the posts and related tags help identify the related\nareas. The final result is an expertise score for every user on every knowledge\narea. We experiment on Stack Exchange Networks, a set of question answering\nwebsites on different topics with a huge group of users and posts. Experiments\nshow our proposed approach produces steady and premium outputs.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 05:55:21 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 02:10:36 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Huang", "Chaoran", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Benatallah", "Boualem", ""], ["Zhang", "Shuai", ""], ["Dong", "Manqing", ""]]}, {"id": "1808.01175", "submitter": "Muhammed Tarik Altuncu", "authors": "M. Tarik Altuncu, Sophia N. Yaliraki, Mauricio Barahona", "title": "Content-driven, unsupervised clustering of news articles through\n  multiscale graph partitioning", "comments": "8 pages; 5 figures; To present at KDD 2018: Data Science, Journalism\n  & Media workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in the amount of news and journalistic content being generated\nacross the globe, coupled with extended and instantaneous access to information\nthrough online media, makes it difficult and time-consuming to monitor news\ndevelopments and opinion formation in real time. There is an increasing need\nfor tools that can pre-process, analyse and classify raw text to extract\ninterpretable content; specifically, identifying topics and content-driven\ngroupings of articles. We present here such a methodology that brings together\npowerful vector embeddings from Natural Language Processing with tools from\nGraph Theory that exploit diffusive dynamics on graphs to reveal natural\npartitions across scales. Our framework uses a recent deep neural network text\nanalysis methodology (Doc2vec) to represent text in vector form and then\napplies a multi-scale community detection method (Markov Stability) to\npartition a similarity graph of document vectors. The method allows us to\nobtain clusters of documents with similar content, at different levels of\nresolution, in an unsupervised manner. We showcase our approach with the\nanalysis of a corpus of 9,000 news articles published by Vox Media over one\nyear. Our results show consistent groupings of documents according to content\nwithout a priori assumptions about the number or type of clusters to be found.\nThe multilevel clustering reveals a quasi-hierarchy of topics and subtopics\nwith increased intelligibility and improved topic coherence as compared to\nexternal taxonomy services and standard topic detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 12:57:15 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Altuncu", "M. Tarik", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "1808.01199", "submitter": "Harold Soh", "authors": "Vinh Vo Thanh, Harold Soh", "title": "Generation Meets Recommendation: Proposing Novel Items for Groups of\n  Users", "comments": "20 pages, 6 figures, Accepted to Recsys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a movie studio aiming to produce a set of new movies for summer\nrelease: What types of movies it should produce? Who would the movies appeal\nto? How many movies should it make? Similar issues are encountered by a variety\nof organizations, e.g., mobile-phone manufacturers and online magazines, who\nhave to create new (non-existent) items to satisfy groups of users with\ndifferent preferences. In this paper, we present a joint problem formalization\nof these interrelated issues, and propose generative methods that address these\nquestions simultaneously. Specifically, we leverage the latent space obtained\nby training a deep generative model---the Variational Autoencoder (VAE)---via a\nloss function that incorporates both rating performance and item reconstruction\nterms. We then apply a greedy search algorithm that utilizes this learned\nlatent space to jointly obtain K plausible new items, and user groups that\nwould find the items appealing. An evaluation of our methods on a synthetic\ndataset indicates that our approach is able to generate novel items similar to\nhighly-desirable unobserved items. As case studies on real-world data, we\napplied our method on the MART abstract art and Movielens Tag Genome dataset,\nwhich resulted in promising results: small and diverse sets of novel items.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 08:26:19 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Thanh", "Vinh Vo", ""], ["Soh", "Harold", ""]]}, {"id": "1808.01459", "submitter": "Alejandro Rodr\\'iguez Gonz\\'alez", "authors": "Eduardo P. Garcia del Valle, Gerardo Lagunes Garcia, Lucia Prieto\n  Santamaria, Massimiliano Zanin, Alejandro Rodriguez-Gonzalez, Ernestina\n  Menasalvas Ruiz", "title": "Evaluating Wikipedia as a source of information for disease\n  understanding", "comments": "6 pages, 5 figures, 5 tables, published at IEEE CBMS 2018, 2018 IEEE\n  31st International Symposium on Computer-Based Medical Systems (CBMS)", "journal-ref": null, "doi": "10.1109/CBMS.2018.00076", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of biological data is improving our understanding\nof diseases and providing new insight into their underlying relationships.\nThanks to the improvements on both text mining techniques and computational\ncapacity, the combination of biological data with semantic information obtained\nfrom medical publications has proven to be a very promising path. However, the\nlimitations in the access to these data and their lack of structure pose\nchallenges to this approach. In this document we propose the use of Wikipedia -\nthe free online encyclopedia - as a source of accessible textual information\nfor disease understanding research. To check its validity, we compare its\nperformance in the determination of relationships between diseases with that of\nPubMed, one of the most consulted data sources of medical texts. The obtained\nresults suggest that the information extracted from Wikipedia is as relevant as\nthat obtained from PubMed abstracts (i.e. the free access portion of its\narticles), although further research is proposed to verify its reliability for\nmedical studies.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 09:38:31 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["del Valle", "Eduardo P. Garcia", ""], ["Garcia", "Gerardo Lagunes", ""], ["Santamaria", "Lucia Prieto", ""], ["Zanin", "Massimiliano", ""], ["Rodriguez-Gonzalez", "Alejandro", ""], ["Ruiz", "Ernestina Menasalvas", ""]]}, {"id": "1808.01591", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Hinrich Sch\\\"utze", "title": "LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse\n  Semantic Accumulation and Example to Pattern Transformation", "comments": "2018 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP2018) workshop on Analyzing and Interpreting Neural Networks for NLP\n  (BlackBoxNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are temporal networks and cumulative in\nnature that have shown promising results in various natural language processing\ntasks. Despite their success, it still remains a challenge to understand their\nhidden behavior. In this work, we analyze and interpret the cumulative nature\nof RNN via a proposed technique named as Layer-wIse-Semantic-Accumulation\n(LISA) for explaining decisions and detecting the most likely (i.e., saliency)\npatterns that the network relies on while decision making. We demonstrate (1)\nLISA: \"How an RNN accumulates or builds semantics during its sequential\nprocessing for a given text example and expected response\" (2) Example2pattern:\n\"How the saliency patterns look like for each category in the data according to\nthe network in decision making\". We analyse the sensitiveness of RNNs about\ndifferent inputs to check the increase or decrease in prediction scores and\nfurther extract the saliency patterns learned by the network. We employ two\nrelation classification datasets: SemEval 10 Task 8 and TAC KBP Slot Filling to\nexplain RNN predictions via the LISA and example2pattern.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 09:50:47 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gupta", "Pankaj", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1808.01843", "submitter": "Yinglong Ma", "authors": "Yinglong Ma, Peng Zhang and Jiangang Ma", "title": "An Efficient Approach to Learning Chinese Judgment Document Similarity\n  Based on Knowledge Summarization", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A previous similar case in common law systems can be used as a reference with\nrespect to the current case such that identical situations can be treated\nsimilarly in every case. However, current approaches for judgment document\nsimilarity computation failed to capture the core semantics of judgment\ndocuments and therefore suffer from lower accuracy and higher computation\ncomplexity. In this paper, a knowledge block summarization based machine\nlearning approach is proposed to compute the semantic similarity of Chinese\njudgment documents. By utilizing domain ontologies for judgment documents, the\ncore semantics of Chinese judgment documents is summarized based on knowledge\nblocks. Then the WMD algorithm is used to calculate the similarity between\nknowledge blocks. At last, the related experiments were made to illustrate that\nour approach is very effective and efficient in achieving higher accuracy and\nfaster computation speed in comparison with the traditional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 12:24:19 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ma", "Yinglong", ""], ["Zhang", "Peng", ""], ["Ma", "Jiangang", ""]]}, {"id": "1808.01968", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo", "title": "Discovering Latent Information By Spreading Activation Algorithm For\n  Document Retrieval", "comments": "12pages, will be published in The International Journal of Artificial\n  Intelligence & Applications (IJAIA). arXiv admin note: text overlap with\n  arXiv:1807.07967", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic search relies on keywords contained in a query to find suitable\ndocuments. So, documents that do not contain the keywords but contain\ninformation related to the query are not retrieved. Spreading activation is an\nalgorithm for finding latent information in a query by exploiting relations\nbetween nodes in an associative network or semantic network. However, the\nclassical spreading activation algorithm uses all relations of a node in the\nnetwork that will add unsuitable information into the query. In this paper, we\npropose a novel approach for semantic text search, called\nquery-oriented-constrained spreading activation that only uses relations\nrelating to the content of the query to find really related information.\nExperiments on a benchmark dataset show that, in terms of the MAP measure, our\nsearch engine is 18.9% and 43.8% respectively better than the syntactic search\nand the search using the classical constrained spreading activation.\n  KEYWORDS: Information Retrieval, Ontology, Semantic Search, Spreading\nActivation\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 08:42:30 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ngo", "Vuong M.", ""]]}, {"id": "1808.02013", "submitter": "Yuanchun Li", "authors": "Yuanchun Li, Ziyue Yang, Yao Guo, Xiangqun Chen, Yuvraj Agarwal, Jason\n  Hong", "title": "Automated Extraction of Personal Knowledge from Smartphone Push\n  Notifications", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized services are in need of a rich and powerful personal knowledge\nbase, i.e. a knowledge base containing information about the user. This paper\nproposes an approach to extracting personal knowledge from smartphone push\nnotifications, which are used by mobile systems and apps to inform users of a\nrich range of information. Our solution is based on the insight that most\nnotifications are formatted using templates, while knowledge entities can be\nusually found within the parameters to the templates. As defining all the\nnotification templates and their semantic rules are impractical due to the huge\nnumber of notification templates used by potentially millions of apps, we\npropose an automated approach for personal knowledge extraction from push\nnotifications. We first discover notification templates through pattern mining,\nthen use machine learning to understand the template semantics. Based on the\ntemplates and their semantics, we are able to translate notification text into\nknowledge facts automatically. Users' privacy is preserved as we only need to\nupload the templates to the server for model training, which do not contain any\npersonal information. According to our experiments with about 120 million push\nnotifications from 100,000 smartphone users, our system is able to extract\npersonal knowledge accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 17:59:45 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Li", "Yuanchun", ""], ["Yang", "Ziyue", ""], ["Guo", "Yao", ""], ["Chen", "Xiangqun", ""], ["Agarwal", "Yuvraj", ""], ["Hong", "Jason", ""]]}, {"id": "1808.02022", "submitter": "Saeedeh Shekarpour", "authors": "Saeedeh Shekarpour, Ankita Saxena, Krishnaprasad Thirunarayan, Valerie\n  L. Shalin, Amit Sheth", "title": "Principles for Developing a Knowledge Graph of Interlinked Events from\n  News Headlines on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing datasets published on Linked Open Data mainly contain\nencyclopedic information. However, there is a lack of quality structured and\nsemantically annotated datasets extracted from unstructured real-time sources.\nIn this paper, we present principles for developing a knowledge graph of\ninterlinked events using the case study of news headlines published on Twitter\nwhich is a real-time and eventful source of fresh information. We represent the\nessential pipeline containing the required tasks ranging from choosing\nbackground data model, event annotation (i.e., event recognition and\nclassification), entity annotation and eventually interlinking events. The\nstate-of-the-art is limited to domain-specific scenarios for recognizing and\nclassifying events, whereas this paper plays the role of a domain-agnostic\nroad-map for developing a knowledge graph of interlinked events.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 03:04:35 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Shekarpour", "Saeedeh", ""], ["Saxena", "Ankita", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Shalin", "Valerie L.", ""], ["Sheth", "Amit", ""]]}, {"id": "1808.02215", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang, Yun Li, Yunhao Yuan, Wei Liu, Xindong Wu", "title": "STTM: A Tool for Short Text Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the emergence and popularity of social communications on the\nInternet, topic discovery from short texts becomes fundamental to many\napplications that require semantic understanding of textual content. As a\nrising research field, short text topic modeling presents a new and\ncomplementary algorithmic methodology to supplement regular text topic\nmodeling, especially targets to limited word co-occurrence information in short\ntexts. This paper presents the first comprehensive open-source package, called\nSTTM, for use in Java that integrates the state-of-the-art models of short text\ntopic modeling algorithms, benchmark datasets, and abundant functions for model\ninference and evaluation. The package is designed to facilitate the expansion\nof new methods in this research field and make evaluations between the new\napproaches and existing ones accessible. STTM is open-sourced at\nhttps://github.com/qiang2100/STTM.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 05:16:55 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Qiang", "Jipeng", ""], ["Li", "Yun", ""], ["Yuan", "Yunhao", ""], ["Liu", "Wei", ""], ["Wu", "Xindong", ""]]}, {"id": "1808.02911", "submitter": "Md Masudur Rahman", "authors": "Md Masudur Rahman, Saikat Chakraborty, Gail Kaiser, Baishakhi Ray", "title": "A Case Study on the Impact of Similarity Measure on Information\n  Retrieval based Software Engineering Tasks", "comments": "22 pages, on submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) plays a pivotal role in diverse Software\nEngineering (SE) tasks, e.g., bug localization and triaging, code retrieval,\nrequirements analysis, etc. The choice of similarity measure is the core\ncomponent of an IR technique. The performance of any IR method critically\ndepends on selecting an appropriate similarity measure for the given\napplication domain. Since different SE tasks operate on different document\ntypes like bug reports, software descriptions, source code, etc. that often\ncontain non-standard domain-specific vocabulary, it is essential to understand\nwhich similarity measures work best for different SE documents.\n  This paper presents two case studies on the effect of different similarity\nmeasure on various SE documents w.r.t. two tasks: (i) project recommendation:\nfinding similar GitHub projects and (ii) bug localization: retrieving buggy\nsource file(s) correspond to a bug report. These tasks contain a diverse\ncombination of textual (i.e. description, readme) and code (i.e. source code,\nAPI, import package) artifacts. We observe that the performance of IR models\nvaries when applied to different artifact types. We find that, in general, the\ncontext-aware models achieve better performance on textual artifacts. In\ncontrast, simple keyword-based bag-of-words models perform better on code\nartifacts. On the other hand, the probabilistic ranking model BM25 performs\nbetter on a mixture of text and code artifacts.\n  We further investigate how such an informed choice of similarity measure\nimpacts the performance of SE tools. In particular, we analyze two previously\nproposed tools for project recommendation and bug localization tasks, which\nleverage diverse software artifacts, and observe that an informed choice of\nsimilarity measure indeed leads to improved performance of the existing SE\ntools.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 18:51:37 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Rahman", "Md Masudur", ""], ["Chakraborty", "Saikat", ""], ["Kaiser", "Gail", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1808.03027", "submitter": "Mohammad Kamel", "authors": "Mohammad Kamel, Neda Keyvani and Hadi Sadoghi Yazdi", "title": "Sentimental Content Analysis and Knowledge Extraction from News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In web era, since technology has revolutionized mankind life, plenty of data\nand information are published on the Internet each day. For instance, news\nagencies publish news on their websites all over the world. These raw data\ncould be an important resource for knowledge extraction. These shared data\ncontain emotions (i.e., positive, neutral or negative) toward various topics;\ntherefore, sentimental content extraction could be a beneficial task in many\naspects. Extracting the sentiment of news illustrates highly valuable\ninformation about the events over a period of time, the viewpoint of a media or\nnews agency to these events. In this paper an attempt is made to propose an\napproach for news analysis and extracting useful knowledge from them. Firstly,\nwe attempt to extract a noise robust sentiment of news documents; therefore,\nthe news associated to six countries: United State, United Kingdom, Germany,\nCanada, France and Australia in 5 different news categories: Politics, Sports,\nBusiness, Entertainment and Technology are downloaded. In this paper we compare\nthe condition of different countries in each 5 news topics based on the\nextracted sentiments and emotional contents in news documents. Moreover, we\npropose an approach to reduce the bulky news data to extract the hottest topics\nand news titles as a knowledge. Eventually, we generate a word model to map\neach word to a fixed-size vector by Word2Vec in order to understand the\nrelations between words in our collected news database.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 05:42:49 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Kamel", "Mohammad", ""], ["Keyvani", "Neda", ""], ["Yazdi", "Hadi Sadoghi", ""]]}, {"id": "1808.03227", "submitter": "Mahtab Ahmed", "authors": "Mahtab Ahmed, Jumayel Islam, Muhammad Rifayat Samee, Robert E. Mercer", "title": "Identifying Protein-Protein Interaction using Tree LSTM and Structured\n  Attention", "comments": "9 Pages, 2 Figures, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying interactions between proteins is important to understand\nunderlying biological processes. Extracting a protein-protein interaction (PPI)\nfrom the raw text is often very difficult. Previous supervised learning methods\nhave used handcrafted features on human-annotated data sets. In this paper, we\npropose a novel tree recurrent neural network with structured attention\narchitecture for doing PPI. Our architecture achieves state of the art results\n(precision, recall, and F1-score) on the AIMed and BioInfer benchmark data\nsets. Moreover, our models achieve a significant improvement over previous best\nmodels without any explicit feature extraction. Our experimental results show\nthat traditional recurrent networks have inferior performance compared to tree\nrecurrent networks for the supervised PPI problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 19:08:12 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Ahmed", "Mahtab", ""], ["Islam", "Jumayel", ""], ["Samee", "Muhammad Rifayat", ""], ["Mercer", "Robert E.", ""]]}, {"id": "1808.03256", "submitter": "Goritsa Ninova", "authors": "Goritsa Ninova (Tech-CICO), Hassan Atifi (Tech-CICO)", "title": "Ethnographie de la structuration d'un corpus collectif de messages de\n  soutien social en ligne", "comments": "in French", "journal-ref": "Widad Mustafa El Hadi. Stabilit{\\'e} et dynamisme dans\n  l'organisation des connaissances, 8e Colloque international de l'ISKO France,\n  Jun 2011, Lille, France. Hermes science publications; Lavoisier, p.135, 2012,\n  L'organisation des connaissances : Dynamisme et stabilit{\\'e}", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a study of progressive development of the structure\nof corpus collected starting from the discussion forums. These are the first\nresults from an ethnographic description of constitution and analysis practices\nof a corpus located within an interdisciplinary project. At first, we describe\nthe personal digital documents folder of a researcher, relative to his corpus\nanalysis of interactions of social support online. Next, we examine the\nconstitution of a shared corpus, result of the pooling of individual corpora,\nsmall size, produced by researchers participating in the project, each\nindividual corpus reflecting his producer's perspectives of research.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:33:59 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Ninova", "Goritsa", "", "Tech-CICO"], ["Atifi", "Hassan", "", "Tech-CICO"]]}, {"id": "1808.03265", "submitter": "Qiwei Han", "authors": "Qiwei Han, Mengxin Ji, Inigo Martinez de Rituerto de Troya, Manas\n  Gaur, Leid Zejnilovic", "title": "A Hybrid Recommender System for Patient-Doctor Matchmaking in Primary\n  Care", "comments": "This paper is accepted at DSAA 2018 as a full paper, Proc. of the 5th\n  IEEE International Conference on Data Science and Advanced Analytics (DSAA),\n  Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We partner with a leading European healthcare provider and design a mechanism\nto match patients with family doctors in primary care. We define the\nmatchmaking process for several distinct use cases given different levels of\navailable information about patients. Then, we adopt a hybrid recommender\nsystem to present each patient a list of family doctor recommendations. In\nparticular, we model patient trust of family doctors using a large-scale\ndataset of consultation histories, while accounting for the temporal dynamics\nof their relationships. Our proposed approach shows higher predictive accuracy\nthan both a heuristic baseline and a collaborative filtering approach, and the\nproposed trust measure further improves model performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 04:08:46 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 18:52:59 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Han", "Qiwei", ""], ["Ji", "Mengxin", ""], ["de Troya", "Inigo Martinez de Rituerto", ""], ["Gaur", "Manas", ""], ["Zejnilovic", "Leid", ""]]}, {"id": "1808.03298", "submitter": "Zhiyu Min", "authors": "Zhiyu Min, Dahua Lin", "title": "Probabilistic Ensemble of Collaborative Filters", "comments": "8 pages. In Proceedings of AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is an important technique for recommendation. Whereas\nit has been repeatedly shown to be effective in previous work, its performance\nremains unsatisfactory in many real-world applications, especially those where\nthe items or users are highly diverse. In this paper, we explore an\nensemble-based framework to enhance the capability of a recommender in handling\ndiverse data. Specifically, we formulate a probabilistic model which integrates\nthe items, the users, as well as the associations between them into a\ngenerative process. On top of this formulation, we further derive a progressive\nalgorithm to construct an ensemble of collaborative filters. In each iteration,\na new filter is derived from re-weighted entries and incorporated into the\nensemble. It is noteworthy that while the algorithmic procedure of our\nalgorithm is apparently similar to boosting, it is derived from an essentially\ndifferent formulation and thus differs in several key technical aspects. We\ntested the proposed method on three large datasets, and observed substantial\nimprovement over the state of the art, including L2Boost, an effective method\nbased on boosting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:48:57 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:49:53 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Min", "Zhiyu", ""], ["Lin", "Dahua", ""]]}, {"id": "1808.03677", "submitter": "Sandro Sozzo", "authors": "Diederik Aerts, Massimiliano Sassoli de Bianchi, Sandro Sozzo, Tomas\n  Veloz", "title": "Modeling Meaning Associated with Documental Entities: Introducing the\n  Brussels Quantum Approach", "comments": "27 pages, 6 figures, LateX", "journal-ref": null, "doi": "10.1007/978-3-030-25913-6_1", "report-no": null, "categories": "cs.IR cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Brussels operational-realistic approach to quantum physics\nand quantum cognition offers a fundamental strategy for modeling the meaning\nassociated with collections of documental entities. To do so, we take the World\nWide Web as a paradigmatic example and emphasize the importance of\ndistinguishing the Web, made of printed documents, from a more abstract meaning\nentity, which we call the Quantum Web, or QWeb, where the former is considered\nto be the collection of traces that can be left by the latter, in specific\nmeasurements, similarly to how a non-spatial quantum entity, like an electron,\ncan leave localized traces of impact on a detection screen. The double-slit\nexperiment is extensively used to illustrate the rationale of the modeling,\nwhich is guided by how physicists constructed quantum theory to describe the\nbehavior of the microscopic entities. We also emphasize that the superposition\nprinciple and the associated interference effects are not sufficient to model\nall experimental probabilistic data, like those obtained by counting the\nrelative number of documents containing certain words and co-occurrences of\nwords. For this, additional effects, like context effects, must also be taken\ninto consideration.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 08:08:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Aerts", "Diederik", ""], ["de Bianchi", "Massimiliano Sassoli", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1808.03733", "submitter": "Yuanfeng Song", "authors": "Di Jiang, Yuanfeng Song, Rongzhong Lian, Siqi Bao, Jinhua Peng, Huang\n  He, and Hua Wu", "title": "Familia: A Configurable Topic Modeling Framework for Industrial Text\n  Engineering", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a variety of topic models have been proposed for text\nengineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and\nLatent Dirichlet Allocation (LDA), most of existing topic models are seldom\napplied or considered in industrial scenarios. This phenomenon is caused by the\nfact that there are very few convenient tools to support these topic models so\nfar. Intimidated by the demanding expertise and labor of designing and\nimplementing parameter inference algorithms, software engineers are prone to\nsimply resort to PLSA/LDA, without considering whether it is proper for their\nproblem at hand or not. In this paper, we propose a configurable topic modeling\nframework named Familia, in order to bridge the huge gap between academic\nresearch fruits and current industrial practice. Familia supports an important\nline of topic models that are widely applicable in text engineering scenarios.\nIn order to relieve burdens of software engineers without knowledge of Bayesian\nnetworks, Familia is able to conduct automatic parameter inference for a\nvariety of topic models. Simply through changing the data organization of\nFamilia, software engineers are able to easily explore a broad spectrum of\nexisting topic models or even design their own topic models, and find the one\nthat best suits the problem at hand. With its superior extendability, Familia\nhas a novel sampling mechanism that strikes balance between effectiveness and\nefficiency of parameter inference. Furthermore, Familia is essentially a big\ntopic modeling framework that supports parallel parameter inference and\ndistributed parameter storage. The utilities and necessity of Familia are\ndemonstrated in real-life industrial applications. Familia would significantly\nenlarge software engineers' arsenal of topic models and pave the way for\nutilizing highly customized topic models in real-life problems.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 01:14:50 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 06:26:12 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Jiang", "Di", ""], ["Song", "Yuanfeng", ""], ["Lian", "Rongzhong", ""], ["Bao", "Siqi", ""], ["Peng", "Jinhua", ""], ["He", "Huang", ""], ["Wu", "Hua", ""]]}, {"id": "1808.03737", "submitter": "Kan Ren", "authors": "Kan Ren, Yuchen Fang, Weinan Zhang, Shuhao Liu, Jiajun Li, Ya Zhang,\n  Yong Yu, Jun Wang", "title": "Learning Multi-touch Conversion Attribution with Dual-attention\n  Mechanisms for Online Advertising", "comments": "10 pages, 11 figures; CIKM 2018", "journal-ref": null, "doi": "10.1145/3269206.3271677", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, the Internet users may be exposed to a sequence of\ndifferent ad campaigns, i.e., display ads, search, or referrals from multiple\nchannels, before led up to any final sales conversion and transaction. For both\ncampaigners and publishers, it is fundamentally critical to estimate the\ncontribution from ad campaign touch-points during the customer journey\n(conversion funnel) and assign the right credit to the right ad exposure\naccordingly. However, the existing research on the multi-touch attribution\nproblem lacks a principled way of utilizing the users' pre-conversion actions\n(i.e., clicks), and quite often fails to model the sequential patterns among\nthe touch points from a user's behavior data. To make it worse, the current\nindustry practice is merely employing a set of arbitrary rules as the\nattribution model, e.g., the popular last-touch model assigns 100% credit to\nthe final touch-point regardless of actual attributions. In this paper, we\npropose a Dual-attention Recurrent Neural Network (DARNN) for the multi-touch\nattribution problem. It learns the attribution values through an attention\nmechanism directly from the conversion estimation objective. To achieve this,\nwe utilize sequence-to-sequence prediction for user clicks, and combine both\npost-view and post-click attribution patterns together for the final conversion\nestimation. To quantitatively benchmark attribution models, we also propose a\nnovel yet practical attribution evaluation scheme through the proxy of budget\nallocation (under the estimated attributions) over ad channels. The\nexperimental results on two real datasets demonstrate the significant\nperformance gains of our attribution model against the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 01:58:19 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 01:31:31 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Ren", "Kan", ""], ["Fang", "Yuchen", ""], ["Zhang", "Weinan", ""], ["Liu", "Shuhao", ""], ["Li", "Jiajun", ""], ["Zhang", "Ya", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""]]}, {"id": "1808.03793", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Florian Buettner and Hinrich Sch\\\"utze", "title": "Document Informed Neural Autoregressive Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context information around words helps in determining their actual meaning,\nfor example \"networks\" used in contexts of artificial neural networks or\nbiological neuron networks. Generative topic models infer topic-word\ndistributions, taking no or only little context into account. Here, we extend a\nneural autoregressive topic model to exploit the full context information\naround words in a document in a language modeling fashion. This results in an\nimproved performance in terms of generalization, interpretability and\napplicability. We apply our modeling approach to seven data sets from various\ndomains and demonstrate that our approach consistently outperforms\nstateof-the-art generative topic models. With the learned representations, we\nshow on an average a gain of 9.6% (0.57 Vs 0.52) in precision at retrieval\nfraction 0.02 and 7.2% (0.582 Vs 0.543) in F1 for text categorization.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 12:16:09 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Gupta", "Pankaj", ""], ["Buettner", "Florian", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1808.03835", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen", "title": "jLDADMM: A Java package for the LDA and DMM topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we present jLDADMM---an easy-to-use Java toolkit\nfor conventional topic models. jLDADMM is released to provide alternatives for\ntopic modeling on normal or short texts. It provides implementations of the\nLatent Dirichlet Allocation topic model and the one-topic-per-document\nDirichlet Multinomial Mixture model (i.e. mixture of unigrams), using collapsed\nGibbs sampling. In addition, jLDADMM supplies a document clustering evaluation\nto compare topic models. jLDADMM is open-source and available to download at:\nhttps://github.com/datquocnguyen/jLDADMM\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:47:58 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Nguyen", "Dat Quoc", ""]]}, {"id": "1808.03843", "submitter": "Wei Tan", "authors": "Wei Tan, Shiyu Chang, Liana Fong, Cheng Li, Zijun Wang, Liangliang Cao", "title": "Matrix Factorization on GPUs with Memory Optimization and Approximate\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) discovers latent features from observations, which\nhas shown great promises in the fields of collaborative filtering, data\ncompression, feature extraction, word embedding, etc. While many\nproblem-specific optimization techniques have been proposed, alternating least\nsquare (ALS) remains popular due to its general applicability e.g. easy to\nhandle positive-unlabeled inputs, fast convergence and parallelization\ncapability. Current MF implementations are either optimized for a single\nmachine or with a need of a large computer cluster but still are insufficient.\nThis is because a single machine provides limited compute power for large-scale\ndata while multiple machines suffer from the network communication bottleneck.\n  To address the aforementioned challenge, accelerating ALS on graphics\nprocessing units (GPUs) is a promising direction. We propose the novel approach\nin enhancing the MF efficiency via both memory optimization and approximate\ncomputing. The former exploits GPU memory hierarchy to increase data reuse,\nwhile the later reduces unnecessary computing without hurting the convergence\nof learning algorithms. Extensive experiments on large-scale datasets show that\nour solution not only outperforms the competing CPU solutions by a large margin\nbut also has a 2x-4x performance gain compared to the state-of-the-art GPU\nsolutions. Our implementations are open-sourced and publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 17:36:10 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Tan", "Wei", ""], ["Chang", "Shiyu", ""], ["Fong", "Liana", ""], ["Li", "Cheng", ""], ["Wang", "Zijun", ""], ["Cao", "Liangliang", ""]]}, {"id": "1808.03908", "submitter": "Xiaoyu Du", "authors": "Xiangnan He, Zhankui He, Xiaoyu Du and Tat-Seng Chua", "title": "Adversarial Personalized Ranking for Recommendation", "comments": "SIGIR 2018", "journal-ref": null, "doi": "10.1145/3209978.3209981", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Item recommendation is a personalized ranking task. To this end, many\nrecommender systems optimize models with pairwise ranking objectives, such as\nthe Bayesian Personalized Ranking (BPR). Using matrix Factorization (MF) ---\nthe most widely used model in recommendation --- as a demonstration, we show\nthat optimizing it with BPR leads to a recommender model that is not robust. In\nparticular, we find that the resultant model is highly vulnerable to\nadversarial perturbations on its model parameters, which implies the possibly\nlarge error in generalization.\n  To enhance the robustness of a recommender model and thus improve its\ngeneralization performance, we propose a new optimization framework, namely\nAdversarial Personalized Ranking (APR). In short, our APR enhances the pairwise\nranking method BPR by performing adversarial training. It can be interpreted as\nplaying a minimax game, where the minimization of the BPR objective function\nmeanwhile defends an adversary, which adds adversarial perturbations on model\nparameters to maximize the BPR objective function. To illustrate how it works,\nwe implement APR on MF by adding adversarial perturbations on the embedding\nvectors of users and items. Extensive experiments on three public real-world\ndatasets demonstrate the effectiveness of APR --- by optimizing MF with APR, it\noutperforms BPR with a relative improvement of 11.2% on average and achieves\nstate-of-the-art performance for item recommendation. Our implementation is\navailable at: https://github.com/hexiangnan/adversarial_personalized_ranking.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 08:26:25 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "Xiangnan", ""], ["He", "Zhankui", ""], ["Du", "Xiaoyu", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1808.03912", "submitter": "Xiaoyu Du", "authors": "Xiangnan He, Xiaoyu Du, Xiang Wang, Feng Tian, Jinhui Tang and\n  Tat-Seng Chua", "title": "Outer Product-based Neural Collaborative Filtering", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we contribute a new multi-layer neural network architecture\nnamed ONCF to perform collaborative filtering. The idea is to use an outer\nproduct to explicitly model the pairwise correlations between the dimensions of\nthe embedding space. In contrast to existing neural recommender models that\ncombine user embedding and item embedding via a simple concatenation or\nelement-wise product, our proposal of using outer product above the embedding\nlayer results in a two-dimensional interaction map that is more expressive and\nsemantically plausible. Above the interaction map obtained by outer product, we\npropose to employ a convolutional neural network to learn high-order\ncorrelations among embedding dimensions. Extensive experiments on two public\nimplicit feedback data demonstrate the effectiveness of our proposed ONCF\nframework, in particular, the positive effect of using outer product to model\nthe correlations between embedding dimensions in the low level of multi-layer\nneural recommender model. The experiment codes are available at:\nhttps://github.com/duxy-me/ConvNCF\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 08:56:06 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "Xiangnan", ""], ["Du", "Xiaoyu", ""], ["Wang", "Xiang", ""], ["Tian", "Feng", ""], ["Tang", "Jinhui", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1808.03967", "submitter": "Akshay Budhkar", "authors": "Akshay Budhkar and Frank Rudzicz", "title": "Augmenting word2vec with latent Dirichlet allocation within a clinical\n  application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents three hybrid models that directly combine latent\nDirichlet allocation and word embedding for distinguishing between speakers\nwith and without Alzheimer's disease from transcripts of picture descriptions.\nTwo of our models get F-scores over the current state-of-the-art using\nautomatic methods on the DementiaBank dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 16:32:18 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Budhkar", "Akshay", ""], ["Rudzicz", "Frank", ""]]}, {"id": "1808.03969", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Ryota Hinami, Shin'ichi Satoh", "title": "Reconfigurable Inverted Index", "comments": "ACMMM 2018 (oral). Code: https://github.com/matsui528/rii", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approximate nearest neighbor search systems suffer from two\nfundamental problems that are of practical importance but have not received\nsufficient attention from the research community. First, although existing\nsystems perform well for the whole database, it is difficult to run a search\nover a subset of the database. Second, there has been no discussion concerning\nthe performance decrement after many items have been newly added to a system.\nWe develop a reconfigurable inverted index (Rii) to resolve these two issues.\nBased on the standard IVFADC system, we design a data layout such that items\nare stored linearly. This enables us to efficiently run a subset search by\nswitching the search method to a linear PQ scan if the size of a subset is\nsmall. Owing to the linear layout, the data structure can be dynamically\nadjusted after new items are added, maintaining the fast speed of the system.\nExtensive comparisons show that Rii achieves a comparable performance with\nstate-of-the art systems such as Faiss.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 16:47:47 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Matsui", "Yusuke", ""], ["Hinami", "Ryota", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1808.03978", "submitter": "Sandip Sinha", "authors": "Sandip Sinha and Omri Weinstein", "title": "Local Decodability of the Burrows-Wheeler Transform", "comments": "The following two technical typos were fixed: (1) On page 2,\n  following Theorem 1, the decoding time of a contiguous substring of size\n  $\\ell$ was corrected from $O(t + \\ell)$ to $O(t + \\ell \\cdot \\lg t)$. (2) In\n  the statement of Theorem 2, the query time to count occurrences of patterns\n  of length $\\ell$ was corrected to $O(t \\ell)$, independent of the number of\n  occurrences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Burrows-Wheeler Transform (BWT) is among the most influential discoveries\nin text compression and DNA storage. It is a reversible preprocessing step that\nrearranges an $n$-letter string into runs of identical characters (by\nexploiting context regularities), resulting in highly compressible strings, and\nis the basis of the \\texttt{bzip} compression program. Alas, the decoding\nprocess of BWT is inherently sequential and requires $\\Omega(n)$ time even to\nretrieve a \\emph{single} character.\n  We study the succinct data structure problem of locally decoding short\nsubstrings of a given text under its \\emph{compressed} BWT, i.e., with small\nadditive redundancy $r$ over the \\emph{Move-To-Front} (\\texttt{bzip})\ncompression. The celebrated BWT-based FM-index (FOCS '00), as well as other\nrelated literature, yield a trade-off of $r=\\tilde{O}(n/\\sqrt{t})$ bits, when a\nsingle character is to be decoded in $O(t)$ time. We give a near-quadratic\nimprovement $r=\\tilde{O}(n\\lg(t)/t)$. As a by-product, we obtain an\n\\emph{exponential} (in $t$) improvement on the redundancy of the FM-index for\ncounting pattern-matches on compressed text. In the interesting regime where\nthe text compresses to $n^{1-o(1)}$ bits, these results provide an $\\exp(t)$\n\\emph{overall} space reduction. For the local decoding problem of BWT, we also\nprove an $\\Omega(n/t^2)$ cell-probe lower bound for \"symmetric\" data\nstructures.\n  We achieve our main result by designing a compressed partial-sums (Rank) data\nstructure over BWT. The key component is a \\emph{locally-decodable}\nMove-to-Front (MTF) code: with only $O(1)$ extra bits per block of length\n$n^{\\Omega(1)}$, the decoding time of a single character can be decreased from\n$\\Omega(n)$ to $O(\\lg n)$. This result is of independent interest in\nalgorithmic information theory.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 18:16:59 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 06:09:03 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Sinha", "Sandip", ""], ["Weinstein", "Omri", ""]]}, {"id": "1808.04122", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen and Thanh Vu and Tu Dinh Nguyen and Dat Quoc Nguyen\n  and Dinh Phung", "title": "A Capsule Network-based Embedding Model for Knowledge Graph Completion\n  and Search Personalization", "comments": "To appear in Proceedings of NAACL 2019. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an embedding model, named CapsE, exploring a\ncapsule network to model relationship triples (subject, relation, object). Our\nCapsE represents each triple as a 3-column matrix where each column vector\nrepresents the embedding of an element in the triple. This 3-column matrix is\nthen fed to a convolution layer where multiple filters are operated to generate\ndifferent feature maps. These feature maps are reconstructed into corresponding\ncapsules which are then routed to another capsule to produce a continuous\nvector. The length of this vector is used to measure the plausibility score of\nthe triple. Our proposed CapsE obtains better performance than previous\nstate-of-the-art embedding models for knowledge graph completion on two\nbenchmark datasets WN18RR and FB15k-237, and outperforms strong search\npersonalization baselines on SEARCH17.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 09:35:44 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 03:46:29 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 10:59:09 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Vu", "Thanh", ""], ["Nguyen", "Tu Dinh", ""], ["Nguyen", "Dat Quoc", ""], ["Phung", "Dinh", ""]]}, {"id": "1808.04124", "submitter": "Bernard Jacquemin", "authors": "Eric Kergosien (GERIICO), Marie-No\\\"elle Bessagnet (LIUPPA),\n  Maguelonne Teisseire (UMR TETIS), Joachim Sch\\\"opfel (GERIICO), Mohammad Amin\n  Farvardin (LAMSADE), St\\'ephane Chaudiron (GERIICO), Bernard Jacquemin\n  (GERIICO), Annig Le Parc-Lacayrelle (LIUPPA), Mathieu Roche (UMR TETIS),\n  Christian Sallaberry (LIUPPA), Jean-Philippe Tonneau (UMR TETIS),\n  Marie-Noelle Bessagnet (LIUPPA), Amin Farvardin (UMR TETIS), Annig Lacayrelle", "title": "Methodology for identifying study sites in scientific corpus", "comments": null, "journal-ref": "Revue des Sciences et Technologies de l'Information - S\\'erie\n  Document Num\\'erique, Lavoisier, 2017, 20 (2-3), pp.11-30", "doi": "10.3166/dn.2017.00011", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TERRE-ISTEX project aims at identifying the evolution of research working\nrelation to study areas, disciplinary crossings and concrete research methods\nbased on the heterogeneous digital content available in scientific corpora. The\nproject is divided into three main actions: (1) to identify the periods and\nplaces which have been the subject of empirical studies, and which reflect the\npublications resulting from the corpus analyzed, (2) to identify the thematics\naddressed in these works and (3) to develop a web-based geographical\ninformation retrieval tool (GIR). The first two actions involve approaches\ncombining Natural languages processing patterns with text mining methods. By\ncrossing the three dimensions (spatial, thematic and temporal) in a GIR engine,\nit will be possible to understand what research has been carried out on which\nterritories and at what time. In the project, the experiments are carried out\non a heterogeneous corpus including electronic thesis and scientific articles\nfrom the ISTEX digital libraries and the CIRAD research center.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 09:46:36 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Kergosien", "Eric", "", "GERIICO"], ["Bessagnet", "Marie-No\u00eblle", "", "LIUPPA"], ["Teisseire", "Maguelonne", "", "UMR TETIS"], ["Sch\u00f6pfel", "Joachim", "", "GERIICO"], ["Farvardin", "Mohammad Amin", "", "LAMSADE"], ["Chaudiron", "St\u00e9phane", "", "GERIICO"], ["Jacquemin", "Bernard", "", "GERIICO"], ["Parc-Lacayrelle", "Annig Le", "", "LIUPPA"], ["Roche", "Mathieu", "", "UMR TETIS"], ["Sallaberry", "Christian", "", "LIUPPA"], ["Tonneau", "Jean-Philippe", "", "UMR TETIS"], ["Bessagnet", "Marie-Noelle", "", "LIUPPA"], ["Farvardin", "Amin", "", "LAMSADE"], ["Lacayrelle", "Annig", ""]]}, {"id": "1808.04152", "submitter": "Jun Yu", "authors": "Jun Yu, Xiao-Jun Wu, and Josef Kittler", "title": "Learning Discriminative Hashing Codes for Cross-Modal Retrieval based on\n  Multi-view Features", "comments": "28 pages, 10 figures, 13 tables. The paper is under consideration at\n  Pattern Analysis and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing techniques have been applied broadly in retrieval tasks due to their\nlow storage requirements and high speed of processing. Many hashing methods\nbased on a single view have been extensively studied for information retrieval.\nHowever, the representation capacity of a single view is insufficient and some\ndiscriminative information is not captured, which results in limited\nimprovement. In this paper, we employ multiple views to represent images and\ntexts for enriching the feature information. Our framework exploits the\ncomplementary information among multiple views to better learn the\ndiscriminative compact hash codes. A discrete hashing learning framework that\njointly performs classifier learning and subspace learning is proposed to\ncomplete multiple search tasks simultaneously. Our framework includes two\nstages, namely a kernelization process and a quantization process.\nKernelization aims to find a common subspace where multi-view features can be\nfused. The quantization stage is designed to learn discriminative unified\nhashing codes. Extensive experiments are performed on single-label datasets\n(WiKi and MMED) and multi-label datasets (MIRFlickr and NUS-WIDE) and the\nexperimental results indicate the superiority of our method compared with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 11:18:49 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 06:11:35 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 08:36:38 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Yu", "Jun", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1808.04216", "submitter": "Tobias Backes", "authors": "Tobias Backes", "title": "Effective Unsupervised Author Disambiguation with Relative Frequencies", "comments": "Proceedings of JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197036", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work addresses the problem of author name homonymy in the Web of\nScience. Aiming for an efficient, simple and straightforward solution, we\nintroduce a novel probabilistic similarity measure for author name\ndisambiguation based on feature overlap. Using the researcher-ID available for\na subset of the Web of Science, we evaluate the application of this measure in\nthe context of agglomeratively clustering author mentions. We focus on a\nconcise evaluation that shows clearly for which problem setups and at which\ntime during the clustering process our approach works best. In contrast to most\nother works in this field, we are sceptical towards the performance of author\nname disambiguation methods in general and compare our approach to the trivial\nsingle-cluster baseline. Our results are presented separately for each correct\nclustering size as we can explain that, when treating all cases together, the\ntrivial baseline and more sophisticated approaches are hardly distinguishable\nin terms of evaluation results. Our model shows state-of-the-art performance\nfor all correct clustering sizes without any discriminative training and with\ntuning only one convergence parameter.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 10:09:54 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Backes", "Tobias", ""]]}, {"id": "1808.04288", "submitter": "Irene Teinemaa", "authors": "Irene Teinemaa, Niek Tax, Carlos Bentes", "title": "Automatic Playlist Continuation through a Composition of Collaborative\n  Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RecSys Challenge 2018 focused on automatic playlist continuation, i.e.,\nthe task was to recommend additional music tracks for playlists based on the\nplaylist's title and/or a subset of the tracks that it already contains. The\nchallenge is based on the Spotify Million Playlist Dataset (MPD), containing\nthe tracks and the metadata from one million real-life playlists. This paper\ndescribes the automatic playlist continuation solution of team Latte, which is\nbased on a composition of collaborative filters that each capture different\naspects of a playlist, where the optimal combination of those collaborative\nfilters is determined using a Tree-structured Parzen Estimator (TPE). The\nsolution obtained the 12th place out of 112 participating teams in the final\nleaderboard. Team Latte participated in the main track of the challenge of the\nRecSys Challenge 2018.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:24:37 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Teinemaa", "Irene", ""], ["Tax", "Niek", ""], ["Bentes", "Carlos", ""]]}, {"id": "1808.04603", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald, Emanuel Lacic, Dieter Theiler, Elisabeth Lex", "title": "AFEL-REC: A Recommender System for Providing Learning Resource\n  Recommendations in Social Learning Environments", "comments": null, "journal-ref": "Social Recommender Systems Workshop @ ACM CIKM 2018 Conference", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present preliminary results of AFEL-REC, a recommender\nsystem for social learning environments. AFEL-REC is build upon a scalable\nsoftware architecture to provide recommendations of learning resources in near\nreal-time. Furthermore, AFEL-REC can cope with any kind of data that is present\nin social learning environments such as resource metadata, user interactions or\nsocial tags. We provide a preliminary evaluation of three recommendation use\ncases implemented in AFEL-REC and we find that utilizing social data in form of\ntags is helpful for not only improving recommendation accuracy but also\ncoverage. This paper should be valuable for both researchers and practitioners\ninterested in providing resource recommendations in social learning\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 10:00:30 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Kowald", "Dominik", ""], ["Lacic", "Emanuel", ""], ["Theiler", "Dieter", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1808.04694", "submitter": "Man Liu", "authors": "Liu Man", "title": "A Hassle-Free Machine Learning Method for Cohort Selection of Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional text classification techniques in clinical domain have heavily\nrelied on the manually extracted textual cues. This paper proposes a generally\nsupervised machine learning method that is equally hassle-free and does not use\nclinical knowledge. The employed methods were simple to implement, fast to run\nand yet effective. This paper proposes a novel named entity recognition (NER)\nbased an ensemble system capable of learning the keyword features in the\ndocument. Instead of merely considering the whole sentence/paragraph for\nanalysis, the NER based keyword features can stress the important clinic\nrelevant phases more. In addition, to capture the semantic information in the\ndocuments, the FastText features originating from the document level FastText\nclassification results are exploited.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 04:24:34 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Man", "Liu", ""]]}, {"id": "1808.04957", "submitter": "Bo Song", "authors": "Bo Song, Xin Yang, Yi Cao, Congfu Xu", "title": "Neural Collaborative Ranking", "comments": "Proceedings of the 2018 ACM on Conference on Information and\n  Knowledge Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are aimed at generating a personalized ranked list of\nitems that an end user might be interested in. With the unprecedented success\nof deep learning in computer vision and speech recognition, recently it has\nbeen a hot topic to bridge the gap between recommender systems and deep neural\nnetwork. And deep learning methods have been shown to achieve state-of-the-art\non many recommendation tasks. For example, a recent model, NeuMF, first\nprojects users and items into some shared low-dimensional latent feature space,\nand then employs neural nets to model the interaction between the user and item\nlatent features to obtain state-of-the-art performance on the recommendation\ntasks. NeuMF assumes that the non-interacted items are inherent negative and\nuses negative sampling to relax this assumption. In this paper, we examine an\nalternative approach which does not assume that the non-interacted items are\nnecessarily negative, just that they are less preferred than interacted items.\nSpecifically, we develop a new classification strategy based on the widely used\npairwise ranking assumption. We combine our classification strategy with the\nrecently proposed neural collaborative filtering framework, and propose a\ngeneral collaborative ranking framework called Neural Network based\nCollaborative Ranking (NCR). We resort to a neural network architecture to\nmodel a user's pairwise preference between items, with the belief that neural\nnetwork will effectively capture the latent structure of latent factors. The\nexperimental results on two real-world datasets show the superior performance\nof our models in comparison with several state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 03:30:57 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Song", "Bo", ""], ["Yang", "Xin", ""], ["Cao", "Yi", ""], ["Xu", "Congfu", ""]]}, {"id": "1808.05163", "submitter": "Fajie Yuan", "authors": "Fajie Yuan, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose,\n  Xiangnan He", "title": "A Simple Convolutional Generative Network for Next Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been recently introduced in the\ndomain of session-based next item recommendation. An ordered collection of past\nitems the user has interacted with in a session (or sequence) are embedded into\na 2-dimensional latent matrix, and treated as an image. The convolution and\npooling operations are then applied to the mapped item embeddings. In this\npaper, we first examine the typical session-based CNN recommender and show that\nboth the generative model and network architecture are suboptimal when modeling\nlong-range dependencies in the item sequence. To address the issues, we\nintroduce a simple, but very effective generative model that is capable of\nlearning high-level representation from both short- and long-range item\ndependencies. The network architecture of the proposed model is formed of a\nstack of \\emph{holed} convolutional layers, which can efficiently increase the\nreceptive fields without relying on the pooling operation. Another contribution\nis the effective use of residual block structure in recommender systems, which\ncan ease the optimization for much deeper networks. The proposed generative\nmodel attains state-of-the-art accuracy with less training time in the next\nitem recommendation task. It accordingly can be used as a powerful\nrecommendation baseline to beat in future, especially when there are long\nsequences of user feedback.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:04:28 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 13:13:02 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 17:48:04 GMT"}, {"version": "v4", "created": "Thu, 29 Nov 2018 03:18:35 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Yuan", "Fajie", ""], ["Karatzoglou", "Alexandros", ""], ["Arapakis", "Ioannis", ""], ["Jose", "Joemon M", ""], ["He", "Xiangnan", ""]]}, {"id": "1808.05329", "submitter": "Ruinan Zhang", "authors": "Ruinan Zhang, Fanglan Zheng, Wei Min", "title": "Sequential Behavioral Data Processing Using Deep Learning and the Markov\n  Transition Field in Online Fraud Detection", "comments": "KDD2018 Data Science in Fintech Workshop Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the popularity of the Internet and smart mobile devices, more and more\nfinancial transactions and activities have been digitalized. Compared to\ntraditional financial fraud detection strategies using credit-related features,\ncustomers are generating a large amount of unstructured behavioral data every\nsecond. In this paper, we propose an Recurrent Neural Netword (RNN) based\ndeep-learning structure integrated with Markov Transition Field (MTF) for\npredicting online fraud behaviors using customer's interactions with websites\nor smart-phone apps as a series of states. In practice, we tested and proved\nthat the proposed network structure for processing sequential behavioral data\ncould significantly boost fraud predictive ability comparing with the\nmultilayer perceptron network and distance based classifier with Dynamic Time\nWarping(DTW) as distance metric.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 02:58:54 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhang", "Ruinan", ""], ["Zheng", "Fanglan", ""], ["Min", "Wei", ""]]}, {"id": "1808.05361", "submitter": "Feng Yuan", "authors": "Feng Yuan (1), Lina Yao (1), and Boualem Benatallah (1) ((1) The\n  University of New South Wales)", "title": "Adversarial Collaborative Auto-encoder for Top-N Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, model-based recommendation methods have evolved from\nlatent factor models to neural network-based models. Most of these techniques\nmainly focus on improving the overall performance, such as the root mean square\nerror for rating predictions and hit ratio for top-N recommendation, where the\nusers' feedback is considered as the ground-truth. However, in real-world\napplications, the users' feedback is possibly contaminated by imperfect user\nbehaviours, namely, careless preference selection. Such data contamination\nposes challenges on the design of robust recommendation methods. In this work,\nto address the above issue, we propose a general adversial training framework\nfor neural network-based recommendation models, which improves both the model\nrobustness and the overall performance. We point out the tradeoffs between\nperformance and robustness enhancement with detailed instructions on how to\nstrike a balance. Specifically, we implement our approach on the collaborative\nauto-encoder, followed by experiments on three public available datasets:\nMovieLens-1M, Ciao, and FilmTrust. We show that our approach outperforms highly\ncompetitive state-of-the-art recommendation methods. In addition, we carry out\na thorough analysis on the noise impacts, as well as the complex interactions\nbetween model nonlinearity and noise levels. Through simple modifications, our\nadversarial training framework can be applied to a host of neural network-based\nmodels whose robustness and performance are expected to be both enhanced.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 07:11:22 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Yuan", "Feng", ""], ["Yao", "Lina", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1808.05480", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Himanshu Jhamb", "title": "A novel Empirical Bayes with Reversible Jump Markov Chain in User-Movie\n  Recommendation system", "comments": "arXiv admin note: text overlap with arXiv:1707.02294", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we select the unknown dimension of the feature by re-\nversible jump MCMC inside a simulated annealing in bayesian set up of\ncollaborative filter. We implement the same in MovieLens small dataset. We also\ntune the hyper parameter by using a modified empirical bayes. It can also be\nused to guess an initial choice for hyper-parameters in grid search procedure\neven for the datasets where MCMC oscillates around the true value or takes long\ntime to converge.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 12:59:14 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Jhamb", "Himanshu", ""]]}, {"id": "1808.05558", "submitter": "Robert Greinacher", "authors": "Robert Greinacher and Franziska Horn", "title": "The DALPHI annotation framework & how its pre-annotations can improve\n  annotator efficiency", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Producing the required amounts of training data for machine learning and NLP\ntasks often involves human annotators doing very repetitive and monotonous\nwork. In this paper, we present and evaluate our novel annotation framework\nDALPHI, which facilitates the annotation process by providing the annotator\nwith suggestions generated by an automated, active-learning based assistance\nsystem. In a study with 66 participants, we demonstrate on the exemplary task\nof annotating named entities in text documents that with this assistance system\nthe annotation processes can be improved with respect to the quality and\nquantity of produced annotations, even if the pre-annotations provided by the\nassistance system are at a recall level of only 50%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:58:08 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Greinacher", "Robert", ""], ["Horn", "Franziska", ""]]}, {"id": "1808.05636", "submitter": "Yaman Kumar", "authors": "Yaman Kumar, Agniv Sharma, Abhigyan Khaund, Akash Kumar, Ponnurangam\n  Kumaraguru, Rajiv Ratn Shah", "title": "IceBreaker: Solving Cold Start Problem for Video Recommendation Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet has brought about a tremendous increase in content of all forms and,\nin that, video content constitutes the major backbone of the total content\nbeing published as well as watched. Thus it becomes imperative for video\nrecommendation engines such as Hulu to look for novel and innovative ways to\nrecommend the newly added videos to their users. However, the problem with new\nvideos is that they lack any sort of metadata and user interaction so as to be\nable to rate the videos for the consumers. To this effect, this paper\nintroduces the several techniques we develop for the Content Based Video\nRelevance Prediction (CBVRP) Challenge being hosted by Hulu for the ACM\nMultimedia Conference 2018. We employ different architectures on the CBVRP\ndataset to make use of the provided frame and video level features and generate\npredictions of videos that are similar to the other videos. We also implement\nseveral ensemble strategies to explore complementarity between both the types\nof provided features. The obtained results are encouraging and will impel the\nboundaries of research for multimedia based video recommendation systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 18:26:46 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Kumar", "Yaman", ""], ["Sharma", "Agniv", ""], ["Khaund", "Abhigyan", ""], ["Kumar", "Akash", ""], ["Kumaraguru", "Ponnurangam", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1808.05857", "submitter": "Zahra Shakeri Hossein Abad", "authors": "Zahra Shakeri Hossein Abad, Vincenzo Gervasi, Didar Zowghi, Ken Barker", "title": "ELICA: An Automated Tool for Dynamic Extraction of Requirements Relevant\n  Information", "comments": "2018 IEEE 26th International Requirements Engineering Conference\n  Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requirements elicitation requires extensive knowledge and deep understanding\nof the problem domain where the final system will be situated. However, in many\nsoftware development projects, analysts are required to elicit the requirements\nfrom an unfamiliar domain, which often causes communication barriers between\nanalysts and stakeholders. In this paper, we propose a requirements ELICitation\nAid tool (ELICA) to help analysts better understand the target application\ndomain by dynamic extraction and labeling of requirements-relevant knowledge.\nTo extract the relevant terms, we leverage the flexibility and power of\nWeighted Finite State Transducers (WFSTs) in dynamic modeling of natural\nlanguage processing tasks. In addition to the information conveyed through\ntext, ELICA captures and processes non-linguistic information about the\nintention of speakers such as their confidence level, analytical tone, and\nemotions. The extracted information is made available to the analysts as a set\nof labeled snippets with highlighted relevant terms which can also be exported\nas an artifact of the Requirements Engineering (RE) process. The application\nand usefulness of ELICA are demonstrated through a case study. This study shows\nhow pre-existing relevant information about the application domain and the\ninformation captured during an elicitation meeting, such as the conversation\nand stakeholders' intentions, can be captured and used to support analysts\nachieving their tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 00:19:13 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Abad", "Zahra Shakeri Hossein", ""], ["Gervasi", "Vincenzo", ""], ["Zowghi", "Didar", ""], ["Barker", "Ken", ""]]}, {"id": "1808.05906", "submitter": "Bichen Shi", "authors": "Bichen Shi, Thanh-Binh Le, Neil Hurley and Georgiana Ifrim", "title": "Story Disambiguation: Tracking Evolving News Stories across News and\n  Social Streams", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a particular news story online is an important but difficult task,\nas the relevant information is often scattered across different domains/sources\n(e.g., news articles, blogs, comments, tweets), presented in various formats\nand language styles, and may overlap with thousands of other stories. In this\nwork we join the areas of topic tracking and entity disambiguation, and propose\na framework named Story Disambiguation - a cross-domain story tracking approach\nthat builds on real-time entity disambiguation and a learning-to-rank framework\nto represent and update the rich semantic structure of news stories. Given a\ntarget news story, specified by a seed set of documents, the goal is to\neffectively select new story-relevant documents from an incoming document\nstream. We represent stories as entity graphs and we model the story tracking\nproblem as a learning-to-rank task. This enables us to track content with high\naccuracy, from multiple domains, in real-time. We study a range of text, entity\nand graph based features to understand which type of features are most\neffective for representing stories. We further propose new semi-supervised\nlearning techniques to automatically update the story representation over time.\nOur empirical study shows that we outperform the accuracy of state-of-the-art\nmethods for tracking mixed-domain document streams, while requiring fewer\nlabeled data to seed the tracked stories. This is particularly the case for\nlocal news stories that are easily over shadowed by other trending stories, and\nfor complex news stories with ambiguous content in noisy stream environments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 09:02:37 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Shi", "Bichen", ""], ["Le", "Thanh-Binh", ""], ["Hurley", "Neil", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1808.05946", "submitter": "Hao Chen", "authors": "Hao Chen, Maria Vasardani, Stephan Winter", "title": "Disambiguating fine-grained place names from descriptions by clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyday place descriptions often contain place names of fine-grained\nfeatures, such as buildings or businesses, that are more difficult to\ndisambiguate than names referring to larger places, for example cities or\nnatural geographic features. Fine-grained places are often significantly more\nfrequent and more similar to each other, and disambiguation heuristics\ndeveloped for larger places, such as those based on population or containment\nrelationships, are often not applicable in these cases. In this research, we\naddress the disambiguation of fine-grained place names from everyday place\ndescriptions. For this purpose, we evaluate the performance of different\nexisting clustering-based approaches, since clustering approaches require no\nmore knowledge other than the locations of ambiguous place names. We consider\nnot only approaches developed specifically for place name disambiguation, but\nalso clustering algorithms developed for general data mining that could\npotentially be leveraged. We compare these methods with a novel algorithm, and\nshow that the novel algorithm outperforms the other algorithms in terms of\ndisambiguation precision and distance error over several tested datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 05:14:41 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Chen", "Hao", ""], ["Vasardani", "Maria", ""], ["Winter", "Stephan", ""]]}, {"id": "1808.05988", "submitter": "Hal Cooper", "authors": "Hal Cooper, Garud Iyengar, Ching-Yung Lin", "title": "Attainment Ratings for Graph-Query Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video game industry is larger than both the film and music industries\ncombined. Recommender systems for video games have received relatively scant\nacademic attention, despite the uniqueness of the medium and its data. In this\npaper, we introduce a graph-based recommender system that makes use of\ninteractivity, arguably the most significant feature of video gaming. We show\nthat the use of implicit data that tracks user-game interactions and levels of\nattainment (e.g. Sony Playstation Trophies, Microsoft Xbox Achievements) has\nhigh predictive value when making recommendations. Furthermore, we argue that\nthe characteristics of the video gaming hobby (low cost, high duration,\nsocially relevant) make clear the necessity of personalized, individual\nrecommendations that can incorporate social networking information. We\ndemonstrate the natural suitability of graph-query based recommendation for\nthis purpose.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 20:35:56 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Cooper", "Hal", ""], ["Iyengar", "Garud", ""], ["Lin", "Ching-Yung", ""]]}, {"id": "1808.06012", "submitter": "Elias K\\\"arle", "authors": "Elias K\\\"arle and Dieter Fensel", "title": "Heuristics for publishing dynamic content as structured data with\n  schema.org", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publishing fast changing dynamic data as open data on the web in a scalable\nmanner is not trivial. So far the only approaches describe publishing as much\ndata as possible, which then leads to problems, like server capacity overload,\nnetwork latency or unwanted knowledge disclosure. With this paper we show ways\nhow to publish dynamic data in a scalable, meaningful manner by applying\ncontext-dependent publication heuristics. The outcome shows that the\napplication of the right publication heuristics in the right domain can improve\nthe publication performance significantly. Good knowledge about the domain help\nchoosing the right publication heuristic and hence lead to very good\npublication results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 22:11:54 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "1808.06414", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Yi Tay and Lina Yao and Aixin Sun", "title": "Next Item Recommendation with Self-Attention", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sequence-aware recommendation model. Our\nmodel utilizes self-attention mechanism to infer the item-item relationship\nfrom user's historical interactions. With self-attention, it is able to\nestimate the relative weights of each item in user interaction trajectories to\nlearn better representations for user's transient interests. The model is\nfinally trained in a metric learning framework, taking both short-term and\nlong-term intentions into consideration. Experiments on a wide range of\ndatasets on different domains demonstrate that our approach outperforms the\nstate-of-the-art by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:21:23 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 07:25:24 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhang", "Shuai", ""], ["Tay", "Yi", ""], ["Yao", "Lina", ""], ["Sun", "Aixin", ""]]}, {"id": "1808.06417", "submitter": "Emanuel Laci\\'c", "authors": "Emanuel Lacic, Dominik Kowald and Elisabeth Lex", "title": "Neighborhood Troubles: On the Value of User Pre-Filtering To Speed Up\n  and Enhance Recommendations", "comments": "4 pages, 2 figures, Entity Retrieval Workshop @ ACM CIKM 2018\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present work-in-progress on applying user pre-filtering to\nspeed up and enhance recommendations based on Collaborative Filtering. We\npropose to pre-filter users in order to extract a smaller set of candidate\nneighbors, who exhibit a high number of overlapping entities and to compute the\nfinal user similarities based on this set. To realize this, we exploit features\nof the high-performance search engine Apache Solr and integrate them into a\nscalable recommender system. We have evaluated our approach on a dataset\ngathered from Foursquare and our evaluation results suggest that our proposed\nuser pre-filtering step can help to achieve both a better runtime performance\nas well as an increase in overall recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:27:33 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1808.06581", "submitter": "Yixin Wang", "authors": "Yixin Wang, Dawen Liang, Laurent Charlin, David M. Blei", "title": "The Deconfounded Recommender: A Causal Inference Approach to\n  Recommendation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of recommendation is to show users items that they will like. Though\nusually framed as a prediction, the spirit of recommendation is to answer an\ninterventional question---for each user and movie, what would the rating be if\nwe \"forced\" the user to watch the movie? To this end, we develop a causal\napproach to recommendation, one where watching a movie is a \"treatment\" and a\nuser's rating is an \"outcome.\" The problem is there may be unobserved\nconfounders, variables that affect both which movies the users watch and how\nthey rate them; unobserved confounders impede causal predictions with\nobservational data. To solve this problem, we develop the deconfounded\nrecommender, a way to use classical recommendation models for causal\nrecommendation. Following Wang & Blei [23], the deconfounded recommender\ninvolves two probabilistic models. The first models which movies the users\nwatch; it provides a substitute for the unobserved confounders. The second one\nmodels how each user rates each movie; it employs the substitute to help\naccount for confounders. This two-stage approach removes bias due to\nconfounding. It improves recommendation and enjoys stable performance against\ninterventions on test sets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:41:39 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 10:17:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Yixin", ""], ["Liang", "Dawen", ""], ["Charlin", "Laurent", ""], ["Blei", "David M.", ""]]}, {"id": "1808.06791", "submitter": "Cheng Wang", "authors": "Cheng Wang, Mathias Niepert, Hui Li", "title": "LRMM: Learning to Recommend with Missing Modalities", "comments": "11 pages, EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning has shown promising performance in content-based\nrecommendation due to the auxiliary user and item information of multiple\nmodalities such as text and images. However, the problem of incomplete and\nmissing modality is rarely explored and most existing methods fail in learning\na recommendation model with missing or corrupted modalities. In this paper, we\npropose LRMM, a novel framework that mitigates not only the problem of missing\nmodalities but also more generally the cold-start problem of recommender\nsystems. We propose modality dropout (m-drop) and a multimodal sequential\nautoencoder (m-auto) to learn multimodal representations for complementing and\nimputing missing modalities. Extensive experiments on real-world Amazon data\nshow that LRMM achieves state-of-the-art performance on rating prediction\ntasks. More importantly, LRMM is more robust to previous methods in alleviating\ndata-sparsity and the cold-start problem.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 07:45:10 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 12:33:22 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wang", "Cheng", ""], ["Niepert", "Mathias", ""], ["Li", "Hui", ""]]}, {"id": "1808.06813", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Matthew Mitsui, Philipp Mayr, Chirag Shah, and\n  Nicholas J. Belkin", "title": "The Role of the Task Topic in Web Search of Different Task Types", "comments": "Proceedings of the 2018 Conference on Human Information Interaction &\n  Retrieval (CHIIR 2018)", "journal-ref": null, "doi": "10.1145/3176349.3176382", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When users are looking for information on the Web, they show different\nbehavior for different task types, e.g., for fact finding vs. information\ngathering tasks. For example, related work in this area has investigated how\nthis behavior can be measured and applied to distinguish between easy and\ndifficult tasks. In this work, we look at the searcher's behavior in the domain\nof journalism for four different task types, and additionally, for two\ndifferent topics in each task type. Search behavior is measured with a number\nof session variables and correlated to subjective measures such as task\ndifficulty, task success and the usefulness of documents. We acknowledge prior\nresults in this area that task difficulty is correlated to user effort and that\neasy and difficult tasks are distinguishable by session variables. However, in\nthis work, we emphasize the role of the task topic - in and of itself - over\nparameters such as the search results and read content pages, dwell times,\nsession variables and subjective measures such as task difficulty or task\nsuccess. With this knowledge researchers should give more attention to the task\ntopic as an important influence factor for user behavior.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 09:20:55 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Hienert", "Daniel", ""], ["Mitsui", "Matthew", ""], ["Mayr", "Philipp", ""], ["Shah", "Chirag", ""], ["Belkin", "Nicholas J.", ""]]}, {"id": "1808.06818", "submitter": "Daniel Hienert", "authors": "Daniel Hienert and Peter Mutschke", "title": "A Usefulness-based Approach for Measuring the Local and Global Effect of\n  IIR Services", "comments": "In CHIIR '16 Proceedings of the 2016 ACM on Conference on Human\n  Information Interaction and Retrieval , 153-162", "journal-ref": null, "doi": "10.1145/2854946.2854962", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Interactive Information Retrieval (IIR) different services such as search\nterm suggestion can support users in their search process. The applicability\nand performance of such services is either measured with different\nuser-centered studies (like usability tests or laboratory experiments) or, in\nthe context of IR, with their contribution to measures like precision and\nrecall. However, each evaluation methodology has its certain disadvantages. For\nexample, user-centered experiments are often costly and small-scaled; IR\nexperiments rely on relevance assessments and measure only relevance of\ndocuments. In this work we operationalize the usefulness model of Cole et al.\n(2009) on the level of system support to measure not only the local effect of\nan IR service, but the impact it has on the whole search process. We therefore\nuse a log-based evaluation approach which models user interactions within\nsessions with positive signals and apply it for the case of a search term\nsuggestion service. We found that the usage of the service significantly often\nimplicates the occurrence of positive signals during the following session\nsteps.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 09:38:53 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Hienert", "Daniel", ""], ["Mutschke", "Peter", ""]]}, {"id": "1808.06885", "submitter": "Fei Sun", "authors": "Fei Sun, Peng Jiang, Hanxiao Sun, Changhua Pei, Wenwu Ou, Xiaobo Wang", "title": "Multi-Source Pointer Network for Product Title Summarization", "comments": "10 pages, To appear in CIKM 2018, fix mistakes in dataset stats", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the product title summarization problem in E-commerce\napplications for display on mobile devices. Comparing with conventional\nsentence summarization, product title summarization has some extra and\nessential constraints. For example, factual errors or loss of the key\ninformation are intolerable for E-commerce applications. Therefore, we abstract\ntwo more constraints for product title summarization: (i) do not introduce\nirrelevant information; (ii) retain the key information (e.g., brand name and\ncommodity name). To address these issues, we propose a novel multi-source\npointer network by adding a new knowledge encoder for pointer network. The\nfirst constraint is handled by pointer mechanism. For the second constraint, we\nrestore the key information by copying words from the knowledge encoder with\nthe help of the soft gating mechanism. For evaluation, we build a large\ncollection of real-world product titles along with human-written short titles.\nExperimental results demonstrate that our model significantly outperforms the\nother baselines. Finally, online deployment of our proposed model has yielded a\nsignificant business impact, as measured by the click-through rate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:07:53 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 15:31:02 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 16:44:32 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Sun", "Fei", ""], ["Jiang", "Peng", ""], ["Sun", "Hanxiao", ""], ["Pei", "Changhua", ""], ["Ou", "Wenwu", ""], ["Wang", "Xiaobo", ""]]}, {"id": "1808.07025", "submitter": "Francesca Spezzano", "authors": "Anu Shrestha and Francesca Spezzano and Maria Soledad Pera", "title": "Who is Really Affected by Fraudulent Reviews? An analysis of shilling\n  attacks on recommender systems in real-world scenarios", "comments": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the results of an initial analysis conducted on a real-life\nsetting to quantify the effect of shilling attacks on recommender systems. We\nfocus on both algorithm performance as well as the types of users who are most\naffected by these attacks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 17:22:37 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Shrestha", "Anu", ""], ["Spezzano", "Francesca", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "1808.07089", "submitter": "Fernando De Aguiar Neto", "authors": "Fernando S. Aguiar Neto, Arthur F. da Costa and Marcelo G. Manzato", "title": "CoBaR: Confidence-Based Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood-based collaborative filtering algorithms usually adopt a fixed\nneighborhood size for every user or item, although groups of users or items may\nhave different lengths depending on users' preferences. In this paper, we\npropose an extension to a non-personalized recommender based on confidence\nintervals and hierarchical clustering to generate groups of users with optimal\nsizes. The evaluation shows that the proposed technique outperformed the\ntraditional recommender algorithms in four publicly available datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 19:14:16 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Neto", "Fernando S. Aguiar", ""], ["da Costa", "Arthur F.", ""], ["Manzato", "Marcelo G.", ""]]}, {"id": "1808.07228", "submitter": "Ganbin Zhou", "authors": "Ganbin Zhou, Rongyu Cao, Xiang Ao, Ping Luo, Fen Lin, Leyu Lin, Qing\n  He", "title": "Hierarchical Neural Network for Extracting Knowledgeable Snippets and\n  Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we focus on extracting knowledgeable snippets and annotating\nknowledgeable documents from Web corpus, consisting of the documents from\nsocial media and We-media. Informally, knowledgeable snippets refer to the text\ndescribing concepts, properties of entities, or relations among entities, while\nknowledgeable documents are the ones with enough knowledgeable snippets. These\nknowledgeable snippets and documents could be helpful in multiple applications,\nsuch as knowledge base construction and knowledge-oriented service. Previous\nstudies extracted the knowledgeable snippets using the pattern-based method.\nHere, we propose the semantic-based method for this task. Specifically, a CNN\nbased model is developed to extract knowledgeable snippets and annotate\nknowledgeable documents simultaneously. Additionally, a \"low-level sharing,\nhigh-level splitting\" structure of CNN is designed to handle the documents from\ndifferent content domains. Compared with building multiple domain-specific\nCNNs, this joint model not only critically saves the training time, but also\nimproves the prediction accuracy visibly. The superiority of the proposed\nmethod is demonstrated in a real dataset from Wechat public platform.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 05:57:13 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Zhou", "Ganbin", ""], ["Cao", "Rongyu", ""], ["Ao", "Xiang", ""], ["Luo", "Ping", ""], ["Lin", "Fen", ""], ["Lin", "Leyu", ""], ["He", "Qing", ""]]}, {"id": "1808.07314", "submitter": "Bruce Ferwerda", "authors": "Bruce Ferwerda and Mark Graus", "title": "Predicting Musical Sophistication from Music Listening Behaviors: A\n  Preliminary Study", "comments": "The Late-Breaking Results track part of the Twelfth ACM Conference on\n  Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Psychological models are increasingly being used to explain online behavioral\ntraces. Aside from the commonly used personality traits as a general user\nmodel, more domain dependent models are gaining attention. The use of domain\ndependent psychological models allows for more fine-grained identification of\nbehaviors and provide a deeper understanding behind the occurrence of those\nbehaviors. Understanding behaviors based on psychological models can provide an\nadvantage over data-driven approaches. For example, relying on psychological\nmodels allow for ways to personalize when data is scarce. In this preliminary\nwork we look at the relation between users' musical sophistication and their\nonline music listening behaviors and to what extent we can successfully predict\nmusical sophistication. An analysis of data from a study with 61 participants\nshows that listening behaviors can successfully be used to infer users' musical\nsophistication.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 11:16:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ferwerda", "Bruce", ""], ["Graus", "Mark", ""]]}, {"id": "1808.07586", "submitter": "Michael Ekstrand", "authors": "Michael D. Ekstrand and Daniel Kluver", "title": "Exploring Author Gender in Book Rating and Recommendation", "comments": "Expanded version under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering algorithms find useful patterns in rating and\nconsumption data and exploit these patterns to guide users to good items. Many\nof the patterns in rating datasets reflect important real-world differences\nbetween the various users and items in the data; other patterns may be\nirrelevant or possibly undesirable for social or ethical reasons, particularly\nif they reflect undesired discrimination, such as discrimination in publishing\nor purchasing against authors who are women or ethnic minorities. In this work,\nwe examine the response of collaborative filtering recommender algorithms to\nthe distribution of their input data with respect to a dimension of social\nconcern, namely content creator gender. Using publicly-available book ratings\ndata, we measure the distribution of the genders of the authors of books in\nuser rating profiles and recommendation lists produced from this data. We find\nthat common collaborative filtering algorithms differ in the gender\ndistribution of their recommendation lists, and in the relationship of that\noutput distribution to user profile distribution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 23:00:26 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 00:14:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ekstrand", "Michael D.", ""], ["Kluver", "Daniel", ""]]}, {"id": "1808.07793", "submitter": "Niluthpol Mithun", "authors": "Niluthpol Chowdhury Mithun, Rameswar Panda, Evangelos E. Papalexakis,\n  Amit K. Roy-Chowdhury", "title": "Webly Supervised Joint Embedding for Cross-Modal Image-Text Retrieval", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval between visual data and natural language description\nremains a long-standing challenge in multimedia. While recent image-text\nretrieval methods offer great promise by learning deep representations aligned\nacross modalities, most of these methods are plagued by the issue of training\nwith small-scale datasets covering a limited number of images with ground-truth\nsentences. Moreover, it is extremely expensive to create a larger dataset by\nannotating millions of images with sentences and may lead to a biased model.\nInspired by the recent success of webly supervised learning in deep neural\nnetworks, we capitalize on readily-available web images with noisy annotations\nto learn robust image-text joint representation. Specifically, our main idea is\nto leverage web images and corresponding tags, along with fully annotated\ndatasets, in training for learning the visual-semantic joint embedding. We\npropose a two-stage approach for the task that can augment a typical supervised\npair-wise ranking loss based formulation with weakly-annotated web images to\nlearn a more robust visual-semantic embedding. Experiments on two standard\nbenchmark datasets demonstrate that our method achieves a significant\nperformance gain in image-text retrieval compared to state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 15:07:52 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mithun", "Niluthpol Chowdhury", ""], ["Panda", "Rameswar", ""], ["Papalexakis", "Evangelos E.", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1808.08013", "submitter": "Jun Feng", "authors": "Jun Feng, Minlie Huang, Li Zhao, Yang Yang and Xiaoyan Zhu", "title": "Reinforcement Learning for Relation Classification from Noisy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing relation classification methods that rely on distant supervision\nassume that a bag of sentences mentioning an entity pair are all describing a\nrelation for the entity pair. Such methods, performing classification at the\nbag level, cannot identify the mapping between a relation and a sentence, and\nlargely suffers from the noisy labeling problem. In this paper, we propose a\nnovel model for relation classification at the sentence level from noisy data.\nThe model has two modules: an instance selector and a relation classifier. The\ninstance selector chooses high-quality sentences with reinforcement learning\nand feeds the selected sentences into the relation classifier, and the relation\nclassifier makes sentence level prediction and provides rewards to the instance\nselector. The two modules are trained jointly to optimize the instance\nselection and relation classification processes. Experiment results show that\nour model can deal with the noise of data effectively and obtains better\nperformance for relation classification at the sentence level.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 06:08:56 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Feng", "Jun", ""], ["Huang", "Minlie", ""], ["Zhao", "Li", ""], ["Yang", "Yang", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1808.08023", "submitter": "Jiayuan He", "authors": "Jiayuan He, Jianzhong Qi, Kotagiri Ramamohanarao", "title": "A Jointly Learned Context-Aware Place of Interest Embedding for Trip\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trip recommendation is an important location-based service that helps relieve\nusers from the time and efforts for trip planning. It aims to recommend a\nsequence of places of interest (POIs) for a user to visit that maximizes the\nuser's satisfaction. When adding a POI to a recommended trip, it is essential\nto understand the context of the recommendation, including the POI popularity,\nother POIs co-occurring in the trip, and the preferences of the user. These\ncontextual factors are learned separately in existing studies, while in\nreality, they impact jointly on a user's choice of a POI to visit. In this\nstudy, we propose a POI embedding model to jointly learn the impact of these\ncontextual factors. We call the learned POI embedding a context-aware POI\nembedding. To showcase the effectiveness of this embedding, we apply it to\ngenerate trip recommendations given a user and a time budget. We propose two\ntrip recommendation algorithms based on our context-aware POI embedding. The\nfirst algorithm finds the exact optimal trip by transforming and solving the\ntrip recommendation problem as an integer linear programming problem. To\nachieve a high computation efficiency, the second algorithm finds a\nheuristically optimal trip based on adaptive large neighborhood search. We\nperform extensive experiments on real datasets. The results show that our\nproposed algorithms consistently outperform state-of-the-art algorithms in trip\nrecommendation quality, with an advantage of up to 43% in F1-score.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 06:52:04 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["He", "Jiayuan", ""], ["Qi", "Jianzhong", ""], ["Ramamohanarao", "Kotagiri", ""]]}, {"id": "1808.08121", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Donald E. Brown, Kiana Jafari\n  Meimandi, Laura E. Barnes", "title": "An Improvement of Data Classification Using Random Multimodel Deep\n  Learning (RMDL)", "comments": "published in International Journal of Machine Learning and Computing\n  (IJMLC). arXiv admin note: substantial text overlap with arXiv:1805.01890", "journal-ref": null, "doi": "10.18178/ijmlc.2018.8.4.703", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential growth in the number of complex datasets every year requires\nmore enhancement in machine learning methods to provide robust and accurate\ndata classification. Lately, deep learning approaches have achieved surpassing\nresults in comparison to previous machine learning algorithms. However, finding\nthe suitable structure for these models has been a challenge for researchers.\nThis paper introduces Random Multimodel Deep Learning (RMDL): a new ensemble,\ndeep learning approach for classification. RMDL solves the problem of finding\nthe best deep learning structure and architecture while simultaneously\nimproving robustness and accuracy through ensembles of deep learning\narchitectures. In short, RMDL trains multiple randomly generated models of Deep\nNeural Network (DNN), Convolutional Neural Network (CNN) and Recurrent Neural\nNetwork (RNN) in parallel and combines their results to produce better result\nof any of those models individually. In this paper, we describe RMDL model and\ncompare the results for image and text classification as well as face\nrecognition. We used MNIST and CIFAR-10 datasets as ground truth datasets for\nimage classification and WOS, Reuters, IMDB, and 20newsgroup datasets for text\nclassification. Lastly, we used ORL dataset to compare the model performance on\nface recognition task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 00:38:14 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Meimandi", "Kiana Jafari", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1808.08274", "submitter": "Maria Soledad Pera", "authors": "Ion Madrazo Azpiazu, Michael Green, Oghenemaro Anuyah, Maria Soledad\n  Pera", "title": "Can we leverage rating patterns from traditional users to enhance\n  recommendations for children?", "comments": "ACM RecSys 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender algorithms performance is often associated with the availability\nof sufficient historical rating data. Unfortunately, when it comes to children,\nthis data is seldom available. In this paper, we report on an initial analysis\nconducted to examine the degree to which data about traditional users, i.e.,\nadults, can be leveraged to enhance the recommendation process for children.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 19:27:31 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Azpiazu", "Ion Madrazo", ""], ["Green", "Michael", ""], ["Anuyah", "Oghenemaro", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "1808.08316", "submitter": "Tu  Nguyen", "authors": "Tu Ngoc Nguyen, Tuan Tran and Wolfgang Nejdl", "title": "A Trio Neural Model for Dynamic Entity Relatedness Ranking", "comments": "In Proceedings of CoNLL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring entity relatedness is a fundamental task for many natural language\nprocessing and information retrieval applications. Prior work often studies\nentity relatedness in static settings and an unsupervised manner. However,\nentities in real-world are often involved in many different relationships,\nconsequently entity-relations are very dynamic over time. In this work, we\npropose a neural networkbased approach for dynamic entity relatedness,\nleveraging the collective attention as supervision. Our model is capable of\nlearning rich and different entity representations in a joint framework.\nThrough extensive experiments on large-scale datasets, we demonstrate that our\nmethod achieves better results than competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 21:29:53 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 22:00:03 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 00:38:54 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Nguyen", "Tu Ngoc", ""], ["Tran", "Tuan", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1808.08357", "submitter": "Bijil Abraham Philip", "authors": "Bijil Abraham Philip, Manas Jog, Apurv Milind Upasani", "title": "Dr. Tux: A Question Answering System for Ubuntu users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various forums and question answering (Q&A) sites are available online that\nallow Ubuntu users to find results similar to their queries. However, searching\nfor a result is often time consuming as it requires the user to find a specific\nproblem instance relevant to his/her query from a large set of questions. In\nthis paper, we present an automated question answering system for Ubuntu users\ncalled Dr. Tux that is designed to answer user's queries by selecting the most\nsimilar question from an online database. The prototype was implemented in\nPython and uses NLTK and CoreNLP tools for Natural Language Processing. The\ndata for the prototype was taken from the AskUbuntu website which contains\nabout 150k questions. The results obtained from the manual evaluation of the\nprototype were promising while also presenting some interesting opportunities\nfor improvement.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 05:17:43 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Philip", "Bijil Abraham", ""], ["Jog", "Manas", ""], ["Upasani", "Apurv Milind", ""]]}, {"id": "1808.08643", "submitter": "Yi Luan", "authors": "Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi", "title": "Scientific Relation Extraction with Selectively Incorporated Concept\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission for the SemEval 2018 Task 7 shared task\non semantic relation extraction and classification in scientific papers. We\nextend the end-to-end relation extraction model of (Miwa and Bansal) with\nenhancements such as a character-level encoding attention mechanism on\nselecting pretrained concept candidate embeddings. Our official submission\nranked the second in relation classification task (Subtask 1.1 and Subtask 2\nSenerio 2), and the first in the relation extraction task (Subtask 2 Scenario\n1).\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 23:31:02 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Luan", "Yi", ""], ["Ostendorf", "Mari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "1808.08836", "submitter": "Ana Valeria Gonzalez", "authors": "Ana V. Gonz\\'alez-Gardu\\~no, Isabelle Augenstein, Anders S{\\o}gaard", "title": "A strong baseline for question relevancy ranking", "comments": "To appear at EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The best systems at the SemEval-16 and SemEval-17 community question\nanswering shared tasks -- a task that amounts to question relevancy ranking --\ninvolve complex pipelines and manual feature engineering. Despite this, many of\nthese still fail at beating the IR baseline, i.e., the rankings provided by\nGoogle's search engine. We present a strong baseline for question relevancy\nranking by training a simple multi-task feed forward network on a bag of 14\ndistance measures for the input question pair. This baseline model, which is\nfast to train and uses only language-independent features, outperforms the best\nshared task systems on the task of retrieving relevant previously asked\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 13:19:49 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Gonz\u00e1lez-Gardu\u00f1o", "Ana V.", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1808.08999", "submitter": "Florian Reitz", "authors": "Florian Reitz", "title": "Harnessing Historical Corrections to build Test Collections for Named\n  Entity Disambiguation", "comments": "Preprint of a paper accepted at TPDL 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00066-0_4", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching mentions of persons to the actual persons (the name disambiguation\nproblem) is central for several digital library applications. Scientists have\nbeen working on algorithms to create this matching for decades without finding\na universal solution. One problem is that test collections for this problem are\noften small and specific to a certain collection. In this work, we present an\napproach that can create large test collections from historical metadata with\nminimal extra cost. We apply this approach to the DBLP collection to generate\ntwo freely available test collections. One collection focuses on the properties\nof defects and one on the evaluation of disambiguation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:05:02 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Reitz", "Florian", ""]]}, {"id": "1808.09036", "submitter": "Dominika Tkaczyk", "authors": "Dominika Tkaczyk, Paraic Sheridan, Joeran Beel", "title": "ParsRec: Meta-Learning Recommendations for Bibliographic Reference\n  Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliographic reference parsers extract metadata (e.g. author names, title,\nyear) from bibliographic reference strings. No reference parser consistently\ngives the best results in every scenario. For instance, one tool may be best in\nextracting titles, and another tool in extracting author names. In this paper,\nwe address the problem of reference parsing from a recommender-systems\nperspective. We propose ParsRec, a meta-learning approach that recommends the\npotentially best parser(s) for a given reference string. We evaluate ParsRec on\n105k references from chemistry. We propose two approaches to meta-learning\nrecommendations. The first approach learns the best parser for an entire\nreference string. The second approach learns the best parser for each field of\na reference string. The second approach achieved a 2.6% increase in F1 (0.909\nvs. 0.886, p < 0.001) over the best single parser (GROBID), reducing the false\npositive rate by 20.2% (0.075 vs. 0.094), and the false negative rate by 18.9%\n(0.107 vs. 0.132).\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 21:27:06 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Sheridan", "Paraic", ""], ["Beel", "Joeran", ""]]}, {"id": "1808.09198", "submitter": "Kwei-Herng Lai", "authors": "Chih-Chun Hsia, Kwei-Herng Lai, Yian Chen, Chuan-Ju Wang, Ming-Feng\n  Tsai", "title": "Representation Learning for Image-based Music Recommendation", "comments": "2 pages, LBRS@RecSys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image perception is one of the most direct ways to provide contextual\ninformation about a user concerning his/her surrounding environment; hence\nimages are a suitable proxy for contextual recommendation. We propose a novel\nrepresentation learning framework for image-based music recommendation that\nbridges the heterogeneity gap between music and image data; the proposed method\nis a key component for various contextual recommendation tasks. Preliminary\nexperiments show that for an image-to-song retrieval task, the proposed method\nretrieves relevant or conceptually similar songs for input images.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 09:47:53 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 04:37:46 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Hsia", "Chih-Chun", ""], ["Lai", "Kwei-Herng", ""], ["Chen", "Yian", ""], ["Wang", "Chuan-Ju", ""], ["Tsai", "Ming-Feng", ""]]}, {"id": "1808.09224", "submitter": "V\\'it Novotn\\'y", "authors": "Petr Sojka (1), Michal R\\r{u}\\v{z}i\\v{c}ka (1) and V\\'it Novotn\\'y (1)\n  ((1) Faculty of Informatics, Masaryk University, Brno, Czech Republic)", "title": "MIaS: Math-Aware Retrieval in Digital Mathematical Libraries", "comments": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in The 27th ACM International Conference on Information and\n  Knowledge Management (CIKM '18), October 22-26, 2018, Torino, Italy,\n  https://doi.org/10.1145/3269206.3269233", "journal-ref": null, "doi": "10.1145/3269206.3269233", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital mathematical libraries (DMLs) such as arXiv, Numdam, and EuDML\ncontain mainly documents from STEM fields, where mathematical formulae are\noften more important than text for understanding. Conventional information\nretrieval (IR) systems are unable to represent formulae and they are therefore\nill-suited for math information retrieval (MIR). To fill the gap, we have\ndeveloped, and open-sourced the MIaS MIR system. MIaS is based on the full-text\nsearch engine Apache Lucene. On top of text retrieval, MIaS also incorporates a\nset of tools for preprocessing mathematical formulae. We describe the design of\nthe system and present speed, and quality evaluation results. We show that MIaS\nis both efficient, and effective, as evidenced by our victory in the NTCIR-11\nMath-2 task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 11:13:20 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Sojka", "Petr", "", "Faculty of Informatics, Masaryk University, Brno, Czech Republic"], ["R\u016f\u017ei\u010dka", "Michal", "", "Faculty of Informatics, Masaryk University, Brno, Czech Republic"], ["Novotn\u00fd", "V\u00edt", "", "Faculty of Informatics, Masaryk University, Brno, Czech Republic"]]}, {"id": "1808.09270", "submitter": "Benjamin Horne", "authors": "Benjamin D. Horne, William Dron, and Sibel Adali", "title": "Models for Predicting Community-Specific Interest in News Articles", "comments": "Published at IEEE MILCOM 2018 in Los Angeles, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we ask two questions: 1. Can we predict the type of community\ninterested in a news article using only features from the article content? and\n2. How well do these models generalize over time? To answer these questions, we\ncompute well-studied content-based features on over 60K news articles from 4\ncommunities on reddit.com. We train and test models over three different time\nperiods between 2015 and 2017 to demonstrate which features degrade in\nperformance the most due to concept drift. Our models can classify news\narticles into communities with high accuracy, ranging from 0.81 ROC AUC to 1.0\nROC AUC. However, while we can predict the community-specific popularity of\nnews articles with high accuracy, practitioners should approach these models\ncarefully. Predictions are both community-pair dependent and feature group\ndependent. Moreover, these feature groups generalize over time differently,\nwith some only degrading slightly over time, but others degrading greatly.\nTherefore, we recommend that community-interest predictions are done in a\nhierarchical structure, where multiple binary classifiers can be used to\nseparate community pairs, rather than a traditional multi-class model. Second,\nthese models should be retrained over time based on accuracy goals and the\navailability of training data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 17:42:14 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Horne", "Benjamin D.", ""], ["Dron", "William", ""], ["Adali", "Sibel", ""]]}, {"id": "1808.09271", "submitter": "Lex Razoux Schultz", "authors": "Lex Razoux Schultz, Marco Loog, Peyman Mohajerin Esfahani", "title": "Distance Based Source Domain Selection for Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sentiment classification (SC) on short text fragments has received\nincreasing attention in recent years. Performing SC on unseen domains with few\nor no labeled samples can significantly affect the classification performance\ndue to different expression of sentiment in source and target domain. In this\nstudy, we aim to mitigate this undesired impact by proposing a methodology\nbased on a predictive measure, which allows us to select an optimal source\ndomain from a set of candidates. The proposed measure is a linear combination\nof well-known distance functions between probability distributions supported on\nthe source and target domains (e.g. Earth Mover's distance and Kullback-Leibler\ndivergence). The performance of the proposed methodology is validated through\nan SC case study in which our numerical experiments suggest a significant\nimprovement in the cross domain classification error in comparison with a\nrandom selected source domain for both a naive and adaptive learning setting.\nIn the case of more heterogeneous datasets, the predictability feature of the\nproposed model can be utilized to further select a subset of candidate domains,\nwhere the corresponding classifier outperforms the one trained on all available\nsource domains. This observation reinforces a hypothesis that our proposed\nmodel may also be deployed as a means to filter out redundant information\nduring a training phase of SC.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:14:33 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Schultz", "Lex Razoux", ""], ["Loog", "Marco", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "1808.09353", "submitter": "Morgan Gallant", "authors": "Morgan Gallant, Haruna Isah, Farhana Zulkernine, Shahzad Khan", "title": "Xu: An Automated Query Expansion and Optimization Tool", "comments": "Accepted to IEEE COMPSAC 2019", "journal-ref": null, "doi": "10.1109/COMPSAC.2019.00070", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The exponential growth of information on the Internet is a big challenge for\ninformation retrieval systems towards generating relevant results. Novel\napproaches are required to reformat or expand user queries to generate a\nsatisfactory response and increase recall and precision. Query expansion (QE)\nis a technique to broaden users' queries by introducing additional tokens or\nphrases based on some semantic similarity metrics. The tradeoff is the added\ncomputational complexity to find semantically similar words and a possible\nincrease in noise in information retrieval. Despite several research efforts on\nthis topic, QE has not yet been explored enough and more work is needed on\nsimilarity matching and composition of query terms with an objective to\nretrieve a small set of most appropriate responses. QE should be scalable,\nfast, and robust in handling complex queries with a good response time and\nnoise ceiling. In this paper, we propose Xu, an automated QE technique, using\nhigh dimensional clustering of word vectors and Datamuse API, an open source\nquery engine to find semantically similar words. We implemented Xu as a command\nline tool and evaluated its performances using datasets containing news\narticles and human-generated QEs. The evaluation results show that Xu was\nbetter than Datamuse by achieving about 88% accuracy with reference to the\nhuman-generated QE.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:18:14 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 22:08:28 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Gallant", "Morgan", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""], ["Khan", "Shahzad", ""]]}, {"id": "1808.09397", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, Naveed Afzal, Sunyang Fu, Liwei Wang, Feichen Shen,\n  Majid Rastegar-Mojarad, Hongfang Liu", "title": "MedSTS: A Resource for Clinical Semantic Textual Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of electronic health records (EHRs) has enabled a wide\nrange of applications leveraging EHR data. However, the meaningful use of EHR\ndata largely depends on our ability to efficiently extract and consolidate\ninformation embedded in clinical text where natural language processing (NLP)\ntechniques are essential. Semantic textual similarity (STS) that measures the\nsemantic similarity between text snippets plays a significant role in many NLP\napplications. In the general NLP domain, STS shared tasks have made available a\nhuge collection of text snippet pairs with manual annotations in various\ndomains. In the clinical domain, STS can enable us to detect and eliminate\nredundant information that may lead to a reduction in cognitive burden and an\nimprovement in the clinical decision-making process. This paper elaborates our\nefforts to assemble a resource for STS in the medical domain, MedSTS. It\nconsists of a total of 174,629 sentence pairs gathered from a clinical corpus\nat Mayo Clinic. A subset of MedSTS (MedSTS_ann) containing 1,068 sentence pairs\nwas annotated by two medical experts with semantic similarity scores of 0-5\n(low to high similarity). We further analyzed the medical concepts in the\nMedSTS corpus, and tested four STS systems on the MedSTS_ann corpus. In the\nfuture, we will organize a shared task by releasing the MedSTS_ann corpus to\nmotivate the community to tackle the real world clinical problems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 16:43:19 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Wang", "Yanshan", ""], ["Afzal", "Naveed", ""], ["Fu", "Sunyang", ""], ["Wang", "Liwei", ""], ["Shen", "Feichen", ""], ["Rastegar-Mojarad", "Majid", ""], ["Liu", "Hongfang", ""]]}, {"id": "1808.09407", "submitter": "V\\'it Novotn\\'y", "authors": "V\\'it Novotn\\'y (1) ((1) Faculty of Informatics, Masaryk University,\n  Brno, Czech Republic)", "title": "Implementation Notes for the Soft Cosine Measure", "comments": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in The 27th ACM International Conference on Information and\n  Knowledge Management (CIKM '18), October 22-26, 2018, Torino, Italy,\n  https://doi.org/10.1145/3269206.3269317", "journal-ref": null, "doi": "10.1145/3269206.3269317", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard bag-of-words vector space model (VSM) is efficient, and\nubiquitous in information retrieval, but it underestimates the similarity of\ndocuments with the same meaning, but different terminology. To overcome this\nlimitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that\nincorporates term similarity relations. Charlet and Damnati showed that the SCM\nis highly effective in question answering (QA) systems. However, the\northonormalization algorithm proposed by Sidorov et al. has an impractical time\ncomplexity of $\\mathcal O(n^4)$, where n is the size of the vocabulary.\n  In this paper, we prove a tighter lower worst-case time complexity bound of\n$\\mathcal O(n^3)$. We also present an algorithm for computing the similarity\nbetween documents and we show that its worst-case time complexity is $\\mathcal\nO(1)$ given realistic conditions. Lastly, we describe implementation in\ngeneral-purpose vector databases such as Annoy, and Faiss and in the inverted\nindices of text search engines such as Apache Lucene, and ElasticSearch. Our\nresults enable the deployment of the SCM in real-world information retrieval\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 16:57:34 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Novotn\u00fd", "V\u00edt", ""]]}, {"id": "1808.09781", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang and Julian McAuley", "title": "Self-Attentive Sequential Recommendation", "comments": "Accepted by ICDM'18 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential dynamics are a key feature of many modern recommender systems,\nwhich seek to capture the `context' of users' activities on the basis of\nactions they have performed recently. To capture such patterns, two approaches\nhave proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs).\nMarkov Chains assume that a user's next action can be predicted on the basis of\njust their last (or last few) actions, while RNNs in principle allow for\nlonger-term semantics to be uncovered. Generally speaking, MC-based methods\nperform best in extremely sparse datasets, where model parsimony is critical,\nwhile RNNs perform better in denser datasets where higher model complexity is\naffordable. The goal of our work is to balance these two goals, by proposing a\nself-attention based sequential model (SASRec) that allows us to capture\nlong-term semantics (like an RNN), but, using an attention mechanism, makes its\npredictions based on relatively few actions (like an MC). At each time step,\nSASRec seeks to identify which items are `relevant' from a user's action\nhistory, and use them to predict the next item. Extensive empirical studies\nshow that our method outperforms various state-of-the-art sequential models\n(including MC/CNN/RNN-based approaches) on both sparse and dense datasets.\nMoreover, the model is an order of magnitude more efficient than comparable\nCNN/RNN-based models. Visualizations on attention weights also show how our\nmodel adaptively handles datasets with various density, and uncovers meaningful\npatterns in activity sequences.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 04:28:05 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["McAuley", "Julian", ""]]}, {"id": "1808.09784", "submitter": "Ting-Hsiang Wang", "authors": "Kwei-Herng Lai, Ting-Hsiang Wang, Heng-Yu Chi, Yian Chen, Ming-Feng\n  Tsai, and Chuan-Ju Wang", "title": "Superhighway: Bypass Data Sparsity in Cross-Domain CF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain collaborative filtering (CF) aims to alleviate data sparsity in\nsingle-domain CF by leveraging knowledge transferred from related domains. Many\ntraditional methods focus on enriching compared neighborhood relations in CF\ndirectly to address the sparsity problem. In this paper, we propose\nsuperhighway construction, an alternative explicit relation-enrichment\nprocedure, to improve recommendations by enhancing cross-domain connectivity.\nSpecifically, assuming partially overlapped items (users), superhighway\nbypasses multi-hop inter-domain paths between cross-domain users (items,\nrespectively) with direct paths to enrich the cross-domain connectivity. The\nexperiments conducted on a real-world cross-region music dataset and a\ncross-platform movie dataset show that the proposed superhighway construction\nsignificantly improves recommendation performance in both target and source\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 13:07:34 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Lai", "Kwei-Herng", ""], ["Wang", "Ting-Hsiang", ""], ["Chi", "Heng-Yu", ""], ["Chen", "Yian", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "1808.09785", "submitter": "Farhan Khawar", "authors": "Farhan Khawar and Nevin L. Zhang", "title": "Using Taste Groups for Collaborative Filtering", "comments": "RecSys 2018 LBRS. arXiv admin note: substantial text overlap with\n  arXiv:1704.01889", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback is the simplest form of user feedback that can be used for\nitem recommendation. It is easy to collect and domain independent. However,\nthere is a lack of negative examples. Existing works circumvent this problem by\nmaking various assumptions regarding the unconsumed items, which fail to hold\nwhen the user did not consume an item because she was unaware of it. In this\npaper, we propose as a novel method for addressing the lack of negative\nexamples in implicit feedback. The motivation is that if there is a large group\nof users who share the same taste and none of them consumed an item, then it is\nhighly likely that the item is irrelevant to this taste. We use Hierarchical\nLatent Tree Analysis(HLTA) to identify taste-based user groups and make\nrecommendations for a user based on her memberships in the groups.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:53:14 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1808.09890", "submitter": "Isak Czeresnia Etinger", "authors": "Isak Czeresnia Etinger", "title": "An Adaptive Conversational Bot Framework", "comments": "5 pages, Microsoft, \\'Ecole polytechnique", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we enable users to heavily specify criteria for database queries in a\nuser-friendly way? This paper describes a general framework of a conversational\nbot that extracts meaningful information from user's sentences, that asks\nsubsequent questions to complete missing information, and that adjusts its\nquestions and information-extraction parameters for later conversations\ndepending on users' behavior. Additionally, we provide a comparison of existing\ntools and give novel techniques to implement such framework. Finally, we\nexemplify the framework with a bot to query movies in a database, whose code is\navailable for Microsoft employees.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 21:37:42 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Etinger", "Isak Czeresnia", ""]]}, {"id": "1808.10031", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Mengting Wan, Julian McAuley", "title": "Recommendation Through Mixtures of Heterogeneous Item Relationships", "comments": "10 pages, 6 figures. Accepted by CIKM'18 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems have proliferated as general-purpose approaches to model\na wide variety of consumer interaction data. Specific instances make use of\nsignals ranging from user feedback, item relationships, geographic locality,\nsocial influence (etc.). Typically, research proceeds by showing that making\nuse of a specific signal (within a carefully designed model) allows for\nhigher-fidelity recommendations on a particular dataset. Of course, the real\nsituation is more nuanced, in which a combination of many signals may be at\nplay, or favored in different proportion by individual users. Here we seek to\ndevelop a framework that is capable of combining such heterogeneous item\nrelationships by simultaneously modeling (a) what modality of recommendation is\na user likely to be susceptible to at a particular point in time; and (b) what\nis the best recommendation from each modality. Our method borrows ideas from\nmixtures-of-experts approaches as well as knowledge graph embeddings. We find\nthat our approach naturally yields more accurate recommendations than\nalternatives, while also providing intuitive `explanations' behind the\nrecommendations it provides.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 20:16:57 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Wan", "Mengting", ""], ["McAuley", "Julian", ""]]}, {"id": "1808.10259", "submitter": "Aboubakr Aqle", "authors": "Aboubakr Aqle, Dena Al-Thani, Ali Jaoua", "title": "Analyze Unstructured Data Patterns for Conceptual Representation", "comments": "4 pages, 6 Figures, 4th Annual Conference on Computational Science &\n  Computational Intelligence (CSCI'17)", "journal-ref": null, "doi": "10.1109/CSCI.2017.46", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online news media provides aggregated news and stories from different sources\nall over the world and up-to-date news coverage. The main goal of this study is\nto have a solution that considered as a homogeneous source for the news and to\nrepresent the news in a new conceptual framework. Furthermore, the user can\neasily find different updated news in a fast way through the designed\ninterface. The Mobile App implementation is based on modeling the multi-level\nconceptual analysis discipline. Discovering main concepts of any domain is\ncaptured from the hidden unstructured data that are analyzed by the proposed\nsolution. Concepts are discovered through analyzing data patterns to be\nstructured into a tree-based interface for easy navigation for the end user,\nthrough the discovered news concepts. Our final experiment results showing that\nanalyzing the news before displaying to the end-user and restructuring the\nfinal output in a conceptual multilevel structure, that producing new display\nframe for the end user to find the related information to his interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:06:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Aqle", "Aboubakr", ""], ["Al-Thani", "Dena", ""], ["Jaoua", "Ali", ""]]}, {"id": "1808.10260", "submitter": "Benedikt Loepp", "authors": "Johannes Kunkel, Benedikt Loepp, J\\\"urgen Ziegler", "title": "Understanding Latent Factors Using a GWAP", "comments": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys '18), Vancouver, BC, Canada,\n  October 2-7, 2018, 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems relying on latent factor models often appear as black\nboxes to their users. Semantic descriptions for the factors might help to\nmitigate this problem. Achieving this automatically is, however, a\nnon-straightforward task due to the models' statistical nature. We present an\noutput-agreement game that represents factors by means of sample items and\nmotivates players to create such descriptions. A user study shows that the\ncollected output actually reflects real-world characteristics of the factors.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 11:04:07 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Kunkel", "Johannes", ""], ["Loepp", "Benedikt", ""], ["Ziegler", "J\u00fcrgen", ""]]}, {"id": "1808.10261", "submitter": "Jiangning Chen", "authors": "Jiangning Chen, Heinrich Matzinger, Haoyan Zhai, Mi Zhou", "title": "Centroid estimation based on symmetric KL divergence for Multinomial\n  text classification problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new method to estimate centroid for text classification based on\nthe symmetric KL-divergence between the distribution of words in training\ndocuments and their class centroids. Experiments on several standard data sets\nindicate that the new method achieves substantial improvements over the\ntraditional classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:24:33 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:40:54 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chen", "Jiangning", ""], ["Matzinger", "Heinrich", ""], ["Zhai", "Haoyan", ""], ["Zhou", "Mi", ""]]}, {"id": "1808.10351", "submitter": "Albin Correya", "authors": "Albin Andrew Correya, Romain Hennequin, Micka\\\"el Arcos", "title": "Large-Scale Cover Song Detection in Digital Music Libraries Using\n  Metadata, Lyrics and Audio Features", "comments": "Music Information Retrieval, Cover Song Identification, Million Song\n  Dataset, Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cover song detection is a very relevant task in Music Information Retrieval\n(MIR) studies and has been mainly addressed using audio-based systems. Despite\nits potential impact in industrial contexts, low performances and lack of\nscalability have prevented such systems from being adopted in practice for\nlarge applications. In this work, we investigate whether textual music\ninformation (such as metadata and lyrics) can be used along with audio for\nlarge-scale cover identification problem in a wide digital music library. We\nbenchmark this problem using standard text and state of the art audio\nsimilarity measures. Our studies shows that these methods can significantly\nincrease the accuracy and scalability of cover detection systems on Million\nSong Dataset (MSD) and Second Hand Song (SHS) datasets. By only leveraging\nstandard tf-idf based text similarity measures on song titles and lyrics, we\nachieved 35.5% of absolute increase in mean average precision compared to the\ncurrent scalable audio content-based state of the art methods on MSD. These\nexperimental results suggests that new methodologies can be encouraged among\nresearchers to leverage and identify more sophisticated NLP-based techniques to\nimprove current cover song identification systems in digital music libraries\nwith metadata.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:26:49 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Correya", "Albin Andrew", ""], ["Hennequin", "Romain", ""], ["Arcos", "Micka\u00ebl", ""]]}, {"id": "1808.10523", "submitter": "Lei Zheng", "authors": "Lei Zheng, Chun-Ta Lu, Fei Jiang, Jiawei Zhang, Philip S. Yu", "title": "Spectral Collaborative Filtering", "comments": "RecSys2018", "journal-ref": null, "doi": "10.1145/3240323.3240343", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularity of Collaborative Filtering (CF), CF-based methods are\nhaunted by the \\textit{cold-start} problem, which has a significantly negative\nimpact on users' experiences with Recommender Systems (RS). In this paper, to\novercome the aforementioned drawback, we first formulate the relationships\nbetween users and items as a bipartite graph. Then, we propose a new spectral\nconvolution operation directly performing in the \\textit{spectral domain},\nwhere not only the proximity information of a graph but also the connectivity\ninformation hidden in the graph are revealed. With the proposed spectral\nconvolution operation, we build a deep recommendation model called Spectral\nCollaborative Filtering (SpectralCF). Benefiting from the rich information of\nconnectivity existing in the \\textit{spectral domain}, SpectralCF is capable of\ndiscovering deep connections between users and items and therefore, alleviates\nthe \\textit{cold-start} problem for CF. To the best of our knowledge,\nSpectralCF is the first CF-based method directly learning from the\n\\textit{spectral domains} of user-item bipartite graphs. We apply our method on\nseveral standard datasets. It is shown that SpectralCF significantly\noutperforms state-of-the-art models. Code and data are available at\n\\url{https://github.com/lzheng21/SpectralCF}.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:12:03 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Zheng", "Lei", ""], ["Lu", "Chun-Ta", ""], ["Jiang", "Fei", ""], ["Zhang", "Jiawei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1808.10550", "submitter": "Georgios Pitsilis", "authors": "Georgios Pitsilis, Heri Ramampiaro, Helge Langseth", "title": "Securing Tag-based recommender systems against profile injection\n  attacks: A comparative study", "comments": "2-pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses challenges related to attacks on social tagging systems,\nwhich often comes in a form of malicious annotations or profile injection\nattacks. In particular, we study various countermeasures against two types of\nthreats for such systems, the Overload and the Piggyback attacks. The studied\ncountermeasures include baseline classifiers such as, Naive Bayes filter and\nSupport Vector Machine, as well as a deep learning-based approach. Our\nevaluation performed over synthetic spam data, generated from del.icio.us,\nshows that in most cases, the deep learning-based approach provides the best\nprotection against threats.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 23:55:33 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Pitsilis", "Georgios", ""], ["Ramampiaro", "Heri", ""], ["Langseth", "Helge", ""]]}, {"id": "1808.10600", "submitter": "Seungjin Lee", "authors": "Seungjin Lee, Juheon Lee, Kyogu lee", "title": "Content-based feature exploration for transparent music recommendation\n  using self-attentive genre classification", "comments": "to appear at ACM RecSys '18 poster(LBR) session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretation of retrieved results is an important issue in music\nrecommender systems, particularly from a user perspective. In this study, we\ninvestigate the methods for providing interpretability of content features\nusing self-attention. We extract lyric features with the self-attentive genre\nclassification model trained on 140,000 tracks of lyrics. Likewise, we extract\nacoustic features using the acoustic model with self-attention trained on\n120,000 tracks of acoustic signals. The experimental results show that the\nproposed methods provide the characteristics that are interpretable in terms of\nboth lyrical and musical contents. We demonstrate this by visualizing the\nattention weights, and by presenting the most similar songs found using lyric\nor audio features.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 05:05:05 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 11:18:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Lee", "Seungjin", ""], ["Lee", "Juheon", ""], ["lee", "Kyogu", ""]]}, {"id": "1808.10628", "submitter": "Kyosuke Nishida", "authors": "Kyosuke Nishida, Itsumi Saito, Atsushi Otsuka, Hisako Asano, Junji\n  Tomita", "title": "Retrieve-and-Read: Multi-task Learning of Information Retrieval and\n  Reading Comprehension", "comments": "10 pages, 6 figure. Accepted as a full paper at CIKM 2018", "journal-ref": "CIKM 2018, October 22-26, 2018, Torino, Italy", "doi": "10.1145/3269206.3271702", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the task of machine reading at scale (MRS) wherein,\ngiven a question, a system first performs the information retrieval (IR) task\nof finding relevant passages in a knowledge source and then carries out the\nreading comprehension (RC) task of extracting an answer span from the passages.\nPrevious MRS studies, in which the IR component was trained without considering\nanswer spans, struggled to accurately find a small number of relevant passages\nfrom a large set of passages. In this paper, we propose a simple and effective\napproach that incorporates the IR and RC tasks by using supervised multi-task\nlearning in order that the IR component can be trained by considering answer\nspans. Experimental results on the standard benchmark, answering SQuAD\nquestions using the full Wikipedia as the knowledge source, showed that our\nmodel achieved state-of-the-art performance. Moreover, we thoroughly evaluated\nthe individual contributions of our model components with our new Japanese\ndataset and SQuAD. The results showed significant improvements in the IR task\nand provided a new perspective on IR for RC: it is effective to teach which\npart of the passage answers the question rather than to give only a relevance\nscore to the whole passage.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 08:22:12 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Nishida", "Kyosuke", ""], ["Saito", "Itsumi", ""], ["Otsuka", "Atsushi", ""], ["Asano", "Hisako", ""], ["Tomita", "Junji", ""]]}, {"id": "1808.10664", "submitter": "Maurizio Ferrari Dacrema", "authors": "Cesare Bernardis, Maurizio Ferrari Dacrema and Paolo Cremonesi", "title": "A novel graph-based model for hybrid recommendations in cold-start\n  scenarios", "comments": null, "journal-ref": "Proceedings of the Late-Breaking Results track part of the Twelfth\n  ACM Conference on Recommender Systems (RecSys 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold-start is a very common and still open problem in the Recommender Systems\nliterature. Since cold start items do not have any interaction, collaborative\nalgorithms are not applicable. One of the main strategies is to use pure or\nhybrid content-based approaches, which usually yield to lower recommendation\nquality than collaborative ones. Some techniques to optimize performance of\nthis type of approaches have been studied in recent past. One of them is called\nfeature weighting, which assigns to every feature a real value, called weight,\nthat estimates its importance. Statistical techniques for feature weighting\ncommonly used in Information Retrieval, like TF-IDF, have been adapted for\nRecommender Systems, but they often do not provide sufficient quality\nimprovements. More recent approaches, FBSM and LFW, estimate weights by\nleveraging collaborative information via machine learning, in order to learn\nthe importance of a feature based on other users opinions. This type of models\nhave shown promising results compared to classic statistical analyzes cited\npreviously. We propose a novel graph, feature-based machine learning model to\nface the cold-start item scenario, learning the relevance of features from\nprobabilities of item-based collaborative filtering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 10:22:32 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bernardis", "Cesare", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}]