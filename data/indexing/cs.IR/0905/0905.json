[{"id": "0905.1130", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Florian Boudin, Patricia Velazquez-Morales and Juan-Manuel\n  Torres-Moreno", "title": "Statistical Automatic Summarization in Organic Chemistry", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an oriented numerical summarizer algorithm, applied to producing\nautomatic summaries of scientific documents in Organic Chemistry. We present\nits implementation named Yachs (Yet Another Chemistry Summarizer) that combines\na specific document pre-processing with a sentence scoring method relying on\nthe statistical properties of documents. We show that Yachs achieves the best\nresults among several other summarizers on a corpus of Organic Chemistry\narticles.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2009 20:21:45 GMT"}], "update_date": "2009-05-11", "authors_parsed": [["Boudin", "Florian", ""], ["Velazquez-Morales", "Patricia", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "0905.1594", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez and David W. Allen and Joshua Shinavier and Gary\n  Ebersole", "title": "A Recommender System to Support the Scholarly Communication Process", "comments": null, "journal-ref": null, "doi": null, "report-no": "KRS-2009-02", "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of researchers, articles, journals, conferences, funding\nopportunities, and other such scholarly resources continues to grow every year\nand at an increasing rate. Many services have emerged to support scholars in\nnavigating particular aspects of this resource-rich environment. Some\ncommercial publishers provide recommender and alert services for the articles\nand journals in their digital libraries. Similarly, numerous noncommercial\nsocial bookmarking services have emerged for citation sharing. While these\nservices do provide some support, they lack an understanding of the various\nproblem-solving scenarios that researchers face daily. Example scenarios, to\nname a few, include when a scholar is in search of an article related to\nanother article of interest, when a scholar is in search of a potential\ncollaborator for a funding opportunity, when a scholar is in search of an\noptimal venue to which to submit their article, and when a scholar, in the role\nof an editor, is in search of referees to review an article. All of these\nexample scenarios can be represented as a problem in information filtering by\nmeans of context-sensitive recommendation. This article presents an overview of\na context-sensitive recommender system to support the scholarly communication\nprocess that is based on the standards and technology set forth by the Semantic\nWeb initiative.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2009 18:58:16 GMT"}], "update_date": "2009-05-12", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Allen", "David W.", ""], ["Shinavier", "Joshua", ""], ["Ebersole", "Gary", ""]]}, {"id": "0905.2416", "submitter": "Dimitrios Katsaros", "authors": "Leonidas Akritidis, Dimitrios Katsaros, Panayiotis Bozanis", "title": "Identifying Influential Bloggers: Time Does Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blogs have recently become one of the most favored services on the Web. Many\nusers maintain a blog and write posts to express their opinion, experience and\nknowledge about a product, an event and every subject of general or specific\ninterest. More users visit blogs to read these posts and comment them. This\n\"participatory journalism\" of blogs has such an impact upon the masses that\nKeller and Berry argued that through blogging \"one American in tens tells the\nother nine how to vote, where to eat and what to buy\" \\cite{keller1}.\nTherefore, a significant issue is how to identify such influential bloggers.\nThis problem is very new and the relevant literature lacks sophisticated\nsolutions, but most importantly these solutions have not taken into account\ntemporal aspects for identifying influential bloggers, even though the time is\nthe most critical aspect of the Blogosphere. This article investigates the\nissue of identifying influential bloggers by proposing two easily computed\nblogger ranking methods, which incorporate temporal aspects of the blogging\nactivity. Each method is based on a specific metric to score the blogger's\nposts. The first metric, termed MEIBI, takes into consideration the number of\nthe blog post's inlinks and its comments, along with the publication date of\nthe post. The second metric, MEIBIX, is used to score a blog post according to\nthe number and age of the blog post's inlinks and its comments. These methods\nare evaluated against the state-of-the-art influential blogger identification\nmethod utilizing data collected from a real-world community blog site. The\nobtained results attest that the new methods are able to better identify\nsignificant temporal patterns in the blogging behaviour.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 20:05:41 GMT"}], "update_date": "2009-05-18", "authors_parsed": [["Akritidis", "Leonidas", ""], ["Katsaros", "Dimitrios", ""], ["Bozanis", "Panayiotis", ""]]}, {"id": "0905.2501", "submitter": "Rom\\`an R. Zapatrin", "authors": "Daniel Sonntag, Rom\\`an R. Zapatrin", "title": "Macrodynamics of users' behavior in Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to geometrize massive data sets from search engines query\nlogs. For this purpose, a macrodynamic-like quantitative model of the\nInformation Retrieval (IR) process is developed, whose paradigm is inspired by\nbasic constructions of Einstein's general relativity theory in which all IR\nobjects are uniformly placed in a common Room. The Room has a structure similar\nto Einsteinian spacetime, namely that of a smooth manifold. Documents and\nqueries are treated as matter objects and sources of material fields.\nRelevance, the central notion of IR, becomes a dynamical issue controlled by\nboth gravitation (or, more precisely, as the motion in a curved spacetime) and\nforces originating from the interactions of matter fields. The spatio-temporal\ndescription ascribes dynamics to any document or query, thus providing a\nuniform description for documents of both initially static and dynamical\nnature. Within the IR context, the techniques presented are based on two ideas.\nThe first is the placement of all objects participating in IR into a common\ncontinuous space. The second idea is the `objectivization' of the IR process;\ninstead of expressing users' wishes, we consider the overall IR as an objective\nphysical process, representing the IR process in terms of motion in a given\nexternal-fields configuration. Various semantic environments are treated as\nvarious IR universes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2009 09:57:56 GMT"}], "update_date": "2009-05-18", "authors_parsed": [["Sonntag", "Daniel", ""], ["Zapatrin", "Rom\u00e0n R.", ""]]}, {"id": "0905.2990", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno and Pier-Luc St-Onge and Michel Gagnon and\n  Marc El-B\\`eze and Patrice Bellot", "title": "Automatic Summarization System coupled with a Question-Answering System\n  (QAAS)", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To select the most relevant sentences of a document, it uses an optimal\ndecision algorithm that combines several metrics. The metrics processes,\nweighting and extract pertinence sentences by statistical and informational\nalgorithms. This technique might improve a Question-Answering system, whose\nfunction is to provide an exact answer to a question in natural language. In\nthis paper, we present the results obtained by coupling the Cortex summarizer\nwith a Question-Answering system (QAAS). Two configurations have been\nevaluated. In the first one, a low compression level is selected and the\nsummarization system is only used as a noise filter. In the second\nconfiguration, the system actually functions as a summarizer, with a very high\nlevel of compression. Our results on French corpus demonstrate that the\ncoupling of Automatic Summarization system with a Question-Answering system is\npromising. Then the system has been adapted to generate a customized summary\ndepending on the specific question. Tests on a french multi-document corpus\nhave been realized, and the personalized QAAS system obtains the best\nperformances.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 21:35:52 GMT"}], "update_date": "2009-05-20", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["St-Onge", "Pier-Luc", ""], ["Gagnon", "Michel", ""], ["El-B\u00e8ze", "Marc", ""], ["Bellot", "Patrice", ""]]}, {"id": "0905.3318", "submitter": "Maarten Hijzelendoorn", "authors": "Maarten Hijzelendoorn and Crit Cremers", "title": "An Object-Oriented and Fast Lexicon for Semantic Generation", "comments": "Paper presented at the 18th Computational Linguistics In the\n  Netherlands Meeting (CLIN), Nijmegen, 10 December 2007, 15pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.DS cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about the technical design of a large computational lexicon,\nits storage, and its access from a Prolog environment. Traditionally, efficient\naccess and storage of data structures is implemented by a relational database\nmanagement system. In Delilah, a lexicon-based NLP system, efficient access to\nthe lexicon by the semantic generator is vital. We show that our highly\ndetailed HPSG-style lexical specifications do not fit well in the Relational\nModel, and that they cannot be efficiently retrieved. We argue that they fit\nmore naturally in the Object-Oriented Model. Although storage of objects is\nredundant, we claim that efficient access is still possible by applying\nindexing, and compression techniques from the Relational Model to the\nObject-Oriented Model. We demonstrate that it is possible to implement\nobject-oriented storage and fast access in ISO Prolog.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 14:12:42 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Hijzelendoorn", "Maarten", ""], ["Cremers", "Crit", ""]]}, {"id": "0905.3356", "submitter": "Rom\\`an R. Zapatrin", "authors": "George Parfionov, Rom\\`an Zapatrin", "title": "Memento Ludi: Information Retrieval from a Game-Theoretic Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a macro-model of information retrieval process using Game Theory\nas a mathematical theory of conflicts. We represent the participants of the\nInformation Retrieval process as a game of two abstract players. The first\nplayer is the `intellectual crowd' of users of search engines, the second is a\ncommunity of information retrieval systems. In order to apply Game Theory, we\ntreat search log data as Nash equilibrium strategies and solve the inverse\nproblem of finding appropriate payoff functions. For that, we suggest a\nparticular model, which we call Alpha model. Within this model, we suggest a\nmethod, called shifting, which makes it possible to partially control the\nbehavior of massive users.\n  This Note is addressed to researchers in both game theory (providing a new\nclass of real life problems) and information retrieval, for whom we present new\ntechniques to control the IR environment.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 17:04:07 GMT"}], "update_date": "2009-05-21", "authors_parsed": [["Parfionov", "George", ""], ["Zapatrin", "Rom\u00e0n", ""]]}, {"id": "0905.4039", "submitter": "Paul Vitanyi", "authors": "Rudi L. Cilibrasi (software consultant Oakland, CA) and Paul M.B.\n  Vitanyi (CWI, Amsterdam)", "title": "Normalized Web Distance and Word Similarity", "comments": "Latex, 20 pages, 7 figures, to appear in: Handbook of Natural\n  Language Processing, Second Edition, Nitin Indurkhya and Fred J. Damerau\n  Eds., CRC Press, Taylor and Francis Group, Boca Raton, FL, 2010, ISBN\n  978-1420085921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great deal of work in cognitive psychology, linguistics, and\ncomputer science, about using word (or phrase) frequencies in context in text\ncorpora to develop measures for word similarity or word association, going back\nto at least the 1960s. The goal of this chapter is to introduce the\nnormalizedis a general way to tap the amorphous low-grade knowledge available\nfor free on the Internet, typed in by local users aiming at personal\ngratification of diverse objectives, and yet globally achieving what is\neffectively the largest semantic electronic database in the world. Moreover,\nthis database is available for all by using any search engine that can return\naggregate page-count estimates for a large range of search-queries. In the\npaper introducing the NWD it was called `normalized Google distance (NGD),' but\nsince Google doesn't allow computer searches anymore, we opt for the more\nneutral and descriptive NWD. web distance (NWD) method to determine similarity\nbetween words and phrases. It\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2009 15:42:40 GMT"}], "update_date": "2009-05-26", "authors_parsed": [["Cilibrasi", "Rudi L.", "", "software consultant Oakland, CA"], ["Vitanyi", "Paul M. B.", "", "CWI, Amsterdam"]]}, {"id": "0905.4162", "submitter": "Dima Shepelyansky L", "authors": "D.L. Shepelyansky and O.V. Zhirov (CNRS, Toulouse & BINP, Novosibirsk)", "title": "Google matrix, dynamical attractors and Ulam networks", "comments": "9 pages, 11 figs; discussion, refs and fig added, data, title\n  modified, research at http://www.quantware.ups-tlse.fr", "journal-ref": "Phys. Rev. E v.81, p.036213 (2010)", "doi": "10.1103/PhysRevE.81.036213", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of the Google matrix generated by a coarse-grained\nPerron-Frobenius operator of the Chirikov typical map with dissipation. The\nfinite size matrix approximant of this operator is constructed by the Ulam\nmethod. This method applied to the simple dynamical model creates the directed\nUlam networks with approximate scale-free scaling and characteristics being\nrather similar to those of the World Wide Web. The simple dynamical attractors\nplay here the role of popular web sites with a strong concentration of\nPageRank. A variation of the Google parameter $\\alpha$ or other parameters of\nthe dynamical map can drive the PageRank of the Google matrix to a delocalized\nphase with a strange attractor where the Google search becomes inefficient.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2009 11:03:39 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2009 16:12:18 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Shepelyansky", "D. L.", "", "CNRS, Toulouse & BINP, Novosibirsk"], ["Zhirov", "O. V.", "", "CNRS, Toulouse & BINP, Novosibirsk"]]}, {"id": "0905.4627", "submitter": "Claudio Lucchese", "authors": "Paolo Bolettieri, Andrea Esuli, Fabrizio Falchi, Claudio Lucchese,\n  Raffaele Perego, Tommaso Piccioli and Fausto Rabitti", "title": "CoPhIR: a Test Collection for Content-Based Image Retrieval", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability, as well as the effectiveness, of the different Content-based\nImage Retrieval (CBIR) approaches proposed in literature, is today an important\nresearch issue. Given the wealth of images on the Web, CBIR systems must in\nfact leap towards Web-scale datasets. In this paper, we report on our\nexperience in building a test collection of 100 million images, with the\ncorresponding descriptive features, to be used in experimenting new scalable\ntechniques for similarity searching, and comparing their results. In the\ncontext of the SAPIR (Search on Audio-visual content using Peer-to-peer\nInformation Retrieval) European project, we had to experiment our distributed\nsimilarity searching technology on a realistic data set. Therefore, since no\nlarge-scale collection was available for research purposes, we had to tackle\nthe non-trivial process of image crawling and descriptive feature extraction\n(we used five MPEG-7 features) using the European EGEE computer GRID. The\nresult of this effort is CoPhIR, the first CBIR test collection of such scale.\nCoPhIR is now open to the research community for experiments and comparisons,\nand access to the collection was already granted to more than 50 research\ngroups worldwide.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 12:14:07 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2009 07:44:19 GMT"}], "update_date": "2009-06-01", "authors_parsed": [["Bolettieri", "Paolo", ""], ["Esuli", "Andrea", ""], ["Falchi", "Fabrizio", ""], ["Lucchese", "Claudio", ""], ["Perego", "Raffaele", ""], ["Piccioli", "Tommaso", ""], ["Rabitti", "Fausto", ""]]}]