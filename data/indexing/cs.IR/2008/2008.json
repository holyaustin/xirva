[{"id": "2008.00103", "submitter": "Peter Christen", "authors": "David J. Hand, Peter Christen, Nishadi Kirielle", "title": "F*: An Interpretable Transformation of the F-measure", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure, also known as the F1-score, is widely used to assess the\nperformance of classification algorithms. However, some researchers find it\nlacking in intuitive interpretation, questioning the appropriateness of\ncombining two aspects of performance as conceptually distinct as precision and\nrecall, and also questioning whether the harmonic mean is the best way to\ncombine them. To ease this concern, we describe a simple transformation of the\nF-measure, which we call F* (F-star), which has an immediate practical\ninterpretation.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 22:37:08 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 22:26:20 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 02:03:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hand", "David J.", ""], ["Christen", "Peter", ""], ["Kirielle", "Nishadi", ""]]}, {"id": "2008.00104", "submitter": "Elliot Creager", "authors": "Martin Mladenov, Elliot Creager, Omer Ben-Porat, Kevin Swersky,\n  Richard Zemel, Craig Boutilier", "title": "Optimizing Long-term Social Welfare in Recommender Systems: A\n  Constrained Matching Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recommender systems (RS) research assumes that a user's utility can be\nmaximized independently of the utility of the other agents (e.g., other users,\ncontent providers). In realistic settings, this is often not true---the\ndynamics of an RS ecosystem couple the long-term utility of all agents. In this\nwork, we explore settings in which content providers cannot remain viable\nunless they receive a certain level of user engagement. We formulate the\nrecommendation problem in this setting as one of equilibrium selection in the\ninduced dynamical system, and show that it can be solved as an optimal\nconstrained matching problem. Our model ensures the system reaches an\nequilibrium with maximal social welfare supported by a sufficiently diverse set\nof viable providers. We demonstrate that even in a simple, stylized dynamical\nRS model, the standard myopic approach to recommendation---always matching a\nuser to the best provider---performs poorly. We develop several scalable\ntechniques to solve the matching problem, and also draw connections to various\nnotions of user regret and fairness, arguing that these outcomes are fairer in\na utilitarian sense.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 22:40:47 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 20:57:28 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Mladenov", "Martin", ""], ["Creager", "Elliot", ""], ["Ben-Porat", "Omer", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""], ["Boutilier", "Craig", ""]]}, {"id": "2008.00137", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Martin Klein, Michele C. Weigle, Michael L. Nelson", "title": "MementoEmbed and Raintale for Web Archive Storytelling", "comments": "54 pages, 5 tables, 46 figures", "journal-ref": "Presented at the Web Archiving and Digital Libraries 2020 Workshop", "doi": null, "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For traditional library collections, archivists can select a representative\nsample from a collection and display it in a featured physical or digital\nlibrary space. Web archive collections may consist of thousands of archived\npages, or mementos. How should an archivist display this sample to drive\nvisitors to their collection? Search engines and social media platforms often\nrepresent web pages as cards consisting of text snippets, titles, and images.\nWeb storytelling is a popular method for grouping these cards in order to\nsummarize a topic. Unfortunately, social media platforms are not archive-aware\nand fail to consistently create a good experience for mementos. They also allow\nno UI alterations for their cards. Thus, we created MementoEmbed to generate\ncards for individual mementos and Raintale for creating entire stories that\narchivists can export to a variety of formats.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:52:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jones", "Shawn M.", ""], ["Klein", "Martin", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2008.00139", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Alexander C. Nwala, Martin Klein, Michele C. Weigle,\n  Michael L. Nelson", "title": "SHARI -- An Integration of Tools to Visualize the Story of the Day", "comments": "19 pages, 16 figures, 1 Table", "journal-ref": "Presented at the Web Archiving and Digital Libraries 2020 Workshop", "doi": null, "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tools such as Google News and Flipboard exist to convey daily news, but what\nabout the past? In this paper, we describe how to combine several existing\ntools with web archive holdings to perform news analysis and visualization of\nthe \"biggest story\" for a given date. StoryGraph clusters news articles\ntogether to identify a common news story. Hypercane leverages ArchiveNow to\nstore URLs produced by StoryGraph in web archives. Hypercane analyzes these\nURLs to identify the most common terms, entities, and highest quality images\nfor social media storytelling. Raintale then uses the output of these tools to\nproduce a visualization of the news story for a given day. We name this process\nSHARI (StoryGraph Hypercane ArchiveNow Raintale Integration).\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:02:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jones", "Shawn M.", ""], ["Nwala", "Alexander C.", ""], ["Klein", "Martin", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2008.00140", "submitter": "Daniel Lee", "authors": "Daniel Lee and Rakesh Verma and Avisha Das and Arjun Mukherjee", "title": "Experiments in Extractive Summarization: Integer Linear Programming,\n  Term/Sentence Scoring, and Title-driven Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the challenging problem of unsupervised\nsingle-document summarization and study the following aspects: Integer linear\nprogramming (ILP) based algorithms, Parameterized normalization of term and\nsentence scores, and Title-driven approaches for summarization. We describe a\nnew framework, NewsSumm, that includes many existing and new approaches for\nsummarization including ILP and title-driven approaches. NewsSumm's flexibility\nallows to combine different algorithms and sentence scoring schemes seamlessly.\nOur results combining sentence scoring with ILP and normalization are in\ncontrast to previous work on this topic, showing the importance of a broader\nsearch for optimal parameters. We also show that the new title-driven reduction\nidea leads to improvement in performance for both unsupervised and supervised\napproaches considered.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:05:55 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lee", "Daniel", ""], ["Verma", "Rakesh", ""], ["Das", "Avisha", ""], ["Mukherjee", "Arjun", ""]]}, {"id": "2008.00150", "submitter": "Mohammed Hamzah Abed", "authors": "Sarah Hussein Toman, Mohammed Hamzah Abed, Zinah Hussein Toman", "title": "Cluster-Based Information Retrieval by using (K-means)- Hierarchical\n  Parallel Genetic Algorithms Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-based information retrieval is one of the Information retrieval(IR)\ntools that organize, extract features and categorize the web documents\naccording to their similarity. Unlike traditional approaches, cluster-based IR\nis fast in processing large datasets of document. To improve the quality of\nretrieved documents, increase the efficiency of IR and reduce irrelevant\ndocuments from user search. in this paper, we proposed a (K-means) -\nHierarchical Parallel Genetic Algorithms Approach (HPGA) that combines the\nK-means clustering algorithm with hybrid PG of multi-deme and master/slave PG\nalgorithms. K-means uses to cluster the population to k subpopulations then\ntake most clusters relevant to the query to manipulate in a parallel way by the\ntwo levels of genetic parallelism, thus, irrelevant documents will not be\nincluded in subpopulations, as a way to improve the quality of results. Three\ncommon datasets (NLP, CISI, and CACM) are used to compute the recall,\nprecision, and F-measure averages. Finally, we compared the precision values of\nthree datasets with Genetic-IR and classic-IR. The proposed approach precision\nimprovements with IR-GA were 45% in the CACM, 27% in the CISI, and 25% in the\nNLP. While, by comparing with Classic-IR, (k-means)-HPGA got 47% in CACM, 28%\nin CISI, and 34% in NLP.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 02:05:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Toman", "Sarah Hussein", ""], ["Abed", "Mohammed Hamzah", ""], ["Toman", "Zinah Hussein", ""]]}, {"id": "2008.00181", "submitter": "Huaxiu Yao", "authors": "Jiatu Shi, Huaxiu Yao, Xian Wu, Tong Li, Zedong Lin, Tengfei Wang,\n  Binqiang Zhao", "title": "Relation-aware Meta-learning for Market Segment Demand Prediction with\n  Limited Records", "comments": "First two authors contributed equally; Accepted by WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce business is revolutionizing our shopping experiences by providing\nconvenient and straightforward services. One of the most fundamental problems\nis how to balance the demand and supply in market segments to build an\nefficient platform. While conventional machine learning models have achieved\ngreat success on data-sufficient segments, it may fail in a large-portion of\nsegments in E-commerce platforms, where there are not sufficient records to\nlearn well-trained models. In this paper, we tackle this problem in the context\nof market segment demand prediction. The goal is to facilitate the learning\nprocess in the target segments by leveraging the learned knowledge from\ndata-sufficient source segments. Specifically, we propose a novel algorithm,\nRMLDP, to incorporate a multi-pattern fusion network (MPFN) with a\nmeta-learning paradigm. The multi-pattern fusion network considers both local\nand seasonal temporal patterns for segment demand prediction. In the\nmeta-learning paradigm, transferable knowledge is regarded as the model\nparameter initialization of MPFN, which are learned from diverse source\nsegments. Furthermore, we capture the segment relations by combining\ndata-driven segment representation and segment knowledge graph representation\nand tailor the segment-specific relations to customize transferable model\nparameter initialization. Thus, even with limited data, the target segment can\nquickly find the most relevant transferred knowledge and adapt to the optimal\nparameters. We conduct extensive experiments on two large-scale industrial\ndatasets. The results justify that our RMLDP outperforms a set of\nstate-of-the-art baselines. Besides, RMLDP has been deployed in Taobao, a\nreal-world E-commerce platform. The online A/B testing results further\ndemonstrate the practicality of RMLDP.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 06:02:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 00:15:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Shi", "Jiatu", ""], ["Yao", "Huaxiu", ""], ["Wu", "Xian", ""], ["Li", "Tong", ""], ["Lin", "Zedong", ""], ["Wang", "Tengfei", ""], ["Zhao", "Binqiang", ""]]}, {"id": "2008.00202", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff", "title": "Contextual Document Similarity for Content-based Literature Recommender\n  Systems", "comments": "In Proceedings of the Doctoral Consortium at ACM/IEEE Joint\n  Conference on Digital Libraries (JCDL 2020)", "journal-ref": "Proceedings of the Doctoral Consortium at ACM/IEEE Joint\n  Conference on Digital Libraries (JCDL 2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the ever-growing information overload, an increasing number of\ndigital libraries employ content-based recommender systems. These systems\ntraditionally recommend related documents with the help of similarity measures.\nHowever, current document similarity measures simply distinguish between\nsimilar and dissimilar documents. This simplification is especially crucial for\nextensive documents, which cover various facets of a topic and are often found\nin digital libraries. Still, these similarity measures neglect to what facet\nthe similarity relates. Therefore, the context of the similarity remains\nill-defined. In this doctoral thesis, we explore contextual document similarity\nmeasures, i.e., methods that determine document similarity as a triple of two\ndocuments and the context of their similarity. The context is here a further\nspecification of the similarity. For example, in the scientific domain,\nresearch papers can be similar with respect to their background, methodology,\nor findings. The measurement of similarity in regards to one or more given\ncontexts will enhance recommender systems. Namely, users will be able to\nexplore document collections by formulating queries in terms of documents and\ntheir contextual similarities. Thus, our research objective is the development\nand evaluation of a recommender system based on contextual similarity. The\nunderlying techniques will apply established similarity measures and as well as\nneural approaches while utilizing semantic features obtained from links between\ndocuments and their text.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 07:42:40 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ostendorff", "Malte", ""]]}, {"id": "2008.00203", "submitter": "Jiawen Huang", "authors": "Jiawen Huang, Yun-Ning Hung, Ashis Pati, Siddharth Kumar Gururani,\n  Alexander Lerch", "title": "Score-informed Networks for Music Performance Assessment", "comments": "To appear at 21st International Society for Music Information\n  Retrieval Conference, Montr\\'eal, Canada, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The assessment of music performances in most cases takes into account the\nunderlying musical score being performed. While there have been several\nautomatic approaches for objective music performance assessment (MPA) based on\nextracted features from both the performance audio and the score, deep neural\nnetwork-based methods incorporating score information into MPA models have not\nyet been investigated. In this paper, we introduce three different models\ncapable of score-informed performance assessment. These are (i) a convolutional\nneural network that utilizes a simple time-series input comprising of aligned\npitch contours and score, (ii) a joint embedding model which learns a joint\nlatent space for pitch contours and scores, and (iii) a distance matrix-based\nconvolutional neural network which utilizes patterns in the distance matrix\nbetween pitch contours and musical score to predict assessment ratings. Our\nresults provide insights into the suitability of different architectures and\ninput representations and demonstrate the benefits of score-informed models as\ncompared to score-independent models.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 07:46:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Huang", "Jiawen", ""], ["Hung", "Yun-Ning", ""], ["Pati", "Ashis", ""], ["Gururani", "Siddharth Kumar", ""], ["Lerch", "Alexander", ""]]}, {"id": "2008.00279", "submitter": "Jie Zou", "authors": "Jie Zou, Evangelos Kanoulas, and Yiqun Liu", "title": "An Empirical Study of Clarifying Question-Based Systems", "comments": "Parts of content are published on CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search and recommender systems that take the initiative to ask clarifying\nquestions to better understand users' information needs are receiving\nincreasing attention from the research community. However, to the best of our\nknowledge, there is no empirical study to quantify whether and to what extent\nusers are willing or able to answer these questions. In this work, we conduct\nan online experiment by deploying an experimental system, which interacts with\nusers by asking clarifying questions against a product repository. We collect\nboth implicit interaction behavior data and explicit feedback from users\nshowing that: (a) users are willing to answer a good number of clarifying\nquestions (11-21 on average), but not many more than that; (b) most users\nanswer questions until they reach the target product, but also a fraction of\nthem stops due to fatigue or due to receiving irrelevant questions; (c) part of\nthe users' answers (12-17%) are actually opposite to the description of the\ntarget product; while (d) most of the users (66-84%) find the question-based\nsystem helpful towards completing their tasks. Some of the findings of the\nstudy contradict current assumptions on simulated evaluations in the field,\nwhile they point towards improvements in the evaluation framework and can\ninspire future interactive search/recommender system designs.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 15:10:11 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zou", "Jie", ""], ["Kanoulas", "Evangelos", ""], ["Liu", "Yiqun", ""]]}, {"id": "2008.00404", "submitter": "Yixin Su", "authors": "Yixin Su, Rui Zhang, Sarah Erfani, Zhenghua Xu", "title": "Detecting Beneficial Feature Interactions for Recommender Systems", "comments": "14 pages, 7 figures, 5 tables, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature interactions are essential for achieving high accuracy in recommender\nsystems. Many studies take into account the interaction between every pair of\nfeatures. However, this is suboptimal because some feature interactions may not\nbe that relevant to the recommendation result, and taking them into account may\nintroduce noise and decrease recommendation accuracy. To make the best out of\nfeature interactions, we propose a graph neural network approach to effectively\nmodel them, together with a novel technique to automatically detect those\nfeature interactions that are beneficial in terms of recommendation accuracy.\nThe automatic feature interaction detection is achieved via edge prediction\nwith an L0 activation regularization. Our proposed model is proved to be\neffective through the information bottleneck principle and statistical\ninteraction theory. Experimental results show that our model (i) outperforms\nexisting baselines in terms of accuracy, and (ii) automatically identifies\nbeneficial feature interactions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 06:08:23 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 04:51:57 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 02:23:31 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2021 06:37:45 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 00:47:40 GMT"}, {"version": "v6", "created": "Tue, 18 May 2021 11:57:21 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Su", "Yixin", ""], ["Zhang", "Rui", ""], ["Erfani", "Sarah", ""], ["Xu", "Zhenghua", ""]]}, {"id": "2008.00582", "submitter": "Verena Haunschmid", "authors": "Verena Haunschmid, Ethan Manilow, Gerhard Widmer", "title": "audioLIME: Listenable Explanations Using Source Separation", "comments": "In The 13th International Workshop on Machine Learning and Music,\n  ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are successfully applied in a wide variety of\nmusic information retrieval (MIR) tasks but their predictions are usually not\ninterpretable. We propose audioLIME, a method based on Local Interpretable\nModel-agnostic Explanations (LIME) extended by a musical definition of\nlocality. The perturbations used in LIME are created by switching on/off\ncomponents extracted by source separation which makes our explanations\nlistenable. We validate audioLIME on two different music tagging systems and\nshow that it produces sensible explanations in situations where a competing\nmethod cannot.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 23:05:02 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 05:37:39 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 08:55:19 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Haunschmid", "Verena", ""], ["Manilow", "Ethan", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2008.00756", "submitter": "Rohit M A", "authors": "Rohit M. A., Preeti Rao", "title": "Structure and Automatic Segmentation of Dhrupad Vocal Bandish Audio", "comments": "Part of this work published in ISMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Dhrupad vocal concert comprises a composition section that is interspersed\nwith improvised episodes of increased rhythmic activity involving the\ninteraction between the vocals and the percussion. Tracking the changing\nrhythmic density, in relation to the underlying metric tempo of the piece, thus\nfacilitates the detection and labeling of the improvised sections in the\nconcert structure. This work concerns the automatic detection of the musically\nrelevant rhythmic densities as they change in time across the bandish\n(composition) performance. An annotated dataset of Dhrupad bandish concert\nsections is presented. We investigate a CNN-based system, trained to detect\nlocal tempo relationships, and follow it with temporal smoothing. We also\nemploy audio source separation as a pre-processing step to the detection of the\nindividual surface densities of the vocals and the percussion. This helps us\nobtain the complete musical description of the concert sections in terms of\ncapturing the changing rhythmic interaction of the two performers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 10:16:42 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["A.", "Rohit M.", ""], ["Rao", "Preeti", ""]]}, {"id": "2008.00783", "submitter": "Anton Steenvoorden", "authors": "Anton Steenvoorden, Emanuele Di Gloria, Wanyu Chen, Pengjie Ren,\n  Maarten de Rijke", "title": "Attribute-aware Diversification for Sequential Recommendations", "comments": "AIIS 2020, as part of SIGIR 2020 https://aiis.newidea.fun/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users prefer diverse recommendations over homogeneous ones. However, most\nprevious work on Sequential Recommenders does not consider diversity, and\nstrives for maximum accuracy, resulting in homogeneous recommendations. In this\npaper, we consider both accuracy and diversity by presenting an Attribute-aware\nDiversifying Sequential Recommender (ADSR). Specifically, ADSR utilizes\navailable attribute information when modeling a user's sequential behavior to\nsimultaneously learn the user's most likely item to interact with, and their\npreference of attributes. Then, ADSR diversifies the recommended items based on\nthe predicted preference for certain attributes. Experiments on two benchmark\ndatasets demonstrate that ADSR can effectively provide diverse recommendations\nwhile maintaining accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 11:20:47 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Steenvoorden", "Anton", ""], ["Di Gloria", "Emanuele", ""], ["Chen", "Wanyu", ""], ["Ren", "Pengjie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2008.01189", "submitter": "Ananth Goyal", "authors": "Ananth Goyal", "title": "A Self-Assessing Compilation Based Search Approach for Analytical\n  Research and Data Retrieval", "comments": "6 pages, 3 Figures, 3 Tables, IYRC Research Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While meta-analytic research is performed, it becomes time-consuming to\nfilter through the sheer amount of sources made available by individual\ndatabases and search engines and therefore degrades the specificity of source\nanalysis. This study sought to predict the feasibility of a research-oriented\nsearching algorithm across all topics and a search technique to combat flaws in\ndealing with large datasets by automating three key components of\nmeta-analysis: a query-based search associated with the intended research\ntopic, selecting given sources and determining their relevance to the original\nquery, and extracting applicable information including excerpts and citations.\nThe algorithm was evaluated using 5 key historical topics, and results were\nbroken down into 4 categories: the total number of relevant sources retrieved,\nthe efficiency given a particular search, the total time it takes to finish a\ncomplete cycle, and the quality of the extracted sources when compared to\nresults from current searching methods. Although results differed through\nseveral searches, on average, the program collected a total of 126 sources per\nsearch with an average efficiency of 19.55 sources per second which, when\ncompared and qualitatively evaluated for definitive results, indicates that an\nalgorithm developed across all subject areas will make progress in future\nresearch methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 23:51:02 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Goyal", "Ananth", ""]]}, {"id": "2008.01190", "submitter": "SeungHeon Doh", "authors": "Seungheon Doh, Jongpil Lee, Tae Hong Park, Juhan Nam", "title": "Musical Word Embedding: Bridging the Gap between Listening Contexts and\n  Music", "comments": "Machine Learning for Media Discovery Workshop, International\n  Conference on Machine Learning (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding pioneered by Mikolov et al. is a staple technique for word\nrepresentations in natural language processing (NLP) research which has also\nfound popularity in music information retrieval tasks. Depending on the type of\ntext data for word embedding, however, vocabulary size and the degree of\nmusical pertinence can significantly vary. In this work, we (1) train the\ndistributed representation of words using combinations of both general text\ndata and music-specific data and (2) evaluate the system in terms of how they\nassociate listening contexts with musical compositions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:42:45 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Doh", "Seungheon", ""], ["Lee", "Jongpil", ""], ["Park", "Tae Hong", ""], ["Nam", "Juhan", ""]]}, {"id": "2008.01191", "submitter": "Sadaqat Ur Rehman", "authors": "Sadaqat ur Rehman, Muhammad Waqas, Shanshan Tu, Anis Koubaa, Obaid ur\n  Rehman, Jawad Ahmad, Muhammad Hanif, Zhu Han", "title": "Deep Learning Techniques for Future Intelligent Cross-Media Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1804.09539 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advancement in technology and the expansion of broadcasting,\ncross-media retrieval has gained much attention. It plays a significant role in\nbig data applications and consists in searching and finding data from different\ntypes of media. In this paper, we provide a novel taxonomy according to the\nchallenges faced by multi-modal deep learning approaches in solving cross-media\nretrieval, namely: representation, alignment, and translation. These challenges\nare evaluated on deep learning (DL) based methods, which are categorized into\nfour main groups: 1) unsupervised methods, 2) supervised methods, 3) pairwise\nbased methods, and 4) rank based methods. Then, we present some well-known\ncross-media datasets used for retrieval, considering the importance of these\ndatasets in the context in of deep learning based cross-media retrieval\napproaches. Moreover, we also present an extensive review of the\nstate-of-the-art problems and its corresponding solutions for encouraging deep\nlearning in cross-media retrieval. The fundamental objective of this work is to\nexploit Deep Neural Networks (DNNs) for bridging the \"media gap\", and provide\nresearchers and developers with a better understanding of the underlying\nproblems and the potential solutions of deep learning assisted cross-media\nretrieval. To the best of our knowledge, this is the first comprehensive survey\nto address cross-media retrieval under deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:49:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rehman", "Sadaqat ur", ""], ["Waqas", "Muhammad", ""], ["Tu", "Shanshan", ""], ["Koubaa", "Anis", ""], ["Rehman", "Obaid ur", ""], ["Ahmad", "Jawad", ""], ["Hanif", "Muhammad", ""], ["Han", "Zhu", ""]]}, {"id": "2008.01192", "submitter": "Kamal Berahmand", "authors": "Saman Forouzandeh, Mehrdad Rostami, Kamal Berahmand", "title": "Presentation of a Recommender System with Ensemble Learning and Graph\n  Embedding: A Case on MovieLens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information technology has spread widely, and extraordinarily large amounts\nof data have been made accessible to users, which has made it challenging to\nselect data that are in accordance with user needs. For the resolution of the\nabove issue, recommender systems have emerged, which much help users go through\nthe process of decision-making and selecting relevant data. A recommender\nsystem predicts users behavior to be capable of detecting their interests and\nneeds, and it often uses the classification technique for this purpose. It may\nnot be sufficiently accurate to employ individual classification, where not all\ncases can be examined, which makes the method inappropriate to specific\nproblems. In this research, group classification and the ensemble learning\ntechnique were used for increasing prediction accuracy in recommender systems.\nAnother issue that is raised here concerns user analysis. Given the large size\nof the data and a large number of users, the process of user needs analysis and\nprediction (using a graph in most cases, representing the relations between\nusers and their selected items) is complicated and cumbersome in recommender\nsystems. Graph embedding was also proposed for resolution of this issue, where\nall or part of user behavior can be simulated through the generation of several\nvectors, resolving the problem of user behavior analysis to a large extent\nwhile maintaining high efficiency. In this research, individuals most similar\nto the target user were classified using ensemble learning, fuzzy rules, and\nthe decision tree, and relevant recommendations were then made to each user\nwith a heterogeneous knowledge graph and embedding vectors. This study was\nperformed on the MovieLens datasets, and the obtained results indicated the\nhigh efficiency of the presented method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 12:52:15 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Forouzandeh", "Saman", ""], ["Rostami", "Mehrdad", ""], ["Berahmand", "Kamal", ""]]}, {"id": "2008.01193", "submitter": "Bo Peng", "authors": "Zhiyun Ren, Bo Peng, Titus K. Schleyer and Xia Ning", "title": "Hybrid Collaborative Filtering Models for Clinical Search Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing and extensive use of electronic health records, clinicians\nare often under time pressure when they need to retrieve important information\nefficiently among large amounts of patients' health records in clinics. While a\nsearch function can be a useful alternative to browsing through a patient's\nrecord, it is cumbersome for clinicians to search repeatedly for the same or\nsimilar information on similar patients. Under such circumstances, there is a\ncritical need to build effective recommender systems that can generate accurate\nsearch term recommendations for clinicians. In this manuscript, we developed a\nhybrid collaborative filtering model using patients' encounter and search term\ninformation to recommend the next search terms for clinicians to retrieve\nimportant information fast in clinics. For each patient, the model will\nrecommend terms that either have high co-occurrence frequencies with his/her\nmost recent ICD codes or are highly relevant to the most recent search terms on\nthis patient. We have conducted comprehensive experiments to evaluate the\nproposed model, and the experimental results demonstrate that our model can\noutperform all the state-of-the-art baseline methods for top-N search term\nrecommendation on different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 19:25:00 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ren", "Zhiyun", ""], ["Peng", "Bo", ""], ["Schleyer", "Titus K.", ""], ["Ning", "Xia", ""]]}, {"id": "2008.01194", "submitter": "Sahil Verma", "authors": "Sahil Verma, Ruoyuan Gao, Chirag Shah", "title": "Facets of Fairness in Search and Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have highlighted how search and recommender systems\nexhibit bias along different dimensions. Counteracting this bias and bringing a\ncertain amount of fairness in search is crucial to not only creating a more\nbalanced environment that considers relevance and diversity but also providing\na more sustainable way forward for both content consumers and content\nproducers. This short paper examines some of the recent works to define\nrelevance, diversity, and related concepts. Then, it focuses on explaining the\nemerging concept of fairness in various recommendation settings. In doing so,\nthis paper presents comparisons and highlights contracts among various\nmeasures, and gaps in our conceptual and evaluative frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:14:08 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Verma", "Sahil", ""], ["Gao", "Ruoyuan", ""], ["Shah", "Chirag", ""]]}, {"id": "2008.01196", "submitter": "Jerome Darmont", "authors": "Abderrazek Azri (ERIC), C\\'ecile Favre (ERIC), Nouria Harbi (ERIC),\n  J\\'er\\^ome Darmont (ERIC)", "title": "Including Images into Message Veracity Assessment in Social Media", "comments": null, "journal-ref": "8th International Conference on Innovation and New Trends in\n  Information Technology (INTIS 2019), Dec 2019, Tangier, Morocco", "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media in the diffusion of information has also\nlaid a fertile ground for the spread of rumors, which could significantly\naffect the credibility of social media. An ever-increasing number of users post\nnews including, in addition to text, multimedia data such as images and videos.\nYet, such multimedia content is easily editable due to the broad availability\nof simple and effective image and video processing tools. The problem of\nassessing the veracity of social network posts has attracted a lot of attention\nfrom researchers in recent years. However, almost all previous works have\nfocused on analyzing textual contents to determine veracity, while visual\ncontents, and more particularly images, remains ignored or little exploited in\nthe literature. In this position paper, we propose a framework that explores\ntwo novel ways to assess the veracity of messages published on social networks\nby analyzing the credibility of both their textual and visual contents.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:42:17 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Azri", "Abderrazek", "", "ERIC"], ["Favre", "C\u00e9cile", "", "ERIC"], ["Harbi", "Nouria", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "2008.01197", "submitter": "Justin Lovelace", "authors": "Justin Lovelace, Nathan C. Hurley, Adrian D. Haimovich, Bobak J.\n  Mortazavi", "title": "Dynamically Extracting Outcome-Specific Problem Lists from Clinical\n  Notes with Guided Multi-Headed Attention", "comments": "To appear in the proceedings of the Machine Learning for Healthcare\n  Conference (MLHC) 2020. Accepted papers can be viewed at\n  https://www.mlforhc.org/accepted-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Problem lists are intended to provide clinicians with a relevant summary of\npatient medical issues and are embedded in many electronic health record\nsystems. Despite their importance, problem lists are often cluttered with\nresolved or currently irrelevant conditions. In this work, we develop a novel\nend-to-end framework that first extracts diagnosis and procedure information\nfrom clinical notes and subsequently uses the extracted medical problems to\npredict patient outcomes. This framework is both more performant and more\ninterpretable than existing models used within the domain, achieving an AU-ROC\nof 0.710 for bounceback readmission and 0.869 for in-hospital mortality\noccurring after ICU discharge. We identify risk factors for both readmission\nand mortality outcomes and demonstrate that our framework can be used to\ndevelop dynamic problem lists that present clinical problems along with their\nquantitative importance. We conduct a qualitative user study with medical\nexperts and demonstrate that they view the lists produced by our framework\nfavorably and find them to be a more effective clinical decision support tool\nthan a strong baseline.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 21:03:50 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lovelace", "Justin", ""], ["Hurley", "Nathan C.", ""], ["Haimovich", "Adrian D.", ""], ["Mortazavi", "Bobak J.", ""]]}, {"id": "2008.01212", "submitter": "Fernando Benjamin Perez Maurera", "authors": "Fernando Benjam\\'in P\\'erez Maurera, Maurizio Ferrari Dacrema, Lorenzo\n  Saule, Mario Scriminaci, Paolo Cremonesi", "title": "ContentWise Impressions: An Industrial Dataset with Impressions Included", "comments": "8 pages, 2 figures", "journal-ref": "Proceedings of the 29th ACM International Conference on\n  Information & Knowledge Management (CIKM 2020)", "doi": "10.1145/3340531.3412774", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the ContentWise Impressions dataset, a\ncollection of implicit interactions and impressions of movies and TV series\nfrom an Over-The-Top media service, which delivers its media contents over the\nInternet. The dataset is distinguished from other already available multimedia\nrecommendation datasets by the availability of impressions, i.e., the\nrecommendations shown to the user, its size, and by being open-source. We\ndescribe the data collection process, the preprocessing applied, its\ncharacteristics, and statistics when compared to other commonly used datasets.\nWe also highlight several possible use cases and research questions that can\nbenefit from the availability of user impressions in an open-source dataset.\nFurthermore, we release software tools to load and split the data, as well as\nexamples of how to use both user interactions and impressions in several common\nrecommendation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:46:38 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 12:51:09 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Maurera", "Fernando Benjam\u00edn P\u00e9rez", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Saule", "Lorenzo", ""], ["Scriminaci", "Mario", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2008.01246", "submitter": "Jin Peng Zhou", "authors": "Jin Peng Zhou, Ga Wu, Zheda Mai, Scott Sanner", "title": "Noise Contrastive Estimation for Autoencoding-based One-Class\n  Collaborative Filtering", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class collaborative filtering (OC-CF) is a common class of recommendation\nproblem where only the positive class is explicitly observed (e.g., purchases,\nclicks). Autoencoder based recommenders such as AutoRec and variants\ndemonstrate strong performance on many OC-CF benchmarks, but also empirically\nsuffer from a strong popularity bias. While a careful choice of negative\nsamples in the OC-CF setting can mitigate popularity bias, Negative Sampling\n(NS) is often better for training embeddings than for the end task itself. To\naddress this, we propose a two-headed AutoRec to first train an embedding layer\nvia one head using Negative Sampling then to train for the final task via the\nsecond head. While this NS-AutoRec improves results for AutoRec and outperforms\nmany state-of-the-art baselines on OC-CF problems, we notice that Negative\nSampling can still take a large amount of time to train. Since Negative\nSampling is known to be a special case of Noise Contrastive Estimation (NCE),\nwe adapt a recently proposed closed-form NCE solution for collaborative\nfiltering to AutoRec yielding NCE-AutoRec. Overall, we show that our novel\ntwo-headed AutoRec models (NCE-AutoRec and NS-AutoRec) successfully mitigate\nthe popularity bias issue and maintain competitive performance in comparison to\nstate-of-the-art recommenders on multiple real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 23:49:45 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 23:20:31 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Zhou", "Jin Peng", ""], ["Wu", "Ga", ""], ["Mai", "Zheda", ""], ["Sanner", "Scott", ""]]}, {"id": "2008.01377", "submitter": "Stefan Heid", "authors": "Stefan Heid, Marcel Wever, Eyke H\\\"ullermeier", "title": "Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued\n  Prediction", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a\nkey requirement for both linguistic research and subsequent automated natural\nlanguage processing (NLP) tasks. This problem is commonly tackled using machine\nlearning methods, i.e., by training a POS tagger on a sufficiently large corpus\nof labeled data. While the problem of POS tagging can essentially be considered\nas solved for modern languages, historical corpora turn out to be much more\ndifficult, especially due to the lack of native speakers and sparsity of\ntraining data. Moreover, most texts have no sentences as we know them today,\nnor a common orthography. These irregularities render the task of automated POS\ntagging more difficult and error-prone. Under these circumstances, instead of\nforcing the POS tagger to predict and commit to a single tag, it should be\nenabled to express its uncertainty. In this paper, we consider POS tagging\nwithin the framework of set-valued prediction, which allows the POS tagger to\nexpress its uncertainty via predicting a set of candidate POS tags instead of\nguessing a single one. The goal is to guarantee a high confidence that the\ncorrect POS tag is included while keeping the number of candidates small. In\nour experimental study, we find that extending state-of-the-art POS taggers to\nset-valued prediction yields more precise and robust taggings, especially for\nunknown words, i.e., words not occurring in the training data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:21:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:59:57 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Heid", "Stefan", ""], ["Wever", "Marcel", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2008.01403", "submitter": "Daizong Liu", "authors": "Daizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng Dong, Pan Zhou,\n  Zichuan Xu", "title": "Jointly Cross- and Self-Modal Graph Attention Network for Query-Based\n  Moment Localization", "comments": "Accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based moment localization is a new task that localizes the best matched\nsegment in an untrimmed video according to a given sentence query. In this\nlocalization task, one should pay more attention to thoroughly mine visual and\nlinguistic information. To this end, we propose a novel Cross- and Self-Modal\nGraph Attention Network (CSMGAN) that recasts this task as a process of\niterative messages passing over a joint graph. Specifically, the joint graph\nconsists of Cross-Modal interaction Graph (CMG) and Self-Modal relation Graph\n(SMG), where frames and words are represented as nodes, and the relations\nbetween cross- and self-modal node pairs are described by an attention\nmechanism. Through parametric message passing, CMG highlights relevant\ninstances across video and sentence, and then SMG models the pairwise relation\ninside each modality for frame (word) correlating. With multiple layers of such\na joint graph, our CSMGAN is able to effectively capture high-order\ninteractions between two modalities, thus enabling a further precise\nlocalization. Besides, to better comprehend the contextual details in the\nquery, we develop a hierarchical sentence encoder to enhance the query\nunderstanding. Extensive experiments on four public datasets demonstrate the\neffectiveness of our proposed model, and GCSMAN significantly outperforms the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 08:25:24 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 01:56:06 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Liu", "Daizong", ""], ["Qu", "Xiaoye", ""], ["Liu", "Xiao-Yang", ""], ["Dong", "Jianfeng", ""], ["Zhou", "Pan", ""], ["Xu", "Zichuan", ""]]}, {"id": "2008.01526", "submitter": "Samarth Rawal", "authors": "Samarth Rawal", "title": "Multi-Perspective Semantic Information Retrieval in the Biomedical\n  Domain", "comments": "Masters thesis, Arizona State Univ (May, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) is the task of obtaining pieces of data (such as\ndocuments) that are relevant to a particular query or need from a large\nrepository of information. IR is a valuable component of several downstream\nNatural Language Processing (NLP) tasks. Practically, IR is at the heart of\nmany widely-used technologies like search engines. While probabilistic ranking\nfunctions like the Okapi BM25 function have been utilized in IR systems since\nthe 1970's, modern neural approaches pose certain advantages compared to their\nclassical counterparts. In particular, the release of BERT (Bidirectional\nEncoder Representations from Transformers) has had a significant impact in the\nNLP community by demonstrating how the use of a Masked Language Model trained\non a large corpus of data can improve a variety of downstream NLP tasks,\nincluding sentence classification and passage re-ranking. IR Systems are also\nimportant in the biomedical and clinical domains. Given the increasing amount\nof scientific literature across biomedical domain, the ability find answers to\nspecific clinical queries from a repository of millions of articles is a matter\nof practical value to medical professionals. Moreover, there are\ndomain-specific challenges present, including handling clinical jargon and\nevaluating the similarity or relatedness of various medical symptoms when\ndetermining the relevance between a query and a sentence. This work presents\ncontributions to several aspects of the Biomedical Semantic Information\nRetrieval domain. First, it introduces Multi-Perspective Sentence Relevance, a\nnovel methodology of utilizing BERT-based models for contextual IR. The system\nis evaluated using the BioASQ Biomedical IR Challenge. Finally, practical\ncontributions in the form of a live IR system for medics and a proposed\nchallenge on the Living Systematic Review clinical task are provided.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 21:05:44 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rawal", "Samarth", ""]]}, {"id": "2008.01533", "submitter": "Fernando Alonso-Fernandez", "authors": "Fernando Alonso-Fernandez, Nicole Mariah Sharon Belvisi, Kevin\n  Hernandez-Diaz, Naveed Muhammad, Josef Bigun", "title": "Writer Identification Using Microblogging Texts for Social Media\n  Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing authorship of online texts is fundamental to combat cybercrimes.\nUnfortunately, text length is limited on some platforms, making the challenge\nharder. We aim at identifying the authorship of Twitter messages limited to 140\ncharacters. We evaluate popular stylometric features, widely used in literary\nanalysis, and specific Twitter features like URLs, hashtags, replies or quotes.\nWe use two databases with 93 and 3957 authors, respectively. We test varying\nsized author sets and varying amounts of training/test texts per author.\nPerformance is further improved by feature combination via automatic selection.\nWith a large number of training Tweets (>500), a good accuracy (Rank-5>80%) is\nachievable with only a few dozens of test Tweets, even with several thousands\nof authors. With smaller sample sizes (10-20 training Tweets), the search space\ncan be diminished by 9-15% while keeping a high chance that the correct author\nis retrieved among the candidates. In such cases, automatic attribution can\nprovide significant time savings to experts in suspect search. For\ncompleteness, we report verification results. With few training/test Tweets,\nthe EER is above 20-25%, which is reduced to < 15% if hundreds of training\nTweets are available. We also quantify the computational complexity and time\npermanence of the employed features.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 00:23:18 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 02:42:18 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Alonso-Fernandez", "Fernando", ""], ["Belvisi", "Nicole Mariah Sharon", ""], ["Hernandez-Diaz", "Kevin", ""], ["Muhammad", "Naveed", ""], ["Bigun", "Josef", ""]]}, {"id": "2008.01544", "submitter": "Manoel Verissimo Dos Santos Neto Mvst", "authors": "Manoel Ver\\'issimo dos Santos Neto, Ayrton Denner da Silva Amaral,\n  N\\'adia F\\'elix Felipe da Silva, Anderson da Silva Soares", "title": "Deep Learning Brasil -- NLP at SemEval-2020 Task 9: Overview of\n  Sentiment Analysis of Code-Mixed Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a methodology to predict sentiment in code-mixed\ntweets (hindi-english). Our team called verissimo.manoel in CodaLab developed\nan approach based on an ensemble of four models (MultiFiT, BERT, ALBERT, and\nXLNET). The final classification algorithm was an ensemble of some predictions\nof all softmax values from these four models. This architecture was used and\nevaluated in the context of the SemEval 2020 challenge (task 9), and our system\ngot 72.7% on the F1 score.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 16:42:41 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Neto", "Manoel Ver\u00edssimo dos Santos", ""], ["Amaral", "Ayrton Denner da Silva", ""], ["da Silva", "N\u00e1dia F\u00e9lix Felipe", ""], ["Soares", "Anderson da Silva", ""]]}, {"id": "2008.01780", "submitter": "Sejal Bhalla", "authors": "Dikshant Sagar, Jatin Garg, Prarthana Kansal, Sejal Bhalla, Rajiv Ratn\n  Shah and Yi Yu", "title": "PAI-BPR: Personalized Outfit Recommendation Scheme with Attribute-wise\n  Interpretability", "comments": "10 pages, 5 figures, to be published in IEEE BigMM, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is an important part of human experience. Events such as interviews,\nmeetings, marriages, etc. are often based on clothing styles. The rise in the\nfashion industry and its effect on social influencing have made outfit\ncompatibility a need. Thus, it necessitates an outfit compatibility model to\naid people in clothing recommendation. However, due to the highly subjective\nnature of compatibility, it is necessary to account for personalization. Our\npaper devises an attribute-wise interpretable compatibility scheme with\npersonal preference modelling which captures user-item interaction along with\ngeneral item-item interaction. Our work solves the problem of interpretability\nin clothing matching by locating the discordant and harmonious attributes\nbetween fashion items. Extensive experiment results on IQON3000, a publicly\navailable real-world dataset, verify the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:30:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sagar", "Dikshant", ""], ["Garg", "Jatin", ""], ["Kansal", "Prarthana", ""], ["Bhalla", "Sejal", ""], ["Shah", "Rajiv Ratn", ""], ["Yu", "Yi", ""]]}, {"id": "2008.01969", "submitter": "Yijiang Lian", "authors": "Yijiang Lian, Zhenjun You, Fan Wu, Wenqiang Liu, Jing Jia", "title": "Retrieve Synonymous keywords for Frequent Queries in Sponsored Search in\n  a Data Augmentation Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sponsored search, retrieving synonymous keywords is of great importance\nfor accurately targeted advertising. The semantic gap between queries and\nkeywords and the extremely high precision requirements (>= 95\\%) are two major\nchallenges to this task. To the best of our knowledge, the problem has not been\nopenly discussed. In an industrial sponsored search system, the retrieved\nkeywords for frequent queries are usually done ahead of time and stored in a\nlookup table. Considering these results as a seed dataset, we propose a\ndata-augmentation-like framework to improve the synonymous retrieval\nperformance for these frequent queries. This framework comprises two steps:\ntranslation-based retrieval and discriminant-based filtering. Firstly, we\ndevise a Trie-based translation model to make a data increment. In this phase,\na Bag-of-Core-Words trick is conducted, which increased the data increment's\nvolume 4.2 times while keeping the original precision. Then we use a BERT-based\ndiscriminant model to filter out nonsynonymous pairs, which exceeds the\ntraditional feature-driven GBDT model with 11\\% absolute AUC improvement. This\nmethod has been successfully applied to Baidu's sponsored search system, which\nhas yielded a significant improvement in revenue. In addition, a commercial\nChinese dataset containing 500K synonymous pairs with a precision of 95\\% is\nreleased to the public for paraphrase study\n(http://ai.baidu.com/broad/subordinate?dataset=paraphrasing).\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:34:23 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lian", "Yijiang", ""], ["You", "Zhenjun", ""], ["Wu", "Fan", ""], ["Liu", "Wenqiang", ""], ["Jia", "Jing", ""]]}, {"id": "2008.01988", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Andreas Breiter", "title": "How Fake News Affect Trust in the Output of a Machine Learning System\n  for News Curation", "comments": "This is a pre-print of an article published in MISDOOM 2020 - 2nd\n  Multidisciplinary International Symposium on Disinformation in Open Online\n  Media", "journal-ref": "MISDOOM 2020 - 2nd Multidisciplinary International Symposium on\n  Disinformation in Open Online Media", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are increasingly consuming news curated by machine learning (ML)\nsystems. Motivated by studies on algorithmic bias, this paper explores which\nrecommendations of an algorithmic news curation system users trust and how this\ntrust is affected by untrustworthy news stories like fake news. In a study with\n82 vocational school students with a background in IT, we found that users are\nable to provide trust ratings that distinguish trustworthy recommendations of\nquality news stories from untrustworthy recommendations. However, a single\nuntrustworthy news story combined with four trustworthy news stories is rated\nsimilarly as five trustworthy news stories. The results could be a first\nindication that untrustworthy news stories benefit from appearing in a\ntrustworthy context. The results also show the limitations of users' abilities\nto rate the recommendations of a news curation system. We discuss the\nimplications of this for the user experience of interactive machine learning\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:17:20 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Heuer", "Hendrik", ""], ["Breiter", "Andreas", ""]]}, {"id": "2008.02002", "submitter": "Jianqiu Lu", "authors": "Xiaozheng Jian, Jianqiu Lu, Zexi Yuan, Ao Li", "title": "Fast top-K Cosine Similarity Search through XOR-Friendly Binary\n  Quantization on GPUs", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of GPU for accelerating large scale nearest neighbor\nsearch and we propose a fast vector-quantization-based exhaustive nearest\nneighbor search algorithm that can achieve high accuracy without any indexing\nconstruction specifically designed for cosine similarity. This algorithm uses a\nnovel XOR-friendly binary quantization method to encode floating-point numbers\nsuch that high-complexity multiplications can be optimized as low-complexity\nbitwise operations. Experiments show that, our quantization method takes short\npreprocessing time, and helps make the search speed of our exhaustive search\nmethod much more faster than that of popular approximate nearest neighbor\nalgorithms when high accuracy is needed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:50:21 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Jian", "Xiaozheng", ""], ["Lu", "Jianqiu", ""], ["Yuan", "Zexi", ""], ["Li", "Ao", ""]]}, {"id": "2008.02011", "submitter": "Bo-Yu Chen", "authors": "Bo-Yu Chen, Jordan B. L. Smith, Yi-Hsuan Yang", "title": "Neural Loop Combiner: Neural Network Models for Assessing the\n  Compatibility of Loops", "comments": "Accepted to the 21st International Society for Music Information\n  Retrieval Conference (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music producers who use loops may have access to thousands in loop libraries,\nbut finding ones that are compatible is a time-consuming process; we hope to\nreduce this burden with automation. State-of-the-art systems for estimating\ncompatibility, such as AutoMashUpper, are mostly rule-based and could be\nimproved on with machine learning. To train a model, we need a large set of\nloops with ground truth compatibility values. No such dataset exists, so we\nextract loops from existing music to obtain positive examples of compatible\nloops, and propose and compare various strategies for choosing negative\nexamples. For reproducibility, we curate data from the Free Music Archive.\nUsing this data, we investigate two types of model architectures for estimating\nthe compatibility of loops: one based on a Siamese network, and the other a\npure convolutional neural network (CNN). We conducted a user study in which\nparticipants rated the quality of the combinations suggested by each model, and\nfound the CNN to outperform the Siamese network. Both model-based approaches\noutperformed the rule-based one. We have opened source the code for building\nthe models and the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 09:16:50 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Chen", "Bo-Yu", ""], ["Smith", "Jordan B. L.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "2008.02014", "submitter": "Yijiang Lian", "authors": "Yijiang Lian, Zhijie Chen, Xin Pei, Shuang Li, Yifei Wang, Yuefeng\n  Qiu, Zhiheng Zhang, Zhipeng Tao, Liang Yuan, Hanju Guan, Kefeng Zhang,\n  Zhigang Li, Xiaochun Liu", "title": "Optimizing AD Pruning of Sponsored Search with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial sponsored search system (SSS) can be logically divided into three\nmodules: keywords matching, ad retrieving, and ranking. During ad retrieving,\nthe ad candidates grow exponentially. A query with high commercial value might\nretrieve a great deal of ad candidates such that the ranking module could not\nafford. Due to limited latency and computing resources, the candidates have to\nbe pruned earlier. Suppose we set a pruning line to cut SSS into two parts:\nupstream and downstream. The problem we are going to address is: how to pick\nout the best $K$ items from $N$ candidates provided by the upstream to maximize\nthe total system's revenue. Since the industrial downstream is very complicated\nand updated quickly, a crucial restriction in this problem is that the\nselection scheme should get adapted to the downstream. In this paper, we\npropose a novel model-free reinforcement learning approach to fixing this\nproblem. Our approach considers downstream as a black-box environment, and the\nagent sequentially selects items and finally feeds into the downstream, where\nrevenue would be estimated and used as a reward to improve the selection\npolicy. To the best of our knowledge, this is first time to consider the system\noptimization from a downstream adaption view. It is also the first time to use\nreinforcement learning techniques to tackle this problem. The idea has been\nsuccessfully realized in Baidu's sponsored search system, and online long time\nA/B test shows remarkable improvements on revenue.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 09:19:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lian", "Yijiang", ""], ["Chen", "Zhijie", ""], ["Pei", "Xin", ""], ["Li", "Shuang", ""], ["Wang", "Yifei", ""], ["Qiu", "Yuefeng", ""], ["Zhang", "Zhiheng", ""], ["Tao", "Zhipeng", ""], ["Yuan", "Liang", ""], ["Guan", "Hanju", ""], ["Zhang", "Kefeng", ""], ["Li", "Zhigang", ""], ["Liu", "Xiaochun", ""]]}, {"id": "2008.02017", "submitter": "Daniel Hienert", "authors": "Masoud Davari, Daniel Hienert, Dagmar Kern, and Stefan Dietze", "title": "The Role of Word-Eye-Fixations for Query Term Prediction", "comments": null, "journal-ref": "In CHIIR 2020, Proceedings of the 2020 Conference on Human\n  Information Interaction and Retrieval, March 2020, Pages 422-426", "doi": "10.1145/3343413.3378010", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the search process, the user's gaze on inspected SERPs and\nwebsites can reveal his or her search interests. Gaze behavior can be captured\nwith eye tracking and described with word-eye-fixations. Word-eye-fixations\ncontain the user's accumulated gaze fixation duration on each individual word\nof a web page. In this work, we analyze the role of word-eye-fixations for\npredicting query terms. We investigate the relationship between a range of\nin-session features, in particular, gaze data, with the query terms and train\nmodels for predicting query terms. We use a dataset of 50 search sessions\nobtained through a lab study in the social sciences domain. Using established\nmachine learning models, we can predict query terms with comparably high\naccuracy, even with only little training data. Feature analysis shows that the\ncategories Fixation, Query Relevance and Session Topic contain the most\neffective features for our task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 09:31:15 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Davari", "Masoud", ""], ["Hienert", "Daniel", ""], ["Kern", "Dagmar", ""], ["Dietze", "Stefan", ""]]}, {"id": "2008.02069", "submitter": "Gabriel Meseguer-Brocal", "authors": "Gabriel Meseguer-Brocal, Rachel Bittner, Simon Durand and Brian Brost", "title": "Data Cleansing with Contrastive Learning for Vocal Note Event\n  Annotations", "comments": "21st International Society for Music Information Retrieval Conference\n  11-15 October 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data cleansing is a well studied strategy for cleaning erroneous labels in\ndatasets, which has not yet been widely adopted in Music Information Retrieval.\nPreviously proposed data cleansing models do not consider structured (e.g. time\nvarying) labels, such as those common to music data. We propose a novel data\ncleansing model for time-varying, structured labels which exploits the local\nstructure of the labels, and demonstrate its usefulness for vocal note event\nannotations in music. %Our model is trained in a contrastive learning manner by\nautomatically creating local deformations of likely correct labels. Our model\nis trained in a contrastive learning manner by automatically contrasting likely\ncorrect labels pairs against local deformations of them. We demonstrate that\nthe accuracy of a transcription model improves greatly when trained using our\nproposed strategy compared with the accuracy when trained using the original\ndataset. Additionally we use our model to estimate the annotation error rates\nin the DALI dataset, and highlight other potential uses for this type of model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 12:24:37 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 10:15:04 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 10:19:17 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Meseguer-Brocal", "Gabriel", ""], ["Bittner", "Rachel", ""], ["Durand", "Simon", ""], ["Brost", "Brian", ""]]}, {"id": "2008.02108", "submitter": "Simona Rombo", "authors": "Mariella Bonomo and Armando La Placa and Simona E. Rombo", "title": "Identifying the $k$ Best Targets for an Advertisement Campaign via\n  Online Social Networks", "comments": "Accepted for publication in Proceedings of the 12th International\n  Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge\n  Management (KDIR2020). arXiv admin note: text overlap with arXiv:1907.01326", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for the recommendation of possible customers\n(users) to advertisers (e.g., brands) based on two main aspects: (i) the\ncomparison between On-line Social Network profiles, and (ii) neighborhood\nanalysis on the On-line Social Network. Profile matching between users and\nbrands is considered based on bag-of-words representation of textual contents\ncoming from the social media, and measures such as the Term Frequency-Inverse\nDocument Frequency are used in order to characterize the importance of words in\nthe comparison. The approach has been implemented relying on Big Data\nTechnologies, allowing this way the efficient analysis of very large Online\nSocial Networks. Results on real datasets show that the combination of profile\nmatching and neighborhood analysis is successful in identifying the most\nsuitable set of users to be used as target for a given advertisement campaign.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:52:26 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Bonomo", "Mariella", ""], ["La Placa", "Armando", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2008.02194", "submitter": "Carlos Eduardo Cancino-Chac\\'on", "authors": "Carlos Cancino-Chac\\'on, Silvan Peter, Shreyan Chowdhury, Anna\n  Aljanaki, Gerhard Widmer", "title": "On the Characterization of Expressive Performance in Classical Music:\n  First Results of the Con Espressione Game", "comments": "8 pages, 2 figures, accepted for the 21st International Society for\n  Music Information Retrieval Conference (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A piece of music can be expressively performed, or interpreted, in a variety\nof ways. With the help of an online questionnaire, the Con Espressione Game, we\ncollected some 1,500 descriptions of expressive character relating to 45\nperformances of 9 excerpts from classical piano pieces, played by different\nfamous pianists. More specifically, listeners were asked to describe, using\nfreely chosen words (preferably: adjectives), how they perceive the expressive\ncharacter of the different performances. In this paper, we offer a first\naccount of this new data resource for expressive performance research, and\nprovide an exploratory analysis, addressing three main questions: (1) how\nsimilarly do different listeners describe a performance of a piece? (2) what\nare the main dimensions (or axes) for expressive character emerging from this?;\nand (3) how do measurable parameters of a performance (e.g., tempo, dynamics)\nand mid- and high-level features that can be predicted by machine learning\nmodels (e.g., articulation, arousal) relate to these expressive dimensions? The\ndataset that we publish along with this paper was enriched by adding\nhand-corrected score-to-performance alignments, as well as descriptive audio\nfeatures such as tempo and dynamics curves.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:40:57 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Cancino-Chac\u00f3n", "Carlos", ""], ["Peter", "Silvan", ""], ["Chowdhury", "Shreyan", ""], ["Aljanaki", "Anna", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2008.02197", "submitter": "Manisha Verma", "authors": "Nisarg Raval, Manisha Verma", "title": "One word at a time: adversarial attacks on retrieval models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial examples, generated by applying small perturbations to input\nfeatures, are widely used to fool classifiers and measure their robustness to\nnoisy inputs. However, little work has been done to evaluate the robustness of\nranking models through adversarial examples. In this work, we present a\nsystematic approach of leveraging adversarial examples to measure the\nrobustness of popular ranking models. We explore a simple method to generate\nadversarial examples that forces a ranker to incorrectly rank the documents.\nUsing this approach, we analyze the robustness of various ranking models and\nthe quality of perturbations generated by the adversarial attacker across two\ndatasets. Our findings suggest that with very few token changes (1-3), the\nattacker can yield semantically similar perturbed documents that can fool\ndifferent rankers into changing a document's score, lowering its rank by\nseveral positions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:50:38 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Raval", "Nisarg", ""], ["Verma", "Manisha", ""]]}, {"id": "2008.02218", "submitter": "Sirui Wang", "authors": "Qiong Wu, Adam Hare, Sirui Wang, Yuwei Tu, Zhenming Liu, Christopher\n  G. Brinton, Yanhua Li", "title": "BATS: A Spectral Biclustering Approach to Single Document Topic Modeling\n  and Segmentation", "comments": "28 pages", "journal-ref": "ACM Transactions on Intelligent Systems and Technology, 2021", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing topic modeling and text segmentation methodologies generally require\nlarge datasets for training, limiting their capabilities when only small\ncollections of text are available. In this work, we reexamine the inter-related\nproblems of \"topic identification\" and \"text segmentation\" for sparse document\nlearning, when there is a single new text of interest. In developing a\nmethodology to handle single documents, we face two major challenges. First is\nsparse information: with access to only one document, we cannot train\ntraditional topic models or deep learning algorithms. Second is significant\nnoise: a considerable portion of words in any single document will produce only\nnoise and not help discern topics or segments. To tackle these issues, we\ndesign an unsupervised, computationally efficient methodology called BATS:\nBiclustering Approach to Topic modeling and Segmentation. BATS leverages three\nkey ideas to simultaneously identify topics and segment text: (i) a new\nmechanism that uses word order information to reduce sample complexity, (ii) a\nstatistically sound graph-based biclustering technique that identifies latent\nstructures of words and sentences, and (iii) a collection of effective\nheuristics that remove noise words and award important words to further improve\nperformance. Experiments on four datasets show that our approach outperforms\nseveral state-of-the-art baselines when considering topic coherence, topic\ndiversity, segmentation, and runtime comparison metrics.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:34:33 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 01:50:07 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 11:38:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wu", "Qiong", ""], ["Hare", "Adam", ""], ["Wang", "Sirui", ""], ["Tu", "Yuwei", ""], ["Liu", "Zhenming", ""], ["Brinton", "Christopher G.", ""], ["Li", "Yanhua", ""]]}, {"id": "2008.02372", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz", "title": "Reinforcement Learning-driven Information Seeking: A Quantum\n  Probabilistic Approach", "comments": "Accepted in Proceedings of Bridging the Gap between Information\n  Science, Information Retrieval and Data Science (BIRDS) at SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding an information forager's actions during interaction is very\nimportant for the study of interactive information retrieval. Although\ninformation spread in uncertain information space is substantially complex due\nto the high entanglement of users interacting with information objects~(text,\nimage, etc.). However, an information forager, in general, accompanies a piece\nof information (information diet) while searching (or foraging) alternative\ncontents, typically subject to decisive uncertainty. Such types of uncertainty\nare analogous to measurements in quantum mechanics which follow the uncertainty\nprinciple. In this paper, we discuss information seeking as a reinforcement\nlearning task. We then present a reinforcement learning-based framework to\nmodel forager exploration that treats the information forager as an agent to\nguide their behaviour. Also, our framework incorporates the inherent\nuncertainty of the foragers' action using the mathematical formalism of quantum\nmechanics.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 21:33:51 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "2008.02434", "submitter": "Ye Liu", "authors": "Ye Liu, Shaika Chowdhury, Chenwei Zhang, Cornelia Caragea, Philip S.\n  Yu", "title": "Interpretable Multi-Step Reasoning with Knowledge Extraction on Complex\n  Healthcare Question Answering", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Healthcare question answering assistance aims to provide customer healthcare\ninformation, which widely appears in both Web and mobile Internet. The\nquestions usually require the assistance to have proficient healthcare\nbackground knowledge as well as the reasoning ability on the knowledge.\nRecently a challenge involving complex healthcare reasoning, HeadQA dataset,\nhas been proposed, which contains multiple-choice questions authorized for the\npublic healthcare specialization exam. Unlike most other QA tasks that focus on\nlinguistic understanding, HeadQA requires deeper reasoning involving not only\nknowledge extraction, but also complex reasoning with healthcare knowledge.\nThese questions are the most challenging for current QA systems, and the\ncurrent performance of the state-of-the-art method is slightly better than a\nrandom guess. In order to solve this challenging task, we present a Multi-step\nreasoning with Knowledge extraction framework (MurKe). The proposed framework\nfirst extracts the healthcare knowledge as supporting documents from the large\ncorpus. In order to find the reasoning chain and choose the correct answer,\nMurKe iterates between selecting the supporting documents, reformulating the\nquery representation using the supporting documents and getting entailment\nscore for each choice using the entailment model. The reformulation module\nleverages selected documents for missing evidence, which maintains\ninterpretability. Moreover, we are striving to make full use of off-the-shelf\npre-trained models. With less trainable weight, the pre-trained model can\neasily adapt to healthcare tasks with limited training samples. From the\nexperimental results and ablation study, our system is able to outperform\nseveral strong baselines on the HeadQA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 02:47:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Ye", ""], ["Chowdhury", "Shaika", ""], ["Zhang", "Chenwei", ""], ["Caragea", "Cornelia", ""], ["Yu", "Philip S.", ""]]}, {"id": "2008.02460", "submitter": "Xiaowei Liu", "authors": "Weiwei Guo, Xiaowei Liu, Sida Wang, Huiji Gao, Ananth Sankar, Zimeng\n  Yang, Qi Guo, Liang Zhang, Bo Long, Bee-Chung Chen and Deepak Agarwal", "title": "DeText: A Deep Text Ranking Framework with BERT", "comments": "Ranking, Deep Language Models, Natural Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is the most important component in a search system. Mostsearch\nsystems deal with large amounts of natural language data,hence an effective\nranking system requires a deep understandingof text semantics. Recently, deep\nlearning based natural languageprocessing (deep NLP) models have generated\npromising results onranking systems. BERT is one of the most successful models\nthatlearn contextual embedding, which has been applied to capturecomplex\nquery-document relations for search ranking. However,this is generally done by\nexhaustively interacting each query wordwith each document word, which is\ninefficient for online servingin search product systems. In this paper, we\ninvestigate how tobuild an efficient BERT-based ranking model for industry use\ncases.The solution is further extended to a general ranking framework,DeText,\nthat is open sourced and can be applied to various rankingproductions. Offline\nand online experiments of DeText on threereal-world search systems present\nsignificant improvement overstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 05:12:11 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Guo", "Weiwei", ""], ["Liu", "Xiaowei", ""], ["Wang", "Sida", ""], ["Gao", "Huiji", ""], ["Sankar", "Ananth", ""], ["Yang", "Zimeng", ""], ["Guo", "Qi", ""], ["Zhang", "Liang", ""], ["Long", "Bo", ""], ["Chen", "Bee-Chung", ""], ["Agarwal", "Deepak", ""]]}, {"id": "2008.02546", "submitter": "Ye Bi", "authors": "Bo Huang, Ye Bi, Zhenyu Wu, Jianming Wang, Jing Xiao", "title": "UBER-GNN: A User-Based Embeddings Recommendation based on Graph Neural\n  Networks", "comments": "6 pages, accepted by CIKM 2019 GRLA workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of session-based recommendation aims to predict user next actions\nbased on session histories. Previous methods models session histories into\nsequences and estimate user latent features by RNN and GNN methods to make\nrecommendations. However under massive-scale and complicated financial\nrecommendation scenarios with both virtual and real commodities , such methods\nare not sufficient to represent accurate user latent features and neglect the\nlong-term characteristics of users. To take long-term preference and dynamic\ninterests into account, we propose a novel method, i.e. User-Based Embeddings\nRecommendation with Graph Neural Network, UBER-GNN for brevity. UBER-GNN takes\nadvantage of structured data to generate longterm user preferences, and\ntransfers session sequences into graphs to generate graph-based dynamic\ninterests. The final user latent feature is then represented as the composition\nof the long-term preferences and the dynamic interests using attention\nmechanism. Extensive experiments conducted on real Ping An scenario show that\nUBER-GNN outperforms the state-of-the-art session-based recommendation methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 09:54:03 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Huang", "Bo", ""], ["Bi", "Ye", ""], ["Wu", "Zhenyu", ""], ["Wang", "Jianming", ""], ["Xiao", "Jing", ""]]}, {"id": "2008.02687", "submitter": "Bereket Abera Yilma Mr.", "authors": "Bereket Abera Yilma, Najib Aghenda, Marcelo Romero, Yannick Naudet and\n  Herve Panetto", "title": "Personalised Visual Art Recommendation by Learning Latent Semantic\n  Representations", "comments": "Accepted at SMAP2020", "journal-ref": "SMAP 2020 15th International Workshop on Semantic and Social Media\n  Adaptation & Personalization", "doi": "10.1109/SMAP49528.2020.9248448", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Recommender systems, data representation techniques play a great role as\nthey have the power to entangle, hide and reveal explanatory factors embedded\nwithin datasets. Hence, they influence the quality of recommendations.\nSpecifically, in Visual Art (VA) recommendations the complexity of the concepts\nembodied within paintings, makes the task of capturing semantics by machines\nfar from trivial. In VA recommendation, prominent works commonly use manually\ncurated metadata to drive recommendations. Recent works in this domain aim at\nleveraging visual features extracted using Deep Neural Networks (DNN). However,\nsuch data representation approaches are resource demanding and do not have a\ndirect interpretation, hindering user acceptance. To address these limitations,\nwe introduce an approach for Personalised Recommendation of Visual arts based\non learning latent semantic representation of paintings. Specifically, we\ntrained a Latent Dirichlet Allocation (LDA) model on textual descriptions of\npaintings. Our LDA model manages to successfully uncover non-obvious semantic\nrelationships between paintings whilst being able to offer explainable\nrecommendations. Experimental evaluations demonstrate that our method tends to\nperform better than exploiting visual features extracted using pre-trained Deep\nNeural Networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:50:10 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Yilma", "Bereket Abera", ""], ["Aghenda", "Najib", ""], ["Romero", "Marcelo", ""], ["Naudet", "Yannick", ""], ["Panetto", "Herve", ""]]}, {"id": "2008.02734", "submitter": "Christopher Tralie", "authors": "Christopher Tralie, Elizabeth Dempsey", "title": "Exact, Parallelizable Dynamic Time Warping Alignment with Linear Memory", "comments": "12 Pages, 6 Figures, 1 Table, ISMIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio alignment is a fundamental preprocessing step in many MIR pipelines.\nFor two audio clips with M and N frames, respectively, the most popular\napproach, dynamic time warping (DTW), has O(MN) requirements in both memory and\ncomputation, which is prohibitive for frame-level alignments at reasonable\nrates. To address this, a variety of memory efficient algorithms exist to\napproximate the optimal alignment under the DTW cost. To our knowledge,\nhowever, no exact algorithms exist that are guaranteed to break the quadratic\nmemory barrier. In this work, we present a divide and conquer algorithm that\ncomputes the exact globally optimal DTW alignment using O(M+N) memory. Its\nruntime is still O(MN), trading off memory for a 2x increase in computation.\nHowever, the algorithm can be parallelized up to a factor of min(M, N) with the\nsame memory constraints, so it can still run more efficiently than the textbook\nversion with an adequate GPU. We use our algorithm to compute exact alignments\non a collection of orchestral music, which we use as ground truth to benchmark\nthe alignment accuracy of several popular approximate alignment schemes at\nscales that were not previously possible.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:00:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Tralie", "Christopher", ""], ["Dempsey", "Elizabeth", ""]]}, {"id": "2008.02736", "submitter": "Dhrubasish Sarkar Prof.", "authors": "Dhrubasish Sarkar", "title": "Recommending Influenceable Targets based on Influence Propagation\n  through Activity Behaviors in Online Social Media", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Media (OSM) is a platform through which the users present\nthemselves to the connected world by means of messaging, posting, reacting,\ntagging, and sharing on different contents with also other social activities.\nNowadays, it has a vast impact on various aspects of the industry, business and\nsociety along with on users life. In an OSN platform, reaching the target users\nis one of the primary focus for most of the businesses and other organizations.\nIdentification and recommendation of influenceable targets help to capture the\nappropriate audience efficiently and effectively. In this paper, an effective\nmodel has been discussed in egocentric OSN by incorporating an efficient\ninfluence measured Recommendation System in order to generate a list of top\nmost influenceable target users among all connected network members for any\nspecific social network user. Firstly the list of interacted network members\nhas been updated based on all activities. On which the interacted network\nmembers with most similar activities have been recommended based on the\nspecific influence category with sentiment type. After that, the top most\ninfluenceable network members in the basis of the required amount among those\nupdated list of interacted network members have been identified with proper\nranking by analyzing the similarity and frequency of their activity contents\nwith respect to the activity contents of the main user. Through these two\ncontinuous stages, an effective list of top influenceable targets of the main\nuser has been distinguished from the egocentric view of any social network.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 20:53:20 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Sarkar", "Dhrubasish", ""]]}, {"id": "2008.02837", "submitter": "Preslav Nakov", "authors": "Anton Chernyavskiy, Dmitry Ilvovsky, Preslav Nakov", "title": "aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF,\n  and Transfer Learning", "comments": "propaganda, persuasion, disinformation, fake news", "journal-ref": "SemEval-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our system for SemEval-2020 Task 11 on Detection of Propaganda\nTechniques in News Articles. We developed ensemble models using RoBERTa-based\nneural architectures, additional CRF layers, transfer learning between the two\nsubtasks, and advanced post-processing to handle the multi-label nature of the\ntask, the consistency between nested spans, repetitions, and labels from\nsimilar spans in training. We achieved sizable improvements over baseline\nfine-tuned RoBERTa models, and the official evaluation ranked our system 3rd\n(almost tied with the 2nd) out of 36 teams on the span identification subtask\nwith an F1 score of 0.491, and 2nd (almost tied with the 1st) out of 31 teams\non the technique classification subtask with an F1 score of 0.62.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 18:45:25 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Chernyavskiy", "Anton", ""], ["Ilvovsky", "Dmitry", ""], ["Nakov", "Preslav", ""]]}, {"id": "2008.02930", "submitter": "Ellie Chio", "authors": "Tao Wu, Ellie Ka-In Chio, Heng-Tze Cheng, Yu Du, Steffen Rendle, Dima\n  Kuzmin, Ritesh Agarwal, Li Zhang, John Anderson, Sarvjeet Singh, Tushar\n  Chandra, Ed H. Chi, Wen Li, Ankit Kumar, Xiang Ma, Alex Soares, Nitin Jindal,\n  Pei Cao", "title": "Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to\n  Cold-Start Search Retrieval", "comments": "Accepted at CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412752", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in neural information retrieval models, which predict\ntop-K items given a query, learn directly from a large training set of (query,\nitem) pairs. However, they are often insufficient when there are many\npreviously unseen (query, item) combinations, often referred to as the cold\nstart problem. Furthermore, the search system can be biased towards items that\nare frequently shown to a query previously, also known as the 'rich get richer'\n(a.k.a. feedback loop) problem. In light of these problems, we observed that\nmost online content platforms have both a search and a recommender system that,\nwhile having heterogeneous input spaces, can be connected through their common\noutput item space and a shared semantic representation. In this paper, we\npropose a new Zero-Shot Heterogeneous Transfer Learning framework that\ntransfers learned knowledge from the recommender system component to improve\nthe search component of a content platform. First, it learns representations of\nitems and their natural-language features by predicting (item, item)\ncorrelation graphs derived from the recommender system as an auxiliary task.\nThen, the learned representations are transferred to solve the target search\nretrieval task, performing query-to-item prediction without having seen any\n(query, item) pairs in training. We conduct online and offline experiments on\none of the world's largest search and recommender systems from Google, and\npresent the results and lessons learned. We demonstrate that the proposed\napproach can achieve high performance on offline search retrieval tasks, and\nmore importantly, achieved significant improvements on relevance and user\ninteractions over the highly-optimized production system in online experiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:22:56 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 00:16:40 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Wu", "Tao", ""], ["Chio", "Ellie Ka-In", ""], ["Cheng", "Heng-Tze", ""], ["Du", "Yu", ""], ["Rendle", "Steffen", ""], ["Kuzmin", "Dima", ""], ["Agarwal", "Ritesh", ""], ["Zhang", "Li", ""], ["Anderson", "John", ""], ["Singh", "Sarvjeet", ""], ["Chandra", "Tushar", ""], ["Chi", "Ed H.", ""], ["Li", "Wen", ""], ["Kumar", "Ankit", ""], ["Ma", "Xiang", ""], ["Soares", "Alex", ""], ["Jindal", "Nitin", ""], ["Cao", "Pei", ""]]}, {"id": "2008.02974", "submitter": "Wentao Ouyang", "authors": "Wentao Ouyang, Xiuwu Zhang, Lei Zhao, Jinmei Luo, Yu Zhang, Heng Zou,\n  Zhaojie Liu, Yanlong Du", "title": "MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate\n  Prediction", "comments": "CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-through rate (CTR) prediction is a critical task in online advertising\nsystems. Existing works mainly address the single-domain CTR prediction problem\nand model aspects such as feature interaction, user behavior history and\ncontextual information. Nevertheless, ads are usually displayed with natural\ncontent, which offers an opportunity for cross-domain CTR prediction. In this\npaper, we address this problem and leverage auxiliary data from a source domain\nto improve the CTR prediction performance of a target domain. Our study is\nbased on UC Toutiao (a news feed service integrated with the UC Browser App,\nserving hundreds of millions of users daily), where the source domain is the\nnews and the target domain is the ad. In order to effectively leverage news\ndata for predicting CTRs of ads, we propose the Mixed Interest Network (MiNet)\nwhich jointly models three types of user interest: 1) long-term interest across\ndomains, 2) short-term interest from the source domain and 3) short-term\ninterest in the target domain. MiNet contains two levels of attentions, where\nthe item-level attention can adaptively distill useful information from clicked\nnews / ads and the interest-level attention can adaptively fuse different\ninterest representations. Offline experiments show that MiNet outperforms\nseveral state-of-the-art methods for CTR prediction. We have deployed MiNet in\nUC Toutiao and the A/B test results show that the online CTR is also improved\nsubstantially. MiNet now serves the main ad traffic in UC Toutiao.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 03:27:38 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ouyang", "Wentao", ""], ["Zhang", "Xiuwu", ""], ["Zhao", "Lei", ""], ["Luo", "Jinmei", ""], ["Zhang", "Yu", ""], ["Zou", "Heng", ""], ["Liu", "Zhaojie", ""], ["Du", "Yanlong", ""]]}, {"id": "2008.03085", "submitter": "Aritra Banerjee", "authors": "Aritra Banerjee", "title": "SimPatch: A Nearest Neighbor Similarity Match between Image Patches", "comments": "12 pages, 13 figures, Submitted to International Journal of\n  Artificial Intelligence and Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarity between patches in images is a fundamental building\nblock in various tasks. Naturally, the patch-size has a major impact on the\nmatching quality, and on the consequent application performance. We try to use\nlarge patches instead of relatively small patches so that each patch contains\nmore information. We use different feature extraction mechanisms to extract the\nfeatures of each individual image patches which forms a feature matrix and find\nout the nearest neighbor patches in the image. The nearest patches are\ncalculated using two different nearest neighbor algorithms in this paper for a\nquery patch for a given image and the results have been demonstrated in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:51:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Banerjee", "Aritra", ""]]}, {"id": "2008.03260", "submitter": "Anshumali Shrivastava", "authors": "Nicholas Meisburger, Anshumali Shrivastava", "title": "Distributed Tera-Scale Similarity Search with MPI: Provably Efficient\n  Similarity Search over billions without a Single Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SLASH (Sketched LocAlity Sensitive Hashing), an MPI (Message\nPassing Interface) based distributed system for approximate similarity search\nover terabyte scale datasets. SLASH provides a multi-node implementation of the\npopular LSH (locality sensitive hashing) algorithm, which is generally\nimplemented on a single machine. We show how we can append the LSH algorithm\nwith heavy hitters sketches to provably solve the (high) similarity search\nproblem without a single distance computation. Overall, we mathematically show\nthat, under realistic data assumptions, we can identify the near-neighbor of a\ngiven query $q$ in sub-linear ($ \\ll O(n)$) number of simple sketch aggregation\noperations only. To make such a system practical, we offer a novel design and\nsketching solution to reduce the inter-machine communication overheads\nexponentially. In a direct comparison on comparable hardware, SLASH is more\nthan 10000x faster than the popular LSH package in PySpark. PySpark is a\nwidely-adopted distributed implementation of the LSH algorithm for large\ndatasets and is deployed in commercial platforms. In the end, we show how our\nsystem scale to Tera-scale Criteo dataset with more than 4 billion samples.\nSLASH can index this 2.3 terabyte data over 20 nodes in under an hour, with\nquery times in a fraction of milliseconds. To the best of our knowledge, there\nis no open-source system that can index and perform a similarity search on\nCriteo with a commodity cluster.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:15:36 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 22:48:52 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Meisburger", "Nicholas", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2008.03397", "submitter": "Lana Yeganova", "authors": "Lana Yeganova, Rezarta Islamaj, Qingyu Chen, Robert Leaman, Alexis\n  Allot, Chin-Hsuan Wei, Donald C. Comeau, Won Kim, Yifan Peng, W. John Wilbur,\n  Zhiyong Lu", "title": "Navigating the landscape of COVID-19 research through literature\n  analysis: A bird's eye view", "comments": "10 pages, 8 Figures, Submitted to KDD 2020 Health Day", "journal-ref": "KDD 2020 Health Day: AI for COVID, August 23-27, 2020, Virtual\n  Conference, CA, US", "doi": null, "report-no": null, "categories": "cs.DL cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely access to accurate scientific literature in the battle with the\nongoing COVID-19 pandemic is critical. This unprecedented public health risk\nhas motivated research towards understanding the disease in general,\nidentifying drugs to treat the disease, developing potential vaccines, etc.\nThis has given rise to a rapidly growing body of literature that doubles in\nnumber of publications every 20 days as of May 2020. Providing medical\nprofessionals with means to quickly analyze the literature and discover growing\nareas of knowledge is necessary for addressing their question and information\nneeds.\n  In this study we analyze the LitCovid collection, 13,369 COVID-19 related\narticles found in PubMed as of May 15th, 2020 with the purpose of examining the\nlandscape of literature and presenting it in a format that facilitates\ninformation navigation and understanding. We do that by applying\nstate-of-the-art named entity recognition, classification, clustering and other\nNLP techniques. By applying NER tools, we capture relevant bioentities (such as\ndiseases, internal body organs, etc.) and assess the strength of their\nrelationship with COVID-19 by the extent they are discussed in the corpus. We\nalso collect a variety of symptoms and co-morbidities discussed in reference to\nCOVID-19. Our clustering algorithm identifies topics represented by groups of\nrelated terms, and computes clusters corresponding to documents associated with\nthe topic terms. Among the topics we observe several that persist through the\nduration of multiple weeks and have numerous associated documents, as well\nseveral that appear as emerging topics with fewer documents. All the tools and\ndata are publicly available, and this framework can be applied to any\nliterature collection. Taken together, these analyses produce a comprehensive,\nsynthesized view of COVID-19 research to facilitate knowledge discovery from\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 23:39:29 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 21:01:27 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Yeganova", "Lana", ""], ["Islamaj", "Rezarta", ""], ["Chen", "Qingyu", ""], ["Leaman", "Robert", ""], ["Allot", "Alexis", ""], ["Wei", "Chin-Hsuan", ""], ["Comeau", "Donald C.", ""], ["Kim", "Won", ""], ["Peng", "Yifan", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2008.03415", "submitter": "Shubhanshu Mishra", "authors": "Shubhanshu Mishra, Sijun He, Luca Belli", "title": "Assessing Demographic Bias in Named Entity Recognition", "comments": "Presented at the AKBC Workshop on Bias in Automatic Knowledge Graph\n  Construction, 2020 (arXiv:2007.11659)", "journal-ref": null, "doi": null, "report-no": "REPORT-NO:KGBias/2020/02", "categories": "cs.CL cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Named Entity Recognition (NER) is often the first step towards automated\nKnowledge Base (KB) generation from raw text. In this work, we assess the bias\nin various Named Entity Recognition (NER) systems for English across different\ndemographic groups with synthetically generated corpora. Our analysis reveals\nthat models perform better at identifying names from specific demographic\ngroups across two datasets. We also identify that debiased embeddings do not\nhelp in resolving this issue. Finally, we observe that character-based\ncontextualized word representation models such as ELMo results in the least\nbias across demographics. Our work can shed light on potential biases in\nautomated KB generation due to systematic exclusion of named entities belonging\nto certain demographics.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 02:01:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mishra", "Shubhanshu", ""], ["He", "Sijun", ""], ["Belli", "Luca", ""]]}, {"id": "2008.03717", "submitter": "Antonios Minas Krasakis", "authors": "Antonios Minas Krasakis, Mohammad Aliannejadi, Nikos Voskarides,\n  Evangelos Kanoulas", "title": "Analysing the Effect of Clarifying Questions on Document Ranking in\n  Conversational Search", "comments": "Proceedings of the 2020 ACM SIGIR International Conference on the\n  Theory of Information Retrieval (ICTIR '20), September 14-17, 2020", "journal-ref": null, "doi": "10.1145/3409256.3409817", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on conversational search highlights the importance of\nmixed-initiative in conversations. To enable mixed-initiative, the system\nshould be able to ask clarifying questions to the user. However, the ability of\nthe underlying ranking models (which support conversational search) to account\nfor these clarifying questions and answers has not been analysed when ranking\ndocuments, at large. To this end, we analyse the performance of a lexical\nranking model on a conversational search dataset with clarifying questions. We\ninvestigate, both quantitatively and qualitatively, how different aspects of\nclarifying questions and user answers affect the quality of ranking. We argue\nthat there needs to be some fine-grained treatment of the entire conversational\nround of clarification, based on the explicit feedback which is present in such\nmixed-initiative settings. Informed by our findings, we introduce a simple\nheuristic-based lexical baseline, that significantly outperforms the existing\nnaive baselines. Our work aims to enhance our understanding of the challenges\npresent in this particular task and inform the design of more appropriate\nconversational ranking models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:55:16 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 10:21:13 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Krasakis", "Antonios Minas", ""], ["Aliannejadi", "Mohammad", ""], ["Voskarides", "Nikos", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2008.03729", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, Juhan Nam", "title": "Metric Learning vs Classification for Disentangled Music Representation\n  Learning", "comments": "Accepted for publication at the 21st International Society for Music\n  Information Retrieval Conference (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep representation learning offers a powerful paradigm for mapping input\ndata onto an organized embedding space and is useful for many music information\nretrieval tasks. Two central methods for representation learning include deep\nmetric learning and classification, both having the same goal of learning a\nrepresentation that can generalize well across tasks. Along with\ngeneralization, the emerging concept of disentangled representations is also of\ngreat interest, where multiple semantic concepts (e.g., genre, mood,\ninstrumentation) are learned jointly but remain separable in the learned\nrepresentation space. In this paper we present a single representation learning\nframework that elucidates the relationship between metric learning,\nclassification, and disentanglement in a holistic manner. For this, we (1)\noutline past work on the relationship between metric learning and\nclassification, (2) extend this relationship to multi-label data by exploring\nthree different learning approaches and their disentangled versions, and (3)\nevaluate all models on four tasks (training time, similarity retrieval,\nauto-tagging, and triplet prediction). We find that classification-based models\nare generally advantageous for training time, similarity retrieval, and\nauto-tagging, while deep metric learning exhibits better performance for\ntriplet-prediction. Finally, we show that our proposed approach yields\nstate-of-the-art results for music auto-tagging.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:53:12 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 21:46:52 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lee", "Jongpil", ""], ["Bryan", "Nicholas J.", ""], ["Salamon", "Justin", ""], ["Jin", "Zeyu", ""], ["Nam", "Juhan", ""]]}, {"id": "2008.03797", "submitter": "Manel Slokom", "authors": "Manel Slokom, Martha Larson and Alan Hanjalic", "title": "Partially Synthetic Data for Recommender Systems: Prediction Performance\n  and Preference Hiding", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper demonstrates the potential of statistical disclosure control for\nprotecting the data used to train recommender systems. Specifically, we use a\nsynthetic data generation approach to hide specific information in the\nuser-item matrix. We apply a transformation to the original data that changes\nsome values, but leaves others the same. The result is a partially synthetic\ndata set that can be used for recommendation but contains less specific\ninformation about individual user preferences. Synthetic data has the potential\nto be useful for companies, who are interested in releasing data to allow\noutside parties to develop new recommender algorithms, i.e., in the case of a\nrecommender system challenge, and also reducing the risks associated with data\nmisappropriation. Our experiments run a set of recommender system algorithms on\nour partially synthetic data sets as well as on the original data. The results\nshow that the relative performance of the algorithms on the partially synthetic\ndata reflects the relative performance on the original data. Further analysis\ndemonstrates that properties of the original data are preserved under\nsynthesis, but that for certain examples of attributes accessible in the\noriginal data are hidden in the synthesized data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 19:39:24 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Slokom", "Manel", ""], ["Larson", "Martha", ""], ["Hanjalic", "Alan", ""]]}, {"id": "2008.03808", "submitter": "Mohammed Alqahtani", "authors": "Mohammed Alqahtani, Susan Gauch, Omar Salman, Mohammed Ibrahim, Reem\n  Al-Saffar", "title": "Diverse Group Formation Based on Multiple Demographic Features", "comments": null, "journal-ref": "KDIR, 169-178, 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of group formation is to build a team to accomplish a specific task.\nAlgorithms are employed to improve the effectiveness of the team so formed and\nthe efficiency of the group selection process. However, there is concern that\nteam formation algorithms could be biased against minorities due to the\nalgorithms themselves or the data on which they are trained. Hence, it is\nessential to build fair team formation systems that incorporate demographic\ninformation into the process of building the group. Although there has been\nextensive work on modeling individuals expertise for expert recommendation and\nor team formation, there has been relatively little prior work on modeling\ndemographics and incorporating demographics into the group formation process.\n  We propose a novel method to represent experts demographic profiles based on\nmultidimensional demographic features. Moreover, we introduce two diversity\nranking algorithms that form a group by considering demographic features along\nwith the minimum required skills. Unlike many ranking algorithms that consider\none Boolean demographic feature (e.g., gender or race), our diversity ranking\nalgorithms consider multiple multivalued demographic attributes simultaneously.\nWe evaluate our proposed algorithms using a real dataset based on members of a\ncomputer science program committee. The result shows that our algorithms form a\nprogram committee that is more diverse with an acceptable loss in utility.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 20:15:04 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 06:11:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Alqahtani", "Mohammed", ""], ["Gauch", "Susan", ""], ["Salman", "Omar", ""], ["Ibrahim", "Mohammed", ""], ["Al-Saffar", "Reem", ""]]}, {"id": "2008.03844", "submitter": "Xiaomei Bai", "authors": "Xiaomei Bai, Ivan Lee, Zhaolong Ning, Amr Tolba, Feng Xia", "title": "The Role of Positive and Negative Citations in Scientific Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the impact of scientific papers objectively is crucial for\nresearch output assessment, which subsequently affects institution and country\nrankings, research funding allocations, academic recruitment and\nnational/international scientific priorities. While most of the assessment\nschemes based on publication citations may potentially be manipulated through\nnegative citations, in this study, we explore Conflict of Interest (COI)\nrelationships and discover negative citations and subsequently weaken the\nassociated citation strength. PANDORA (Positive And Negative COI- Distinguished\nObjective Rank Algorithm) has been developed, which captures the positive and\nnegative COI, together with the positive and negative suspected COI\nrelationships. In order to alleviate the influence caused by negative COI\nrelationship, collaboration times, collaboration time span, citation times and\ncitation time span are employed to determine the citing strength; while for\npositive COI relationship, we regard it as normal citation relationship.\nFurthermore, we calculate the impact of scholarly papers by PageRank and HITS\nalgorithms, based on a credit allocation algorithm which is utilized to assess\nthe impact of institutions fairly and objectively. Experiments are conducted on\nthe publication dataset from American Physical Society (APS) dataset, and the\nresults demonstrate that our method significantly outperforms the current\nsolutions in Recommendation Intensity of list R at top-K and Spearman's rank\ncorrelation coefficient at top-K.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 00:23:53 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Bai", "Xiaomei", ""], ["Lee", "Ivan", ""], ["Ning", "Zhaolong", ""], ["Tolba", "Amr", ""], ["Xia", "Feng", ""]]}, {"id": "2008.03917", "submitter": "Kuan Fang", "authors": "Kuan Fang, Long Zhao, Zhan Shen, RuiXing Wang, RiKang Zhour, LiWen Fan", "title": "Beyond Lexical: A Semantic Retrieval Framework for Textual SearchEngine", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine has become a fundamental component in various web and mobile\napplications. Retrieving relevant documents from the massive datasets is\nchallenging for a search engine system, especially when faced with verbose or\ntail queries. In this paper, we explore a vector space search framework for\ndocument retrieval. Specifically, we trained a deep semantic matching model so\nthat each query and document can be encoded as a low dimensional embedding. Our\nmodel was trained based on BERT architecture. We deployed a fast\nk-nearest-neighbor index service for online serving. Both offline and online\nmetrics demonstrate that our method improved retrieval performance and search\nquality considerably, particularly for tail\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 06:37:47 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fang", "Kuan", ""], ["Zhao", "Long", ""], ["Shen", "Zhan", ""], ["Wang", "RuiXing", ""], ["Zhour", "RiKang", ""], ["Fan", "LiWen", ""]]}, {"id": "2008.04185", "submitter": "Junwei Zhang", "authors": "Junwei Zhang, Min Gao, Junliang Yu, Linda Yang, Zongwei Wang and\n  Qingyu Xiong", "title": "Path-Based Reasoning over Heterogeneous Networks for Recommendation via\n  Bidirectional Modeling", "comments": "11 pages, 5 figures, Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Information Network (HIN) is a natural and general\nrepresentation of data in recommender systems. Combining HIN and recommender\nsystems can not only help model user behaviors but also make the recommendation\nresults explainable by aligning the users/items with various types of entities\nin the network. Over the past few years, path-based reasoning models have shown\ngreat capacity in HIN-based recommendation. The basic idea of these models is\nto explore HIN with predefined path schemes. Despite their effectiveness, these\nmodels are often confronted with the following limitations: (1) Most prior\npath-based reasoning models only consider the influence of the predecessors on\nthe subsequent nodes when modeling the sequences, and ignore the reciprocity\nbetween the nodes in a path; (2) The weights of nodes in the same path instance\nare usually assumed to be constant, whereas varied weights of nodes can bring\nmore flexibility and lead to expressive modeling; (3) User-item interactions\nare noisy, but they are often indiscriminately exploited. To overcome the\naforementioned issues, in this paper, we propose a novel path-based reasoning\napproach for recommendation over HIN. Concretely, we use a bidirectional LSTM\nto enable the two-way modeling of paths and capture the reciprocity between\nnodes. Then an attention mechanism is employed to learn the dynamical influence\nof nodes in different contexts. Finally, the adversarial regularization terms\nare imposed on the loss function of the model to mitigate the effects of noise\nand enhance HIN-based recommendation. Extensive experiments conducted on three\npublic datasets show that our model outperforms the state-of-the-art baselines.\nThe case study further demonstrates the feasibility of our model on the\nexplainable recommendation task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 15:12:21 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Junwei", ""], ["Gao", "Min", ""], ["Yu", "Junliang", ""], ["Yang", "Linda", ""], ["Wang", "Zongwei", ""], ["Xiong", "Qingyu", ""]]}, {"id": "2008.04374", "submitter": "Preslav Nakov", "authors": "Preslav Nakov", "title": "Can We Spot the \"Fake News\" Before It Was Even Written?", "comments": "Fake News, Disinformation, Media Bias, Propaganda, Infodemic,\n  COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the recent proliferation of disinformation online, there has been also\ngrowing research interest in automatically debunking rumors, false claims, and\n\"fake news.\" A number of fact-checking initiatives have been launched so far,\nboth manual and automatic, but the whole enterprise remains in a state of\ncrisis: by the time a claim is finally fact-checked, it could have reached\nmillions of users, and the harm caused could hardly be undone. An arguably more\npromising direction is to focus on fact-checking entire news outlets, which can\nbe done in advance. Then, we could fact-check the news before it was even\nwritten: by checking how trustworthy the outlets that published it is. We\ndescribe how we do this in the Tanbih news aggregator, which makes readers\naware of what they are reading. In particular, we develop media profiles that\nshow the general factuality of reporting, the degree of propagandistic content,\nhyper-partisanship, leading political ideology, general frame of reporting, and\nstance with respect to various claims and topics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:21:06 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Nakov", "Preslav", ""]]}, {"id": "2008.04467", "submitter": "Mubin Ul Haque", "authors": "Mubin Ul Haque, Leonardo Horn Iwaya and M. Ali Babar", "title": "Challenges in Docker Development: A Large-scale Study Using Stack\n  Overflow", "comments": "11 pages, 3 Figures, conference", "journal-ref": null, "doi": "10.1145/3382494.3410693", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Docker technology has been increasingly used among software developers in a\nmultitude of projects. This growing interest is due to the fact that Docker\ntechnology supports a convenient process for creating and building containers,\npromoting close cooperation between developer and operations teams, and\nenabling continuous software delivery. As a fast-growing technology, it is\nimportant to identify the Docker-related topics that are most popular as well\nas existing challenges and difficulties that developers face. This paper\npresents a large-scale empirical study identifying practitioners' perspectives\non Docker technology by mining posts from the Stack Overflow (SoF) community.\nMethod: A dataset of 113,922 Docker-related posts was created based on a set of\nrelevant tags and contents. The dataset was cleaned and prepared. Topic\nmodelling was conducted using Latent Dirichlet Allocation (LDA), allowing the\nidentification of dominant topics in the domain. Our results show that most\ndevelopers use SoF to ask about a broad spectrum of Docker topics including\nframework development, application deployment, continuous integration,\nweb-server configuration and many more. We determined that 30 topics that\ndevelopers discuss can be grouped into 13 main categories. Most of the posts\nbelong to categories of application development, configuration, and networking.\nOn the other hand, we find that the posts on monitoring status, transferring\ndata, and authenticating users are more popular among developers compared to\nthe other topics. Specifically, developers face challenges in web browser\nissues, networking error and memory management. Besides, there is a lack of\nexperts in this domain. Our research findings will guide future work on the\ndevelopment of new tools and techniques, helping the community to focus efforts\nand understand existing trade-offs on Docker topics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 01:19:23 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Haque", "Mubin Ul", ""], ["Iwaya", "Leonardo Horn", ""], ["Babar", "M. Ali", ""]]}, {"id": "2008.04504", "submitter": "Li Jiacheng", "authors": "Jiacheng Li, Siliang Tang, Juncheng Li, Jun Xiao, Fei Wu, Shiliang Pu,\n  Yueting Zhuang", "title": "Topic Adaptation and Prototype Encoding for Few-Shot Visual Storytelling", "comments": "ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413886", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Storytelling~(VIST) is a task to tell a narrative story about a\ncertain topic according to the given photo stream. The existing studies focus\non designing complex models, which rely on a huge amount of human-annotated\ndata. However, the annotation of VIST is extremely costly and many topics\ncannot be covered in the training dataset due to the long-tail topic\ndistribution. In this paper, we focus on enhancing the generalization ability\nof the VIST model by considering the few-shot setting. Inspired by the way\nhumans tell a story, we propose a topic adaptive storyteller to model the\nability of inter-topic generalization. In practice, we apply the gradient-based\nmeta-learning algorithm on multi-modal seq2seq models to endow the model the\nability to adapt quickly from topic to topic. Besides, We further propose a\nprototype encoding structure to model the ability of intra-topic derivation.\nSpecifically, we encode and restore the few training story text to serve as a\nreference to guide the generation at inference time. Experimental results show\nthat topic adaptation and prototype encoding structure mutually bring benefit\nto the few-shot model on BLEU and METEOR metric. The further case study shows\nthat the stories generated after few-shot adaptation are more relative and\nexpressive.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 03:55:11 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Li", "Jiacheng", ""], ["Tang", "Siliang", ""], ["Li", "Juncheng", ""], ["Xiao", "Jun", ""], ["Wu", "Fei", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2008.04545", "submitter": "Zusheng Zhang", "authors": "Jiachun Feng, Zusheng Zhang, Cheng Ding, Yanghui Rao and Haoran Xie", "title": "Context Reinforced Neural Topic Modeling over Short Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the prevalent topic mining tools, neural topic modeling has\nattracted a lot of interests for the advantages of high efficiency in training\nand strong generalisation abilities. However, due to the lack of context in\neach short text, the existing neural topic models may suffer from feature\nsparsity on such documents. To alleviate this issue, we propose a Context\nReinforced Neural Topic Model (CRNTM), whose characteristics can be summarized\nas follows. Firstly, by assuming that each short text covers only a few salient\ntopics, CRNTM infers the topic for each word in a narrow range. Secondly, our\nmodel exploits pre-trained word embeddings by treating topics as multivariate\nGaussian distributions or Gaussian mixture distributions in the embedding\nspace. Extensive experiments on two benchmark datasets validate the\neffectiveness of the proposed model on both topic discovery and text\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 06:41:53 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Feng", "Jiachun", ""], ["Zhang", "Zusheng", ""], ["Ding", "Cheng", ""], ["Rao", "Yanghui", ""], ["Xie", "Haoran", ""]]}, {"id": "2008.04563", "submitter": "Masahiro Sato", "authors": "Masahiro Sato, Sho Takemori, Janmajay Singh, Tomoko Ohkuma", "title": "Unbiased Learning for the Causal Effect of Recommendation", "comments": "accepted at RecSys 2020, updated several experiments", "journal-ref": null, "doi": "10.1145/3383313.3412261", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing users' positive interactions, such as purchases or clicks, is an\nimportant objective of recommender systems. Recommenders typically aim to\nselect items that users will interact with. If the recommended items are\npurchased, an increase in sales is expected. However, the items could have been\npurchased even without recommendation. Thus, we want to recommend items that\nresults in purchases caused by recommendation. This can be formulated as a\nranking problem in terms of the causal effect. Despite its importance, this\nproblem has not been well explored in the related research. It is challenging\nbecause the ground truth of causal effect is unobservable, and estimating the\ncausal effect is prone to the bias arising from currently deployed\nrecommenders. This paper proposes an unbiased learning framework for the causal\neffect of recommendation. Based on the inverse propensity scoring technique,\nthe proposed framework first constructs unbiased estimators for ranking\nmetrics. Then, it conducts empirical risk minimization on the estimators with\npropensity capping, which reduces variance under finite training samples. Based\non the framework, we develop an unbiased learning method for the causal effect\nextension of a ranking metric. We theoretically analyze the unbiasedness of the\nproposed method and empirically demonstrate that the proposed method\noutperforms other biased learning methods in various settings.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:30:44 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 04:34:11 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 11:15:39 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Sato", "Masahiro", ""], ["Takemori", "Sho", ""], ["Singh", "Janmajay", ""], ["Ohkuma", "Tomoko", ""]]}, {"id": "2008.04579", "submitter": "Ye Bi", "authors": "Liqiang Song, Ye Bi, Mengqiu Yao, Zhenyu Wu, Jianming Wang, Jing Xiao", "title": "DREAM: A Dynamic Relational-Aware Model for Social Recommendation", "comments": "5 pages, accepted by CIKM 2020 Short Paper Session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social connections play a vital role in improving the performance of\nrecommendation systems (RS). However, incorporating social information into RS\nis challenging. Most existing models usually consider social influences in a\ngiven session, ignoring that both users preferences and their friends\ninfluences are evolving. Moreover, in real world, social relations are sparse.\nModeling dynamic influences and alleviating data sparsity is of great\nimportance. In this paper, we propose a unified framework named Dynamic\nRElation Aware Model (DREAM) for social recommendation, which tries to model\nboth users dynamic interests and their friends temporal influences.\nSpecifically, we design temporal information encoding modules, because of which\nuser representations are updated in each session. The updated user\nrepresentations are transferred to relational-GAT modules, subsequently\ninfluence the operations on social networks. In each session, to solve social\nrelation sparsity, we utilize glove-based method to complete social network\nwith virtual friends. Then we employ relational-GAT module over completed\nsocial networks to update users representations. In the extensive experiments\non the public datasets, DREAM significantly outperforms the state-of-the-art\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:34:37 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Song", "Liqiang", ""], ["Bi", "Ye", ""], ["Yao", "Mengqiu", ""], ["Wu", "Zhenyu", ""], ["Wang", "Jianming", ""], ["Xiao", "Jing", ""]]}, {"id": "2008.04653", "submitter": "Feng Xia", "authors": "Feng Xia, Nana Yaw Asabere, Haifeng Liu, Zhen Chen, and Wei Wang", "title": "Socially-Aware Conference Participant Recommendation with Personality\n  Traits", "comments": "12 pages, 13 figures", "journal-ref": "IEEE Systems Journal, 11(4): 2255-2266, 2017", "doi": "10.1109/JSYST.2014.2342375", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of the importance of academic collaboration at smart conferences,\nvarious researchers have utilized recommender systems to generate effective\nrecommendations for participants. Recent research has shown that the\npersonality traits of users can be used as innovative entities for effective\nrecommendations. Nevertheless, subjective perceptions involving the personality\nof participants at smart conferences are quite rare and haven't gained much\nattention. Inspired by the personality and social characteristics of users, we\npresent an algorithm called Socially and Personality Aware Recommendation of\nParticipants (SPARP). Our recommendation methodology hybridizes the\ncomputations of similar interpersonal relationships and personality traits\namong participants. SPARP models the personality and social characteristic\nprofiles of participants at a smart conference. By combining the above\nrecommendation entities, SPARP then recommends participants to each other for\neffective collaborations. We evaluate SPARP using a relevant dataset.\nExperimental results confirm that SPARP is reliable and outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 03:43:37 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Xia", "Feng", ""], ["Asabere", "Nana Yaw", ""], ["Liu", "Haifeng", ""], ["Chen", "Zhen", ""], ["Wang", "Wei", ""]]}, {"id": "2008.04820", "submitter": "Sopan Khosla", "authors": "Sopan Khosla, Rishabh Joshi, Ritam Dutt, Alan W Black, Yulia Tsvetkov", "title": "LTIatCMU at SemEval-2020 Task 11: Incorporating Multi-Level Features for\n  Multi-Granular Propaganda Span Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we describe our submission for the task of Propaganda Span\nIdentification in news articles. We introduce a BERT-BiLSTM based span-level\npropaganda classification model that identifies which token spans within the\nsentence are indicative of propaganda. The \"multi-granular\" model incorporates\nlinguistic knowledge at various levels of text granularity, including word,\nsentence and document level syntactic, semantic and pragmatic affect features,\nwhich significantly improve model performance, compared to its\nlanguage-agnostic variant. To facilitate better representation learning, we\nalso collect a corpus of 10k news articles, and use it for fine-tuning the\nmodel. The final model is a majority-voting ensemble which learns different\npropaganda class boundaries by leveraging different subsets of incorporated\nknowledge and attains $4^{th}$ position on the test leaderboard. Our final\nmodel and code is released at https://github.com/sopu/PropagandaSemEval2020.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:14:47 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 15:14:18 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Khosla", "Sopan", ""], ["Joshi", "Rishabh", ""], ["Dutt", "Ritam", ""], ["Black", "Alan W", ""], ["Tsvetkov", "Yulia", ""]]}, {"id": "2008.04876", "submitter": "Jiaxi Tang", "authors": "Jiaxi Tang, Hongyi Wen, Ke Wang", "title": "Revisiting Adversarially Learned Injection Attacks Against Recommender\n  Systems", "comments": "Accepted at Recsys 20", "journal-ref": null, "doi": "10.1145/3383313.3412243", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play an important role in modern information and\ne-commerce applications. While increasing research is dedicated to improving\nthe relevance and diversity of the recommendations, the potential risks of\nstate-of-the-art recommendation models are under-explored, that is, these\nmodels could be subject to attacks from malicious third parties, through\ninjecting fake user interactions to achieve their purposes. This paper revisits\nthe adversarially-learned injection attack problem, where the injected fake\nuser `behaviors' are learned locally by the attackers with their own model --\none that is potentially different from the model under attack, but shares\nsimilar properties to allow attack transfer. We found that most existing works\nin literature suffer from two major limitations: (1) they do not solve the\noptimization problem precisely, making the attack less harmful than it could\nbe, (2) they assume perfect knowledge for the attack, causing the lack of\nunderstanding for realistic attack capabilities. We demonstrate that the exact\nsolution for generating fake users as an optimization problem could lead to a\nmuch larger impact. Our experiments on a real-world dataset reveal important\nproperties of the attack, including attack transferability and its limitations.\nThese findings can inspire useful defensive methods against this possible\nexisting attack.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:30:02 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 05:03:41 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Tang", "Jiaxi", ""], ["Wen", "Hongyi", ""], ["Wang", "Ke", ""]]}, {"id": "2008.05180", "submitter": "Christian Schulz", "authors": "Alexander Gellner, Sebastian Lamm, Christian Schulz, Darren Strash,\n  Bogd\\'an Zav\\'alnij", "title": "Boosting Data Reduction for the Maximum Weight Independent Set Problem\n  Using Increasing Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a vertex-weighted graph, the maximum weight independent set problem\nasks for a pair-wise non-adjacent set of vertices such that the sum of their\nweights is maximum. The branch-and-reduce paradigm is the de facto standard\napproach to solve the problem to optimality in practice. In this paradigm, data\nreduction rules are applied to decrease the problem size. These data reduction\nrules ensure that given an optimum solution on the new (smaller) input, one can\nquickly construct an optimum solution on the original input.\n  We introduce new generalized data reduction and transformation rules for the\nproblem. A key feature of our work is that some transformation rules can\nincrease the size of the input. Surprisingly, these so-called increasing\ntransformations can simplify the problem and also open up the reduction space\nto yield even smaller irreducible graphs later throughout the algorithm. In\nexperiments, our algorithm computes significantly smaller irreducible graphs on\nall except one instance, solves more instances to optimality than previously\npossible, is up to two orders of magnitude faster than the best\nstate-of-the-art solver, and finds higher-quality solutions than heuristic\nsolvers DynWVC and HILS on many instances. While the increasing transformations\nare only efficient enough for preprocessing at this time, we see this as a\ncritical initial step towards a new branch-and-transform paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:52:50 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 05:45:23 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Gellner", "Alexander", ""], ["Lamm", "Sebastian", ""], ["Schulz", "Christian", ""], ["Strash", "Darren", ""], ["Zav\u00e1lnij", "Bogd\u00e1n", ""]]}, {"id": "2008.05363", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Markus Zlabinger, Mete Sertkan, Michael\n  Schr\\\"oder, Allan Hanbury", "title": "Fine-Grained Relevance Annotations for Multi-Task Document Ranking and\n  Question Answering", "comments": "Accepted at CIKM 2020 (Resource Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many existing retrieval and question answering datasets. However,\nmost of them either focus on ranked list evaluation or single-candidate\nquestion answering. This divide makes it challenging to properly evaluate\napproaches concerned with ranking documents and providing snippets or answers\nfor a given query. In this work, we present FiRA: a novel dataset of\nFine-Grained Relevance Annotations. We extend the ranked retrieval annotations\nof the Deep Learning track of TREC 2019 with passage and word level graded\nrelevance annotations for all relevant documents. We use our newly created data\nto study the distribution of relevance in long documents, as well as the\nattention of annotators to specific positions of the text. As an example, we\nevaluate the recently introduced TKL document ranking model. We find that\nalthough TKL exhibits state-of-the-art retrieval results for long documents, it\nmisses many relevant passages.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:59:50 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zlabinger", "Markus", ""], ["Sertkan", "Mete", ""], ["Schr\u00f6der", "Michael", ""], ["Hanbury", "Allan", ""]]}, {"id": "2008.05399", "submitter": "Zhiyun Ren", "authors": "Ziwei Fan, Evan Burgun, Zhiyun Ren, Titus Schleyer, Xia Ning", "title": "Improving information retrieval from electronic health records using\n  dynamic and multi-collaborative filtering", "comments": "18 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the rapid growth of information available about individual patients,\nmost physicians suffer from information overload when they review patient\ninformation in health information technology systems. In this manuscript, we\npresent a novel hybrid dynamic and multi-collaborative filtering method to\nimprove information retrieval from electronic health records. This method\nrecommends relevant information from electronic health records for physicians\nduring patient visits. It models information search dynamics using a Markov\nmodel. It also leverages the key idea of collaborative filtering, originating\nfrom Recommender Systems, to prioritize information based on various\nsimilarities among physicians, patients and information items. We tested this\nnew method using real electronic health record data from the Indiana Network\nfor Patient Care. Our experimental results demonstrated that for 46.7% of\ntesting cases, this new method is able to correctly prioritize relevant\ninformation among top-5 recommendations that physicians are truly interested\nin.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:46:33 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Fan", "Ziwei", ""], ["Burgun", "Evan", ""], ["Ren", "Zhiyun", ""], ["Schleyer", "Titus", ""], ["Ning", "Xia", ""]]}, {"id": "2008.05587", "submitter": "Corentin Lonjarret", "authors": "Corentin Lonjarret, Roch Auburtin, C\\'eline Robardet and Marc\n  Plantevit", "title": "Sequential recommendation with metric models based on frequent sequences", "comments": "25 pages, 6 figures, submitted to DAMI (under review)", "journal-ref": "Data Min Knowl Disc (2021)", "doi": "10.1007/s10618-021-00744-w", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling user preferences (long-term history) and user dynamics (short-term\nhistory) is of greatest importance to build efficient sequential recommender\nsystems. The challenge lies in the successful combination of the whole user's\nhistory and his recent actions (sequential dynamics) to provide personalized\nrecommendations. Existing methods capture the sequential dynamics of a user\nusing fixed-order Markov chains (usually first order chains) regardless of the\nuser, which limits both the impact of the past of the user on the\nrecommendation and the ability to adapt its length to the user profile. In this\narticle, we propose to use frequent sequences to identify the most relevant\npart of the user history for the recommendation. The most salient items are\nthen used in a unified metric model that embeds items based on user preferences\nand sequential dynamics. Extensive experiments demonstrate that our method\noutperforms state-of-the-art, especially on sparse datasets. We show that\nconsidering sequences of varying lengths improves the recommendations and we\nalso emphasize that these sequences provide explanations on the recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 22:08:04 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lonjarret", "Corentin", ""], ["Auburtin", "Roch", ""], ["Robardet", "C\u00e9line", ""], ["Plantevit", "Marc", ""]]}, {"id": "2008.05646", "submitter": "Sudipta Paul Ms.", "authors": "Sudipta Paul and Subhankar Mishra", "title": "LAC : LSTM AUTOENCODER with Community for Insider Threat Detection", "comments": "10 pages, 8 figures, 5 tables, Accepted to the 3rd ICIST 2020, Tokyo,\n  Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The employees of any organization, institute, or industry, spend a\nsignificant amount of time on a computer network, where they develop their own\nroutine of activities in the form of network transactions over a time period.\nInsider threat detection involves identifying deviations in the routines or\nanomalies which may cause harm to the organization in the form of data leaks\nand secrets sharing. If not automated, this process involves feature\nengineering for modeling human behavior which is a tedious and time-consuming\ntask. Anomalies in human behavior are forwarded to a human analyst for final\nthreat classification. We developed an unsupervised deep neural network model\nusing LSTM AUTOENCODER which learns to mimic the behavior of individual\nemployees from their day-wise time-stamped sequence of activities. It predicts\nthe threat scenario via significant loss from anomalous routine. Employees in a\ncommunity tend to align their routine with each other rather than the employees\noutside their communities, this motivates us to explore a variation of the\nAUTOENCODER, LSTM AUTOENCODER- trained on the interleaved sequences of\nactivities in the Community (LAC). We evaluate the model on the CERT v6.2\ndataset and perform analysis on the loss for normal and anomalous routine\nacross 4000 employees. The aim of our paper is to detect the anomalous\nemployees as well as to explore how the surrounding employees are affecting\nthat employees' routine over time.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:08:39 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Paul", "Sudipta", ""], ["Mishra", "Subhankar", ""]]}, {"id": "2008.05673", "submitter": "Yufei Feng", "authors": "Yufei Feng, Fuyu Lv, Binbin Hu, Fei Sun, Kun Kuang, Yang Liu, Qingwen\n  Liu, Wenwu Ou", "title": "MTBRN: Multiplex Target-Behavior Relation Enhanced Network for\n  Click-Through Rate Prediction", "comments": "Accepted by CIKM2020", "journal-ref": null, "doi": "10.1145/3340531.3412729", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a critical task for many industrial\nsystems, such as display advertising and recommender systems. Recently,\nmodeling user behavior sequences attracts much attention and shows great\nimprovements in the CTR field. Existing works mainly exploit attention\nmechanism based on embedding product when considering relations between user\nbehaviors and target item. However, this methodology lacks of concrete\nsemantics and overlooks the underlying reasons driving a user to click on a\ntarget item. In this paper, we propose a new framework named Multiplex\nTarget-Behavior Relation enhanced Network (MTBRN) to leverage multiplex\nrelations between user behaviors and target item to enhance CTR prediction.\nMultiplex relations consist of meaningful semantics, which can bring a better\nunderstanding on users' interests from different perspectives. To explore and\nmodel multiplex relations, we propose to incorporate various graphs (e.g.,\nknowledge graph and item-item similarity graph) to construct multiple\nrelational paths between user behaviors and target item. Then Bi-LSTM is\napplied to encode each path in the path extractor layer. A path fusion network\nand a path activation network are devised to adaptively aggregate and finally\nlearn the representation of all paths for CTR prediction. Extensive offline and\nonline experiments clearly verify the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 03:48:26 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 07:26:02 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 07:56:53 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Feng", "Yufei", ""], ["Lv", "Fuyu", ""], ["Hu", "Binbin", ""], ["Sun", "Fei", ""], ["Kuang", "Kun", ""], ["Liu", "Yang", ""], ["Liu", "Qingwen", ""], ["Ou", "Wenwu", ""]]}, {"id": "2008.05701", "submitter": "Damiano Spina", "authors": "Kevin Roitero, Michael Soprano, Beatrice Portelli, Damiano Spina,\n  Vincenzo Della Mea, Giuseppe Serra, Stefano Mizzaro and Gianluca Demartini", "title": "The COVID-19 Infodemic: Can the Crowd Judge Recent Misinformation\n  Objectively?", "comments": "10 pages; Preprint of the full paper accepted at CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412048", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misinformation is an ever increasing problem that is difficult to solve for\nthe research community and has a negative impact on the society at large. Very\nrecently, the problem has been addressed with a crowdsourcing-based approach to\nscale up labeling efforts: to assess the truthfulness of a statement, instead\nof relying on a few experts, a crowd of (non-expert) judges is exploited. We\nfollow the same approach to study whether crowdsourcing is an effective and\nreliable method to assess statements truthfulness during a pandemic. We\nspecifically target statements related to the COVID-19 health emergency, that\nis still ongoing at the time of the study and has arguably caused an increase\nof the amount of misinformation that is spreading online (a phenomenon for\nwhich the term \"infodemic\" has been used). By doing so, we are able to address\n(mis)information that is both related to a sensitive and personal issue like\nhealth and very recent as compared to when the judgment is done: two issues\nthat have not been analyzed in related work. In our experiment, crowd workers\nare asked to assess the truthfulness of statements, as well as to provide\nevidence for the assessments as a URL and a text justification. Besides showing\nthat the crowd is able to accurately judge the truthfulness of the statements,\nwe also report results on many different aspects, including: agreement among\nworkers, the effect of different aggregation functions, of scales\ntransformations, and of workers background / bias. We also analyze workers\nbehavior, in terms of queries submitted, URLs found / selected, text\njustifications, and other behavioral data like clicks and mouse actions\ncollected by means of an ad hoc logger.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:53:24 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Roitero", "Kevin", ""], ["Soprano", "Michael", ""], ["Portelli", "Beatrice", ""], ["Spina", "Damiano", ""], ["Della Mea", "Vincenzo", ""], ["Serra", "Giuseppe", ""], ["Mizzaro", "Stefano", ""], ["Demartini", "Gianluca", ""]]}, {"id": "2008.06079", "submitter": "Francielle Alves Vargas", "authors": "Francielle Alves Vargas and Thiago Alexandre Salgueiro Pardo", "title": "Studying Dishonest Intentions in Brazilian Portuguese Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work in the social sciences, psychology and linguistics has show\nthat liars have some control over the content of their stories, however their\nunderlying state of mind may \"leak out\" through the way that they tell them. To\nthe best of our knowledge, no previous systematic effort exists in order to\ndescribe and model deception language for Brazilian Portuguese. To fill this\nimportant gap, we carry out an initial empirical linguistic study on false\nstatements in Brazilian news. We methodically analyze linguistic features using\na deceptive news corpus, which includes both fake and true news. The results\nshow that they present substantial lexical, syntactic and semantic variations,\nas well as punctuation and emotion distinctions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 18:44:52 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 21:31:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Vargas", "Francielle Alves", ""], ["Pardo", "Thiago Alexandre Salgueiro", ""]]}, {"id": "2008.06176", "submitter": "Ye Bi", "authors": "Ye Bi, Shuo Wang, Zhongrui Fan", "title": "A Hybrid BERT and LightGBM based Model for Predicting Emotion GIF\n  Categories on Twitter", "comments": "4 pages, ACL 2020 EmotionGIF Challenge Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The animated Graphical Interchange Format (GIF) images have been widely used\non social media as an intuitive way of expression emotion. Given their\nexpressiveness, GIFs offer a more nuanced and precise way to convey emotions.\nIn this paper, we present our solution for the EmotionGIF 2020 challenge, the\nshared task of SocialNLP 2020. To recommend GIF categories for unlabeled\ntweets, we regarded this problem as a kind of matching tasks and proposed a\nlearning to rank framework based on Bidirectional Encoder Representations from\nTransformer (BERT) and LightGBM. Our team won the 4th place with a Mean Average\nPrecision @ 6 (MAP@6) score of 0.5394 on the round 1 leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:23:09 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Bi", "Ye", ""], ["Wang", "Shuo", ""], ["Fan", "Zhongrui", ""]]}, {"id": "2008.06179", "submitter": "Ye Bi", "authors": "Ye Bi, Shuo Wang, Zhongrui Fan", "title": "A Multimodal Late Fusion Model for E-Commerce Product Classification", "comments": "4 pages, SIGIR 2020 E-commerce Workshop Data Challenge Technical\n  Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cataloging of product listings is a fundamental problem for most\ne-commerce platforms. Despite promising results obtained by unimodal-based\nmethods, it can be expected that their performance can be further boosted by\nthe consideration of multimodal product information. In this study, we\ninvestigated a multimodal late fusion approach based on text and image\nmodalities to categorize e-commerce products on Rakuten. Specifically, we\ndeveloped modal specific state-of-the-art deep neural networks for each input\nmodal, and then fused them at the decision level. Experimental results on\nMultimodal Product Classification Task of SIGIR 2020 E-Commerce Workshop Data\nChallenge demonstrate the superiority and effectiveness of our proposed method\ncompared with unimodal and other multimodal methods. Our team named pa_curis\nwon the 1st place with a macro-F1 of 0.9144 on the final leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:46:24 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Bi", "Ye", ""], ["Wang", "Shuo", ""], ["Fan", "Zhongrui", ""]]}, {"id": "2008.06310", "submitter": "Feng Xia", "authors": "Nana Yaw Asabere, Feng Xia, Wei Wang, Joel J.P.C. Rodrigues, Filippo\n  Basso, and Jianhua Ma", "title": "Improving Smart Conference Participation through Socially-Aware\n  Recommendation", "comments": "12 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1312.6808", "journal-ref": "IEEE Transactions on Human-Machine Systems, 44(5): 689-700, 2014", "doi": "10.1109/THMS.2014.2325837", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research addresses recommending presentation sessions at smart\nconferences to participants. We propose a venue recommendation algorithm,\nSocially-Aware Recommendation of Venues and Environments (SARVE). SARVE\ncomputes correlation and social characteristic information of conference\nparticipants. In order to model a recommendation process using distributed\ncommunity detection, SARVE further integrates the current context of both the\nsmart conference community and participants. SARVE recommends presentation\nsessions that may be of high interest to each participant. We evaluate SARVE\nusing a real world dataset. In our experiments, we compare SARVE to two related\nstate-of-the-art methods, namely: Context-Aware Mobile Recommendation Services\n(CAMRS) and Conference Navigator (Recommender) Model. Our experimental results\nshow that in terms of the utilized evaluation metrics: precision, recall, and\nf-measure, SARVE achieves more reliable and favorable social (relations and\ncontext) recommendation results.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 03:56:35 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Asabere", "Nana Yaw", ""], ["Xia", "Feng", ""], ["Wang", "Wei", ""], ["Rodrigues", "Joel J. P. C.", ""], ["Basso", "Filippo", ""], ["Ma", "Jianhua", ""]]}, {"id": "2008.06414", "submitter": "Lihong He", "authors": "Lihong He, Chen Shen, Arjun Mukherjee, Slobodan Vucetic, Eduard Dragut", "title": "Cannot Predict Comment Volume of a News Article before (a few) Users\n  Read It", "comments": "12 pages; to be published at ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many news outlets allow users to contribute comments on topics about daily\nworld events. News articles are the seeds that spring users' interest to\ncontribute content, i.e., comments. An article may attract an apathetic user\nengagement (several tens of comments) or a spontaneous fervent user engagement\n(thousands of comments). In this paper, we study the problem of predicting the\ntotal number of user comments a news article will receive. Our main insight is\nthat the early dynamics of user comments contribute the most to an accurate\nprediction, while news article specific factors have surprisingly little\ninfluence. This appears to be an interesting and understudied phenomenon:\ncollective social behavior at a news outlet shapes user response and may even\ndownplay the content of an article. We compile and analyze a large number of\nfeatures, both old and novel from literature. The features span a broad\nspectrum of facets including news article and comment contents, temporal\ndynamics, sentiment/linguistic features, and user behaviors. We show that the\nearly arrival rate of comments is the best indicator of the eventual number of\ncomments. We conduct an in-depth analysis of this feature across several\ndimensions, such as news outlets and news article categories. We show that the\nrelationship between the early rate and the final number of comments as well as\nthe prediction accuracy vary considerably across news outlets and news article\ncategories (e.g., politics, sports, or health).\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:21:11 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 20:44:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["He", "Lihong", ""], ["Shen", "Chen", ""], ["Mukherjee", "Arjun", ""], ["Vucetic", "Slobodan", ""], ["Dragut", "Eduard", ""]]}, {"id": "2008.06460", "submitter": "Marzieh Mozafari", "authors": "Marzieh Mozafari, Reza Farahbakhsh, Noel Crespi", "title": "Hate Speech Detection and Racial Bias Mitigation in Social Media based\n  on BERT model", "comments": "This paper has been accepted in the PLOS ONE journal in August 2020", "journal-ref": null, "doi": "10.1371/journal.pone.0237861", "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparate biases associated with datasets and trained classifiers in hateful\nand abusive content identification tasks have raised many concerns recently.\nAlthough the problem of biased datasets on abusive language detection has been\naddressed more frequently, biases arising from trained classifiers have not yet\nbeen a matter of concern. Here, we first introduce a transfer learning approach\nfor hate speech detection based on an existing pre-trained language model\ncalled BERT and evaluate the proposed model on two publicly available datasets\nannotated for racism, sexism, hate or offensive content on Twitter. Next, we\nintroduce a bias alleviation mechanism in hate speech detection task to\nmitigate the effect of bias in training set during the fine-tuning of our\npre-trained BERT-based model. Toward that end, we use an existing\nregularization method to reweight input samples, thereby decreasing the effects\nof high correlated training set' s n-grams with class labels, and then\nfine-tune our pre-trained BERT-based model with the new re-weighted samples. To\nevaluate our bias alleviation mechanism, we employ a cross-domain approach in\nwhich we use the trained classifiers on the aforementioned datasets to predict\nthe labels of two new datasets from Twitter, AAE-aligned and White-aligned\ngroups, which indicate tweets written in African-American English (AAE) and\nStandard American English (SAE) respectively. The results show the existence of\nsystematic racial bias in trained classifiers as they tend to assign tweets\nwritten in AAE from AAE-aligned group to negative classes such as racism,\nsexism, hate, and offensive more often than tweets written in SAE from\nWhite-aligned. However, the racial bias in our classifiers reduces\nsignificantly after our bias alleviation mechanism is incorporated. This work\ncould institute the first step towards debiasing hate speech and abusive\nlanguage detection systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:47:25 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 10:06:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mozafari", "Marzieh", ""], ["Farahbakhsh", "Reza", ""], ["Crespi", "Noel", ""]]}, {"id": "2008.06487", "submitter": "Craig Macdonald", "authors": "Xi Wang and Iadh Ounis and Craig Macdonald", "title": "Negative Confidence-Aware Weakly Supervised Binary Classification for\n  Effective Review Helpfulness Classification", "comments": "CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3411978", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incompleteness of positive labels and the presence of many unlabelled\ninstances are common problems in binary classification applications such as in\nreview helpfulness classification. Various studies from the classification\nliterature consider all unlabelled instances as negative examples. However, a\nclassification model that learns to classify binary instances with incomplete\npositive labels while assuming all unlabelled data to be negative examples will\noften generate a biased classifier. In this work, we propose a novel Negative\nConfidence-aware Weakly Supervised approach (NCWS), which customises a binary\nclassification loss function by discriminating the unlabelled examples with\ndifferent negative confidences during the classifier's training. We use the\nreview helpfulness classification as a test case for examining the\neffectiveness of our NCWS approach. We thoroughly evaluate NCWS by using three\ndifferent datasets, namely one from Yelp (venue reviews), and two from Amazon\n(Kindle and Electronics reviews). Our results show that NCWS outperforms strong\nbaselines from the literature including an existing SVM-based approach (i.e.\nSVM-P), the positive and unlabelled learning-based approach (i.e. C-PU) and the\npositive confidence-based approach (i.e. P-conf) in addressing the classifier's\nbias problem. Moreover, we further examine the effectiveness of NCWS by using\nits classified helpful reviews in a state-of-the-art review-based venue\nrecommendation model (i.e. DeepCoNN) and demonstrate the benefits of using NCWS\nin enhancing venue recommendation effectiveness in comparison to the baselines.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:45:46 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Wang", "Xi", ""], ["Ounis", "Iadh", ""], ["Macdonald", "Craig", ""]]}, {"id": "2008.06716", "submitter": "Evgeny Frolov", "authors": "Leyla Mirvakhabova, Evgeny Frolov, Valentin Khrulkov, Ivan Oseledets,\n  Alexander Tuzhilin", "title": "Performance of Hyperbolic Geometry Models on Top-N Recommendation Tasks", "comments": "Accepted at ACM RecSys 2020; 7 pages", "journal-ref": null, "doi": "10.1145/3383313.3412219", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple autoencoder based on hyperbolic geometry for solving\nstandard collaborative filtering problem. In contrast to many modern deep\nlearning techniques, we build our solution using only a single hidden layer.\nRemarkably, even with such a minimalistic approach, we not only outperform the\nEuclidean counterpart but also achieve a competitive performance with respect\nto the current state-of-the-art. We additionally explore the effects of space\ncurvature on the quality of hyperbolic models and propose an efficient\ndata-driven method for estimating its optimal value.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 13:21:10 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Mirvakhabova", "Leyla", ""], ["Frolov", "Evgeny", ""], ["Khrulkov", "Valentin", ""], ["Oseledets", "Ivan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2008.06759", "submitter": "Xiaowei Liu", "authors": "Xiaowei Liu, Weiwei Guo, Huiji Gao, Bo Long", "title": "Deep Search Query Intent Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a user's query intent behind a search is critical for modern\nsearch engine success. Accurate query intent prediction allows the search\nengine to better serve the user's need by rendering results from more relevant\ncategories. This paper aims to provide a comprehensive learning framework for\nmodeling query intent under different stages of a search. We focus on the\ndesign for 1) predicting users' intents as they type in queries on-the-fly in\ntypeahead search using character-level models; and 2) accurate word-level\nintent prediction models for complete queries. Various deep learning components\nfor query text understanding are experimented. Offline evaluation and online\nA/B test experiments show that the proposed methods are effective in\nunderstanding query intent and efficient to scale for online search systems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 18:19:56 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 04:59:27 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Liu", "Xiaowei", ""], ["Guo", "Weiwei", ""], ["Gao", "Huiji", ""], ["Long", "Bo", ""]]}, {"id": "2008.06787", "submitter": "Arman Dehpanah", "authors": "Arman Dehpanah, Muheeb Faizan Ghori, Jonathan Gemmell, Bamshad\n  Mobasher", "title": "The Evaluation of Rating Systems in Online Free-for-All Games", "comments": "10 pages, 1 figure, accepted and presented in 16th International\n  Conference on Data Science (ICDATA'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online competitive games have become increasingly popular. To ensure an\nexciting and competitive environment, these games routinely attempt to match\nplayers with similar skill levels. Matching players is often accomplished\nthrough a rating system. There has been an increasing amount of research on\ndeveloping such rating systems. However, less attention has been given to the\nevaluation metrics of these systems. In this paper, we present an exhaustive\nanalysis of six metrics for evaluating rating systems in online competitive\ngames. We compare traditional metrics such as accuracy. We then introduce other\nmetrics adapted from the field of information retrieval. We evaluate these\nmetrics against several well-known rating systems on a large real-world dataset\nof over 100,000 free-for-all matches. Our results show stark differences in\ntheir utility. Some metrics do not consider deviations between two ranks.\nOthers are inordinately impacted by new players. Many do not capture the\nimportance of distinguishing between errors in higher ranks and lower ranks.\nAmong all metrics studied, we recommend Normalized Discounted Cumulative Gain\n(NDCG) because not only does it resolve the issues faced by other metrics, but\nit also offers flexibility to adjust the evaluations based on the goals of the\nsystem\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 21:00:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Dehpanah", "Arman", ""], ["Ghori", "Muheeb Faizan", ""], ["Gemmell", "Jonathan", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2008.06877", "submitter": "Meysam Asgari-Chenaghlu", "authors": "Meysam Asgari-Chenaghlu, Mohammad-Reza Feizi-Derakhshi, Leili\n  farzinvash, Mohammad-Ali Balafar, Cina Motamed", "title": "TopicBERT: A Transformer transfer learning based memory-graph approach\n  for multimodal streaming social media topic detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time nature of social networks with bursty short messages and their\nrespective large data scale spread among vast variety of topics are research\ninterest of many researchers. These properties of social networks which are\nknown as 5'Vs of big data has led to many unique and enlightenment algorithms\nand techniques applied to large social networking datasets and data streams.\nMany of these researches are based on detection and tracking of hot topics and\ntrending social media events that help revealing many unanswered questions.\nThese algorithms and in some cases software products mostly rely on the nature\nof the language itself. Although, other techniques such as unsupervised data\nmining methods are language independent but many requirements for a\ncomprehensive solution are not met. Many research issues such as noisy\nsentences that adverse grammar and new online user invented words are\nchallenging maintenance of a good social network topic detection and tracking\nmethodology; The semantic relationship between words and in most cases,\nsynonyms are also ignored by many of these researches. In this research, we use\nTransformers combined with an incremental community detection algorithm.\nTransformer in one hand, provides the semantic relation between words in\ndifferent contexts. On the other hand, the proposed graph mining technique\nenhances the resulting topics with aid of simple structural rules. Named entity\nrecognition from multimodal data, image and text, labels the named entities\nwith entity type and the extracted topics are tuned using them. All operations\nof proposed system has been applied with big social data perspective under\nNoSQL technologies. In order to present a working and systematic solution, we\ncombined MongoDB with Neo4j as two major database systems of our work. The\nproposed system shows higher precision and recall compared to other methods in\nthree different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:39:50 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Asgari-Chenaghlu", "Meysam", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""], ["farzinvash", "Leili", ""], ["Balafar", "Mohammad-Ali", ""], ["Motamed", "Cina", ""]]}, {"id": "2008.06908", "submitter": "Parth Tiwari", "authors": "Parth Tiwari, Yash Jain, Shivansh Mundra, Jenny Harding, Manoj Kumar\n  Tiwari", "title": "Visually Aware Skip-Gram for Image Based Recommendations", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The visual appearance of a product significantly influences purchase\ndecisions on e-commerce websites. We propose a novel framework VASG (Visually\nAware Skip-Gram) for learning user and product representations in a common\nlatent space using product image features. Our model is an amalgamation of the\nSkip-Gram architecture and a deep neural network based Decoder. Here the\nSkip-Gram attempts to capture user preference by optimizing user-product\nco-occurrence in a Heterogeneous Information Network while the Decoder\nsimultaneously learns a mapping to transform product image features to the\nSkip-Gram embedding space. This architecture is jointly optimized in an\nend-to-end, multitask fashion. The proposed framework enables us to make\npersonalized recommendations for cold-start products which have no purchase\nhistory. Experiments conducted on large real-world datasets show that the\nlearned embeddings can generate effective recommendations using nearest\nneighbour searches.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:16:29 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Tiwari", "Parth", ""], ["Jain", "Yash", ""], ["Mundra", "Shivansh", ""], ["Harding", "Jenny", ""], ["Tiwari", "Manoj Kumar", ""]]}, {"id": "2008.06948", "submitter": "Leon Moonen", "authors": "Carl Martin Rosenberg and Leon Moonen", "title": "Spectrum-Based Log Diagnosis", "comments": "Published in ESEM'20: ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM), October 8-9, 2020, Bari, Italy.\n  ACM, 12 pages", "journal-ref": null, "doi": "10.1145/3382494.3410684", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present and evaluate Spectrum-Based Log Diagnosis (SBLD), a method to help\ndevelopers quickly diagnose problems found in complex integration and\ndeployment runs. Inspired by Spectrum-Based Fault Localization, SBLD leverages\nthe differences in event occurrences between logs for failing and passing runs,\nto highlight events that are stronger associated with failing runs.\n  Using data provided by our industrial partner, we empirically investigate the\nfollowing questions: (i) How well does SBLD reduce the effort needed to\nidentify all failure-relevant events in the log for a failing run? (ii) How is\nthe performance of SBLD affected by available data? (iii) How does SBLD compare\nto searching for simple textual patterns that often occur in failure-relevant\nevents? We answer (i) and (ii) using summary statistics and heatmap\nvisualizations, and for (iii) we compare three configurations of SBLD (with\nresp. minimum, median and maximum data) against a textual search using Wilcoxon\nsigned-rank tests and the Vargha-Delaney measure of stochastic superiority.\n  Our evaluation shows that (i) SBLD achieves a significant effort reduction\nfor the dataset used, (ii) SBLD benefits from additional logs for passing runs\nin general, and it benefits from additional logs for failing runs when there is\na proportional amount of logs for passing runs in the data. Finally, (iii) SBLD\nand textual search are roughly equally effective at effort-reduction, while\ntextual search has a slightly better recall. We investigate the cause, and\ndiscuss how it is due to the characteristics of a specific part of our data.\n  We conclude that SBLD shows promise as a method for diagnosing failing runs,\nthat its performance is positively affected by additional data, but that it\ndoes not outperform textual search on the dataset considered. Future work\nincludes investigating SBLD's generalizability on additional datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 16:03:22 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Rosenberg", "Carl Martin", ""], ["Moonen", "Leon", ""]]}, {"id": "2008.06957", "submitter": "Suchithra Rajendran", "authors": "Suchithra Rajendran, John Fennewald", "title": "Improving Services Offered by Internet Providers by Analyzing Online\n  Reviews using Text Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of digital infrastructure, there is a plethora of\ndemand for internet services, which makes the wireless communications industry\nhighly competitive. Thus internet service providers (ISPs) must ensure that\ntheir efforts are targeted towards attracting and retaining customers to ensure\ncontinued growth. As Web 2.0 has gained traction and more tools have become\navailable, customers in recent times are equipped to make well-informed\ndecisions, specifically due to the colossal information available in online\nreviews. ISPs can use this information to better understand the views of the\ncustomers about their products and services. The goal of this paper is to\nidentify the current strengths, weaknesses, opportunities, and threats (SWOT)\nof each ISP by exploring consumer reviews using text analytics. The proposed\napproach consists of four different stages: bigram and trigram analyses, topic\nidentification, SWOT analysis and Root Cause Analysis (RCA). For each ISP, we\nfirst categorize online reviews into positive and negative based on customer\nratings and then leverage text analytic tools to determine the most frequently\nused and co-occurring words in each categorization of reviews. Subsequently,\nlooking at the positive and negative topics in each ISP, we conduct the SWOT\nanalysis as well as the RCA to help companies identify the internal and\nexternal factors impacting customer satisfaction. We use a case study to\nillustrate the proposed approach. The proposed managerial insights that are\nderived from the results can act as a decision support tool for ISPs to offer\nbetter products and services for their customers.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 16:44:55 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rajendran", "Suchithra", ""], ["Fennewald", "John", ""]]}, {"id": "2008.06974", "submitter": "Mona Jalal", "authors": "Alyssa Smith, David Assefa Tofu, Mona Jalal, Edward Edberg Halim,\n  Yimeng Sun, Vidya Akavoor, Margrit Betke, Prakash Ishwar, Lei Guo, Derry\n  Wijaya", "title": "OpenFraming: We brought the ML; you bring the data. Interact with your\n  data and discover its frames", "comments": "8 pages, 8 figures, EMNLP 2020 demonstration papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When journalists cover a news story, they can cover the story from multiple\nangles or perspectives. A news article written about COVID-19 for example,\nmight focus on personal preventative actions such as mask-wearing, while\nanother might focus on COVID-19's impact on the economy. These perspectives are\ncalled \"frames,\" which when used may influence public perception and opinion of\nthe issue. We introduce a Web-based system for analyzing and classifying frames\nin text documents. Our goal is to make effective tools for automatic frame\ndiscovery and labeling based on topic modeling and deep learning widely\naccessible to researchers from a diverse array of disciplines. To this end, we\nprovide both state-of-the-art pre-trained frame classification models on\nvarious issues as well as a user-friendly pipeline for training novel\nclassification models on user-provided corpora. Researchers can submit their\ndocuments and obtain frames of the documents. The degree of user involvement is\nflexible: they can run models that have been pre-trained on select issues;\nsubmit labeled documents and train a new model for frame classification; or\nsubmit unlabeled documents and obtain potential frames of the documents. The\ncode making up our system is also open-sourced and well-documented, making the\nsystem transparent and expandable. The system is available on-line at\nhttp://www.openframing.org and via our GitHub page\nhttps://github.com/davidatbu/openFraming .\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 18:59:30 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Smith", "Alyssa", ""], ["Tofu", "David Assefa", ""], ["Jalal", "Mona", ""], ["Halim", "Edward Edberg", ""], ["Sun", "Yimeng", ""], ["Akavoor", "Vidya", ""], ["Betke", "Margrit", ""], ["Ishwar", "Prakash", ""], ["Guo", "Lei", ""], ["Wijaya", "Derry", ""]]}, {"id": "2008.07045", "submitter": "Jina Suh", "authors": "Jina Suh, Eric Horvitz, Ryen W. White, Tim Althoff", "title": "Population-Scale Study of Human Needs During the COVID-19 Pandemic:\n  Analysis and Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work to date on mitigating the COVID-19 pandemic is focused urgently on\nbiomedicine and epidemiology. Yet, pandemic-related policy decisions cannot be\nmade on health information alone. Decisions need to consider the broader\nimpacts on people and their needs. Quantifying human needs across the\npopulation is challenging as it requires high geo-temporal granularity, high\ncoverage across the population, and appropriate adjustment for seasonal and\nother external effects. Here, we propose a computational methodology, building\non Maslow's hierarchy of needs, that can capture a holistic view of relative\nchanges in needs following the pandemic through a difference-in-differences\napproach that corrects for seasonality and volume variations. We apply this\napproach to characterize changes in human needs across physiological,\nsocioeconomic, and psychological realms in the US, based on more than 35\nbillion search interactions spanning over 36,000 ZIP codes over a period of 14\nmonths. The analyses reveal that the expression of basic human needs has\nincreased exponentially while higher-level aspirations declined during the\npandemic in comparison to the pre-pandemic period. In exploring the timing and\nvariations in statewide policies, we find that the durations of\nshelter-in-place mandates have influenced social and emotional needs\nsignificantly. We demonstrate that potential barriers to addressing critical\nneeds, such as support for unemployment and domestic violence, can be\nidentified through web search interactions. Our approach and results suggest\nthat population-scale monitoring of shifts in human needs can inform policies\nand recovery efforts for current and anticipated needs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 01:21:58 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:00:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Suh", "Jina", ""], ["Horvitz", "Eric", ""], ["White", "Ryen W.", ""], ["Althoff", "Tim", ""]]}, {"id": "2008.07142", "submitter": "Ziyu Wang", "authors": "Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai,\n  Xianbin Gu, Gus Xia", "title": "POP909: A Pop-song Dataset for Music Arrangement Generation", "comments": null, "journal-ref": "In Proceedings of 21st International Conference on Music\n  Information Retrieval (ISMIR), Montreal, Canada (virtual conference), 2020", "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music arrangement generation is a subtask of automatic music generation,\nwhich involves reconstructing and re-conceptualizing a piece with new\ncompositional techniques. Such a generation process inevitably requires\nreference from the original melody, chord progression, or other structural\ninformation. Despite some promising models for arrangement, they lack more\nrefined data to achieve better evaluations and more practical results. In this\npaper, we propose POP909, a dataset which contains multiple versions of the\npiano arrangements of 909 popular songs created by professional musicians. The\nmain body of the dataset contains the vocal melody, the lead instrument melody,\nand the piano accompaniment for each song in MIDI format, which are aligned to\nthe original audio files. Furthermore, we provide the annotations of tempo,\nbeat, key, and chords, where the tempo curves are hand-labeled and others are\ndone by MIR algorithms. Finally, we conduct several baseline experiments with\nthis dataset using standard deep music generation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 08:08:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Ziyu", ""], ["Chen", "Ke", ""], ["Jiang", "Junyan", ""], ["Zhang", "Yiyi", ""], ["Xu", "Maoran", ""], ["Dai", "Shuqi", ""], ["Gu", "Xianbin", ""], ["Xia", "Gus", ""]]}, {"id": "2008.07178", "submitter": "Zeyu Cui", "authors": "Zeyu Cui, Feng Yu, Shu Wu, Qiang Liu, Liang Wang", "title": "Disentangled Item Representation for Recommender Systems", "comments": "accepted by TIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item representations in recommendation systems are expected to reveal the\nproperties of items. Collaborative recommender methods usually represent an\nitem as one single latent vector. Nowadays the e-commercial platforms provide\nvarious kinds of attribute information for items (e.g., category, price and\nstyle of clothing). Utilizing these attribute information for better item\nrepresentations is popular in recent years. Some studies use the given\nattribute information as side information, which is concatenated with the item\nlatent vector to augment representations. However, the mixed item\nrepresentations fail to fully exploit the rich attribute information or provide\nexplanation in recommender systems. To this end, we propose a fine-grained\nDisentangled Item Representation (DIR) for recommender systems in this paper,\nwhere the items are represented as several separated attribute vectors instead\nof a single latent vector. In this way, the items are represented at the\nattribute level, which can provide fine-grained information of items in\nrecommendation. We introduce a learning strategy, LearnDIR, which can allocate\nthe corresponding attribute vectors to items. We show how DIR can be applied to\ntwo typical models, Matrix Factorization (MF) and Recurrent Neural Network\n(RNN). Experimental results on two real-world datasets show that the models\ndeveloped under the framework of DIR are effective and efficient. Even using\nfewer parameters, the proposed model can outperform the state-of-the-art\nmethods, especially in the cold-start situation. In addition, we make\nvisualizations to show that our proposition can provide explanation for users\nin real-world applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:39:10 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 03:52:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Zeyu", ""], ["Yu", "Feng", ""], ["Wu", "Shu", ""], ["Liu", "Qiang", ""], ["Wang", "Liang", ""]]}, {"id": "2008.07226", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dietmar Jannach and Xavier Serra", "title": "Exploring Longitudinal Effects of Session-based Recommendations", "comments": "The 14th ACM Conference on Recommender Systems", "journal-ref": null, "doi": "10.1145/3383313.3412213", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Session-based recommendation is a problem setting where the task of a\nrecommender system is to make suitable item suggestions based only on a few\nobserved user interactions in an ongoing session. The lack of long-term\npreference information about individual users in such settings usually results\nin a limited level of personalization, where a small set of popular items may\nbe recommended to many users. This repeated exposure of such a subset of the\nitems through the recommendations may in turn lead to a reinforcement effect\nover time, and to a system which is not able to help users discover new content\nanymore to the desirable extent.\n  In this work, we investigate such potential longitudinal effects of\nsession-based recommendations in a simulation-based approach. Specifically, we\nanalyze to what extent algorithms of different types may lead to concentration\neffects over time. Our experiments in the music domain reveal that all\ninvestigated algorithms---both neural and heuristic ones---may lead to lower\nitem coverage and to a higher concentration on a subset of the items.\nAdditional simulation experiments however also indicate that relatively simple\nre-ranking strategies, e.g., by avoiding too many repeated recommendations in\nthe music domain, may help to deal with this problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 11:24:08 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ferraro", "Andres", ""], ["Jannach", "Dietmar", ""], ["Serra", "Xavier", ""]]}, {"id": "2008.07467", "submitter": "Shaunak Mishra", "authors": "Shaunak Mishra, Manisha Verma, Yichao Zhou, Kapil Thadani, Wei Wang", "title": "Learning to Create Better Ads: Generation and Ranking Approaches for Ad\n  Creative Refinement", "comments": "9 pages, accepted for publication in CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the online advertising industry, the process of designing an ad creative\n(i.e., ad text and image) requires manual labor. Typically, each advertiser\nlaunches multiple creatives via online A/B tests to infer effective creatives\nfor the target audience, that are then refined further in an iterative fashion.\nDue to the manual nature of this process, it is time-consuming to learn,\nrefine, and deploy the modified creatives. Since major ad platforms typically\nrun A/B tests for multiple advertisers in parallel, we explore the possibility\nof collaboratively learning ad creative refinement via A/B tests of multiple\nadvertisers. In particular, given an input ad creative, we study approaches to\nrefine the given ad text and image by: (i) generating new ad text, (ii)\nrecommending keyphrases for new ad text, and (iii) recommending image tags\n(objects in image) to select new ad image. Based on A/B tests conducted by\nmultiple advertisers, we form pairwise examples of inferior and superior ad\ncreatives, and use such pairs to train models for the above tasks. For\ngenerating new ad text, we demonstrate the efficacy of an encoder-decoder\narchitecture with copy mechanism, which allows some words from the (inferior)\ninput text to be copied to the output while incorporating new words associated\nwith higher click-through-rate. For the keyphrase and image tag recommendation\ntask, we demonstrate the efficacy of a deep relevance matching model, as well\nas the relative robustness of ranking approaches compared to ad text generation\nin cold-start scenarios with unseen advertisers. We also share broadly\napplicable insights from our experiments using data from the Yahoo Gemini ad\nplatform.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 16:46:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 00:16:11 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Mishra", "Shaunak", ""], ["Verma", "Manisha", ""], ["Zhou", "Yichao", ""], ["Thadani", "Kapil", ""], ["Wang", "Wei", ""]]}, {"id": "2008.07577", "submitter": "Bahare Askari", "authors": "Bahare Askari, Jaroslaw Szlichta, Amirali Salehi-Abari", "title": "Joint Variational Autoencoders for Recommendation with Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders (VAEs) have recently shown promising performance in\ncollaborative filtering with implicit feedback. These existing recommendation\nmodels learn user representations to reconstruct or predict user preferences.\nWe introduce joint variational autoencoders (JoVA), an ensemble of two VAEs, in\nwhich VAEs jointly learn both user and item representations and collectively\nreconstruct and predict user preferences. This design allows JoVA to capture\nuser-user and item-item correlations simultaneously. By extending the objective\nfunction of JoVA with a hinge-based pairwise loss function (JoVA-Hinge), we\nfurther specialize it for top-k recommendation with implicit feedback. Our\nextensive experiments on several real-world datasets show that JoVA-Hinge\noutperforms a broad set of state-of-the-art collaborative filtering methods,\nunder a variety of commonly-used metrics. Our empirical results also confirm\nthe outperformance of JoVA-Hinge over existing methods for cold-start users\nwith a limited number of training data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 19:06:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Askari", "Bahare", ""], ["Szlichta", "Jaroslaw", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2008.07680", "submitter": "Denilson Barbosa", "authors": "Erin Macdonald and Denilson Barbosa", "title": "An Annotated Corpus of Webtables for Information Extraction Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Information Extraction is a well-researched area of Natural Language\nProcessing with applications in web search and question answering concerned\nwith identifying entities and relationships between them as expressed in a\ngiven context, usually a sentence of a paragraph of running text. Given the\nimportance of the task, several datasets and benchmarks have been curated over\nthe years. However, focusing on running text alone leaves out tables which are\ncommon in many structured documents and in which pairs of entities also\nco-occur in context (e.g., the same row of the table). While there are recent\npapers on relation extraction from tables in the literature, their experimental\nevaluations have been on ad-hoc datasets for the lack of a standard benchmark.\nThis paper helps close that gap. We introduce an annotation framework and a\ndataset of 217,834 tables from Wikipedia which are annotated with 28 relations,\nusing both classifiers and carefully designed queries over a reference\nknowledge graph. Binary classifiers are then applied to the resulting dataset\nto remove false positives, resulting in an average annotation accuracy of 94%.\nThe resulting dataset is the first of its kind to be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 00:30:56 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 17:24:28 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Macdonald", "Erin", ""], ["Barbosa", "Denilson", ""]]}, {"id": "2008.07688", "submitter": "Vaibhav Kumar", "authors": "Vaibhav Kumar and Vikas Raunak and Jamie Callan", "title": "Ranking Clarification Questions via Natural Language Inference", "comments": "Accepted at CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412137", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given a natural language query, teaching machines to ask clarifying questions\nis of immense utility in practical natural language processing systems. Such\ninteractions could help in filling information gaps for better machine\ncomprehension of the query. For the task of ranking clarification questions, we\nhypothesize that determining whether a clarification question pertains to a\nmissing entry in a given post (on QA forums such as StackExchange) could be\nconsidered as a special case of Natural Language Inference (NLI), where both\nthe post and the most relevant clarification question point to a shared latent\npiece of information or context. We validate this hypothesis by incorporating\nrepresentations from a Siamese BERT model fine-tuned on NLI and Multi-NLI\ndatasets into our models and demonstrate that our best performing model obtains\na relative performance improvement of 40 percent and 60 percent respectively\n(on the key metric of Precision@1), over the state-of-the-art baseline(s) on\nthe two evaluation sets of the StackExchange dataset, thereby, significantly\nsurpassing the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 01:32:29 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kumar", "Vaibhav", ""], ["Raunak", "Vikas", ""], ["Callan", "Jamie", ""]]}, {"id": "2008.07703", "submitter": "Yi Ren", "authors": "Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, Tie-Yan Liu", "title": "PopMAG: Pop Music Accompaniment Generation", "comments": "Accepted by ACM-MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pop music, accompaniments are usually played by multiple instruments\n(tracks) such as drum, bass, string and guitar, and can make a song more\nexpressive and contagious by arranging together with its melody. Previous works\nusually generate multiple tracks separately and the music notes from different\ntracks not explicitly depend on each other, which hurts the harmony modeling.\nTo improve harmony, in this paper, we propose a novel MUlti-track MIDI\nrepresentation (MuMIDI), which enables simultaneous multi-track generation in a\nsingle sequence and explicitly models the dependency of the notes from\ndifferent tracks. While this greatly improves harmony, unfortunately, it\nenlarges the sequence length and brings the new challenge of long-term music\nmodeling. We further introduce two new techniques to address this challenge: 1)\nWe model multiple note attributes (e.g., pitch, duration, velocity) of a\nmusical note in one step instead of multiple steps, which can shorten the\nlength of a MuMIDI sequence. 2) We introduce extra long-context as memory to\ncapture long-term dependency in music. We call our system for pop music\naccompaniment generation as PopMAG. We evaluate PopMAG on multiple datasets\n(LMD, FreeMidi and CPMD, a private dataset of Chinese pop songs) with both\nsubjective and objective metrics. The results demonstrate the effectiveness of\nPopMAG for multi-track harmony modeling and long-term context modeling.\nSpecifically, PopMAG wins 42\\%/38\\%/40\\% votes when comparing with ground truth\nmusical pieces on LMD, FreeMidi and CPMD datasets respectively and largely\noutperforms other state-of-the-art music accompaniment generation models and\nmulti-track MIDI representations in terms of subjective and objective metrics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:28:36 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ren", "Yi", ""], ["He", "Jinzheng", ""], ["Tan", "Xu", ""], ["Qin", "Tao", ""], ["Zhao", "Zhou", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2008.07759", "submitter": "Senci Ying", "authors": "Senci Ying", "title": "Shared MF: A privacy-preserving recommendation system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is one of the most commonly used technologies in\nrecommendation system. With the promotion of recommendation system in\ne-commerce shopping, online video and other aspects, distributed recommendation\nsystem has been widely promoted, and the privacy problem of multi-source data\nbecomes more and more important. Based on Federated learning technology, this\npaper proposes a shared matrix factorization scheme called SharedMF. Firstly, a\ndistributed recommendation system is built, and then secret sharing technology\nis used to protect the privacy of local data. Experimental results show that\ncompared with the existing homomorphic encryption methods, our method can have\nfaster execution speed without privacy disclosure, and can better adapt to\nrecommendation scenarios with large amount of data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 06:19:38 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ying", "Senci", ""]]}, {"id": "2008.07873", "submitter": "Kun Zhou", "authors": "Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng\n  Zhang, Zhongyuan Wang and Ji-Rong Wen", "title": "S^3-Rec: Self-Supervised Learning for Sequential Recommendation with\n  Mutual Information Maximization", "comments": "Accepted as CIKM2020 long paper", "journal-ref": null, "doi": "10.1145/3340531.3411954", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant progress has been made in sequential recommendation\nwith deep learning. Existing neural sequential recommendation models usually\nrely on the item prediction loss to learn model parameters or data\nrepresentations. However, the model trained with this loss is prone to suffer\nfrom data sparsity problem. Since it overemphasizes the final performance, the\nassociation or fusion between context data and sequence data has not been well\ncaptured and utilized for sequential recommendation. To tackle this problem, we\npropose the model S^3-Rec, which stands for Self-Supervised learning for\nSequential Recommendation, based on the self-attentive neural architecture. The\nmain idea of our approach is to utilize the intrinsic data correlation to\nderive self-supervision signals and enhance the data representations via\npre-training methods for improving sequential recommendation. For our task, we\ndevise four auxiliary self-supervised objectives to learn the correlations\namong attribute, item, subsequence, and sequence by utilizing the mutual\ninformation maximization (MIM) principle. MIM provides a unified way to\ncharacterize the correlation between different types of data, which is\nparticularly suitable in our scenario. Extensive experiments conducted on six\nreal-world datasets demonstrate the superiority of our proposed method over\nexisting state-of-the-art methods, especially when only limited training data\nis available. Besides, we extend our self-supervised learning method to other\nrecommendation models, which also improve their performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:44:10 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhou", "Kun", ""], ["Wang", "Hui", ""], ["Zhao", "Wayne Xin", ""], ["Zhu", "Yutao", ""], ["Wang", "Sirui", ""], ["Zhang", "Fuzheng", ""], ["Wang", "Zhongyuan", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2008.07880", "submitter": "Karin Verspoor", "authors": "Karin Verspoor, Simon \\v{S}uster, Yulia Otmakhova, Shevon Mendis,\n  Zenan Zhai, Biaoyan Fang, Jey Han Lau, Timothy Baldwin, Antonio Jimeno Yepes,\n  David Martinez", "title": "COVID-SEE: Scientific Evidence Explorer for COVID-19 Related Research", "comments": "COVID-SEE is available at http://covid-see.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COVID-SEE, a system for medical literature discovery based on the\nconcept of information exploration, which builds on several distinct text\nanalysis and natural language processing methods to structure and organise\ninformation in publications, and augments search by providing a visual overview\nsupporting exploration of a collection to identify key articles of interest. We\ndeveloped this system over COVID-19 literature to help medical professionals\nand researchers explore the literature evidence, and improve findability of\nrelevant information. COVID-SEE is available at http://covid-see.com.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 12:14:36 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Verspoor", "Karin", ""], ["\u0160uster", "Simon", ""], ["Otmakhova", "Yulia", ""], ["Mendis", "Shevon", ""], ["Zhai", "Zenan", ""], ["Fang", "Biaoyan", ""], ["Lau", "Jey Han", ""], ["Baldwin", "Timothy", ""], ["Yepes", "Antonio Jimeno", ""], ["Martinez", "David", ""]]}, {"id": "2008.07939", "submitter": "Van-Hoang Nguyen", "authors": "Van-Hoang Nguyen and Kazunari Sugiyama and Preslav Nakov and Min-Yen\n  Kan", "title": "FANG: Leveraging Social Context for Fake News Detection Using Graph\n  Representation", "comments": "To appear in CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412046", "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Factual News Graph (FANG), a novel graphical social context\nrepresentation and learning framework for fake news detection. Unlike previous\ncontextual models that have targeted performance, our focus is on\nrepresentation learning. Compared to transductive models, FANG is scalable in\ntraining as it does not have to maintain all nodes, and it is efficient at\ninference time, without the need to re-process the entire graph. Our\nexperimental results show that FANG is better at capturing the social context\ninto a high fidelity representation, compared to recent graphical and\nnon-graphical models. In particular, FANG yields significant improvements for\nthe task of fake news detection, and it is robust in the case of limited\ntraining data. We further demonstrate that the representations learned by FANG\ngeneralize to related tasks, such as predicting the factuality of reporting of\na news medium.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:05:16 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 11:45:23 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nguyen", "Van-Hoang", ""], ["Sugiyama", "Kazunari", ""], ["Nakov", "Preslav", ""], ["Kan", "Min-Yen", ""]]}, {"id": "2008.07956", "submitter": "Farhan Khawar", "authors": "Farhan Khawar, Leonard Kin Man Poon, Nevin Lianwen Zhang", "title": "Learning the Structure of Auto-Encoding Recommenders", "comments": "Proceedings of The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380135", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder recommenders have recently shown state-of-the-art performance in\nthe recommendation task due to their ability to model non-linear item\nrelationships effectively. However, existing autoencoder recommenders use\nfully-connected neural network layers and do not employ structure learning.\nThis can lead to inefficient training, especially when the data is sparse as\ncommonly found in collaborative filtering. The aforementioned results in lower\ngeneralization ability and reduced performance. In this paper, we introduce\nstructure learning for autoencoder recommenders by taking advantage of the\ninherent item groups present in the collaborative filtering domain. Due to the\nnature of items in general, we know that certain items are more related to each\nother than to other items. Based on this, we propose a method that first learns\ngroups of related items and then uses this information to determine the\nconnectivity structure of an auto-encoding neural network. This results in a\nnetwork that is sparsely connected. This sparse structure can be viewed as a\nprior that guides the network training. Empirically we demonstrate that the\nproposed structure learning enables the autoencoder to converge to a local\noptimum with a much smaller spectral norm and generalization error bound than\nthe fully-connected network. The resultant sparse network considerably\noutperforms the state-of-the-art methods like \\textsc{Mult-vae/Mult-dae} on\nmultiple benchmarked datasets even when the same number of parameters and flops\nare used. It also has a better cold-start performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:37:40 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Khawar", "Farhan", ""], ["Poon", "Leonard Kin Man", ""], ["Zhang", "Nevin Lianwen", ""]]}, {"id": "2008.07962", "submitter": "Xin Mao", "authors": "Xin Mao, Wenting Wang, Huimin Xu, Yuanbin Wu, Man Lan", "title": "Relational Reflection Entity Alignment", "comments": "10 pages, Accepted by CIKM2020", "journal-ref": null, "doi": "10.1145/3340531.3412001", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment aims to identify equivalent entity pairs from different\nKnowledge Graphs (KGs), which is essential in integrating multi-source KGs.\nRecently, with the introduction of GNNs into entity alignment, the\narchitectures of recent models have become more and more complicated. We even\nfind two counter-intuitive phenomena within these methods: (1) The standard\nlinear transformation in GNNs is not working well. (2) Many advanced KG\nembedding models designed for link prediction task perform poorly in entity\nalignment. In this paper, we abstract existing entity alignment methods into a\nunified framework, Shape-Builder & Alignment, which not only successfully\nexplains the above phenomena but also derives two key criteria for an ideal\ntransformation operation. Furthermore, we propose a novel GNNs-based method,\nRelational Reflection Entity Alignment (RREA). RREA leverages Relational\nReflection Transformation to obtain relation specific embeddings for each\nentity in a more efficient way. The experimental results on real-world datasets\nshow that our model significantly outperforms the state-of-the-art methods,\nexceeding by 5.8%-10.9% on Hits@1.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:49:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Mao", "Xin", ""], ["Wang", "Wenting", ""], ["Xu", "Huimin", ""], ["Wu", "Yuanbin", ""], ["Lan", "Man", ""]]}, {"id": "2008.08134", "submitter": "Martin Aum\\\"uller", "authors": "Martin Aum\\\"uller and Anders Bourgeat and Jana Schmurr", "title": "Differentially Private Sketches for Jaccard Similarity Estimation", "comments": "Accepted at SISAP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes two locally-differential private algorithms for\nreleasing user vectors such that the Jaccard similarity between these vectors\ncan be efficiently estimated. The basic building block is the well known\nMinHash method. To achieve a privacy-utility trade-off, MinHash is extended in\ntwo ways using variants of Generalized Randomized Response and the Laplace\nMechanism. A theoretical analysis provides bounds on the absolute error and\nexperiments show the utility-privacy trade-off on synthetic and real-world\ndata. The paper ends with a critical discussion of related work.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 19:42:46 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Bourgeat", "Anders", ""], ["Schmurr", "Jana", ""]]}, {"id": "2008.08180", "submitter": "Jason Ingyu Choi", "authors": "Jason Ingyu Choi, Surya Kallumadi, Bhaskar Mitra, Eugene Agichtein,\n  Faizan Javed", "title": "Semantic Product Search for Matching Structured Product Catalogs in\n  E-Commerce", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving all semantically relevant products from the product catalog is an\nimportant problem in E-commerce. Compared to web documents, product catalogs\nare more structured and sparse due to multi-instance fields that encode\nheterogeneous aspects of products (e.g. brand name and product dimensions). In\nthis paper, we propose a new semantic product search algorithm that learns to\nrepresent and aggregate multi-instance fields into a document representation\nusing state of the art transformers as encoders. Our experiments investigate\ntwo aspects of the proposed approach: (1) effectiveness of field\nrepresentations and structured matching; (2) effectiveness of adding lexical\nfeatures to semantic search. After training our models using user click logs\nfrom a well-known E-commerce platform, we show that our results provide useful\ninsights for improving product search. Lastly, we present a detailed error\nanalysis to show which types of queries benefited the most by fielded\nrepresentations and structured matching.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:01:16 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Choi", "Jason Ingyu", ""], ["Kallumadi", "Surya", ""], ["Mitra", "Bhaskar", ""], ["Agichtein", "Eugene", ""], ["Javed", "Faizan", ""]]}, {"id": "2008.08247", "submitter": "Kun Zhou", "authors": "Kun Zhou, Wayne Xin Zhao, Hui Wang, Sirui Wang, Fuzheng Zhang,\n  Zhongyuan Wang and Ji-Rong Wen", "title": "Leveraging Historical Interaction Data for Improving Conversational\n  Recommender System", "comments": "Accepted as CIKM short paper", "journal-ref": null, "doi": "10.1145/3340531.3412098", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, conversational recommender system (CRS) has become an emerging and\npractical research topic. Most of the existing CRS methods focus on learning\neffective preference representations for users from conversation data alone.\nWhile, we take a new perspective to leverage historical interaction data for\nimproving CRS. For this purpose, we propose a novel pre-training approach to\nintegrating both item-based preference sequence (from historical interaction\ndata) and attribute-based preference sequence (from conversation data) via\npre-training methods. We carefully design two pre-training tasks to enhance\ninformation fusion between item- and attribute-based preference. To improve the\nlearning performance, we further develop an effective negative sample generator\nwhich can produce high-quality negative samples. Experiment results on two\nreal-world datasets have demonstrated the effectiveness of our approach for\nimproving CRS.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:43:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhou", "Kun", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Hui", ""], ["Wang", "Sirui", ""], ["Zhang", "Fuzheng", ""], ["Wang", "Zhongyuan", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2008.08273", "submitter": "Sung Min Cho", "authors": "Sung Min Cho, Eunhyeok Park, Sungjoo Yoo", "title": "MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings\n  for Sequential Recommendation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": "10.1145/3383313.3412216", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-attention based models have achieved state-of-the-art\nperformance in sequential recommendation task. Following the custom from\nlanguage processing, most of these models rely on a simple positional embedding\nto exploit the sequential nature of the user's history. However, there are some\nlimitations regarding the current approaches. First, sequential recommendation\nis different from language processing in that timestamp information is\navailable. Previous models have not made good use of it to extract additional\ncontextual information. Second, using a simple embedding scheme can lead to\ninformation bottleneck since the same embedding has to represent all possible\ncontextual biases. Third, since previous models use the same positional\nembedding in each attention head, they can wastefully learn overlapping\npatterns. To address these limitations, we propose MEANTIME (MixturE of\nAtteNTIon mechanisms with Multi-temporal Embeddings) which employs multiple\ntypes of temporal embeddings designed to capture various patterns from the\nuser's behavior sequence, and an attention structure that fully leverages such\ndiversity. Experiments on real-world data show that our proposed method\noutperforms current state-of-the-art sequential recommendation methods, and we\nprovide an extensive ablation study to analyze how the model gains from the\ndiverse positional information.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 05:32:14 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 07:18:14 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Cho", "Sung Min", ""], ["Park", "Eunhyeok", ""], ["Yoo", "Sungjoo", ""]]}, {"id": "2008.08302", "submitter": "Zhichao Xu", "authors": "Zhichao Xu, Yi Han, Yongfeng Zhang, Qingyao Ai", "title": "E-commerce Recommendation with Weighted Expected Utility", "comments": "accepted by CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3411993", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from shopping at retail stores, consumers on e-commerce platforms\nusually cannot touch or try products before purchasing, which means that they\nhave to make decisions when they are uncertain about the outcome (e.g.,\nsatisfaction level) of purchasing a product. To study people's preferences,\neconomics researchers have proposed the hypothesis of Expected Utility (EU)\nthat models the subject value associated with an individual's choice as the\nstatistical expectations of that individual's valuations of the outcomes of\nthis choice. Despite its success in studies of game theory and decision theory,\nthe effectiveness of EU, however, is mostly unknown in e-commerce\nrecommendation systems. Previous research on e-commerce recommendation\ninterprets the utility of purchase decisions either as a function of the\nconsumed quantity of the product or as the gain of sellers/buyers in the\nmonetary sense. As most consumers just purchase one unit of a product at a time\nand most alternatives have similar prices, such modeling of purchase utility is\nlikely to be inaccurate in practice. In this paper, we interpret purchase\nutility as the satisfaction level a consumer gets from a product and propose a\nrecommendation framework using EU to model consumers' behavioral patterns. We\nassume that consumer estimates the expected utilities of all the alternatives\nand choose products with maximum expected utility for each purchase. To deal\nwith the potential psychological biases of each consumer, we introduce the\nusage of Probability Weight Function (PWF) and design our algorithm based on\nWeighted Expected Utility (WEU). Empirical study on real-world e-commerce\ndatasets shows that our proposed ranking-based recommendation framework\nachieves statistically significant improvement against both classical\nCollaborative Filtering/Latent Factor Models and state-of-the-art deep models\nin top-K recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 07:19:24 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Xu", "Zhichao", ""], ["Han", "Yi", ""], ["Zhang", "Yongfeng", ""], ["Ai", "Qingyao", ""]]}, {"id": "2008.08428", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog and Jamie Callan", "title": "Generating Categories for Sets of Entities", "comments": "Proceedings of the 29th ACM International Conference on Information\n  and Knowledge Management (CIKM '20)", "journal-ref": null, "doi": "10.1145/3340531.3412019", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Category systems are central components of knowledge bases, as they provide a\nhierarchical grouping of semantically related concepts and entities. They are a\nunique and valuable resource that is utilized in a broad range of information\naccess tasks. To aid knowledge editors in the manual process of expanding a\ncategory system, this paper presents a method of generating categories for sets\nof entities. First, we employ neural abstractive summarization models to\ngenerate candidate categories. Next, the location within the hierarchy is\nidentified for each candidate. Finally, structure-, content-, and\nhierarchy-based features are used to rank candidates to identify by the most\npromising ones (measured in terms of specificity, hierarchy, and importance).\nWe develop a test collection based on Wikipedia categories and demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 13:31:07 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""], ["Callan", "Jamie", ""]]}, {"id": "2008.08551", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri", "title": "Popularity Bias in Recommendation: A Multi-stakeholder Perspective", "comments": "PhD Dissertation in Information Science (University of Colorado\n  Boulder)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, especially in academic research in recommender systems, the\nfocus has been solely on the satisfaction of the end-user. While user\nsatisfaction has, indeed, been associated with the success of the business, it\nis not the only factor. In many recommendation domains, there are other\nstakeholders whose needs should be taken into account in the recommendation\ngeneration and evaluation. In this dissertation, I describe the notion of\nmulti-stakeholder recommendation. In particular, I study one of the most\nimportant challenges in recommendation research, popularity bias, from a\nmulti-stakeholder perspective since, as I show later in this dissertation, it\nimpacts different stakeholders in a recommender system. Popularity bias is a\nwell-known phenomenon in recommender systems where popular items are\nrecommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multi-stakeholder\nenvironments. In this dissertation, I study the impact of popularity bias in\nrecommender systems from a multi-stakeholder perspective. In addition, I\npropose several algorithms each approaching the popularity bias mitigation from\na different angle and compare their performances using several metrics with\nsome other state-of-the-art approaches in the literature. I show that, often,\nthe standard evaluation measures of popularity bias mitigation in the\nliterature do not reflect the real picture of an algorithm's performance when\nit is evaluated from a multi-stakeholder point of view.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:56:29 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Abdollahpouri", "Himan", ""]]}, {"id": "2008.08650", "submitter": "Amir Jalaly Bidgoly", "authors": "Amir Jalaly Bidgolya, Zoleikha Rahmaniana", "title": "A Robust Opinion Spam Detection Method Against Malicious Attackers in\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews are potent sources for industry owners and buyers, however\nopportunistic people may try to destruct or promote their desired product by\npublishing fake comments named spam opinion. So far, many models have been\ndeveloped to detect spam opinions, but none have addressed the issue of spam\nattack. It is a way a smart spammer can deceive the system in a manner in which\nhe can continue generating spams without the fear of being detected and blocked\nby the system. In this paper, the spam attacks are discussed. Moreover, a\nrobust graph-based spam detection method is proposed. The method respectively\nestimates honesty, trust and reliability values of reviews, reviewers, and\nproducts considering possible deception scenarios. The paper also presents the\nefficiency of the proposed method as compared to other graph-based methods\nthrough some case studies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 19:54:44 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bidgolya", "Amir Jalaly", ""], ["Rahmaniana", "Zoleikha", ""]]}, {"id": "2008.08899", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Ruben Tito, Dimosthenis Karatzas, R. Manmatha, C.V.\n  Jawahar", "title": "Document Visual Question Answering Challenge 2020", "comments": "to be published as a short paper in DAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of Document Visual Question Answering Challenge\norganized as part of \"Text and Documents in the Deep Learning Era\" workshop, in\nCVPR 2020. The challenge introduces a new problem - Visual Question Answering\non document images. The challenge comprised two tasks. The first task concerns\nwith asking questions on a single document image. On the other hand, the second\ntask is set as a retrieval task where the question is posed over a collection\nof images. For the task 1 a new dataset is introduced comprising 50,000\nquestions-answer(s) pairs defined over 12,767 document images. For task 2\nanother dataset has been created comprising 20 questions over 14,362 document\nimages which share the same document template.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 11:36:36 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 03:09:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mathew", "Minesh", ""], ["Tito", "Ruben", ""], ["Karatzas", "Dimosthenis", ""], ["Manmatha", "R.", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2008.08903", "submitter": "Nan Gao", "authors": "Nan Gao, Hao Xue, Wei Shao, Sichen Zhao, Kyle Kai Qin, Arian Prabowo,\n  Mohammad Saiedur Rahaman, Flora D. Salim", "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey", "comments": "This paper has been accepted by ACM Transactions on Intelligent\n  Systems and Technology (TIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:05:40 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 01:30:20 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 06:01:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gao", "Nan", ""], ["Xue", "Hao", ""], ["Shao", "Wei", ""], ["Zhao", "Sichen", ""], ["Qin", "Kyle Kai", ""], ["Prabowo", "Arian", ""], ["Rahaman", "Mohammad Saiedur", ""], ["Salim", "Flora D.", ""]]}, {"id": "2008.09061", "submitter": "Tao Yang", "authors": "Tao Yang, Shikai Fang, Shibo Li, Yulan Wang, Qingyao Ai", "title": "Analysis of Multivariate Scoring Functions for Automatic Unbiased\n  Learning to Rank", "comments": "4 pages, 2 figures. It has already been accepted and will show in\n  Proceedings of the 29th ACM International Conference on Information and\n  Knowledge Management (CIKM '20), October 19--23, 2020", "journal-ref": null, "doi": "10.1145/3340531.3412128", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Leveraging biased click data for optimizing learning to rank systems has been\na popular approach in information retrieval. Because click data is often noisy\nand biased, a variety of methods have been proposed to construct unbiased\nlearning to rank (ULTR) algorithms for the learning of unbiased ranking models.\nAmong them, automatic unbiased learning to rank (AutoULTR) algorithms that\njointly learn user bias models (i.e., propensity models) with unbiased rankers\nhave received a lot of attention due to their superior performance and low\ndeployment cost in practice. Despite their differences in theories and\nalgorithm design, existing studies on ULTR usually use uni-variate ranking\nfunctions to score each document or result independently. On the other hand,\nrecent advances in context-aware learning-to-rank models have shown that\nmultivariate scoring functions, which read multiple documents together and\npredict their ranking scores jointly, are more powerful than uni-variate\nranking functions in ranking tasks with human-annotated relevance labels.\nWhether such superior performance would hold in ULTR with noisy data, however,\nis mostly unknown. In this paper, we investigate existing multivariate scoring\nfunctions and AutoULTR algorithms in theory and prove that permutation\ninvariance is a crucial factor that determines whether a context-aware\nlearning-to-rank model could be applied to existing AutoULTR framework. Our\nexperiments with synthetic clicks on two large-scale benchmark datasets show\nthat AutoULTR models with permutation-invariant multivariate scoring functions\nsignificantly outperform those with uni-variate scoring functions and\npermutation-variant multivariate scoring functions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:31:59 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Yang", "Tao", ""], ["Fang", "Shikai", ""], ["Li", "Shibo", ""], ["Wang", "Yulan", ""], ["Ai", "Qingyao", ""]]}, {"id": "2008.09093", "submitter": "Andrew Yates", "authors": "Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, Yingfei Sun", "title": "PARADE: Passage Representation Aggregation for Document Reranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained transformer models, such as BERT and T5, have shown to be highly\neffective at ad-hoc passage and document ranking. Due to inherent sequence\nlength limits of these models, they need to be run over a document's passages,\nrather than processing the entire document sequence at once. Although several\napproaches for aggregating passage-level signals have been proposed, there has\nyet to be an extensive comparison of these techniques. In this work, we explore\nstrategies for aggregating relevance signals from a document's passages into a\nfinal ranking score. We find that passage representation aggregation techniques\ncan significantly improve over techniques proposed in prior work, such as\ntaking the maximum passage score. We call this new approach PARADE. In\nparticular, PARADE can significantly improve results on collections with broad\ninformation needs where relevance signals can be spread throughout the document\n(such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation\ntechniques may work better on collections with an information need that can\noften be pinpointed to a single passage (such as TREC DL and TREC Genomics). We\nalso conduct efficiency analyses, and highlight several strategies for\nimproving transformer-based aggregation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:32:30 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:46:31 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Canjia", ""], ["Yates", "Andrew", ""], ["MacAvaney", "Sean", ""], ["He", "Ben", ""], ["Sun", "Yingfei", ""]]}, {"id": "2008.09237", "submitter": "Zuohui Fu", "authors": "Zuohui Fu, Yikun Xian, Yaxin Zhu, Yongfeng Zhang, Gerard de Melo", "title": "COOKIE: A Dataset for Conversational Recommendation over Knowledge\n  Graphs in E-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new dataset for conversational recommendation over\nknowledge graphs in e-commerce platforms called COOKIE. The dataset is\nconstructed from an Amazon review corpus by integrating both user-agent\ndialogue and custom knowledge graphs for recommendation. Specifically, we first\nconstruct a unified knowledge graph and extract key entities between\nuser--product pairs, which serve as the skeleton of a conversation. Then we\nsimulate conversations mirroring the human coarse-to-fine process of choosing\npreferred items. The proposed baselines and experiments demonstrate that our\ndataset is able to provide innovative opportunities for conversational\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:11:31 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Fu", "Zuohui", ""], ["Xian", "Yikun", ""], ["Zhu", "Yaxin", ""], ["Zhang", "Yongfeng", ""], ["de Melo", "Gerard", ""]]}, {"id": "2008.09273", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher", "title": "The Connection Between Popularity Bias, Calibration, and Fairness in\n  Recommendation", "comments": "Accepted at the 14th ACM Conference on Recommender Systems (RecSys\n  2020) Late Breaking Results Track. arXiv admin note: substantial text overlap\n  with arXiv:1910.05755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a growing interest in fairness-aware recommender\nsystems including fairness in providing consistent performance across different\nusers or groups of users. A recommender system could be considered unfair if\nthe recommendations do not fairly represent the tastes of a certain group of\nusers while other groups receive recommendations that are consistent with their\npreferences. In this paper, we use a metric called miscalibration for measuring\nhow a recommendation algorithm is responsive to users' true preferences and we\nconsider how various algorithms may result in different degrees of\nmiscalibration for different users. In particular, we conjecture that\npopularity bias which is a well-known phenomenon in recommendation is one\nimportant factor leading to miscalibration in recommendation. Our experimental\nresults using two real-world datasets show that there is a connection between\nhow different user groups are affected by algorithmic popularity bias and their\nlevel of interest in popular items. Moreover, we show that the more a group is\naffected by the algorithmic popularity bias, the more their recommendations are\nmiscalibrated.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 02:33:17 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2008.09340", "submitter": "Sasho Nedelkoski", "authors": "Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso,\n  Odej Kao", "title": "Self-Attentive Classification-Based Anomaly Detection in Unstructured\n  Logs", "comments": "11 pages, 8 figures, Accepted at ICDM 2020: 20th IEEE International\n  Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of anomalies is essential mining task for the security and\nreliability in computer systems. Logs are a common and major data source for\nanomaly detection methods in almost every computer system. They collect a range\nof significant events describing the runtime system status. Recent studies have\nfocused predominantly on one-class deep learning methods on predefined\nnon-learnable numerical log representations. The main limitation is that these\nmodels are not able to learn log representations describing the semantic\ndifferences between normal and anomaly logs, leading to a poor generalization\nof unseen logs. We propose Logsy, a classification-based method to learn log\nrepresentations in a way to distinguish between normal data from the system of\ninterest and anomaly samples from auxiliary log datasets, easily accessible via\nthe internet. The idea behind such an approach to anomaly detection is that the\nauxiliary dataset is sufficiently informative to enhance the representation of\nthe normal data, yet diverse to regularize against overfitting and improve\ngeneralization. We propose an attention-based encoder model with a new\nhyperspherical loss function. This enables learning compact log representations\ncapturing the intrinsic differences between normal and anomaly logs.\nEmpirically, we show an average improvement of 0.25 in the F1 score, compared\nto the previous methods. To investigate the properties of Logsy, we perform\nadditional experiments including evaluation of the effect of the auxiliary data\nsize, the influence of expert knowledge, and the quality of the learned log\nrepresentations. The results show that the learned representation boost the\nperformance of the previous methods such as PCA with a relative improvement of\n28.2%.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 07:26:55 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Nedelkoski", "Sasho", ""], ["Bogatinovski", "Jasmin", ""], ["Acker", "Alexander", ""], ["Cardoso", "Jorge", ""], ["Kao", "Odej", ""]]}, {"id": "2008.09368", "submitter": "Xu He", "authors": "Xu He, Bo An, Yanghua Li, Haikai Chen, Qingyu Guo, Xin Li, and Zhirong\n  Wang", "title": "Contextual User Browsing Bandits for Large-Scale Online Mobile\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommendation services recommend multiple commodities to users.\nNowadays, a considerable proportion of users visit e-commerce platforms by\nmobile devices. Due to the limited screen size of mobile devices, positions of\nitems have a significant influence on clicks: 1) Higher positions lead to more\nclicks for one commodity. 2) The 'pseudo-exposure' issue: Only a few\nrecommended items are shown at first glance and users need to slide the screen\nto browse other items. Therefore, some recommended items ranked behind are not\nviewed by users and it is not proper to treat this kind of items as negative\nsamples. While many works model the online recommendation as contextual bandit\nproblems, they rarely take the influence of positions into consideration and\nthus the estimation of the reward function may be biased. In this paper, we aim\nat addressing these two issues to improve the performance of online mobile\nrecommendation. Our contributions are four-fold. First, since we concern the\nreward of a set of recommended items, we model the online recommendation as a\ncontextual combinatorial bandit problem and define the reward of a recommended\nset. Second, we propose a novel contextual combinatorial bandit method called\nUBM-LinUCB to address two issues related to positions by adopting the User\nBrowsing Model (UBM), a click model for web search. Third, we provide a formal\nregret analysis and prove that our algorithm achieves sublinear regret\nindependent of the number of items. Finally, we evaluate our algorithm on two\nreal-world datasets by a novel unbiased estimator. An online experiment is also\nimplemented in Taobao, one of the most popular e-commerce platforms in the\nworld. Results on two CTR metrics show that our algorithm outperforms the other\ncontextual bandit algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:22:30 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["He", "Xu", ""], ["An", "Bo", ""], ["Li", "Yanghua", ""], ["Chen", "Haikai", ""], ["Guo", "Qingyu", ""], ["Li", "Xin", ""], ["Wang", "Zhirong", ""]]}, {"id": "2008.09391", "submitter": "Nicol\\'as E. D\\'iaz Ferreyra", "authors": "Nicolas E. Diaz Ferreyra, Rene Meis and Maritta Heisel", "title": "Learning from Online Regrets: From Deleted Posts to Risk Awareness in\n  Social Network Sites", "comments": null, "journal-ref": null, "doi": "10.1145/3314183.3323849", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Network Sites (SNSs) like Facebook or Instagram are spaces where\npeople expose their lives to wide and diverse audiences. This practice can lead\nto unwanted incidents such as reputation damage, job loss or harassment when\npieces of private information reach unintended recipients. As a consequence,\nusers often regret to have posted private information in these platforms and\nproceed to delete such content after having a negative experience. Risk\nawareness is a strategy that can be used to persuade users towards safer\nprivacy decisions. However, many risk awareness technologies for SNSs assume\nthat information about risks is retrieved and measured by an expert in the\nfield. Consequently, risk estimation is an activity that is often passed over\ndespite its importance. In this work we introduce an approach that employs\ndeleted posts as risk information vehicles to measure the frequency and\nconsequence level of self-disclosure patterns in SNSs. In this method,\nconsequence is reported by the users through an ordinal scale and used later on\nto compute a risk criticality index. We thereupon show how this index can serve\nin the design of adaptive privacy nudges for SNSs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 09:43:18 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ferreyra", "Nicolas E. Diaz", ""], ["Meis", "Rene", ""], ["Heisel", "Maritta", ""]]}, {"id": "2008.09472", "submitter": "Mawulolo Ameko", "authors": "Mawulolo K. Ameko, Miranda L. Beltzer, Lihua Cai, Mehdi Boukhechba,\n  Bethany A. Teachman, Laura E. Barnes", "title": "Offline Contextual Multi-armed Bandits for Mobile Health Interventions:\n  A Case Study on Emotion Regulation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": "10.1145/3383313.3412244", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering treatment recommendations via pervasive electronic devices such as\nmobile phones has the potential to be a viable and scalable treatment medium\nfor long-term health behavior management. But active experimentation of\ntreatment options can be time-consuming, expensive and altogether unethical in\nsome cases. There is a growing interest in methodological approaches that allow\nan experimenter to learn and evaluate the usefulness of a new treatment\nstrategy before deployment. We present the first development of a treatment\nrecommender system for emotion regulation using real-world historical mobile\ndigital data from n = 114 high socially anxious participants to test the\nusefulness of new emotion regulation strategies. We explore a number of offline\ncontextual bandits estimators for learning and propose a general framework for\nlearning algorithms. Our experimentation shows that the proposed doubly robust\noffline learning algorithms performed significantly better than baseline\napproaches, suggesting that this type of recommender algorithm could improve\nemotion regulation. Given that emotion regulation is impaired across many\nmental illnesses and such a recommender algorithm could be scaled up easily,\nthis approach holds potential to increase access to treatment for many people.\nWe also share some insights that allow us to translate contextual bandit models\nto this complex real-world data, including which contextual features appear to\nbe most important for predicting emotion regulation strategy effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:41:24 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ameko", "Mawulolo K.", ""], ["Beltzer", "Miranda L.", ""], ["Cai", "Lihua", ""], ["Boukhechba", "Mehdi", ""], ["Teachman", "Bethany A.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "2008.09514", "submitter": "Yongfeng Zhang", "authors": "Shaoyun Shi, Hanxiong Chen, Weizhi Ma, Jiaxin Mao, Min Zhang, Yongfeng\n  Zhang", "title": "Neural Logic Reasoning", "comments": "Accepted to ACM CIKM 2020. arXiv admin note: substantial text overlap\n  with arXiv:1910.08629", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the success of deep neural networks in many\nresearch areas. The fundamental idea behind the design of most neural networks\nis to learn similarity patterns from data for prediction and inference, which\nlacks the ability of cognitive reasoning. However, the concrete ability of\nreasoning is critical to many theoretical and practical problems. On the other\nhand, traditional symbolic reasoning methods do well in making logical\ninference, but they are mostly hard rule-based reasoning, which limits their\ngeneralization ability to different tasks since difference tasks may require\ndifferent rules. Both reasoning and generalization ability are important for\nprediction tasks such as recommender systems, where reasoning provides strong\nconnection between user history and target items for accurate prediction, and\ngeneralization helps the model to draw a robust user portrait over noisy\ninputs.\n  In this paper, we propose Logic-Integrated Neural Network (LINN) to integrate\nthe power of deep learning and logic reasoning. LINN is a dynamic neural\narchitecture that builds the computational graph according to input logical\nexpressions. It learns basic logical operations such as AND, OR, NOT as neural\nmodules, and conducts propositional logical reasoning through the network for\ninference. Experiments on theoretical task show that LINN achieves significant\nperformance on solving logical equations and variables. Furthermore, we test\nour approach on the practical task of recommendation by formulating the task\ninto a logical inference problem. Experiments show that LINN significantly\noutperforms state-of-the-art recommendation models in Top-K recommendation,\nwhich verifies the potential of LINN in practice.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:53:23 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Shi", "Shaoyun", ""], ["Chen", "Hanxiong", ""], ["Ma", "Weizhi", ""], ["Mao", "Jiaxin", ""], ["Zhang", "Min", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2008.09689", "submitter": "Yue Shang", "authors": "Yunjiang Jiang, Yue Shang, Hongwei Shen, Wen-Yun Yang and Yun Xiao", "title": "Fine-tune BERT for E-commerce Non-Default Search Ranking", "comments": "4 pages, SIGIR 2019 eCommerce Data Challenge report paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of non-default ranking on e-commerce platforms, such as based on\nascending item price or descending historical sales volume, often suffers from\nacute relevance problems, since the irrelevant items are much easier to be\nexposed at the top of the ranking results. In this work, we propose a two-stage\nranking scheme, which first recalls wide range of candidate items through\nrefined query/title keyword matching, and then classifies the recalled items\nusing BERT-Large fine-tuned on human label data. We also implemented parallel\nprediction on multiple GPU hosts and a C++ tokenization custom op of\nTensorflow. In this data challenge, our model won the 1st place in the\nsupervised phase (based on overall F1 score) and 2nd place in the final phase\n(based on average per query F1 score).\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:48:00 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jiang", "Yunjiang", ""], ["Shang", "Yue", ""], ["Shen", "Hongwei", ""], ["Yang", "Wen-Yun", ""], ["Xiao", "Yun", ""]]}, {"id": "2008.09706", "submitter": "Pengjie Ren", "authors": "Yangjun Zhang, Pengjie Ren, Maarten de Rijke", "title": "Detecting and Classifying Malevolent Dialogue Responses: Taxonomy, Data\n  and Methodology", "comments": "under review at JASIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational interfaces are increasingly popular as a way of connecting\npeople to information. Corpus-based conversational interfaces are able to\ngenerate more diverse and natural responses than template-based or\nretrieval-based agents. With their increased generative capacity of corpusbased\nconversational agents comes the need to classify and filter out malevolent\nresponses that are inappropriate in terms of content and dialogue acts.\nPrevious studies on the topic of recognizing and classifying inappropriate\ncontent are mostly focused on a certain category of malevolence or on single\nsentences instead of an entire dialogue. In this paper, we define the task of\nMalevolent Dialogue Response Detection and Classification (MDRDC). We make\nthree contributions to advance research on this task. First, we present a\nHierarchical Malevolent Dialogue Taxonomy (HMDT). Second, we create a labelled\nmulti-turn dialogue dataset and formulate the MDRDC task as a hierarchical\nclassification task over this taxonomy. Third, we apply stateof-the-art text\nclassification methods to the MDRDC task and report on extensive experiments\naimed at assessing the performance of these approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:43:27 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhang", "Yangjun", ""], ["Ren", "Pengjie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2008.09733", "submitter": "Junyu Cao", "authors": "Junyu Cao, Wei Sun, Zuo-Jun (Max) Shen, Markus Ettl", "title": "Fatigue-aware Bandits for Dependent Click Models", "comments": null, "journal-ref": "AAAI. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As recommender systems send a massive amount of content to keep users\nengaged, users may experience fatigue which is contributed by 1) an\noverexposure to irrelevant content, 2) boredom from seeing too many similar\nrecommendations. To address this problem, we consider an online learning\nsetting where a platform learns a policy to recommend content that takes user\nfatigue into account. We propose an extension of the Dependent Click Model\n(DCM) to describe users' behavior. We stipulate that for each piece of content,\nits attractiveness to a user depends on its intrinsic relevance and a discount\nfactor which measures how many similar contents have been shown. Users view the\nrecommended content sequentially and click on the ones that they find\nattractive. Users may leave the platform at any time, and the probability of\nexiting is higher when they do not like the content. Based on user's feedback,\nthe platform learns the relevance of the underlying content as well as the\ndiscounting effect due to content fatigue. We refer to this learning task as\n\"fatigue-aware DCM Bandit\" problem. We consider two learning scenarios\ndepending on whether the discounting effect is known. For each scenario, we\npropose a learning algorithm which simultaneously explores and exploits, and\ncharacterize its regret bound.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 02:18:15 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Cao", "Junyu", "", "Max"], ["Sun", "Wei", "", "Max"], ["Zuo-Jun", "", "", "Max"], ["Shen", "", ""], ["Ettl", "Markus", ""]]}, {"id": "2008.09872", "submitter": "Xuanji Xiao", "authors": "Xuanji Xiao, Huabin Chen, Yuzhen Liu, Xing Yao, Pei Liu, Chaosheng\n  Fan, Nian Ji, Xirong Jiang", "title": "LT4REC:A Lottery Ticket Hypothesis Based Multi-task Practice for Video\n  Recommendation System", "comments": "6 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate prediction (CTR) and post-click conversion rate prediction\n(CVR) play key roles across all industrial ranking systems, such as\nrecommendation systems, online advertising, and search engines. Different from\nthe extensive research on CTR, there is much less research on CVR estimation,\nwhose main challenge is extreme data sparsity with one or two orders of\nmagnitude reduction in the number of samples than CTR. People try to solve this\nproblem with the paradigm of multi-task learning with the sufficient samples of\nCTR, but the typical hard sharing method can't effectively solve this problem,\nbecause it is difficult to analyze which parts of network components can be\nshared and which parts are in conflict, i.e., there is a large inaccuracy with\nartificially designed neurons sharing. In this paper, we model CVR in a\nbrand-new method by adopting the lottery-ticket-hypothesis-based sparse sharing\nmulti-task learning, which can automatically and flexibly learn which neuron\nweights to be shared without artificial experience. Experiments on the dataset\ngathered from traffic logs of Tencent video's recommendation system demonstrate\nthat sparse sharing in the CVR model significantly outperforms competitive\nmethods. Due to the nature of weight sparsity in sparse sharing, it can also\nsignificantly reduce computational complexity and memory usage which are very\nimportant in the industrial recommendation system.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 16:48:08 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Xiao", "Xuanji", ""], ["Chen", "Huabin", ""], ["Liu", "Yuzhen", ""], ["Yao", "Xing", ""], ["Liu", "Pei", ""], ["Fan", "Chaosheng", ""], ["Ji", "Nian", ""], ["Jiang", "Xirong", ""]]}, {"id": "2008.10022", "submitter": "Oladapo Oyebode", "authors": "Oladapo Oyebode, Chinenye Ndulue, Dinesh Mulchandani, Banuchitra\n  Suruliraj, Ashfaq Adib, Fidelia Anulika Orji, Evangelos Milios, Stan Matwin,\n  and Rita Orji", "title": "COVID-19 Pandemic: Identifying Key Issues using Social Media and Natural\n  Language Processing", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has affected people's lives in many ways. Social media\ndata can reveal public perceptions and experience with respect to the pandemic,\nand also reveal factors that hamper or support efforts to curb global spread of\nthe disease. In this paper, we analyzed COVID-19-related comments collected\nfrom six social media platforms using Natural Language Processing (NLP)\ntechniques. We identified relevant opinionated keyphrases and their respective\nsentiment polarity (negative or positive) from over 1 million randomly selected\ncomments, and then categorized them into broader themes using thematic\nanalysis. Our results uncover 34 negative themes out of which 17 are economic,\nsocio-political, educational, and political issues. 20 positive themes were\nalso identified. We discuss the negative issues and suggest interventions to\ntackle them based on the positive themes and research evidence.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 12:05:12 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Oyebode", "Oladapo", ""], ["Ndulue", "Chinenye", ""], ["Mulchandani", "Dinesh", ""], ["Suruliraj", "Banuchitra", ""], ["Adib", "Ashfaq", ""], ["Orji", "Fidelia Anulika", ""], ["Milios", "Evangelos", ""], ["Matwin", "Stan", ""], ["Orji", "Rita", ""]]}, {"id": "2008.10166", "submitter": "Jiaxu Dao", "authors": "Jiaxu Dao, Jin Wang, Xuejie Zhang", "title": "YNU-HPCC at SemEval-2020 Task 11: LSTM Network for Detection of\n  Propaganda Techniques in News Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our studies on propaganda detection techniques for news\narticles in the SemEval-2020 task 11. This task is divided into the SI and TC\nsubtasks. We implemented the GloVe word representation, the BERT pretraining\nmodel, and the LSTM model architecture to accomplish this task. Our approach\nachieved good results for both the SI and TC subtasks. The macro-F1-score for\nthe SI subtask is 0.406, and the micro-F1-score for the TC subtask is 0.505.\nOur method significantly outperforms the officially released baseline method,\nand the SI and TC subtasks rank 17th and 22nd, respectively, for the test set.\nThis paper also compares the performances of different deep learning model\narchitectures, such as the Bi-LSTM, LSTM, BERT, and XGBoost models, on the\ndetection of news promotion techniques. The code of this paper is availabled\nat: https://github.com/daojiaxu/semeval_11.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 02:42:12 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 11:03:18 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dao", "Jiaxu", ""], ["Wang", "Jin", ""], ["Zhang", "Xuejie", ""]]}, {"id": "2008.10242", "submitter": "Harrie Oosterhuis", "authors": "Ali Vardasbi, Harrie Oosterhuis, Maarten de Rijke", "title": "When Inverse Propensity Scoring does not Work: Affine Corrections for\n  Unbiased Learning to Rank", "comments": "CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412031", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides position bias, which has been well-studied, trust bias is another\ntype of bias prevalent in user interactions with rankings: users are more\nlikely to click incorrectly w.r.t. their preferences on highly ranked items\nbecause they trust the ranking system. While previous work has observed this\nbehavior in users, we prove that existing Counterfactual Learning to Rank\n(CLTR) methods do not remove this bias, including methods specifically designed\nto mitigate this type of bias. Moreover, we prove that Inverse Propensity\nScoring (IPS) is principally unable to correct for trust bias under non-trivial\ncircumstances. Our main contribution is a new estimator based on affine\ncorrections: it both reweights clicks and penalizes items displayed on ranks\nwith high trust bias. Our estimator is the first estimator that is proven to\nremove the effect of both trust bias and position bias. Furthermore, we show\nthat our estimator is a generalization of the existing CLTR framework: if no\ntrust bias is present, it reduces to the original IPS estimator. Our\nsemi-synthetic experiments indicate that by removing the effect of trust bias\nin addition to position bias, CLTR can approximate the optimal ranking system\neven closer than previously possible.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:01:05 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 09:53:13 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Vardasbi", "Ali", ""], ["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2008.10267", "submitter": "Taejun Kim", "authors": "Taejun Kim, Minsuk Choi, Evan Sacks, Yi-Hsuan Yang and Juhan Nam", "title": "A Computational Analysis of Real-World DJ Mixes using Mix-To-Track\n  Subsequence Alignment", "comments": "Accepted for publication at 21st International Society for Music\n  Information Retrieval Conference (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A DJ mix is a sequence of music tracks concatenated seamlessly, typically\nrendered for audiences in a live setting by a DJ on stage. As a DJ mix is\nproduced in a studio or the live version is recorded for music streaming\nservices, computational methods to analyze DJ mixes, for example, extracting\ntrack information or understanding DJ techniques, have drawn research\ninterests. Many of previous works are, however, limited to identifying\nindividual tracks in a mix or segmenting it, and the sizes of the datasets are\nusually small. In this paper, we provide an in-depth analysis of DJ music by\naligning a mix to its original music tracks. We set up the subsequence\nalignment such that the audio features are less sensitive to the tempo or key\nchange of the original track in a mix. This approach provides temporally tight\nmix-to-track matching from which we can obtain cue-points, transition length,\nmix segmentation, and musical changes in DJ performance. Using 1,557 mixes from\n1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the\nproposed analysis and show a wide range of statistics, which may elucidate the\ncreative process of DJ music making.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:54:19 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Kim", "Taejun", ""], ["Choi", "Minsuk", ""], ["Sacks", "Evan", ""], ["Yang", "Yi-Hsuan", ""], ["Nam", "Juhan", ""]]}, {"id": "2008.10419", "submitter": "Amine Dadoun", "authors": "Amine Dadoun (1 and 2), Ismail Harrando (1), Pasquale Lisena (1),\n  Alison Reboud (1), Raphael Troncy (1) ((1) Eurecom, (2) Amadeus SAS)", "title": "Two Stages Approach for Tweet Engagement Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the approach proposed by the D2KLab team for the 2020\nRecSys Challenge on the task of predicting user engagement facing tweets. This\napproach relies on two distinct stages. First, relevant features are learned\nfrom the challenge dataset. These features are heterogeneous and are the\nresults of different learning modules such as handcrafted features, knowledge\ngraph embeddings, sentiment analysis features and BERT word embeddings. Second,\nthese features are provided in input to an ensemble system based on XGBoost.\nThis approach, only trained on a subset of the entire challenge dataset, ranked\n22 in the final leaderboard.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:18:10 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Dadoun", "Amine", "", "1 and 2"], ["Harrando", "Ismail", "", "Eurecom"], ["Lisena", "Pasquale", "", "Eurecom"], ["Reboud", "Alison", "", "Eurecom"], ["Troncy", "Raphael", "", "Eurecom"]]}, {"id": "2008.10549", "submitter": "Alireza Heidari", "authors": "Alireza Heidari, Shrinu Kushagra, Ihab F. Ilyas", "title": "On sampling from data with duplicate records", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data deduplication is the task of detecting records in a database that\ncorrespond to the same real-world entity. Our goal is to develop a procedure\nthat samples uniformly from the set of entities present in the database in the\npresence of duplicates. We accomplish this by a two-stage process. In the first\nstep, we estimate the frequencies of all the entities in the database. In the\nsecond step, we use rejection sampling to obtain a (approximately) uniform\nsample from the set of entities. However, efficiently estimating the frequency\nof all the entities is a non-trivial task and not attainable in the general\ncase. Hence, we consider various natural properties of the data under which\nsuch frequency estimation (and consequently uniform sampling) is possible.\nUnder each of those assumptions, we provide sampling algorithms and give proofs\nof the complexity (both statistical and computational) of our approach. We\ncomplement our study by conducting extensive experiments on both real and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:41:47 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Heidari", "Alireza", ""], ["Kushagra", "Shrinu", ""], ["Ilyas", "Ihab F.", ""]]}, {"id": "2008.10570", "submitter": "Morteza Ziyadi", "authors": "Morteza Ziyadi, Yuting Sun, Abhishek Goswami, Jade Huang, and Weizhu\n  Chen", "title": "Example-Based Named Entity Recognition", "comments": "15 pages, 6 figures, 5 tables with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to named entity recognition (NER) in the presence\nof scarce data that we call example-based NER. Our train-free few-shot learning\napproach takes inspiration from question-answering to identify entity spans in\na new and unseen domain. In comparison with the current state-of-the-art, the\nproposed method performs significantly better, especially when using a low\nnumber of support examples.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:18:24 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ziyadi", "Morteza", ""], ["Sun", "Yuting", ""], ["Goswami", "Abhishek", ""], ["Huang", "Jade", ""], ["Chen", "Weizhu", ""]]}, {"id": "2008.10808", "submitter": "Hao Li", "authors": "Mingkai Huang, Hao Li, Bing Bai, Chang Wang, Kun Bai, Fei Wang", "title": "A Federated Multi-View Deep Learning Framework for Privacy-Preserving\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-preserving recommendations are recently gaining momentum, since the\ndecentralized user data is increasingly harder to collect, by recommendation\nservice providers, due to the serious concerns over user privacy and data\nsecurity. This situation is further exacerbated by the strict government\nregulations such as Europe's General Data Privacy Regulations(GDPR). Federated\nLearning(FL) is a newly developed privacy-preserving machine learning paradigm\nto bridge data repositories without compromising data security and privacy.\nThus many federated recommendation(FedRec) algorithms have been proposed to\nrealize personalized privacy-preserving recommendations. However, existing\nFedRec algorithms, mostly extended from traditional collaborative filtering(CF)\nmethod, cannot address cold-start problem well. In addition, their performance\noverhead w.r.t. model accuracy, trained in a federated setting, is often\nnon-negligible comparing to centralized recommendations. This paper studies\nthis issue and presents FL-MV-DSSM, a generic content-based federated\nmulti-view recommendation framework that not only addresses the cold-start\nproblem, but also significantly boosts the recommendation performance by\nlearning a federated model from multiple data source for capturing richer\nuser-level features. The new federated multi-view setting, proposed by\nFL-MV-DSSM, opens new usage models and brings in new security challenges to FL\nin recommendation scenarios. We prove the security guarantees of \\xxx, and\nempirical evaluations on FL-MV-DSSM and its variations with public datasets\ndemonstrate its effectiveness. Our codes will be released if this paper is\naccepted.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:19:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Huang", "Mingkai", ""], ["Li", "Hao", ""], ["Bai", "Bing", ""], ["Wang", "Chang", ""], ["Bai", "Kun", ""], ["Wang", "Fei", ""]]}, {"id": "2008.10845", "submitter": "Dilruk Perera", "authors": "Dilruk Perera and Roger Zimmermann", "title": "CnGAN: Generative Adversarial Networks for Cross-network user preference\n  generation for non-overlapped users", "comments": null, "journal-ref": "The World Wide Web Conference, 2019 (WWW'19)", "doi": "10.1145/3308558.3313733", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major drawback of cross-network recommender solutions is that they can only\nbe applied to users that are overlapped across networks. Thus, the\nnon-overlapped users, which form the majority of users are ignored. As a\nsolution, we propose CnGAN, a novel multi-task learning based,\nencoder-GAN-recommender architecture. The proposed model synthetically\ngenerates source network user preferences for non-overlapped users by learning\nthe mapping from target to source network preference manifolds. The resultant\nuser preferences are used in a Siamese network based neural recommender\narchitecture. Furthermore, we propose a novel user based pairwise loss function\nfor recommendations using implicit interactions to better guide the generation\nprocess in the multi-task learning environment.We illustrate our solution by\ngenerating user preferences on the Twitter source network for recommendations\non the YouTube target network. Extensive experiments show that the generated\npreferences can be used to improve recommendations for non-overlapped users.\nThe resultant recommendations achieve superior performance compared to the\nstate-of-the-art cross-network recommender solutions in terms of accuracy,\nnovelty and diversity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 06:47:44 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Perera", "Dilruk", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2008.10849", "submitter": "Dilruk Perera", "authors": "Dilruk Perera and Roger Zimmermann", "title": "LSTM Networks for Online Cross-Network Recommendations", "comments": null, "journal-ref": "International Joint Conference on Artificial Intelligence, 2018\n  (IJCAI-18)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-network recommender systems use auxiliary information from multiple\nsource networks to create holistic user profiles and improve recommendations in\na target network. However, we find two major limitations in existing\ncross-network solutions that reduce overall recommender performance. Existing\nmodels (1) fail to capture complex non-linear relationships in user\ninteractions, and (2) are designed for offline settings hence, not updated\nonline with incoming interactions to capture the dynamics in the recommender\nenvironment. We propose a novel multi-layered Long Short-Term Memory (LSTM)\nnetwork based online solution to mitigate these issues. The proposed model\ncontains three main extensions to the standard LSTM: First, an attention gated\nmechanism to capture long-term user preference changes. Second, a higher order\ninteraction layer to alleviate data sparsity. Third, time aware LSTM cell gates\nto capture irregular time intervals between user interactions. We illustrate\nour solution using auxiliary information from Twitter and Google Plus to\nimprove recommendations on YouTube. Extensive experiments show that the\nproposed model consistently outperforms state-of-the-art in terms of accuracy,\ndiversity and novelty.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 07:10:24 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:34:10 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Perera", "Dilruk", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2008.10866", "submitter": "Dilruk Perera", "authors": "Dilruk Perera and Roger Zimmermann", "title": "Exploring the use of Time-Dependent Cross-Network Information for\n  Personalized Recommendations", "comments": null, "journal-ref": "ACM Multimedia 2017, (MM'17)", "doi": "10.1145/3123266.3123447", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overwhelming volume and complexity of information in online applications\nmake recommendation essential for users to find information of interest.\nHowever, two major limitations that coexist in real world applications (1)\nincomplete user profiles, and (2) the dynamic nature of user preferences\ncontinue to degrade recommender quality in aspects such as timeliness,\naccuracy, diversity and novelty. To address both the above limitations in a\nsingle solution, we propose a novel cross-network time aware recommender\nsolution. The solution first learns historical user models in the target\nnetwork by aggregating user preferences from multiple source networks. Second,\nuser level time aware latent factors are learnt to develop current user models\nfrom the historical models and conduct timely recommendations. We illustrate\nour solution by using auxiliary information from the Twitter source network to\nimprove recommendations for the YouTube target network. Experiments conducted\nusing multiple time aware and cross-network baselines under different time\ngranularities show that the proposed solution achieves superior performance in\nterms of accuracy, novelty and diversity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 07:52:47 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Perera", "Dilruk", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2008.10874", "submitter": "Lixin Su", "authors": "Lixin Su, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yanyan Lan, Xueqi\n  Cheng", "title": "Continual Domain Adaptation for Machine Reading Comprehension", "comments": "Accepted by CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412047", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC) has become a core component in a variety\nof natural language processing (NLP) applications such as question answering\nand dialogue systems. It becomes a practical challenge that an MRC model needs\nto learn in non-stationary environments, in which the underlying data\ndistribution changes over time. A typical scenario is the domain drift, i.e.\ndifferent domains of data come one after another, where the MRC model is\nrequired to adapt to the new domain while maintaining previously learned\nability. To tackle such a challenge, in this work, we introduce the\n\\textit{Continual Domain Adaptation} (CDA) task for MRC. So far as we know,\nthis is the first study on the continual learning perspective of MRC. We build\ntwo benchmark datasets for the CDA task, by re-organizing existing MRC\ncollections into different domains with respect to context type and question\ntype, respectively. We then analyze and observe the catastrophic forgetting\n(CF) phenomenon of MRC under the CDA setting. To tackle the CDA task, we\npropose several BERT-based continual learning MRC models using either\nregularization-based methodology or dynamic-architecture paradigm. We analyze\nthe performance of different continual learning MRC models under the CDA task\nand show that the proposed dynamic-architecture based model achieves the best\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 08:22:22 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Su", "Lixin", ""], ["Guo", "Jiafeng", ""], ["Zhang", "Ruqing", ""], ["Fan", "Yixing", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2008.11081", "submitter": "Amanuel Alambo", "authors": "Amanuel Alambo, Ryan Andrew, Sid Gollarahalli, Jacqueline Vaughn,\n  Tanvi Banerjee, Krishnaprasad Thirunarayan, Daniel Abrams, Nirmish Shah", "title": "Measuring Pain in Sickle Cell Disease using Clinical Text", "comments": "The 42nd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sickle Cell Disease (SCD) is a hereditary disorder of red blood cells in\nhumans. Complications such as pain, stroke, and organ failure occur in SCD as\nmalformed, sickled red blood cells passing through small blood vessels get\ntrapped. Particularly, acute pain is known to be the primary symptom of SCD.\nThe insidious and subjective nature of SCD pain leads to challenges in pain\nassessment among Medical Practitioners (MPs). Thus, accurate identification of\nmarkers of pain in patients with SCD is crucial for pain management.\nClassifying clinical notes of patients with SCD based on their pain level\nenables MPs to give appropriate treatment. We propose a binary classification\nmodel to predict pain relevance of clinical notes and a multiclass\nclassification model to predict pain level. While our four binary machine\nlearning (ML) classifiers are comparable in their performance, Decision Trees\nhad the best performance for the multiclass classification task achieving 0.70\nin F-measure. Our results show the potential clinical text analysis and machine\nlearning offer to pain management in sickle cell patients.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 23:39:57 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Alambo", "Amanuel", ""], ["Andrew", "Ryan", ""], ["Gollarahalli", "Sid", ""], ["Vaughn", "Jacqueline", ""], ["Banerjee", "Tanvi", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Abrams", "Daniel", ""], ["Shah", "Nirmish", ""]]}, {"id": "2008.11136", "submitter": "Amine Dadoun", "authors": "Amine Dadoun (1 and 2), Raphael Troncy (1) ((1) Eurecom, (2) Amadeus\n  SAS)", "title": "Many-to-one Recurrent Neural Network for Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the D2KLab team's approach to the RecSys Challenge 2019\nwhich focuses on the task of recommending accommodations based on user\nsessions. What is the feeling of a person who says \"Rooms of the hotel are\nenormous, staff are friendly and efficient\"? It is positive. Similarly to the\nsequence of words in a sentence where one can affirm what the feeling is,\nanalysing a sequence of actions performed by a user in a website can lead to\npredict what will be the item the user will add to his basket at the end of the\nshopping session. We propose to use a many-to-one recurrent neural network that\nlearns the probability that a user will click on an accommodation based on the\nsequence of actions he has performed during his browsing session. More\nspecifically, we combine a rule-based algorithm with a Gated Recurrent Unit RNN\nin order to sort the list of accommodations that is shown to the user. We\noptimized the RNN on a validation set, tuning the hyper-parameters such as the\nlearning rate, the batch-size and the accommodation embedding size. This\nanalogy with the sentiment analysis task gives promising results. However, it\nis computationally demanding in the training phase and it needs to be further\ntuned.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:07:23 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Dadoun", "Amine", "", "1 and 2"], ["Troncy", "Raphael", ""]]}, {"id": "2008.11238", "submitter": "Vishal Dey", "authors": "Vishal Dey, Peter Krasniak, Minh Nguyen, Clara Lee and Xia Ning", "title": "Understanding Breast Implant Illness via Social Media Data Analysis", "comments": "22 pages (including a cover page, acknowledgements, references), 1\n  figure, 4 tables. The manuscript is submitted to the Journal of the American\n  Medical Informatics Association and is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Breast implants have been increasingly popular over the last 20\nyears. There have been growing concerns with the risks of breast implants.\nMeanwhile, media phenomenon called \"breast implant illness\" (BII) has emerged.\nObjective: To identify and summarize key attributes of BII using social media\ndata. Materials and Methods: We conducted social media data analysis to better\nunderstand the symptoms, signs, etc., that are associated with BII using\nNatural Language Processing (NLP) and topic modeling. We extracted mentions\nrelated to signs/symptoms, diseases/disorders and medical procedures using the\nClinical Text Analysis and Knowledge Extraction System (cTAKES). Extracted\nmentions are mapped to standard medical concepts. We summarized mapped concepts\nto topics using Latent Dirichlet Allocation (LDA). Results: Our analysis\nidentified topics related to toxicity, cancer and mental health issues that are\nhighly associated with breast implant illness. We also identified pains and\nother disorders commonly associated with breast implant illness. Discussion:\nOur analysis suggests that breast implant illness can possibly lead to serious\nhealth issues such as autoimmune disorders, cancer, pain, fatigue. We also find\nthat toxicity from silicone implants and mental health concerns are some\nunderlying factors in BII. Our study could inspire future work to further study\nthe suggested symptoms and factors of BII. Conclusion: Our analysis over social\nmedia data identifies mentions such as rupture, infection, pain and fatigue\nthat are considered common self-reported issues among the public. Our analysis\nalso shows that cancers, autoimmune disorders and mental health problems are\nemerging concerns, albeit less studied for breast implants.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 19:00:25 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Dey", "Vishal", ""], ["Krasniak", "Peter", ""], ["Nguyen", "Minh", ""], ["Lee", "Clara", ""], ["Ning", "Xia", ""]]}, {"id": "2008.11290", "submitter": "Athar Sefid", "authors": "Athar Sefid, Clyde Lee Giles, Prasenjit Mitra", "title": "Extractive Summarizer for Scholarly Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extractive method that will summarize long scientific papers.\nOur model uses presentation slides provided by the authors of the papers as the\ngold summary standard to label the sentences. The sentences are ranked based on\ntheir novelty and their importance as estimated by deep neural networks. Our\nwindow-based extractive labeling of sentences results in the improvement of at\nleast 4 ROUGE1-Recall points.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:08:34 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Sefid", "Athar", ""], ["Giles", "Clyde Lee", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "2008.11400", "submitter": "Flora D. Salim", "authors": "Manpreet Kaur, Flora D. Salim, Yongli Ren, Jeffrey Chan, Martin Tomko,\n  Mark Sanderson", "title": "Joint Modelling of Cyber Activities and Physical Context to Improve\n  Prediction of Visitor Behaviors", "comments": "Accepted in ACM Transactions on Sensor Networks, 2020", "journal-ref": "ACM Transactions on Sensor Networks, 2020", "doi": "10.1145/3393692", "report-no": null, "categories": "cs.IR cs.DC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the Cyber-Physical behavior of users in a large\nindoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and\nbrowsing logs recorded by the mall operators. Our analysis shows that many\nusers exhibit a high correlation between their cyber activities and their\nphysical context. To find this correlation, we propose a mechanism to\nsemantically label a physical space with rich categorical information from\nDBPedia concepts and compute a contextual similarity that represents a user's\nactivities with the mall context. We demonstrate the application of\ncyber-physical contextual similarity in two situations: user visit intent\nclassification and future location prediction. The experimental results\ndemonstrate that exploitation of contextual similarity significantly improves\nthe accuracy of such applications.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 06:37:43 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Kaur", "Manpreet", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Chan", "Jeffrey", ""], ["Tomko", "Martin", ""], ["Sanderson", "Mark", ""]]}, {"id": "2008.11432", "submitter": "Mar\\'ia N. Moreno Garc\\'ia", "authors": "Diego S\\'anchez-Moreno, Yong Zheng and Mar\\'ia N. Moreno-Garc\\'ia", "title": "Time-Aware Music Recommender Systems: Modeling the Evolution of Implicit\n  User Preferences and User Listening Habits in A Collaborative Filtering\n  Approach", "comments": null, "journal-ref": "Applied Sciences, 10(15), 5324, 33 pages, 2020", "doi": "10.3390/app10155324", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online streaming services have become the most popular way of listening to\nmusic. The majority of these services are endowed with recommendation\nmechanisms that help users to discover songs and artists that may interest them\nfrom the vast amount of music available. However, many are not reliable as they\nmay not take into account contextual aspects or the ever-evolving user\nbehavior. Therefore, it is necessary to develop systems that consider these\naspects. In the field of music, time is one of the most important factors\ninfluencing user preferences and managing its effects, and is the motivation\nbehind the work presented in this paper. Here, the temporal information\nregarding when songs are played is examined. The purpose is to model both the\nevolution of user preferences in the form of evolving implicit ratings and user\nlistening behavior. In the collaborative filtering method proposed in this\nwork, daily listening habits are captured in order to characterize users and\nprovide them with more reliable recommendations. The results of the validation\nprove that this approach outperforms other methods in generating both\ncontext-aware and context-free recommendations\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:00:11 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["S\u00e1nchez-Moreno", "Diego", ""], ["Zheng", "Yong", ""], ["Moreno-Garc\u00eda", "Mar\u00eda N.", ""]]}, {"id": "2008.11567", "submitter": "Jieming Zhu", "authors": "Kelong Mao, Xi Xiao, Jieming Zhu, Biao Lu, Ruiming Tang, Xiuqiang He", "title": "Item Tagging for Information Retrieval: A Tripartite Graph Neural\n  Network based Approach", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging has been recognized as a successful practice to boost relevance\nmatching for information retrieval (IR), especially when items lack rich\ntextual descriptions. A lot of research has been done for either multi-label\ntext categorization or image annotation. However, there is a lack of published\nwork that targets at item tagging specifically for IR. Directly applying a\ntraditional multi-label classification model for item tagging is sub-optimal,\ndue to the ignorance of unique characteristics in IR. In this work, we propose\nto formulate item tagging as a link prediction problem between item nodes and\ntag nodes. To enrich the representation of items, we leverage the query logs\navailable in IR tasks, and construct a query-item-tag tripartite graph. This\nformulation results in a TagGNN model that utilizes heterogeneous graph neural\nnetworks with multiple types of nodes and edges. Different from previous\nresearch, we also optimize both full tag prediction and partial tag completion\ncases in a unified framework via a primary-dual loss mechanism. Experimental\nresults on both open and industrial datasets show that our TagGNN approach\noutperforms the state-of-the-art multi-label classification approaches.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:58:19 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Mao", "Kelong", ""], ["Xiao", "Xi", ""], ["Zhu", "Jieming", ""], ["Lu", "Biao", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2008.11701", "submitter": "Alberto Casagrande", "authors": "Alberto Casagrande, Francesco Fabris, Rossano Girometti", "title": "Computing Information Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agreement measures are useful to both compare different evaluations of the\nsame diagnostic outcomes and validate new rating systems or devices.\nInformation Agreement (IA) is an information-theoretic-based agreement measure\nintroduced to overcome all the limitations and alleged pitfalls of Cohen's\nKappa. However, it is only able to deal with agreement matrices whose values\nare positive natural numbers. This work extends IA admitting also 0 as a\npossible value for the agreement matrix cells.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:41:33 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Casagrande", "Alberto", ""], ["Fabris", "Francesco", ""], ["Girometti", "Rossano", ""]]}, {"id": "2008.11858", "submitter": "Jes\\'us S\\'anchez Cuadrado", "authors": "Jos\\'e Antonio Hern\\'andez L\\'opez, Jes\\'us S\\'anchez Cuadrado", "title": "MAR: A structure-based search engine for models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of shared software models provides opportunities for\nreusing, adapting and learning from them. Public models are typically stored in\na variety of locations, including model repositories, regular source code\nrepositories, web pages, etc. To profit from them developers need effective\nsearch mechanisms to locate the models relevant for their tasks. However, to\ndate, there has been little success in creating a generic and efficient search\nengine specially tailored to the modelling domain.\n  In this paper we present MAR, a search engine for models. MAR is generic in\nthe sense that it can index any type of model if its meta-model is known. MAR\nuses a query-by-example approach, that is, it uses example models as queries.\nThe search takes the model structure into account using the notion of bag of\npaths, which encodes the structure of a model using paths between model\nelements and is a representation amenable for indexing. MAR is built over HBase\nusing a specific design to deal with large repositories. Our benchmarks show\nthat the engine is efficient and has fast response times in most cases. We have\nalso evaluated the precision of the search engine by creating model mutants\nwhich simulate user queries. A REST API is available to perform queries and an\nEclipse plug-in allows end users to connect to the search engine from model\neditors. We have currently indexed more than 50.000 models of different kinds,\nincluding Ecore meta-models, BPMN diagrams and UML models. MAR is available at\nhttp://mar-search.org.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 23:30:20 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["L\u00f3pez", "Jos\u00e9 Antonio Hern\u00e1ndez", ""], ["Cuadrado", "Jes\u00fas S\u00e1nchez", ""]]}, {"id": "2008.11897", "submitter": "Fahimeh Rezazadegan", "authors": "Dana Rezazadegan, Shlomo Berkovsky, Juan C. Quiroz, A. Baki Kocaballi,\n  Ying Wang, Liliana Laranjo, Enrico Coiera", "title": "Automatic Speech Summarisation: A Scoping Review", "comments": "21 Pages, excluding Supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech summarisation techniques take human speech as input and then output an\nabridged version as text or speech. Speech summarisation has applications in\nmany domains from information technology to health care, for example improving\nspeech archives or reducing clinical documentation burden. This scoping review\nmaps the speech summarisation literature, with no restrictions on time frame,\nlanguage summarised, research method, or paper type. We reviewed a total of 110\npapers out of a set of 153 found through a literature search and extracted\nspeech features used, methods, scope, and training corpora. Most studies employ\none of four speech summarisation architectures: (1) Sentence extraction and\ncompaction; (2) Feature extraction and classification or rank-based sentence\nselection; (3) Sentence compression and compression summarisation; and (4)\nLanguage modelling. We also discuss the strengths and weaknesses of these\ndifferent methods and speech features. Overall, supervised methods (e.g. Hidden\nMarkov support vector machines, Ranking support vector machines, Conditional\nrandom fields) performed better than unsupervised methods. As supervised\nmethods require manually annotated training data which can be costly, there was\nmore interest in unsupervised methods. Recent research into unsupervised\nmethods focusses on extending language modelling, for example by combining\nUni-gram modelling with deep neural networks. Protocol registration: The\nprotocol for this scoping review is registered at https://osf.io.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:15:40 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Rezazadegan", "Dana", ""], ["Berkovsky", "Shlomo", ""], ["Quiroz", "Juan C.", ""], ["Kocaballi", "A. Baki", ""], ["Wang", "Ying", ""], ["Laranjo", "Liliana", ""], ["Coiera", "Enrico", ""]]}, {"id": "2008.11908", "submitter": "Nasser Ghadiri", "authors": "Ensieh Davoodijam, Nasser Ghadiri, Maryam Lotfi Shahreza, Fabio\n  Rinaldi", "title": "MultiGBS: A multi-layer graph approach to biomedical summarization", "comments": null, "journal-ref": "Journal of Biomedical Informatics, Available online 18 February\n  2021, 103706", "doi": "10.1016/j.jbi.2021.103706", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization methods generate a shorter version of the input\ntext to assist the reader in gaining a quick yet informative gist. Existing\ntext summarization methods generally focus on a single aspect of text when\nselecting sentences, causing the potential loss of essential information. In\nthis study, we propose a domain-specific method that models a document as a\nmulti-layer graph to enable multiple features of the text to be processed at\nthe same time. The features we used in this paper are word similarity, semantic\nsimilarity, and co-reference similarity, which are modelled as three different\nlayers. The unsupervised method selects sentences from the multi-layer graph\nbased on the MultiRank algorithm and the number of concepts. The proposed\nMultiGBS algorithm employs UMLS and extracts the concepts and relationships\nusing different tools such as SemRep, MetaMap, and OGER. Extensive evaluation\nby ROUGE and BERTScore shows increased F-measure values.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 04:22:37 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:24:13 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Davoodijam", "Ensieh", ""], ["Ghadiri", "Nasser", ""], ["Shahreza", "Maryam Lotfi", ""], ["Rinaldi", "Fabio", ""]]}, {"id": "2008.11922", "submitter": "Maxim Naumov", "authors": "Tigran Ishkhanov, Maxim Naumov, Xianjie Chen, Yan Zhu, Yuan Zhong,\n  Alisson Gusatti Azzolini, Chonglin Sun, Frank Jiang, Andrey Malevich and\n  Liang Xiong", "title": "Time-based Sequence Model for Personalization and Recommendation Systems", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a novel recommendation model that explicitly\nincorporates time information. The model relies on an embedding layer and TSL\nattention-like mechanism with inner products in different vector spaces, that\ncan be thought of as a modification of multi-headed attention. This mechanism\nallows the model to efficiently treat sequences of user behavior of different\nlength. We study the properties of our state-of-the-art model on statistically\ndesigned data set. Also, we show that it outperforms more complex models with\nlonger sequence length on the Taobao User Behavior dataset.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 05:46:47 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ishkhanov", "Tigran", ""], ["Naumov", "Maxim", ""], ["Chen", "Xianjie", ""], ["Zhu", "Yan", ""], ["Zhong", "Yuan", ""], ["Azzolini", "Alisson Gusatti", ""], ["Sun", "Chonglin", ""], ["Jiang", "Frank", ""], ["Malevich", "Andrey", ""], ["Xiong", "Liang", ""]]}, {"id": "2008.11972", "submitter": "Yang Deng", "authors": "Yang Deng, Wenxuan Zhang, Wai Lam", "title": "Opinion-aware Answer Generation for Review-driven Question Answering in\n  E-Commerce", "comments": "Accepted by CIKM 2020 (Full Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product-related question answering (QA) is an important but challenging task\nin E-Commerce. It leads to a great demand on automatic review-driven QA, which\naims at providing instant responses towards user-posted questions based on\ndiverse product reviews. Nevertheless, the rich information about personal\nopinions in product reviews, which is essential to answer those\nproduct-specific questions, is underutilized in current generation-based\nreview-driven QA studies. There are two main challenges when exploiting the\nopinion information from the reviews to facilitate the opinion-aware answer\ngeneration: (i) jointly modeling opinionated and interrelated information\nbetween the question and reviews to capture important information for answer\ngeneration, (ii) aggregating diverse opinion information to uncover the common\nopinion towards the given question. In this paper, we tackle opinion-aware\nanswer generation by jointly learning answer generation and opinion mining\ntasks with a unified model. Two kinds of opinion fusion strategies, namely,\nstatic and dynamic fusion, are proposed to distill and aggregate important\nopinion information learned from the opinion mining task into the answer\ngeneration process. Then a multi-view pointer-generator network is employed to\ngenerate opinion-aware answers for a given product-related question.\nExperimental results show that our method achieves superior performance in\nreal-world E-Commerce QA datasets, and effectively generate opinionated and\ninformative answers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:54:45 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 05:29:47 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Deng", "Yang", ""], ["Zhang", "Wenxuan", ""], ["Lam", "Wai", ""]]}, {"id": "2008.12039", "submitter": "Angelika Romanou", "authors": "Angelika Romanou, Panayiotis Smeros, Carlos Castillo, Karl Aberer", "title": "SciLens News Platform: A System for Real-Time Evaluation of News\n  Articles", "comments": "Conference demo paper, 4 pages, 5 figures", "journal-ref": "Proceedings of the 46th International Conference on Very Large\n  Data Bases, Tokyo, Japan, Aug 31-Sept 4, 2020", "doi": "10.14778/3415478.3415521", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We demonstrate the SciLens News Platform, a novel system for evaluating the\nquality of news articles. The SciLens News Platform automatically collects\ncontextual information about news articles in real-time and provides quality\nindicators about their validity and trustworthiness. These quality indicators\nderive from i) social media discussions regarding news articles, showcasing the\nreach and stance towards these articles, and ii) their content and their\nreferenced sources, showcasing the journalistic foundations of these articles.\nFurthermore, the platform enables domain-experts to review articles and rate\nthe quality of news sources. This augmented view of news articles, which\ncombines automatically extracted indicators and domain-expert reviews, has\nprovably helped the platform users to have a better consensus about the quality\nof the underlying articles. The platform is built in a distributed and robust\nfashion and runs operationally handling daily thousands of news articles. We\nevaluate the SciLens News Platform on the emerging topic of COVID-19 where we\nhighlight the discrepancies between low and high-quality news outlets based on\nthree axes, namely their newsroom activity, evidence seeking and social\nengagement. A live demonstration of the platform can be found here:\nhttp://scilens.epfl.ch.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 10:35:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Romanou", "Angelika", ""], ["Smeros", "Panayiotis", ""], ["Castillo", "Carlos", ""], ["Aberer", "Karl", ""]]}, {"id": "2008.12193", "submitter": "Geert Heyman", "authors": "Geert Heyman and Tom Van Cutsem", "title": "Neural Code Search Revisited: Enhancing Code Snippet Retrieval through\n  Natural Language Intent", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose and study annotated code search: the retrieval of\ncode snippets paired with brief descriptions of their intent using natural\nlanguage queries. On three benchmark datasets, we investigate how code\nretrieval systems can be improved by leveraging descriptions to better capture\nthe intents of code snippets. Building on recent progress in transfer learning\nand natural language processing, we create a domain-specific retrieval model\nfor code annotated with a natural language description. We find that our model\nyields significantly more relevant search results (with absolute gains up to\n20.6% in mean reciprocal rank) compared to state-of-the-art code retrieval\nmethods that do not use descriptions but attempt to compute the intent of\nsnippets solely from unannotated code.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 15:39:09 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Heyman", "Geert", ""], ["Van Cutsem", "Tom", ""]]}, {"id": "2008.12353", "submitter": "Connor Heaton", "authors": "Connor T. Heaton, Prasenjit Mitra", "title": "Repurposing TREC-COVID Annotations to Answer the Key Questions of\n  CORD-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus disease 2019 (COVID-19) began in Wuhan, China in late\n2019 and to date has infected over 14M people worldwide, resulting in over\n750,000 deaths. On March 10, 2020 the World Health Organization (WHO) declared\nthe outbreak a global pandemic. Many academics and researchers, not restricted\nto the medical domain, began publishing papers describing new discoveries.\nHowever, with the large influx of publications, it was hard for these\nindividuals to sift through the large amount of data and make sense of the\nfindings. The White House and a group of industry research labs, lead by the\nAllen Institute for AI, aggregated over 200,000 journal articles related to a\nvariety of coronaviruses and tasked the community with answering key questions\nrelated to the corpus, releasing the dataset as CORD-19. The information\nretrieval (IR) community repurposed the journal articles within CORD-19 to more\nclosely resemble a classic TREC-style competition, dubbed TREC-COVID, with\nhuman annotators providing relevancy judgements at the end of each round of\ncompetition. Seeing the related endeavors, we set out to repurpose the\nrelevancy annotations for TREC-COVID tasks to identify journal articles in\nCORD-19 which are relevant to the key questions posed by CORD-19. A BioBERT\nmodel trained on this repurposed dataset prescribes relevancy annotations for\nCORD-19 tasks that have an overall agreement of 0.4430 with majority human\nannotations in terms of Cohen's kappa. We present the methodology used to\nconstruct the new dataset and describe the decision process used throughout.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 19:51:07 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Heaton", "Connor T.", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "2008.12435", "submitter": "Md Abul Bashar", "authors": "Md Abul Bashar, Richi Nayak, Thirunavukarasu Balasubramaniam", "title": "Topic, Sentiment and Impact Analysis: COVID19 Information Seeking on\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people notice something unusual, they discuss it on social media. They\nleave traces of their emotions via text expressions. A systematic collection,\nanalysis, and interpretation of social media data across time and space can\ngive insights on local outbreaks, mental health, and social issues. Such timely\ninsights can help in developing strategies and resources with an appropriate\nand efficient response. This study analysed a large Spatio-temporal tweet\ndataset of the Australian sphere related to COVID19. The methodology included a\nvolume analysis, dynamic topic modelling, sentiment detection, and semantic\nbrand score to obtain an insight on the COVID19 pandemic outbreak and public\ndiscussion in different states and cities of Australia over time. The obtained\ninsights are compared with independently observed phenomena such as government\nreported instances.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 02:03:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Bashar", "Md Abul", ""], ["Nayak", "Richi", ""], ["Balasubramaniam", "Thirunavukarasu", ""]]}, {"id": "2008.12916", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos", "title": "Random Surfing Revisited: Generalizing PageRank's Teleportation Model", "comments": "34 pages, 11 figures; Preliminary versions of (part of) this work\n  have been presented as an ACM WSDM conference paper\n  (https://dl.acm.org/doi/10.1145/2433396.2433415), as well as in\n  arXiv:1506.00092 (This article supersedes arXiv:1506.00092) v2: corrected\n  numerical example", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We revisit the Random Surfer model, focusing on its--often\noverlooked--Teleportation component, and we introduce NCDawareRank; a novel\nranking framework designed to exploit network meta-information as well as\naspects of its higher-order structural organization in a way that preserves the\nmathematical structure and the attractive computational characteristics of\nPageRank. A rigorous theoretical exploration of the proposed model reveals a\nwealth of mathematical properties that entail tangible benefits in terms of\nrobustness, computability, as well as modeling flexibility and expressiveness.\nA set of experiments on real-work networks verify the theoretically predicted\nproperties of NCDawareRank, and showcase its effectiveness as a network\ncentrality measure.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 05:45:05 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 03:06:05 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""]]}, {"id": "2008.13078", "submitter": "Peter Sheridan Dodds", "authors": "P. S. Dodds, J. R. Minot, M. V. Arnold, T. Alshaabi, J. L. Adams, D.\n  R. Dewhurst, A. J. Reagan, C. M. Danforth", "title": "Probability-turbulence divergence: A tunable allotaxonometric instrument\n  for comparing heavy-tailed categorical distributions", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world complex systems often comprise many distinct types of elements as\nwell as many more types of networked interactions between elements. When the\nrelative abundances of types can be measured well, we further observe\nheavy-tailed categorical distributions for type frequencies. For the comparison\nof type frequency distributions of two systems or a system with itself at\ndifferent time points in time -- a facet of allotaxonometry -- a great range of\nprobability divergences are available. Here, we introduce and explore\n`probability-turbulence divergence', a tunable, straightforward, and\ninterpretable instrument for comparing normalizable categorical frequency\ndistributions. We model probability-turbulence divergence (PTD) after\nrank-turbulence divergence (RTD). While probability-turbulence divergence is\nmore limited in application than rank-turbulence divergence, it is more\nsensitive to changes in type frequency. We build allotaxonographs to display\nprobability turbulence, incorporating a way to visually accommodate zero\nprobabilities for `exclusive types' which are types that appear in only one\nsystem. We explore comparisons of example distributions taken from literature,\nsocial media, and ecology. We show how probability-turbulence divergence either\nexplicitly or functionally generalizes many existing kinds of distances and\nmeasures, including, as special cases, $L^{(p)}$ norms, the S{\\o}rensen-Dice\ncoefficient (the $F_1$ statistic), and the Hellinger distance. We discuss\nsimilarities with the generalized entropies of R{\\'e}nyi and Tsallis, and the\ndiversity indices (or Hill numbers) from ecology. We close with thoughts on\nopen problems concerning the optimization of the tuning of rank- and\nprobability-turbulence divergence.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 02:57:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Dodds", "P. S.", ""], ["Minot", "J. R.", ""], ["Arnold", "M. V.", ""], ["Alshaabi", "T.", ""], ["Adams", "J. L.", ""], ["Dewhurst", "D. R.", ""], ["Reagan", "A. J.", ""], ["Danforth", "C. M.", ""]]}, {"id": "2008.13121", "submitter": "Tom Tabak", "authors": "Tom Tabak and Matthew Purver", "title": "Temporal Mental Health Dynamics on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of experiments for building a temporal mental health\ndynamics system. We utilise a pre-existing methodology for distant-supervision\nof mental health data mining from social media platforms and deploy the system\nduring the global COVID-19 pandemic as a case study. Despite the challenging\nnature of the task, we produce encouraging results, both explicit to the global\npandemic and implicit to a global phenomenon, Christmas Depression, supported\nby the literature. We propose a methodology for providing insight into temporal\nmental health dynamics to be utilised for strategic decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 09:05:11 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 16:50:41 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 12:31:13 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Tabak", "Tom", ""], ["Purver", "Matthew", ""]]}, {"id": "2008.13141", "submitter": "Hyunsung Lee", "authors": "Hyunsung Lee, Yeongjae Jang, Jaekwang Kim and Honguk Woo", "title": "A Differentiable Ranking Metric Using Relaxed Sorting Operation for\n  Top-K Recommender Systems", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system generates personalized recommendations for a user by\ncomputing the preference score of items, sorting the items according to the\nscore, and filtering top-K items with high scores. While sorting and ranking\nitems are integral for this recommendation procedure, it is nontrivial to\nincorporate them in the process of end-to-end model training since sorting is\nnondifferentiable and hard to optimize with gradient descent. This incurs the\ninconsistency issue between existing learning objectives and ranking metrics of\nrecommenders. In this work, we present DRM (differentiable ranking metric) that\nmitigates the inconsistency and improves recommendation performance by\nemploying the differentiable relaxation of ranking metrics. Via experiments\nwith several real-world datasets, we demonstrate that the joint learning of the\nDRM objective upon existing factor based recommenders significantly improves\nthe quality of recommendations, in comparison with other state-of-the-art\nrecommendation methods.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 10:57:33 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 13:53:24 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 01:42:09 GMT"}, {"version": "v4", "created": "Sat, 5 Dec 2020 10:44:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lee", "Hyunsung", ""], ["Jang", "Yeongjae", ""], ["Kim", "Jaekwang", ""], ["Woo", "Honguk", ""]]}, {"id": "2008.13225", "submitter": "Tharun Kumar Reddy Medini", "authors": "Tharun Medini, Beidi Chen, Anshumali Shrivastava", "title": "SOLAR: Sparse Orthogonal Learned and Random Embeddings", "comments": "Under review at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense embedding models are commonly deployed in commercial search engines,\nwherein all the document vectors are pre-computed, and near-neighbor search\n(NNS) is performed with the query vector to find relevant documents. However,\nthe bottleneck of indexing a large number of dense vectors and performing an\nNNS hurts the query time and accuracy of these models. In this paper, we argue\nthat high-dimensional and ultra-sparse embedding is a significantly superior\nalternative to dense low-dimensional embedding for both query efficiency and\naccuracy. Extreme sparsity eliminates the need for NNS by replacing them with\nsimple lookups, while its high dimensionality ensures that the embeddings are\ninformative even when sparse. However, learning extremely high dimensional\nembeddings leads to blow up in the model size. To make the training feasible,\nwe propose a partitioning algorithm that learns such high dimensional\nembeddings across multiple GPUs without any communication. This is facilitated\nby our novel asymmetric mixture of Sparse, Orthogonal, Learned and Random\n(SOLAR) Embeddings. The label vectors are random, sparse, and near-orthogonal\nby design, while the query vectors are learned and sparse. We theoretically\nprove that our way of one-sided learning is equivalent to learning both query\nand label embeddings. With these unique properties, we can successfully train\n500K dimensional SOLAR embeddings for the tasks of searching through 1.6M books\nand multi-label classification on the three largest public datasets. We achieve\nsuperior precision and recall compared to the respective state-of-the-art\nbaselines for each of the tasks with up to 10 times faster speed.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:35:35 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Medini", "Tharun", ""], ["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2008.13281", "submitter": "Makbule Gulcin Ozsoy", "authors": "Makbule Gulcin Ozsoy", "title": "Beyond Next Item Recommendation: Recommending and Evaluating List of\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS) suggest items-based on the estimated preferences of\nusers. Recent RS methods utilise vector space embeddings and deep learning\nmethods to make efficient recommendations. However, most of these methods\noverlook the sequentiality feature and consider each interaction, e.g.,\ncheck-in, independent from each other. The proposed method considers the\nsequentiality of the interactions of users with items and uses them to make\nrecommendations of a list of multi-item sequences. The proposed method uses\nFastText \\cite{bojanowski2016enriching}, a well-known technique in natural\nlanguage processing (NLP), to model the relationship among the subunits of\nsequences, e.g., tracks, playlists, and utilises the trained representation as\nan input to a traditional recommendation method. The recommended lists of\nmulti-item sequences are evaluated by the ROUGE\n\\cite{lin2003automatic,lin2004rouge} metric, which is also commonly used in the\nNLP literature. The current experimental results reveal that it is possible to\nrecommend a list of multi-item sequences, in addition to the traditional next\nitem recommendation. Also, the usage of FastText, which utilise sub-units of\nthe input sequences, helps to overcome cold-start user problem.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 21:30:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ozsoy", "Makbule Gulcin", ""]]}, {"id": "2008.13298", "submitter": "Nirmit Desai", "authors": "Shalisha Witherspoon, Dean Steuer, Graham Bent, Nirmit Desai", "title": "SEEC: Semantic Vector Federation across Edge Computing Environments", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic vector embedding techniques have proven useful in learning semantic\nrepresentations of data across multiple domains. A key application enabled by\nsuch techniques is the ability to measure semantic similarity between given\ndata samples and find data most similar to a given sample. State-of-the-art\nembedding approaches assume all data is available on a single site. However, in\nmany business settings, data is distributed across multiple edge locations and\ncannot be aggregated due to a variety of constraints. Hence, the applicability\nof state-of-the-art embedding approaches is limited to freely shared datasets,\nleaving out applications with sensitive or mission-critical data. This paper\naddresses this gap by proposing novel unsupervised algorithms called\n\\emph{SEEC} for learning and applying semantic vector embedding in a variety of\ndistributed settings. Specifically, for scenarios where multiple edge locations\ncan engage in joint learning, we adapt the recently proposed federated learning\ntechniques for semantic vector embedding. Where joint learning is not possible,\nwe propose novel semantic vector translation algorithms to enable semantic\nquery across multiple edge locations, each with its own semantic vector-space.\nExperimental results on natural language as well as graph datasets show that\nthis may be a promising new direction.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 23:51:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Witherspoon", "Shalisha", ""], ["Steuer", "Dean", ""], ["Bent", "Graham", ""], ["Desai", "Nirmit", ""]]}, {"id": "2008.13335", "submitter": "Thanh Tran", "authors": "Thanh Tran, Di You, Kyumin Lee", "title": "Quaternion-Based Self-Attentive Long Short-Term User Preference Encoding\n  for Recommendation", "comments": null, "journal-ref": "CIKM 2020", "doi": "10.1145/3340531.3411926", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quaternion space has brought several benefits over the traditional Euclidean\nspace: Quaternions (i) consist of a real and three imaginary components,\nencouraging richer representations; (ii) utilize Hamilton product which better\nencodes the inter-latent interactions across multiple Quaternion components;\nand (iii) result in a model with smaller degrees of freedom and less prone to\noverfitting. Unfortunately, most of the current recommender systems rely on\nreal-valued representations in Euclidean space to model either user's long-term\nor short-term interests. In this paper, we fully utilize Quaternion space to\nmodel both user's long-term and short-term preferences. We first propose a\nQUaternion-based self-Attentive Long term user Encoding (QUALE) to study the\nuser's long-term intents. Then, we propose a QUaternion-based self-Attentive\nShort term user Encoding (QUASE) to learn the user's short-term interests. To\nenhance our models' capability, we propose to fuse QUALE and QUASE into one\nmodel, namely QUALSE, by using a Quaternion-based gating mechanism. We further\ndevelop Quaternion-based Adversarial learning along with the Bayesian\nPersonalized Ranking (QABPR) to improve our model's robustness. Extensive\nexperiments on six real-world datasets show that our fused QUALSE model\noutperformed 11 state-of-the-art baselines, improving 8.43% at HIT@1 and 10.27%\nat NDCG@1 on average compared with the best baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:22:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tran", "Thanh", ""], ["You", "Di", ""], ["Lee", "Kyumin", ""]]}, {"id": "2008.13368", "submitter": "Hai-Tao Yu", "authors": "Hai-Tao Yu", "title": "PT-Ranking: A Benchmarking Platform for Neural Learning-to-Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks has become the first choice for researchers working on\nalgorithmic aspects of learning-to-rank. Unfortunately, it is not trivial to\nfind the optimal setting of hyper-parameters that achieves the best ranking\nperformance. As a result, it becomes more and more difficult to develop a new\nmodel and conduct a fair comparison with prior methods, especially for\nnewcomers. In this work, we propose PT-Ranking, an open-source project based on\nPyTorch for developing and evaluating learning-to-rank methods using deep\nneural networks as the basis to construct a scoring function. On one hand,\nPT-Ranking includes many representative learning-to-rank methods. Besides the\ntraditional optimization framework via empirical risk minimization, adversarial\noptimization framework is also integrated. Furthermore, PT-Ranking's modular\ndesign provides a set of building blocks that users can leverage to develop new\nranking models. On the other hand, PT-Ranking supports to compare different\nlearning-to-rank methods based on the widely used datasets (e.g., MSLR-WEB30K,\nYahoo!LETOR and Istella LETOR) in terms of different metrics, such as\nprecision, MAP, nDCG, nERR. By randomly masking the ground-truth labels with a\nspecified ratio, PT-Ranking allows to examine to what extent the ratio of\nunlabelled query-document pairs affects the performance of different\nlearning-to-rank methods. We further conducted a series of demo experiments to\nclearly show the effect of different factors on neural learning-to-rank\nmethods, such as the activation function, the number of layers and the\noptimization strategy.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:12:54 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Yu", "Hai-Tao", ""]]}, {"id": "2008.13373", "submitter": "Hai-Tao Yu", "authors": "Hai-Tao Yu", "title": "Optimize What You Evaluate With: A Simple Yet Effective Framework For\n  Direct Optimization Of IR Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-to-rank has been intensively studied and has shown significantly\nincreasing values in a wide range of domains. The performance of\nlearning-to-rank methods is commonly evaluated using rank-sensitive metrics,\nsuch as average precision (AP) and normalized discounted cumulative gain\n(nDCG). Unfortunately, how to effectively optimize rank-sensitive objectives is\nfar from being resolved, which has been an open problem since the dawn of\nlearning-to-rank over a decade ago. In this paper, we introduce a simple yet\neffective framework for directly optimizing information retrieval (IR) metrics.\nSpecifically, we propose a novel twin-sigmoid function for deriving the exact\nrank positions of documents during the optimization process instead of using\napproximated rank positions or relying on the traditional sorting algorithms.\nThanks to this, the rank positions are differentiable, enabling us to\nreformulate the widely used IR metrics as differentiable ones and directly\noptimize them based on neural networks. Furthermore, by carrying out an\nin-depth analysis of the gradients, we pinpoint the potential limitations\ninherent with direct optimization of IR metrics based on the vanilla sigmoid.\nTo break the limitations, we propose different strategies by explicitly\nmodifying the gradient computation. To validate the effectiveness of the\nproposed framework for direct optimization of IR metrics, we conduct a series\nof experiments on the widely used benchmark collection MSLRWEB30K.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:31:25 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Yu", "Hai-Tao", ""]]}, {"id": "2008.13405", "submitter": "Seyedmostafa Safavi", "authors": "Seyedmostafa Safavi, Zarina Shukur", "title": "CenterYou: A cloud-based Approach to Simplify Android Privacy Management", "comments": "14 Figures, 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With mobile applications and associated services becoming increasingly\npopular, concerns are being raised about private data leakages have raised.\nPrevious solutions to this well-known set of problems have approached it from\nthe ground up but required rewriting the operating system which is unnecessary\nand burdensome. In this work, a framework we proposed to overcome these issues\nby applying a pseudo data technique and cloud-based decision-making system to\nidentify potential privacy leakages.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 07:41:58 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Safavi", "Seyedmostafa", ""], ["Shukur", "Zarina", ""]]}, {"id": "2008.13516", "submitter": "Dilruk Perera", "authors": "Dilruk Perera and Roger Zimmermann", "title": "Towards Comprehensive Recommender Systems: Time-Aware\n  UnifiedcRecommendations Based on Listwise Ranking of Implicit Cross-Network\n  Data", "comments": null, "journal-ref": "Association for the Advancement of Artificial Intelligence, 2020\n  (AAAI'20)", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of information in web applications make recommendation\nessential for users as well as applications. Despite the effectiveness of\nexisting recommender systems, we find two major limitations that reduce their\noverall performance: (1) inability to provide timely recommendations for both\nnew and existing users by considering the dynamic nature of user preferences,\nand (2) not fully optimized for the ranking task when using implicit feedback.\nTherefore, we propose a novel deep learning based unified cross-network\nsolution to mitigate cold-start and data sparsity issues and provide timely\nrecommendations for new and existing users.Furthermore, we consider the ranking\nproblem under implicit feedback as a classification task, and propose a generic\npersonalized listwise optimization criterion for implicit data to effectively\nrank a list of items. We illustrate our cross-network model using Twitter\nauxiliary information for recommendations on YouTube target network. Extensive\ncomparisons against multiple time aware and cross-network base-lines show that\nthe proposed solution is superior in terms of accuracy, novelty and diversity.\nFurthermore, experiments conducted on the popular MovieLens dataset suggest\nthat the proposed listwise ranking method outperforms existing state-of-the-art\nranking techniques.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 08:08:03 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Perera", "Dilruk", ""], ["Zimmermann", "Roger", ""]]}, {"id": "2008.13517", "submitter": "Yishi Xu", "authors": "Yishi Xu, Yingxue Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, Mark\n  Coates", "title": "GraphSAIL: Graph Structure Aware Incremental Learning for Recommender\n  Systems", "comments": "Accepted by CIKM2020 Applied Research Track", "journal-ref": null, "doi": "10.1145/3340531.3412754", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the convenience of collecting information through online services,\nrecommender systems now consume large scale data and play a more important role\nin improving user experience. With the recent emergence of Graph Neural\nNetworks (GNNs), GNN-based recommender models have shown the advantage of\nmodeling the recommender system as a user-item bipartite graph to learn\nrepresentations of users and items. However, such models are expensive to train\nand difficult to perform frequent updates to provide the most up-to-date\nrecommendations. In this work, we propose to update GNN-based recommender\nmodels incrementally so that the computation time can be greatly reduced and\nmodels can be updated more frequently. We develop a Graph Structure Aware\nIncremental Learning framework, GraphSAIL, to address the commonly experienced\ncatastrophic forgetting problem that occurs when training a model in an\nincremental fashion. Our approach preserves a user's long-term preference (or\nan item's long-term property) during incremental model updating. GraphSAIL\nimplements a graph structure preservation strategy which explicitly preserves\neach node's local structure, global structure, and self-information,\nrespectively. We argue that our incremental training framework is the first\nattempt tailored for GNN based recommender systems and demonstrate its\nimprovement compared to other incremental learning techniques on two public\ndatasets. We further verify the effectiveness of our framework on a large-scale\nindustrial dataset.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:33:59 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 02:56:15 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Xu", "Yishi", ""], ["Zhang", "Yingxue", ""], ["Guo", "Wei", ""], ["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["Coates", "Mark", ""]]}, {"id": "2008.13526", "submitter": "Sami Khenissi", "authors": "Sami Khenissi and Mariem Boujelbene and Olfa Nasraoui", "title": "Theoretical Modeling of the Iterative Properties of User Discovery in a\n  Collaborative Filtering Recommender System", "comments": "Accepted in Recsys2020. Code available at:\n  https://github.com/samikhenissi/TheoretUserModeling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The closed feedback loop in recommender systems is a common setting that can\nlead to different types of biases. Several studies have dealt with these biases\nby designing methods to mitigate their effect on the recommendations. However,\nmost existing studies do not consider the iterative behavior of the system\nwhere the closed feedback loop plays a crucial role in incorporating different\nbiases into several parts of the recommendation steps.\n  We present a theoretical framework to model the asymptotic evolution of the\ndifferent components of a recommender system operating within a feedback loop\nsetting, and derive theoretical bounds and convergence properties on\nquantifiable measures of the user discovery and blind spots. We also validate\nour theoretical findings empirically using a real-life dataset and empirically\ntest the efficiency of a basic exploration strategy within our theoretical\nframework.\n  Our findings lay the theoretical basis for quantifying the effect of feedback\nloops and for designing Artificial Intelligence and machine learning algorithms\nthat explicitly incorporate the iterative nature of feedback loops in the\nmachine learning and recommendation process.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 20:30:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Khenissi", "Sami", ""], ["Boujelbene", "Mariem", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "2008.13527", "submitter": "Zhimeng Pan", "authors": "Zhimeng Pan, Wenzheng Tao, Qingyao Ai", "title": "Review Regularized Neural Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, text-aware collaborative filtering methods have been\nproposed to address essential challenges in recommendations such as data\nsparsity, cold start problem, and long-tail distribution. However, many of\nthese text-oriented methods rely heavily on the availability of text\ninformation for every user and item, which obviously does not hold in\nreal-world scenarios. Furthermore, specially designed network structures for\ntext processing are highly inefficient for on-line serving and are hard to\nintegrate into current systems. In this paper, we propose a flexible neural\nrecommendation framework, named Review Regularized Recommendation, short as R3.\nIt consists of a neural collaborative filtering part that focuses on prediction\noutput, and a text processing part that serves as a regularizer. This modular\ndesign incorporates text information as richer data sources in the training\nphase while being highly friendly for on-line serving as it needs no on-the-fly\ntext processing in serving time. Our preliminary results show that by using a\nsimple text processing approach, it could achieve better prediction performance\nthan state-of-the-art text-aware methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:54:27 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Pan", "Zhimeng", ""], ["Tao", "Wenzheng", ""], ["Ai", "Qingyao", ""]]}, {"id": "2008.13528", "submitter": "Scott Graham", "authors": "Scott Graham, Jun-Ki Min and Tao Wu", "title": "Microsoft Recommenders: Tools to Accelerate Developing Recommender\n  Systems", "comments": "pages: 2; submitted to: RecSys '19", "journal-ref": "Proceedings of the 13th ACM Conference on Recommender Systems\n  (2019)", "doi": "10.1145/3298689.3346967", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work is to highlight the content of the Microsoft\nRecommenders repository and show how it can be used to reduce the time involved\nin developing recommender systems. The open source repository provides python\nutilities to simplify common recommender-related data science work as well as\nexample Jupyter notebooks that demonstrate use of the algorithms and tools\nunder various environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:25:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Graham", "Scott", ""], ["Min", "Jun-Ki", ""], ["Wu", "Tao", ""]]}, {"id": "2008.13532", "submitter": "Rohan Anand", "authors": "Rohan Anand and Joeran Beel", "title": "Auto-Surprise: An Automated Recommender-System (AutoRecSys) Library with\n  Tree of Parzens Estimator (TPE) Optimization", "comments": "To be presented at RecSys '20 Fourteenth ACM Conference on\n  Recommender Systems, September 21-26, 2020, Virtual Event", "journal-ref": null, "doi": "10.1145/3383313.3411467", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Auto-Surprise, an Automated Recommender System library.\nAuto-Surprise is an extension of the Surprise recommender system library and\neases the algorithm selection and configuration process. Compared to\nout-of-the-box Surprise library, Auto-Surprise performs better when evaluated\nwith MovieLens, Book Crossing and Jester Datasets. It may also result in the\nselection of an algorithm with significantly lower runtime. Compared to\nSurprise's grid search, Auto-Surprise performs equally well or slightly better\nin terms of RMSE, and is notably faster in finding the optimum hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 08:29:27 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Anand", "Rohan", ""], ["Beel", "Joeran", ""]]}, {"id": "2008.13534", "submitter": "Jiwei Guan", "authors": "Min Fu, Jiwei Guan, Xi Zheng, Jie Zhou, Jianchao Lu, Tianyi Zhang,\n  Shoujie Zhuo, Lijun Zhan, and Jian Yang", "title": "ICS-Assist: Intelligent Customer Inquiry Resolution Recommendation in\n  Online Customer Service for Large E-Commerce Businesses", "comments": "International Conference on Service Oriented Computing (ICSOC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and appropriate online customer service is essential to large\ne-commerce businesses. Existing solution recommendation methods for online\ncustomer service are unable to determine the best solutions at runtime, leading\nto poor satisfaction of end customers. This paper proposes a novel intelligent\nframework, called ICS-Assist, to recommend suitable customer service solutions\nfor service staff at runtime. Specifically, we develop a generalizable\ntwo-stage machine learning model to identify customer service scenarios and\ndetermine customer service solutions based on a scenario-solution mapping\ntable. We implement ICS-Assist and evaluate it using an over 6-month field\nstudy with Alibaba Group. In our experiment, over 12,000 customer service staff\nuse ICS-Assist to serve for over 230,000 cases per day on average. The\nexperimen-tal results show that ICS-Assist significantly outperforms the\ntraditional manual method, and improves the solution acceptance rate, the\nsolution coverage rate, the average service time, the customer satisfaction\nrate, and the business domain catering rate by up to 16%, 25%, 6%, 14% and 17%\nrespectively, compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 01:39:07 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Fu", "Min", ""], ["Guan", "Jiwei", ""], ["Zheng", "Xi", ""], ["Zhou", "Jie", ""], ["Lu", "Jianchao", ""], ["Zhang", "Tianyi", ""], ["Zhuo", "Shoujie", ""], ["Zhan", "Lijun", ""], ["Yang", "Jian", ""]]}, {"id": "2008.13535", "submitter": "Ruoxi Wang", "authors": "Ruoxi Wang, Rakesh Shivanna, Derek Z. Cheng, Sagar Jain, Dong Lin,\n  Lichan Hong, Ed H. Chi", "title": "DCN V2: Improved Deep & Cross Network and Practical Lessons for\n  Web-scale Learning to Rank Systems", "comments": null, "journal-ref": "In Proceedings of the Web Conference 2021 (WWW '21)", "doi": "10.1145/3442381.3450078", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning effective feature crosses is the key behind building recommender\nsystems. However, the sparse and large feature space requires exhaustive search\nto identify effective crosses. Deep & Cross Network (DCN) was proposed to\nautomatically and efficiently learn bounded-degree predictive feature\ninteractions. Unfortunately, in models that serve web-scale traffic with\nbillions of training examples, DCN showed limited expressiveness in its cross\nnetwork at learning more predictive feature interactions. Despite significant\nresearch progress made, many deep learning models in production still rely on\ntraditional feed-forward neural networks to learn feature crosses\ninefficiently.\n  In light of the pros/cons of DCN and existing feature interaction learning\napproaches, we propose an improved framework DCN-V2 to make DCN more practical\nin large-scale industrial settings. In a comprehensive experimental study with\nextensive hyper-parameter search and model tuning, we observed that DCN-V2\napproaches outperform all the state-of-the-art algorithms on popular benchmark\ndatasets. The improved DCN-V2 is more expressive yet remains cost efficient at\nfeature interaction learning, especially when coupled with a mixture of\nlow-rank architecture. DCN-V2 is simple, can be easily adopted as building\nblocks, and has delivered significant offline accuracy and online business\nmetrics gains across many web-scale learning to rank systems at Google.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 20:33:02 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 21:01:21 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Ruoxi", ""], ["Shivanna", "Rakesh", ""], ["Cheng", "Derek Z.", ""], ["Jain", "Sagar", ""], ["Lin", "Dong", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""]]}, {"id": "2008.13537", "submitter": "He Zhao", "authors": "He Zhao, Dinh Phung, Viet Huynh, Trung Le, Wray Buntine", "title": "Neural Topic Model via Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Neural Topic Models (NTMs) inspired by variational autoencoders\nhave obtained increasingly research interest due to their promising results on\ntext analysis. However, it is usually hard for existing NTMs to achieve good\ndocument representation and coherent/diverse topics at the same time. Moreover,\nthey often degrade their performance severely on short documents. The\nrequirement of reparameterisation could also comprise their training quality\nand model flexibility. To address these shortcomings, we present a new neural\ntopic model via the theory of optimal transport (OT). Specifically, we propose\nto learn the topic distribution of a document by directly minimising its OT\ndistance to the document's word distributions. Importantly, the cost matrix of\nthe OT distance models the weights between topics and words, which is\nconstructed by the distances between topics and words in an embedding space.\nOur proposed model can be trained efficiently with a differentiable loss.\nExtensive experiments show that our framework significantly outperforms the\nstate-of-the-art NTMs on discovering more coherent and diverse topics and\nderiving better document representations for both regular and short texts.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:37:09 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 01:49:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhao", "He", ""], ["Phung", "Dinh", ""], ["Huynh", "Viet", ""], ["Le", "Trung", ""], ["Buntine", "Wray", ""]]}, {"id": "2008.13538", "submitter": "Xiaomei Bai", "authors": "Xiaomei Bai, Mengyang Wang, Ivan Lee, Zhuo Yang, Xiangjie Kong and\n  Feng Xia", "title": "Scientific Paper Recommendation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally, recommendation services have become important due to the fact that\nthey support e-commerce applications and different research communities.\nRecommender systems have a large number of applications in many fields\nincluding economic, education, and scientific research. Different empirical\nstudies have shown that recommender systems are more effective and reliable\nthan keyword-based search engines for extracting useful knowledge from massive\namounts of data. The problem of recommending similar scientific articles in\nscientific community is called scientific paper recommendation. Scientific\npaper recommendation aims to recommend new articles or classical articles that\nmatch researchers' interests. It has become an attractive area of study since\nthe number of scholarly papers increases exponentially. In this survey, we\nfirst introduce the importance and advantages of paper recommender systems.\nSecond, we review the recommendation algorithms and methods, such as\nContent-Based methods, Collaborative Filtering methods, Graph-Based methods and\nHybrid methods. Then, we introduce the evaluation methods of different\nrecommender systems. Finally, we summarize open issues in the paper recommender\nsystems, including cold start, sparsity, scalability, privacy, serendipity and\nunified scholarly data standards. The purpose of this survey is to provide\ncomprehensive reviews on scholarly paper recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 03:17:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bai", "Xiaomei", ""], ["Wang", "Mengyang", ""], ["Lee", "Ivan", ""], ["Yang", "Zhuo", ""], ["Kong", "Xiangjie", ""], ["Xia", "Feng", ""]]}, {"id": "2008.13541", "submitter": "Andrei Khrennikov Yu", "authors": "Alexander Lebedev and Andrei Khrennikov", "title": "Introductory review to quantum information retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently people started to understand that applications of the mathematical\nformalism of quantum theory are not reduced to physics. Nowadays, this\nformalism is widely used outside of quantum physics, in particular, in\ncognition, psychology, decision making, information processing, especially\ninformation retrieval. The latter is very promising. The aim of this brief\nintroductory review is to stimulate research in this exciting area of\ninformation science. This paper is not aimed to present a complete review on\nthe state of art in quantum information retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 15:23:42 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lebedev", "Alexander", ""], ["Khrennikov", "Andrei", ""]]}, {"id": "2008.13542", "submitter": "Maksim Eren", "authors": "Maksim Ekin Eren, Nick Solovyev, Edward Raff, Charles Nicholas, Ben\n  Johnson", "title": "COVID-19 Kaggle Literature Organization", "comments": "Maksim Ekin Eren, Nick Solovyev, Edward Raff, Charles Nicholas, and\n  Ben Johnson. 2020. COVID-19 Kaggle Literature Organization. In ACM Sym-posium\n  on Document Engineering 2020 (DocEng 20), September 29-October2, 2020,\n  Virtual Event, CA, USA.ACM, New York, NY, USA, 4 pages.\n  https://doi.org/10.1145/3395027.3419591", "journal-ref": null, "doi": "10.1145/3395027.3419591", "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world has faced the devastating outbreak of Severe Acute Respiratory\nSyndrome Coronavirus-2 (SARS-CoV-2), or COVID-19, in 2020. Research in the\nsubject matter was fast-tracked to such a point that scientists were struggling\nto keep up with new findings. With this increase in the scientific literature,\nthere arose a need for organizing those documents. We describe an approach to\norganize and visualize the scientific literature on or related to COVID-19\nusing machine learning techniques so that papers on similar topics are grouped\ntogether. By doing so, the navigation of topics and related papers is\nsimplified. We implemented this approach using the widely recognized CORD-19\ndataset to present a publicly available proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 21:02:32 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:09:43 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 03:54:34 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Eren", "Maksim Ekin", ""], ["Solovyev", "Nick", ""], ["Raff", "Edward", ""], ["Nicholas", "Charles", ""], ["Johnson", "Ben", ""]]}, {"id": "2008.13544", "submitter": "Hamada Zahera", "authors": "Hamada M. Zahera, Rricha Jalota, Mohamed A. Sherif, Axel N. Ngomo", "title": "I-AID: Identifying Actionable Information from Disaster-related Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Social media plays a significant role in disaster management by providing\nvaluable data about affected people, donations and help requests. Recent\nstudies highlight the need to filter information on social media into\nfine-grained content labels. However, identifying useful information from\nmassive amounts of social media posts during a crisis is a challenging task. In\nthis paper, we propose I-AID, a multimodel approach to automatically categorize\ntweets into multi-label information types and filter critical information from\nthe enormous volume of social media data. I-AID incorporates three main\ncomponents: i) a BERT-based encoder to capture the semantics of a tweet and\nrepresent as a low-dimensional vector, ii) a graph attention network (GAT) to\napprehend correlations between tweets' words/entities and the corresponding\ninformation types, and iii) a Relation Network as a learnable distance metric\nto compute the similarity between tweets and their corresponding information\ntypes in a supervised way. We conducted several experiments on two real\npublicly-available datasets. Our results indicate that I-AID outperforms\nstate-of-the-art approaches in terms of weighted average F1 score by +6% and\n+4% on the TREC-IS dataset and COVID-19 Tweets, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:07:50 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 02:32:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Zahera", "Hamada M.", ""], ["Jalota", "Rricha", ""], ["Sherif", "Mohamed A.", ""], ["Ngomo", "Axel N.", ""]]}, {"id": "2008.13546", "submitter": "Xavier Amatriain", "authors": "Clara H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani,\n  Xavier Amatriain", "title": "Effective Transfer Learning for Identifying Similar Questions: Matching\n  User Questions to COVID-19 FAQs", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.04192", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People increasingly search online for answers to their medical questions but\nthe rate at which medical questions are asked online significantly exceeds the\ncapacity of qualified people to answer them. This leaves many questions\nunanswered or inadequately answered. Many of these questions are not unique,\nand reliable identification of similar questions would enable more efficient\nand effective question answering schema. COVID-19 has only exacerbated this\nproblem. Almost every government agency and healthcare organization has tried\nto meet the informational need of users by building online FAQs, but there is\nno way for people to ask their question and know if it is answered on one of\nthese pages. While many research efforts have focused on the problem of general\nquestion similarity, these approaches do not generalize well to domains that\nrequire expert knowledge to determine semantic similarity, such as the medical\ndomain. In this paper, we show how a double fine-tuning approach of pretraining\na neural network on medical question-answer pairs followed by fine-tuning on\nmedical question-question pairs is a particularly useful intermediate task for\nthe ultimate goal of determining medical question similarity. While other\npretraining tasks yield an accuracy below 78.7% on this task, our model\nachieves an accuracy of 82.6% with the same number of training examples, an\naccuracy of 80.0% with a much smaller training set, and an accuracy of 84.5%\nwhen the full corpus of medical question-answer data is used. We also describe\na currently live system that uses the trained model to match user questions to\nCOVID-related FAQs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 18:20:04 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["McCreery", "Clara H.", ""], ["Katariya", "Namit", ""], ["Kannan", "Anitha", ""], ["Chablani", "Manish", ""], ["Amatriain", "Xavier", ""]]}, {"id": "2008.13569", "submitter": "Suman Bhoi", "authors": "Suman Bhoi, Lee Mong Li, Wynne Hsu", "title": "PREMIER: Personalized REcommendation for Medical prescrIptions from\n  Electronic Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The broad adoption of Electronic Health Records (EHR) has led to vast amounts\nof data being accumulated on a patient's history, diagnosis, prescriptions, and\nlab tests. Advances in recommender technologies have the potential to utilize\nthis information to help doctors personalize the prescribed medications. In\nthis work, we design a two-stage attention-based personalized medication\nrecommender system called PREMIER which incorporates information from the EHR\nto suggest a set of medications. Our system takes into account the interactions\namong drugs in order to minimize the adverse effects for the patient. We\nutilize the various attention weights in the system to compute the\ncontributions from the information sources for the recommended medications.\nExperiment results on MIMIC-III and a proprietary outpatient dataset show that\nPREMIER outperforms state-of-the-art medication recommendation systems while\nachieving the best tradeoff between accuracy and drug-drug interaction. Two\ncase studies are also presented demonstrating that the justifications provided\nby PREMIER are appropriate and aligned to clinical practices.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 04:48:32 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bhoi", "Suman", ""], ["Li", "Lee Mong", ""], ["Hsu", "Wynne", ""]]}, {"id": "2008.13585", "submitter": "Jacopo de Berardinis", "authors": "Jacopo de Berardinis, Gabriella Pizzuto, Francesco Lanza, Antonio\n  Chella, Jorge Meira, Angelo Cangelosi", "title": "At Your Service: Coffee Beans Recommendation From a Robot Assistant", "comments": "Extended version of submission to ACM HAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in the field of machine learning, precisely algorithms for\nrecommendation systems, robot assistants are envisioned to become more present\nin the hospitality industry. Additionally, the COVID-19 pandemic has also\nhighlighted the need to have more service robots in our everyday lives, to\nminimise the risk of human to-human transmission. One such example would be\ncoffee shops, which have become intrinsic to our everyday lives. However,\nserving an excellent cup of coffee is not a trivial feat as a coffee blend\ntypically comprises rich aromas, indulgent and unique flavours and a lingering\naftertaste. Our work addresses this by proposing a computational model which\nrecommends optimal coffee beans resulting from the user's preferences.\nSpecifically, given a set of coffee bean properties (objective features), we\napply different supervised learning techniques to predict coffee qualities\n(subjective features). We then consider an unsupervised learning method to\nanalyse the relationship between coffee beans in the subjective feature space.\nEvaluated on a real coffee beans dataset based on digitised reviews, our\nresults illustrate that the proposed computational model gives up to 92.7\npercent recommendation accuracy for coffee beans prediction. From this, we\npropose how this computational model can be deployed on a service robot to\nreliably predict customers' coffee bean preferences, starting from the user\ninputting their coffee preferences to the robot recommending the coffee beans\nthat best meet the user's likings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:03:25 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["de Berardinis", "Jacopo", ""], ["Pizzuto", "Gabriella", ""], ["Lanza", "Francesco", ""], ["Chella", "Antonio", ""], ["Meira", "Jorge", ""], ["Cangelosi", "Angelo", ""]]}]