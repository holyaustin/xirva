[{"id": "1204.0156", "submitter": "Raju Balakrishnan", "authors": "Srijith Ravikumar, Raju Balakrishnan, Subbarao Kambhampati", "title": "Ranking Tweets Considering Trust and Relevance", "comments": "four pages short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of Twitter and other microblogs makes improved\ntrustworthiness and relevance assessment of microblogs evermore important. We\npropose a method of ranking of tweets considering trustworthiness and content\nbased popularity. The analysis of trustworthiness and popularity exploits the\nimplicit relationships between the tweets. We model microblog ecosystem as a\nthree-layer graph consisting of : (i) users (ii) tweets and (iii) web pages. We\npropose to derive trust and popularity scores of entities in these three\nlayers, and propagate the scores to tweets considering the inter-layer\nrelations. Our preliminary evaluations show improvement in precision and\ntrustworthiness over the baseline methods and acceptable computation timings.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 02:30:00 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Ravikumar", "Srijith", ""], ["Balakrishnan", "Raju", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1204.0170", "submitter": "Jia Zeng", "authors": "Jia Zeng, Zhi-Qiang Liu and Xiao-Qin Cao", "title": "A New Approach to Speeding Up Topic Modeling", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is a widely-used probabilistic topic\nmodeling paradigm, and recently finds many applications in computer vision and\ncomputational biology. In this paper, we propose a fast and accurate batch\nalgorithm, active belief propagation (ABP), for training LDA. Usually batch LDA\nalgorithms require repeated scanning of the entire corpus and searching the\ncomplete topic space. To process massive corpora having a large number of\ntopics, the training iteration of batch LDA algorithms is often inefficient and\ntime-consuming. To accelerate the training speed, ABP actively scans the subset\nof corpus and searches the subset of topic space for topic modeling, therefore\nsaves enormous training time in each iteration. To ensure accuracy, ABP selects\nonly those documents and topics that contribute to the largest residuals within\nthe residual belief propagation (RBP) framework. On four real-world corpora,\nABP performs around $10$ to $100$ times faster than state-of-the-art batch LDA\nalgorithms with a comparable topic modeling accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 07:07:27 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 02:17:47 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Cao", "Xiao-Qin", ""]]}, {"id": "1204.0182", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "Hybrid Information Retrieval Model For Web Images", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; International Journal of Computer Science & Emerging\n  Technologies (IJCSET), Vol. 3, No. 1, February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bing Bang of the Internet in the early 90's increased dramatically the\nnumber of images being distributed and shared over the web. As a result, image\ninformation retrieval systems were developed to index and retrieve image files\nspread over the Internet. Most of these systems are keyword-based which search\nfor images based on their textual metadata; and thus, they are imprecise as it\nis vague to describe an image with a human language. Besides, there exist the\ncontent-based image retrieval systems which search for images based on their\nvisual information. However, content-based type systems are still immature and\nnot that effective as they suffer from low retrieval recall/precision rate.\nThis paper proposes a new hybrid image information retrieval model for indexing\nand retrieving web images published in HTML documents. The distinguishing mark\nof the proposed model is that it is based on both graphical content and textual\nmetadata. The graphical content is denoted by color features and color\nhistogram of the image; while textual metadata are denoted by the terms that\nsurround the image in the HTML document, more particularly, the terms that\nappear in the tags p, h1, and h2, in addition to the terms that appear in the\nimage's alt attribute, filename, and class-label. Moreover, this paper presents\na new term weighting scheme called VTF-IDF short for Variable Term\nFrequency-Inverse Document Frequency which unlike traditional schemes, it\nexploits the HTML tag structure and assigns an extra bonus weight for terms\nthat appear within certain particular HTML tags that are correlated to the\nsemantics of the image. Experiments conducted to evaluate the proposed IR model\nshowed a high retrieval precision rate that outpaced other current models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 09:12:40 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1204.0186", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Paul Semaan", "title": "Semantic-Sensitive Web Information Retrieval Model for HTML Documents", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; European Journal of Scientific Research, Vol. 69, No.\n  4, February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet, a new era of digital information exchange\nhas begun. Currently, the Internet encompasses more than five billion online\nsites and this number is exponentially increasing every day. Fundamentally,\nInformation Retrieval (IR) is the science and practice of storing documents and\nretrieving information from within these documents. Mathematically, IR systems\nare at the core based on a feature vector model coupled with a term weighting\nscheme that weights terms in a document according to their significance with\nrespect to the context in which they appear. Practically, Vector Space Model\n(VSM), Term Frequency (TF), and Inverse Term Frequency (IDF) are among other\nlong-established techniques employed in mainstream IR systems. However, present\nIR models only target generic-type text documents, in that, they do not\nconsider specific formats of files such as HTML web documents. This paper\nproposes a new semantic-sensitive web information retrieval model for HTML\ndocuments. It consists of a vector model called SWVM and a weighting scheme\ncalled BTF-IDF, particularly designed to support the indexing and retrieval of\nHTML web documents. The chief advantage of the proposed model is that it\nassigns extra weights for terms that appear in certain pre-specified HTML tags\nthat are correlated to the semantics of the document. Additionally, the model\nis semantic-sensitive as it generates synonyms for every term being indexed and\nlater weights them appropriately to increase the likelihood of retrieving\ndocuments with similar context but different vocabulary terms. Experiments\nconducted, revealed a momentous enhancement in the precision of web IR systems\nand a radical increase in the number of relevant documents being retrieved. As\nfurther research, the proposed model is to be upgraded so as to support the\nindexing and retrieval of web images in multimedia-rich web documents.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 09:50:42 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Semaan", "Paul", ""]]}, {"id": "1204.0188", "submitter": "Youssef Bassil", "authors": "Youssef Bassil, Mohammad Alwani", "title": "OCR Context-Sensitive Error Correction Based on Google Web 1T 5-Gram\n  Data Set", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org/; American Journal of Scientific Research, Issue. 50,\n  February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the dawn of the computing era, information has been represented\ndigitally so that it can be processed by electronic computers. Paper books and\ndocuments were abundant and widely being published at that time; and hence,\nthere was a need to convert them into digital format. OCR, short for Optical\nCharacter Recognition was conceived to translate paper-based books into digital\ne-books. Regrettably, OCR systems are still erroneous and inaccurate as they\nproduce misspellings in the recognized text, especially when the source\ndocument is of low printing quality. This paper proposes a post-processing OCR\ncontext-sensitive error correction method for detecting and correcting non-word\nand real-word OCR errors. The cornerstone of this proposed approach is the use\nof Google Web 1T 5-gram data set as a dictionary of words to spell-check OCR\ntext. The Google data set incorporates a very large vocabulary and word\nstatistics entirely reaped from the Internet, making it a reliable source to\nperform dictionary-based error correction. The core of the proposed solution is\na combination of three algorithms: The error detection, candidate spellings\ngenerator, and error correction algorithms, which all exploit information\nextracted from Google Web 1T 5-gram data set. Experiments conducted on scanned\nimages written in different languages showed a substantial improvement in the\nOCR error correction rate. As future developments, the proposed algorithm is to\nbe parallelised so as to support parallel and distributed computing\narchitectures.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 10:06:55 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bassil", "Youssef", ""], ["Alwani", "Mohammad", ""]]}, {"id": "1204.0255", "submitter": "Mario Jarmasz", "authors": "Mario Jarmasz and Caroline Barri\\`ere", "title": "Keyphrase Extraction : Enhancing Lists", "comments": "8 pages; Proceedings of the 2nd Conference on Computational\n  Linguistics in the North-East (CLiNE 2004), Montr\\'eal, Canada, August", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes some modest improvements to Extractor, a state-of-the-art\nkeyphrase extraction system, by using a terabyte-sized corpus to estimate the\ninformativeness and semantic similarity of keyphrases. We present two\ntechniques to improve the organization and remove outliers of lists of\nkeyphrases. The first is a simple ordering according to their occurrences in\nthe corpus; the second is clustering according to semantic similarity.\nEvaluation issues are discussed. We present a novel technique of comparing\nextracted keyphrases to a gold standard which relies on semantic similarity\nrather than string matching or an evaluation involving human judges.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 19:15:58 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Jarmasz", "Mario", ""], ["Barri\u00e8re", "Caroline", ""]]}, {"id": "1204.0309", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "A Model for Personalized Keyword Extraction from Web Pages using\n  Segmentation", "comments": "6 Pages, 2 Figures", "journal-ref": "International Journal of Computer Applications (0975 - 8887),\n  Volume 42 - No.4, March 2012", "doi": "10.5120/5681-7720", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web caters to the needs of billions of users in heterogeneous\ngroups. Each user accessing the World Wide Web might have his / her own\nspecific interest and would expect the web to respond to the specific\nrequirements. The process of making the web to react in a customized manner is\nachieved through personalization. This paper proposes a novel model for\nextracting keywords from a web page with personalization being incorporated\ninto it. The keyword extraction problem is approached with the help of web page\nsegmentation which facilitates in making the problem simpler and solving it\neffectively. The proposed model is implemented as a prototype and the\nexperiments conducted on it empirically validate the model's efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 04:00:16 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1204.1162", "submitter": "Anis Ismail", "authors": "Abd El Salam Al Hajjar, Anis Ismail, Mohammad Hajjar, Mazen El-Sayed", "title": "Performance of the Google Desktop, Arabic Google Desktop and Peer to\n  Peer Application in Arabic Language", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Arabic language is a complex language; it is different from Western\nlanguages especially at the morphological and spelling variations. Indeed, the\nperformance of information retrieval systems in the Arabic language is still a\nproblem. For this reason, we are interested in studying the performance of the\nmost famous search engine, which is a Google Desktop, while searching in Arabic\nlanguage documents. Then, we propose an update to the Google Desktop to take\ninto consideration in search the Arabic words that have the same root. After\nthat, we evaluate the performance of the Google Desktop in this context. Also,\nwe are interested in evaluation the performance of peer-to-peer application in\ntwo ways. The first one uses a simple indexation that indexes Arabic documents\nwithout taking in consideration the root of words. The second way takes in\nconsideration the roots in the indexation of Arabic documents. This evaluation\nis done by using a corpus of ten thousand documents and one hundred different\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 09:38:23 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Hajjar", "Abd El Salam Al", ""], ["Ismail", "Anis", ""], ["Hajjar", "Mohammad", ""], ["El-Sayed", "Mazen", ""]]}, {"id": "1204.1185", "submitter": "Michal Batko", "authors": "Petra Budikova, Michal Batko, Pavel Zezula", "title": "Query Language for Complex Similarity Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex data types such as multimedia, traditional data management\nmethods are not suitable. Instead of attribute matching approaches, access\nmethods based on object similarity are becoming popular. Recently, this\nresulted in an intensive research of indexing and searching methods for the\nsimilarity-based retrieval. Nowadays, many efficient methods are already\navailable, but using them to build an actual search system still requires\nspecialists that tune the methods and build the system manually. Several\nattempts have already been made to provide a more convenient high-level\ninterface in a form of query languages for such systems, but these are limited\nto support only basic similarity queries. In this paper, we propose a new\nlanguage that allows to formulate content-based queries in a flexible way,\ntaking into account the functionality offered by a particular search engine in\nuse. To ensure this, the language is based on a general data model with an\nabstract set of operations. Consequently, the language supports various\nadvanced query operations such as similarity joins, reverse nearest neighbor\nqueries, or distinct kNN queries, as well as multi-object and multi-modal\nqueries. The language is primarily designed to be used with the MESSIF\nframework for content-based searching but can be employed by other retrieval\nsystems as well.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 11:30:49 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Budikova", "Petra", ""], ["Batko", "Michal", ""], ["Zezula", "Pavel", ""]]}, {"id": "1204.1259", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Domonkos Tikk", "title": "Fast ALS-based tensor factorization for context-aware recommendation\n  from implicit feedback", "comments": "Accepted for ECML/PKDD 2012, presented on 25th September 2012,\n  Bristol, UK", "journal-ref": "Proceedings of the 2012 European conference on Machine Learning\n  and Knowledge Discovery in Databases - Volume Part II", "doi": "10.1007/978-3-642-33486-3_5", "report-no": null, "categories": "cs.LG cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit, the implicit feedback based recommendation problem - when only the\nuser history is available but there are no ratings - is the most typical\nsetting in real-world applications, it is much less researched than the\nexplicit feedback case. State-of-the-art algorithms that are efficient on the\nexplicit case cannot be straightforwardly transformed to the implicit case if\nscalability should be maintained. There are few if any implicit feedback\nbenchmark datasets, therefore new ideas are usually experimented on explicit\nbenchmarks. In this paper, we propose a generic context-aware implicit feedback\nrecommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensor\nfactorization learning method that scales linearly with the number of non-zero\nelements in the tensor. The method also allows us to incorporate diverse\ncontext information into the model while maintaining its computational\nefficiency. In particular, we present two such context-aware implementation\nvariants of iTALS. The first incorporates seasonality and enables to\ndistinguish user behavior in different time intervals. The other views the user\nhistory as sequential information and has the ability to recognize usage\npattern typical to certain group of items, e.g. to automatically tell apart\nproduct types or categories that are typically purchased repetitively\n(collectibles, grocery goods) or once (household appliances). Experiments\nperformed on three implicit datasets (two proprietary ones and an implicit\nvariant of the Netflix dataset) show that by integrating context-aware\ninformation with our factorization framework into the state-of-the-art implicit\nrecommender algorithm the recommendation quality improves significantly.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 15:34:30 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2013 15:33:31 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1204.1406", "submitter": "S. K. Sahay", "authors": "R. K. Roul and S. K. Sahay", "title": "An Effective Information Retrieval for Ambiguous Query", "comments": "11 Pages, 1 figure", "journal-ref": "AJCSIT, Vol. 2, No. 3, P. 26-30, 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine returns thousands of web pages for a single user query, in\nwhich most of them are not relevant. In this context, effective information\nretrieval from the expanding web is a challenging task, in particular, if the\nquery is ambiguous. The major question arises here is that how to get the\nrelevant pages for an ambiguous query. We propose an approach for the effective\nresult of an ambiguous query by forming community vector based on association\nconcept of data minning using vector space model and the freedictionary. We\ndevelop clusters by computing the similarity between community vectors and\ndocument vectors formed from the extracted web pages by the search engine. We\nuse Gensim package to implement the algorithm because of its simplicity and\nrobust nature. Analysis shows that our approach is an effective way to form\nclusters for an ambiguous query.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 04:37:38 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Roul", "R. K.", ""], ["Sahay", "S. K.", ""]]}, {"id": "1204.1528", "submitter": "Thomas Sandholm", "authors": "Leandro Balby Marinho, Cl\\'audio de Souza Baptista, Thomas Sandholm,\n  Iury Nunes, Caio N\\'obrega, Jord\\~ao Ara\\'ujo", "title": "Extracting Geospatial Preferences Using Relational Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of location-based social media applications\nand devices that automatically tag generated content with locations, large\nrepositories of collaborative geo-referenced data are appearing on-line.\nEfficiently extracting user preferences from these data to determine what\ninformation to recommend is challenging because of the sheer volume of data as\nwell as the frequency of updates. Traditional recommender systems focus on the\ninterplay between users and items, but ignore contextual parameters such as\nlocation. In this paper we take a geospatial approach to determine locational\npreferences and similarities between users. We propose to capture the\ngeographic context of user preferences for items using a relational graph,\nthrough which we are able to derive many new and state-of-the-art\nrecommendation algorithms, including combinations of them, requiring changes\nonly in the definition of the edge weights. Furthermore, we discuss several\nsolutions for cold-start scenarios. Finally, we conduct experiments using two\nreal-world datasets and provide empirical evidence that many of the proposed\nalgorithms outperform existing location-aware recommender algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 18:15:55 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Marinho", "Leandro Balby", ""], ["Baptista", "Cl\u00e1udio de Souza", ""], ["Sandholm", "Thomas", ""], ["Nunes", "Iury", ""], ["N\u00f3brega", "Caio", ""], ["Ara\u00fajo", "Jord\u00e3o", ""]]}, {"id": "1204.1615", "submitter": "Sofiene Haboubi", "authors": "Sofiene Haboubi and Samia Maddouri and Hamid Amiri", "title": "Discrimination between Arabic and Latin from bilingual documents", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/CCCA.2011.6031496", "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  2011 International Conference on Communications, Computing and Control\nApplications (CCCA)\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 09:28:19 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Haboubi", "Sofiene", ""], ["Maddouri", "Samia", ""], ["Amiri", "Hamid", ""]]}, {"id": "1204.1631", "submitter": "Mohamed Ali Mahjoub", "authors": "Khlifia jayech, mohamed ali mahjoub", "title": "New approach using Bayesian Network to improve content based image\n  classification systems", "comments": "10 pages, IJCSI International Journal of Computer Science Issues,\n  Vol. 7, Issue 6, November 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach based on augmented naive Bayes for image\nclassification. Initially, each image is cutting in a whole of blocks. For each\nblock, we compute a vector of descriptors. Then, we propose to carry out a\nclassification of the vectors of descriptors to build a vector of labels for\neach image. Finally, we propose three variants of Bayesian Networks such as\nNaive Bayesian Network (NB), Tree Augmented Naive Bayes (TAN) and Forest\nAugmented Naive Bayes (FAN) to classify the image using the vector of labels.\nThe results showed a marked improvement over the FAN, NB and TAN.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:29:17 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["jayech", "Khlifia", ""], ["mahjoub", "mohamed ali", ""]]}, {"id": "1204.1832", "submitter": "Hong Xie", "authors": "Hong Xie, John C.S. Lui", "title": "Mathematical Modeling of Competitive Group Recommendation Systems with\n  Application to Peer Review Systems", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a mathematical model to capture various factors\nwhich may influence the accuracy of a competitive group recommendation system.\nWe apply this model to peer review systems, i.e., conference or research grants\nreview, which is an essential component in our scientific community. We explore\nnumber of important questions, i.e., how will the number of reviews per paper\naffect the accuracy of the overall recommendation? Will the score aggregation\npolicy influence the final recommendation? How reviewers' preference may affect\nthe accuracy of the final recommendation? To answer these important questions,\nwe formally analyze our model. Through this analysis, we obtain the insight on\nhow to design a randomized algorithm which is both computationally efficient\nand asymptotically accurate in evaluating the accuracy of a competitive group\nrecommendation system. We obtain number of interesting observations: i.e., for\na medium tier conference, three reviews per paper is sufficient for a high\naccuracy recommendation. For prestigious conferences, one may need at least\nseven reviews per paper to achieve high accuracy. We also propose a\nheterogeneous review strategy which requires equal or less reviewing workload,\nbut can improve over a homogeneous review strategy in recommendation accuracy\nby as much as 30% . We believe our models and methodology are important\nbuilding blocks to study competitive group recommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 08:47:58 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 06:26:56 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Xie", "Hong", ""], ["Lui", "John C. S.", ""]]}, {"id": "1204.1868", "submitter": "Konstantinos Chorianopoulos", "authors": "Konstantinos Chorianopoulos", "title": "User-based key frame detection in social web video", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": "10.1145/2382636.2382642", "report-no": null, "categories": "cs.MM cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video search results and suggested videos on web sites are represented with a\nvideo thumbnail, which is manually selected by the video up-loader among three\nrandomly generated ones (e.g., YouTube). In contrast, we present a grounded\nuser-based approach for automatically detecting interesting key-frames within a\nvideo through aggregated users' replay interactions with the video player.\nPrevious research has focused on content-based systems that have the benefit of\nanalyzing a video without user interactions, but they are monolithic, because\nthe resulting video thumbnails are the same regardless of the user preferences.\nWe constructed a user interest function, which is based on aggregate video\nreplays, and analyzed hundreds of user interactions. We found that the local\nmaximum of the replaying activity stands for the semantics of information rich\nvideos, such as lecture, and how-to. The concept of user-based key-frame\ndetection could be applied to any video on the web, in order to generate a\nuser-based and dynamic video thumbnail in search results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 12:23:36 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Chorianopoulos", "Konstantinos", ""]]}, {"id": "1204.1949", "submitter": "Zi-Ke Zhang Mr.", "authors": "Xiao Hu, Chuibo Chen, Xiaolong Chen, Zi-Ke Zhang", "title": "Social Recommender Systems Based on Coupling Network Structure Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years has witnessed the great success of recommender systems,\nwhich can significantly help users find relevant and interesting items for them\nin the information era. However, a vast class of researches in this area mainly\nfocus on predicting missing links in bipartite user-item networks (represented\nas behavioral networks). Comparatively, the social impact, especially the\nnetwork structure based properties, is relatively lack of study. In this paper,\nwe firstly obtain five corresponding network-based features, including user\nactivity, average neighbors' degree, clustering coefficient, assortative\ncoefficient and discrimination, from social and behavioral networks,\nrespectively. A hybrid algorithm is proposed to integrate those features from\ntwo respective networks. Subsequently, we employ a machine learning process to\nuse those features to provide recommendation results in a binary classifier\nmethod. Experimental results on a real dataset, Flixster, suggest that the\nproposed method can significantly enhance the algorithmic accuracy. In\naddition, as network-based properties consider not only the social activities,\nbut also take into account user preferences in the behavioral networks,\ntherefore, it performs much better than that from either social or behavioral\nnetworks. Furthermore, since the features based on the behavioral network\ncontain more diverse and meaningfully structural information, they play a vital\nrole in uncovering users' potential preference, which, might show light in\ndeeply understanding the structure and function of the social and behavioral\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 18:46:53 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Hu", "Xiao", ""], ["Chen", "Chuibo", ""], ["Chen", "Xiaolong", ""], ["Zhang", "Zi-Ke", ""]]}, {"id": "1204.1956", "submitter": "Rong Ge", "authors": "Sanjeev Arora, Rong Ge, Ankur Moitra", "title": "Learning Topic Models - Going beyond SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic Modeling is an approach used for automatic comprehension and\nclassification of data in a variety of settings, and perhaps the canonical\napplication is in uncovering thematic structure in a corpus of documents. A\nnumber of foundational works both in machine learning and in theory have\nsuggested a probabilistic model for documents, whereby documents arise as a\nconvex combination of (i.e. distribution on) a small number of topic vectors,\neach topic vector being a distribution on words (i.e. a vector of\nword-frequencies). Similar models have since been used in a variety of\napplication areas; the Latent Dirichlet Allocation or LDA model of Blei et al.\nis especially popular.\n  Theoretical studies of topic modeling focus on learning the model's\nparameters assuming the data is actually generated from it. Existing approaches\nfor the most part rely on Singular Value Decomposition(SVD), and consequently\nhave one of two limitations: these works need to either assume that each\ndocument contains only one topic, or else can only recover the span of the\ntopic vectors instead of the topic vectors themselves.\n  This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main\ntool in this context, which is an analog of SVD where all vectors are\nnonnegative. Using this tool we give the first polynomial-time algorithm for\nlearning topic models without the above two limitations. The algorithm uses a\nfairly mild assumption about the underlying topic matrix called separability,\nwhich is usually found to hold in real-life data. A compelling feature of our\nalgorithm is that it generalizes to models that incorporate topic-topic\ncorrelations, such as the Correlated Topic Model and the Pachinko Allocation\nModel.\n  We hope that this paper will motivate further theoretical results that use\nNMF as a replacement for SVD - just as NMF has come to replace SVD in many\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 19:33:47 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2012 01:08:52 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1204.2032", "submitter": "Wei Zeng", "authors": "Wei Zeng and Li Chen", "title": "Multi-Output Recommender: Items, Groups and Friends, and Their Mutual\n  Contributing Effects", "comments": "withdraw the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the development of social media technology, it becomes easier for\nusers to gather together to form groups. Take the Last.fm for example, users\ncan join groups they may be interested where they can share their loved songs\nand discuss topics about songs and singers. However, the number of groups grows\nover time, users need effective groups recommendations in order to meet more\nlike-minded users.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 02:53:03 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2012 02:45:47 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 07:00:44 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Zeng", "Wei", ""], ["Chen", "Li", ""]]}, {"id": "1204.2058", "submitter": "Shalini Puri", "authors": "Shalini Puri and Sona Kaushik", "title": "A technical study and analysis on fuzzy similarity based models for text\n  classification", "comments": "15 pages, 3 tables, 1 figure", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) March, 2012, Vol. 2, Number 2,pp. 1-15", "doi": "10.5121/ijdkp.2012.2201", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this new and current era of technology, advancements and techniques,\nefficient and effective text document classification is becoming a challenging\nand highly required area to capably categorize text documents into mutually\nexclusive categories. Fuzzy similarity provides a way to find the similarity of\nfeatures among various documents. In this paper, a technical review on various\nfuzzy similarity based models is given. These models are discussed and compared\nto frame out their use and necessity. A tour of different methodologies is\nprovided which is based upon fuzzy similarity related concerns. It shows that\nhow text and web documents are categorized efficiently into different\ncategories. Various experimental results of these models are also discussed.\nThe technical comparisons among each model's parameters are shown in the form\nof a 3-D chart. Such study and technical review provide a strong base of\nresearch work done on fuzzy similarity based text document categorization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 06:59:48 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Puri", "Shalini", ""], ["Kaushik", "Sona", ""]]}, {"id": "1204.2061", "submitter": "Shalini Puri", "authors": "Shalini Puri", "title": "A Fuzzy Similarity Based Concept Mining Model for Text Classification", "comments": "7 Pages, 3 Figures, 2 Tables, International Journal of Advanced\n  Computer Science and Applications(IJACSA)", "journal-ref": "Volume 2, Number 11, pp. 115 - 121, November, 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Classification is a challenging and a red hot field in the current\nscenario and has great importance in text categorization applications. A lot of\nresearch work has been done in this field but there is a need to categorize a\ncollection of text documents into mutually exclusive categories by extracting\nthe concepts or features using supervised learning paradigm and different\nclassification algorithms. In this paper, a new Fuzzy Similarity Based Concept\nMining Model (FSCMM) is proposed to classify a set of text documents into pre -\ndefined Category Groups (CG) by providing them training and preparing on the\nsentence, document and integrated corpora levels along with feature reduction,\nambiguity removal on each level to achieve high system performance. Fuzzy\nFeature Category Similarity Analyzer (FFCSA) is used to analyze each extracted\nfeature of Integrated Corpora Feature Vector (ICFV) with the corresponding\ncategories or classes. This model uses Support Vector Machine Classifier (SVMC)\nto classify correctly the training data patterns into two groups; i. e., + 1\nand - 1, thereby producing accurate and correct results. The proposed model\nworks efficiently and effectively with great performance and high - accuracy\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:05:20 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Puri", "Shalini", ""]]}, {"id": "1204.2079", "submitter": "Nacim Yanes", "authors": "Nacim Yanes, Sihem Ben Sassi, and Henda Hajjami Ben Ghezala", "title": "A Theoretical and Empirical Evaluation of Software Component Search\n  Engines, Semantic Search Engines and Google Search Engine in the Context of\n  COTS-Based Development", "comments": "9 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COTS-based development is a component reuse approach promising to reduce\ncosts and risks, and ensure higher quality. The growing availability of COTS\ncomponents on the Web has concretized the possibility of achieving these\nobjectives. In this multitude, a recurrent problem is the identification of the\nCOTS components that best satisfy the user requirements. Finding an adequate\nCOTS component implies searching among heterogeneous descriptions of the\ncomponents within a broad search space. Thus, the use of search engines is\nrequired to make more efficient the COTS components identification. In this\npaper, we investigate, theoretically and empirically, the COTS component search\nperformance of eight software component search engines, nine semantic search\nengines and a conventional search engine (Google). Our empirical evaluation is\nconducted with respect to precision and normalized recall. We defined ten\nqueries for the assessed search engines. These queries were carefully selected\nto evaluate the capability of each search engine for handling COTS component\nidentification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 08:30:18 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Yanes", "Nacim", ""], ["Sassi", "Sihem Ben", ""], ["Ghezala", "Henda Hajjami Ben", ""]]}, {"id": "1204.2231", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Robert E. Mercer", "title": "Investigating Keyphrase Indexing with Text Denoising", "comments": "The full paper submitted to 12th ACM/ IEEE-CS Joint Conference on\n  Digital Libraries (JCDL2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we report on indexing performance by a state-of-the-art\nkeyphrase indexer, Maui, when paired with a text extraction procedure called\ntext denoising. Text denoising is a method that extracts the denoised text,\ncomprising the content-rich sentences, from full texts. The performance of the\nkeyphrase indexer is demonstrated on three standard corpora collected from\nthree domains, namely food and agriculture, high energy physics, and biomedical\nscience. Maui is trained using the full texts and denoised texts. The indexer,\nusing its trained models, then extracts keyphrases from test sets comprising\nfull texts, and their denoised and noise parts (i.e., the part of texts that\nremains after denoising). Experimental findings show that against a gold\nstandard, the denoised-text-trained indexer indexing full texts, performs\neither better than or as good as its benchmark performance produced by a\nfull-text-trained indexer indexing full texts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 17:57:42 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Shams", "Rushdi", ""], ["Mercer", "Robert E.", ""]]}, {"id": "1204.2245", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Adel Elsayed", "title": "Development of a Conceptual Structure for a Domain-Specific Corpus", "comments": "3rd International Conference on Concept Maps (CMC2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The corpus reported in this paper was developed for the evaluation of a\ndomain-specific Text to Knowledge Mapping (TKM) prototype. The TKM prototype\noperates on the basis of both a combinatory categorical grammar (CCG)\nlinguistic model and a knowledge model that consists of three layers: ontology,\nqualitative and quantitative layers. In the course of this evaluation it was\nnecessary to populate these initial models with lexical items and semantic\nrelations. Both elements, the lexicon and semantic relations, are meant to\nreflect the domain of the prototype; hence both had to be extracted from the\ncorpus. While dealing with the lexicon was straight forward, the identification\nand extraction of appropriate semantic relations was much more involved. It was\nnecessary, therefore, to manually develop a conceptual structure for the domain\nwhich was then used to formulate a domain-specific framework of semantic\nrelations. The conceptual structure was developed using the Cmap tool of IHMC.\nThe framework of semantic relations- that has resulted from this study\nconsisted of 55 relations, out of which 42 have inverse relations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 18:47:49 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Shams", "Rushdi", ""], ["Elsayed", "Adel", ""]]}, {"id": "1204.2523", "submitter": "Khalid El-Arini", "authors": "Khalid El-Arini, Emily B. Fox, Carlos Guestrin", "title": "Concept Modeling with Superwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information retrieval, a fundamental goal is to transform a document into\nconcepts that are representative of its content. The term \"representative\" is\nin itself challenging to define, and various tasks require different\ngranularities of concepts. In this paper, we aim to model concepts that are\nsparse over the vocabulary, and that flexibly adapt their content based on\nother relevant semantic information such as textual structure or associated\nimage features. We explore a Bayesian nonparametric model based on nested beta\nprocesses that allows for inferring an unknown number of strictly sparse\nconcepts. The resulting model provides an inherently different representation\nof concepts than a standard LDA (or HDP) based topic model, and allows for\ndirect incorporation of semantic features. We demonstrate the utility of this\nrepresentation on multilingual blog data and the Congressional Record.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 18:53:58 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["El-Arini", "Khalid", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1204.2712", "submitter": "David Vallet David Vallet", "authors": "Sumio Fujita, Georges Dupret and Ricardo Baeza-Yates", "title": "Learning to Rank Query Recommendations by Semantic Similarities", "comments": "2nd International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),\n  Lyon, France, April 17th, 2012", "journal-ref": null, "doi": null, "report-no": "WWW2012USEWOD/2012/fuduba", "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logs of the interactions with a search engine show that users often\nreformulate their queries. Examining these reformulations shows that\nrecommendations that precise the focus of a query are helpful, like those based\non expansions of the original queries. But it also shows that queries that\nexpress some topical shift with respect to the original query can help user\naccess more rapidly the information they need. We propose a method to identify\nfrom the query logs of past users queries that either focus or shift the\ninitial query topic. This method combines various click-based, topic-based and\nsession based ranking strategies and uses supervised learning in order to\nmaximize the semantic similarities between the query and the recommendations,\nwhile at the same diversifying them. We evaluate our method using the\nquery/click logs of a Japanese web search engine and we show that the\ncombination of the three methods proposed is significantly better than any of\nthem taken individually.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 13:15:43 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Fujita", "Sumio", ""], ["Dupret", "Georges", ""], ["Baeza-Yates", "Ricardo", ""]]}, {"id": "1204.2713", "submitter": "Julia Hoxha", "authors": "Julia Hoxha, Martin Junghans and Sudhir Agarwal", "title": "Enabling Semantic Analysis of User Browsing Patterns in the Web of Data", "comments": "2nd International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),\n  Lyon, France, April 17th, 2012", "journal-ref": null, "doi": null, "report-no": "WWW2012USEWOD/2012/hojuag", "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A useful step towards better interpretation and analysis of the usage\npatterns is to formalize the semantics of the resources that users are\naccessing in the Web. We focus on this problem and present an approach for the\nsemantic formalization of usage logs, which lays the basis for eective\ntechniques of querying expressive usage patterns. We also present a query\nanswering approach, which is useful to nd in the logs expressive patterns of\nusage behavior via formulation of semantic and temporal-based constraints. We\nhave processed over 30 thousand user browsing sessions extracted from usage\nlogs of DBPedia and Semantic Web Dog Food. All these events are formalized\nsemantically using respective domain ontologies and RDF representations of the\nWeb resources being accessed. We show the eectiveness of our approach through\nexperimental results, providing in this way an exploratory analysis of the way\nusers browse theWeb of Data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 13:17:01 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2012 16:54:08 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2012 06:46:51 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Hoxha", "Julia", ""], ["Junghans", "Martin", ""], ["Agarwal", "Sudhir", ""]]}, {"id": "1204.2715", "submitter": "David Vallet David Vallet", "authors": "Magnus Knuth, Johannes Hercher and Harald Sack", "title": "Collaboratively Patching Linked Data", "comments": "2nd International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),\n  Lyon, France, April 17th, 2012", "journal-ref": null, "doi": null, "report-no": "WWW2012USEWOD/2012/knhesa", "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Web of Data is noisy. Linked Data often needs extensive preprocessing\nto enable efficient use of heterogeneous resources. While consistent and valid\ndata provides the key to efficient data processing and aggregation we are\nfacing two main challenges: (1st) Identification of erroneous facts and\ntracking their origins in dynamically connected datasets is a difficult task,\nand (2nd) efforts in the curation of deficient facts in Linked Data are\nexchanged rather rarely. Since erroneous data often is duplicated and\n(re-)distributed by mashup applications it is not only the responsibility of a\nfew original publishers to keep their data tidy, but progresses to be a mission\nfor all distributers and consumers of Linked Data too. We present a new\napproach to expose and to reuse patches on erroneous data to enhance and to add\nquality information to the Web of Data. The feasibility of our approach is\ndemonstrated by example of a collaborative game that patches statements in\nDBpedia data and provides notifications for relevant changes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 13:27:08 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Knuth", "Magnus", ""], ["Hercher", "Johannes", ""], ["Sack", "Harald", ""]]}, {"id": "1204.2718", "submitter": "David Vallet David Vallet", "authors": "Andreas Thalhammer, Ioan Toma, Antonio Roa-Valverde and Dieter Fensel", "title": "Leveraging Usage Data for Linked Data Movie Entity Summarization", "comments": "2nd International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),\n  Lyon, France, April 17th, 2012", "journal-ref": null, "doi": null, "report-no": "WWW2012USEWOD/2012/thtorofe", "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel research in the field of Linked Data focuses on the problem of entity\nsummarization. This field addresses the problem of ranking features according\nto their importance for the task of identifying a particular entity. Next to a\nmore human friendly presentation, these summarizations can play a central role\nfor semantic search engines and semantic recommender systems. In current\napproaches, it has been tried to apply entity summarization based on patterns\nthat are inherent to the regarded data.\n  The proposed approach of this paper focuses on the movie domain. It utilizes\nusage data in order to support measuring the similarity between movie entities.\nUsing this similarity it is possible to determine the k-nearest neighbors of an\nentity. This leads to the idea that features that entities share with their\nnearest neighbors can be considered as significant or important for these\nentities. Additionally, we introduce a downgrading factor (similar to TF-IDF)\nin order to overcome the high number of commonly occurring features. We\nexemplify the approach based on a movie-ratings dataset that has been linked to\nFreebase entities.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 13:31:52 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Thalhammer", "Andreas", ""], ["Toma", "Ioan", ""], ["Roa-Valverde", "Antonio", ""], ["Fensel", "Dieter", ""]]}, {"id": "1204.3362", "submitter": "Andreas Bauer", "authors": "Andreas Bauer, Christian Wolff", "title": "Event based classification of Web 2.0 text streams", "comments": "11 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web 2.0 applications like Twitter or Facebook create a continuous stream of\ninformation. This demands new ways of analysis in order to offer insight into\nthis stream right at the moment of the creation of the information, because\nlots of this data is only relevant within a short period of time. To address\nthis problem real time search engines have recently received increased\nattention. They take into account the continuous flow of information\ndifferently than traditional web search by incorporating temporal and social\nfeatures, that describe the context of the information during its creation.\nStandard approaches where data first get stored and then is processed from a\nperistent storage suffer from latency. We want to address the fluent and rapid\nnature of text stream by providing an event based approach that analyses\ndirectly the stream of information. In a first step we want to define the\ndifference between real time search and traditional search to clarify the\ndemands in modern text filtering. In a second step we want to show how event\nbased features can be used to support the tasks of real time search engines.\nUsing the example of Twitter we present in this paper a way how to combine an\nevent based approach with text mining and information filtering concepts in\norder to classify incoming information based on stream features. We calculate\nstream dependant features and feed them into a neural network in order to\nclassify the text streams. We show the separative capabilities of event based\nfeatures as the foundation for a real time search engine.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 04:50:13 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 12:31:37 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Bauer", "Andreas", ""], ["Wolff", "Christian", ""]]}, {"id": "1204.3471", "submitter": "Arockia Anand Raj", "authors": "Arockia Anand Raj and T. Mala", "title": "Cloudpress 2.0: A MapReduce Approach for News Retrieval on the Cloud", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of the Internet, the amount of news articles added every minute\nof everyday is humongous. As a result of this explosive amount of news\narticles, news retrieval systems are required to process the news articles\nfrequently and intensively. The news retrieval systems that are in-use today\nare not capable of coping up with these data-intensive computations. Cloudpress\n2.0 presented here, is designed and implemented to be scalable, robust and\nfault tolerant. It is designed in such a way that, all the processes involved\nin news retrieval such as fetching, pre-processing, indexing, storing and\nsummarizing, exploit MapReduce paradigm and use the power of the Cloud\ncomputing. It uses novel approaches for parallel processing, for storing the\nnews articles in a distributed database and for visualizing them as a 3D\nvisual. It uses Lucene-based indexing for efficient and faster retrieval. It\nalso includes a novel query expansion feature for searching the news articles.\nCloudpress 2.0 also allows on-the-fly, extractive summarization of news\narticles based on the input query.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 13:06:59 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Raj", "Arockia Anand", ""], ["Mala", "T.", ""]]}, {"id": "1204.3677", "submitter": "Sushovan De", "authors": "Yuheng Hu, Sushovan De, Yi Chen, Subbarao Kambhampati", "title": "Bayesian Data Cleaning for Web Data", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Cleaning is a long standing problem, which is growing in importance with\nthe mass of uncurated web data. State of the art approaches for handling\ninconsistent data are systems that learn and use conditional functional\ndependencies (CFDs) to rectify data. These methods learn data\npatterns--CFDs--from a clean sample of the data and use them to rectify the\ndirty/inconsistent data. While getting a clean training sample is feasible in\nenterprise data scenarios, it is infeasible in web databases where there is no\nseparate curated data. CFD based methods are unfortunately particularly\nsensitive to noise; we will empirically demonstrate that the number of CFDs\nlearned falls quite drastically with even a small amount of noise. In order to\novercome this limitation, we propose a fully probabilistic framework for\ncleaning data. Our approach involves learning both the generative and error\n(corruption) models of the data and using them to clean the data. For\ngenerative models, we learn Bayes networks from the data. For error models, we\nconsider a maximum entropy framework for combing multiple error processes. The\ngenerative and error models are learned directly from the noisy data. We\npresent the details of the framework and demonstrate its effectiveness in\nrectifying web data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 00:59:53 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Hu", "Yuheng", ""], ["De", "Sushovan", ""], ["Chen", "Yi", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1204.3731", "submitter": "Damiano Spina", "authors": "Arkaitz Zubiaga, Damiano Spina, Enrique Amig\\'o and Julio Gonzalo", "title": "Towards Real-Time Summarization of Scheduled Events from Twitter Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper explores the real-time summarization of scheduled events such as\nsoccer games from torrential flows of Twitter streams. We propose and evaluate\nan approach that substantially shrinks the stream of tweets in real-time, and\nconsists of two steps: (i) sub-event detection, which determines if something\nnew has occurred, and (ii) tweet selection, which picks a representative tweet\nto describe each sub-event. We compare the summaries generated in three\nlanguages for all the soccer games in \"Copa America 2011\" to reference live\nreports offered by Yahoo! Sports journalists. We show that simple text analysis\nmethods which do not involve external knowledge lead to summaries that cover\n84% of the sub-events on average, and 100% of key types of sub-events (such as\ngoals in soccer). Our approach should be straightforwardly applicable to other\nkinds of scheduled events such as other sports, award ceremonies, keynote\ntalks, TV shows, etc.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 08:58:39 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Spina", "Damiano", ""], ["Amig\u00f3", "Enrique", ""], ["Gonzalo", "Julio", ""]]}, {"id": "1204.5373", "submitter": "Chris De Vries", "authors": "Shlomo Geva and Christopher M. De Vries", "title": "TopSig: Topology Preserving Document Signatures", "comments": "12 pages, 8 figures, CIKM 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance comparisons between File Signatures and Inverted Files for text\nretrieval have previously shown several significant shortcomings of file\nsignatures relative to inverted files. The inverted file approach underpins\nmost state-of-the-art search engine algorithms, such as Language and\nProbabilistic models. It has been widely accepted that traditional file\nsignatures are inferior alternatives to inverted files. This paper describes\nTopSig, a new approach to the construction of file signatures. Many advances in\nsemantic hashing and dimensionality reduction have been made in recent times,\nbut these were not so far linked to general purpose, signature file based,\nsearch engines. This paper introduces a different signature file approach that\nbuilds upon and extends these recent advances. We are able to demonstrate\nsignificant improvements in the performance of signature file based indexing\nand retrieval, performance that is comparable to that of state of the art\ninverted file based systems, including Language models and BM25. These findings\nsuggest that file signatures offer a viable alternative to inverted files in\nsuitable settings and from the theoretical perspective it positions the file\nsignatures model in the class of Vector Space retrieval models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 13:36:39 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Geva", "Shlomo", ""], ["De Vries", "Christopher M.", ""]]}, {"id": "1204.6321", "submitter": "Konstantinos Chorianopoulos", "authors": "Ioannis Leftheriotis, Chrysoula Gkonela, Konstantinos Chorianopoulos", "title": "Efficient Video Indexing on the Web: A System that Leverages User\n  Interactions with a Video Player", "comments": "9 pages, 3 figures, UCMedia 2010: 2nd International ICST Conference\n  on User Centric Media", "journal-ref": null, "doi": "10.1007/978-3-642-35145-7_16", "report-no": null, "categories": "cs.MM cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a user-based video indexing method, that\nautomatically generates thumbnails of the most important scenes of an online\nvideo stream, by analyzing users' interactions with a web video player. As a\ntest bench to verify our idea we have extended the YouTube video player into\nthe VideoSkip system. In addition, VideoSkip uses a web-database (Google\nApplication Engine) to keep a record of some important parameters, such as the\ntiming of basic user actions (play, pause, skip). Moreover, we implemented an\nalgorithm that selects representative thumbnails. Finally, we populated the\nsystem with data from an experiment with nine users. We found that the\nVideoSkip system indexes video content by leveraging implicit users\ninteractions, such as pause and thirty seconds skip. Our early findings point\ntoward improvements of the web video player and its thumbnail generation\ntechnique. The VideSkip system could compliment content-based algorithms, in\norder to achieve efficient video-indexing in difficult videos, such as lectures\nor sports.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 20:00:20 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Leftheriotis", "Ioannis", ""], ["Gkonela", "Chrysoula", ""], ["Chorianopoulos", "Konstantinos", ""]]}, {"id": "1204.6362", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Adel Elsayed", "title": "A Corpus-based Evaluation of Lexical Components of a Domainspecific Text\n  to Knowledge Mapping Prototype", "comments": "2008 IEEE International Conference on Computer and Information\n  Technology (ICCIT 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The aim of this paper is to evaluate the lexical components of a Text to\nKnowledge Mapping (TKM) prototype. The prototype is domain-specific, the\npurpose of which is to map instructional text onto a knowledge domain. The\ncontext of the knowledge domain of the prototype is physics, specifically DC\nelectrical circuits. During development, the prototype has been tested with a\nlimited data set from the domain. The prototype now reached a stage where it\nneeds to be evaluated with a representative linguistic data set called corpus.\nA corpus is a collection of text drawn from typical sources which can be used\nas a test data set to evaluate NLP systems. As there is no available corpus for\nthe domain, we developed a representative corpus and annotated it with\nlinguistic information. The evaluation of the prototype considers one of its\ntwo main components- lexical knowledge base. With the corpus, the evaluation\nenriches the lexical knowledge resources like vocabulary and grammar structure.\nThis leads the prototype to parse a reasonable amount of sentences in the\ncorpus.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 03:48:16 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Shams", "Rushdi", ""], ["Elsayed", "Adel", ""]]}, {"id": "1204.6521", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "Harnessing Folksonomies for Resource Classification", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In our daily lives, organizing resources into a set of categories is a common\ntask. Categorization becomes more useful as the collection of resources\nincreases. Large collections of books, movies, and web pages, for instance, are\ncataloged in libraries, organized in databases and classified in directories,\nrespectively. However, the usual largeness of these collections requires a vast\nendeavor and an outrageous expense to organize manually.\n  Recent research is moving towards developing automated classifiers that\nreduce the increasing costs and effort of the task. Little work has been done\nanalyzing the appropriateness of and exploring how to harness the annotations\nprovided by users on social tagging systems as a data source. Users on these\nsystems save resources as bookmarks in a social environment by attaching\nannotations in the form of tags. It has been shown that these tags facilitate\nretrieval of resources not only for the annotators themselves but also for the\nwhole community. Likewise, these tags provide meaningful metadata that refers\nto the content of the resources.\n  In this thesis, we deal with the utilization of these user-provided tags in\nsearch of the most accurate classification of resources as compared to\nexpert-driven categorizations. To the best of our knowledge, this is the first\nresearch work performing actual classification experiments utilizing social\ntags. By exploring the characteristics and nature of these systems and the\nunderlying folksonomies, this thesis sheds new light on the way of getting the\nmost out of social tags for the sake of automated resource classification\ntasks. Therefore, we believe that the contributions in this work are of utmost\ninterest for future researchers in the field, as well as for the scientific\ncommunity in order to better understand these systems and further utilize the\nknowledge garnered from social tags.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 21:11:05 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "1204.6610", "submitter": "Jia Zeng", "authors": "Jia Zeng, Xiao-Qin Cao and Zhi-Qiang Liu", "title": "Residual Belief Propagation for Topic Modeling", "comments": "6 pages, 8 figures", "journal-ref": "Advanced Data Mining and Applications Lecture Notes in Computer\n  Science Volume 7713, 739-752, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast convergence speed is a desired property for training latent Dirichlet\nallocation (LDA), especially in online and parallel topic modeling for massive\ndata sets. This paper presents a novel residual belief propagation (RBP)\nalgorithm to accelerate the convergence speed for training LDA. The proposed\nRBP uses an informed scheduling scheme for asynchronous message passing, which\npasses fast-convergent messages with a higher priority to influence those\nslow-convergent messages at each learning iteration. Extensive empirical\nstudies confirm that RBP significantly reduces the training time until\nconvergence while achieves a much lower predictive perplexity than other\nstate-of-the-art training algorithms for LDA, including variational Bayes (VB),\ncollapsed Gibbs sampling (GS), loopy belief propagation (BP), and residual VB\n(RVB).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 12:18:40 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Zeng", "Jia", ""], ["Cao", "Xiao-Qin", ""], ["Liu", "Zhi-Qiang", ""]]}]