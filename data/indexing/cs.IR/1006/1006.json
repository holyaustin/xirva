[{"id": "1006.0289", "submitter": "Carlos Lorenzetti", "authors": "Carlos M. Lorenzetti and Roc\\'io L. Cecchini and Ana G. Maguitman and\n  Andr\\'as A. Bencz\\'ur", "title": "M\\'{e}todos para la Selecci\\'{o}n y el Ajuste de Caracter\\'{i}sticas en\n  el Problema de la Detecci\\'{o}n de Spam", "comments": "5 pages, 1 figure, Workshop de Investigadores en Ciencias de la\n  Computaci\\'{o}n, WICC 2010, pp 48-52", "journal-ref": "Workshop de Investigadores en Ciencias de la Computacion, WICC\n  2010, El Calafate, Santa Cruz, Argentina", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The email is used daily by millions of people to communicate around the globe\nand it is a mission-critical application for many businesses. Over the last\ndecade, unsolicited bulk email has become a major problem for email users. An\noverwhelming amount of spam is flowing into users' mailboxes daily. In 2004, an\nestimated 62% of all email was attributed to spam. Spam is not only frustrating\nfor most email users, it strains the IT infrastructure of organizations and\ncosts businesses billions of dollars in lost productivity. In recent years,\nspam has evolved from an annoyance into a serious security threat, and is now a\nprime medium for phishing of sensitive information, as well the spread of\nmalicious software. This work presents a first approach to attack the spam\nproblem. We propose an algorithm that will improve a classifier's results by\nadjusting its training set data. It improves the document's vocabulary\nrepresentation by detecting good topic descriptors and discriminators.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 03:48:49 GMT"}, {"version": "v2", "created": "Thu, 14 Oct 2010 15:43:13 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Lorenzetti", "Carlos M.", ""], ["Cecchini", "Roc\u00edo L.", ""], ["Maguitman", "Ana G.", ""], ["Bencz\u00far", "Andr\u00e1s A.", ""]]}, {"id": "1006.1029", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin, Dimitar Hristovski", "title": "Chi-square-based scoring function for categorization of MEDLINE\n  citations", "comments": "34 pages, 2 figures", "journal-ref": "Methods of Information in Medicine, 2010;49(4):371-380", "doi": "10.3414/ME09-01-0009", "report-no": null, "categories": "cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Text categorization has been used in biomedical informatics for\nidentifying documents containing relevant topics of interest. We developed a\nsimple method that uses a chi-square-based scoring function to determine the\nlikelihood of MEDLINE citations containing genetic relevant topic. Methods: Our\nprocedure requires construction of a genetic and a nongenetic domain document\ncorpus. We used MeSH descriptors assigned to MEDLINE citations for this\ncategorization task. We compared frequencies of MeSH descriptors between two\ncorpora applying chi-square test. A MeSH descriptor was considered to be a\npositive indicator if its relative observed frequency in the genetic domain\ncorpus was greater than its relative observed frequency in the nongenetic\ndomain corpus. The output of the proposed method is a list of scores for all\nthe citations, with the highest score given to those citations containing MeSH\ndescriptors typical for the genetic domain. Results: Validation was done on a\nset of 734 manually annotated MEDLINE citations. It achieved predictive\naccuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method\nby comparing it to three machine learning algorithms (support vector machines,\ndecision trees, na\\\"ive Bayes). Although the differences were not statistically\nsignificantly different, results showed that our chi-square scoring performs as\ngood as compared machine learning algorithms. Conclusions: We suggest that the\nchi-square scoring is an effective solution to help categorize MEDLINE\ncitations. The algorithm is implemented in the BITOLA literature-based\ndiscovery support system as a preprocessor for gene symbol disambiguation\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:14:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""], ["Hristovski", "Dimitar", ""]]}, {"id": "1006.1184", "submitter": "Secretary Aircc Journal", "authors": "Natarajan Meghanathan, Nataliya Kostyuk, Raphael Isokpehi, Hari Cohly", "title": "An Algorithm to Self-Extract Secondary Keywords and Their Combinations\n  Based on Abstracts Collected using Primary Keywords from Online Digital\n  Libraries", "comments": "9 Pages", "journal-ref": "International Journal of Computer Science and Information\n  Technology 2.3 (2010) 93-101", "doi": "10.5121/ijcsit.2010.2307", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The high-level contribution of this paper is the development and\nimplementation of an algorithm to selfextract secondary keywords and their\ncombinations (combo words) based on abstracts collected using standard primary\nkeywords for research areas from reputed online digital libraries like IEEE\nExplore, PubMed Central and etc. Given a collection of N abstracts, we\narbitrarily select M abstracts (M<< N; M/N as low as 0.15) and parse each of\nthe M abstracts, word by word. Upon the first-time appearance of a word, we\nquery the user for classifying the word into an Accept-List or non-Accept-List.\nThe effectiveness of the training approach is evaluated by measuring the\npercentage of words for which the user is queried for classification when the\nalgorithm parses through the words of each of the M abstracts. We observed that\nas M grows larger, the percentage of words for which the user is queried for\nclassification reduces drastically. After the list of acceptable words is built\nby parsing the M abstracts, we now parse all the N abstracts, word by word, and\ncount the frequency of appearance of each of the words in Accept-List in these\nN abstracts. We also construct a Combo-Accept-List comprising of all possible\ncombinations of the single keywords in Accept-List and parse all the N\nabstracts, two successive words (combo word) at a time, and count the frequency\nof appearance of each of the combo words in the Combo-Accept-List in these N\nabstracts.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 06:52:44 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Meghanathan", "Natarajan", ""], ["Kostyuk", "Nataliya", ""], ["Isokpehi", "Raphael", ""], ["Cohly", "Hari", ""]]}, {"id": "1006.2880", "submitter": "Bahman Bahmani", "authors": "Bahman Bahmani, Abdur Chowdhury, Ashish Goel", "title": "Fast Incremental and Personalized PageRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the efficiency of Monte Carlo methods for\nincremental computation of PageRank, personalized PageRank, and similar random\nwalk based methods (with focus on SALSA), on large-scale dynamically evolving\nsocial networks. We assume that the graph of friendships is stored in\ndistributed shared memory, as is the case for large social networks such as\nTwitter.\n  For global PageRank, we assume that the social network has $n$ nodes, and $m$\nadversarially chosen edges arrive in a random order. We show that with a reset\nprobability of $\\epsilon$, the total work needed to maintain an accurate\nestimate (using the Monte Carlo method) of the PageRank of every node at all\ntimes is $O(\\frac{n\\ln m}{\\epsilon^{2}})$. This is significantly better than\nall known bounds for incremental PageRank. For instance, if we naively\nrecompute the PageRanks as each edge arrives, the simple power iteration method\nneeds $\\Omega(\\frac{m^2}{\\ln(1/(1-\\epsilon))})$ total time and the Monte Carlo\nmethod needs $O(mn/\\epsilon)$ total time; both are prohibitively expensive.\nFurthermore, we also show that we can handle deletions equally efficiently.\n  We then study the computation of the top $k$ personalized PageRanks starting\nfrom a seed node, assuming that personalized PageRanks follow a power-law with\nexponent $\\alpha < 1$. We show that if we store $R>q\\ln n$ random walks\nstarting from every node for large enough constant $q$ (using the approach\noutlined for global PageRank), then the expected number of calls made to the\ndistributed social network database is $O(k/(R^{(1-\\alpha)/\\alpha}))$.\n  We also present experimental results from the social networking site,\nTwitter, verifying our assumptions and analyses. The overall result is that\nthis algorithm is fast enough for real-time queries over a dynamic social\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 03:37:51 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 14:17:30 GMT"}], "update_date": "2010-09-01", "authors_parsed": [["Bahmani", "Bahman", ""], ["Chowdhury", "Abdur", ""], ["Goel", "Ashish", ""]]}, {"id": "1006.3425", "submitter": "Dmitry Lande", "authors": "D.V. Lande, A.A. Snarskii", "title": "Power law in website ratings", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the practical work of websites popularization, analysis of their\nefficiency and downloading it is of key importance to take into account\nweb-ratings data. The main indicators of website traffic include the number of\nunique hosts from which the analyzed website was addressed and the number of\ngranted web pages (hits) per unit time (for example, day, month or year). Of\ncertain interest is the ratio between the number of hits (S) and hosts (H). In\npractice there is even used such a concept as \"average number of viewed pages\"\n(S/H), which on default supposes a linear dependence of S on H. What actually\nhappens is that linear dependence is observed only as a partial case of power\ndependence, and not always. Another new power law has been discovered on the\nInternet, in particular, on the WWW.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 10:52:07 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Lande", "D. V.", ""], ["Snarskii", "A. A.", ""]]}, {"id": "1006.3498", "submitter": "Ugo Scaiella", "authors": "Paolo Ferragina and Ugo Scaiella", "title": "Fast and accurate annotation of short texts with Wikipedia pages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of cross-referencing text fragments with Wikipedia\npages, in a way that synonymy and polysemy issues are resolved accurately and\nefficiently. We take inspiration from a recent flow of work [Cucerzan 2007,\nMihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti et al 2009], and\nextend their scenario from the annotation of long documents to the annotation\nof short texts, such as snippets of search-engine results, tweets, news, blogs,\netc.. These short and poorly composed texts pose new challenges in terms of\nefficiency and effectiveness of the annotation process, that we address by\ndesigning and engineering TAGME, the first system that performs an accurate and\non-the-fly annotation of these short textual fragments. A large set of\nexperiments shows that TAGME outperforms state-of-the-art algorithms when they\nare adapted to work on short texts and it results fast and competitive on long\ntexts.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 15:43:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2010 11:59:11 GMT"}], "update_date": "2010-07-29", "authors_parsed": [["Ferragina", "Paolo", ""], ["Scaiella", "Ugo", ""]]}, {"id": "1006.3514", "submitter": "Rajendra Shinde", "authors": "Rajendra Shinde, Ashish Goel, Pankaj Gupta, Debojyoti Dutta", "title": "Similarity Search and Locality Sensitive Hashing using TCAMs", "comments": "11 pages, in SIGMOD 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search methods are widely used as kernels in various machine\nlearning applications. Nearest neighbor search (NNS) algorithms are often used\nto retrieve similar entries, given a query. While there exist efficient\ntechniques for exact query lookup using hashing, similarity search using exact\nnearest neighbors is known to be a hard problem and in high dimensions, best\nknown solutions offer little improvement over a linear scan. Fast solutions to\nthe approximate NNS problem include Locality Sensitive Hashing (LSH) based\ntechniques, which need storage polynomial in $n$ with exponent greater than\n$1$, and query time sublinear, but still polynomial in $n$, where $n$ is the\nsize of the database. In this work we present a new technique of solving the\napproximate NNS problem in Euclidean space using a Ternary Content Addressable\nMemory (TCAM), which needs near linear space and has O(1) query time. In fact,\nthis method also works around the best known lower bounds in the cell probe\nmodel for the query time using a data structure near linear in the size of the\ndata base. TCAMs are high performance associative memories widely used in\nnetworking applications such as access control lists. A TCAM can query for a\nbit vector within a database of ternary vectors, where every bit position\nrepresents $0$, $1$ or $*$. The $*$ is a wild card representing either a $0$ or\na $1$. We leverage TCAMs to design a variant of LSH, called Ternary Locality\nSensitive Hashing (TLSH) wherein we hash database entries represented by\nvectors in the Euclidean space into $\\{0,1,*\\}$. By using the added\nfunctionality of a TLSH scheme with respect to the $*$ character, we solve an\ninstance of the approximate nearest neighbor problem with 1 TCAM access and\nstorage nearly linear in the size of the database. We believe that this work\ncan open new avenues in very high speed data mining.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 16:51:51 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Shinde", "Rajendra", ""], ["Goel", "Ashish", ""], ["Gupta", "Pankaj", ""], ["Dutta", "Debojyoti", ""]]}, {"id": "1006.4114", "submitter": "Liang Wang", "authors": "Wang Liang, Fang Bo", "title": "How to build a DNA search engine like Google?", "comments": "5 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.ET cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper proposed a new method to build the large scale DNA sequences\nsearch system based on web search engine technology. We give a very brief\nintroduction for the methods used in search engine first. Then how to build a\nDNA search system like Google is illustrated in detail. Since there is no local\nalignment process, this system is able to provide the ms level search services\nfor billions of DNA sequences in a typical server.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 16:41:46 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2010 07:25:59 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2010 02:49:40 GMT"}, {"version": "v4", "created": "Mon, 10 Oct 2011 02:20:14 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Liang", "Wang", ""], ["Bo", "Fang", ""]]}, {"id": "1006.4270", "submitter": "Dima Shepelyansky L", "authors": "A.O.Zhirov, O.V.Zhirov, D.L.Shepelyansky", "title": "Two-dimensional ranking of Wikipedia articles", "comments": "RevTex 9 pages, data, discussion added, more data at\n  http://www.quantware.ups-tlse.fr/QWLIB/2drankwikipedia/", "journal-ref": "Eur. Phys. J. B v.77, p.523 (2010)", "doi": "10.1140/epjb/e2010-10500-7", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Library of Babel, described by Jorge Luis Borges, stores an enormous\namount of information. The Library exists {\\it ab aeterno}. Wikipedia, a free\nonline encyclopaedia, becomes a modern analogue of such a Library. Information\nretrieval and ranking of Wikipedia articles become the challenge of modern\nsociety. While PageRank highlights very well known nodes with many ingoing\nlinks, CheiRank highlights very communicative nodes with many outgoing links.\nIn this way the ranking becomes two-dimensional. Using CheiRank and PageRank we\nanalyze the properties of two-dimensional ranking of all Wikipedia English\narticles and show that it gives their reliable classification with rich and\nnontrivial features. Detailed studies are done for countries, universities,\npersonalities, physicists, chess players, Dow-Jones companies and other\ncategories.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2010 11:43:35 GMT"}, {"version": "v2", "created": "Mon, 20 Sep 2010 09:57:22 GMT"}], "update_date": "2010-11-15", "authors_parsed": [["Zhirov", "A. O.", ""], ["Zhirov", "O. V.", ""], ["Shepelyansky", "D. L.", ""]]}, {"id": "1006.4458", "submitter": "Shrinivaasan Ka", "authors": "Ka.Shrinivaasan", "title": "Few Algorithms for ascertaining merit of a document and their\n  applications", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models for ranking documents(mostly in world wide web) are prestige\nbased. In this article, three algorithms to objectively judge the merit of a\ndocument are proposed - 1) Citation graph maxflow 2) Recursive Gloss Overlap\nbased intrinsic merit scoring and 3) Interview algorithm. A short discussion on\ngeneric judgement and its mathematical treatment is presented in introduction\nto motivate these algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 09:50:06 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Shrinivaasan", "Ka.", ""]]}, {"id": "1006.4535", "submitter": "William Jackson", "authors": "Judith Gelernter, Dong Cao and Jaime Carbonell", "title": "Studies on Relevance, Ranking and Results Display", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2 No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the extent to which users with the same query agree as\nto what is relevant, and how what is considered relevant may translate into a\nretrieval algorithm and results display. To combine user perceptions of\nrelevance with algorithm rank and to present results, we created a prototype\ndigital library of scholarly literature. We confine studies to one population\nof scientists (paleontologists), one domain of scholarly scientific articles\n(paleo-related), and a prototype system (PaleoLit) that we built for the\npurpose. Based on the principle that users do not pre-suppose answers to a\ngiven query but that they will recognize what they want when they see it, our\nsystem uses a rules-based algorithm to cluster results into fuzzy categories\nwith three relevance levels. Our system matches at least 1/3 of our\nparticipants' relevancy ratings 87% of the time. Our subsequent usability study\nfound that participants trusted our uncertainty labels but did not value our\ncolor-coded horizontal results layout above a standard retrieval list. We posit\nthat users make such judgments in limited time, and that time optimization per\ntask might help explain some of our findings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 14:43:22 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Gelernter", "Judith", ""], ["Cao", "Dong", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1006.4568", "submitter": "William Jackson", "authors": "Hui Hui Wang, Dzulkifli Mohamad and N. A. Ismail", "title": "Approaches, Challenges and Future Direction of Image Retrieval", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to discuss the evolution of the retrieval approaches\nfocusing on development, challenges and future direction of the image\nretrieval. It highlights both the already addressed and outstanding issues. The\nexplosive growth of image data leads to the need of research and development of\nImage Retrieval. However, Image retrieval researches are moving from keyword,\nto low level features and to semantic features. Drive towards semantic features\nis due to the problem of the keywords which can be very subjective and time\nconsuming while low level features cannot always describe high level concepts\nin the users' mind. Hence, introducing an interpretation inconsistency between\nimage descriptors and high level semantics that known as the semantic gap. This\npaper also discusses the semantic gap issues, user query mechanisms as well as\ncommon ways used to bridge the gap in image retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:21:11 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wang", "Hui Hui", ""], ["Mohamad", "Dzulkifli", ""], ["Ismail", "N. A.", ""]]}, {"id": "1006.4953", "submitter": "Jacint Szabo", "authors": "Istv\\'an B\\'ir\\'o and J\\'acint Szab\\'o", "title": "Large scale link based latent Dirichlet allocation for web document\n  classification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the applicability of latent Dirichlet allocation\n(LDA) for classifying large Web document collections. One of our main results\nis a novel influence model that gives a fully generative model of the document\ncontent taking linkage into account. In our setup, topics propagate along links\nin such a way that linked documents directly influence the words in the linking\ndocument. As another main contribution we develop LDA specific boosting of\nGibbs samplers resulting in a significant speedup in our experiments. The\ninferred LDA model can be applied for classification as dimensionality\nreduction similarly to latent semantic indexing. In addition, the model yields\nlink weights that can be applied in algorithms to process the Web graph; as an\nexample we deploy LDA link weights in stacked graphical learning. By using\nWeka's BayesNet classifier, in terms of the AUC of classification, we achieve\n4% improvement over plain LDA with BayesNet and 18% over tf.idf with SVM. Our\nGibbs sampling strategies yield about 5-10 times speedup with less than 1%\ndecrease in accuracy in terms of likelihood and AUC of classification.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 10:19:09 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["B\u00edr\u00f3", "Istv\u00e1n", ""], ["Szab\u00f3", "J\u00e1cint", ""]]}, {"id": "1006.5040", "submitter": "Andrew Krizhanovsky A", "authors": "A. A. Krizhanovsky", "title": "The comparison of Wiktionary thesauri transformed into the\n  machine-readable format", "comments": "23 pages, 3 tables, 6 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Wiktionary is a unique, peculiar, valuable and original resource for natural\nlanguage processing (NLP). The paper describes an open-source Wiktionary\nparser: its architecture and requirements followed by a description of\nWiktionary features to be taken into account, some open problems of Wiktionary\nand the parser. The current implementation of the parser extracts the\ndefinitions, semantic relations, and translations from English and Russian\nWiktionaries. The paper's goal is to interest researchers (1) in using the\nconstructed machine-readable dictionary for different NLP tasks, (2) in\nextending the software to parse 170 still unused Wiktionaries. The comparison\nof a number and types of semantic relations, a number of definitions, and a\nnumber of translations in the English Wiktionary and the Russian Wiktionary has\nbeen carried out. It was found that the number of semantic relations in the\nEnglish Wiktionary is larger by 1.57 times than in Russian (157 and 100\nthousands). But the Russian Wiktionary has more \"rich\" entries (with a big\nnumber of semantic relations), e.g. the number of entries with three or more\nsemantic relations is larger by 1.63 times than in the English Wiktionary. Upon\ncomparison, it was found out the methodological shortcomings of the Wiktionary.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 18:51:13 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Krizhanovsky", "A. A.", ""]]}, {"id": "1006.5059", "submitter": "Claudine Badue", "authors": "Claudine Badue, Jussara Almeida, Virgilio Almeida, Ricardo\n  Baeza-Yates, Berthier Ribeiro-Neto, Artur Ziviani, Nivio Ziviani", "title": "Capacity Planning for Vertical Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertical search engines focus on specific slices of content, such as the Web\nof a single country or the document collection of a large corporation. Despite\nthis, like general open web search engines, they are expensive to maintain,\nexpensive to operate, and hard to design. Because of this, predicting the\nresponse time of a vertical search engine is usually done empirically through\nexperimentation, requiring a costly setup. An alternative is to develop a model\nof the search engine for predicting performance. However, this alternative is\nof interest only if its predictions are accurate. In this paper we propose a\nmethodology for analyzing the performance of vertical search engines. Applying\nthe proposed methodology, we present a capacity planning model based on a\nqueueing network for search engines with a scale typically suitable for the\nneeds of large corporations. The model is simple and yet reasonably accurate\nand, in contrast to previous work, considers the imbalance in query service\ntimes among homogeneous index servers. We discuss how we tune up the model and\nhow we apply it to predict the impact on the query response time when\nparameters such as CPU and disk capacities are changed. This allows a manager\nof a vertical search engine to determine a priori whether a new configuration\nof the system might keep the query response under specified performance\nconstraints.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 20:22:08 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Badue", "Claudine", ""], ["Almeida", "Jussara", ""], ["Almeida", "Virgilio", ""], ["Baeza-Yates", "Ricardo", ""], ["Ribeiro-Neto", "Berthier", ""], ["Ziviani", "Artur", ""], ["Ziviani", "Nivio", ""]]}, {"id": "1006.5278", "submitter": "William Nzoukou", "authors": "Dhoha Almazro and Ghadeer Shahatah and Lamia Albdulkarim and Mona\n  Kherees and Romy Martinez and William Nzoukou", "title": "A Survey Paper on Recommender Systems", "comments": "This paper has some typos in it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems apply data mining techniques and prediction algorithms to\npredict users' interest on information, products and services among the\ntremendous amount of available items. The vast growth of information on the\nInternet as well as number of visitors to websites add some key challenges to\nrecommender systems. These are: producing accurate recommendation, handling\nmany recommendations efficiently and coping with the vast growth of number of\nparticipants in the system. Therefore, new recommender system technologies are\nneeded that can quickly produce high quality recommendations even for huge data\nsets.\n  To address these issues we have explored several collaborative filtering\ntechniques such as the item based approach, which identify relationship between\nitems and indirectly compute recommendations for users based on these\nrelationships. The user based approach was also studied, it identifies\nrelationships between users of similar tastes and computes recommendations\nbased on these relationships.\n  In this paper, we introduce the topic of recommender system. It provides ways\nto evaluate efficiency, scalability and accuracy of recommender system. The\npaper also analyzes different algorithms of user based and item based\ntechniques for recommendation generation. Moreover, a simple experiment was\nconducted using a data mining application -Weka- to apply data mining\nalgorithms to recommender system. We conclude by proposing our approach that\nmight enhance the quality of recommender systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 07:20:28 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 07:37:17 GMT"}, {"version": "v3", "created": "Mon, 13 Sep 2010 04:16:30 GMT"}, {"version": "v4", "created": "Fri, 24 Dec 2010 07:22:48 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Almazro", "Dhoha", ""], ["Shahatah", "Ghadeer", ""], ["Albdulkarim", "Lamia", ""], ["Kherees", "Mona", ""], ["Martinez", "Romy", ""], ["Nzoukou", "William", ""]]}]