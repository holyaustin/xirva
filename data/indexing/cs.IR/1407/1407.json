[{"id": "1407.0165", "submitter": "Beatriz Garcia-Jimenez", "authors": "Beatriz Garc\\'ia-Jim\\'enez and Mark D. Wilkinson", "title": "Automatic annotation of bioinformatics workflows with biomedical\n  ontologies", "comments": "6th International Symposium on Leveraging Applications (ISoLA 2014\n  conference), 15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legacy scientific workflows, and the services within them, often present\nscarce and unstructured (i.e. textual) descriptions. This makes it difficult to\nfind, share and reuse them, thus dramatically reducing their value to the\ncommunity. This paper presents an approach to annotating workflows and their\nsubcomponents with ontology terms, in an attempt to describe these artifacts in\na structured way. Despite a dearth of even textual descriptions, we\nautomatically annotated 530 myExperiment bioinformatics-related workflows,\nincluding more than 2600 workflow-associated services, with relevant\nontological terms. Quantitative evaluation of the Information Content of these\nterms suggests that, in cases where annotation was possible at all, the\nannotation quality was comparable to manually curated bioinformatics resources.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 09:53:07 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Garc\u00eda-Jim\u00e9nez", "Beatriz", ""], ["Wilkinson", "Mark D.", ""]]}, {"id": "1407.0167", "submitter": "Moritz Schubotz", "authors": "Robert Pagael and Moritz Schubotz", "title": "Mathematical Language Processing Project", "comments": "8 pages, one figure, Conferences on Intelligent Computer Mathematics\n  (CICM) 2014", "journal-ref": null, "doi": null, "report-no": "urn:nbn:de:0074-1186-1", "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language, words and phrases themselves imply the semantics. In\ncontrast, the meaning of identifiers in mathematical formulae is undefined.\nThus scientists must study the context to decode the meaning. The Mathematical\nLanguage Processing (MLP) project aims to support that process. In this paper,\nwe compare two approaches to discover identifier-definition tuples. At first we\nuse a simple pattern matching approach. Second, we present the MLP approach\nthat uses part-of-speech tag based distances as well as sentence positions to\ncalculate identifier-definition probabilities. The evaluation of our\nprototypical system, applied on the Wikipedia text corpus, shows that our\napproach augments the user experience substantially. While hovering the\nidentifiers in the formula, tool-tips with the most probable definitions occur.\nTests with random samples show that the displayed definitions provide a good\nmatch with the actual meaning of the identifiers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 09:56:42 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Pagael", "Robert", ""], ["Schubotz", "Moritz", ""]]}, {"id": "1407.0374", "submitter": "Walter Lasecki", "authors": "Juan Galan-Paez, and Joaqu\\'in Borrego-D\\'iaz", "title": "Discovering New Sentiments from the Social Web", "comments": null, "journal-ref": null, "doi": null, "report-no": "ci-2014/135", "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A persistent challenge in Complex Systems (CS) research is the\nphenomenological reconstruction of systems from raw data. In order to face the\nproblem, the use of sound features to reason on the system from data processing\nis a key step. In the specific case of complex societal systems, sentiment\nanalysis allows to mirror (part of) the affective dimension. However it is not\nreasonable to think that individual sentiment categorization can encompass the\nnew affective phenomena in digital social networks.\n  The present papers addresses the problem of isolating sentiment concepts\nwhich emerge in social networks. In an analogy to Artificial Intelligent\nSingularity, we propose the study and analysis of these new complex sentiment\nstructures and how they are similar to or diverge from classic conceptual\nstructures associated to sentiment lexicons. The conjecture is that it is\nhighly probable that hypercomplex sentiment structures -not explained with\nhuman categorizations- emerge from high dynamic social information networks.\nRoughly speaking, new sentiment can emerge from the new global nervous systems\nas it occurs in humans.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 02:27:14 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Galan-Paez", "Juan", ""], ["Borrego-D\u00edaz", "Joaqu\u00edn", ""]]}, {"id": "1407.0481", "submitter": "Konstantinos Kotis", "authors": "Konstantinos Kotis, Iraklis Athanasakis, George Vouros", "title": "Semantic Integration & Single-Site Opening of Multiple Governmental Data\n  Sources", "comments": "21 pages, 7 figures, live demo at\n  http://www.samos.gr/apps/s3-ai/eGovTicketApp.xhtml", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In many cases, government data is still \"locked\" in several \"data silos\",\neven within the boundaries of a single (inter-)national public organization\nwith disparate and distributed organizational units and departments spread\nacross multiple sites. Opening data and enabling its unified querying from a\nsingle site in an efficient and effective way is a semantic application\nintegration and open government data challenge. This paper describes how NARA\nis using Semantic Web technology to implement an application integration\napproach within the boundaries of its organization via opening and querying\nmultiple governmental data sources from a single site. The generic approach\nproposed, namely S3-AI, provides support to answering unified,\nontology-mediated, federated queries to data produced and exploited by\ndisparate applications, while these are being located in different\norganizational sites. S3-AI preserves ownership, autonomy and independency of\napplications and data. The paper extensively demonstrates S3-AI, using the D2RQ\nand Fuseki technologies, for addressing the needs of a governmental \"IT\nhelpdesk support\" case.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 08:40:41 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Kotis", "Konstantinos", ""], ["Athanasakis", "Iraklis", ""], ["Vouros", "George", ""]]}, {"id": "1407.0623", "submitter": "Lamberto Ballan", "authors": "Lamberto Ballan, Marco Bertini, Giuseppe Serra, Alberto Del Bimbo", "title": "A Data-Driven Approach for Tag Refinement and Localization in Web Videos", "comments": "Preprint submitted to Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2015.05.009", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging of visual content is becoming more and more widespread as web-based\nservices and social networks have popularized tagging functionalities among\ntheir users. These user-generated tags are used to ease browsing and\nexploration of media collections, e.g. using tag clouds, or to retrieve\nmultimedia content. However, not all media are equally tagged by users. Using\nthe current systems is easy to tag a single photo, and even tagging a part of a\nphoto, like a face, has become common in sites like Flickr and Facebook. On the\nother hand, tagging a video sequence is more complicated and time consuming, so\nthat users just tag the overall content of a video. In this paper we present a\nmethod for automatic video annotation that increases the number of tags\noriginally provided by users, and localizes them temporally, associating tags\nto keyframes. Our approach exploits collective knowledge embedded in\nuser-generated tags and web sources, and visual similarity of keyframes and\nimages uploaded to social sites like YouTube and Flickr, as well as web sources\nlike Google and Bing. Given a keyframe, our method is able to select on the fly\nfrom these visual sources the training exemplars that should be the most\nrelevant for this test sample, and proceeds to transfer labels across similar\nimages. Compared to existing video tagging approaches that require training\nclassifiers for each tag, our system has few parameters, is easy to implement\nand can deal with an open vocabulary scenario. We demonstrate the approach on\ntag refinement and localization on DUT-WEBV, a large dataset of web videos, and\nshow state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 15:48:37 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2015 18:12:36 GMT"}, {"version": "v3", "created": "Thu, 28 May 2015 17:12:54 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Ballan", "Lamberto", ""], ["Bertini", "Marco", ""], ["Serra", "Giuseppe", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1407.0822", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM), B\\'en\\'edicte Le Grand (CRI), Boris\n  Golden (Viadeo), Fabrice Rossi (SAMM)", "title": "Reducing Offline Evaluation Bias in Recommendation Systems", "comments": "23rd annual Belgian-Dutch Conference on Machine Learning (Benelearn\n  2014), Bruxelles : Belgium (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems. They tailor those systems to individual users by filtering and ranking\ninformation according to user profiles. This adaptation process influences the\nway users interact with the system and, as a consequence, increases the\ndifficulty of evaluating a recommendation algorithm with historical data (via\noffline evaluation). This paper analyses this evaluation bias and proposes a\nsimple item weighting solution that reduces its impact. The efficiency of the\nproposed solution is evaluated on real world data extracted from Viadeo\nprofessional social network.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 09:05:33 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Golden", "Boris", "", "Viadeo"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1407.1133", "submitter": "Tarun Bhalla", "authors": "Palvi Arora and Tarun Bhalla", "title": "A Synonym Based Approach of Data Mining in Search Engine Optimization", "comments": "5 pages, 2figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "Palvi Arora , Tarun Bhalla.\"A Synonym Based Approach of Data\n  Mining in Search Engine Optimization\". International Journal of Computer\n  Trends and Technology (IJCTT) V12(4):201-205, June 2014. ISSN:2231-2803.\n  www.ijcttjournal.org", "doi": "10.14445/22312803/IJCTT-V12P140", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays era with the rapid growth of information on the web, makes users\nturn to search engines as a replacement of traditional media. This makes\nsorting of particular information through billions of webpages and displaying\nthe relevant data makes the task tough for the search engine. Remedy for this\nis SEO i.e. having a website optimized in such a way that it will display the\nrelevant webpages based on ranking. This is the main reason that makes search\nengine optimization a prominent position in online world. This paper present a\nsynonym based data mining approach for SEO that makes the task of improving the\nranking of the website much easier way and user will get answer to their query\neasily through any of search engine available in market.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 06:38:02 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Arora", "Palvi", ""], ["Bhalla", "Tarun", ""]]}, {"id": "1407.1539", "submitter": "Philipp Mayr", "authors": "Thomas L\\\"uke, Philipp Schaer, Philipp Mayr", "title": "A Framework for Specific Term Recommendation Systems", "comments": "2 pages, 1 figure, SIGIR 13, July 28-August 1, 2013, Dublin, Ireland", "journal-ref": null, "doi": "10.1145/2484028.2484207", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the IRSA framework that enables the automatic\ncreation of search term suggestion or recommendation systems (TS). Such TS are\nused to operationalize interactive query expansion and help users in refining\ntheir information need in the query formulation phase. Our recent research has\nshown TS to be more effective when specific to a certain domain. The presented\ntechnical framework allows owners of Digital Libraries to create their own\nspecific TS constructed via OAI-harvested metadata with very little effort.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:14:08 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["L\u00fcke", "Thomas", ""], ["Schaer", "Philipp", ""], ["Mayr", "Philipp", ""]]}, {"id": "1407.1540", "submitter": "Philipp Mayr", "authors": "Dagmar Kern, Peter Mutschke, Philipp Mayr", "title": "Establishing an Online Access Panel for Interactive Information\n  Retrieval Research", "comments": "2 pages, 1 figure, 2014 IEEE/ACM Joint Conference on Digital\n  Libraries (JCDL), London, 8th-12th September 2014", "journal-ref": null, "doi": "10.1109/JCDL.2014.6970231", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online access panel to support the evaluation process of\nInteractive Information Retrieval (IIR) systems - called IIRpanel. By\nmaintaining an online access panel with users of IIR systems we assume that the\nrecurring effort to recruit participants for web-based as well as for lab\nstudies can be minimized. We target on using the online access panel not only\nfor our own development processes but to open it for other interested\nresearchers in the field of IIR. In this paper we present the concept of\nIIRpanel as well as first implementation details.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:20:13 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kern", "Dagmar", ""], ["Mutschke", "Peter", ""], ["Mayr", "Philipp", ""]]}, {"id": "1407.2433", "submitter": "Peter Foster", "authors": "Peter Foster, Simon Dixon, Anssi Klapuri", "title": "Identifying Cover Songs Using Information-Theoretic Measures of\n  Similarity", "comments": "13 pages, 5 figures, 4 tables. v3: Accepted version", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 23 no. 6, pp. 993-1005, 2015", "doi": "10.1109/TASLP.2015.2416655", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates methods for quantifying similarity between audio\nsignals, specifically for the task of of cover song detection. We consider an\ninformation-theoretic approach, where we compute pairwise measures of\npredictability between time series. We compare discrete-valued approaches\noperating on quantised audio features, to continuous-valued approaches. In the\ndiscrete case, we propose a method for computing the normalised compression\ndistance, where we account for correlation between time series. In the\ncontinuous case, we propose to compute information-based measures of similarity\nas statistics of the prediction error between time series. We evaluate our\nmethods on two cover song identification tasks using a data set comprised of\n300 Jazz standards and using the Million Song Dataset. For both datasets, we\nobserve that continuous-valued approaches outperform discrete-valued\napproaches. We consider approaches to estimating the normalised compression\ndistance (NCD) based on string compression and prediction, where we observe\nthat our proposed normalised compression distance with alignment (NCDA)\nimproves average performance over NCD, for sequential compression algorithms.\nFinally, we demonstrate that continuous-valued distances may be combined to\nimprove performance with respect to baseline approaches. Using a large-scale\nfilter-and-refine approach, we demonstrate state-of-the-art performance for\ncover song identification using the Million Song Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 11:04:15 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 00:53:47 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 15:53:43 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Foster", "Peter", ""], ["Dixon", "Simon", ""], ["Klapuri", "Anssi", ""]]}, {"id": "1407.2515", "submitter": "Lionel Tabourier", "authors": "Lionel Tabourier, Daniel Faria Bernardes, Anne-Sophie Libert, Renaud\n  Lambiotte", "title": "RankMerging: A supervised learning-to-rank framework to predict links in\n  large social network", "comments": "43 pages, published in Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering unknown or missing links in social networks is a difficult task\nbecause of their sparsity and because links may represent different types of\nrelationships, characterized by different structural patterns. In this paper,\nwe define a simple yet efficient supervised learning-to-rank framework, called\nRankMerging, which aims at combining information provided by various\nunsupervised rankings. We illustrate our method on three different kinds of\nsocial networks and show that it substantially improves the performances of\nunsupervised metrics of ranking. We also compare it to other combination\nstrategies based on standard methods. Finally, we explore various aspects of\nRankMerging, such as feature selection and parameter estimation and discuss its\narea of relevance: the prediction of an adjustable number of links on large\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 15:05:56 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 14:16:57 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2015 22:32:04 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 14:23:37 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Tabourier", "Lionel", ""], ["Bernardes", "Daniel Faria", ""], ["Libert", "Anne-Sophie", ""], ["Lambiotte", "Renaud", ""]]}, {"id": "1407.2806", "submitter": "Preux Philippe", "authors": "J\\'er\\'emie Mary (INRIA Lille - Nord Europe, LIFL), Romaric Gaudel\n  (INRIA Lille - Nord Europe, LIFL), Preux Philippe (INRIA Lille - Nord Europe,\n  LIFL)", "title": "Bandits Warm-up Cold Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "RR-8563", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the cold start problem in recommendation systems assuming no\ncontextual information is available neither about users, nor items. We consider\nthe case in which we only have access to a set of ratings of items by users.\nMost of the existing works consider a batch setting, and use cross-validation\nto tune parameters. The classical method consists in minimizing the root mean\nsquare error over a training subset of the ratings which provides a\nfactorization of the matrix of ratings, interpreted as a latent representation\nof items and users. Our contribution in this paper is 5-fold. First, we\nexplicit the issues raised by this kind of batch setting for users or items\nwith very few ratings. Then, we propose an online setting closer to the actual\nuse of recommender systems; this setting is inspired by the bandit framework.\nThe proposed methodology can be used to turn any recommender system dataset\n(such as Netflix, MovieLens,...) into a sequential dataset. Then, we explicit a\nstrong and insightful link between contextual bandit algorithms and matrix\nfactorization; this leads us to a new algorithm that tackles the\nexploration/exploitation dilemma associated to the cold start problem in a\nstrikingly new perspective. Finally, experimental evidence confirm that our\nalgorithm is effective in dealing with the cold start problem on publicly\navailable datasets. Overall, the goal of this paper is to bridge the gap\nbetween recommender systems based on matrix factorizations and those based on\ncontextual bandits.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 14:32:37 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Mary", "J\u00e9r\u00e9mie", "", "INRIA Lille - Nord Europe, LIFL"], ["Gaudel", "Romaric", "", "INRIA Lille - Nord Europe, LIFL"], ["Philippe", "Preux", "", "INRIA Lille - Nord Europe,\n  LIFL"]]}, {"id": "1407.2845", "submitter": "Emilio Ferrara", "authors": "Santa Agreste, Pasquale De Meo, Emilio Ferrara, Domenico Ursino", "title": "XML Matchers: approaches and challenges", "comments": "34 pages, 8 tables, 7 figures", "journal-ref": "Knowledge-based systems 66: 190-209, 2014", "doi": "10.1016/j.knosys.2014.04.044", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema Matching, i.e. the process of discovering semantic correspondences\nbetween concepts adopted in different data source schemas, has been a key topic\nin Database and Artificial Intelligence research areas for many years. In the\npast, it was largely investigated especially for classical database models\n(e.g., E/R schemas, relational databases, etc.). However, in the latest years,\nthe widespread adoption of XML in the most disparate application fields pushed\na growing number of researchers to design XML-specific Schema Matching\napproaches, called XML Matchers, aiming at finding semantic matchings between\nconcepts defined in DTDs and XSDs. XML Matchers do not just take well-known\ntechniques originally designed for other data models and apply them on\nDTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical\nstructure of a DTD/XSD) to improve the performance of the Schema Matching\nprocess. The design of XML Matchers is currently a well-established research\narea. The main goal of this paper is to provide a detailed description and\nclassification of XML Matchers. We first describe to what extent the\nspecificities of DTDs/XSDs impact on the Schema Matching task. Then we\nintroduce a template, called XML Matcher Template, that describes the main\ncomponents of an XML Matcher, their role and behavior. We illustrate how each\nof these components has been implemented in some popular XML Matchers. We\nconsider our XML Matcher Template as the baseline for objectively comparing\napproaches that, at first glance, might appear as unrelated. The introduction\nof this template can be useful in the design of future XML Matchers. Finally,\nwe analyze commercial tools implementing XML Matchers and introduce two\nchallenging issues strictly related to this topic, namely XML source clustering\nand uncertainty management in XML Matchers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 16:14:11 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Agreste", "Santa", ""], ["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Ursino", "Domenico", ""]]}, {"id": "1407.2919", "submitter": "Weike Pan", "authors": "Weike Pan", "title": "Collaborative Recommendation with Auxiliary Data: A Transfer Learning\n  View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent recommendation technology has been playing an increasingly\nimportant role in various industry applications such as e-commerce product\npromotion and Internet advertisement display. Besides users' feedbacks (e.g.,\nnumerical ratings) on items as usually exploited by some typical recommendation\nalgorithms, there are often some additional data such as users' social circles\nand other behaviors. Such auxiliary data are usually related to users'\npreferences on items behind the numerical ratings. Collaborative recommendation\nwith auxiliary data (CRAD) aims to leverage such additional information so as\nto improve the personalization services, which have received much attention\nfrom both researchers and practitioners.\n  Transfer learning (TL) is proposed to extract and transfer knowledge from\nsome auxiliary data in order to assist the learning task on some target data.\nIn this paper, we consider the CRAD problem from a transfer learning view,\nespecially on how to achieve knowledge transfer from some auxiliary data.\nFirst, we give a formal definition of transfer learning for CRAD (TL-CRAD).\nSecond, we extend the existing categorization of TL techniques (i.e., adaptive,\ncollective and integrative knowledge transfer algorithm styles) with three\nknowledge transfer strategies (i.e., prediction rule, regularization and\nconstraint). Third, we propose a novel generic knowledge transfer framework for\nTL-CRAD. Fourth, we describe some representative works of each specific\nknowledge transfer strategy of each algorithm style in detail, which are\nexpected to inspire further works. Finally, we conclude the paper with some\nsummary discussions and several future directions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jul 2014 01:39:03 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Pan", "Weike", ""]]}, {"id": "1407.2987", "submitter": "Eren Golge", "authors": "Eren Golge and Pinar Duygulu", "title": "FAME: Face Association through Model Evolution", "comments": "Draft version of the study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We attack the problem of learning face models for public faces from\nweakly-labelled images collected from web through querying a name. The data is\nvery noisy even after face detection, with several irrelevant faces\ncorresponding to other people. We propose a novel method, Face Association\nthrough Model Evolution (FAME), that is able to prune the data in an iterative\nway, for the face models associated to a name to evolve. The idea is based on\ncapturing discriminativeness and representativeness of each instance and\neliminating the outliers. The final models are used to classify faces on novel\ndatasets with possibly different characteristics. On benchmark datasets, our\nresults are comparable to or better than state-of-the-art studies for the task\nof face identification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jul 2014 23:52:44 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Golge", "Eren", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1407.3392", "submitter": "Khaled Sellami KSI", "authors": "Khaled Sellami, Mohamed Ahmed-Nacer, Pierre Tiako", "title": "From Social Network to Semantic Social Network in Recommender System", "comments": "International Journal of Computer Science Issues (IJCSI),2012, 9(4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due the success of emerging Web 2.0, and different social network Web sites\nsuch as Amazon and movie lens, recommender systems are creating unprecedented\nopportunities to help people browsing the web when looking for relevant\ninformation, and making choices. Generally, these recommender systems are\nclassified in three categories: content based, collaborative filtering, and\nhybrid based recommendation systems. Usually, these systems employ standard\nrecommendation methods such as artificial neural networks, nearest neighbor, or\nBayesian networks. However, these approaches are limited compared to methods\nbased on web applications, such as social networks or semantic web. In this\npaper, we propose a novel approach for recommendation systems called semantic\nsocial recommendation systems that enhance the analysis of social networks\nexploiting the power of semantic social network analysis. Experiments on\nreal-world data from Amazon examine the quality of our recommendation method as\nwell as the performance of our recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jul 2014 14:37:01 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 20:37:02 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Sellami", "Khaled", ""], ["Ahmed-Nacer", "Mohamed", ""], ["Tiako", "Pierre", ""]]}, {"id": "1407.3636", "submitter": "Ana Mestrovic", "authors": "Sabina \\v{S}i\\v{s}ovi\\'c, Sanda Martin\\v{c}i\\'c-Ip\\v{s}i\\'c and Ana\n  Me\\v{s}trovi\\'c", "title": "Toward Network-based Keyword Extraction from Multitopic Web Documents", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the selectivity measure calculated from the complex\nnetwork in the task of the automatic keyword extraction. Texts, collected from\ndifferent web sources (portals, forums), are represented as directed and\nweighted co-occurrence complex networks of words. Words are nodes and links are\nestablished between two nodes if they are directly co-occurring within the\nsentence. We test different centrality measures for ranking nodes - keyword\ncandidates. The promising results are achieved using the selectivity measure.\nThen we propose an approach which enables extracting word pairs according to\nthe values of the in/out selectivity and weight measures combined with\nfiltering.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 13:22:36 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["\u0160i\u0161ovi\u0107", "Sabina", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""], ["Me\u0161trovi\u0107", "Ana", ""]]}, {"id": "1407.3751", "submitter": "Sutanay Choudhury", "authors": "Sutanay Choudhury, Chase Dowling", "title": "Benchmarking Named Entity Disambiguation approaches for Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": "PNNL-23455 2014-05, Pacific Northwest National Laboratory, Richland,\n  WA", "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Named Entity Disambiaguation (NED) is a central task for applications dealing\nwith natural language text. Assume that we have a graph based knowledge base\n(subsequently referred as Knowledge Graph) where nodes represent various real\nworld entities such as people, location, organization and concepts. Given data\nsources such as social media streams and web pages Entity Linking is the task\nof mapping named entities that are extracted from the data to those present in\nthe Knowledge Graph. This is an inherently difficult task due to several\nreasons. Almost all these data sources are generated without any formal\nontology; the unstructured nature of the input, limited context and the\nambiguity involved when multiple entities are mapped to the same name make this\na hard task. This report looks at two state of the art systems employing two\ndistinctive approaches: graph based Accurate Online Disambiguation of Entities\n(AIDA) and Mined Evidence Named Entity Disambiguation (MENED), which employs a\nstatistical inference approach. We compare both approaches using the data set\nand queries provided by the Knowledge Base Population (KBP) track at 2011 NIST\nText Analytics Conference (TAC). This report begins with an overview of the\nrespective approaches, followed by detailed description of the experimental\nsetup. It concludes with our findings from the benchmarking exercise.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 18:01:08 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Choudhury", "Sutanay", ""], ["Dowling", "Chase", ""]]}, {"id": "1407.4416", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "In Defense of MinHash Over SimHash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MinHash and SimHash are the two widely adopted Locality Sensitive Hashing\n(LSH) algorithms for large-scale data processing applications. Deciding which\nLSH to use for a particular problem at hand is an important question, which has\nno clear answer in the existing literature. In this study, we provide a\ntheoretical answer (validated by experiments) that MinHash virtually always\noutperforms SimHash when the data are binary, as common in practice such as\nsearch.\n  The collision probability of MinHash is a function of resemblance similarity\n($\\mathcal{R}$), while the collision probability of SimHash is a function of\ncosine similarity ($\\mathcal{S}$). To provide a common basis for comparison, we\nevaluate retrieval results in terms of $\\mathcal{S}$ for both MinHash and\nSimHash. This evaluation is valid as we can prove that MinHash is a valid LSH\nwith respect to $\\mathcal{S}$, by using a general inequality $\\mathcal{S}^2\\leq\n\\mathcal{R}\\leq \\frac{\\mathcal{S}}{2-\\mathcal{S}}$. Our worst case analysis can\nshow that MinHash significantly outperforms SimHash in high similarity region.\n  Interestingly, our intensive experiments reveal that MinHash is also\nsubstantially better than SimHash even in datasets where most of the data\npoints are not too similar to each other. This is partly because, in practical\ndata, often $\\mathcal{R}\\geq \\frac{\\mathcal{S}}{z-\\mathcal{S}}$ holds where $z$\nis only slightly larger than 2 (e.g., $z\\leq 2.1$). Our restricted worst case\nanalysis by assuming $\\frac{\\mathcal{S}}{z-\\mathcal{S}}\\leq \\mathcal{R}\\leq\n\\frac{\\mathcal{S}}{2-\\mathcal{S}}$ shows that MinHash indeed significantly\noutperforms SimHash even in low similarity region.\n  We believe the results in this paper will provide valuable guidelines for\nsearch in practice, especially when the data are sparse.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 18:27:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1407.4723", "submitter": "Ana Mestrovic", "authors": "Slobodan Beliga, Ana Me\\v{s}trovi\\'c, Sanda\n  Martin\\v{c}i\\'c-Ip\\v{s}i\\'c", "title": "Toward Selectivity Based Keyword Extraction for Croatian News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preliminary report on network based keyword extraction for Croatian is an\nunsupervised method for keyword extraction from the complex network. We build\nour approach with a new network measure the node selectivity, motivated by the\nresearch of the graph based centrality approaches. The node selectivity is\ndefined as the average weight distribution on the links of the single node. We\nextract nodes (keyword candidates) based on the selectivity value. Furthermore,\nwe expand extracted nodes to word-tuples ranked with the highest in/out\nselectivity values. Selectivity based extraction does not require linguistic\nknowledge while it is purely derived from statistical and structural\ninformation en-compassed in the source text which is reflected into the\nstructure of the network. Obtained sets are evaluated on a manually annotated\nkeywords: for the set of extracted keyword candidates average F1 score is\n24,63%, and average F2 score is 21,19%; for the exacted words-tuples candidates\naverage F1 score is 25,9% and average F2 score is 24,47%.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 16:12:04 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Beliga", "Slobodan", ""], ["Me\u0161trovi\u0107", "Ana", ""], ["Martin\u010di\u0107-Ip\u0161i\u0107", "Sanda", ""]]}, {"id": "1407.4832", "submitter": "Ernesto Diaz-Aviles", "authors": "Bernat Coma-Puig and Ernesto Diaz-Aviles and Wolfgang Nejdl", "title": "Collaborative Filtering Ensemble for Personalized Name Recommendation", "comments": "Top-N recommendation; personalized ranking; given name recommendation", "journal-ref": "Proceedings of the ECML PKDD Discovery Challenge - Recommending\n  Given Names. Co-located with ECML PKDD 2013. Prague, Czech Republic,\n  September 27, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out of thousands of names to choose from, picking the right one for your\nchild is a daunting task. In this work, our objective is to help parents making\nan informed decision while choosing a name for their baby. We follow a\nrecommender system approach and combine, in an ensemble, the individual\nrankings produced by simple collaborative filtering algorithms in order to\nproduce a personalized list of names that meets the individual parents' taste.\nOur experiments were conducted using real-world data collected from the query\nlogs of 'nameling' (nameling.net), an online portal for searching and exploring\nnames, which corresponds to the dataset released in the context of the ECML\nPKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and\nfeatures fast training and prediction steps.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2014 12:07:36 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Coma-Puig", "Bernat", ""], ["Diaz-Aviles", "Ernesto", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1407.4833", "submitter": "Nikita Zhiltsov", "authors": "Olga Nevzorova, Nikita Zhiltsov, Alexander Kirillovich, and Evgeny\n  Lipachev", "title": "$OntoMath^{PRO}$ Ontology: A Linked Data Hub for Mathematics", "comments": "15 pages, 6 images, 1 table, Knowledge Engineering and the Semantic\n  Web - 5th International Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an ontology of mathematical knowledge concepts that\ncovers a wide range of the fields of mathematics and introduces a balanced\nrepresentation between comprehensive and sensible models. We demonstrate the\napplications of this representation in information extraction, semantic search,\nand education. We argue that the ontology can be a core of future integration\nof math-aware data sets in the Web of Data and, therefore, provide mappings\nonto relevant datasets, such as DBpedia and ScienceWISE.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2014 20:36:36 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 06:54:16 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Nevzorova", "Olga", ""], ["Zhiltsov", "Nikita", ""], ["Kirillovich", "Alexander", ""], ["Lipachev", "Evgeny", ""]]}, {"id": "1407.5416", "submitter": "Luis Marujo", "authors": "Luis Marujo, Jos\\'e Port\\^elo, David Martins de Matos, Jo\\~ao P. Neto,\n  Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha Raj", "title": "Privacy-Preserving Important Passage Retrieval", "comments": "Secure Passage Retrieval, Important Passage Retrieval, KP-Centrality,\n  Secure Binary Embeddings, Data Privacy, Automatic Key Phrase Extraction,\n  Proceedings of SIGIR 2014 Workshop Privacy-Preserving IR: When Information\n  Retrieval Meets Privacy and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art important passage retrieval methods obtain very good\nresults, but do not take into account privacy issues. In this paper, we present\na privacy preserving method that relies on creating secure representations of\ndocuments. Our approach allows for third parties to retrieve important passages\nfrom documents without learning anything regarding their content. We use a\nhashing scheme known as Secure Binary Embeddings to convert a key phrase and\nbag-of-words representation to bit strings in a way that allows the computation\nof approximate distances, instead of exact ones. Experiments show that our\nsecure system yield similar results to its non-private counterpart on both\nclean text and noisy speech recognized text.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 08:56:09 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Marujo", "Luis", ""], ["Port\u00ealo", "Jos\u00e9", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Trancoso", "Isabel", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1407.5732", "submitter": "Sonali Gupta", "authors": "Sonali Gupta, Komal Kumar Bhatia", "title": "A Comparative Study of Hidden Web Crawlers", "comments": "8 pages, 8 figures", "journal-ref": "Vol 12 number 3 , Jun 2014 V12(3):111-118", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of data on the WWW remains inaccessible to crawlers of Web\nsearch engines because it can only be exposed on demand as users fill out and\nsubmit forms. The Hidden web refers to the collection of Web data which can be\naccessed by the crawler only through an interaction with the Web-based search\nform and not simply by traversing hyperlinks. Research on Hidden Web has\nemerged almost a decade ago with the main line being exploring ways to access\nthe content in online databases that are usually hidden behind search forms.\nThe efforts in the area mainly focus on designing hidden Web crawlers that\nfocus on learning forms and filling them with meaningful values. The paper\ngives an insight into the various Hidden Web crawlers developed for the purpose\ngiving a mention to the advantages and shortcoming of the techniques employed\nin each.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 05:25:53 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Gupta", "Sonali", ""], ["Bhatia", "Komal Kumar", ""]]}, {"id": "1407.5764", "submitter": "Truyen Tran", "authors": "Tran The Truyen, Dinh Q. Phung and Svetha Venkatesh", "title": "Preference Networks: Probabilistic Models for Recommendation Systems", "comments": "In Proc. of 6th Australasian Data Mining Conference (AusDM), Gold\n  Coast, Australia, pages 195--202, 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are important to help users select relevant and\npersonalised information over massive amounts of data available. We propose an\nunified framework called Preference Network (PN) that jointly models various\ntypes of domain knowledge for the task of recommendation. The PN is a\nprobabilistic model that systematically combines both content-based filtering\nand collaborative filtering into a single conditional Markov random field. Once\nestimated, it serves as a probabilistic database that supports various useful\nqueries such as rating prediction and top-$N$ recommendation. To handle the\nchallenging problem of learning large networks of users and items, we employ a\nsimple but effective pseudo-likelihood with regularisation. Experiments on the\nmovie rating data demonstrate the merits of the PN.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jul 2014 07:17:48 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Truyen", "Tran The", ""], ["Phung", "Dinh Q.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6089", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung, Svetha Venkatesh", "title": "Learning Rank Functionals: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a key aspect of many applications, such as information retrieval,\nquestion answering, ad placement and recommender systems. Learning to rank has\nthe goal of estimating a ranking model automatically from training data. In\npractical settings, the task often reduces to estimating a rank functional of\nan object with respect to a query. In this paper, we investigate key issues in\ndesigning an effective learning to rank algorithm. These include data\nrepresentation, the choice of rank functionals, the design of the loss function\nso that it is correlated with the rank metrics used in evaluation. For the loss\nfunction, we study three techniques: approximating the rank metric by a smooth\nfunction, decomposition of the loss into a weighted sum of element-wise losses\nand into a weighted sum of pairwise losses. We then present derivations of\npiecewise losses using the theory of high-order Markov chains and Markov random\nfields. In experiments, we evaluate these design aspects on two tasks: answer\nranking in a Social Question Answering site, and Web Information Retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 01:54:31 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 23:50:43 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6100", "submitter": "Andrew Connor", "authors": "Dilip K. Limbu, Andy M. Connor, Stephen G. MacDonell", "title": "A framework for contextual information retrieval from the WWW", "comments": "Proceedings of the 14th International Conference on Adaptive Systems\n  and Software Engineering (IASSE 2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines are the most commonly used type of tool for finding relevant\ninformation on the Internet. However, today's search engines are far from\nperfect. Typical search queries are short, often one or two words, and can be\nambiguous therefore returning inappropriate results. Contextual information\nretrieval (CIR) is a critical technique for these search engines to facilitate\nqueries and return relevant information. Despite its importance, little\nprogress has been made in CIR due to the difficulty of capturing and\nrepresenting contextual information about users. Numerous contextual\ninformation retrieval approaches exist today, but to the best of our knowledge\nnone of them offer a similar service to the one proposed in this paper.\n  This paper proposes an alternative framework for contextual information\nretrieval from the WWW. The framework aims to improve query results (or make\nsearch results more relevant) by constructing a contextual profile based on a\nuser's behaviour, their preferences, and a shared knowledge base, and using\nthis information in the search engine framework to find and return relevant\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 03:34:58 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Limbu", "Dilip K.", ""], ["Connor", "Andy M.", ""], ["MacDonell", "Stephen G.", ""]]}, {"id": "1407.6101", "submitter": "Andrew Connor", "authors": "Dilip K. Limbu, Andy M. Connor, Russel Pears, Stephen G. MacDonell", "title": "Improving web search using contextual retrieval", "comments": null, "journal-ref": "Proceedings of the 6th International Conference on Information\n  Technology: New Generations (ITNG 2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual retrieval is a critical technique for today's search engines in\nterms of facilitating queries and returning relevant information. This paper\nreports on the development and evaluation of a system designed to tackle some\nof the challenges associated with contextual information retrieval from the\nWorld Wide Web (WWW). The developed system has been designed with a view to\ncapturing both implicit and explicit user data which is used to develop a\npersonal contextual profile. Such profiles can be shared across multiple users\nto create a shared contextual knowledge base. These are used to refine search\nqueries and improve both the search results for a user as well as their search\nexperience. An empirical study has been undertaken to evaluate the system\nagainst a number of hypotheses. In this paper, results related to one are\npresented that support the claim that users can find information more readily\nusing the contextual search system.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 03:38:12 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Limbu", "Dilip K.", ""], ["Connor", "Andy M.", ""], ["Pears", "Russel", ""], ["MacDonell", "Stephen G.", ""]]}, {"id": "1407.6128", "submitter": "Truyen Tran", "authors": "Truyen Tran and Svetha Venkatesh", "title": "Permutation Models for Collaborative Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of collaborative filtering where ranking information is\navailable. Focusing on the core of the collaborative ranking process, the user\nand their community, we propose new models for representation of the underlying\npermutations and prediction of ranks. The first approach is based on the\nassumption that the user makes successive choice of items in a stage-wise\nmanner. In particular, we extend the Plackett-Luce model in two ways -\nintroducing parameter factoring to account for user-specific contribution, and\nmodelling the latent community in a generative setting. The second approach\nrelies on log-linear parameterisation, which relaxes the discrete-choice\nassumption, but makes learning and inference much more involved. We propose\nMCMC-based learning and inference methods and derive linear-time prediction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 08:20:09 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1407.6692", "submitter": "Sivakanth Gopi", "authors": "Zeev Dvir, Sivakanth Gopi", "title": "2-Server PIR with sub-polynomial communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 2-server Private Information Retrieval (PIR) scheme allows a user to\nretrieve the $i$th bit of an $n$-bit database replicated among two servers\n(which do not communicate) while not revealing any information about $i$ to\neither server. In this work we construct a 1-round 2-server PIR with total\ncommunication cost $n^{O({\\sqrt{\\log\\log n/\\log n}})}$. This improves over the\ncurrently known 2-server protocols which require $O(n^{1/3})$ communication and\nmatches the communication cost of known 3-server PIR schemes. Our improvement\ncomes from reducing the number of servers in existing protocols, based on\nMatching Vector Codes, from 3 or 4 servers to 2. This is achieved by viewing\nthese protocols in an algebraic way (using polynomial interpolation) and\nextending them using partial derivatives.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 19:07:24 GMT"}], "update_date": "2014-07-25", "authors_parsed": [["Dvir", "Zeev", ""], ["Gopi", "Sivakanth", ""]]}, {"id": "1407.6812", "submitter": "Robert Hoehndorf", "authors": "Robert Hoehndorf and Luke Slater and Paul N. Schofield and Georgios V.\n  Gkoutos", "title": "Aber-OWL: a framework for ontology-based data access in biology", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-015-0456-9", "report-no": null, "categories": "cs.DB cs.IR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ontologies have been developed in biology and these ontologies\nincreasingly contain large volumes of formalized knowledge commonly expressed\nin the Web Ontology Language (OWL). Computational access to the knowledge\ncontained within these ontologies relies on the use of automated reasoning. We\nhave developed the Aber-OWL infrastructure that provides reasoning services for\nbio-ontologies. Aber-OWL consists of an ontology repository, a set of web\nservices and web interfaces that enable ontology-based semantic access to\nbiological data and literature. Aber-OWL is freely available at\nhttp://aber-owl.net.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 08:33:12 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Hoehndorf", "Robert", ""], ["Slater", "Luke", ""], ["Schofield", "Paul N.", ""], ["Gkoutos", "Georgios V.", ""]]}, {"id": "1407.6872", "submitter": "Ivan Ivek", "authors": "Ivan Ivek", "title": "Interpretable Low-Rank Document Representations with Label-Dependent\n  Sparsity Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In context of document classification, where in a corpus of documents their\nlabel tags are readily known, an opportunity lies in utilizing label\ninformation to learn document representation spaces with better discriminative\nproperties. To this end, in this paper application of a Variational Bayesian\nSupervised Nonnegative Matrix Factorization (supervised vbNMF) with\nlabel-driven sparsity structure of coefficients is proposed for learning of\ndiscriminative nonsubtractive latent semantic components occuring in TF-IDF\ndocument representations. Constraints are such that the components pursued are\nmade to be frequently occuring in a small set of labels only, making it\npossible to yield document representations with distinctive label-specific\nsparse activation patterns. A simple measure of quality of this kind of\nsparsity structure, dubbed inter-label sparsity, is introduced and\nexperimentally brought into tight connection with classification performance.\nRepresenting a great practical convenience, inter-label sparsity is shown to be\neasily controlled in supervised vbNMF by a single parameter.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 12:46:18 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Ivek", "Ivan", ""]]}, {"id": "1407.6952", "submitter": "Monika Rani", "authors": "Monika Rani, Anubha Parashar, Jyoti Chaturvedi and Anu Malviya", "title": "Search Space Engine Optimize Search Using FCC_STF Algorithm in Fuzzy\n  Co-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy co-clustering can be improved if we handle two main problem first is\noutlier and second curse of dimensionality .outlier problem can be reduce by\nimplementing page replacement algorithm like FIFO, LRU or priority algorithm in\na set of frame of web pages efficiently through a search engine. The web page\nwhich has zero priority (outlier) can be represented in separate slot of frame.\nWhereas curse of dimensionality problem can be improved by implementing FCC_STF\nalgorithm for web pages obtain by search engine that reduce the outlier problem\nfirst. The algorithm FCCM and FUZZY CO-DOK are compared with FCC_STF algorithm\nwith merit and demerits on the bases of different fuzzifier used. FCC_STF\nalgorithm in which fuzzifier fused into one entity who have shown high\nperformance by experiment result of values (A1,B1,Vcj,A2,B2) seem to less\nsensitive to local maxima and obtain optimization search space in 2-D for web\npages by plotting graph between J(fcc_stf) and Vcj.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 16:10:40 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Rani", "Monika", ""], ["Parashar", "Anubha", ""], ["Chaturvedi", "Jyoti", ""], ["Malviya", "Anu", ""]]}, {"id": "1407.7049", "submitter": "Jianguo Liu", "authors": "Qiang Guo, Wen-Jun Song, Jian-Guo Liu", "title": "Ultra accurate collaborative information filtering via directed user\n  similarity", "comments": "6 pages, 4 figures", "journal-ref": "EPL, 107 18001 (2014)", "doi": "10.1209/0295-5075/107/18001", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge of the collaborative filtering (CF) information filtering is\nhow to obtain the reliable and accurate results with the help of peers'\nrecommendation. Since the similarities from small-degree users to large-degree\nusers would be larger than the ones opposite direction, the large-degree users'\nselections are recommended extensively by the traditional second-order CF\nalgorithms. By considering the users' similarity direction and the second-order\ncorrelations to depress the influence of mainstream preferences, we present the\ndirected second-order CF (HDCF) algorithm specifically to address the challenge\nof accuracy and diversity of the CF algorithm. The numerical results for two\nbenchmark data sets, MovieLens and Netflix, show that the accuracy of the new\nalgorithm outperforms the state-of-the-art CF algorithms. Comparing with the CF\nalgorithm based on random-walks proposed in the Ref.7, the average ranking\nscore could reach 0.0767 and 0.0402, which is enhanced by 27.3\\% and 19.1\\% for\nMovieLens and Netflix respectively. In addition, the diversity, precision and\nrecall are also enhanced greatly. Without relying on any context-specific\ninformation, tuning the similarity direction of CF algorithms could obtain\naccurate and diverse recommendations. This work suggests that the user\nsimilarity direction is an important factor to improve the personalized\nrecommendation performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jul 2014 08:22:17 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Guo", "Qiang", ""], ["Song", "Wen-Jun", ""], ["Liu", "Jian-Guo", ""]]}, {"id": "1407.7072", "submitter": "Seungyeon Kim", "authors": "Seungyeon Kim, Haesun Park, Guy Lebanon", "title": "Fast Spammer Detection Using Structural Rank", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comments for a product or a news article are rapidly growing and became a\nmedium of measuring quality products or services. Consequently, spammers have\nbeen emerged in this area to bias them toward their favor. In this paper, we\npropose an efficient spammer detection method using structural rank of author\nspecific term-document matrices. The use of structural rank was found effective\nand far faster than similar methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 22:33:49 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Kim", "Seungyeon", ""], ["Park", "Haesun", ""], ["Lebanon", "Guy", ""]]}, {"id": "1407.7276", "submitter": "Philipp Mayr", "authors": "Zeljko Carevic, Philipp Mayr", "title": "Recommender Systems using Pennant Diagrams in Digital Libraries", "comments": "3 pages, paper accepted for the 13th European Networked Knowledge\n  Organization Systems (NKOS) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital libraries recommendations can be valuable for researchers, e.g.\nrecommending related literature to a given context. Typically, in a scientific\ncontext the simple presentation of related content is not sufficient. Often the\nusers demand a more detailed view on the connection of a document and its\nspecific recommendations. The aim of pennants introduced by Howard White (2007)\nis to provide the user with a graph showing the relatedness / distance between\na given document and related documents. Co-citation but also co-occurrence\nanalysis are established methods for finding related documents to a seed. A\nseed could be for instance an author, a keyword, or a publication. In this\npaper we introduce a recommender system in the digital library sowiport using\npennant diagrams which can be created from co-citation and/or co-occurrence\nanalysis. The presentation at the NKOS workshop will present demos of pennants\nin sowiport and will elaborate on practical questions in visualizing pennants\nand evaluating the utility of pennants for search.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jul 2014 19:39:55 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Carevic", "Zeljko", ""], ["Mayr", "Philipp", ""]]}, {"id": "1407.7314", "submitter": "Wu-Chen Su", "authors": "Wu-Chen Su", "title": "A Preliminary Survey of Knowledge Discovery on Smartphone Applications\n  (apps): Principles, Techniques and Research Directions for E-health", "comments": "6 pages, 2 figures, 2 tables, 2014 ICME International Conference on\n  Complex Medical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People usually seek out varied information to deal with their health\nproblems. However, the large volume of information available may present\nchallenges for the public to distinguish good from suboptimal advice. How to\nensure the right information for the right person at the right time and place\nhas always been a challenge. For example, smart phone application vendor\nmarkets provide a varied selection of health applications for users. However,\nthere is a lack of substantive reference information for consumers to base\nwell-informed decisions about whether or not to adopt the applications they\nreview and to ascertain the validity of the information provided by these\ne-health solutions. Thus, this study aims to review the existing relevant\nresearch about smart phone applications and identify pertinent research\nquestions in the field of knowledge discovery for health applications that can\nbe addressed in future research. Therefore, this study can be seen as an\nimportant step for researchers to explore this domain and extend our work for\nthe well-being of public.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 03:49:34 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Su", "Wu-Chen", ""]]}, {"id": "1407.7357", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Philippe Lenca", "title": "Text Classification Using Association Rules, Dependency Pruning and\n  Hyperonymization", "comments": "16 pages, 2 figures, presented at DMNLP 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for pruning and enhancing item- sets for text\nclassification via association rule mining. Pruning methods are based on\ndependency syntax and enhancing methods are based on replacing words by their\nhyperonyms of various orders. We discuss the impact of these methods, compared\nto pruning based on tfidf rank of words.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jul 2014 09:00:57 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Haralambous", "Yannis", ""], ["Lenca", "Philippe", ""]]}, {"id": "1407.7930", "submitter": "EPTCS", "authors": "Roi Blanco (Yahoo! Research Barcelona, Spain), Paolo Boldi\n  (Dipartimento di informatica, Universit\\`a degli Studi di Milano), Andrea\n  Marino (Dipartimento di informatica, Universit\\`a degli Studi di Milano)", "title": "Entity-Linking via Graph-Distance Minimization", "comments": "In Proceedings GRAPHITE 2014, arXiv:1407.7671. The second and third\n  authors were supported by the EU-FET grant NADINE (GA 288956)", "journal-ref": "EPTCS 159, 2014, pp. 30-43", "doi": "10.4204/EPTCS.159.4", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity-linking is a natural-language-processing task that consists in\nidentifying the entities mentioned in a piece of text, linking each to an\nappropriate item in some knowledge base; when the knowledge base is Wikipedia,\nthe problem comes to be known as wikification (in this case, items are\nwikipedia articles). One instance of entity-linking can be formalized as an\noptimization problem on the underlying concept graph, where the quantity to be\noptimized is the average distance between chosen items. Inspired by this\napplication, we define a new graph problem which is a natural variant of the\nMaximum Capacity Representative Set. We prove that our problem is NP-hard for\ngeneral graphs; nonetheless, under some restrictive assumptions, it turns out\nto be solvable in linear time. For the general case, we propose two heuristics:\none tries to enforce the above assumptions and another one is based on the\nnotion of hitting distance; we show experimentally how these approaches perform\nwith respect to some baselines on a real-world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 03:22:51 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Blanco", "Roi", "", "Yahoo! Research Barcelona, Spain"], ["Boldi", "Paolo", "", "Dipartimento di informatica, Universit\u00e0 degli Studi di Milano"], ["Marino", "Andrea", "", "Dipartimento di informatica, Universit\u00e0 degli Studi di Milano"]]}, {"id": "1407.7989", "submitter": "Yasser El Madani El Alami", "authors": "Yasser El Madani El Alami, El Habib Nfaoui, Omar El Beqqali", "title": "Multi-agents Architecture for Semantic Retrieving Video in Distributed\n  Environment", "comments": "11 pages, 11 figures, The Proceeding of International Conference on\n  Soft Computing and Software Engineering 2013", "journal-ref": "International Journal of Soft Computing and Software Engineering\n  [JSCSE], Vol. 3, No. 3, pp.430-440, 2013", "doi": "10.7321/jscse.v3.n3.65", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an integrated multi-agents architecture for indexing and\nretrieving video information.The focus of our work is to elaborate an\nextensible approach that gathers a priori almost of the mandatory tools which\npalliate to the major intertwining problems raised in the whole process of the\nvideo lifecycle (classification, indexing and retrieval). In fact, effective\nand optimal retrieval video information needs a collaborative approach based on\nmultimodal aspects. Clearly, it must to take into account the distributed\naspect of the data sources, the adaptation of the contents, semantic\nannotation, personalized request and active feedback which constitute the\nbackbone of a vigorous system which improve its performances in a smart way\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 10:23:18 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 13:30:05 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Alami", "Yasser El Madani El", ""], ["Nfaoui", "El Habib", ""], ["Beqqali", "Omar El", ""]]}, {"id": "1407.8186", "submitter": "Xiaoting Zhao", "authors": "Xiaoting Zhao, Peter I. Frazier", "title": "Exploration vs. Exploitation in the Information Filtering Problem", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider information filtering, in which we face a stream of items too\nvoluminous to process by hand (e.g., scientific articles, blog posts, emails),\nand must rely on a computer system to automatically filter out irrelevant\nitems. Such systems face the exploration vs. exploitation tradeoff, in which it\nmay be beneficial to present an item despite a low probability of relevance,\njust to learn about future items with similar content. We present a Bayesian\nsequential decision-making model of this problem, show how it may be solved to\noptimality using a decomposition to a collection of two-armed bandit problems,\nand show structural results for the optimal policy. We show that the resulting\nmethod is especially useful when facing the cold start problem, i.e., when\nfiltering items for new users without a long history of past interactions. We\nthen present an application of this information filtering method to a\nhistorical dataset from the arXiv.org repository of scientific articles.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 20:00:11 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 20:01:00 GMT"}, {"version": "v3", "created": "Sun, 8 Feb 2015 21:03:08 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Zhao", "Xiaoting", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1407.8258", "submitter": "Jill Slay", "authors": "Jill Slay and Fiona Schulz", "title": "Development of an Ontology Based Forensic Search Mechanism: Proof of\n  Concept", "comments": null, "journal-ref": "Journal of Digital Forensics, Security and Law, 1(1), 25-44", "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the problems faced by Law Enforcement in searching large\nquantities of electronic evidence. It examines the use of ontologies as the\nbasis for new forensic software filters and provides a proof of concept tool\nbased on an ontological design. It demonstrates that efficient searching is\nproduced through the use of such a design and points to further work that might\nbe carried out to extend this concept.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 01:58:02 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Slay", "Jill", ""], ["Schulz", "Fiona", ""]]}, {"id": "1407.8365", "submitter": "Mohammad Dehghan", "authors": "Mohammad Dehghan Bahabadi, Alireza Hashemi Golpayegani, Leila Esmaeili", "title": "A Novel C2C E-Commerce Recommender System Based on Link Prediction:\n  Applying Social Network Analysis", "comments": "7 pages, 5 figures", "journal-ref": "International Journal of Advanced Studies in Computer Science &\n  Engineering (ijascse), Vol 3, Issue 7, July 2014", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis emerged as an important research topic in sociology\ndecades ago, and it has also attracted scientists from various fields of study\nlike psychology, anthropology, geography and economics. In recent years, a\nsignificant number of researches has been conducted on using social network\nanalysis to design e-commerce recommender systems. Most of the current\nrecommender systems are designed for B2C e-commerce websites. This paper\nfocuses on building a recommendation algorithm for C2C e-commerce business\nmodel by considering special features of C2C e-commerce websites. In this\npaper, we consider users and their transactions as a network; by this mapping,\nlink prediction technique which is an important task in social network analysis\ncould be used to build the recommender system. The proposed tow-level\nrecommendation algorithm, rather than topology of the network, uses nodes\nfeatures like: category of items, ratings of users, and reputation of sellers.\nThe results show that the proposed model can be used to predict a portion of\nfuture trades between users in a C2C commercial network.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 11:45:05 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Bahabadi", "Mohammad Dehghan", ""], ["Golpayegani", "Alireza Hashemi", ""], ["Esmaeili", "Leila", ""]]}, {"id": "1407.8499", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal and Khushboo Singhal", "title": "Twitter User Classification using Ambient Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging websites, especially Twitter have become an important means of\ncommunication, in today's time. Often these services have been found to be\nfaster than conventional news services. With millions of users, a need was felt\nto classify users based on ambient metadata associated with their user\naccounts. We particularly look at the effectiveness of the profile description\nfield in order to carry out the task of user classification. Our results show\nthat such metadata can be an effective feature for any classification task.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 17:55:25 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Nagpal", "Chirag", ""], ["Singhal", "Khushboo", ""]]}]