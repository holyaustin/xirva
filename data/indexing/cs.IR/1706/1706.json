[{"id": "1706.00178", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "M.A. K{\\l}opotek and S.T. Wierzcho\\'n and R.A. K{\\l}opotek", "title": "Network Capacity Bound for Personalized PageRank in Multimodal Networks", "comments": "28 pages. arXiv admin note: text overlap with arXiv:1702.03734", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a former paper the concept of Bipartite PageRank was introduced and a\ntheorem on the limit of authority flowing between nodes for personalized\nPageRank has been generalized. In this paper we want to extend those results to\nmultimodal networks. In particular we introduce a hypergraph type that may be\nused for describing multimodal network where a hyperlink connects nodes from\neach of the modalities. We introduce a generalisation of PageRank for such\ngraphs and define the respective random walk model that can be used for\ncomputations. we finally state and prove theorems on the limit of outflow of\nauthority for cases where individual modalities have identical and distinct\ndamping factors.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 06:46:30 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["K\u0142opotek", "M. A.", ""], ["Wierzcho\u0144", "S. T.", ""], ["K\u0142opotek", "R. A.", ""]]}, {"id": "1706.00188", "submitter": "Pinkesh Badjatiya", "authors": "Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, Vasudeva Varma", "title": "Deep Learning for Hate Speech Detection in Tweets", "comments": "In Proceedings of ACM WWW'17 Companion, Perth, Western Australia, Apr\n  2017 (WWW'17), 2 pages", "journal-ref": null, "doi": "10.1145/3041021.3054223", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hate speech detection on Twitter is critical for applications like\ncontroversial event extraction, building AI chatterbots, content\nrecommendation, and sentiment analysis. We define this task as being able to\nclassify a tweet as racist, sexist or neither. The complexity of the natural\nlanguage constructs makes this task very challenging. We perform extensive\nexperiments with multiple deep learning architectures to learn semantic word\nembeddings to handle this complexity. Our experiments on a benchmark dataset of\n16K annotated tweets show that such deep learning methods outperform\nstate-of-the-art char/word n-gram methods by ~18 F1 points.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 07:25:22 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Badjatiya", "Pinkesh", ""], ["Gupta", "Shashank", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1706.00218", "submitter": "\\\"Ozg\\\"ur Demir", "authors": "\\\"Ozg\\\"ur Demir, Alexey Rodriguez Yakushev, Rany Keddo, Ursula Kallio", "title": "Item-Item Music Recommendations With Side Information", "comments": "Updated GitHub repository link", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online music services have tens of millions of tracks. The content itself is\nbroad and covers various musical genres as well as non-musical audio content\nsuch as radio plays and podcasts. The sheer scale and diversity of content\nmakes it difficult for a user to find relevant tracks. Relevant recommendations\nare therefore crucial for a good user experience. Here we present a method to\ncompute track-track similarities using collaborative filtering signals with\nside information. On a data set from music streaming service SoundCloud, the\nmethod here outperforms the widely adopted implicit matrix factorization\ntechnique. The implementation of our method is open sourced and can be applied\nto related item-item recommendation tasks with side information.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 09:14:58 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 13:40:00 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 14:03:16 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Demir", "\u00d6zg\u00fcr", ""], ["Yakushev", "Alexey Rodriguez", ""], ["Keddo", "Rany", ""], ["Kallio", "Ursula", ""]]}, {"id": "1706.00359", "submitter": "Yishu Miao", "authors": "Yishu Miao, Edward Grefenstette, Phil Blunsom", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:55:42 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 19:00:21 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Miao", "Yishu", ""], ["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""]]}, {"id": "1706.00447", "submitter": "Allan Pinto", "authors": "Allan Pinto, Daniel Moreira, Aparna Bharati, Joel Brogan, Kevin\n  Bowyer, Patrick Flynn, Walter Scheirer and Anderson Rocha", "title": "Provenance Filtering for Multimedia Phylogeny", "comments": "5 pages, Accepted in IEEE International Conference on Image\n  Processing (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Departing from traditional digital forensics modeling, which seeks to analyze\nsingle objects in isolation, multimedia phylogeny analyzes the evolutionary\nprocesses that influence digital objects and collections over time. One of its\nintegral pieces is provenance filtering, which consists of searching a\npotentially large pool of objects for the most related ones with respect to a\ngiven query, in terms of possible ancestors (donors or contributors) and\ndescendants. In this paper, we propose a two-tiered provenance filtering\napproach to find all the potential images that might have contributed to the\ncreation process of a given query $q$. In our solution, the first (coarse) tier\naims to find the most likely \"host\" images --- the major donor or background\n--- contributing to a composite/doctored image. The search is then refined in\nthe second tier, in which we search for more specific (potentially small) parts\nof the query that might have been extracted from other images and spliced into\nthe query image. Experimental results with a dataset containing more than a\nmillion images show that the two-tiered solution underpinned by the context of\nthe query is highly useful for solving this difficult task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 18:12:57 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Pinto", "Allan", ""], ["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Brogan", "Joel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Scheirer", "Walter", ""], ["Rocha", "Anderson", ""]]}, {"id": "1706.00516", "submitter": "Oren Halvani", "authors": "Oren Halvani, Christian Winter, Lukas Graner", "title": "Authorship Verification based on Compression-Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression models represent an interesting approach for different\nclassification tasks and have been used widely across many research fields. We\nadapt compression models to the field of authorship verification (AV), a branch\nof digital text forensics. The task in AV is to verify if a questioned document\nand a reference document of a known author are written by the same person. We\npropose an intrinsic AV method, which yields competitive results compared to a\nnumber of current state-of-the-art approaches, based on support vector machines\nor neural networks. However, in contrast to these approaches our method does\nnot make use of machine learning algorithms, natural language processing\ntechniques, feature engineering, hyperparameter optimization or external\ndocuments (a common strategy to transform AV from a one-class to a multi-class\nclassification problem). Instead, the only three key components of our method\nare a compressing algorithm, a dissimilarity measure and a threshold, needed to\naccept or reject the authorship of the questioned document. Due to its\ncompactness, our method performs very fast and can be reimplemented with\nminimal effort. In addition, the method can handle complicated AV cases where\nboth, the questioned and the reference document, are not related to each other\nin terms of topic or genre. We evaluated our approach against publicly\navailable datasets, which were used in three international AV competitions.\nFurthermore, we constructed our own corpora, where we evaluated our method\nagainst state-of-the-art approaches and achieved, in both cases, promising\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 22:48:49 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Halvani", "Oren", ""], ["Winter", "Christian", ""], ["Graner", "Lukas", ""]]}, {"id": "1706.00695", "submitter": "Jitao Sang", "authors": "Yuqi Gao and Jitao Sang and Tongwei Ren and Changsheng Xu", "title": "Hashtag-centric Immersive Search on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media information distributes in different Online Social Networks\n(OSNs). This paper addresses the problem integrating the cross-OSN information\nto facilitate an immersive social media search experience. We exploit hashtag,\nwhich is widely used to annotate and organize multi-modal items in different\nOSNs, as the bridge for information aggregation and organization. A three-stage\nsolution framework is proposed for hashtag representation, clustering and\ndemonstration. Given an event query, the related items from three OSNs,\nTwitter, Flickr and YouTube, are organized in cluster-hashtag-item hierarchy\nfor display. The effectiveness of the proposed solution is validated by\nqualitative and quantitative experiments on hundreds of trending event queries.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 14:20:34 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Gao", "Yuqi", ""], ["Sang", "Jitao", ""], ["Ren", "Tongwei", ""], ["Xu", "Changsheng", ""]]}, {"id": "1706.00816", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ameni Kacem", "title": "A Complete Year of User Retrieval Sessions in a Social Sciences Academic\n  Search Engine", "comments": "6 pages, 2 figures, accepted short paper at the 21st International\n  Conference on Theory and Practice of Digital Libraries (TPDL 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-67008-9_46", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an open data set extracted from the transaction log\nof the social sciences academic search engine sowiport. The data set includes a\nfiltered set of 484,449 retrieval sessions which have been carried out by\nsowiport users in the period from April 2014 to April 2015. We propose a\ndescription of interactions performed by the academic search engine users that\ncan be used in different applications such as result ranking improvement, user\nmodeling, query reformulation analysis, search pattern recognition.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 18:46:20 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 19:26:22 GMT"}, {"version": "v3", "created": "Sat, 23 Sep 2017 19:56:15 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Mayr", "Philipp", ""], ["Kacem", "Ameni", ""]]}, {"id": "1706.00884", "submitter": "Xintao Wu", "authors": "Shuhan Yuan, Xintao Wu, Yang Xiang", "title": "Task-specific Word Identification from Short Texts Using a Convolutional\n  Neural Network", "comments": "accepted by Intelligent Data Analysis, an International Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-specific word identification aims to choose the task-related words that\nbest describe a short text. Existing approaches require well-defined seed words\nor lexical dictionaries (e.g., WordNet), which are often unavailable for many\napplications such as social discrimination detection and fake review detection.\nHowever, we often have a set of labeled short texts where each short text has a\ntask-related class label, e.g., discriminatory or non-discriminatory, specified\nby users or learned by classification algorithms. In this paper, we focus on\nidentifying task-specific words and phrases from short texts by exploiting\ntheir class labels rather than using seed words or lexical dictionaries. We\nconsider the task-specific word and phrase identification as feature learning.\nWe train a convolutional neural network over a set of labeled texts and use\nscore vectors to localize the task-specific words and phrases. Experimental\nresults on sentiment word identification show that our approach significantly\noutperforms existing methods. We further conduct two case studies to show the\neffectiveness of our approach. One case study on a crawled tweets dataset\ndemonstrates that our approach can successfully capture the\ndiscrimination-related words/phrases. The other case study on fake review\ndetection shows that our approach can identify the fake-review words/phrases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 02:15:44 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yuan", "Shuhan", ""], ["Wu", "Xintao", ""], ["Xiang", "Yang", ""]]}, {"id": "1706.00923", "submitter": "Shashank Gupta", "authors": "Shashank Gupta, Pulkit Parikh, Manish Gupta, Vasudeva Varma", "title": "Simultaneous Inference of User Representations and Trust", "comments": "To appear in the proceedings of ASONAM'17. Please cite that version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring trust relations between social media users is critical for a number\nof applications wherein users seek credible information. The fact that\navailable trust relations are scarce and skewed makes trust prediction a\nchallenging task. To the best of our knowledge, this is the first work on\nexploring representation learning for trust prediction. We propose an approach\nthat uses only a small amount of binary user-user trust relations to\nsimultaneously learn user embeddings and a model to predict trust between user\npairs. We empirically demonstrate that for trust prediction, our approach\noutperforms classifier-based approaches which use state-of-the-art\nrepresentation learning methods like DeepWalk and LINE as features. We also\nconduct experiments which use embeddings pre-trained with DeepWalk and LINE\neach as an input to our model, resulting in further performance improvement.\nExperiments with a dataset of $\\sim$356K user pairs show that the proposed\nmethod can obtain an high F-score of 92.65%.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 10:25:27 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Gupta", "Shashank", ""], ["Parikh", "Pulkit", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1706.00957", "submitter": "Michal R\\r{u}\\v{z}i\\v{c}ka", "authors": "Jan Rygl, Jan Pomik\\'alek, Radim \\v{R}eh\\r{u}\\v{r}ek, Michal\n  R\\r{u}\\v{z}i\\v{c}ka, V\\'it Novotn\\'y, Petr Sojka", "title": "Semantic Vector Encoding and Similarity Search Using Fulltext Search\n  Engines", "comments": "Preprint of the paper accepted to the ACL 2017 (http://acl2017.org/)\n  workshop RepL4NLP 2017 (https://sites.google.com/site/repl4nlp2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representations and vector space modeling (VSM) play a central role in\nmodern machine learning. We propose a novel approach to `vector similarity\nsearching' over dense semantic representations of words and documents that can\nbe deployed on top of traditional inverted-index-based fulltext engines, taking\nadvantage of their robustness, stability, scalability and ubiquity.\n  We show that this approach allows the indexing and querying of dense vectors\nin text domains. This opens up exciting avenues for major efficiency gains,\nalong with simpler deployment, scaling and monitoring.\n  The end result is a fast and scalable vector database with a tunable\ntrade-off between vector search performance and quality, backed by a standard\nfulltext engine such as Elasticsearch.\n  We empirically demonstrate its querying performance and quality by applying\nthis solution to the task of semantic searching over a dense vector\nrepresentation of the entire English Wikipedia.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 14:21:22 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Rygl", "Jan", ""], ["Pomik\u00e1lek", "Jan", ""], ["\u0158eh\u016f\u0159ek", "Radim", ""], ["R\u016f\u017ei\u010dka", "Michal", ""], ["Novotn\u00fd", "V\u00edt", ""], ["Sojka", "Petr", ""]]}, {"id": "1706.00973", "submitter": "Soumen Chakrabarti", "authors": "Uma Sawant, Saurabh Garg, Soumen Chakrabarti and Ganesh Ramakrishnan", "title": "Neural Architecture for Question Answering Using a Knowledge Graph and\n  Web Corpus", "comments": "Accepted to Information Retrieval Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Web search, entity-seeking queries often trigger a special Question\nAnswering (QA) system. It may use a parser to interpret the question to a\nstructured query, execute that on a knowledge graph (KG), and return direct\nentity responses. QA systems based on precise parsing tend to be brittle: minor\nsyntax variations may dramatically change the response. Moreover, KG coverage\nis patchy. At the other extreme, a large corpus may provide broader coverage,\nbut in an unstructured, unreliable form. We present AQQUCN, a QA system that\ngracefully combines KG and corpus evidence. AQQUCN accepts a broad spectrum of\nquery syntax, between well-formed questions to short `telegraphic' keyword\nsequences. In the face of inherent query ambiguities, AQQUCN aggregates signals\nfrom KGs and large corpora to directly rank KG entities, rather than commit to\none semantic interpretation of the query. AQQUCN models the ideal\ninterpretation as an unobservable or latent variable. Interpretations and\ncandidate entity responses are scored as pairs, by combining signals from\nmultiple convolutional networks that operate collectively on the query, KG and\ncorpus. On four public query workloads, amounting to over 8,000 queries with\ndiverse query syntax, we see 5--16% absolute improvement in mean average\nprecision (MAP), compared to the entity ranking performance of recent systems.\nOur system is also competitive at entity set retrieval, almost doubling F1\nscores for challenging short queries.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 16:27:32 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 18:38:56 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 10:28:17 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Sawant", "Uma", ""], ["Garg", "Saurabh", ""], ["Chakrabarti", "Soumen", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1706.01038", "submitter": "Van-Khanh Tran", "authors": "Danilo S. Carvalho, Duc-Vu Tran, Van-Khanh Tran, Le-Nguyen Minh", "title": "Improving Legal Information Retrieval by Distributional Composition with\n  Term Order Probabilities", "comments": "wrong version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal professionals worldwide are currently trying to get up-to-pace with the\nexplosive growth in legal document availability through digital means. This\ndrives a need for high efficiency Legal Information Retrieval (IR) and Question\nAnswering (QA) methods. The IR task in particular has a set of unique\nchallenges that invite the use of semantic motivated NLP techniques. In this\nwork, a two-stage method for Legal Information Retrieval is proposed, combining\nlexical statistics and distributional sentence representations in the context\nof Competition on Legal Information Extraction/Entailment (COLIEE). The\ncombination is done with the use of disambiguation rules, applied over the\nrankings obtained through n-gram statistics. After the ranking is done, its\nresults are evaluated for ambiguity, and disambiguation is done if a result is\ndecided to be unreliable for a given query. Competition and experimental\nresults indicate small gains in overall retrieval performance using the\nproposed approach. Additionally, an analysis of error and improvement cases is\npresented for a better understanding of the contributions.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 06:57:09 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 10:49:15 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Carvalho", "Danilo S.", ""], ["Tran", "Duc-Vu", ""], ["Tran", "Van-Khanh", ""], ["Minh", "Le-Nguyen", ""]]}, {"id": "1706.01084", "submitter": "Ting Chen", "authors": "Ting Chen, Liangjie Hong, Yue Shi, Yizhou Sun", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "comments": "typo fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good representation of text is key to many recommendation\napplications. Examples include news recommendation where texts to be\nrecommended are constantly published everyday. However, most existing\nrecommendation techniques, such as matrix factorization based methods, mainly\nrely on interaction histories to learn representations of items. While latent\nfactors of items can be learned effectively from user interaction data, in many\ncases, such data is not available, especially for newly emerged items.\n  In this work, we aim to address the problem of personalized recommendation\nfor completely new items with text information available. We cast the problem\nas a personalized text ranking problem and propose a general framework that\ncombines text embedding with personalized recommendation. Users and textual\ncontent are embedded into latent feature space. The text embedding function can\nbe learned end-to-end by predicting user interactions with items. To alleviate\nsparsity in interaction data, and leverage large amount of text data with\nlittle or no user interactions, we further propose a joint text embedding model\nthat incorporates unsupervised text embedding with a combination module.\nExperimental results show that our model can significantly improve the\neffectiveness of recommendation systems on real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:48:28 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 21:55:56 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Ting", ""], ["Hong", "Liangjie", ""], ["Shi", "Yue", ""], ["Sun", "Yizhou", ""]]}, {"id": "1706.01442", "submitter": "Karim Banawan", "authors": "Karim Banawan, Sennur Ulukus", "title": "The Capacity of Private Information Retrieval from Byzantine and\n  Colluding Databases", "comments": "Submitted to IEEE Transactions on Information Theory, June 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of single-round private information retrieval (PIR)\nfrom $N$ replicated databases. We consider the case when $B$ databases are\noutdated (unsynchronized), or even worse, adversarial (Byzantine), and\ntherefore, can return incorrect answers. In the PIR problem with Byzantine\ndatabases (BPIR), a user wishes to retrieve a specific message from a set of\n$M$ messages with zero-error, irrespective of the actions performed by the\nByzantine databases. We consider the $T$-privacy constraint in this paper,\nwhere any $T$ databases can collude, and exchange the queries submitted by the\nuser. We derive the information-theoretic capacity of this problem, which is\nthe maximum number of \\emph{correct symbols} that can be retrieved privately\n(under the $T$-privacy constraint) for every symbol of the downloaded data. We\ndetermine the exact BPIR capacity to be\n$C=\\frac{N-2B}{N}\\cdot\\frac{1-\\frac{T}{N-2B}}{1-(\\frac{T}{N-2B})^M}$, if $2B+T\n< N$. This capacity expression shows that the effect of Byzantine databases on\nthe retrieval rate is equivalent to removing $2B$ databases from the system,\nwith a penalty factor of $\\frac{N-2B}{N}$, which signifies that even though the\nnumber of databases needed for PIR is effectively $N-2B$, the user still needs\nto access the entire $N$ databases. The result shows that for the\nunsynchronized PIR problem, if the user does not have any knowledge about the\nfraction of the messages that are mis-synchronized, the single-round capacity\nis the same as the BPIR capacity. Our achievable scheme extends the optimal\nachievable scheme for the robust PIR (RPIR) problem to correct the\n\\emph{errors} introduced by the Byzantine databases as opposed to\n\\emph{erasures} in the RPIR problem. Our converse proof uses the idea of the\ncut-set bound in the network coding problem against adversarial nodes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:45:43 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1706.01449", "submitter": "Firas Abuzaid", "authors": "Firas Abuzaid, Geet Sethi, Peter Bailis, Matei Zaharia", "title": "To Index or Not to Index: Optimizing Exact Maximum Inner Product Search", "comments": "12 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Maximum Inner Product Search (MIPS) is an important task that is widely\npertinent to recommender systems and high-dimensional similarity search. The\nbrute-force approach to solving exact MIPS is computationally expensive, thus\nspurring recent development of novel indexes and pruning techniques for this\ntask. In this paper, we show that a hardware-efficient brute-force approach,\nblocked matrix multiply (BMM), can outperform the state-of-the-art MIPS solvers\nby over an order of magnitude, for some -- but not all -- inputs.\n  In this paper, we also present a novel MIPS solution, MAXIMUS, that takes\nadvantage of hardware efficiency and pruning of the search space. Like BMM,\nMAXIMUS is faster than other solvers by up to an order of magnitude, but again\nonly for some inputs. Since no single solution offers the best runtime\nperformance for all inputs, we introduce a new data-dependent optimizer,\nOPTIMUS, that selects online with minimal overhead the best MIPS solver for a\ngiven input. Together, OPTIMUS and MAXIMUS outperform state-of-the-art MIPS\nsolvers by 3.2$\\times$ on average, and up to 10.9$\\times$, on widely studied\nMIPS datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:56:43 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 22:08:15 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 00:52:25 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Abuzaid", "Firas", ""], ["Sethi", "Geet", ""], ["Bailis", "Peter", ""], ["Zaharia", "Matei", ""]]}, {"id": "1706.01574", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra and Emine Yilmaz", "title": "Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian\n  Nonparametric Approach", "comments": "10 pages. Accepted at SIGIR 2017 as a full paper", "journal-ref": null, "doi": "10.1145/3077136.3080823", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of search queries originate from some real world\ninformation need or tasks. In order to improve the search experience of the end\nusers, it is important to have accurate representations of tasks. As a result,\nsignificant amount of research has been devoted to extracting proper\nrepresentations of tasks in order to enable search systems to help users\ncomplete their tasks, as well as providing the end user with better query\nsuggestions, for better recommendations, for satisfaction prediction, and for\nimproved personalization in terms of tasks. Most existing task extraction\nmethodologies focus on representing tasks as flat structures. However, tasks\noften tend to have multiple subtasks associated with them and a more\nnaturalistic representation of tasks would be in terms of a hierarchy, where\neach task can be composed of multiple (sub)tasks. To this end, we propose an\nefficient Bayesian nonparametric model for extracting hierarchies of such tasks\n\\& subtasks. We evaluate our method based on real world query log data both\nthrough quantitative and crowdsourced experiments and highlight the importance\nof considering task/subtask hierarchies.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 01:10:51 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 01:47:30 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Yilmaz", "Emine", ""]]}, {"id": "1706.02061", "submitter": "Nir Levine", "authors": "Nir Levine, Haggai Roitman, and Doron Cohen", "title": "An Extended Relevance Model for Session Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The session search task aims at best serving the user's information need\ngiven her previous search behavior during the session. We propose an extended\nrelevance model that captures the user's dynamic information need in the\nsession. Our relevance modelling approach is directly driven by the user's\nquery reformulation (change) decisions and the estimate of how much the user's\nsearch behavior affects such decisions. Overall, we demonstrate that, the\nproposed approach significantly boosts session search performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:57:25 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Levine", "Nir", ""], ["Roitman", "Haggai", ""], ["Cohen", "Doron", ""]]}, {"id": "1706.02093", "submitter": "Shichen Liu", "authors": "Shichen Liu, Fei Xiao, Wenwu Ou, Luo Si", "title": "Cascade Ranking for Operational E-commerce Search", "comments": null, "journal-ref": null, "doi": "10.1145/3097983.3098011", "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 'Big Data' era, many real-world applications like search involve the\nranking problem for a large number of items. It is important to obtain\neffective ranking results and at the same time obtain the results efficiently\nin a timely manner for providing good user experience and saving computational\ncosts. Valuable prior research has been conducted for learning to efficiently\nrank like the cascade ranking (learning) model, which uses a sequence of\nranking functions to progressively filter some items and rank the remaining\nitems. However, most existing research of learning to efficiently rank in\nsearch is studied in a relatively small computing environments with simulated\nuser queries.\n  This paper presents novel research and thorough study of designing and\ndeploying a Cascade model in a Large-scale Operational E-commerce Search\napplication (CLOES), which deals with hundreds of millions of user queries per\nday with hundreds of servers. The challenge of the real-world application\nprovides new insights for research: 1). Real-world search applications often\ninvolve multiple factors of preferences or constraints with respect to user\nexperience and computational costs such as search accuracy, search latency,\nsize of search results and total CPU cost, while most existing search solutions\nonly address one or two factors; 2). Effectiveness of e-commerce search\ninvolves multiple types of user behaviors such as click and purchase, while\nmost existing cascade ranking in search only models the click behavior. Based\non these observations, a novel cascade ranking model is designed and deployed\nin an operational e-commerce search application. An extensive set of\nexperiments demonstrate the advantage of the proposed work to address multiple\nfactors of effectiveness, efficiency and user experience in the real-world\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 09:03:13 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Liu", "Shichen", ""], ["Xiao", "Fei", ""], ["Ou", "Wenwu", ""], ["Si", "Luo", ""]]}, {"id": "1706.02153", "submitter": "Edwin Henneken", "authors": "Edwin A. Henneken, Michael J. Kurtz", "title": "Usage Bibliometrics as a Tool to Measure Research Activity", "comments": "25 pages, 11 figures, accepted for publication in Handbook of\n  Quantitative Science and Technology Research, Springer", "journal-ref": null, "doi": "10.1007/978-3-030-02511-3_32", "report-no": null, "categories": "cs.DL astro-ph.IM cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures for research activity and impact have become an integral ingredient\nin the assessment of a wide range of entities (individual researchers,\norganizations, instruments, regions, disciplines). Traditional bibliometric\nindicators, like publication and citation based indicators, provide an\nessential part of this picture, but cannot describe the complete picture. Since\nreading scholarly publications is an essential part of the research life cycle,\nit is only natural to introduce measures for this activity in attempts to\nquantify the efficiency, productivity and impact of an entity. Citations and\nreads are significantly different signals, so taken together, they provide a\nmore complete picture of research activity. Most scholarly publications are now\naccessed online, making the study of reads and their patterns possible.\nClick-stream logs allow us to follow information access by the entire research\ncommunity, real-time. Publication and citation datasets just reflect activity\nby authors. In addition, download statistics will help us identify publications\nwith significant impact, but which do not attract many citations. Click-stream\nsignals are arguably more complex than, say, citation signals. For one, they\nare a superposition of different classes of readers. Systematic downloads by\ncrawlers also contaminate the signal, as does browsing behavior. We discuss the\ncomplexities associated with clickstream data and how, with proper filtering,\nstatistically significant relations and conclusions can be inferred from\ndownload statistics. We describe how download statistics can be used to\ndescribe research activity at different levels of aggregation, ranging from\norganizations to countries. These statistics show a correlation with\nsocio-economic indicators. A comparison will be made with traditional\nbibliometric indicators. We will argue that astronomy is representative of more\ngeneral trends.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 12:33:13 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Henneken", "Edwin A.", ""], ["Kurtz", "Michael J.", ""]]}, {"id": "1706.02263", "submitter": "Rianne van den Berg", "authors": "Rianne van den Berg, Thomas N. Kipf, Max Welling", "title": "Graph Convolutional Matrix Completion", "comments": "9 pages, 3 figures, updated with additional experimental evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider matrix completion for recommender systems from the point of view\nof link prediction on graphs. Interaction data such as movie ratings can be\nrepresented by a bipartite user-item graph with labeled edges denoting observed\nratings. Building on recent progress in deep learning on graph-structured data,\nwe propose a graph auto-encoder framework based on differentiable message\npassing on the bipartite interaction graph. Our model shows competitive\nperformance on standard collaborative filtering benchmarks. In settings where\ncomplimentary feature information or structured data such as a social network\nis available, our framework outperforms recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:05:19 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:20:03 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Berg", "Rianne van den", ""], ["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1706.02361", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Kyunghyun Cho and Mark Sandler", "title": "The Effects of Noisy Labels on Deep Convolutional Neural Networks for\n  Music Tagging", "comments": "The section that overlapped with arXiv:1709.01922 is completely\n  removed since the earlier version. This is the camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) have been successfully applied to music\nclassification including music tagging. However, there are several open\nquestions regarding the training, evaluation, and analysis of DNNs. In this\narticle, we investigate specific aspects of neural networks, the effects of\nnoisy labels, to deepen our understanding of their properties. We analyse and\n(re-)validate a large music tagging dataset to investigate the reliability of\ntraining and evaluation. Using a trained network, we compute label vector\nsimilarities which is compared to groundtruth similarity.\n  The results highlight several important aspects of music tagging and neural\nnetworks. We show that networks can be effective despite relatively large error\nrates in groundtruth datasets, while conjecturing that label noise can be the\ncause of varying tag-wise performance differences. Lastly, the analysis of our\ntrained network provides valuable insight into the relationships between music\ntags. These results highlight the benefit of using data-driven methods to\naddress automatic music tagging.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:54:39 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 23:47:42 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 15:54:38 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1706.02509", "submitter": "Philipp Mayr", "authors": "Muthu Kumar Chandrasekaran, Kokil Jaidka, Philipp Mayr", "title": "Joint Workshop on Bibliometric-enhanced Information Retrieval and\n  Natural Language Processing for Digital Libraries (BIRNDL 2017)", "comments": "2 pages, workshop paper accepted at the SIGIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large scale of scholarly publications poses a challenge for scholars in\ninformation seeking and sensemaking. Bibliometrics, information retrieval (IR),\ntext mining and NLP techniques could help in these search and look-up\nactivities, but are not yet widely used. This workshop is intended to stimulate\nIR researchers and digital library professionals to elaborate on new approaches\nin natural language processing, information retrieval, scientometrics, text\nmining and recommendation techniques that can advance the state-of-the-art in\nscholarly document understanding, analysis, and retrieval at scale. The BIRNDL\nworkshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the\nthird edition of the Computational Linguistics (CL) Scientific Summarization\nShared Task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 10:53:57 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Chandrasekaran", "Muthu Kumar", ""], ["Jaidka", "Kokil", ""], ["Mayr", "Philipp", ""]]}, {"id": "1706.02769", "submitter": "Vineeth Kashyap", "authors": "Vineeth Kashyap, David Bingham Brown, Ben Liblit, David Melski, Thomas\n  Reps", "title": "Source Forager: A Search Engine for Similar Source Code", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers spend a significant amount of time searching for code: e.g., to\nunderstand how to complete, correct, or adapt their own code for a new context.\nUnfortunately, the state of the art in code search has not evolved much beyond\ntext search over tokenized source. Code has much richer structure and semantics\nthan normal text, and this property can be exploited to specialize the\ncode-search process for better querying, searching, and ranking of code-search\nresults.\n  We present a new code-search engine named Source Forager. Given a query in\nthe form of a C/C++ function, Source Forager searches a pre-populated code\ndatabase for similar C/C++ functions. Source Forager preprocesses the database\nto extract a variety of simple code features that capture different aspects of\ncode. A search returns the $k$ functions in the database that are most similar\nto the query, based on the various extracted code features.\n  We tested the usefulness of Source Forager using a variety of code-search\nqueries from two domains. Our experiments show that the ranked results returned\nby Source Forager are accurate, and that query-relevant functions can be\nreliably retrieved even when searching through a large code database that\ncontains very few query-relevant functions.\n  We believe that Source Forager is a first step towards much-needed tools that\nprovide a better code-search experience.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 20:57:20 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kashyap", "Vineeth", ""], ["Brown", "David Bingham", ""], ["Liblit", "Ben", ""], ["Melski", "David", ""], ["Reps", "Thomas", ""]]}, {"id": "1706.03154", "submitter": "Fan Yang", "authors": "Fan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi\n  Kiapour, Robinson Piramuthu", "title": "Visual Search at eBay", "comments": "To appear in 23rd SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD), 2017. A demonstration video can be found at\n  https://youtu.be/iYtjs32vh4g", "journal-ref": null, "doi": "10.1145/3097983.3098162", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end approach for scalable visual\nsearch infrastructure. We discuss the challenges we faced for a massive\nvolatile inventory like at eBay and present our solution to overcome those. We\nharness the availability of large image collection of eBay listings and\nstate-of-the-art deep learning techniques to perform visual search at scale.\nSupervised approach for optimized search limited to top predicted categories\nand also for compact binary signature are key to scale up without compromising\naccuracy and precision. Both use a common deep neural network requiring only a\nsingle forward inference. The system architecture is presented with in-depth\ndiscussions of its basic components and optimizations for a trade-off between\nsearch relevance and latency. This solution is currently deployed in a\ndistributed cloud infrastructure and fuels visual search in eBay ShopBot and\nClose5. We show benchmark on ImageNet dataset on which our approach is faster\nand more accurate than several unsupervised baselines. We share our learnings\nwith the hope that visual search becomes a first class citizen for all large\nscale search engines rather than an afterthought.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:02:34 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 17:21:23 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Yang", "Fan", ""], ["Kale", "Ajinkya", ""], ["Bubnov", "Yury", ""], ["Stein", "Leon", ""], ["Wang", "Qiaosong", ""], ["Kiapour", "Hadi", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1706.03205", "submitter": "Xiang Wang", "authors": "Xiang Wang, Xiangnan He, Liqiang Nie, Tat-Seng Chua", "title": "Item Silk Road: Recommending Items from Information Domains to Social\n  Users", "comments": "10 pages, 7 figures, SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080771", "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online platforms can be divided into information-oriented and social-oriented\ndomains. The former refers to forums or E-commerce sites that emphasize\nuser-item interactions, like Trip.com and Amazon; whereas the latter refers to\nsocial networking services (SNSs) that have rich user-user connections, such as\nFacebook and Twitter. Despite their heterogeneity, these two domains can be\nbridged by a few overlapping users, dubbed as bridge users. In this work, we\naddress the problem of cross-domain social recommendation, i.e., recommending\nrelevant items of information domains to potential users of social networks. To\nour knowledge, this is a new problem that has rarely been studied before.\n  Existing cross-domain recommender systems are unsuitable for this task since\nthey have either focused on homogeneous information domains or assumed that\nusers are fully overlapped. Towards this end, we present a novel Neural Social\nCollaborative Ranking (NSCR) approach, which seamlessly sews up the user-item\ninteractions in information domains and user-user connections in SNSs. In the\ninformation domain part, the attributes of users and items are leveraged to\nstrengthen the embedding learning of users and items. In the SNS part, the\nembeddings of bridge users are propagated to learn the embeddings of other\nnon-bridge users. Extensive experiments on two real-world datasets demonstrate\nthe effectiveness and rationality of our NSCR method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 08:58:02 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Wang", "Xiang", ""], ["He", "Xiangnan", ""], ["Nie", "Liqiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1706.03249", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra and Prasanta Bhattacharya", "title": "Characterizing and Predicting Supply-side Engagement on\n  Crowd-contributed Video Sharing Platforms", "comments": "8 pages, ICTIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sharing and entertainment websites have rapidly grown in popularity and\nnow constitute some of the most visited websites on the Internet. Despite the\nactive user engagement on these online video-sharing platforms, most of recent\nresearch on online media platforms have restricted themselves to networking\nbased social media sites, like Facebook or Twitter. We depart from previous\nstudies in the online media space that have focused exclusively on demand-side\nuser engagement, by modeling the supply-side of the crowd-contributed videos on\nthis platform. The current study is among the first to perform a large-scale\nempirical study using longitudinal video upload data from a large online video\nplatform. The modeling and subsequent prediction of video uploads is made\ncomplicated by the heterogeneity of video types (e.g. popular vs. niche video\ngenres), and the inherent time trend effects associated with media uploads. We\nidentify distinct genre-clusters from our dataset and employ a self-exciting\nHawkes point-process model on each of these clusters to fully specify and\nestimate the video upload process. Additionally, we go beyond prediction to\ndisentangle potential factors that govern user engagement and determine the\nvideo upload rates, which improves our analysis with additional explanatory\npower. Our findings show that using a relatively parsimonious point-process\nmodel, we are able to achieve higher model fit, and predict video uploads to\nthe platform with a higher accuracy than competing models. The findings from\nthis study can benefit platform owners in better understanding how their\nsupply-side users engage with their site over time. We also offer a robust\nmethod for performing media upload prediction that is likely to be\ngeneralizable across media platforms which demonstrate similar temporal and\ngenre-level heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 16:26:48 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Bhattacharya", "Prasanta", ""]]}, {"id": "1706.03266", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar, Avisek Gupta", "title": "An Empirical Study of Some Selected IR Models for Bengali Monolingual\n  Information Retrieval", "comments": "6 pages, In Proceedings of ICBIM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evaluation and an analysis of some selected\ninformation retrieval models for Bengali monolingual information retrieval\ntask. Two models, TF-IDF model and the Okapi BM25 model have been considered\nfor our study. The developed IR models are tested on FIRE ad hoc retrieval data\nsets released for different years from 2008 to 2012 and the obtained results\nhave been reported in this paper.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:29:19 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Sarkar", "Kamal", ""], ["Gupta", "Avisek", ""]]}, {"id": "1706.03428", "submitter": "Joeran Beel", "authors": "Joeran Beel, Zeljko Carevic, Johann Schaible, Gabor Neusch", "title": "RARD: The Related-Article Recommendation Dataset", "comments": null, "journal-ref": "D-Lib Magazine, Vol. 23, No. 7/8. Publication date: July 2017", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender-system datasets are used for recommender-system evaluations,\ntraining machine-learning algorithms, and exploring user behavior. While there\nare many datasets for recommender systems in the domains of movies, books, and\nmusic, there are rather few datasets from research-paper recommender systems.\nIn this paper, we introduce RARD, the Related-Article Recommendation Dataset,\nfrom the digital library Sowiport and the recommendation-as-a-service provider\nMr. DLib. The dataset contains information about 57.4 million recommendations\nthat were displayed to the users of Sowiport. Information includes details on\nwhich recommendation approaches were used (e.g. content-based filtering,\nstereotype, most popular), what types of features were used in content based\nfiltering (simple terms vs. keyphrases), where the features were extracted from\n(title or abstract), and the time when recommendations were delivered and\nclicked. In addition, the dataset contains an implicit item-item rating matrix\nthat was created based on the recommendation click logs. RARD enables\nresearchers to train machine learning algorithms for research-paper\nrecommendations, perform offline evaluations, and do research on data from Mr.\nDLib's recommender system, without implementing a recommender system\nthemselves. In the field of scientific recommender systems, our dataset is\nunique. To the best of our knowledge, there is no dataset with more (implicit)\nratings available, and that many variations of recommendation algorithms. The\ndataset is available at http://data.mr-dlib.org, and published under the\nCreative Commons Attribution 3.0 Unported (CC-BY) license.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 01:00:25 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 06:47:33 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Beel", "Joeran", ""], ["Carevic", "Zeljko", ""], ["Schaible", "Johann", ""], ["Neusch", "Gabor", ""]]}, {"id": "1706.03583", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Stefanie Jegelka, Andreas Krause", "title": "Streaming Non-monotone Submodular Maximization: Personalized Video\n  Summarization on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for real time analysis of rapidly producing data streams (e.g.,\nvideo and image streams) motivated the design of streaming algorithms that can\nefficiently extract and summarize useful information from massive data \"on the\nfly\". Such problems can often be reduced to maximizing a submodular set\nfunction subject to various constraints. While efficient streaming methods have\nbeen recently developed for monotone submodular maximization, in a wide range\nof applications, such as video summarization, the underlying utility function\nis non-monotone, and there are often various constraints imposed on the\noptimization problem to consider privacy or personalization. We develop the\nfirst efficient single pass streaming algorithm, Streaming Local Search, that\nfor any streaming monotone submodular maximization algorithm with approximation\nguarantee $\\alpha$ under a collection of independence systems ${\\cal I}$,\nprovides a constant $1/\\big(1+2/\\sqrt{\\alpha}+1/\\alpha\n+2d(1+\\sqrt{\\alpha})\\big)$ approximation guarantee for maximizing a\nnon-monotone submodular function under the intersection of ${\\cal I}$ and $d$\nknapsack constraints. Our experiments show that for video summarization, our\nmethod runs more than 1700 times faster than previous work, while maintaining\npractically the same performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:58:57 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 14:57:15 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 09:58:13 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Jegelka", "Stefanie", ""], ["Krause", "Andreas", ""]]}, {"id": "1706.03757", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas", "title": "Semantic Entity Retrieval Toolkit", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17). 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of low-dimensional, semantic representations of words\nand entities has recently gained attention. In this paper we describe the\nSemantic Entity Retrieval Toolkit (SERT) that provides implementations of our\npreviously published entity representation models. The toolkit provides a\nunified interface to different representation learning algorithms, fine-grained\nparsing configuration and can be used transparently with GPUs. In addition,\nusers can easily modify existing models or implement their own models in the\nframework. After model training, SERT can be used to rank entities according to\na textual query and extract the learned entity/word representation for use in\ndownstream algorithms, such as clustering or recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:51:05 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:30:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1706.03849", "submitter": "Krishnaram Kenthapadi", "authors": "Jian Wang, Krishnaram Kenthapadi, Kaushik Rangadurai, David Hardtke", "title": "Dionysius: A Framework for Modeling Hierarchical User Interactions in\n  Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the following problem: How do we incorporate user item interaction\nsignals as part of the relevance model in a large-scale personalized\nrecommendation system such that, (1) the ability to interpret the model and\nexplain recommendations is retained, and (2) the existing infrastructure\ndesigned for the (user profile) content-based model can be leveraged? We\npropose Dionysius, a hierarchical graphical model based framework and system\nfor incorporating user interactions into recommender systems, with minimal\nchange to the underlying infrastructure. We learn a hidden fields vector for\neach user by considering the hierarchy of interaction signals, and replace the\nuser profile-based vector with this learned vector, thereby not expanding the\nfeature space at all. Thus, our framework allows the use of existing\nrecommendation infrastructure that supports content based features. We\nimplemented and deployed this system as part of the recommendation platform at\nLinkedIn for more than one year. We validated the efficacy of our approach\nthrough extensive offline experiments with different model choices, as well as\nonline A/B testing experiments. Our deployment of this system as part of the\njob recommendation engine resulted in significant improvement in the quality of\nretrieved results, thereby generating improved user experience and positive\nimpact for millions of users.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 20:53:24 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Wang", "Jian", ""], ["Kenthapadi", "Krishnaram", ""], ["Rangadurai", "Kaushik", ""], ["Hardtke", "David", ""]]}, {"id": "1706.03860", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Subspace Clustering via Optimal Direction Search", "comments": null, "journal-ref": "IEEE Signal Processing Letters ( Volume: 24, Issue: 12, Dec. 2017\n  )", "doi": "10.1109/LSP.2017.2757901", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:52:57 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 22:56:21 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 20:36:57 GMT"}, {"version": "v4", "created": "Sun, 26 Nov 2017 15:43:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1706.03960", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Natasa Milic-Frayling, Eduarda Mendes Rodrigues, Carlos\n  Soares", "title": "RELink: A Research Framework and Test Collection for Entity-Relationship\n  Retrieval", "comments": "SIGIR 17 (resource)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements of entity-relationship (E-R) search techniques have been\nhampered by a lack of test collections, particularly for complex queries\ninvolving multiple entities and relationships. In this paper we describe a\nmethod for generating E-R test queries to support comprehensive E-R search\nexperiments. Queries and relevance judgments are created from content that\nexists in a tabular form where columns represent entity types and the table\nstructure implies one or more relationships among the entities. Editorial work\ninvolves creating natural language queries based on relationships represented\nby the entries in the table. We have publicly released the RELink test\ncollection comprising 600 queries and relevance judgments obtained from a\nsample of Wikipedia List-of-lists-of-lists tables. The latter comprise tuples\nof entities that are extracted from columns and labelled by corresponding\nentity types and relationships they represent. In order to facilitate research\nin complex E-R retrieval, we have created and released as open source the\nRELink Framework that includes Apache Lucene indexing and search specifically\ntailored to E-R retrieval. RELink includes entity and relationship indexing\nbased on the ClueWeb-09-B Web collection with FACC1 text span annotations\nlinked to Wikipedia entities. With ready to use search resources and a\ncomprehensive test collection, we support community in pursuing E-R research at\nscale.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 09:10:10 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Saleiro", "Pedro", ""], ["Milic-Frayling", "Natasa", ""], ["Rodrigues", "Eduarda Mendes", ""], ["Soares", "Carlos", ""]]}, {"id": "1706.03993", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Alexandros Karatzoglou", "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary\n  input/output networks", "comments": "Accepted for publication at ACM RecSys 2017; previous version\n  submitted to ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms that incorporate techniques from deep learning are\nbecoming increasingly popular. Due to the structure of the data coming from\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\nthese algorithms tend to have large input and output dimensionalities that\ndominate their overall size. This makes them difficult to train, due to the\nlimited memory of graphical processing units, and difficult to deploy on mobile\ndevices with limited hardware. To address these difficulties, we propose Bloom\nembeddings, a compression technique that can be applied to the input and output\nof neural network models dealing with sparse high-dimensional binary-coded\ninstances. Bloom embeddings are computationally efficient, and do not seriously\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\ncases, they even improve over the original accuracy, with relative increases up\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\nalternative methods, obtaining favorable results. We also discuss a number of\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\noperation, zero or marginal space requirements, training time speedups, or the\nfact that they do not require any change to the core model architecture or\ntraining configuration.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 10:50:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1706.04026", "submitter": "Panayiotis Christodoulou", "authors": "Sotirios Chatzis, Panayiotis Christodoulou, Andreas S. Andreou", "title": "Recurrent Latent Variable Networks for Session-Based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt to ameliorate the impact of data sparsity in the\ncontext of session-based recommendation. Specifically, we seek to devise a\nmachine learning mechanism capable of extracting subtle and complex underlying\ntemporal dynamics in the observed session data, so as to inform the\nrecommendation algorithm. To this end, we improve upon systems that utilize\ndeep learning techniques with recurrently connected units; we do so by adopting\nconcepts from the field of Bayesian statistics, namely variational inference.\nOur proposed approach consists in treating the network recurrent units as\nstochastic latent variables with a prior distribution imposed over them. On\nthis basis, we proceed to infer corresponding posteriors; these can be used for\nprediction and recommendation generation, in a way that accounts for the\nuncertainty in the available sparse training data. To allow for our approach to\neasily scale to large real-world datasets, we perform inference under an\napproximate amortized variational inference (AVI) setup, whereby the learned\nposteriors are parameterized via (conventional) neural networks. We perform an\nextensive experimental evaluation of our approach using challenging benchmark\ndatasets, and illustrate its superiority over existing state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 12:35:56 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Chatzis", "Sotirios", ""], ["Christodoulou", "Panayiotis", ""], ["Andreou", "Andreas S.", ""]]}, {"id": "1706.04148", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Alexandros Karatzoglou, Bal\\'azs Hidasi and Paolo\n  Cremonesi", "title": "Personalizing Session-based Recommendations with Hierarchical Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3109859.3109896", "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:33:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 08:02:51 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 16:52:18 GMT"}, {"version": "v4", "created": "Fri, 14 Jul 2017 16:42:43 GMT"}, {"version": "v5", "created": "Wed, 23 Aug 2017 18:42:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Quadrana", "Massimo", ""], ["Karatzoglou", "Alexandros", ""], ["Hidasi", "Bal\u00e1zs", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1706.04206", "submitter": "Hossein Hematialam", "authors": "Hossein Hematialam, Wlodek Zadrozny", "title": "Identifying Condition-Action Statements in Medical Guidelines Using\n  Domain-Independent Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advances the state of the art in text understanding of medical\nguidelines by releasing two new annotated clinical guidelines datasets, and\nestablishing baselines for using machine learning to extract condition-action\npairs. In contrast to prior work that relies on manually created rules, we\nreport experiment with several supervised machine learning techniques to\nclassify sentences as to whether they express conditions and actions. We show\nthe limitations and possible extensions of this work on text mining of medical\nguidelines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 18:02:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 18:35:26 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Hematialam", "Hossein", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1706.04453", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Lina Yao, Xiwei Xu, Sen Wang, Liming Zhu", "title": "Hybrid Collaborative Recommendation via Semi-AutoEncoder", "comments": "9 pages, ICONIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel structure, Semi-AutoEncoder, based on\nAutoEncoder. We generalize it into a hybrid collaborative filtering model for\nrating prediction as well as personalized top-n recommendations. Experimental\nresults on two real-world datasets demonstrate its state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 12:47:36 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 06:58:38 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhang", "Shuai", ""], ["Yao", "Lina", ""], ["Xu", "Xiwei", ""], ["Wang", "Sen", ""], ["Zhu", "Liming", ""]]}, {"id": "1706.04524", "submitter": "Julia Kiseleva", "authors": "Julia Kiseleva and Maarten de Rijke", "title": "Evaluating Personal Assistants on Mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iPhone was introduced only a decade ago in 2007 but has fundamentally\nchanged the way we interact with online information. Mobile devices differ\nradically from classic command-based and point-and-click user interfaces, now\nallowing for gesture-based interaction using fine-grained touch and swipe\nsignals. Due to the rapid growth in the use of voice-controlled intelligent\npersonal assistants on mobile devices, such as Microsoft's Cortana, Google Now,\nand Apple's Siri, mobile devices have become personal, allowing us to be online\nall the time, and assist us in any task, both in work and in our daily lives,\nmaking context a crucial factor to consider.\n  Mobile usage is now exceeding desktop usage, and is still growing at a rapid\nrate, yet our main ways of training and evaluating personal assistants are\nstill based on (and framed in) classical desktop interactions, focusing on\nexplicit queries, clicks, and dwell time spent. However, modern user\ninteraction with mobile devices is radically different due to touch screens\nwith a gesture- and voice-based control and the varying context of use, e.g.,\nin a car, by bike, often invalidating the assumptions underlying today's user\nsatisfaction evaluation.\n  There is an urgent need to understand voice- and gesture-based interaction,\ntaking all interaction signals and context into account in appropriate ways. We\npropose a research agenda for developing methods to evaluate and improve\ncontext-aware user satisfaction with mobile interactions using gesture-based\nsignals at scale.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:57:18 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1706.04922", "submitter": "Gia-Hung Nguyen", "authors": "Gia-Hung Nguyen, Laure Soulier, Lynda Tamine, Nathalie Bricon-Souf", "title": "DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge\n  Resource Driven Representation of Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art solutions to the vocabulary mismatch in information\nretrieval (IR) mainly aim at leveraging either the relational semantics\nprovided by external resources or the distributional semantics, recently\ninvestigated by deep neural approaches. Guided by the intuition that the\nrelational semantics might improve the effectiveness of deep neural approaches,\nwe propose the Deep Semantic Resource Inference Model (DSRIM) that relies on:\n1) a representation of raw-data that models the relational semantics of text by\njointly considering objects and relations expressed in a knowledge resource,\nand 2) an end-to-end neural architecture that learns the query-document\nrelevance by leveraging the distributional and relational semantics of\ndocuments and queries. The experimental evaluation carried out on two TREC\ndatasets from TREC Terabyte and TREC CDS tracks relying respectively on WordNet\nand MeSH resources, indicates that our model outperforms state-of-the-art\nsemantic and deep neural IR models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:24:32 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 12:32:30 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Nguyen", "Gia-Hung", ""], ["Soulier", "Laure", ""], ["Tamine", "Lynda", ""], ["Bricon-Souf", "Nathalie", ""]]}, {"id": "1706.04979", "submitter": "Md. Iqbal Hossain", "authors": "Md Iqbal Hossain and Stephen Kobourov", "title": "Research Topics Map: rtopmap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a system for visualizing and analyzing worldwide\nresearch topics, {\\tt rtopmap}. We gather data from google scholar academic\nresearch profiles, putting together a weighted topics graph, consisting of over\n35,000 nodes and 646,000 edges. The nodes correspond to self-reported research\ntopics, and edges correspond to co-occurring topics in google scholar profiles.\nThe {\\tt rtopmap} system supports zooming/panning/searching and other\ngoogle-maps-based interactive features. With the help of map overlays, we also\nvisualize the strengths and weaknesses of different academic institutions in\nterms of human resources (e.g., number of researchers in different areas), as\nwell as scholarly output (e.g., citation counts in different areas). Finally,\nwe also visualize what parts of the map are associated with different academic\ndepartments, or with specific documents (such as research papers, or calls for\nproposals). The system itself is available at\n\\url{http://rtopmap.arl.arizona.edu/}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:29:53 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Hossain", "Md Iqbal", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1706.05011", "submitter": "Piotr Sapiezynski", "authors": "Anik\\'o Hann\\'ak, Piotr Sapie\\.zy\\'nski, Arash Molavi Khaki, David\n  Lazer, Alan Mislove, Christo Wilson", "title": "Measuring Personalization of Web Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search is an integral part of our daily lives. Recently, there has been a\ntrend of personalization in Web search, where different users receive different\nresults for the same search query. The increasing level of personalization is\nleading to concerns about Filter Bubble effects, where certain users are simply\nunable to access information that the search engines' algorithm decides is\nirrelevant. Despite these concerns, there has been little quantification of the\nextent of personalization in Web search today, or the user attributes that\ncause it.\n  In light of this situation, we make three contributions. First, we develop a\nmethodology for measuring personalization in Web search results. While\nconceptually simple, there are numerous details that our methodology must\nhandle in order to accurately attribute differences in search results to\npersonalization. Second, we apply our methodology to 200 users on Google Web\nSearch and 100 users on Bing. We find that, on average, 11.7% of results show\ndifferences due to personalization on Google, while 15.8% of results are\npersonalized on Bing, but that this varies widely by search query and by result\nranking. Third, we investigate the user features used to personalize on Google\nWeb Search and Bing. Surprisingly, we only find measurable personalization as a\nresult of searching with a logged in account and the IP address of the\nsearching user. Our results are a first step towards understanding the extent\nand effects of personalization on Web search engines today.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:00:05 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Hann\u00e1k", "Anik\u00f3", ""], ["Sapie\u017cy\u0144ski", "Piotr", ""], ["Khaki", "Arash Molavi", ""], ["Lazer", "David", ""], ["Mislove", "Alan", ""], ["Wilson", "Christo", ""]]}, {"id": "1706.05084", "submitter": "James Wilson", "authors": "Kelsey MacMillan and James D. Wilson", "title": "Topic supervised non-negative matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been extensively used to organize and interpret the\ncontents of large, unstructured corpora of text documents. Although topic\nmodels often perform well on traditional training vs. test set evaluations, it\nis often the case that the results of a topic model do not align with human\ninterpretation. This interpretability fallacy is largely due to the\nunsupervised nature of topic models, which prohibits any user guidance on the\nresults of a model. In this paper, we introduce a semi-supervised method called\ntopic supervised non-negative matrix factorization (TS-NMF) that enables the\nuser to provide labeled example documents to promote the discovery of more\nmeaningful semantic structure of a corpus. In this way, the results of TS-NMF\nbetter match the intuition and desired labeling of the user. The core of TS-NMF\nrelies on solving a non-convex optimization problem for which we derive an\niterative algorithm that is shown to be monotonic and convergent to a local\noptimum. We demonstrate the practical utility of TS-NMF on the Reuters and\nPubMed corpora, and find that TS-NMF is especially useful for conceptual or\nbroad topics, where topic key terms are not well understood. Although\nidentifying an optimal latent structure for the data is not a primary objective\nof the proposed approach, we find that TS-NMF achieves higher weighted Jaccard\nsimilarity scores than the contemporary methods, (unsupervised) NMF and latent\nDirichlet allocation, at supervision rates as low as 10% to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:20:04 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 16:00:27 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["MacMillan", "Kelsey", ""], ["Wilson", "James D.", ""]]}, {"id": "1706.05122", "submitter": "Takuma Yoneda", "authors": "Takuma Yoneda and Koki Mori and Makoto Miwa and Yutaka Sasaki", "title": "Bib2vec: An Embedding-based Search System for Bibliographic Information", "comments": "EACL2017 extended version. The demonstration is available at\n  http://tti-coin.jp/demo/bib2vec/", "journal-ref": "Proceedings of the EACL 2017 Software Demonstrations, Valencia,\n  Spain, April 3-7 2017, pages 112-115", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel embedding model that represents relationships among\nseveral elements in bibliographic information with high representation ability\nand flexibility. Based on this model, we present a novel search system that\nshows the relationships among the elements in the ACL Anthology Reference\nCorpus. The evaluation results show that our model can achieve a high\nprediction ability and produce reasonable search results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 00:53:28 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 16:33:20 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 09:19:57 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Yoneda", "Takuma", ""], ["Mori", "Koki", ""], ["Miwa", "Makoto", ""], ["Sasaki", "Yutaka", ""]]}, {"id": "1706.05361", "submitter": "Dhanasekar Sundararaman", "authors": "Dhanasekar Sundararaman, Sudharshan Srinivasan", "title": "Twigraph: Discovering and Visualizing Influential Words between Twitter\n  Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media craze is on an ever increasing spree, and people are\nconnected with each other like never before, but these vast connections are\nvisually unexplored. We propose a methodology Twigraph to explore the\nconnections between persons using their Twitter profiles. First, we propose a\nhybrid approach of recommending social media profiles, articles, and\nadvertisements to a user.The profiles are recommended based on the similarity\nscore between the user profile, and profile under evaluation. The similarity\nbetween a set of profiles is investigated by finding the top influential words\nthus causing a high similarity through an Influence Term Metric for each word.\nThen, we group profiles of various domains such as politics, sports, and\nentertainment based on the similarity score through a novel clustering\nalgorithm. The connectivity between profiles is envisaged using word graphs\nthat help in finding the words that connect a set of profiles and the profiles\nthat are connected to a word. Finally, we analyze the top influential words\nover a set of profiles through clustering by finding the similarity of that\nprofiles enabling to break down a Twitter profile with a lot of followers to\nfine level word connections using word graphs. The proposed method was\nimplemented on datasets comprising 1.1 M Tweets obtained from Twitter.\nExperimental results show that the resultant influential words were highly\nrepresentative of the relationship between two profiles or a set of profiles\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 17:33:07 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 13:28:26 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Sundararaman", "Dhanasekar", ""], ["Srinivasan", "Sudharshan", ""]]}, {"id": "1706.05549", "submitter": "Andrey Ignatov", "authors": "Liliya Akhtyamova, Andrey Ignatov, John Cardiff", "title": "A Large-Scale CNN Ensemble for Medication Safety Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Revealing Adverse Drug Reactions (ADR) is an essential part of post-marketing\ndrug surveillance, and data from health-related forums and medical communities\ncan be of a great significance for estimating such effects. In this paper, we\npropose an end-to-end CNN-based method for predicting drug safety on user\ncomments from healthcare discussion forums. We present an architecture that is\nbased on a vast ensemble of CNNs with varied structural parameters, where the\nprediction is determined by the majority vote. To evaluate the performance of\nthe proposed solution, we present a large-scale dataset collected from a\nmedical website that consists of over 50 thousand reviews for more than 4000\ndrugs. The results demonstrate that our model significantly outperforms\nconventional approaches and predicts medicine safety with an accuracy of 87.17%\nfor binary and 62.88% for multi-classification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 15:06:58 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Akhtyamova", "Liliya", ""], ["Ignatov", "Andrey", ""], ["Cardiff", "John", ""]]}, {"id": "1706.05719", "submitter": "Thomas Krause", "authors": "Thomas Krause", "title": "Towards the Improvement of Automated Scientific Document Categorization\n  by Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This master thesis describes an algorithm for automated categorization of\nscientific documents using deep learning techniques and compares the results to\nthe results of existing classification algorithms. As an additional goal a\nreusable API is to be developed allowing the automation of classification tasks\nin existing software. A design will be proposed using a convolutional neural\nnetwork as a classifier and integrating this into a REST based API. This is\nthen used as the basis for an actual proof of concept implementation presented\nas well in this thesis. It will be shown that the deep learning classifier\nprovides very good result in the context of multi-class document categorization\nand that it is feasible to integrate such classifiers into a larger ecosystem\nusing REST based services.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:29:15 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Krause", "Thomas", ""]]}, {"id": "1706.05730", "submitter": "Ivica Obadi\\'c", "authors": "Ivica Obadi\\'c, Gjorgji Madjarov (1), Ivica Dimitrovski (1), Dejan\n  Gjorgjevikj (1) ((1) Faculty of Computer Science and Engineering, Ss. Cyril\n  and Methodius University, Skopje, Macedonia)", "title": "Addressing Item-Cold Start Problem in Recommendation Systems using Model\n  Based Approach and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommendation systems rely on past usage data in order to\ngenerate new recommendations. Those approaches fail to generate sensible\nrecommendations for new users and items into the system due to missing\ninformation about their past interactions. In this paper, we propose a solution\nfor successfully addressing item-cold start problem which uses model-based\napproach and recent advances in deep learning. In particular, we use latent\nfactor model for recommendation, and predict the latent factors from item's\ndescriptions using convolutional neural network when they cannot be obtained\nfrom usage data. Latent factors obtained by applying matrix factorization to\nthe available usage data are used as ground truth to train the convolutional\nneural network. To create latent factor representations for the new items, the\nconvolutional neural network uses their textual description. The results from\nthe experiments reveal that the proposed approach significantly outperforms\nseveral baseline estimators.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 21:51:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Obadi\u0107", "Ivica", ""], ["Madjarov", "Gjorgji", ""], ["Dimitrovski", "Ivica", ""], ["Gjorgjevikj", "Dejan", ""]]}, {"id": "1706.05786", "submitter": "Denis Parra", "authors": "Pablo Messina and Vicente Dominguez and Denis Parra and Christoph\n  Trattner and Alvaro Soto", "title": "Exploring Content-based Artwork Recommendation with Metadata and Visual\n  Features", "comments": "1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to other areas, artwork recommendation has received little\nattention, despite the continuous growth of the artwork market. Previous\nresearch has relied on ratings and metadata to make artwork recommendations, as\nwell as visual features extracted with deep neural networks (DNN). However,\nthese features have no direct interpretation to explicit visual features (e.g.\nbrightness, texture) which might hinder explainability and user-acceptance. In\nthis work, we study the impact of artwork metadata as well as visual features\n(DNN-based and attractiveness-based) for physical artwork recommendation, using\nimages and transaction data from the UGallery online artwork store.\n  Our results indicate that: (i) visual features perform better than manually\ncurated data, (ii) DNN-based visual features perform better than\nattractiveness-based ones, and (iii) a hybrid approach improves the performance\nfurther. Our research can inform the development of new artwork recommenders\nrelying on diverse content data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 04:55:10 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 05:02:04 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 19:23:14 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Messina", "Pablo", ""], ["Dominguez", "Vicente", ""], ["Parra", "Denis", ""], ["Trattner", "Christoph", ""], ["Soto", "Alvaro", ""]]}, {"id": "1706.05826", "submitter": "Di Wang", "authors": "Di Wang, Kimon Fountoulakis, Monika Henzinger, Michael W. Mahoney,\n  Satish Rao", "title": "Capacity Releasing Diffusion for Speed and Locality", "comments": "Appeared in ICML 2017. Current version added reference and discussion\n  of work on generalized Cheeger's inequalities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusions and related random walk procedures are of central importance in\nmany areas of machine learning, data analysis, and applied mathematics. Because\nthey spread mass agnostically at each step in an iterative manner, they can\nsometimes spread mass \"too aggressively,\" thereby failing to find the \"right\"\nclusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,\nwhich is both faster and stays more local than the classical spectral diffusion\nprocess. As an application, we use our CRD Process to develop an improved local\nalgorithm for graph clustering. Our local graph clustering method can find\nlocal clusters in a model of clustering where one begins the CRD Process in a\ncluster whose vertices are connected better internally than externally by an\n$O(\\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,\nour CRD Process is the first local graph clustering algorithm that is not\nsubject to the well-known quadratic Cheeger barrier. Our result requires a\ncertain smoothness condition, which we expect to be an artifact of our\nanalysis. Our empirical evaluation demonstrates improved results, in particular\nfor realistic social graphs where there are moderately good---but not very\ngood---clusters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 08:18:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 08:58:07 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Wang", "Di", ""], ["Fountoulakis", "Kimon", ""], ["Henzinger", "Monika", ""], ["Mahoney", "Michael W.", ""], ["Rao", "Satish", ""]]}, {"id": "1706.05985", "submitter": "Ayush Singhal", "authors": "Ayush Singhal, Ravindra Kasturi, Ankit Sharma and Jaideep Srivastava", "title": "Leveraging web resources for keyword assignment to short text documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assigning relevant keywords to documents is very important for efficient\nretrieval, clustering and management of the documents. Especially with the web\ncorpus deluged with digital documents, automation of this task is of prime\nimportance. Keyword assignment is a broad topic of research which refers to\ntagging of document with keywords, key-phrases or topics. For text documents,\nthe keyword assignment techniques have been developed under two sub-topics:\nautomatic keyword extraction (AKE) and automatic key-phrase abstraction.\nHowever, the approaches developed in the literature for full text documents\ncannot be used to assign keywords to low text content documents like twitter\nfeeds, news clips, product reviews or even short scholarly text. In this work,\nwe point out several practical challenges encountered in tagging such low text\ncontent documents. As a solution to these challenges, we show that the proposed\napproaches which leverage knowledge from several open source web resources\nenhance the quality of the tags (keywords) assigned to the low text content\ndocuments. The performance of the proposed approach is tested on real world\ncorpus consisting of scholarly documents with text content ranging from only\nthe text in the title of the document (5-10 words) to the summary text/abstract\n(100- 150 words). We find that the proposed approach not just improves the\naccuracy of keyword assignment but offer a computationally efficient solution\nwhich can be used in real world applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:29:19 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Singhal", "Ayush", ""], ["Kasturi", "Ravindra", ""], ["Sharma", "Ankit", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1706.05995", "submitter": "Zaenal Akbar", "authors": "Zaenal Akbar, Elias K\\\"arle, Oleksandra Panasiuk, Umutcan\n  \\c{S}im\\c{s}ek, Ioan Toma, Dieter Fensel", "title": "Complete Semantics to empower Touristic Service Providers", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tourism industry has a significant impact on the world's economy,\ncontributes 10.2% of the world's gross domestic product in 2016. It becomes a\nvery competitive industry, where having a strong online presence is an\nessential aspect for business success. To achieve this goal, the proper usage\nof latest Web technologies, particularly schema.org annotations is crucial. In\nthis paper, we present our effort to improve the online visibility of touristic\nservice providers in the region of Tyrol, Austria, by creating and deploying a\nsubstantial amount of semantic annotations according to schema.org, a widely\nused vocabulary for structured data on the Web. We started our work from\nTourismusverband (TVB) Mayrhofen-Hippach and all touristic service providers in\nthe Mayrhofen-Hippach region and applied the same approach to other TVBs and\nregions, as well as other use cases. The rationale for doing this is\nstraightforward. Having schema.org annotations enables search engines to\nunderstand the content better, and provide better results for end users, as\nwell as enables various intelligent applications to utilize them. As a direct\nconsequence, the region of Tyrol and its touristic service increase their\nonline visibility and decrease the dependency on intermediaries, i.e. Online\nTravel Agency (OTA).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:55:23 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 08:52:06 GMT"}, {"version": "v3", "created": "Fri, 15 Sep 2017 08:32:11 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Akbar", "Zaenal", ""], ["K\u00e4rle", "Elias", ""], ["Panasiuk", "Oleksandra", ""], ["\u015eim\u015fek", "Umutcan", ""], ["Toma", "Ioan", ""], ["Fensel", "Dieter", ""]]}, {"id": "1706.06026", "submitter": "Alessia Amelio Dr.", "authors": "Alessia Amelio and Darko Brodi\\'c", "title": "The $\\mathcal{E}$-Average Common Submatrix: Approximate Searching in a\n  Restricted Neighborhood", "comments": "4 pages, 18th International Workshop on Combinatorial Image Analysis\n  (IWCIA 2017), Short Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new (dis)similarity measure for 2D arrays, extending\nthe Average Common Submatrix measure. This is accomplished by: (i) considering\nthe frequency of matching patterns, (ii) restricting the pattern matching to a\nfixed-size neighborhood, and (iii) computing a distance-based approximate\nmatching. This will achieve better performances with low execution time and\nlarger information retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:53:07 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Amelio", "Alessia", ""], ["Brodi\u0107", "Darko", ""]]}, {"id": "1706.06064", "submitter": "Wengang Zhou", "authors": "Wengang Zhou, Houqiang Li, and Qi Tian", "title": "Recent Advance in Content-based Image Retrieval: A Literature Survey", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive increase and ubiquitous accessibility of visual data on the Web\nhave led to the prosperity of research activity in image search or retrieval.\nWith the ignorance of visual content as a ranking clue, methods with text\nsearch techniques for visual retrieval may suffer inconsistency between the\ntext words and visual content. Content-based image retrieval (CBIR), which\nmakes use of the representation of visual content to identify relevant images,\nhas attracted sustained attention in recent two decades. Such a problem is\nchallenging due to the intention gap and the semantic gap problems. Numerous\ntechniques have been developed for content-based image retrieval in the last\ndecade. The purpose of this paper is to categorize and evaluate those\nalgorithms proposed during the period of 2003 to 2016. We conclude with several\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:14:48 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 08:20:19 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Tian", "Qi", ""]]}, {"id": "1706.06239", "submitter": "Hao Wang", "authors": "Hao Wang, Yanmei Fu, Qinyong Wang, Hongzhi Yin, Changying Du, Hui\n  Xiong", "title": "A Location-Sentiment-Aware Recommender System for Both Home-Town and\n  Out-of-Town Users", "comments": "Accepted by KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial item recommendation has become an important means to help people\ndiscover interesting locations, especially when people pay a visit to\nunfamiliar regions. Some current researches are focusing on modelling\nindividual and collective geographical preferences for spatial item\nrecommendation based on users' check-in records, but they fail to explore the\nphenomenon of user interest drift across geographical regions, i.e., users\nwould show different interests when they travel to different regions. Besides,\nthey ignore the influence of public comments for subsequent users' check-in\nbehaviors. Specifically, it is intuitive that users would refuse to check in to\na spatial item whose historical reviews seem negative overall, even though it\nmight fit their interests. Therefore, it is necessary to recommend the right\nitem to the right user at the right location. In this paper, we propose a\nlatent probabilistic generative model called LSARS to mimic the decision-making\nprocess of users' check-in activities both in home-town and out-of-town\nscenarios by adapting to user interest drift and crowd sentiments, which can\nlearn location-aware and sentiment-aware individual interests from the contents\nof spatial items and user reviews. Due to the sparsity of user activities in\nout-of-town regions, LSARS is further designed to incorporate the public\npreferences learned from local users' check-in behaviors. Finally, we deploy\nLSARS into two practical application scenes: spatial item recommendation and\ntarget user discovery. Extensive experiments on two large-scale location-based\nsocial networks (LBSNs) datasets show that LSARS achieves better performance\nthan existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 01:54:01 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Wang", "Hao", ""], ["Fu", "Yanmei", ""], ["Wang", "Qinyong", ""], ["Yin", "Hongzhi", ""], ["Du", "Changying", ""], ["Xiong", "Hui", ""]]}, {"id": "1706.06291", "submitter": "Denis Parra", "authors": "Gabriel Sepulveda, Vicente Dominguez and Denis Parra", "title": "pyRecLab: A Software Library for Quick Prototyping of Recommender\n  Systems", "comments": "2 pages, poster submited to RecSys 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces pyRecLab, a software library written in C++ with Python\nbindings which allows to quickly train, test and develop recommender systems.\nAlthough there are several software libraries for this purpose, only a few let\ndevelopers to get quickly started with the most traditional methods, permitting\nthem to try different parameters and approach several tasks without a\nsignificant loss of performance. Among the few libraries that have all these\nfeatures, they are available in languages such as Java, Scala or C#, what is a\ndisadvantage for less experienced programmers more used to the popular Python\nprogramming language. In this article we introduce details of pyRecLab, showing\nas well performance analysis in terms of error metrics (MAE and RMSE) and\ntrain/test time. We benchmark it against the popular Java-based library LibRec,\nshowing similar results. We expect programmers with little experience and\npeople interested in quickly prototyping recommender systems to be benefited\nfrom pyRecLab.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:21:59 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:07:20 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Sepulveda", "Gabriel", ""], ["Dominguez", "Vicente", ""], ["Parra", "Denis", ""]]}, {"id": "1706.06314", "submitter": "Qiang Liu", "authors": "Qiang Liu, Feng Yu, Shu Wu, Liang Wang", "title": "Mining Significant Microblogs for Misinformation Identification: An\n  Attention-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of social media, massive misinformation is also\nspreading widely on social media, such as microblog, and bring negative effects\nto human life. Nowadays, automatic misinformation identification has drawn\nattention from academic and industrial communities. For an event on social\nmedia usually consists of multiple microblogs, current methods are mainly based\non global statistical features. However, information on social media is full of\nnoisy and outliers, which should be alleviated. Moreover, most of microblogs\nabout an event have little contribution to the identification of\nmisinformation, where useful information can be easily overwhelmed by useless\ninformation. Thus, it is important to mine significant microblogs for a\nreliable misinformation identification method. In this paper, we propose an\nAttention-based approach for Identification of Misinformation (AIM). Based on\nthe attention mechanism, AIM can select microblogs with largest attention\nvalues for misinformation identification. The attention mechanism in AIM\ncontains two parts: content attention and dynamic attention. Content attention\nis calculated based textual features of each microblog. Dynamic attention is\nrelated to the time interval between the posting time of a microblog and the\nbeginning of the event. To evaluate AIM, we conduct a series of experiments on\nthe Weibo dataset and the Twitter dataset, and the experimental results show\nthat the proposed AIM model outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 08:36:56 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Liu", "Qiang", ""], ["Yu", "Feng", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "1706.06368", "submitter": "Carlos Castillo", "authors": "Meike Zehlike, Francesco Bonchi, Carlos Castillo, Sara Hajian, Mohamed\n  Megahed, Ricardo Baeza-Yates", "title": "FA*IR: A Fair Top-k Ranking Algorithm", "comments": "In Proceedings of the 26th ACM International Conference on\n  Information and Knowledge Management (CIKM'17). This version corrects an\n  error on Table 4", "journal-ref": null, "doi": "10.1145/3132847.3132938", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we define and solve the Fair Top-k Ranking problem, in which we\nwant to determine a subset of k candidates from a large pool of n >> k\ncandidates, maximizing utility (i.e., select the \"best\" candidates) subject to\ngroup fairness criteria. Our ranked group fairness definition extends group\nfairness using the standard notion of protected groups and is based on ensuring\nthat the proportion of protected candidates in every prefix of the top-k\nranking remains statistically above or indistinguishable from a given minimum.\n  Utility is operationalized in two ways: (i) every candidate included in the\ntop-$k$ should be more qualified than every candidate not included; and (ii)\nfor every pair of candidates in the top-k, the more qualified candidate should\nbe ranked above. An efficient algorithm is presented for producing the Fair\nTop-k Ranking, and tested experimentally on existing datasets as well as new\ndatasets released with this paper, showing that our approach yields small\ndistortions with respect to rankings that maximize utility without considering\nfairness criteria.\n  To the best of our knowledge, this is the first algorithm grounded in\nstatistical tests that can mitigate biases in the representation of an\nunder-represented group along a ranked list.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 11:20:12 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 10:45:32 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 12:58:10 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zehlike", "Meike", ""], ["Bonchi", "Francesco", ""], ["Castillo", "Carlos", ""], ["Hajian", "Sara", ""], ["Megahed", "Mohamed", ""], ["Baeza-Yates", "Ricardo", ""]]}, {"id": "1706.06384", "submitter": "Umutcan \\c{S}im\\c{s}ek", "authors": "Umutcan \\c{S}im\\c{s}ek, Elias K\\\"arle, Omar Holzknecht, Dieter Fensel", "title": "Domain Specific Semantic Validation of Schema.org Annotations", "comments": "Accepted to PSI 2017 Conference in Moscow, Russia 13 pages, 4\n  figures, 3 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its unveiling in 2011, schema.org has become the de facto standard for\npublishing semantically described structured data on the web, typically in the\nform of web page annotations. The increasing adoption of schema.org facilitates\nthe growth of the web of data, as well as the development of automated agents\nthat operate on this data. Schema.org is a large heterogeneous vocabulary that\ncovers many domains. This is obviously not a bug, but a feature, since\nschema.org aims to describe almost everything on the web, and the web is huge.\nHowever, the heterogeneity of schema.org may cause a side effect, which is the\nchallenge of picking the right classes and properties for an annotation in a\ncertain domain, as well as keeping the annotation semantically consistent. In\nthis work, we introduce our rule based approach and an implementation of it for\nvalidating schema.org annotations from two aspects: (a) the completeness of the\nannotations in terms of a specified domain, (b) the semantic consistency of the\nvalues based on pre-defined rules. We demonstrate our approach in the tourism\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 12:24:12 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 09:57:27 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["\u015eim\u015fek", "Umutcan", ""], ["K\u00e4rle", "Elias", ""], ["Holzknecht", "Omar", ""], ["Fensel", "Dieter", ""]]}, {"id": "1706.06410", "submitter": "Zeljko Carevic", "authors": "Zeljko Carevic, Maria Lusky, Wilko van Hoek and Philipp Mayr", "title": "Investigating Exploratory Search Activities based on the Stratagem Level\n  in Digital Libraries", "comments": null, "journal-ref": null, "doi": "10.1007/s00799-017-0226-6", "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the results of a user study on exploratory search\nactivities in a social science digital library. We conducted a user study with\n32 participants with a social sciences background -- 16 postdoctoral\nresearchers and 16 students -- who were asked to solve a task on searching\nrelated work to a given topic. The exploratory search task was performed in a\n10-minutes time slot. The use of certain search activities is measured and\ncompared to gaze data recorded with an eye tracking device. We use a novel tree\ngraph representation to visualise the users' search patterns and introduce a\nway to combine multiple search session trees. The tree graph representation is\ncapable to create one single tree for multiple users and to identify common\nsearch patterns. In addition, the information behaviour of students and\npostdoctoral researchers is being compared. The results show that search\nactivities on the stratagem level are frequently utilised by both user groups.\nThe most heavily used search activities were keyword search, followed by\nbrowsing through references and citations, and author searching. The eye\ntracking results showed an intense examination of documents metadata,\nespecially on the level of citations and references. When comparing the group\nof students and postdoctoral researchers we found significant differences\nregarding gaze data on the area of the journal name of the seed document. In\ngeneral, we found a tendency of the postdoctoral researchers to examine the\nmetadata records more intensively with regards to dwell time and the number of\nfixations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:20:49 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Carevic", "Zeljko", ""], ["Lusky", "Maria", ""], ["van Hoek", "Wilko", ""], ["Mayr", "Philipp", ""]]}, {"id": "1706.06613", "submitter": "Chenyan Xiong", "authors": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell\n  Power", "title": "End-to-End Neural Ad-hoc Ranking with Kernel Pooling", "comments": null, "journal-ref": null, "doi": "10.1145/3077136.3080809", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes K-NRM, a kernel based neural model for document ranking.\nGiven a query and a set of documents, K-NRM uses a translation matrix that\nmodels word-level similarities via word embeddings, a new kernel-pooling\ntechnique that uses kernels to extract multi-level soft match features, and a\nlearning-to-rank layer that combines those features into the final ranking\nscore. The whole model is trained end-to-end. The ranking layer learns desired\nfeature patterns from the pairwise ranking loss. The kernels transfer the\nfeature patterns into soft-match targets at each similarity level and enforce\nthem on the translation matrix. The word embeddings are tuned accordingly so\nthat they can produce the desired soft matches. Experiments on a commercial\nsearch engine's query log demonstrate the improvements of K-NRM over prior\nfeature-based and neural-based states-of-the-art, and explain the source of\nK-NRM's advantage: Its kernel-guided embedding encodes a similarity metric\ntailored for matching query words to document words, and provides effective\nmulti-level soft matches.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:19:54 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Xiong", "Chenyan", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""], ["Liu", "Zhiyuan", ""], ["Power", "Russell", ""]]}, {"id": "1706.06636", "submitter": "Chenyan Xiong", "authors": "Chenyan Xiong, Jamie Callan, and Tie-Yan Liu", "title": "Word-Entity Duet Representations for Document Ranking", "comments": null, "journal-ref": "SIGIR 2017", "doi": "10.1145/3077136.3080768", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a word-entity duet framework for utilizing knowledge\nbases in ad-hoc retrieval. In this work, the query and documents are modeled by\nword-based representations and entity-based representations. Ranking features\nare generated by the interactions between the two representations,\nincorporating information from the word space, the entity space, and the\ncross-space connections through the knowledge graph. To handle the\nuncertainties from the automatically constructed entity representations, an\nattention-based ranking model AttR-Duet is developed. With back-propagation\nfrom ranking labels, the model learns simultaneously how to demote noisy\nentities and how to rank documents with the word-entity duet. Evaluation\nresults on TREC Web Track ad-hoc task demonstrate that all of the four-way\ninteractions in the duet are useful, the attention mechanism successfully\nsteers the model away from noisy entities, and together they significantly\noutperform both word-based and entity-based learning to rank systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:23:07 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Xiong", "Chenyan", ""], ["Callan", "Jamie", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1706.06701", "submitter": "Denis Parra", "authors": "Felipe del-Rio and Denis Parra and Jovan Kuzmicic and Erick Svec", "title": "Towards a Recommender System for Undergraduate Research", "comments": "3 pages, submitted to RecSys 2017 posters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies indicate that attracting students to research careers\nrequires to engage them from early undergraduate years. Following this\nparadigm, our Engineering School has developed an undergraduate research\nprogram that allows students to enroll in research in exchange for course\ncredits. Moreover, we developed a web portal to inform students about the\nprogram and the opportunities, but participation remains lower than expected.\nIn order to promote student engagement, we attempt to build a personalized\nrecommender system of research opportunities to undergraduates. With this goal\nin mind we investigate two tasks. First, one that identifies students that are\nmore willing to participate on this kind of program. A second task is\ngenerating a personalized list of recommendations of research opportunities for\neach student. To evaluate our approach, we perform a simulated prediction\nexperiment with data from our School, which has more than 4,000 active\nundergraduate students nowadays. Our results indicate that there is a big\npotential to create a personalized recommender system for this purpose. Our\nresults can be used as a baseline for colleges seeking strategies to encourage\nresearch activities within undergraduate students.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:17:16 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["del-Rio", "Felipe", ""], ["Parra", "Denis", ""], ["Kuzmicic", "Jovan", ""], ["Svec", "Erick", ""]]}, {"id": "1706.06716", "submitter": "Chanyoung Park", "authors": "Chanyoung Park, Donghyun Kim, Min-Chul Yang, Jung-Tae Lee, Hwanjo Yu", "title": "Click-aware purchase prediction with push at the top", "comments": "For the final published journal version, see\n  https://doi.org/10.1016/j.ins.2020.02.062", "journal-ref": "Information Sciences 521 (2020): 350-364", "doi": "10.1016/j.ins.2020.02.062", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting user preferences from purchase records for performing purchase\nprediction is challenging because negative feedback is not explicitly observed,\nand because treating all non-purchased items equally as negative feedback is\nunrealistic. Therefore, in this study, we present a framework that leverages\nthe past click records of users to compensate for the missing user-item\ninteractions of purchase records, i.e., non-purchased items. We begin by\nformulating various model assumptions, each one assuming a different order of\nuser preferences among purchased, clicked-but-not-purchased, and non-clicked\nitems, to study the usefulness of leveraging click records. We implement the\nmodel assumptions using the Bayesian personalized ranking model, which\nmaximizes the area under the curve for bipartite ranking. However, we argue\nthat using click records for bipartite ranking needs a meticulously designed\nmodel because of the relative unreliableness of click records compared with\nthat of purchase records. Therefore, we ultimately propose a novel\nlearning-to-rank method, called P3Stop, for performing purchase prediction. The\nproposed model is customized to be robust to relatively unreliable click\nrecords by particularly focusing on the accuracy of top-ranked items.\nExperimental results on two real-world e-commerce datasets demonstrate that\nP3STop considerably outperforms the state-of-the-art implicit-feedback-based\nrecommendation methods, especially for top-ranked items.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 01:22:57 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 07:09:39 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 02:41:18 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Park", "Chanyoung", ""], ["Kim", "Donghyun", ""], ["Yang", "Min-Chul", ""], ["Lee", "Jung-Tae", ""], ["Yu", "Hwanjo", ""]]}, {"id": "1706.06836", "submitter": "Mandy Neumann", "authors": "Philipp Schaer and Mandy Neumann", "title": "Enriching Existing Test Collections with OXPath", "comments": "Experimental IR Meets Multilinguality, Multimodality, and Interaction\n  - 8th International Conference of the CLEF Association, CLEF 2017, Dublin,\n  Ireland, September 11-14, 2017", "journal-ref": null, "doi": "10.1007/978-3-319-65813-1_16", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending TREC-style test collections by incorporating external resources is\na time consuming and challenging task. Making use of freely available web data\nrequires technical skills to work with APIs or to create a web scraping program\nspecifically tailored to the task at hand. We present a light-weight\nalternative that employs the web data extraction language OXPath to harvest\ndata to be added to an existing test collection from web resources. We\ndemonstrate this by creating an extended version of GIRT4 called GIRT4-XT with\nadditional metadata fields harvested via OXPath from the social sciences portal\nSowiport. This allows the re-use of this collection for other evaluation\npurposes like bibliometrics-enhanced retrieval. The demonstrated method can be\napplied to a variety of similar scenarios and is not limited to extending\nexisting collections but can also be used to create completely new ones with\nlittle effort.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 11:43:39 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Schaer", "Philipp", ""], ["Neumann", "Mandy", ""]]}, {"id": "1706.07202", "submitter": "Philipp Schaer", "authors": "Philipp Schaer", "title": "Living Labs - An Ethical Challenge for Researchers and Platform\n  Providers", "comments": "to appear in: Zimmer, M., & Kinder-Kurlanda, K. (Eds.): Internet\n  Research Ethics for the Social Age: New Challenges, Cases, and Contexts.\n  Peter Lang (2017)", "journal-ref": null, "doi": "10.3726/b11077", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infamous Facebook emotion contagion experiment is one of the most\nprominent and best-known online experiments based on the concept of what we\nhere call \"living labs\". In these kinds of experiments, real-world applications\nsuch as social web platforms trigger experimental switches inside their system\nto present experimental changes to their users - most of the time without the\nusers being aware of their role as virtual guinea pigs. In the Facebook example\nthe researches changed the way users' personal timeline was compiled to test\nthe influence on the users' moods and feelings. The reactions to these\nexperiments showed the inherent ethical issues such living labs settings bring\nup, mainly the study's lack of informed consent procedures, as well as a more\ngeneral critique of the flaws in the experimental design.\n  In this chapter, we describe additional use cases: The so-called living labs\nthat focus on experimentation with information systems such as search engines\nand wikis and especially on their real-world usage. The living labs paradigm\nallows researchers to conduct research in real-world environments or systems.\nIn the field of information science and especially information retrieval -\nwhich is the scientific discipline that is concerned with the research of\nsearch engines, information systems, and search related algorithms and\ntechniques - it is still common practice to perform in vitro or offline\nevaluations using static test collections. Living labs are widely unknown or\nunavailable to academic researchers in these fields. A main benefit of living\nlabs is their potential to offer new ways and possibilities to experiment with\ninformation systems and especially their users, but on the other hand they\nintroduce a whole set of ethical issues that we would like to address in this\nchapter.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 08:09:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Schaer", "Philipp", ""]]}, {"id": "1706.07276", "submitter": "Bei Shi", "authors": "Bei Shi, Wai Lam, Shoaib Jameel, Steven Schockaert, Kwun Ping Lai", "title": "Jointly Learning Word Embeddings and Latent Topics", "comments": "10 pagess, 2 figures, full paper. To appear in the proceedings of The\n  40th International ACM SIGIR Conference on Research and Development in\n  Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": "10.1145/3077136.3080806", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models such as Skip-gram learn a vector-space representation\nfor each word, based on the local word collocation patterns that are observed\nin a text corpus. Latent topic models, on the other hand, take a more global\nview, looking at the word distributions across the corpus to assign a topic to\neach word occurrence. These two paradigms are complementary in how they\nrepresent the meaning of word occurrences. While some previous works have\nalready looked at using word embeddings for improving the quality of latent\ntopics, and conversely, at using latent topics for improving word embeddings,\nsuch \"two-step\" methods cannot capture the mutual interaction between the two\nparadigms. In this paper, we propose STE, a framework which can learn word\nembeddings and latent topics in a unified manner. STE naturally obtains\ntopic-specific word embeddings, and thus addresses the issue of polysemy. At\nthe same time, it also learns the term distributions of the topics, and the\ntopic distributions of the documents. Our experimental results demonstrate that\nthe STE model can indeed generate useful topic-specific word embeddings and\ncoherent latent topics in an effective and efficient way.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 06:19:24 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Shi", "Bei", ""], ["Lam", "Wai", ""], ["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""], ["Lai", "Kwun Ping", ""]]}, {"id": "1706.07479", "submitter": "Maciej Kula", "authors": "Maciej Kula", "title": "Binary Latent Representations for Efficient Ranking: Empirical\n  Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale recommender systems often face severe latency and storage\nconstraints at prediction time. These are particularly acute when the number of\nitems that could be recommended is large, and calculating predictions for the\nfull set is computationally intensive. In an attempt to relax these\nconstraints, we train recommendation models that use binary rather than\nreal-valued user and item representations, and show that while they are\nsubstantially faster to evaluate, the gains in speed come at a large cost in\naccuracy. In our Movielens 1M experiments, we show that reducing the latent\ndimensionality of traditional models offers a more attractive accuracy/speed\ntrade-off than using binary representations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 20:20:34 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 11:13:43 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kula", "Maciej", ""]]}, {"id": "1706.07506", "submitter": "Massimiliano Ruocco", "authors": "Massimiliano Ruocco, Ole Steinar Lillest{\\o}l Skrede, Helge Langseth", "title": "Inter-Session Modeling for Session-Based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research has been done on applying Recurrent Neural Networks\n(RNNs) as recommender systems. Results have been promising, especially in the\nsession-based setting where RNNs have been shown to outperform state-of-the-art\nmodels. In many of these experiments, the RNN could potentially improve the\nrecommendations by utilizing information about the user's past sessions, in\naddition to its own interactions in the current session. A problem for\nsession-based recommendation, is how to produce accurate recommendations at the\nstart of a session, before the system has learned much about the user's current\ninterests. We propose a novel approach that extends a RNN recommender to be\nable to process the user's recent sessions, in order to improve\nrecommendations. This is done by using a second RNN to learn from recent\nsessions, and predict the user's interest in the current session. By feeding\nthis information to the original RNN, it is able to improve its\nrecommendations. Our experiments on two different datasets show that the\nproposed approach can significantly improve recommendations throughout the\nsessions, compared to a single RNN working only on the current session. The\nproposed model especially improves recommendations at the start of sessions,\nand is therefore able to deal with the cold start problem within sessions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:17:00 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Ruocco", "Massimiliano", ""], ["Skrede", "Ole Steinar Lillest\u00f8l", ""], ["Langseth", "Helge", ""]]}, {"id": "1706.07513", "submitter": "Georgios Alexandridis Dr.", "authors": "Georgios Alexandridis, Georgios Siolas and Andreas Stafylopatis", "title": "ParVecMF: A Paragraph Vector-based Matrix Factorization Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review-based recommender systems have gained noticeable ground in recent\nyears. In addition to the rating scores, those systems are enriched with\ntextual evaluations of items by the users. Neural language processing models,\non the other hand, have already found application in recommender systems,\nmainly as a means of encoding user preference data, with the actual textual\ndescription of items serving only as side information. In this paper, a novel\napproach to incorporating the aforementioned models into the recommendation\nprocess is presented. Initially, a neural language processing model and more\nspecifically the paragraph vector model is used to encode textual user reviews\nof variable length into feature vectors of fixed length. Subsequently this\ninformation is fused along with the rating scores in a probabilistic matrix\nfactorization algorithm, based on maximum a-posteriori estimation. The\nresulting system, ParVecMF, is compared to a ratings' matrix factorization\napproach on a reference dataset. The obtained preliminary results on a set of\ntwo metrics are encouraging and may stimulate further research in this area.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:31:46 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 08:11:36 GMT"}, {"version": "v3", "created": "Wed, 10 Jan 2018 15:56:29 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Alexandridis", "Georgios", ""], ["Siolas", "Georgios", ""], ["Stafylopatis", "Andreas", ""]]}, {"id": "1706.07515", "submitter": "Denis Parra", "authors": "Vicente Dominguez and Pablo Messina and Denis Parra and Domingo Mery\n  and Christoph Trattner and Alvaro Soto", "title": "Comparing Neural and Attractiveness-based Visual Features for Artwork\n  Recommendation", "comments": "DLRS 2017 workshop, co-located at RecSys 2017", "journal-ref": null, "doi": "10.1145/3125486.3125495", "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in image processing and computer vision in the latest years have\nbrought about the use of visual features in artwork recommendation. Recent\nworks have shown that visual features obtained from pre-trained deep neural\nnetworks (DNNs) perform very well for recommending digital art. Other recent\nworks have shown that explicit visual features (EVF) based on attractiveness\ncan perform well in preference prediction tasks, but no previous work has\ncompared DNN features versus specific attractiveness-based visual features\n(e.g. brightness, texture) in terms of recommendation performance. In this\nwork, we study and compare the performance of DNN and EVF features for the\npurpose of physical artwork recommendation using transactional data from\nUGallery, an online store of physical paintings. In addition, we perform an\nexploratory analysis to understand if DNN embedded features have some relation\nwith certain EVF. Our results show that DNN features outperform EVF, that\ncertain EVF features are more suited for physical artwork recommendation and,\nfinally, we show evidence that certain neurons in the DNN might be partially\nencoding visual features such as brightness, providing an opportunity for\nexplaining recommendations based on visual neural models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:48:48 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 22:17:48 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Dominguez", "Vicente", ""], ["Messina", "Pablo", ""], ["Parra", "Denis", ""], ["Mery", "Domingo", ""], ["Trattner", "Christoph", ""], ["Soto", "Alvaro", ""]]}, {"id": "1706.07613", "submitter": "Yann Bayle", "authors": "Yann Bayle and Matthias Robine and Pierre Hanna", "title": "Toward Faultless Content-Based Playlists Generation for Instrumentals", "comments": "single-column 20 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study deals with content-based musical playlists generation focused on\nSongs and Instrumentals. Automatic playlist generation relies on collaborative\nfiltering and autotagging algorithms. Autotagging can solve the cold start\nissue and popularity bias that are critical in music recommender systems.\nHowever, autotagging remains to be improved and cannot generate satisfying\nmusic playlists. In this paper, we suggest improvements toward better\nautotagging-generated playlists compared to state-of-the-art. To assess our\nmethod, we focus on the Song and Instrumental tags. Song and Instrumental are\ntwo objective and opposite tags that are under-studied compared to genres or\nmoods, which are subjective and multi-modal tags. In this paper, we consider an\nindustrial real-world musical database that is unevenly distributed between\nSongs and Instrumentals and bigger than databases used in previous studies. We\nset up three incremental experiments to enhance automatic playlist generation.\nOur suggested approach generates an Instrumental playlist with up to three\ntimes less false positives than cutting edge methods. Moreover, we provide a\ndesign of experiment framework to foster research on Songs and Instrumentals.\nWe give insight on how to improve further the quality of generated playlists\nand to extend our methods to other musical tags. Furthermore, we provide the\nsource code to guarantee reproducible research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 09:27:35 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:05:00 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Bayle", "Yann", ""], ["Robine", "Matthias", ""], ["Hanna", "Pierre", ""]]}, {"id": "1706.07625", "submitter": "Thomas Nedelec", "authors": "Thomas Nedelec, Elena Smirnova, Flavian Vasile", "title": "Specializing Joint Representations for the task of Product\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified product embedded representation that is optimized for\nthe task of retrieval-based product recommendation. To this end, we introduce a\nnew way to fuse modality-specific product embeddings into a joint product\nembedding, in order to leverage both product content information, such as\ntextual descriptions and images, and product collaborative filtering signal. By\nintroducing the fusion step at the very end of our architecture, we are able to\ntrain each modality separately, allowing us to keep a modular architecture that\nis preferable in real-world recommendation deployments. We analyze our\nperformance on normal and hard recommendation setups such as cold-start and\ncross-category recommendations and achieve good performance on a large product\nshopping dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 10:09:59 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 15:58:33 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Nedelec", "Thomas", ""], ["Smirnova", "Elena", ""], ["Vasile", "Flavian", ""]]}, {"id": "1706.07639", "submitter": "Stephen Bonner", "authors": "Stephen Bonner, Flavian Vasile", "title": "Causal Embeddings for Recommendation", "comments": "Accepted as a long paper at the Twelfth ACM Conference on Recommender\n  Systems (RecSys '18), October 2--7, 2018, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3240323.3240360", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current applications use recommendations in order to modify the natural\nuser behavior, such as to increase the number of sales or the time spent on a\nwebsite. This results in a gap between the final recommendation objective and\nthe classical setup where recommendation candidates are evaluated by their\ncoherence with past user behavior, by predicting either the missing entries in\nthe user-item matrix, or the most likely next event. To bridge this gap, we\noptimize a recommendation policy for the task of increasing the desired outcome\nversus the organic user behavior. We show this is equivalent to learning to\npredict recommendation outcomes under a fully random recommendation policy. To\nthis end, we propose a new domain adaptation algorithm that learns from logged\ndata containing outcomes from a biased recommendation policy and predicts\nrecommendation outcomes according to random exposure. We compare our method\nagainst state-of-the-art factorization methods, in addition to new approaches\nof causal recommendation and show significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 11:22:31 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 15:24:30 GMT"}, {"version": "v3", "created": "Thu, 27 Jul 2017 16:22:49 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 08:58:13 GMT"}, {"version": "v5", "created": "Mon, 14 May 2018 12:26:41 GMT"}, {"version": "v6", "created": "Fri, 3 Aug 2018 12:56:13 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bonner", "Stephen", ""], ["Vasile", "Flavian", ""]]}, {"id": "1706.07684", "submitter": "Flavian Vasile", "authors": "Elena Smirnova, Flavian Vasile", "title": "Contextual Sequence Modeling for Recommendation with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations can greatly benefit from good representations of the user\nstate at recommendation time. Recent approaches that leverage Recurrent Neural\nNetworks (RNNs) for session-based recommendations have shown that Deep Learning\nmodels can provide useful user representations for recommendation. However,\ncurrent RNN modeling approaches summarize the user state by only taking into\naccount the sequence of items that the user has interacted with in the past,\nwithout taking into account other essential types of context information such\nas the associated types of user-item interactions, the time gaps between events\nand the time of day for each interaction. To address this, we propose a new\nclass of Contextual Recurrent Neural Networks for Recommendation (CRNNs) that\ncan take into account the contextual information both in the input and output\nlayers and modifying the behavior of the RNN by combining the context embedding\nwith the item embedding and more explicitly, in the model dynamics, by\nparametrizing the hidden unit transitions as a function of context information.\nWe compare our CRNNs approach with RNNs and non-sequential baselines and show\ngood improvements on the next event prediction task.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:22:11 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Smirnova", "Elena", ""], ["Vasile", "Flavian", ""]]}, {"id": "1706.07881", "submitter": "Ting Chen", "authors": "Ting Chen, Yizhou Sun, Yue Shi, Liangjie Hong", "title": "On Sampling Strategies for Neural Network-based Collaborative Filtering", "comments": "This is a longer version (with supplementary attached) of the KDD'17\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks have inspired people to design hybrid\nrecommendation algorithms that can incorporate both (1) user-item interaction\ninformation and (2) content information including image, audio, and text.\nDespite their promising results, neural network-based recommendation algorithms\npose extensive computational costs, making it challenging to scale and improve\nupon. In this paper, we propose a general neural network-based recommendation\nframework, which subsumes several existing state-of-the-art recommendation\nalgorithms, and address the efficiency issue by investigating sampling\nstrategies in the stochastic gradient descent training for the framework. We\ntackle this issue by first establishing a connection between the loss functions\nand the user-item interaction bipartite graph, where the loss function terms\nare defined on links while major computation burdens are located at nodes. We\ncall this type of loss functions \"graph-based\" loss functions, for which varied\nmini-batch sampling strategies can have different computational costs. Based on\nthe insight, three novel sampling strategies are proposed, which can\nsignificantly improve the training efficiency of the proposed framework (up to\n$\\times 30$ times speedup in our experiments), as well as improving the\nrecommendation performance. Theoretical analysis is also provided for both the\ncomputational cost and the convergence. We believe the study of sampling\nstrategies have further implications on general graph-based loss functions, and\nwould also enable more research under the neural network-based recommendation\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 22:56:40 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""], ["Shi", "Yue", ""], ["Hong", "Liangjie", ""]]}, {"id": "1706.07912", "submitter": "Mahamad Suhil", "authors": "Lavanya Narayana Raju, Mahamad Suhil, D S Guru and Harsha S Gowda", "title": "Cluster Based Symbolic Representation for Skewed Text Categorization", "comments": "14 Pages, 15 Figures, 1 Table, Conference: RTIP2R", "journal-ref": null, "doi": "10.1007/978-981-10-4859-3_19", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a problem associated with imbalanced text corpora is addressed.\nA method of converting an imbalanced text corpus into a balanced one is\npresented. The presented method employs a clustering algorithm for conversion.\nInitially to avoid curse of dimensionality, an effective representation scheme\nbased on term class relevancy measure is adapted, which drastically reduces the\ndimension to the number of classes in the corpus. Subsequently, the samples of\nlarger sized classes are grouped into a number of subclasses of smaller sizes\nto make the entire corpus balanced. Each subclass is then given a single\nsymbolic vector representation by the use of interval valued features. This\nsymbolic representation in addition to being compact helps in reducing the\nspace requirement and also the classification time. The proposed model has been\nempirically demonstrated for its superiority on bench marking datasets viz.,\nReuters 21578 and TDT2. Further, it has been compared against several other\nexisting contemporary models including model based on support vector machine.\nThe comparative analysis indicates that the proposed model outperforms the\nother existing models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 06:04:21 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Raju", "Lavanya Narayana", ""], ["Suhil", "Mahamad", ""], ["Guru", "D S", ""], ["Gowda", "Harsha S", ""]]}, {"id": "1706.07913", "submitter": "Mahamad Suhil", "authors": "Harsha S. Gowda, Mahamad Suhil, D.S. Guru, and Lavanya Narayana Raju", "title": "Semi-supervised Text Categorization Using Recursive K-means Clustering", "comments": "11 Pages, 8 Figures, Conference: RTIP2R", "journal-ref": null, "doi": "10.1007/978-981-10-4859-3_20", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a semi-supervised learning algorithm for\nclassification of text documents. A method of labeling unlabeled text documents\nis presented. The presented method is based on the principle of divide and\nconquer strategy. It uses recursive K-means algorithm for partitioning both\nlabeled and unlabeled data collection. The K-means algorithm is applied\nrecursively on each partition till a desired level partition is achieved such\nthat each partition contains labeled documents of a single class. Once the\ndesired clusters are obtained, the respective cluster centroids are considered\nas representatives of the clusters and the nearest neighbor rule is used for\nclassifying an unknown text document. Series of experiments have been conducted\nto bring out the superiority of the proposed model over other recent state of\nthe art models on 20Newsgroups dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 06:08:27 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Gowda", "Harsha S.", ""], ["Suhil", "Mahamad", ""], ["Guru", "D. S.", ""], ["Raju", "Lavanya Narayana", ""]]}, {"id": "1706.07956", "submitter": "Vito Bellini", "authors": "Vito Bellini, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio", "title": "Auto-Encoding User Ratings via Knowledge Graphs in Recommendation\n  Scenarios", "comments": null, "journal-ref": null, "doi": "10.1145/3125486.3125496", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, driven also by the availability of an unprecedented\ncomputational power and storage capabilities in cloud environments we assisted\nto the proliferation of new algorithms, methods, and approaches in two areas of\nartificial intelligence: knowledge representation and machine learning. On the\none side, the generation of a high rate of structured data on the Web led to\nthe creation and publication of the so-called knowledge graphs. On the other\nside, deep learning emerged as one of the most promising approaches in the\ngeneration and training of models that can be applied to a wide variety of\napplication fields. More recently, autoencoders have proven their strength in\nvarious scenarios, playing a fundamental role in unsupervised learning. In this\npaper, we instigate how to exploit the semantic information encoded in a\nknowledge graph to build connections between units in a Neural Network, thus\nleading to a new method, SEM-AUTO, to extract and weigh semantic features that\ncan eventually be used to build a recommender system. As adding content-based\nside information may mitigate the cold user problems, we tested how our\napproach behave in the presence of a few rating from a user on the Movielens 1M\ndataset and compare results with BPRSLIM.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 13:46:40 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 17:18:45 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Bellini", "Vito", ""], ["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Di Sciascio", "Eugenio", ""]]}, {"id": "1706.08094", "submitter": "Franziska Horn", "authors": "Franziska Horn", "title": "Interactive Exploration and Discovery of Scientific Publications with\n  PubVis", "comments": "6 pages, see http://pubvis.herokuapp.com/ for a demo and\n  https://github.com/cod3licious/pubvis for the code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an exponentially growing number of scientific papers published each\nyear, advanced tools for exploring and discovering publications of interest are\nbecoming indispensable. To empower users beyond a simple keyword search\nprovided e.g. by Google Scholar, we present the novel web application PubVis.\nPowered by a variety of machine learning techniques, it combines essential\nfeatures to help researchers find the content most relevant to them. An\ninteractive visualization of a large collection of scientific publications\nprovides an overview of the field and encourages the user to explore articles\nbeyond a narrow research focus. This is augmented by personalized content based\narticle recommendations as well as an advanced full text search to discover\nrelevant references. The open sourced implementation of the app can be easily\nset up and run locally on a desktop computer to provide access to content\ntailored to the specific needs of individual users. Additionally, a PubVis demo\nwith access to a collection of 10,000 papers can be tested online.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 12:58:15 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Horn", "Franziska", ""]]}, {"id": "1706.08184", "submitter": "Pantelis Pipergias Analytis", "authors": "Pantelis P. Analytis, Tobias Schnabel, Stefan Herzog, Daniel Barkoczi,\n  Thorsten Joachims", "title": "A preference elicitation interface for collecting dense recommender\n  datasets with rich user information", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an interface that can be leveraged to quickly and effortlessly\nelicit people's preferences for visual stimuli, such as photographs, visual art\nand screensavers, along with rich side-information about its users. We plan to\nemploy the new interface to collect dense recommender datasets that will\ncomplement existing sparse industry-scale datasets. The new interface and the\ncollected datasets are intended to foster integration of research in\nrecommender systems with research in social and behavioral sciences. For\ninstance, we will use the datasets to assess the diversity of human preferences\nin different domains of visual experience. Further, using the datasets we will\nbe able to measure crucial psychological effects, such as preference\nconsistency, scale acuity and anchoring biases. Last, we the datasets will\nfacilitate evaluation in counterfactual learning experiments.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 22:58:22 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 01:00:24 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Analytis", "Pantelis P.", ""], ["Schnabel", "Tobias", ""], ["Herzog", "Stefan", ""], ["Barkoczi", "Daniel", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1706.08746", "submitter": "Andrew Yates", "authors": "Andrew Yates, Kai Hui", "title": "DE-PACRR: Exploring Layers Inside the PACRR Model", "comments": "Neu-IR 2017 SIGIR Workshop on Neural Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural IR models have demonstrated deep learning's utility in ad-hoc\ninformation retrieval. However, deep models have a reputation for being black\nboxes, and the roles of a neural IR model's components may not be obvious at\nfirst glance. In this work, we attempt to shed light on the inner workings of a\nrecently proposed neural IR model, namely the PACRR model, by visualizing the\noutput of intermediate layers and by investigating the relationship between\nintermediate weights and the ultimate relevance score produced. We highlight\nseveral insights, hoping that such insights will be generally applicable.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 09:23:37 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 16:03:03 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Yates", "Andrew", ""], ["Hui", "Kai", ""]]}, {"id": "1706.08928", "submitter": "Syed Arefinul Haque", "authors": "Xindi Wang and Syed Arefinul Haque", "title": "Classical Music Clustering Based on Acoustic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we cluster 330 classical music pieces collected from MusicNet\ndatabase based on their musical note sequence. We use shingling and chord\ntrajectory matrices to create signature for each music piece and performed\nspectral clustering to find the clusters. Based on different resolution, the\noutput clusters distinctively indicate composition from different classical\nmusic era and different composing style of the musicians.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:25:00 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Wang", "Xindi", ""], ["Haque", "Syed Arefinul", ""]]}, {"id": "1706.09067", "submitter": "Dawei Chen", "authors": "Dawei Chen, Lexing Xie, Aditya Krishna Menon, Cheng Soon Ong", "title": "Structured Recommendation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current recommender systems largely focus on static, unstructured content. In\nmany scenarios, we would like to recommend content that has structure, such as\na trajectory of points-of-interests in a city, or a playlist of songs. Dubbed\nStructured Recommendation, this problem differs from the typical structured\nprediction problem in that there are multiple correct answers for a given\ninput. Motivated by trajectory recommendation, we focus on sequential\nstructures but in contrast to classical Viterbi decoding we require that valid\npredictions are sequences with no repeated elements. We propose an approach to\nsequence recommendation based on the structured support vector machine. For\nprediction, we modify the inference procedure to avoid predicting loops in the\nsequence. For training, we modify the objective function to account for the\nexistence of multiple ground truths for a given input. We also modify the\nloss-augmented inference procedure to exclude the known ground truths.\nExperiments on real-world trajectory recommendation datasets show the benefits\nof our approach over existing, non-structured recommendation approaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 22:22:19 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Chen", "Dawei", ""], ["Xie", "Lexing", ""], ["Menon", "Aditya Krishna", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1706.09088", "submitter": "Dorien Herremans", "authors": "Dorien Herremans, Ching-Hua Chuan", "title": "Modeling Musical Context with Word2vec", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 11-18 (2017)", "doi": null, "report-no": "DLM/2017/1", "categories": "cs.SD cs.IR cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semantic vector space model for capturing complex polyphonic\nmusical context. A word2vec model based on a skip-gram representation with\nnegative sampling was used to model slices of music from a dataset of\nBeethoven's piano sonatas. A visualization of the reduced vector space using\nt-distributed stochastic neighbor embedding shows that the resulting embedded\nvector space captures tonal relationships, even without any explicit\ninformation about the musical contents of the slices. Secondly, an excerpt of\nthe Moonlight Sonata from Beethoven was altered by replacing slices based on\ncontext similarity. The resulting music shows that the selected slice based on\nsimilar word2vec context also has a relatively short tonal distance from the\noriginal slice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:46:50 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 02:33:06 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Herremans", "Dorien", ""], ["Chuan", "Ching-Hua", ""]]}, {"id": "1706.09200", "submitter": "Jaeyoon Yoo", "authors": "Jaeyoon Yoo, Heonseok Ha, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-Woo\n  Ha, Young-Han Kim, and Sungroh Yoon", "title": "Energy-Based Sequence GANs for Recommendation and Their Connection to\n  Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems aim to find an accurate and efficient mapping from\nhistoric data of user-preferred items to a new item that is to be liked by a\nuser. Towards this goal, energy-based sequence generative adversarial nets\n(EB-SeqGANs) are adopted for recommendation by learning a generative model for\nthe time series of user-preferred items. By recasting the energy function as\nthe feature function, the proposed EB-SeqGANs is interpreted as an instance of\nmaximum-entropy imitation learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 10:12:01 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Yoo", "Jaeyoon", ""], ["Ha", "Heonseok", ""], ["Yi", "Jihun", ""], ["Ryu", "Jongha", ""], ["Kim", "Chanju", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Young-Han", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1706.09206", "submitter": "Jyoti Verma", "authors": "Jyotsna Parmar, Jyoti", "title": "Semantic Web Prefetching Using Semantic Relatedness between Web pages", "comments": "7 pages,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet as become the way of life in the fast growing digital life.Even with\nthe increase in the internet speed, higher latency time is still a challenge.\nTo reduce latency, caching and pre fetching techniques can be used. However,\ncaching fails for dynamic websites which keeps on changing rapidly. Another\ntechnique is web prefetching, which prefetches the web pages that the user is\nlikely to request for in the future. Semantic web prefetching makes use of\nkeywords and descriptive texts like anchor text, titles, text surrounding\nanchor text of the present web pages for predicting users future requests.\nSemantic information is embedded within the web pages during their designing\nfor the purpose of reflecting the relationship between the web pages. The\nclient can fetch this information from the server. However, this technique\ninvolves load on web designers for adding external tags and on server for\nproviding this information along with the desired page, which is not desirable.\nThis paper is an effort to find the semantic relation between web pages using\nthe keywords provided by the user and the anchor texts of the hyperlinks on the\npresent web page.It provides algorithms for sequential and similar semantic\nrelations. These algorithms will be implemented on the client side which will\nnot cause overhead on designers and load on server for semantic information.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 10:36:19 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Parmar", "Jyotsna", ""], ["Jyoti", "", ""]]}, {"id": "1706.09317", "submitter": "Qian Wang", "authors": "Qian Wang and Ke Chen", "title": "Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition", "comments": "Technical Report, School of Computer Science, The University of\n  Manchester, Accepted to ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:32:57 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1706.09739", "submitter": "Sergio Oramas", "authors": "Sergio Oramas, Oriol Nieto, Mohamed Sordo, Xavier Serra", "title": "A Deep Multimodal Approach for Cold-start Music Recommendation", "comments": "In Proceedings of the 2nd Workshop on Deep Learning for Recommender\n  Systems (DLRS 2017), collocated with RecSys 2017", "journal-ref": null, "doi": "10.1145/3125486.3125492", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of digital music is being published daily. Music\nstreaming services often ingest all available music, but this poses a\nchallenge: how to recommend new artists for which prior knowledge is scarce? In\nthis work we aim to address this so-called cold-start problem by combining text\nand audio information with user feedback data using deep network architectures.\nOur method is divided into three steps. First, artist embeddings are learned\nfrom biographies by combining semantics, text features, and aggregated usage\ndata. Second, track embeddings are learned from the audio signal and available\nfeedback data. Finally, artist and track embeddings are combined in a\nmultimodal network. Results suggest that both splitting the recommendation\nproblem between feature levels (i.e., artist metadata and audio track), and\nmerging feature embeddings in a multimodal approach improve the accuracy of the\nrecommendations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 13:13:26 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 08:07:28 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Oramas", "Sergio", ""], ["Nieto", "Oriol", ""], ["Sordo", "Mohamed", ""], ["Serra", "Xavier", ""]]}, {"id": "1706.10067", "submitter": "Elias K\\\"arle", "authors": "Elias K\\\"arle, Umutcan \\c{S}im\\c{s}ek and Dieter Fensel", "title": "semantify.it, a Platform for Creation, Publication and Distribution of\n  Semantic Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of semantic technologies to content on the web is, in many\nregards, important and urgent. Search engines, chatbots, intelligent personal\nassistants and other technologies increasingly rely on content published as\nsemantic structured data. Yet, the process of creating this kind of data is\nstill complicated and widely unknown. The semantify.it platform implements an\napproach to solve three of the most challenging question regarding the\npublication of structured semantic data, namely: a) what vocabulary to use, b)\nhow to create annotation files and c) how to publish or integrate annotations\nwithin a website without programming. This paper presents the idea and the\ndevelopment of the semantify.it platform. It demonstrates that the creation\nprocess of semantically annotated data does not have to be hard, shows use\ncases and pilot users of the created software and presents where the\ndevelopment of this platform or alike projects lead to in the future.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 08:49:20 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 10:44:19 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["K\u00e4rle", "Elias", ""], ["\u015eim\u015fek", "Umutcan", ""], ["Fensel", "Dieter", ""]]}, {"id": "1706.10076", "submitter": "Pavel Kucherbaev", "authors": "Pavel Kucherbaev, Achilleas Psyllidis, Alessandro Bozzon", "title": "Chatbots as Conversational Recommender Systems in Urban Contexts", "comments": "2 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we outline the vision of chatbots that facilitate the\ninteraction between citizens and policy-makers at the city scale. We report the\nresults of a co-design session attended by more than 60 participants. We give\nan outlook of how some challenges associated with such chatbot systems could be\naddressed in the future.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 09:24:39 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 09:03:49 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Kucherbaev", "Pavel", ""], ["Psyllidis", "Achilleas", ""], ["Bozzon", "Alessandro", ""]]}, {"id": "1706.10192", "submitter": "Andrew Yates", "authors": "Kai Hui, Andrew Yates, Klaus Berberich, Gerard de Melo", "title": "Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval", "comments": "To appear in WSDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural IR models, such as DRMM and PACRR, have achieved strong results by\nsuccessfully capturing relevance matching signals. We argue that the context of\nthese matching signals is also important. Intuitively, when extracting,\nmodeling, and combining matching signals, one would like to consider the\nsurrounding text (local context) as well as other signals from the same\ndocument that can contribute to the overall relevance score. In this work, we\nhighlight three potential shortcomings caused by not considering context\ninformation and propose three neural ingredients to address them: a\ndisambiguation component, cascade k-max pooling, and a shuffling combination\nlayer. Incorporating these components into the PACRR model yields Co-PACRR, a\nnovel context-aware neural IR model. Extensive comparisons with established\nmodels on Trec Web Track data confirm that the proposed model can achieve\nsuperior search results. In addition, an ablation analysis is conducted to gain\ninsights into the impact of and interactions between different components. We\nrelease our code to enable future comparisons.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:39:03 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 13:42:11 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 13:43:56 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Hui", "Kai", ""], ["Yates", "Andrew", ""], ["Berberich", "Klaus", ""], ["de Melo", "Gerard", ""]]}, {"id": "1706.10231", "submitter": "Alexander Dallmann", "authors": "Alexander Dallmann (1), Alexander Grimm (1), Christian P\\\"olitz (1),\n  Daniel Zoller (1), Andreas Hotho (1 and 2) ((1) University of W\\\"urzburg, (2)\n  L3S Research Center)", "title": "Improving Session Recommendation with Recurrent Neural Networks by\n  Exploiting Dwell Time", "comments": "6 pages, 3 figures, submission to DLRS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Recurrent Neural Networks (RNNs) have been applied to the task of\nsession-based recommendation. These approaches use RNNs to predict the next\nitem in a user session based on the previ- ously visited items. While some\napproaches consider additional item properties, we argue that item dwell time\ncan be used as an implicit measure of user interest to improve session-based\nitem recommen- dations. We propose an extension to existing RNN approaches that\ncaptures user dwell time in addition to the visited items and show that\nrecommendation performance can be improved. Additionally, we investigate the\nusefulness of a single validation split for model selection in the case of\nminor improvements and find that in our case the best model is not selected and\na fold-like study with different validation sets is necessary to ensure the\nselection of the best model.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:58:52 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Dallmann", "Alexander", "", "1 and 2"], ["Grimm", "Alexander", "", "1 and 2"], ["P\u00f6litz", "Christian", "", "1 and 2"], ["Zoller", "Daniel", "", "1 and 2"], ["Hotho", "Andreas", "", "1 and 2"]]}]