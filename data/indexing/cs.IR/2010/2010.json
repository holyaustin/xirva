[{"id": "2010.00038", "submitter": "Mohit Chandra", "authors": "Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta,\n  Manish Shrivastava, Ponnurangam Kumaraguru", "title": "AbuseAnalyzer: Abuse Detection, Severity and Target Prediction for Gab\n  Posts", "comments": "Extended version for our paper accepted at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While extensive popularity of online social media platforms has made\ninformation dissemination faster, it has also resulted in widespread online\nabuse of different types like hate speech, offensive language, sexist and\nracist opinions, etc. Detection and curtailment of such abusive content is\ncritical for avoiding its psychological impact on victim communities, and\nthereby preventing hate crimes. Previous works have focused on classifying user\nposts into various forms of abusive behavior. But there has hardly been any\nfocus on estimating the severity of abuse and the target. In this paper, we\npresent a first of the kind dataset with 7601 posts from Gab which looks at\nonline abuse from the perspective of presence of abuse, severity and target of\nabusive behavior. We also propose a system to address these tasks, obtaining an\naccuracy of ~80% for abuse presence, ~82% for abuse target prediction, and ~65%\nfor abuse severity prediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 18:12:50 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 17:42:33 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chandra", "Mohit", ""], ["Pathak", "Ashwin", ""], ["Dutta", "Eesha", ""], ["Jain", "Paryul", ""], ["Gupta", "Manish", ""], ["Shrivastava", "Manish", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2010.00182", "submitter": "Yang Zhang", "authors": "Yang Zhang, Qiang Ma", "title": "Dual Attention Model for Citation Recommendation", "comments": null, "journal-ref": "Proceedings of the 28th International Conference on Computational\n  Linguistics (COLING2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on an exponentially increasing number of academic articles, discovering\nand citing comprehensive and appropriate resources has become a non-trivial\ntask. Conventional citation recommender methods suffer from severe information\nloss. For example, they do not consider the section of the paper that the user\nis writing and for which they need to find a citation, the relatedness between\nthe words in the local context (the text span that describes a citation), or\nthe importance on each word from the local context. These shortcomings make\nsuch methods insufficient for recommending adequate citations to academic\nmanuscripts. In this study, we propose a novel embedding-based neural network\ncalled \"dual attention model for citation recommendation (DACR)\" to recommend\ncitations during manuscript preparation. Our method adapts embedding of three\ndimensions of semantic information: words in the local context, structural\ncontexts, and the section on which a user is working. A neural network is\ndesigned to maximize the similarity between the embedding of the three input\n(local context words, section and structural contexts) and the target citation\nappearing in the context. The core of the neural network is composed of\nself-attention and additive attention, where the former aims to capture the\nrelatedness between the contextual words and structural context, and the latter\naims to learn the importance of them. The experiments on real-world datasets\ndemonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 02:41:47 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 11:27:20 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 12:57:58 GMT"}, {"version": "v4", "created": "Thu, 29 Oct 2020 12:31:26 GMT"}, {"version": "v5", "created": "Thu, 3 Dec 2020 05:16:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Zhang", "Yang", ""], ["Ma", "Qiang", ""]]}, {"id": "2010.00200", "submitter": "Michael Bendersky", "authors": "Michael Bendersky and Honglei Zhuang and Ji Ma and Shuguang Han and\n  Keith Hall and Ryan McDonald", "title": "RRF102: Meeting the TREC-COVID Challenge with a 100+ Runs Ensemble", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report the results of our participation in the TREC-COVID\nchallenge. To meet the challenge of building a search engine for rapidly\nevolving biomedical collection, we propose a simple yet effective weighted\nhierarchical rank fusion approach, that ensembles together 102 runs from (a)\nlexical and semantic retrieval systems, (b) pre-trained and fine-tuned BERT\nrankers, and (c) relevance feedback runs. Our ablation studies demonstrate the\ncontributions of each of these systems to the overall ensemble. The submitted\nensemble runs achieved state-of-the-art performance in rounds 4 and 5 of the\nTREC-COVID challenge.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 05:27:51 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bendersky", "Michael", ""], ["Zhuang", "Honglei", ""], ["Ma", "Ji", ""], ["Han", "Shuguang", ""], ["Hall", "Keith", ""], ["McDonald", "Ryan", ""]]}, {"id": "2010.00482", "submitter": "Arash Mahyari", "authors": "Arash Mahyari, Peter Pirolli", "title": "Physical Exercise Recommendation and Success Prediction Using\n  Interconnected Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unhealthy behaviors, e.g., physical inactivity and unhealthful food choice,\nare the primary healthcare cost drivers in developed countries. Pervasive\ncomputational, sensing, and communication technology provided by smartphones\nand smartwatches have made it possible to support individuals in their everyday\nlives to develop healthier lifestyles. In this paper, we propose an exercise\nrecommendation system that also predicts individual success rates. The system,\nconsisting of two inter-connected recurrent neural networks (RNNs), uses the\nhistory of workouts to recommend the next workout activity for each individual.\nThe system then predicts the probability of successful completion of the\npredicted activity by the individual. The prediction accuracy of this\ninterconnected-RNN model is assessed on previously published data from a\nfour-week mobile health experiment and is shown to improve upon previous\npredictions from a computational cognitive model.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:22:59 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 20:04:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Mahyari", "Arash", ""], ["Pirolli", "Peter", ""]]}, {"id": "2010.00502", "submitter": "Gautam Kishore Shahi", "authors": "Gautam Kishore Shahi", "title": "AMUSED: An Annotation Framework of Multi-modal Social Media Data", "comments": "10 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a semi-automated framework called AMUSED for\ngathering multi-modal annotated data from the multiple social media platforms.\nThe framework is designed to mitigate the issues of collecting and annotating\nsocial media data by cohesively combining machine and human in the data\ncollection process. From a given list of the articles from professional news\nmedia or blog, AMUSED detects links to the social media posts from news\narticles and then downloads contents of the same post from the respective\nsocial media platform to gather details about that specific post. The framework\nis capable of fetching the annotated data from multiple platforms like Twitter,\nYouTube, Reddit. The framework aims to reduce the workload and problems behind\nthe data annotation from the social media platforms. AMUSED can be applied in\nmultiple application domains, as a use case, we have implemented the framework\nfor collecting COVID-19 misinformation data from different social media\nplatforms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:50:41 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Shahi", "Gautam Kishore", ""]]}, {"id": "2010.00571", "submitter": "Julian Eisenschlos", "authors": "Julian Martin Eisenschlos, Syrine Krichene, Thomas M\\\"uller", "title": "Understanding tables with intermediate pre-training", "comments": "Accepted to EMNLP Findings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table entailment, the binary classification task of finding if a sentence is\nsupported or refuted by the content of a table, requires parsing language and\ntable structure as well as numerical and discrete reasoning. While there is\nextensive work on textual entailment, table entailment is less well studied. We\nadapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize\nentailment. Motivated by the benefits of data augmentation, we create a\nbalanced dataset of millions of automatically created training examples which\nare learned in an intermediate step prior to fine-tuning. This new data is not\nonly useful for table entailment, but also for SQA (Iyyer et al., 2017), a\nsequential table QA task. To be able to use long examples as input of BERT\nmodels, we evaluate table pruning techniques as a pre-processing step to\ndrastically improve the training and prediction efficiency at a moderate drop\nin accuracy. The different methods set the new state-of-the-art on the TabFact\n(Chen et al., 2020) and SQA datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:43:27 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 12:26:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Eisenschlos", "Julian Martin", ""], ["Krichene", "Syrine", ""], ["M\u00fcller", "Thomas", ""]]}, {"id": "2010.00665", "submitter": "Mohammad Javad Shayegan", "authors": "Parinaz Rahimizadeh, Mohammad Javad Shayegan", "title": "Event Detection in Twitter by Weighting Tweet's Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, people spend a lot of time on social networks. They use\nsocial networks as a place to comment on personal or public events. Thus, a\nlarge amount of information is generated and shared daily in these networks.\nUsing such a massive amount of information can help authorities to react to\nevents accurately and timely. In this study, the social network investigated is\nTwitter. The main idea of this research is to differentiate among tweets based\non some of their features. This study aimed at investigating the performance of\nevent detection by weighting three attributes of tweets; including the\nfollowers count, the retweets count, and the user location. The results show\nthat the average execution time and the precision of event detection in the\npresented method improved 27% and 31%, respectively, than the base method.\nAnother result of this research is the ability to detect all events (including\nhot events and less important ones) in the presented method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 19:58:48 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Rahimizadeh", "Parinaz", ""], ["Shayegan", "Mohammad Javad", ""]]}, {"id": "2010.00722", "submitter": "Ameet Deshpande", "authors": "Ameet Deshpande and Mitesh M. Khapra", "title": "Evaluating a Generative Adversarial Framework for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have resulted in\nits widespread applications to multiple domains. A recent model, IRGAN, applies\nthis framework to Information Retrieval (IR) and has gained significant\nattention over the last few years. In this focused work, we critically analyze\nmultiple components of IRGAN, while providing experimental and theoretical\nevidence of some of its shortcomings. Specifically, we identify issues with the\nconstant baseline term in the policy gradients optimization and show that the\ngenerator harms IRGAN's performance. Motivated by our findings, we propose two\nmodels influenced by self-contrastive estimation and co-training which\noutperform IRGAN on two out of the three tasks considered.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:11:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Deshpande", "Ameet", ""], ["Khapra", "Mitesh M.", ""]]}, {"id": "2010.00768", "submitter": "Xiaoguang Li", "authors": "Yang Bai, Xiaoguang Li, Gang Wang, Chaoliang Zhang, Lifeng Shang, Jun\n  Xu, Zhaowei Wang, Fangshan Wang, Qun Liu", "title": "SparTerm: Learning Term-based Sparse Representation for Fast Text\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term-based sparse representations dominate the first-stage text retrieval in\nindustrial applications, due to its advantage in efficiency, interpretability,\nand exact term matching. In this paper, we study the problem of transferring\nthe deep knowledge of the pre-trained language model (PLM) to Term-based Sparse\nrepresentations, aiming to improve the representation capacity of\nbag-of-words(BoW) method for semantic-level matching, while still keeping its\nadvantages. Specifically, we propose a novel framework SparTerm to directly\nlearn sparse text representations in the full vocabulary space. The proposed\nSparTerm comprises an importance predictor to predict the importance for each\nterm in the vocabulary, and a gating controller to control the term activation.\nThese two modules cooperatively ensure the sparsity and flexibility of the\nfinal text representation, which unifies the term-weighting and expansion in\nthe same framework. Evaluated on MSMARCO dataset, SparTerm significantly\noutperforms traditional sparse methods and achieves state of the art ranking\nperformance among all the PLM-based sparse models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 03:54:56 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Bai", "Yang", ""], ["Li", "Xiaoguang", ""], ["Wang", "Gang", ""], ["Zhang", "Chaoliang", ""], ["Shang", "Lifeng", ""], ["Xu", "Jun", ""], ["Wang", "Zhaowei", ""], ["Wang", "Fangshan", ""], ["Liu", "Qun", ""]]}, {"id": "2010.00813", "submitter": "Qinyong Wang", "authors": "Hongzhi Yin, Qinyong Wang, Kai Zheng, Zhixu Li, Xiaofang Zhou", "title": "Overcoming Data Sparsity in Group Recommendation", "comments": "To appear in TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been an important task for recommender systems to suggest satisfying\nactivities to a group of users in people's daily social life. The major\nchallenge in this task is how to aggregate personal preferences of group\nmembers to infer the decision of a group. Conventional group recommendation\nmethods applied a predefined strategy for preference aggregation. However,\nthese static strategies are too simple to model the real and complex process of\ngroup decision-making, especially for occasional groups which are formed\nad-hoc. Moreover, group members should have non-uniform influences or weights\nin a group, and the weight of a user can be varied in different groups.\nTherefore, an ideal group recommender system should be able to accurately learn\nnot only users' personal preferences but also the preference aggregation\nstrategy from data. In this paper, we propose a novel end-to-end group\nrecommender system named CAGR (short for Centrality Aware Group Recommender\"),\nwhich takes Bipartite Graph Embedding Model (BGEM), the self-attention\nmechanism and Graph Convolutional Networks (GCNs) as basic building blocks to\nlearn group and user representations in a unified way. Specifically, we first\nextend BGEM to model group-item interactions, and then in order to overcome the\nlimitation and sparsity of the interaction data generated by occasional groups,\nwe propose a self-attentive mechanism to represent groups based on the group\nmembers. In addition, to overcome the sparsity issue of user-item interaction\ndata, we leverage the user social networks to enhance user representation\nlearning, obtaining centrality-aware user representations. We create three\nlarge-scale benchmark datasets and conduct extensive experiments on them. The\nexperimental results show the superiority of our proposed CAGR by comparing it\nwith state-of-the-art group recommender models.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:11:19 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Yin", "Hongzhi", ""], ["Wang", "Qinyong", ""], ["Zheng", "Kai", ""], ["Li", "Zhixu", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "2010.00860", "submitter": "Claire N\\'edellec", "authors": "Claire N\\'edellec, Wiktoria Golik, Sophie Aubin, Robert Bossy", "title": "Building Large Lexicalized Ontologies from Text: a Use Case in Automatic\n  Indexing of Biotechnology Patents", "comments": null, "journal-ref": "International Conference on Knowledge Engineering and Knowledge\n  Management. EKAW 2010. Lecture Notes in Computer Science, vol 6317. (pp.\n  514-523) Springer, Berlin, Heidelberg", "doi": "10.1007/978-3-642-16438-5_41", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a tool, TyDI, and methods experimented in the building of\na termino-ontology, i.e. a lexicalized ontology aimed at fine-grained\nindexation for semantic search applications. TyDI provides facilities for\nknowledge engineers and domain experts to efficiently collaborate to validate,\norganize and conceptualize corpus extracted terms. A use case on biotechnology\npatent search demonstrates TyDI's potential.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:42:56 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["N\u00e9dellec", "Claire", ""], ["Golik", "Wiktoria", ""], ["Aubin", "Sophie", ""], ["Bossy", "Robert", ""]]}, {"id": "2010.00904", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Gautier Izacard, Sebastian Riedel, Fabio Petroni", "title": "Autoregressive Entity Retrieval", "comments": "Accepted (spotlight) at International Conference on Learning\n  Representations (ICLR) 2021. Code at\n  https://github.com/facebookresearch/GENRE. 20 pages, 9 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entities are at the center of how we represent and aggregate knowledge. For\ninstance, Encyclopedias such as Wikipedia are structured by entities (e.g., one\nper Wikipedia article). The ability to retrieve such entities given a query is\nfundamental for knowledge-intensive tasks such as entity linking and\nopen-domain question answering. Current approaches can be understood as\nclassifiers among atomic labels, one for each entity. Their weight vectors are\ndense entity representations produced by encoding entity meta information such\nas their descriptions. This approach has several shortcomings: (i) context and\nentity affinity is mainly captured through a vector dot product, potentially\nmissing fine-grained interactions; (ii) a large memory footprint is needed to\nstore dense representations when considering large entity sets; (iii) an\nappropriately hard set of negative data has to be subsampled at training time.\nIn this work, we propose GENRE, the first system that retrieves entities by\ngenerating their unique names, left to right, token-by-token in an\nautoregressive fashion. This mitigates the aforementioned technical issues\nsince: (i) the autoregressive formulation directly captures relations between\ncontext and entity name, effectively cross encoding both; (ii) the memory\nfootprint is greatly reduced because the parameters of our encoder-decoder\narchitecture scale with vocabulary size, not entity count; (iii) the softmax\nloss is computed without subsampling negative data. We experiment with more\nthan 20 datasets on entity disambiguation, end-to-end entity linking and\ndocument retrieval tasks, achieving new state-of-the-art or very competitive\nresults while using a tiny fraction of the memory footprint of competing\nsystems. Finally, we demonstrate that new entities can be added by simply\nspecifying their names. Code and pre-trained models at\nhttps://github.com/facebookresearch/GENRE.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:13:31 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 15:20:52 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 07:21:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["De Cao", "Nicola", ""], ["Izacard", "Gautier", ""], ["Riedel", "Sebastian", ""], ["Petroni", "Fabio", ""]]}, {"id": "2010.00980", "submitter": "Andreas R\\\"uckl\\'e", "authors": "Andreas R\\\"uckl\\'e, Jonas Pfeiffer, Iryna Gurevych", "title": "MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on\n  a Massive Scale", "comments": "EMNLP-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the zero-shot transfer capabilities of text matching models on a\nmassive scale, by self-supervised training on 140 source domains from community\nquestion answering forums in English. We investigate the model performances on\nnine benchmarks of answer selection and question similarity tasks, and show\nthat all 140 models transfer surprisingly well, where the large majority of\nmodels substantially outperforms common IR baselines. We also demonstrate that\nconsidering a broad selection of source domains is crucial for obtaining the\nbest zero-shot transfer performances, which contrasts the standard procedure\nthat merely relies on the largest and most similar domains. In addition, we\nextensively study how to best combine multiple source domains. We propose to\nincorporate self-supervised with supervised multi-task learning on all\navailable source domains. Our best zero-shot transfer model considerably\noutperforms in-domain BERT and the previous state of the art on six benchmarks.\nFine-tuning of our model with in-domain data results in additional large gains\nand achieves the new state of the art on all nine benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:22:12 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["R\u00fcckl\u00e9", "Andreas", ""], ["Pfeiffer", "Jonas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2010.00984", "submitter": "Felice Antonio Merra", "authors": "Vito Walter Anelli, Tommaso Di Noia, Daniele Malitesta, Felice Antonio\n  Merra", "title": "An Empirical Study of DNNs Robustification Inefficacy in Protecting\n  Visual Recommenders", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-based recommender systems (VRSs) enhance recommendation performance by\nintegrating users' feedback with the visual features of product images\nextracted from a deep neural network (DNN). Recently, human-imperceptible\nimages perturbations, defined \\textit{adversarial attacks}, have been\ndemonstrated to alter the VRSs recommendation performance, e.g., pushing/nuking\ncategory of products. However, since adversarial training techniques have\nproven to successfully robustify DNNs in preserving classification accuracy, to\nthe best of our knowledge, two important questions have not been investigated\nyet: 1) How well can these defensive mechanisms protect the VRSs performance?\n2) What are the reasons behind ineffective/effective defenses? To answer these\nquestions, we define a set of defense and attack settings, as well as\nrecommender models, to empirically investigate the efficacy of defensive\nmechanisms. The results indicate alarming risks in protecting a VRS through the\nDNN robustification. Our experiments shed light on the importance of visual\nfeatures in very effective attack scenarios. Given the financial impact of VRSs\non many companies, we believe this work might rise the need to investigate how\nto successfully protect visual-based recommenders. Source code and data are\navailable at\nhttps://anonymous.4open.science/r/868f87ca-c8a4-41ba-9af9-20c41de33029/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:29:41 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Di Noia", "Tommaso", ""], ["Malitesta", "Daniele", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2010.01195", "submitter": "Saar Kuzi", "authors": "Saar Kuzi, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork", "title": "Leveraging Semantic and Lexical Matching to Improve the Recall of\n  Document Retrieval Systems: A Hybrid Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines often follow a two-phase paradigm where in the first stage\n(the retrieval stage) an initial set of documents is retrieved and in the\nsecond stage (the re-ranking stage) the documents are re-ranked to obtain the\nfinal result list. While deep neural networks were shown to improve the\nperformance of the re-ranking stage in previous works, there is little\nliterature about using deep neural networks to improve the retrieval stage. In\nthis paper, we study the merits of combining deep neural network models and\nlexical models for the retrieval stage. A hybrid approach, which leverages both\nsemantic (deep neural network-based) and lexical (keyword matching-based)\nretrieval models, is proposed. We perform an empirical study, using a publicly\navailable TREC collection, which demonstrates the effectiveness of our approach\nand sheds light on the different characteristics of the semantic approach, the\nlexical approach, and their combination.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 20:59:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kuzi", "Saar", ""], ["Zhang", "Mingyang", ""], ["Li", "Cheng", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2010.01258", "submitter": "Areej Alsini", "authors": "Areej Alsini, Du Q. Huynh, Amitava Datta", "title": "Hit ratio: An Evaluation Metric for Hashtag Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtag recommendation is a crucial task, especially with an increase of\ninterest in using social media platforms such as Twitter in the last decade.\nHashtag recommendation systems automatically suggest hashtags to a user while\nwriting a tweet. Most of the research in the area of hashtag recommendation\nhave used classical metrics such as hit rate, precision, recall, and F1-score\nto measure the accuracy of hashtag recommendation systems. These metrics are\nbased on the exact match of the recommended hashtags with their corresponding\nground truth. However, it is not clear how adequate these metrics to evaluate\nhashtag recommendation. The research question that we are interested in seeking\nan answer is: are these metrics adequate for evaluating hashtag recommendation\nsystems when the numbers of ground truth hashtags in tweets are highly\nvariable? In this paper, we propose a new metric which we call hit ratio for\nhashtag recommendation. Extensive evaluation through hypothetical examples and\nreal-world application across a range of hashtag recommendation models indicate\nthat the hit ratio is a useful metric. A comparison of hit ratio with the\nclassical evaluation metrics reveals their limitations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 02:07:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Alsini", "Areej", ""], ["Huynh", "Du Q.", ""], ["Datta", "Amitava", ""]]}, {"id": "2010.01329", "submitter": "Felice Antonio Merra", "authors": "Vito Walter Anelli, Alejandro Bellog\\'in, Yashar Deldjoo, Tommaso Di\n  Noia, Felice Antonio Merra", "title": "Multi-Step Adversarial Perturbations on Recommender Systems Embeddings", "comments": "10 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RSs) have attained exceptional performance in learning\nusers' preferences and helping them in finding the most suitable products.\nRecent advances in adversarial machine learning (AML) in the computer vision\ndomain have raised interests in the security of state-of-the-art model-based\nrecommenders. Recently, worrying deterioration of recommendation accuracy has\nbeen acknowledged on several state-of-the-art model-based recommenders (e.g.,\nBPR-MF) when machine-learned adversarial perturbations contaminate model\nparameters. However, while the single-step fast gradient sign method (FGSM) is\nthe most explored perturbation strategy, multi-step (iterative) perturbation\nstrategies, that demonstrated higher efficacy in the computer vision domain,\nhave been highly under-researched in recommendation tasks.\n  In this work, inspired by the basic iterative method (BIM) and the projected\ngradient descent (PGD) strategies proposed in the CV domain, we adapt the\nmulti-step strategies for the item recommendation task to study the possible\nweaknesses of embedding-based recommender models under minimal adversarial\nperturbations. Letting the magnitude of the perturbation be fixed, we\nillustrate the highest efficacy of the multi-step perturbation compared to the\nsingle-step one with extensive empirical evaluation on two widely adopted\nrecommender datasets. Furthermore, we study the impact of structural dataset\ncharacteristics, i.e., sparsity, density, and size, on the performance\ndegradation issued by presented perturbations to support RS designer in\ninterpreting recommendation performance variation due to minimal variations of\nmodel parameters. Our implementation and datasets are available at\nhttps://anonymous.4open.science/r/9f27f909-93d5-4016-b01c-8976b8c14bc5/.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:25:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Bellog\u00edn", "Alejandro", ""], ["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2010.01450", "submitter": "Kexin Huang", "authors": "Yue Yu, Kexin Huang, Chao Zhang, Lucas M. Glass, Jimeng Sun, and Cao\n  Xiao", "title": "SumGNN: Multi-typed Drug Interaction Prediction via Efficient Knowledge\n  Graph Summarization", "comments": "Published in Bioinformatics 2021", "journal-ref": null, "doi": "10.1093/bioinformatics/btab207", "report-no": null, "categories": "cs.LG cs.CL cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the increasing availability of drug-drug interactions (DDI)\ndatasets and large biomedical knowledge graphs (KGs), accurate detection of\nadverse DDI using machine learning models becomes possible. However, it remains\nlargely an open problem how to effectively utilize large and noisy biomedical\nKG for DDI detection. Due to its sheer size and amount of noise in KGs, it is\noften less beneficial to directly integrate KGs with other smaller but higher\nquality data (e.g., experimental data). Most of the existing approaches ignore\nKGs altogether. Some try to directly integrate KGs with other data via graph\nneural networks with limited success. Furthermore, most previous works focus on\nbinary DDI prediction whereas the multi-typed DDI pharmacological effect\nprediction is a more meaningful but harder task. To fill the gaps, we propose a\nnew method SumGNN: knowledge summarization graph neural network, which is\nenabled by a subgraph extraction module that can efficiently anchor on relevant\nsubgraphs from a KG, a self-attention based subgraph summarization scheme to\ngenerate a reasoning path within the subgraph, and a multi-channel knowledge\nand data integration module that utilizes massive external biomedical knowledge\nfor significantly improved multi-typed DDI predictions. SumGNN outperforms the\nbest baseline by up to 5.54\\%, and the performance gain is particularly\nsignificant in low data relation types. In addition, SumGNN provides\ninterpretable prediction via the generated reasoning paths for each prediction.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 00:14:57 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 21:07:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yu", "Yue", ""], ["Huang", "Kexin", ""], ["Zhang", "Chao", ""], ["Glass", "Lucas M.", ""], ["Sun", "Jimeng", ""], ["Xiao", "Cao", ""]]}, {"id": "2010.01470", "submitter": "Lequn Luke Wang", "authors": "Lequn Wang and Thorsten Joachims", "title": "Fairness and Diversity for Rankings in Two-Sided Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking items by their probability of relevance has long been the goal of\nconventional ranking systems. While this maximizes traditional criteria of\nranking performance, there is a growing understanding that it is an\noversimplification in online platforms that serve not only a diverse user\npopulation, but also the producers of the items. In particular, ranking\nalgorithms are expected to be fair in how they serve all groups of users -- not\njust the majority group -- and they also need to be fair in how they divide\nexposure among the items. These fairness considerations can partially be met by\nadding diversity to the rankings, as done in several recent works. However, we\nshow in this paper that user fairness, item fairness and diversity are\nfundamentally different concepts. In particular, we find that algorithms that\nconsider only one of the three desiderata can fail to satisfy and even harm the\nother two. To overcome this shortcoming, we present the first ranking algorithm\nthat explicitly enforces all three desiderata. The algorithm optimizes user and\nitem fairness as a convex optimization problem which can be solved optimally.\nFrom its solution, a ranking policy can be derived via a novel Birkhoff-von\nNeumann decomposition algorithm that optimizes diversity. Beyond the\ntheoretical analysis, we investigate empirically on a new benchmark dataset how\neffectively the proposed ranking algorithm can control user fairness, item\nfairness and diversity, as well as the trade-offs between them.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 02:53:09 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 04:00:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wang", "Lequn", ""], ["Joachims", "Thorsten", ""]]}, {"id": "2010.01494", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Jianxun Lian, Yongfeng Huang, Xing Xie", "title": "PTUM: Pre-training User Model from Unlabeled User Behaviors via\n  Self-supervision", "comments": "To appear in Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User modeling is critical for many personalized web services. Many existing\nmethods model users based on their behaviors and the labeled data of target\ntasks. However, these methods cannot exploit useful information in unlabeled\nuser behavior data, and their performance may be not optimal when labeled data\nis scarce. Motivated by pre-trained language models which are pre-trained on\nlarge-scale unlabeled corpus to empower many downstream tasks, in this paper we\npropose to pre-train user models from large-scale unlabeled user behaviors\ndata. We propose two self-supervision tasks for user model pre-training. The\nfirst one is masked behavior prediction, which can model the relatedness\nbetween historical behaviors. The second one is next $K$ behavior prediction,\nwhich can model the relatedness between past and future behaviors. The\npre-trained user models are finetuned in downstream tasks to learn\ntask-specific user representations. Experimental results on two real-world\ndatasets validate the effectiveness of our proposed user model pre-training\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 07:01:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Lian", "Jianxun", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2010.01521", "submitter": "Shibli Nisar", "authors": "Shibli Nisar, Syed Muhammad Ali Zuhaib, Abasin Ulasyar and Muhammad\n  Tariq", "title": "A Privacy Preserved and Cost Efficient Control Scheme for Coronavirus\n  Outbreak Using Call Data Record and Contact Tracing", "comments": null, "journal-ref": "IEEE Consumer Electronics Magazine 2020", "doi": "10.1109/MCE.2020.3038023", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus or COVID-19, which has been declared pandemic by the World Health\nOrganization, has incurred huge losses to the lives of people throughout the\nworld. Although, the scientists, researchers and doctors are working round the\nclock to develop a vaccine for COVID-19, it may take a year or two to make a\nsafe and effective vaccine available for the world. In current circumstances, a\nsolution must be developed to control or stop the spread of the virus. For this\npurpose, a novel technique based on call data record analysis (CDRA)and contact\ntracing is proposed that can effectively control the coronavirus outbreak. A\npositive coronavirus patient can be traced through CDRA and contact tracing.\nThe technique can track the path traversed by the patient and collect the cell\nnumbers of all those people who have met with the patient. Keeping in tact the\nprivacy of this group of people, who are contacted through their cell numbers\nso that they can isolate themselves till the result of their coronavirus test\narrives. If a test result of a person comes positive among the group, then\nhe/she must be isolated and same CDRA and contact tracing procedures are\nadopted for that person. A COVID-19 patient is geo tagged and alerts are sent\nif any violation of isolation is done by the patient. Moreover, the general\npublic is informed in advance to avoid the path followed by the patients. This\ncost effective mechanism is not only capable to control the coronavirus\noutbreak but also helps in isolating the patient in his/her house.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 09:18:46 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 16:45:02 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nisar", "Shibli", ""], ["Zuhaib", "Syed Muhammad Ali", ""], ["Ulasyar", "Abasin", ""], ["Tariq", "Muhammad", ""]]}, {"id": "2010.01600", "submitter": "Lara Kassab", "authors": "Lara Kassab, Alona Kryshchenko, Hanbaek Lyu, Denali Molitor, Deanna\n  Needell, Elizaveta Rebrova", "title": "On Nonnegative Matrix and Tensor Decompositions for COVID-19 Twitter\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NA cs.SI math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze Twitter data relating to the COVID-19 pandemic using dynamic topic\nmodeling techniques to learn topics and their prevalence over time. Topics are\nlearned using four methods: nonnegative matrix factorization (NMF), nonnegative\nCP tensor decomposition (NCPD), online NMF, and online NCPD. All of the methods\nconsidered discover major topics that persist for multiple weeks relating to\nChina, social distancing, and U.S. President Trump. The topics about China\ndominate in early February before giving way to more diverse topics. We observe\nthat NCPD and online NCPD can detect topics that are prevalent over a few days,\nsuch as the outbreak in South Korea. The topics detected by NMF and online NMF,\nhowever, are prevalent over longer periods of time. Our results are validated\nagainst external news sources.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 15:20:05 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Kassab", "Lara", ""], ["Kryshchenko", "Alona", ""], ["Lyu", "Hanbaek", ""], ["Molitor", "Denali", ""], ["Needell", "Deanna", ""], ["Rebrova", "Elizaveta", ""]]}, {"id": "2010.01666", "submitter": "Aashish Misraa", "authors": "Aashish Kumar Misraa, Ajinkya Kale, Pranav Aggarwal, Ali Aminian", "title": "Multi-Modal Retrieval using Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real world applications of image retrieval such as Adobe Stock, which is\na marketplace for stock photography and illustrations, need a way for users to\nfind images which are both visually (i.e. aesthetically) and conceptually (i.e.\ncontaining the same salient objects) as a query image. Learning visual-semantic\nrepresentations from images is a well studied problem for image retrieval.\nFiltering based on image concepts or attributes is traditionally achieved with\nindex-based filtering (e.g. on textual tags) or by re-ranking after an initial\nvisual embedding based retrieval. In this paper, we learn a joint vision and\nconcept embedding in the same high-dimensional space. This joint model gives\nthe user fine-grained control over the semantics of the result set, allowing\nthem to explore the catalog of images more rapidly. We model the visual and\nconcept relationships as a graph structure, which captures the rich information\nthrough node neighborhood. This graph structure helps us learn multi-modal node\nembeddings using Graph Neural Networks. We also introduce a novel inference\ntime control, based on selective neighborhood connectivity allowing the user\ncontrol over the retrieval algorithm. We evaluate these multi-modal embeddings\nquantitatively on the downstream relevance task of image retrieval on MS-COCO\ndataset and qualitatively on MS-COCO and an Adobe Stock dataset.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:34:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Misraa", "Aashish Kumar", ""], ["Kale", "Ajinkya", ""], ["Aggarwal", "Pranav", ""], ["Aminian", "Ali", ""]]}, {"id": "2010.02247", "submitter": "Peter Jan van Leeuwen", "authors": "Peter Jan van Leeuwen and Michael DeCaria and Nachiketa Chakaborty and\n  Manuel Pulido", "title": "A Framework for Causal Discovery in non-intervenable systems", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many frameworks exist to infer cause and effect relations in complex\nnonlinear systems but a complete theory is lacking. A new framework is\npresented that is fully nonlinear, provides a complete information theoretic\ndisentanglement of causal processes, allows for nonlinear interactions between\ncauses, identifies the causal strength of missing or unknown processes, and can\nanalyze systems that cannot be represented on standard graphs. The basic\nbuilding blocks are information theoretic measures such as (conditional) mutual\ninformation and a new concept called certainty that monotonically increases\nwith the information available about the target process. The framework is\npresented in detail and compared with other existing frameworks, and the\ntreatment of confounders is discussed. It is tested on several highly\nsimplified stochastic processes to demonstrate how blocking and gateways are\nhandled, and on the chaotic Lorentz 1963 system. It is shown that the framework\nprovides information on the local dynamics, but also reveals information on the\nlarger scale structure of the underlying attractor. While there are systems\nwith structures that the framework cannot disentangle, it is argued that any\ncausal framework that is based on integrated quantities will miss out\npotentially important information of the underlying probability density\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:02:13 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 21:24:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["van Leeuwen", "Peter Jan", ""], ["DeCaria", "Michael", ""], ["Chakaborty", "Nachiketa", ""], ["Pulido", "Manuel", ""]]}, {"id": "2010.02377", "submitter": "Alexander Hoyle", "authors": "Alexander Hoyle, Pranav Goel, Philip Resnik", "title": "Improving Neural Topic Models using Knowledge Distillation", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are often used to identify human-interpretable topics to help\nmake sense of large document collections. We use knowledge distillation to\ncombine the best attributes of probabilistic topic models and pretrained\ntransformers. Our modular method can be straightforwardly applied with any\nneural topic model to improve topic quality, which we demonstrate using two\nmodels having disparate architectures, obtaining state-of-the-art topic\ncoherence. We show that our adaptable framework not only improves performance\nin the aggregate over all estimated topics, as is commonly reported, but also\nin head-to-head comparisons of aligned topics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 22:49:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Hoyle", "Alexander", ""], ["Goel", "Pranav", ""], ["Resnik", "Philip", ""]]}, {"id": "2010.02458", "submitter": "Zhao Wang", "authors": "Zhao Wang and Aron Culotta", "title": "Identifying Spurious Correlations for Robust Text Classification", "comments": "Findings of EMNLP-2020", "journal-ref": "Findings of EMNLP-2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictions of text classifiers are often driven by spurious correlations\n-- e.g., the term `Spielberg' correlates with positively reviewed movies, even\nthough the term itself does not semantically convey a positive sentiment. In\nthis paper, we propose a method to distinguish spurious and genuine\ncorrelations in text classification. We treat this as a supervised\nclassification problem, using features derived from treatment effect estimators\nto distinguish spurious correlations from \"genuine\" ones. Due to the generic\nnature of these features and their small dimensionality, we find that the\napproach works well even with limited training examples, and that it is\npossible to transport the word classifier to new domains. Experiments on four\ndatasets (sentiment classification and toxicity detection) suggest that using\nthis approach to inform feature selection also leads to more robust\nclassification, as measured by improved worst-case accuracy on the samples\naffected by spurious correlations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:49:22 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zhao", ""], ["Culotta", "Aron", ""]]}, {"id": "2010.02573", "submitter": "Phillip Keung", "authors": "Phillip Keung, Yichao Lu, Gy\\\"orgy Szarvas, Noah A. Smith", "title": "The Multilingual Amazon Reviews Corpus", "comments": "To appear in EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale\ncollection of Amazon reviews for multilingual text classification. The corpus\ncontains reviews in English, Japanese, German, French, Spanish, and Chinese,\nwhich were collected between 2015 and 2019. Each record in the dataset contains\nthe review text, the review title, the star rating, an anonymized reviewer ID,\nan anonymized product ID, and the coarse-grained product category (e.g.,\n'books', 'appliances', etc.) The corpus is balanced across the 5 possible star\nratings, so each rating constitutes 20% of the reviews in each language. For\neach language, there are 200,000, 5,000, and 5,000 reviews in the training,\ndevelopment, and test sets, respectively. We report baseline results for\nsupervised text classification and zero-shot cross-lingual transfer learning by\nfine-tuning a multilingual BERT model on reviews data. We propose the use of\nmean absolute error (MAE) instead of classification accuracy for this task,\nsince MAE accounts for the ordinal nature of the ratings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:34:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Keung", "Phillip", ""], ["Lu", "Yichao", ""], ["Szarvas", "Gy\u00f6rgy", ""], ["Smith", "Noah A.", ""]]}, {"id": "2010.02605", "submitter": "Ekaterina Artemova", "authors": "Taisia Glushkova and Alexey Machnev and Alena Fenogenova and Tatiana\n  Shavrina and Ekaterina Artemova and Dmitry I. Ignatov", "title": "DaNetQA: a yes/no Question Answering Dataset for the Russian Language", "comments": "Analysis of Images, Social Networks and Texts - 9 th International\n  Conference, AIST 2020, Skolkovo, Russia, October 15-16, 2020, Revised\n  Selected Papers. Lecture Notes in Computer Science\n  (https://dblp.org/db/series/lncs/index.html), Springer 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DaNetQA, a new question-answering corpus, follows (Clark et. al, 2019)\ndesign: it comprises natural yes/no questions. Each question is paired with a\nparagraph from Wikipedia and an answer, derived from the paragraph. The task is\nto take both the question and a paragraph as input and come up with a yes/no\nanswer, i.e. to produce a binary output. In this paper, we present a\nreproducible approach to DaNetQA creation and investigate transfer learning\nmethods for task and language transferring. For task transferring we leverage\nthree similar sentence modelling tasks: 1) a corpus of paraphrases,\nParaphraser, 2) an NLI task, for which we use the Russian part of XNLI, 3)\nanother question answering task, SberQUAD. For language transferring we use\nEnglish to Russian translation together with multilingual language fine-tuning.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 10:30:48 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 10:36:06 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Glushkova", "Taisia", ""], ["Machnev", "Alexey", ""], ["Fenogenova", "Alena", ""], ["Shavrina", "Tatiana", ""], ["Artemova", "Ekaterina", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "2010.02666", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Sophia Althammer, Michael Schr\\\"oder, Mete\n  Sertkan, Allan Hanbury", "title": "Improving Efficient Neural Ranking Models with Cross-Architecture\n  Knowledge Distillation", "comments": "Updated paper with dense retrieval results and query-level analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval and ranking models are the backbone of many applications such as\nweb search, open domain QA, or text-based recommender systems. The latency of\nneural ranking models at query time is largely dependent on the architecture\nand deliberate choices by their designers to trade-off effectiveness for higher\nefficiency. This focus on low query latency of a rising number of efficient\nranking architectures make them feasible for production deployment. In machine\nlearning an increasingly common approach to close the effectiveness gap of more\nefficient models is to apply knowledge distillation from a large teacher model\nto a smaller student model. We find that different ranking architectures tend\nto produce output scores in different magnitudes. Based on this finding, we\npropose a cross-architecture training procedure with a margin focused loss\n(Margin-MSE), that adapts knowledge distillation to the varying score output\ndistributions of different BERT and non-BERT passage ranking architectures. We\napply the teachable information as additional fine-grained labels to existing\ntraining triples of the MSMARCO-Passage collection. We evaluate our procedure\nof distilling knowledge from state-of-the-art concatenated BERT models to four\ndifferent efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot\nproduct model). We show that across our evaluated architectures our Margin-MSE\nknowledge distillation significantly improves re-ranking effectiveness without\ncompromising their efficiency. Additionally, we show our general distillation\nmethod to improve nearest neighbor based index retrieval with the BERT dot\nproduct model, offering competitive results with specialized and much more\ncostly training methods. To benefit the community, we publish the teacher-score\ntraining files in a ready-to-use package.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:35:53 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 16:24:52 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Althammer", "Sophia", ""], ["Schr\u00f6der", "Michael", ""], ["Sertkan", "Mete", ""], ["Hanbury", "Allan", ""]]}, {"id": "2010.02667", "submitter": "Ruey-Cheng Chen", "authors": "Ruey-Cheng Chen, Chia-Jung Lee", "title": "Incorporating Behavioral Hypotheses for Query Generation", "comments": "EMNLP 2020 short paper, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural networks have been shown effective on query suggestion.\nCommonly posed as a conditional generation problem, the task aims to leverage\nearlier inputs from users in a search session to predict queries that they will\nlikely issue at a later time. User inputs come in various forms such as\nquerying and clicking, each of which can imply different semantic signals\nchanneled through the corresponding behavioral patterns. This paper induces\nthese behavioral biases as hypotheses for query generation, where a generic\nencoder-decoder Transformer framework is presented to aggregate arbitrary\nhypotheses of choice. Our experimental results show that the proposed approach\nleads to significant improvements on top-$k$ word error rate and Bert F1 Score\ncompared to a recent BART model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 12:38:02 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chen", "Ruey-Cheng", ""], ["Lee", "Chia-Jung", ""]]}, {"id": "2010.03073", "submitter": "Cicero Nogueira dos Santos", "authors": "Cicero Nogueira dos Santos, Xiaofei Ma, Ramesh Nallapati, Zhiheng\n  Huang, Bing Xiang", "title": "Beyond [CLS] through Ranking by Generation", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for Information Retrieval, where ranking of documents is\nviewed as the task of generating a query from a document's language model, were\nvery successful in various IR tasks in the past. However, with the advent of\nmodern deep neural networks, attention has shifted to discriminative ranking\nfunctions that model the semantic similarity of documents and queries instead.\nRecently, deep generative models such as GPT2 and BART have been shown to be\nexcellent text generators, but their effectiveness as rankers have not been\ndemonstrated yet. In this work, we revisit the generative framework for\ninformation retrieval and show that our generative approaches are as effective\nas state-of-the-art semantic similarity-based discriminative models for the\nanswer selection task. Additionally, we demonstrate the effectiveness of\nunlikelihood losses for IR.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:56:31 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Santos", "Cicero Nogueira dos", ""], ["Ma", "Xiaofei", ""], ["Nallapati", "Ramesh", ""], ["Huang", "Zhiheng", ""], ["Xiang", "Bing", ""]]}, {"id": "2010.03159", "submitter": "Nguyen Vo", "authors": "Nguyen Vo, Kyumin Lee", "title": "Where Are the Facts? Searching for Fact-checked Information to Alleviate\n  the Spread of Fake News", "comments": "Full paper, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many fact-checking systems have been developed in academia and\nindustry, fake news is still proliferating on social media. These systems\nmostly focus on fact-checking but usually neglect online users who are the main\ndrivers of the spread of misinformation. How can we use fact-checked\ninformation to improve users' consciousness of fake news to which they are\nexposed? How can we stop users from spreading fake news? To tackle these\nquestions, we propose a novel framework to search for fact-checking articles,\nwhich address the content of an original tweet (that may contain\nmisinformation) posted by online users. The search can directly warn fake news\nposters and online users (e.g. the posters' followers) about misinformation,\ndiscourage them from spreading fake news, and scale up verified content on\nsocial media. Our framework uses both text and images to search for\nfact-checking articles, and achieves promising results on real-world datasets.\nOur code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:55:34 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""]]}, {"id": "2010.03240", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, Xiangnan He", "title": "Bias and Debias in Recommender System: A Survey and Future Directions", "comments": "20 pages, submitting to TKDE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent years have witnessed a rapid growth of research papers on\nrecommender system (RS), most of the papers focus on inventing machine learning\nmodels to better fit user behavior data. However, user behavior data is\nobservational rather than experimental. This makes various biases widely exist\nin the data, including but not limited to selection bias, position bias,\nexposure bias, and popularity bias. Blindly fitting the data without\nconsidering the inherent biases will result in many serious issues, e.g., the\ndiscrepancy between offline evaluation and online metrics, hurting user\nsatisfaction and trust on the recommendation service, etc. To transform the\nlarge volume of research models into practical improvements, it is highly\nurgent to explore the impacts of the biases and perform debiasing when\nnecessary. When reviewing the papers that consider biases in RS, we find that,\nto our surprise, the studies are rather fragmented and lack a systematic\norganization. The terminology \"bias\" is widely used in the literature, but its\ndefinition is usually vague and even inconsistent across papers. This motivates\nus to provide a systematic survey of existing work on RS biases. In this paper,\nwe first summarize seven types of biases in recommendation, along with their\ndefinitions and characteristics. We then provide a taxonomy to position and\norganize the existing work on recommendation debiasing. Finally, we identify\nsome open challenges and envision some future directions, with the hope of\ninspiring more research work on this important yet less investigated topic.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:44:30 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Chen", "Jiawei", ""], ["Dong", "Hande", ""], ["Wang", "Xiang", ""], ["Feng", "Fuli", ""], ["Wang", "Meng", ""], ["He", "Xiangnan", ""]]}, {"id": "2010.03343", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "Slice-Aware Neural Ranking", "comments": "Paper accepted to EMNLP workshop SCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding when and why neural ranking models fail for an IR task via\nerror analysis is an important part of the research cycle. Here we focus on the\nchallenges of (i) identifying categories of difficult instances (a pair of\nquestion and response candidates) for which a neural ranker is ineffective and\n(ii) improving neural ranking for such instances. To address both challenges we\nresort to slice-based learning for which the goal is to improve effectiveness\nof neural models for slices (subsets) of data. We address challenge (i) by\nproposing different slicing functions (SFs) that select slices of the\ndataset---based on prior work we heuristically capture different failures of\nneural rankers. Then, for challenge (ii) we adapt a neural ranking model to\nlearn slice-aware representations, i.e. the adapted model learns to represent\nthe question and responses differently based on the model's prediction of which\nslices they belong to. Our experimental results (the source code and data are\navailable at https://github.com/Guzpenha/slice_based_learning) across three\ndifferent ranking tasks and four corpora show that slice-based learning\nimproves the effectiveness by an average of 2% over a neural ranker that is not\nslice-aware.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 11:40:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2010.03544", "submitter": "Nima Ebadi", "authors": "Nima Ebadi, Peyman Najafirad", "title": "A Self-supervised Approach for Semantic Indexing in the Context of\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic has accelerated the pace at which COVID-19 scientific papers are\npublished. In addition, the process of manually assigning semantic indexes to\nthese papers by experts is even more time-consuming and overwhelming in the\ncurrent health crisis. Therefore, there is an urgent need for automatic\nsemantic indexing models which can effectively scale-up to newly introduced\nconcepts and rapidly evolving distributions of the hyperfocused related\nliterature. In this research, we present a novel semantic indexing approach\nbased on the state-of-the-art self-supervised representation learning and\ntransformer encoding exclusively suitable for pandemic crises. We present a\ncase study on a novel dataset that is based on COVID-19 papers published and\nmanually indexed in PubMed. Our study shows that our self-supervised model\noutperforms the best performing models of BioASQ Task 8a by micro-F1 score of\n0.1 and LCA-F score of 0.08 on average. Our model also shows superior\nperformance on detecting the supplementary concepts which is quite important\nwhen the focus of the literature has drastically shifted towards specific\nconcepts related to the pandemic. Our study sheds light on the main challenges\nconfronting semantic indexing models during a pandemic, namely new domains and\ndrastic changes of their distributions, and as a superior alternative for such\nsituations, propose a model founded on approaches which have shown auspicious\nperformance in improving generalization and data efficiency in various NLP\ntasks. We also show the joint indexing of major Medical Subject Headings (MeSH)\nand supplementary concepts improves the overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:43:55 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Ebadi", "Nima", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2010.03710", "submitter": "Yihuang Kang", "authors": "Sheng-Tai Huang, Yihuang Kang, Shao-Min Hung, Bowen Kuo, I-Ling Cheng", "title": "Topic Diffusion Discovery Based on Deep Non-negative Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers have been overwhelmed by the explosion of research articles\npublished by various research communities. Many research scholarly websites,\nsearch engines, and digital libraries have been created to help researchers\nidentify potential research topics and keep up with recent progress on research\nof interests. However, it is still difficult for researchers to keep track of\nthe research topic diffusion and evolution without spending a large amount of\ntime reviewing numerous relevant and irrelevant articles. In this paper, we\nconsider a novel topic diffusion discovery technique. Specifically, we propose\nusing a Deep Non-negative Autoencoder with information divergence measurement\nthat monitors evolutionary distance of the topic diffusion to understand how\nresearch topics change with time. The experimental results show that the\nproposed approach is able to identify the evolution of research topics as well\nas to discover topic diffusions in online fashions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 00:58:10 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Huang", "Sheng-Tai", ""], ["Kang", "Yihuang", ""], ["Hung", "Shao-Min", ""], ["Kuo", "Bowen", ""], ["Cheng", "I-Ling", ""]]}, {"id": "2010.03824", "submitter": "Tom Hope", "authors": "Tom Hope, Aida Amini, David Wadden, Madeleine van Zuylen, Sravanthi\n  Parasa, Eric Horvitz, Daniel Weld, Roy Schwartz, Hannaneh Hajishirzi", "title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers", "comments": "Accepted to NAACL 2021 (long paper). Tom Hope and Aida Amini made an\n  equal contribution. Data and code: https://git.io/JUhv7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has spawned a diverse body of scientific literature\nthat is challenging to navigate, stimulating interest in automated tools to\nhelp find useful knowledge. We pursue the construction of a knowledge base (KB)\nof mechanisms -- a fundamental concept across the sciences encompassing\nactivities, functions and causal relations, ranging from cellular processes to\neconomic impacts. We extract this information from the natural language of\nscientific papers by developing a broad, unified schema that strikes a balance\nbetween relevance and breadth. We annotate a dataset of mechanisms with our\nschema and train a model to extract mechanism relations from papers. Our\nexperiments demonstrate the utility of our KB in supporting interdisciplinary\nscientific search over COVID-19 literature, outperforming the prominent PubMed\nsearch in a study with clinical experts.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 07:54:14 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 19:40:19 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 10:59:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hope", "Tom", ""], ["Amini", "Aida", ""], ["Wadden", "David", ""], ["van Zuylen", "Madeleine", ""], ["Parasa", "Sravanthi", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel", ""], ["Schwartz", "Roy", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2010.04125", "submitter": "Kun Zhou", "authors": "Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang and Ji-Rong Wen", "title": "Towards Topic-Guided Conversational Recommender System", "comments": "12 pages, Accepted by Coling2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. To develop an effective CRS, the\nsupport of high-quality datasets is essential. Existing CRS datasets mainly\nfocus on immediate requests from users, while lack proactive guidance to the\nrecommendation scenario. In this paper, we contribute a new CRS dataset named\n\\textbf{TG-ReDial} (\\textbf{Re}commendation through\n\\textbf{T}opic-\\textbf{G}uided \\textbf{Dial}og). Our dataset has two major\nfeatures. First, it incorporates topic threads to enforce natural semantic\ntransitions towards the recommendation scenario. Second, it is created in a\nsemi-automatic way, hence human annotation is more reasonable and controllable.\nBased on TG-ReDial, we present the task of topic-guided conversational\nrecommendation, and propose an effective approach to this task. Extensive\nexperiments have demonstrated the effectiveness of our approach on three\nsub-tasks, namely topic prediction, item recommendation and response\ngeneration. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:04:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:25:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Kun", ""], ["Zhou", "Yuanhang", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Xiaoke", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.04260", "submitter": "Akbar Siami Namin", "authors": "Faranak Abri, Luis Felipe Gutierrez, Akbar Siami Namin, Keith S.\n  Jones, David R. W. Sears", "title": "Fake Reviews Detection through Analysis of Linguistic Features", "comments": "The pre-print of a paper to appear in the proceedings of the IEEE\n  International Conference on Machine Learning Applications (ICMLA 2020), 11\n  pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews play an integral part for success or failure of businesses.\nPrior to purchasing services or goods, customers first review the online\ncomments submitted by previous customers. However, it is possible to\nsuperficially boost or hinder some businesses through posting counterfeit and\nfake reviews. This paper explores a natural language processing approach to\nidentify fake reviews. We present a detailed analysis of linguistic features\nfor distinguishing fake and trustworthy online reviews. We study 15 linguistic\nfeatures and measure their significance and importance towards the\nclassification schemes employed in this study. Our results indicate that fake\nreviews tend to include more redundant terms and pauses, and generally contain\nlonger sentences. The application of several machine learning classification\nalgorithms revealed that we were able to discriminate fake from real reviews\nwith high accuracy using these linguistic features.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:16:30 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Abri", "Faranak", ""], ["Gutierrez", "Luis Felipe", ""], ["Namin", "Akbar Siami", ""], ["Jones", "Keith S.", ""], ["Sears", "David R. W.", ""]]}, {"id": "2010.04321", "submitter": "Alexandra DeLucia", "authors": "Alexandra DeLucia, Elisabeth Moore", "title": "Analyzing HPC Support Tickets: Experience and Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance computing (HPC) user support teams are the first line of\ndefense against large-scale problems, as they are often the first to learn of\nproblems reported by users. Developing tools to better assist support teams in\nsolving user problems and tracking issue trends is critical for maintaining\nsystem health. Our work examines the Los Alamos National Laboratory HPC Consult\nTeam's user support ticketing system and develops proof of concept tools to\nautomate tasks such as category assignment and similar ticket recommendation.\nWe also generate new categories for reporting and discuss ideas to improve\nfuture ticketing systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 01:40:47 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["DeLucia", "Alexandra", ""], ["Moore", "Elisabeth", ""]]}, {"id": "2010.04335", "submitter": "Priyanshu Kumar", "authors": "Priyanshu Kumar and Aadarsh Singh", "title": "NutCracker at WNUT-2020 Task 2: Robustly Identifying Informative\n  COVID-19 Tweets using Ensembling and Adversarial Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We experiment with COVID-Twitter-BERT and RoBERTa models to identify\ninformative COVID-19 tweets. We further experiment with adversarial training to\nmake our models robust. The ensemble of COVID-Twitter-BERT and RoBERTa obtains\na F1-score of 0.9096 (on the positive class) on the test data of WNUT-2020 Task\n2 and ranks 1st on the leaderboard. The ensemble of the models trained using\nadversarial training also produces similar result.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 02:46:51 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kumar", "Priyanshu", ""], ["Singh", "Aadarsh", ""]]}, {"id": "2010.04388", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, S\\\"oren Auer", "title": "Sentence, Phrase, and Triple Annotations to Build a Knowledge Graph of\n  Natural Language Processing Contributions -- A Trial Dataset", "comments": "22 pages, 9 figures, 4 tables", "journal-ref": "Journal of Data and Information Science, 6(3) (2021)", "doi": "10.2478/jdis-2021-0023", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Purpose: The aim of this work is to normalize the NLPCONTRIBUTIONS scheme\n(henceforward, NLPCONTRIBUTIONGRAPH) to structure, directly from article\nsentences, the contributions information in Natural Language Processing (NLP)\nscholarly articles via a two-stage annotation methodology: 1) pilot stage - to\ndefine the scheme (described in prior work); and 2) adjudication stage - to\nnormalize the graphing model (the focus of this paper).\n  Design/methodology/approach: We re-annotate, a second time, the\ncontributions-pertinent information across 50 prior-annotated NLP scholarly\narticles in terms of a data pipeline comprising: contribution-centered\nsentences, phrases, and triple statements. To this end, specifically, care was\ntaken in the adjudication annotation stage to reduce annotation noise while\nformulating the guidelines for our proposed novel NLP contributions structuring\nand graphing scheme.\n  Findings: The application of NLPCONTRIBUTIONGRAPH on the 50 articles resulted\nfinally in a dataset of 900 contribution-focused sentences, 4,702\ncontribution-information-centered phrases, and 2,980 surface-structured\ntriples. The intra-annotation agreement between the first and second stages, in\nterms of F1, was 67.92% for sentences, 41.82% for phrases, and 22.31% for\ntriple statements indicating that with increased granularity of the\ninformation, the annotation decision variance is greater.\n  Practical Implications: We demonstrate NLPCONTRIBUTIONGRAPH data integrated\ninto the Open Research Knowledge Graph (ORKG), a next-generation KG-based\ndigital library with intelligent computations enabled over structured scholarly\nknowledge, as a viable aid to assist researchers in their day-to-day tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 06:45:35 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:24:45 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 06:08:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2010.04484", "submitter": "Qixuan Wang", "authors": "Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu and Ji-Rong Wen", "title": "Revisiting Alternative Experimental Settings for Evaluating Top-N Item\n  Recommendation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-N item recommendation has been a widely studied task from implicit\nfeedback. Although much progress has been made with neural methods, there is\nincreasing concern on appropriate evaluation of recommendation algorithms. In\nthis paper, we revisit alternative experimental settings for evaluating top-N\nrecommendation algorithms, considering three important factors, namely dataset\nsplitting, sampled metrics and domain selection. We select eight representative\nrecommendation algorithms (covering both traditional and neural methods) and\nconstruct extensive experiments on a very large dataset. By carefully\nrevisiting different options, we make several important findings on the three\nfactors, which directly provide useful suggestions on how to appropriately set\nup the experiments for top-N item recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:15:28 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Zhao", "Wayne Xin", ""], ["Chen", "Junhua", ""], ["Wang", "Pengfei", ""], ["Gu", "Qi", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.04486", "submitter": "Pierangelo Lombardo", "authors": "Pierangelo Lombardo, Alessio Boiardi, Luca Colombo, Angelo Schiavone,\n  Nicol\\`o Tamagnone", "title": "Top-Rank-Focused Adaptive Vote Collection for the Evaluation of\n  Domain-Specific Semantic Models", "comments": "This is a pre-print of an article published in the proceedings of the\n  2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "journal-ref": "Proceedings of the 2020 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP) (pp. 3081-3093)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of domain-specific applications of semantic models, boosted by the\nrecent achievements of unsupervised embedding learning algorithms, demands\ndomain-specific evaluation datasets. In many cases, content-based recommenders\nbeing a prime example, these models are required to rank words or texts\naccording to their semantic relatedness to a given concept, with particular\nfocus on top ranks. In this work, we give a threefold contribution to address\nthese requirements: (i) we define a protocol for the construction, based on\nadaptive pairwise comparisons, of a relatedness-based evaluation dataset\ntailored on the available resources and optimized to be particularly accurate\nin top-rank evaluation; (ii) we define appropriate metrics, extensions of\nwell-known ranking correlation coefficients, to evaluate a semantic model via\nthe aforementioned dataset by taking into account the greater significance of\ntop ranks. Finally, (iii) we define a stochastic transitivity model to simulate\nsemantic-driven pairwise comparisons, which confirms the effectiveness of the\nproposed dataset construction protocol.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 10:20:58 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lombardo", "Pierangelo", ""], ["Boiardi", "Alessio", ""], ["Colombo", "Luca", ""], ["Schiavone", "Angelo", ""], ["Tamagnone", "Nicol\u00f2", ""]]}, {"id": "2010.04609", "submitter": "Guohou Shan", "authors": "Guohou Shan, James Foulds, Shimei Pan", "title": "Causal Feature Selection with Dimension Reduction for Interpretable Text\n  Classification", "comments": "11 pages, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text features that are correlated with class labels, but do not directly\ncause them, are sometimesuseful for prediction, but they may not be insightful.\nAs an alternative to traditional correlation-basedfeature selection, causal\ninference could reveal more principled, meaningful relationships betweentext\nfeatures and labels. To help researchers gain insight into text data, e.g. for\nsocial scienceapplications, in this paper we investigate a class of\nmatching-based causal inference methods fortext feature selection. Features\nused in document classification are often high dimensional, howeverexisting\ncausal feature selection methods use Propensity Score Matching (PSM) which is\nknown to beless effective in high-dimensional spaces. We propose a new causal\nfeature selection framework thatcombines dimension reduction with causal\ninference to improve text feature selection. Experiments onboth synthetic and\nreal-world data demonstrate the promise of our methods in improving\nclassificationand enhancing interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:36:49 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Shan", "Guohou", ""], ["Foulds", "James", ""], ["Pan", "Shimei", ""]]}, {"id": "2010.04638", "submitter": "Christopher Hahne", "authors": "Christopher Hahne, Amar Aggoun, Vladan Velisavljevic, Susanne Fiebig,\n  Matthias Pesch", "title": "Baseline and Triangulation Geometry in a Standard Plenoptic Camera", "comments": "clarified remarks around Eqs.(16-17)", "journal-ref": "International Journal of Computer Vision, volume 126, pages 21-35\n  (2018)", "doi": "10.1007/s11263-017-1036-4", "report-no": null, "categories": "cs.IR cs.CG cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate light field triangulation to determine depth\ndistances and baselines in a plenoptic camera. Advances in micro lenses and\nimage sensors have enabled plenoptic cameras to capture a scene from different\nviewpoints with sufficient spatial resolution. While object distances can be\ninferred from disparities in a stereo viewpoint pair using triangulation, this\nconcept remains ambiguous when applied in the case of plenoptic cameras. We\npresent a geometrical light field model allowing the triangulation to be\napplied to a plenoptic camera in order to predict object distances or specify\nbaselines as desired. It is shown that distance estimates from our novel method\nmatch those of real objects placed in front of the camera. Additional benchmark\ntests with an optical design software further validate the model's accuracy\nwith deviations of less than +-0.33 % for several main lens types and focus\nsettings. A variety of applications in the automotive and robotics field can\nbenefit from this estimation model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:31:14 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 12:02:36 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Hahne", "Christopher", ""], ["Aggoun", "Amar", ""], ["Velisavljevic", "Vladan", ""], ["Fiebig", "Susanne", ""], ["Pesch", "Matthias", ""]]}, {"id": "2010.04665", "submitter": "Seraphina Goldfarb-Tarrant", "authors": "Seraphina Goldfarb-Tarrant, Alexander Robertson, Jasmina Lazic,\n  Theodora Tsouloufi, Louise Donnison, Karen Smyth", "title": "Scaling Systematic Literature Reviews with Machine Learning Pipelines", "comments": "In EMNLP 2020 Scholarly Document Processing Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Systematic reviews, which entail the extraction of data from large numbers of\nscientific documents, are an ideal avenue for the application of machine\nlearning. They are vital to many fields of science and philanthropy, but are\nvery time-consuming and require experts. Yet the three main stages of a\nsystematic review are easily done automatically: searching for documents can be\ndone via APIs and scrapers, selection of relevant documents can be done via\nbinary classification, and extraction of data can be done via\nsequence-labelling classification. Despite the promise of automation for this\nfield, little research exists that examines the various ways to automate each\nof these tasks. We construct a pipeline that automates each of these aspects,\nand experiment with many human-time vs. system quality trade-offs. We test the\nability of classifiers to work well on small amounts of data and to generalise\nto data from countries not represented in the training data. We test different\ntypes of data extraction with varying difficulty in annotation, and five\ndifferent neural architectures to do the extraction. We find that we can get\nsurprising accuracy and generalisability of the whole pipeline system with only\n2 weeks of human-expert annotation, which is only 15% of the time it takes to\ndo the whole review manually and can be repeated and extended to new data with\nno additional effort.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:19:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Goldfarb-Tarrant", "Seraphina", ""], ["Robertson", "Alexander", ""], ["Lazic", "Jasmina", ""], ["Tsouloufi", "Theodora", ""], ["Donnison", "Louise", ""], ["Smyth", "Karen", ""]]}, {"id": "2010.04880", "submitter": "Debasish Chakroborti", "authors": "Debasish Chakroborti, Banani Roy, Sristy Sumana Nath", "title": "Designing for Recommending Intermediate States in A Scientific Workflow\n  Management System", "comments": "Preprint, 13th Engineering Interactive Computing Systems (EICS)\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To process a large amount of data sequentially and systematically, proper\nmanagement of workflow components (i.e., modules, data, configurations,\nassociations among ports and links) in a Scientific Workflow Management System\n(SWfMS) is inevitable. Managing data with provenance in a SWfMS to support\nreusability of workflows, modules, and data is not a simple task. Handling such\ncomponents is even more burdensome for frequently assembled and executed\ncomplex workflows for investigating large datasets with different technologies\n(i.e., various learning algorithms or models). However, a great many studies\npropose various techniques and technologies for managing and recommending\nservices in a SWfMS, but only a very few studies consider the management of\ndata in a SWfMS for efficient storing and facilitating workflow executions.\nFurthermore, there is no study to inquire about the effectiveness and\nefficiency of such data management in a SWfMS from a user perspective. In this\npaper, we present and evaluate a GUI version of such a novel approach of\nintermediate data management with two use cases (Plant Phenotyping and\nBioinformatics). The technique we call GUI-RISPTS (Recommending Intermediate\nStates from Pipelines Considering Tool-States) can facilitate executions of\nworkflows with processed data (i.e., intermediate outcomes of modules in a\nworkflow) and can thus reduce the computational time of some modules in a\nSWfMS. We integrated GUI-RISPTS with an existing workflow management system\ncalled SciWorCS. In SciWorCS, we present an interface that users use for\nselecting the recommendation of intermediate states (i.e., modules' outcomes).\nWe investigated GUI-RISP's effectiveness from users' perspectives along with\nmeasuring its overhead in terms of storage and efficiency in workflow\nexecution.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:31:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chakroborti", "Debasish", ""], ["Roy", "Banani", ""], ["Nath", "Sristy Sumana", ""]]}, {"id": "2010.04898", "submitter": "Svitlana Vakulenko", "authors": "Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre,\n  Stephen Pulman, Srinivas Chappidi", "title": "Open-Domain Question Answering Goes Conversational via Question\n  Rewriting", "comments": "15 pages, 10 tables, 3 figures, accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dataset for Question Rewriting in Conversational Context\n(QReCC), which contains 14K conversations with 80K question-answer pairs. The\ntask in QReCC is to find answers to conversational questions within a\ncollection of 10M web pages (split into 54M passages). Answers to questions in\nthe same conversation may be distributed across several web pages. QReCC\nprovides annotations that allow us to train and evaluate individual subtasks of\nquestion rewriting, passage retrieval and reading comprehension required for\nthe end-to-end conversational question answering (QA) task. We report the\neffectiveness of a strong baseline approach that combines the state-of-the-art\nmodel for question rewriting, and competitive models for open-domain QA. Our\nresults set the first baseline for the QReCC dataset with F1 of 19.10, compared\nto the human upper bound of 75.45, indicating the difficulty of the setup and a\nlarge room for improvement.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 04:28:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:01:39 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 19:09:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Anantha", "Raviteja", ""], ["Vakulenko", "Svitlana", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""], ["Pulman", "Stephen", ""], ["Chappidi", "Srinivas", ""]]}, {"id": "2010.04971", "submitter": "Issa Annamoradnejad", "authors": "Navid Khezrian, Jafar Habibi, Issa Annamoradnejad", "title": "Tag Recommendation for Online Q&A Communities based on BERT Pre-Training\n  Technique", "comments": "5 pages, initial results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Q&A and open source communities use tags and keywords to index,\ncategorize, and search for specific content. The most obvious advantage of tag\nrecommendation is the correct classification of information. In this study, we\nused the BERT pre-training technique in tag recommendation task for online Q&A\nand open-source communities for the first time. Our evaluation on freecode\ndatasets show that the proposed method, called TagBERT, is more accurate\ncompared to deep learning and other baseline methods. Moreover, our model\nachieved a high stability by solving the problem of previous researches, where\nincreasing the number of tag recommendations significantly reduced model\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 10:52:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Khezrian", "Navid", ""], ["Habibi", "Jafar", ""], ["Annamoradnejad", "Issa", ""]]}, {"id": "2010.05025", "submitter": "Hyuk-Yoon Kwon", "authors": "Han-Sub Shin and Hyuk-Yoon Kwon", "title": "Weakly Supervised Learning for Judging the Credibility of Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we deal with the problem of judging the credibility of movie\nreviews. The problem is challenging because even experts cannot clearly and\nefficiently judge the credibility of a movie review and the number of movie\nreviews is very large. To attack this problem, we propose a weakly supervised\nlearning method for fast annotation. In terms of predefined criteria for weakly\nsupervised learning, we present a simple and clear criterion based on\nhistorical movie ratings associated with movie reviewers. The proposed method\nhas the following two advantages. First, it is significantly efficient because\nwe can annotate the entire data sets according to the predefined rule. Indeed,\nwe show that the proposed method can annotate 8,000 movie reviews only in 0.712\nseconds. Second, a criterion adapted for weakly supervised learning is simple\nbut effective. We use as a comparison learning method that uses the helpfulness\nvotes of other reviewers as the criterion to judge the credibility of movie\nreviews, which has been widely used to judge the credibility of online reviews.\nWe indicate that the proposed learning method is comparable to or even better\nthan the helpfulness vote method by showing an improvement over the accuracy of\nthe latter method of 1.57% $~ 4.54%.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 15:16:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Shin", "Han-Sub", ""], ["Kwon", "Hyuk-Yoon", ""]]}, {"id": "2010.05037", "submitter": "Samuel Hsia", "authors": "Samuel Hsia, Udit Gupta, Mark Wilkening, Carole-Jean Wu, Gu-Yeon Wei\n  and David Brooks", "title": "Cross-Stack Workload Characterization of Deep Recommendation Systems", "comments": "Published in 2020 IEEE International Symposium on Workload\n  Characterization (IISWC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based recommendation systems form the backbone of most\npersonalized cloud services. Though the computer architecture community has\nrecently started to take notice of deep recommendation inference, the resulting\nsolutions have taken wildly different approaches - ranging from near memory\nprocessing to at-scale optimizations. To better design future hardware systems\nfor deep recommendation inference, we must first systematically examine and\ncharacterize the underlying systems-level impact of design decisions across the\ndifferent levels of the execution stack. In this paper, we characterize eight\nindustry-representative deep recommendation models at three different levels of\nthe execution stack: algorithms and software, systems platforms, and hardware\nmicroarchitectures. Through this cross-stack characterization, we first show\nthat system deployment choices (i.e., CPUs or GPUs, batch size granularity) can\ngive us up to 15x speedup. To better understand the bottlenecks for further\noptimization, we look at both software operator usage breakdown and CPU\nfrontend and backend microarchitectural inefficiencies. Finally, we model the\ncorrelation between key algorithmic model architecture features and hardware\nbottlenecks, revealing the absence of a single dominant algorithmic component\nbehind each hardware bottleneck.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 16:11:43 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hsia", "Samuel", ""], ["Gupta", "Udit", ""], ["Wilkening", "Mark", ""], ["Wu", "Carole-Jean", ""], ["Wei", "Gu-Yeon", ""], ["Brooks", "David", ""]]}, {"id": "2010.05317", "submitter": "Sai Prabhakar Pandi Selvaraj", "authors": "Dhruvesh Patel, Sandeep Konam, Sai P. Selvaraj", "title": "Weakly Supervised Medication Regimen Extraction from Medical\n  Conversations", "comments": "To appear in the Proceedings of the Clinical Natural Language\n  Processing Workshop, EMNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Medication Regimen (MR) extraction from medical conversations can\nnot only improve recall and help patients follow through with their care plan,\nbut also reduce the documentation burden for doctors. In this paper, we focus\non extracting spans for frequency, route and change, corresponding to\nmedications discussed in the conversation. We first describe a unique dataset\nof annotated doctor-patient conversations and then present a weakly supervised\nmodel architecture that can perform span extraction using noisy classification\ndata. The model utilizes an attention bottleneck inside a classification model\nto perform the extraction. We experiment with several variants of attention\nscoring and projection functions and propose a novel transformer-based\nattention scoring function (TAScore). The proposed combination of TAScore and\nFusedmax projection achieves a 10 point increase in Longest Common Substring F1\ncompared to the baseline of additive scoring plus softmax projection.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 18:53:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Patel", "Dhruvesh", ""], ["Konam", "Sandeep", ""], ["Selvaraj", "Sai P.", ""]]}, {"id": "2010.05349", "submitter": "Rahim Dehkharghani", "authors": "Ali Najafi, Araz Gholipour-Shilabin, Rahim Dehkharghani, Ali\n  Mohammadpur-Fard, Meysam Asgari-Chenaghlu", "title": "ComStreamClust: a communicative multi-agent approach to text clustering\n  in streaming data", "comments": "11 pages, 6 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic detection is the task of determining and tracking hot topics in social\nmedia. Twitter is arguably the most popular platform for people to share their\nideas with others about different issues. One such prevalent issue is the\nCOVID-19 pandemic. Detecting and tracking topics on these kinds of issues would\nhelp governments and healthcare companies deal with this phenomenon. In this\npaper, we propose a novel, multi-agent, communicative clustering approach,\nso-called ComStreamClust for clustering sub-topics inside a broader topic,\ne.g., COVID-19. The proposed approach is parallelizable, and can simultaneously\nhandle several data-point. The LaBSE sentence embedding is used to measure the\nsemantic similarity between two tweets. ComStreamClust has been evaluated on\ntwo datasets: the COVID-19 and the FA CUP. The results obtained from\nComStreamClust approve the effectiveness of the proposed approach when compared\nto existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 21:19:19 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:58:49 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Najafi", "Ali", ""], ["Gholipour-Shilabin", "Araz", ""], ["Dehkharghani", "Rahim", ""], ["Mohammadpur-Fard", "Ali", ""], ["Asgari-Chenaghlu", "Meysam", ""]]}, {"id": "2010.05525", "submitter": "Xiaoyong Yang", "authors": "Xiaoyong Yang, Yadong Zhu, Yi Zhang, Xiaobo Wang, Quan Yuan", "title": "Large Scale Product Graph Construction for Recommendation in E-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a recommendation system that serves billions of users on daily basis\nis a challenging problem, as the system needs to make astronomical number of\npredictions per second based on real-time user behaviors with O(1) time\ncomplexity. Such kind of large scale recommendation systems usually rely\nheavily on pre-built index of products to speedup the recommendation service so\nthat online user waiting time is un-noticeable. One important indexing\nstructure is the product-product index, where one can retrieval a list of\nranked products given a seed product. The index can be viewed as a weighted\nproduct-product graph. In this paper, we present our novel technologies to\nefficiently build such kind of indexed product graphs. In particular, we\npropose the Swing algorithm to capture the substitute relationships between\nproducts, which can utilize the substructures of user-item click bi-partitive\ngraph. Then we propose the Surprise algorithm for the modeling of complementary\nproduct relationships, which utilizes product category information and solves\nthe sparsity problem of user co-purchasing graph via clustering technique. Base\non these two approaches, we can build the basis product graph for\nrecommendation in Taobao. The approaches are evaluated comprehensively with\nboth offline and online experiments, and the results demonstrate the\neffectiveness and efficiency of the work.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:27:30 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yang", "Xiaoyong", ""], ["Zhu", "Yadong", ""], ["Zhang", "Yi", ""], ["Wang", "Xiaobo", ""], ["Yuan", "Quan", ""]]}, {"id": "2010.05569", "submitter": "Suranjana Samanta", "authors": "Suranjana Samanta, Ajay Gupta, Prateeti Mohapatra, Amar Prakash Azad", "title": "Carbon to Diamond: An Incident Remediation Assistant System From Site\n  Reliability Engineers' Conversations in Hybrid Cloud Operations", "comments": "6 Pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational channels are changing the landscape of hybrid cloud service\nmanagement. These channels are becoming important avenues for Site Reliability\nEngineers (SREs) %Subject Matter Experts (SME) to collaboratively work together\nto resolve an incident or issue. Identifying segmented conversations and\nextracting key insights or artefacts from them can help engineers to improve\nthe efficiency of the incident remediation process by using information\nretrieval mechanisms for similar incidents. However, it has been empirically\nobserved that due to the semi-formal behavior of such conversations (human\nlanguage) they are very unique in nature and also contain lot of\ndomain-specific terms. This makes it difficult to use the standard natural\nlanguage processing frameworks directly, which are popularly used in standard\nNLP tasks. %It is important to identify the correct keywords and artefacts like\nsymptoms, issue etc., present in the conversation chats. In this paper, we\nbuild a framework that taps into the conversational channels and uses various\nlearning methods to (a) understand and extract key artefacts from conversations\nlike diagnostic steps and resolution actions taken, and (b) present an approach\nto identify past conversations about similar issues. Experimental results on\nour dataset show the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:43:35 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Samanta", "Suranjana", ""], ["Gupta", "Ajay", ""], ["Mohapatra", "Prateeti", ""], ["Azad", "Amar Prakash", ""]]}, {"id": "2010.05894", "submitter": "Wenqi Jiang", "authors": "Wenqi Jiang, Zhenhao He, Shuai Zhang, Thomas B. Preu{\\ss}er, Kai Zeng,\n  Liang Feng, Jiansong Zhang, Tongxuan Liu, Yong Li, Jingren Zhou, Ce Zhang,\n  Gustavo Alonso", "title": "MicroRec: Efficient Recommendation Inference by Hardware and Data\n  Structure Solutions", "comments": "Accepted by MLSys'21 (the 4th Conference on Machine Learning and\n  Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are widely used in personalized recommendation systems.\nUnlike regular DNN inference workloads, recommendation inference is\nmemory-bound due to the many random memory accesses needed to lookup the\nembedding tables. The inference is also heavily constrained in terms of latency\nbecause producing a recommendation for a user must be done in about tens of\nmilliseconds. In this paper, we propose MicroRec, a high-performance inference\nengine for recommendation systems. MicroRec accelerates recommendation\ninference by (1) redesigning the data structures involved in the embeddings to\nreduce the number of lookups needed and (2) taking advantage of the\navailability of High-Bandwidth Memory (HBM) in FPGA accelerators to tackle the\nlatency by enabling parallel lookups. We have implemented the resulting design\non an FPGA board including the embedding lookup step as well as the complete\ninference process. Compared to the optimized CPU baseline (16 vCPU,\nAVX2-enabled), MicroRec achieves 13.8~14.7x speedup on embedding lookup alone\nand 2.5$~5.4x speedup for the entire recommendation inference in terms of\nthroughput. As for latency, CPU-based engines needs milliseconds for inferring\na recommendation while MicroRec only takes microseconds, a significant\nadvantage in real-time recommendation systems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:42:30 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 10:44:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Jiang", "Wenqi", ""], ["He", "Zhenhao", ""], ["Zhang", "Shuai", ""], ["Preu\u00dfer", "Thomas B.", ""], ["Zeng", "Kai", ""], ["Feng", "Liang", ""], ["Zhang", "Jiansong", ""], ["Liu", "Tongxuan", ""], ["Li", "Yong", ""], ["Zhou", "Jingren", ""], ["Zhang", "Ce", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2010.05987", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Arman Cohan, Nazli Goharian", "title": "SLEDGE-Z: A Zero-Shot Baseline for COVID-19 Literature Search", "comments": "EMNLP 2020. This article draws heavily from arXiv:2005.02365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific\nliterature on the virus. Clinicians, researchers, and policy-makers need to be\nable to search these articles effectively. In this work, we present a zero-shot\nranking algorithm that adapts to COVID-related scientific literature. Our\napproach filters training data from another collection down to medical-related\nqueries, uses a neural re-ranking model pre-trained on scientific text\n(SciBERT), and filters the target document collection. This approach ranks top\namong zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a\nP@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2\njudgments. Despite not relying on TREC-COVID data, our method outperforms\nmodels that do. As one of the first search methods to thoroughly evaluate\nCOVID-19 search, we hope that this serves as a strong baseline and helps in the\nglobal crisis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:28:29 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["MacAvaney", "Sean", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2010.06070", "submitter": "Ramin Raziperchikolaei", "authors": "Ramin Raziperchikolaei, Tianyu Li, Young-joo Chung", "title": "Neural Representations in Hybrid Recommender Systems: Prediction versus\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoder-based hybrid recommender systems have become popular recently\nbecause of their ability to learn user and item representations by\nreconstructing various information sources, including users' feedback on items\n(e.g., ratings) and side information of users and items (e.g., users'\noccupation and items' title). However, existing systems still use\nrepresentations learned by matrix factorization (MF) to predict the rating,\nwhile using representations learned by neural networks as the regularizer. In\nthis paper, we define the neural representation for prediction (NRP) framework\nand apply it to the autoencoder-based recommendation systems. We theoretically\nanalyze how our objective function is related to the previous MF and\nautoencoder-based methods and explain what it means to use neural\nrepresentations as the regularizer. We also apply the NRP framework to a direct\nneural network structure which predicts the ratings without reconstructing the\nuser and item information. We conduct extensive experiments on two MovieLens\ndatasets and two real-world e-commerce datasets. The results confirm that\nneural representations are better for prediction than regularization and show\nthat the NRP framework, combined with the direct neural network structure,\noutperforms the state-of-the-art methods in the prediction task, with less\ntraining time and memory.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:12:49 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Raziperchikolaei", "Ramin", ""], ["Li", "Tianyu", ""], ["Chung", "Young-joo", ""]]}, {"id": "2010.06197", "submitter": "Kai Huang", "authors": "Luyang Wang, Kai Huang, Jiao Wang, Shengsheng Huang, Jason Dai, Yue\n  Zhuang", "title": "Context-Aware Drive-thru Recommendation Service at Fast Food Restaurants", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drive-thru is a popular sales channel in the fast food industry where\nconsumers can make food purchases without leaving their cars. Drive-thru\nrecommendation systems allow restaurants to display food recommendations on the\ndigital menu board as guests are making their orders. Popular recommendation\nmodels in eCommerce scenarios rely on user attributes (such as user profiles or\npurchase history) to generate recommendations, while such information is hard\nto obtain in the drive-thru use case. Thus, in this paper, we propose a new\nrecommendation model Transformer Cross Transformer (TxT), which exploits the\nguest order behavior and contextual features (such as location, time, and\nweather) using Transformer encoders for drive-thru recommendations. Empirical\nresults show that our TxT model achieves superior results in Burger King's\ndrive-thru production environment compared with existing recommendation\nsolutions. In addition, we implement a unified system to run end-to-end big\ndata analytics and deep learning workloads on the same cluster. We find that in\npractice, maintaining a single big data cluster for the entire pipeline is more\nefficient and cost-saving. Our recommendation system is not only beneficial for\ndrive-thru scenarios, and it can also be generalized to other customer\ninteraction channels.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 06:31:59 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Luyang", ""], ["Huang", "Kai", ""], ["Wang", "Jiao", ""], ["Huang", "Shengsheng", ""], ["Dai", "Jason", ""], ["Zhuang", "Yue", ""]]}, {"id": "2010.06233", "submitter": "Maurizio Ferrari Dacrema", "authors": "Sebastiano Antenucci, Simone Boglio, Emanuele Chioso, Ervin Dervishaj,\n  Shuwen Kang, Tommaso Scarlatti, Maurizio Ferrari Dacrema", "title": "Artist-driven layering and user's behaviour impact on recommendations in\n  a playlist continuation scenario", "comments": "Source code available here:\n  https://github.com/MaurizioFD/spotify-recsys-challenge", "journal-ref": "Proceedings of the ACM Recommender Systems Challenge 2018 (RecSys\n  Challenge '18)", "doi": "10.1145/3267471.3267475", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide an overview of the approach we used as team Creamy\nFireflies for the ACM RecSys Challenge 2018. The competition, organized by\nSpotify, focuses on the problem of playlist continuation, that is suggesting\nwhich tracks the user may add to an existing playlist. The challenge addresses\nthis issue in many use cases, from playlist cold start to playlists already\ncomposed by up to a hundred tracks. Our team proposes a solution based on a few\nwell known models both content based and collaborative, whose predictions are\naggregated via an ensembling step. Moreover by analyzing the underlying\nstructure of the data, we propose a series of boosts to be applied on top of\nthe final predictions and improve the recommendation quality. The proposed\napproach leverages well-known algorithms and is able to offer a high\nrecommendation quality while requiring a limited amount of computational\nresources.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:47:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Antenucci", "Sebastiano", ""], ["Boglio", "Simone", ""], ["Chioso", "Emanuele", ""], ["Dervishaj", "Ervin", ""], ["Kang", "Shuwen", ""], ["Scarlatti", "Tommaso", ""], ["Dacrema", "Maurizio Ferrari", ""]]}, {"id": "2010.06328", "submitter": "Diana C. Hernandez-Bocanegra", "authors": "D. C. Hernandez-Bocanegra, J. Ziegler", "title": "Assessing the Helpfulness of Review Content for Explaining\n  Recommendations", "comments": "4 pages, In Proceedings of SIGIR 2019 Workshop on ExplainAble\n  Recommendation and Search (EARS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the maturity already achieved by recommender systems algorithms,\nlittle is known about how to obtain and provide users with a proper rationale\nfor a recommendation. Transparency and effectiveness of recommender systems may\nbe increased when explanations are provided. In particular, identifying of\nhelpful argumentative content from reviews can be leveraged to generate textual\nexplanations. In this paper, we investigate the reasons why a review might be\nconsidered helpful, and show that the perception of credibility and\nconvincingness mediates the relationship between helpfulness and the perception\nof objectivity and relevant aspects addressed. Our findings led us to suggest\nan argumentbased approach to automatically extracting helpful content from\nhotel reviews, a domain that differs from those that best fit classical\nargumentation theories.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:24:43 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hernandez-Bocanegra", "D. C.", ""], ["Ziegler", "J.", ""]]}, {"id": "2010.06392", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Vassilis Kalantzis, Georgios Kollias, Shashanka Ubaru, Athanasios N.\n  Nikolakopoulos, Lior Horesh, Kenneth L. Clarkson", "title": "Projection techniques to update the truncated SVD of evolving matrices", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.IR cs.NA stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper considers the problem of updating the rank-k truncated Singular\nValue Decomposition (SVD) of matrices subject to the addition of new rows\nand/or columns over time. Such matrix problems represent an important\ncomputational kernel in applications such as Latent Semantic Indexing and\nRecommender Systems. Nonetheless, the proposed framework is purely algebraic\nand targets general updating problems. The algorithm presented in this paper\nundertakes a projection view-point and focuses on building a pair of subspaces\nwhich approximate the linear span of the sought singular vectors of the updated\nmatrix. We discuss and analyze two different choices to form the projection\nsubspaces. Results on matrices from real applications suggest that the proposed\nalgorithm can lead to higher accuracy, especially for the singular triplets\nassociated with the largest modulus singular values. Several practical details\nand key differences with other approaches are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:46:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Kalantzis", "Vassilis", ""], ["Kollias", "Georgios", ""], ["Ubaru", "Shashanka", ""], ["Nikolakopoulos", "Athanasios N.", ""], ["Horesh", "Lior", ""], ["Clarkson", "Kenneth L.", ""]]}, {"id": "2010.06395", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm", "title": "Aspect-based Document Similarity for Research Papers", "comments": "Accepted for publication at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional document similarity measures provide a coarse-grained distinction\nbetween similar and dissimilar documents. Typically, they do not consider in\nwhat aspects two documents are similar. This limits the granularity of\napplications like recommender systems that rely on document similarity. In this\npaper, we extend similarity with aspect information by performing a pairwise\ndocument classification task. We evaluate our aspect-based document similarity\nfor research papers. Paper citations indicate the aspect-based similarity,\ni.e., the section title in which a citation occurs acts as a label for the pair\nof citing and cited paper. We apply a series of Transformer models such as\nRoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM\nbaseline. We perform our experiments on two newly constructed datasets of\n172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. Our\nresults show SciBERT as the best performing system. A qualitative examination\nvalidates our quantitative results. Our findings motivate future research of\naspect-based document similarity and the development of a recommender system\nbased on the evaluated techniques. We make our datasets, code, and trained\nmodels publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:51:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ruas", "Terry", ""], ["Blume", "Till", ""], ["Gipp", "Bela", ""], ["Rehm", "Georg", ""]]}, {"id": "2010.06444", "submitter": "Thiago H. Silva", "authors": "Frances Santos, Thiago H Silva, Antonio A F Loureiro, Leandro Villas", "title": "Automatic Extraction of Urban Outdoor Perception from Geolocated\n  Free-Texts", "comments": "Paper accepted - to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic extraction of urban perception shared by people on\nlocation-based social networks (LBSNs) is an important multidisciplinary\nresearch goal. One of the reasons is because it facilitates the understanding\nof the intrinsic characteristics of urban areas in a scalable way, helping to\nleverage new services. However, content shared on LBSNs is diverse,\nencompassing several topics, such as politics, sports, culture, religion, and\nurban perceptions, making the task of content extraction regarding a particular\ntopic very challenging. Considering free-text messages shared on LBSNs, we\npropose an automatic and generic approach to extract people's perceptions. For\nthat, our approach explores opinions that are spatial-temporal and semantically\nsimilar. We exemplify our approach in the context of urban outdoor areas in\nChicago, New York City and London. Studying those areas, we found evidence that\nLBSN data brings valuable information about urban regions. To analyze and\nvalidate our outcomes, we conducted a temporal analysis to measure the results'\nrobustness over time. We show that our approach can be helpful to better\nunderstand urban areas considering different perspectives. We also conducted a\ncomparative analysis based on a public dataset, which contains volunteers'\nperceptions regarding urban areas expressed in a controlled experiment. We\nobserve that both results yield a very similar level of agreement.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:59:46 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Santos", "Frances", ""], ["Silva", "Thiago H", ""], ["Loureiro", "Antonio A F", ""], ["Villas", "Leandro", ""]]}, {"id": "2010.06467", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Rodrigo Nogueira, and Andrew Yates", "title": "Pretrained Transformers for Text Ranking: BERT and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of text ranking is to generate an ordered list of texts retrieved\nfrom a corpus in response to a query. Although the most common formulation of\ntext ranking is search, instances of the task can also be found in many natural\nlanguage processing applications. This survey provides an overview of text\nranking with neural network architectures known as transformers, of which BERT\nis the best-known example. The combination of transformers and self-supervised\npretraining has been responsible for a paradigm shift in natural language\nprocessing (NLP), information retrieval (IR), and beyond. In this survey, we\nprovide a synthesis of existing work as a single point of entry for\npractitioners who wish to gain a better understanding of how to apply\ntransformers to text ranking problems and researchers who wish to pursue work\nin this area. We cover a wide range of modern techniques, grouped into two\nhigh-level categories: transformer models that perform reranking in multi-stage\narchitectures and dense retrieval techniques that perform ranking directly.\nThere are two themes that pervade our survey: techniques for handling long\ndocuments, beyond typical sentence-by-sentence processing in NLP, and\ntechniques for addressing the tradeoff between effectiveness (i.e., result\nquality) and efficiency (e.g., query latency, model and index size). Although\ntransformer architectures and pretraining techniques are recent innovations,\nmany aspects of how they are applied to text ranking are relatively well\nunderstood and represent mature techniques. However, there remain many open\nresearch questions, and thus in addition to laying out the foundations of\npretrained transformers for text ranking, this survey also attempts to\nprognosticate where the field is heading.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:20:32 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:35:29 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lin", "Jimmy", ""], ["Nogueira", "Rodrigo", ""], ["Yates", "Andrew", ""]]}, {"id": "2010.06492", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Kai Wan, Hua Sun, Mingyue Ji, Giuseppe Caire", "title": "On the Fundamental Limits of Cache-aided Multiuser Private Information\n  Retrieval", "comments": "Submitted to IEEE Transactions on Communications. Part of this work\n  has been presented in IEEE ISIT 2020 and WiOpt 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of cache-aided Multiuser Private Information\nRetrieval (MuPIR) which is an extension of the single-user cache-aided PIR\nproblem to the case of multiple users. In MuPIR, each of the $K_{\\rm u}$\ncache-equipped users wishes to privately retrieve a message out of $K$ messages\nfrom $N$ databases each having access to the entire message library. The\nprivacy constraint requires that any individual database learns nothing about\nthe demands of all users. The users are connected to each database via an\nerror-free shared-link. In this paper, we aim to characterize the optimal\ntrade-off between users' memory and communication load for such systems. Based\non the proposed novel approach of \\emph{cache-aided interference alignment\n(CIA)}, first, for the MuPIR problem with $K=2$ messages, $K_{\\rm u}=2$ users\nand $N\\ge 2$ databases, we propose achievable retrieval schemes for both\nuncoded and general cache placement. The CIA approach is optimal when the cache\nplacement is uncoded. For general cache placement, the CIA approach is optimal\nwhen $N=2$ and $3$ verified by the computer-aided approach. Second, when\n$K,K_{\\rm u}$ and $N$ are general, we propose a new \\emph{product design} (PD)\nwhich incorporates the PIR code into the linear caching code.\n  The product design is shown to be order optimal within a multiplicative\nfactor of 8 and is exactly optimal when the user cache memory size is large.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:53:29 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 23:38:14 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 22:47:01 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2020 18:06:10 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 22:14:55 GMT"}, {"version": "v6", "created": "Fri, 20 Nov 2020 19:35:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhang", "Xiang", ""], ["Wan", "Kai", ""], ["Sun", "Hua", ""], ["Ji", "Mingyue", ""], ["Caire", "Giuseppe", ""]]}, {"id": "2010.06705", "submitter": "Jiaxin Huang", "authors": "Jiaxin Huang, Yu Meng, Fang Guo, Heng Ji, Jiawei Han", "title": "Weakly-Supervised Aspect-Based Sentiment Analysis via Joint\n  Aspect-Sentiment Topic Embedding", "comments": "accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis of review texts is of great value for\nunderstanding user feedback in a fine-grained manner. It has in general two\nsub-tasks: (i) extracting aspects from each review, and (ii) classifying\naspect-based reviews by sentiment polarity. In this paper, we propose a\nweakly-supervised approach for aspect-based sentiment analysis, which uses only\na few keywords describing each aspect/sentiment without using any labeled\nexamples. Existing methods are either designed only for one of the sub-tasks,\nneglecting the benefit of coupling both, or are based on topic models that may\ncontain overlapping concepts. We propose to first learn <sentiment, aspect>\njoint topic embeddings in the word embedding space by imposing regularizations\nto encourage topic distinctiveness, and then use neural models to generalize\nthe word-level discriminative information by pre-training the classifiers with\nembedding-based predictions and self-training them on unlabeled data. Our\ncomprehensive performance analysis shows that our method generates quality\njoint topics and outperforms the baselines significantly (7.4% and 5.1%\nF1-score gain on average for aspect and sentiment classification respectively)\non benchmark datasets. Our code and data are available at\nhttps://github.com/teapot123/JASen.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 21:33:24 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Huang", "Jiaxin", ""], ["Meng", "Yu", ""], ["Guo", "Fang", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.06714", "submitter": "Jiaxin Huang", "authors": "Jiaxin Huang, Yiqing Xie, Yu Meng, Yunyi Zhang, Jiawei Han", "title": "CoRel: Seed-Guided Topical Taxonomy Construction by Concept Learning and\n  Relation Transferring", "comments": "KDD 2020 Research Track", "journal-ref": null, "doi": "10.1145/3394486.3403244", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomy is not only a fundamental form of knowledge representation, but also\ncrucial to vast knowledge-rich applications, such as question answering and web\nsearch. Most existing taxonomy construction methods extract hypernym-hyponym\nentity pairs to organize a \"universal\" taxonomy. However, these generic\ntaxonomies cannot satisfy user's specific interest in certain areas and\nrelations. Moreover, the nature of instance taxonomy treats each node as a\nsingle word, which has low semantic coverage. In this paper, we propose a\nmethod for seed-guided topical taxonomy construction, which takes a corpus and\na seed taxonomy described by concept names as input, and constructs a more\ncomplete taxonomy based on user's interest, wherein each node is represented by\na cluster of coherent terms. Our framework, CoRel, has two modules to fulfill\nthis goal. A relation transferring module learns and transfers the user's\ninterested relation along multiple paths to expand the seed taxonomy structure\nin width and depth. A concept learning module enriches the semantics of each\nconcept node by jointly embedding the taxonomy and text. Comprehensive\nexperiments conducted on real-world datasets show that Corel generates\nhigh-quality topical taxonomies and outperforms all the baselines\nsignificantly.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:00:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Huang", "Jiaxin", ""], ["Xie", "Yiqing", ""], ["Meng", "Yu", ""], ["Zhang", "Yunyi", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.06727", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Muhao Chen, Hongming Zhang, Dan Roth", "title": "Joint Constrained Learning for Event-Event Relation Extraction", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding natural language involves recognizing how multiple event\nmentions structurally and temporally interact with each other. In this process,\none can induce event complexes that organize multi-granular events with\ntemporal order and membership relations interweaving among them. Due to the\nlack of jointly labeled data for these relational phenomena and the restriction\non the structures they articulate, we propose a joint constrained learning\nframework for modeling event-event relations. Specifically, the framework\nenforces logical constraints within and across multiple temporal and subevent\nrelations by converting these constraints into differentiable learning\nobjectives. We show that our joint constrained learning approach effectively\ncompensates for the lack of jointly labeled data, and outperforms SOTA methods\non benchmarks for both temporal relation extraction and event hierarchy\nconstruction, replacing a commonly used but more expensive global inference\nprocess. We also present a promising case study showing the effectiveness of\nour approach in inducing event complexes on an external corpus.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:45:28 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 21:51:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Haoyu", ""], ["Chen", "Muhao", ""], ["Zhang", "Hongming", ""], ["Roth", "Dan", ""]]}, {"id": "2010.06848", "submitter": "Chen Gao", "authors": "Jun Zhang, Chen Gao, Depeng Jin, Yong Li", "title": "Group-Buying Recommendation for Social E-Commerce", "comments": "IEEE International Conference on Data Engineering (ICDE), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group buying, as an emerging form of purchase in social e-commerce websites,\nsuch as Pinduoduo, has recently achieved great success. In this new business\nmodel, users, initiator, can launch a group and share products to their social\nnetworks, and when there are enough friends, participants, join it, the deal is\nclinched. Group-buying recommendation for social e-commerce, which recommends\nan item list when users want to launch a group, plays an important role in the\ngroup success ratio and sales. However, designing a personalized recommendation\nmodel for group buying is an entirely new problem that is seldom explored. In\nthis work, we take the first step to approach the problem of group-buying\nrecommendation for social e-commerce and develop a GBGCN method (short for\nGroup-Buying Graph Convolutional Network). Considering there are multiple types\nof behaviors (launch and join) and structured social network data, we first\npropose to construct directed heterogeneous graphs to represent behavioral data\nand social networks. We then develop a graph convolutional network model with\nmulti-view embedding propagation, which can extract the complicated high-order\ngraph structure to learn the embeddings. Last, since a failed group-buying\nimplies rich preferences of the initiator and participants, we design a\ndouble-pairwise loss function to distill such preference signals. We collect a\nreal-world dataset of group-buying and conduct experiments to evaluate the\nperformance. Empirical results demonstrate that our proposed GBGCN can\nsignificantly outperform baseline methods by 2.69%-7.36%. The codes and the\ndataset are released at\nhttps://github.com/Sweetnow/group-buying-recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:20:21 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 04:42:58 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhang", "Jun", ""], ["Gao", "Chen", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2010.06952", "submitter": "Ankur Verma", "authors": "Ankur Verma", "title": "Consumer Behaviour in Retail: Next Logical Purchase using Deep Neural\n  Network", "comments": "7 pages of main content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future consumer behaviour is one of the most challenging problems\nfor large scale retail firms. Accurate prediction of consumer purchase pattern\nenables better inventory planning and efficient personalized marketing\nstrategies. Optimal inventory planning helps minimise instances of\nOut-of-stock/ Excess Inventory and, smart Personalized marketing strategy\nensures smooth and delightful shopping experience. Consumer purchase prediction\nproblem has generally been addressed by ML researchers in conventional manners,\neither through recommender systems or traditional ML approaches. Such modelling\napproaches do not generalise well in predicting consumer purchase pattern. In\nthis paper, we present our study of consumer purchase behaviour, wherein, we\nestablish a data-driven framework to predict whether a consumer is going to\npurchase an item within a certain time frame using e-commerce retail data. To\nmodel this relationship, we create a sequential time-series data for all\nrelevant consumer-item combinations. We then build generalized non-linear\nmodels by generating features at the intersection of consumer, item, and time.\nWe demonstrate robust performance by experimenting with different neural\nnetwork architectures, ML models, and their combinations. We present the\nresults of 60 modelling experiments with varying Hyperparameters along with\nStacked Generalization ensemble and F1-Maximization framework. We then present\nthe benefits that neural network architectures like Multi Layer Perceptron,\nLong Short Term Memory (LSTM), Temporal Convolutional Networks (TCN) and\nTCN-LSTM bring over ML models like Xgboost and RandomForest.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:00:00 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Verma", "Ankur", ""]]}, {"id": "2010.06975", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji and Shirui Pan and Pekka Marttinen", "title": "Medical Code Assignment with Gated Convolution and Note-Code Interaction", "comments": "Findings of ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical code assignment from clinical text is a fundamental task in clinical\ninformation system management. As medical notes are typically lengthy and the\nmedical coding system's code space is large, this task is a long-standing\nchallenge. Recent work applies deep neural network models to encode the medical\nnotes and assign medical codes to clinical documents. However, these methods\nare still ineffective as they do not fully encode and capture the lengthy and\nrich semantic information of medical notes nor explicitly exploit the\ninteractions between the notes and codes. We propose a novel method, gated\nconvolutional neural networks, and a note-code interaction (GatedCNN-NCI), for\nautomatic medical code assignment to overcome these challenges. Our methods\ncapture the rich semantic information of the lengthy clinical text for better\nrepresentation by utilizing embedding injection and gated information\npropagation in the medical note encoding module. With a novel note-code\ninteraction design and a graph message passing mechanism, we explicitly capture\nthe underlying dependency between notes and codes, enabling effective code\nprediction. A weight sharing scheme is further designed to decrease the number\nof trainable parameters. Empirical experiments on real-world clinical datasets\nshow that our proposed model outperforms state-of-the-art models in most cases,\nand our model size is on par with light-weighted baselines.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 11:37:24 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:11:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Pan", "Shirui", ""], ["Marttinen", "Pekka", ""]]}, {"id": "2010.06985", "submitter": "Simone Leonardi", "authors": "Andrea Fiandro, Jeanpierre Francois, Isabeau Oliveri, Simone Leonardi,\n  Matteo A. Senese, Giorgio Crepaldi, Alberto Benincasa, Giuseppe Rizzo", "title": "Understanding Twitter Engagement with a Click-Through Rate-based Method", "comments": "8 pages, 4 figures, 4 tables, Recsys2020 Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the POLINKS solution to the RecSys Challenge 2020 that\nranked 6th in the final leaderboard. We analyze the performance of our solution\nthat utilizes the click-through rate value to address the challenge task, we\ncompare it with a gradient boosting model, and we report the quality indicators\nutilized for computing the final leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 16:53:49 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fiandro", "Andrea", ""], ["Francois", "Jeanpierre", ""], ["Oliveri", "Isabeau", ""], ["Leonardi", "Simone", ""], ["Senese", "Matteo A.", ""], ["Crepaldi", "Giorgio", ""], ["Benincasa", "Alberto", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "2010.06986", "submitter": "Sruthi Gorantla", "authors": "Sruthi Gorantla, Amit Deshpande, Anand Louis", "title": "On the Problem of Underranking in Group-Fair Ranking", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search and recommendation systems, such as search engines, recruiting tools,\nonline marketplaces, news, and social media, output ranked lists of content,\nproducts, and sometimes, people. Credit ratings, standardized tests, risk\nassessments output only a score, but are also used implicitly for ranking. Bias\nin such ranking systems, especially among the top ranks, can worsen social and\neconomic inequalities, polarize opinions, and reinforce stereotypes. On the\nother hand, a bias correction for minority groups can cause more harm if\nperceived as favoring group-fair outcomes over meritocracy. In this paper, we\nformulate the problem of underranking in group-fair rankings, which was not\naddressed in previous work. Most group-fair ranking algorithms post-process a\ngiven ranking and output a group-fair ranking. We define underranking based on\nhow close the group-fair rank of each item is to its original rank, and prove a\nlower bound on the trade-off achievable for simultaneous underranking and group\nfairness in ranking. We give a fair ranking algorithm that takes any given\nranking and outputs another ranking with simultaneous underranking and group\nfairness guarantees comparable to the lower bound we prove. Our algorithm works\nwith group fairness constraints for any number of groups. Our experimental\nresults confirm the theoretical trade-off between underranking and group\nfairness, and also show that our algorithm achieves the best of both when\ncompared to the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 14:56:10 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 17:20:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Gorantla", "Sruthi", ""], ["Deshpande", "Amit", ""], ["Louis", "Anand", ""]]}, {"id": "2010.06987", "submitter": "Ehtsham Elahi", "authors": "Ehtsham Elahi and Ashok Chandrashekar", "title": "Learning Representations of Hierarchical Slates in Collaborative\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in building collaborative filtering models for\nrecommendation systems where users interact with slates instead of individual\nitems. These slates can be hierarchical in nature. The central idea of our\napproach is to learn low dimensional embeddings of these slates. We present a\nnovel way to learn these embeddings by making use of the (unknown) statistics\nof the underlying distribution generating the hierarchical data. Our\nrepresentation learning algorithm can be viewed as a simple composition rule\nthat can be applied recursively in a bottom-up fashion to represent arbitrarily\ncomplex hierarchical structures in terms of the representations of its\nconstituent components. We demonstrate our ideas on two real world\nrecommendation systems datasets including the one used for the RecSys 2019\nchallenge. For that dataset, we improve upon the performance achieved by the\nwinning team's model by incorporating embeddings as features generated by our\napproach in their solution.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:34:02 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Elahi", "Ehtsham", ""], ["Chandrashekar", "Ashok", ""]]}, {"id": "2010.07024", "submitter": "Xiang Hui Nicholas Lim", "authors": "Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh,\n  Renrong Weng, Jagannadan Varadarajan", "title": "STP-UDGAT: Spatial-Temporal-Preference User Dimensional Graph Attention\n  Network for Next POI Recommendation", "comments": "To appear in Proceedings of the 29th ACM International Conference on\n  Information and Knowledge Management (CIKM), 2020", "journal-ref": null, "doi": "10.1145/3340531.3411876", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next Point-of-Interest (POI) recommendation is a longstanding problem across\nthe domains of Location-Based Social Networks (LBSN) and transportation. Recent\nRecurrent Neural Network (RNN) based approaches learn POI-POI relationships in\na local view based on independent user visit sequences. This limits the model's\nability to directly connect and learn across users in a global view to\nrecommend semantically trained POIs. In this work, we propose a\nSpatial-Temporal-Preference User Dimensional Graph Attention Network\n(STP-UDGAT), a novel explore-exploit model that concurrently exploits\npersonalized user preferences and explores new POIs in global\nspatial-temporal-preference (STP) neighbourhoods, while allowing users to\nselectively learn from other users. In addition, we propose random walks as a\nmasked self-attention option to leverage the STP graphs' structures and find\nnew higher-order POI neighbours during exploration. Experimental results on six\nreal-world datasets show that our model significantly outperforms baseline and\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:03:42 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lim", "Nicholas", ""], ["Hooi", "Bryan", ""], ["Ng", "See-Kiong", ""], ["Wang", "Xueou", ""], ["Goh", "Yong Liang", ""], ["Weng", "Renrong", ""], ["Varadarajan", "Jagannadan", ""]]}, {"id": "2010.07027", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang, Zhiqiang Guo, Guohui Li, Jianjun Li, Peng Pan, Ke Liu", "title": "Heterogeneous Graph Collaborative Filtering using Textual Information", "comments": "This version use stander R-GCN with non-linear compants, which has\n  been proved is not efficient enough in next version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the development of graph neural network models, like graph\nconvolutional network (GCN), graph-based representation learning methods have\nmade great progress in recommender systems. However, the data sparsity is still\na challenging problem that graph-based methods are confronted with. Recent\nworks try to solve this problem by utilizing the side information. In this\npaper, we introduce easily accessible textual information to alleviate the\nnegative effects of data sparsity. Specifically, to incorporate with rich\ntextual knowledge, we utilize a pre-trained context-awareness natural language\nprocessing model to initialize the embeddings of text nodes. By a GCN-based\nnode information propagation on the constructed heterogeneous graph, the\nembeddings of users and items can finally be enriched by the textual knowledge.\nThe matching function used by most graph-based representation learning methods\nis the inner product, this linear operation can not fit complex semantics well.\nWe design a predictive network, which can combine the graph-based\nrepresentation learning with the matching function learning, and demonstrate\nthat this predictive architecture can gain significant improvements. Extensive\nexperiments are conducted on three public datasets and the results verify the\nsuperior performance of our method over several baselines.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:10:37 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 15:22:51 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Chaoyang", ""], ["Guo", "Zhiqiang", ""], ["Li", "Guohui", ""], ["Li", "Jianjun", ""], ["Pan", "Peng", ""], ["Liu", "Ke", ""]]}, {"id": "2010.07035", "submitter": "Marlesson Santana", "authors": "Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo,\n  Bruno Brand\\~ao, Anderson Soares, Renan M. Oliveira and Sandor Caetano", "title": "MARS-Gym: A Gym framework to model, train, and evaluate Recommender\n  Systems for Marketplaces", "comments": "15 pages, 14 figures, see\n  https://github.com/deeplearningbrasil/mars-gym", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are especially challenging for marketplaces since they\nmust maximize user satisfaction while maintaining the healthiness and fairness\nof such ecosystems. In this context, we observed a lack of resources to design,\ntrain, and evaluate agents that learn by interacting within these environments.\nFor this matter, we propose MARS-Gym, an open-source framework to empower\nresearchers and engineers to quickly build and evaluate Reinforcement Learning\nagents for recommendations in marketplaces. MARS-Gym addresses the whole\ndevelopment pipeline: data processing, model design and optimization, and\nmulti-sided evaluation. We also provide the implementation of a diverse set of\nbaseline agents, with a metrics-driven analysis of them in the Trivago\nmarketplace dataset, to illustrate how to conduct a holistic assessment using\nthe available metrics of recommendation, off-policy estimation, and fairness.\nWith MARS-Gym, we expect to bridge the gap between academic research and\nproduction systems, as well as to facilitate the design of new algorithms and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 16:39:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Santana", "Marlesson R. O.", ""], ["Melo", "Luckeciano C.", ""], ["Camargo", "Fernando H. F.", ""], ["Brand\u00e3o", "Bruno", ""], ["Soares", "Anderson", ""], ["Oliveira", "Renan M.", ""], ["Caetano", "Sandor", ""]]}, {"id": "2010.07042", "submitter": "Avi Caciularu", "authors": "Oren Barkan, Yonatan Fuchs, Avi Caciularu, Noam Koenigstein", "title": "Explainable Recommendations via Attentive Multi-Persona Collaborative\n  Filtering", "comments": "Accepted to RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two main challenges in recommender systems are modeling users with\nheterogeneous taste, and providing explainable recommendations. In this paper,\nwe propose the neural Attentive Multi-Persona Collaborative Filtering (AMP-CF)\nmodel as a unified solution for both problems. AMP-CF breaks down the user to\nseveral latent 'personas' (profiles) that identify and discern the different\ntastes and inclinations of the user. Then, the revealed personas are used to\ngenerate and explain the final recommendation list for the user. AMP-CF models\nusers as an attentive mixture of personas, enabling a dynamic user\nrepresentation that changes based on the item under consideration. We\ndemonstrate AMP-CF on five collaborative filtering datasets from the domains of\nmovies, music, video games and social networks. As an additional contribution,\nwe propose a novel evaluation scheme for comparing the different items in a\nrecommendation list based on the distance from the underlying distribution of\n\"tastes\" in the user's historical items. Experimental results show that AMP-CF\nis competitive with other state-of-the-art models. Finally, we provide\nqualitative results to showcase the ability of AMP-CF to explain its\nrecommendations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 11:52:57 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Barkan", "Oren", ""], ["Fuchs", "Yonatan", ""], ["Caciularu", "Avi", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2010.07050", "submitter": "Javier Maroto", "authors": "Javier Maroto, Cl\\'ement Vignac and Pascal Frossard", "title": "Modurec: Recommender Systems with Feature and Time Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state of the art algorithms for recommender systems are mainly based\non collaborative filtering, which exploits user ratings to discover latent\nfactors in the data. These algorithms unfortunately do not make effective use\nof other features, which can help solve two well identified problems of\ncollaborative filtering: cold start (not enough data is available for new users\nor products) and concept shift (the distribution of ratings changes over time).\nTo address these problems, we propose Modurec: an autoencoder-based method that\ncombines all available information using the feature-wise modulation mechanism,\nwhich has demonstrated its effectiveness in several fields. While time\ninformation helps mitigate the effects of concept shift, the combination of\nuser and item features improve prediction performance when little data is\navailable. We show on Movielens datasets that these modifications produce\nstate-of-the-art results in most evaluated settings compared with standard\nautoencoder-based methods and other collaborative filtering approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:18:33 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Maroto", "Javier", ""], ["Vignac", "Cl\u00e9ment", ""], ["Frossard", "Pascal", ""]]}, {"id": "2010.07061", "submitter": "Qiuqiang Kong", "authors": "Qiuqiang Kong, Bochen Li, Jitong Chen, Yuxuan Wang", "title": "GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic music datasets are important for music information retrieval and\nmusical analysis. However, there is a lack of large-scale symbolic dataset for\nclassical piano music. In this article, we create a GiantMIDI-Piano dataset\ncontaining 10,854 unique piano solo pieces composed by 2,786 composers. The\ndataset is collected as follows, we extract music piece names and composer\nnames from the International Music Score Library Project (IMSLP). We search and\ndownload their corresponding audio recordings from the internet. We apply a\nconvolutional neural network to detect piano solo pieces. Then, we transcribe\nthose piano solo recordings to Musical Instrument Digital Interface (MIDI)\nfiles using our recently proposed high-resolution piano transcription system.\nEach transcribed MIDI file contains onset, offset, pitch and velocity\nattributes of piano notes, and onset and offset attributes of sustain pedals.\nGiantMIDI-Piano contains 34,504,873 transcribed notes, and contains metadata\ninformation of each music piece. To our knowledge, GiantMIDI-Piano is the\nlargest classical piano MIDI dataset so far. We analyses the statistics of\nGiantMIDI-Piano including the nationalities, the number and duration of works\nof composers. We show the chroma, interval, trichord and tetrachord frequencies\nof six composers from different eras to show that GiantMIDI-Piano can be used\nfor musical analysis. Our piano solo detection system achieves an accuracy of\n89\\%, and the piano note transcription achieves an onset F1 of 96.72\\%\nevaluated on the MAESTRO dataset. GiantMIDI-Piano achieves an alignment error\nrate (ER) of 0.154 to the manually input MIDI files, comparing to MAESTRO with\nan alignment ER of 0.061 to the manually input MIDI files. We release the\nsource code of acquiring the GiantMIDI-Piano dataset at\nhttps://github.com/bytedance/GiantMIDI-Piano.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 01:23:43 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kong", "Qiuqiang", ""], ["Li", "Bochen", ""], ["Chen", "Jitong", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2010.07075", "submitter": "Yujing Wang", "authors": "Yiren Chen, Yaming Yang, Hong Sun, Yujing Wang, Yu Xu, Wei Shen, Rong\n  Zhou, Yunhai Tong, Jing Bai, Ruofei Zhang", "title": "AutoADR: Automatic Model Design for Ad Relevance", "comments": "CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-trained models have attracted extensive attention in the\nresearch community and shown promising results on various tasks of natural\nlanguage processing. However, these pre-trained models are memory and\ncomputation intensive, hindering their deployment into industrial online\nsystems like Ad Relevance. Meanwhile, how to design an effective yet efficient\nmodel architecture is another challenging problem in online Ad Relevance.\nRecently, AutoML shed new lights on architecture design, but how to integrate\nit with pre-trained language models remains unsettled. In this paper, we\npropose AutoADR (Automatic model design for AD Relevance) -- a novel end-to-end\nframework to address this challenge, and share our experience to ship these\ncutting-edge techniques into online Ad Relevance system at Microsoft Bing.\nSpecifically, AutoADR leverages a one-shot neural architecture search algorithm\nto find a tailored network architecture for Ad Relevance. The search process is\nsimultaneously guided by knowledge distillation from a large pre-trained\nteacher model (e.g. BERT), while taking the online serving constraints (e.g.\nmemory and latency) into consideration. We add the model designed by AutoADR as\na sub-model into the production Ad Relevance model. This additional sub-model\nimproves the Precision-Recall AUC (PR AUC) on top of the original Ad Relevance\nmodel by 2.65X of the normalized shipping bar. More importantly, adding this\nautomatically designed sub-model leads to a statistically significant 4.6%\nBad-Ad ratio reduction in online A/B testing. This model has been shipped into\nMicrosoft Bing Ad Relevance Production model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:24:43 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Chen", "Yiren", ""], ["Yang", "Yaming", ""], ["Sun", "Hong", ""], ["Wang", "Yujing", ""], ["Xu", "Yu", ""], ["Shen", "Wei", ""], ["Zhou", "Rong", ""], ["Tong", "Yunhai", ""], ["Bai", "Jing", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2010.07100", "submitter": "Manik Bhandari", "authors": "Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu and Graham\n  Neubig", "title": "Re-evaluating Evaluation in Text Summarization", "comments": "Accepted at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated evaluation metrics as a stand-in for manual evaluation are an\nessential part of the development of text-generation tasks such as text\nsummarization. However, while the field has progressed, our standard metrics\nhave not -- for nearly 20 years ROUGE has been the standard evaluation in most\nsummarization papers. In this paper, we make an attempt to re-evaluate the\nevaluation method for text summarization: assessing the reliability of\nautomatic metrics using top-scoring system outputs, both abstractive and\nextractive, on recently popular datasets for both system-level and\nsummary-level evaluation settings. We find that conclusions about evaluation\nmetrics on older datasets do not necessarily hold on modern datasets and\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:58:53 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bhandari", "Manik", ""], ["Gour", "Pranav", ""], ["Ashfaq", "Atabak", ""], ["Liu", "Pengfei", ""], ["Neubig", "Graham", ""]]}, {"id": "2010.07458", "submitter": "Razieh Nabi", "authors": "Razieh Nabi, Joel Pfeiffer, Murat Ali Bayir, Denis Charles, Emre\n  K{\\i}c{\\i}man", "title": "Causal Inference in the Presence of Interference in Sponsored Search\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical causal inference, inferring cause-effect relations from data\nrelies on the assumption that units are independent and identically\ndistributed. This assumption is violated in settings where units are related\nthrough a network of dependencies. An example of such a setting is ad placement\nin sponsored search advertising, where the clickability of a particular ad is\npotentially influenced by where it is placed and where other ads are placed on\nthe search result page. In such scenarios, confounding arises due to not only\nthe individual ad-level covariates but also the placements and covariates of\nother ads in the system. In this paper, we leverage the language of causal\ninference in the presence of interference to model interactions among the ads.\nQuantification of such interactions allows us to better understand the click\nbehavior of users, which in turn impacts the revenue of the host search engine\nand enhances user satisfaction. We illustrate the utility of our formalization\nthrough experiments carried out on the ad placement system of the Bing search\nengine.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 01:13:14 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Nabi", "Razieh", ""], ["Pfeiffer", "Joel", ""], ["Bayir", "Murat Ali", ""], ["Charles", "Denis", ""], ["K\u0131c\u0131man", "Emre", ""]]}, {"id": "2010.07562", "submitter": "Jinyue Guo", "authors": "Jinyue Guo, Aozhi Liu and Jing Xiao", "title": "Melody Classification based on Performance Event Vector and BRNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a model for the Conference of Music and Technology (CSMT2020)\ndata challenge of melody classification. Our model used the Performance Event\nVector as the input sequence to build a Bidirectional RNN network for\nclassfication. The model achieved a satisfying performance on the development\ndataset and Wikifonia dataset. We also discussed the effect of several\nhyper-parameters, and created multiple prediction outputs for the evaluation\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 07:21:58 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:50:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Guo", "Jinyue", ""], ["Liu", "Aozhi", ""], ["Xiao", "Jing", ""]]}, {"id": "2010.07628", "submitter": "Jingwei Ma", "authors": "Jiahui Wen and Jingwei Ma and Hongkui Tu and Wei Yin and Jian Fang", "title": "Hierarchical Text Interaction for Rating Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommender systems encounter several challenges such as data\nsparsity and unexplained recommendation. To address these challenges, many\nworks propose to exploit semantic information from review data. However, these\nmethods have two major limitations in terms of the way to model textual\nfeatures and capture textual interaction. For textual modeling, they simply\nconcatenate all the reviews of a user/item into a single review. However,\nfeature extraction at word/phrase level can violate the meaning of the original\nreviews. As for textual interaction, they defer the interactions to the\nprediction layer, making them fail to capture complex correlations between\nusers and items. To address those limitations, we propose a novel Hierarchical\nText Interaction model(HTI) for rating prediction. In HTI, we propose to model\nlow-level word semantics and high-level review representations hierarchically.\nThe hierarchy allows us to exploit textual features at different granularities.\nTo further capture complex user-item interactions, we propose to exploit\nsemantic correlations between each user-item pair at different hierarchies. At\nword level, we propose an attention mechanism specialized to each user-item\npair, and capture the important words for representing each review. At review\nlevel, we mutually propagate textual features between the user and item, and\ncapture the informative reviews. The aggregated review representations are\nintegrated into a collaborative filtering framework for rating prediction.\nExperiments on five real-world datasets demonstrate that HTI outperforms\nstate-of-the-art models by a large margin. Further case studies provide a deep\ninsight into HTI's ability to capture semantic correlations at different levels\nof granularities for rating prediction.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:52:40 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wen", "Jiahui", ""], ["Ma", "Jingwei", ""], ["Tu", "Hongkui", ""], ["Yin", "Wei", ""], ["Fang", "Jian", ""]]}, {"id": "2010.07717", "submitter": "Weijie Yu", "authors": "Weijie Yu, Chen Xu, Jun Xu, Liang Pang, Xiaopeng Gao, Xiaozhao Wang\n  and Ji-Rong Wen", "title": "Wasserstein Distance Regularized Sequence Representation for Text\n  Matching in Asymmetrical Domains", "comments": "accepted as long paper by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to matching texts from asymmetrical domains is projecting the\ninput sequences into a common semantic space as feature vectors upon which the\nmatching function can be readily defined and learned. In real-world matching\npractices, it is often observed that with the training goes on, the feature\nvectors projected from different domains tend to be indistinguishable. The\nphenomenon, however, is often overlooked in existing matching models. As a\nresult, the feature vectors are constructed without any regularization, which\ninevitably increases the difficulty of learning the downstream matching\nfunctions. In this paper, we propose a novel match method tailored for text\nmatching in asymmetrical domains, called WD-Match. In WD-Match, a Wasserstein\ndistance-based regularizer is defined to regularize the features vectors\nprojected from different domains. As a result, the method enforces the feature\nprojection function to generate vectors such that those correspond to different\ndomains cannot be easily discriminated. The training process of WD-Match\namounts to a game that minimizes the matching loss regularized by the\nWasserstein distance. WD-Match can be used to improve different text matching\nmethods, by using the method as its underlying matching model. Four popular\ntext matching methods have been exploited in the paper. Experimental results\nbased on four publicly available benchmarks showed that WD-Match consistently\noutperformed the underlying methods and the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:52:09 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 01:32:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yu", "Weijie", ""], ["Xu", "Chen", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Gao", "Xiaopeng", ""], ["Wang", "Xiaozhao", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.08125", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "Testing the Quantitative Spacetime Hypothesis using Artificial Narrative\n  Comprehension (II) : Establishing the Geometry of Invariant Concepts, Themes,\n  and Namespaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pool of observations selected from a sensor stream, input data can be\nrobustly represented, via a multiscale process, in terms of invariant concepts,\nand themes. Applying this to episodic natural language data, one may obtain a\ngraph geometry associated with the decomposition, which is a direct encoding of\nspacetime relationships for the events. This study contributes to an ongoing\napplication of the Semantic Spacetime Hypothesis, and demonstrates the\nunsupervised analysis of narrative texts using inexpensive computational\nmethods without knowledge of linguistics. Data streams are parsed and\nfractionated into small constituents, by multiscale interferometry, in the\nmanner of bioinformatic analysis. Fragments may then be recombined to construct\noriginal sensory episodes---or form new narratives by a chemistry of\nassociation and pattern reconstruction, based only on the four fundamental\nspacetime relationships. There is a straightforward correspondence between\nbioinformatic processes and this cognitive representation of natural language.\nFeatures identifiable as `concepts' and `narrative themes' span three main\nscales (micro, meso, and macro). Fragments of the input act as symbols in a\nhierarchy of alphabets that define new effective languages at each scale.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 11:19:17 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "2010.08187", "submitter": "Guang-Neng Hu", "authors": "Guangneng Hu, Qiang Yang", "title": "PrivNet: Safeguarding Private Attributes in Transfer Learning for\n  Recommendation", "comments": "Findings of EMNLP 2020", "journal-ref": "Findings of ACL: EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is an effective technique to improve a target recommender\nsystem with the knowledge from a source domain. Existing research focuses on\nthe recommendation performance of the target domain while ignores the privacy\nleakage of the source domain. The transferred knowledge, however, may\nunintendedly leak private information of the source domain. For example, an\nattacker can accurately infer user demographics from their historical purchase\nprovided by a source domain data owner. This paper addresses the above\nprivacy-preserving issue by learning a privacy-aware neural representation by\nimproving target performance while protecting source privacy. The key idea is\nto simulate the attacks during the training for protecting unseen users'\nprivacy in the future, modeled by an adversarial game, so that the transfer\nlearning model becomes robust to attacks. Experiments show that the proposed\nPrivNet model can successfully disentangle the knowledge benefitting the\ntransfer from leaking the privacy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:33:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hu", "Guangneng", ""], ["Yang", "Qiang", ""]]}, {"id": "2010.08191", "submitter": "Jing Liu", "authors": "Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin\n  Zhao, Daxiang Dong, Hua Wu, Haifeng Wang", "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for\n  Open-Domain Question Answering", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-domain question answering, dense passage retrieval has become a new\nparadigm to retrieve relevant passages for finding answers. Typically, the\ndual-encoder architecture is adopted to learn dense representations of\nquestions and passages for semantic matching. However, it is difficult to\neffectively train a dual-encoder due to the challenges including the\ndiscrepancy between training and inference, the existence of unlabeled\npositives and limited training data. To address these challenges, we propose an\noptimized training approach, called RocketQA, to improving dense passage\nretrieval. We make three major technical contributions in RocketQA, namely\ncross-batch negatives, denoised hard negatives and data augmentation. The\nexperiment results show that RocketQA significantly outperforms previous\nstate-of-the-art models on both MSMARCO and Natural Questions. We also conduct\nextensive experiments to examine the effectiveness of the three strategies in\nRocketQA. Besides, we demonstrate that the performance of end-to-end QA can be\nimproved based on our RocketQA retriever.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:54:05 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 07:52:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Qu", "Yingqi", ""], ["Ding", "Yuchen", ""], ["Liu", "Jing", ""], ["Liu", "Kai", ""], ["Ren", "Ruiyang", ""], ["Zhao", "Wayne Xin", ""], ["Dong", "Daxiang", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2010.08269", "submitter": "Mark Berger", "authors": "Mark Berger, Jakub Zavrel, Paul Groth", "title": "Effective Distributed Representations for Academic Expert Search", "comments": "To be published in the Scholarly Document Processing 2020 Workshop @\n  EMNLP 2020 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert search aims to find and rank experts based on a user's query. In\nacademia, retrieving experts is an efficient way to navigate through a large\namount of academic knowledge. Here, we study how different distributed\nrepresentations of academic papers (i.e. embeddings) impact academic expert\nretrieval. We use the Microsoft Academic Graph dataset and experiment with\ndifferent configurations of a document-centric voting model for retrieval. In\nparticular, we explore the impact of the use of contextualized embeddings on\nsearch performance. We also present results for paper embeddings that\nincorporate citation information through retrofitting. Additionally,\nexperiments are conducted using different techniques for assigning author\nweights based on author order. We observe that using contextual embeddings\nproduced by a transformer model trained for sentence similarity tasks produces\nthe most effective paper representations for document-centric expert retrieval.\nHowever, retrofitting the paper embeddings and using elaborate author\ncontribution weighting strategies did not improve retrieval performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:43:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Berger", "Mark", ""], ["Zavrel", "Jakub", ""], ["Groth", "Paul", ""]]}, {"id": "2010.08300", "submitter": "Zhoujian Sun", "authors": "Zhoujian Sun, Wei Dong, Jinlong Shi and Zhengxing Huang", "title": "Interpretable Disease Prediction based on Reinforcement Path Reasoning\n  over Knowledge Graphs", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: To combine medical knowledge and medical data to interpretably\npredict the risk of disease. Methods: We formulated the disease prediction task\nas a random walk along a knowledge graph (KG). Specifically, we build a KG to\nrecord relationships between diseases and risk factors according to validated\nmedical knowledge. Then, a mathematical object walks along the KG. It starts\nwalking at a patient entity, which connects the KG based on the patient current\ndiseases or risk factors and stops at a disease entity, which represents the\npredicted disease. The trajectory generated by the object represents an\ninterpretable disease progression path of the given patient. The dynamics of\nthe object are controlled by a policy-based reinforcement learning (RL) module,\nwhich is trained by electronic health records (EHRs). Experiments: We utilized\ntwo real-world EHR datasets to evaluate the performance of our model. In the\ndisease prediction task, our model achieves 0.743 and 0.639 in terms of macro\narea under the curve (AUC) in predicting 53 circulation system diseases in the\ntwo datasets, respectively. This performance is comparable to the commonly used\nmachine learning (ML) models in medical research. In qualitative analysis, our\nclinical collaborator reviewed the disease progression paths generated by our\nmodel and advocated their interpretability and reliability. Conclusion:\nExperimental results validate the proposed model in interpretably evaluating\nand optimizing disease prediction. Significance: Our work contributes to\nleveraging the potential of medical knowledge and medical data jointly for\ninterpretable prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:46:28 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Sun", "Zhoujian", ""], ["Dong", "Wei", ""], ["Shi", "Jinlong", ""], ["Huang", "Zhengxing", ""]]}, {"id": "2010.08319", "submitter": "Tim Nugent", "authors": "Tim Nugent, Nicole Stelea and Jochen L. Leidner", "title": "Detecting ESG topics using domain-specific language models and data\n  augmentation approaches", "comments": "11 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite recent advances in deep learning-based language modelling, many\nnatural language processing (NLP) tasks in the financial domain remain\nchallenging due to the paucity of appropriately labelled data. Other issues\nthat can limit task performance are differences in word distribution between\nthe general corpora - typically used to pre-train language models - and\nfinancial corpora, which often exhibit specialized language and symbology.\nHere, we investigate two approaches that may help to mitigate these issues.\nFirstly, we experiment with further language model pre-training using large\namounts of in-domain data from business and financial news. We then apply\naugmentation approaches to increase the size of our dataset for model\nfine-tuning. We report our findings on an Environmental, Social and Governance\n(ESG) controversies dataset and demonstrate that both approaches are beneficial\nto accuracy in classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:20:07 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Nugent", "Tim", ""], ["Stelea", "Nicole", ""], ["Leidner", "Jochen L.", ""]]}, {"id": "2010.08433", "submitter": "Andrey Kormilitzin", "authors": "Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Hao Ni, Goran Nenadic,\n  Alejo Nevado-Holgado", "title": "An efficient representation of chronological events in medical texts", "comments": "4 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we addressed the problem of capturing sequential information\ncontained in longitudinal electronic health records (EHRs). Clinical notes,\nwhich is a particular type of EHR data, are a rich source of information and\npractitioners often develop clever solutions how to maximise the sequential\ninformation contained in free-texts. We proposed a systematic methodology for\nlearning from chronological events available in clinical notes. The proposed\nmethodological {\\it path signature} framework creates a non-parametric\nhierarchical representation of sequential events of any type and can be used as\nfeatures for downstream statistical learning tasks. The methodology was\ndeveloped and externally validated using the largest in the UK secondary care\nmental health EHR data on a specific task of predicting survival risk of\npatients diagnosed with Alzheimer's disease. The signature-based model was\ncompared to a common survival random forest model. Our results showed a\n15.4$\\%$ increase of risk prediction AUC at the time point of 20 months after\nthe first admission to a specialist memory clinic and the signature method\noutperformed the baseline mixed-effects model by 13.2 $\\%$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 14:54:29 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:52:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kormilitzin", "Andrey", ""], ["Vaci", "Nemanja", ""], ["Liu", "Qiang", ""], ["Ni", "Hao", ""], ["Nenadic", "Goran", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "2010.08547", "submitter": "Jingwei Ma", "authors": "Jingwei Ma and Jiahui Wen and Panpan Zhang and Guangda Zhang and Xue\n  Li", "title": "A Unified Model for Recommendation with Selective Neighborhood Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood-based recommenders are a major class of Collaborative Filtering\n(CF) models. The intuition is to exploit neighbors with similar preferences for\nbridging unseen user-item pairs and alleviating data sparseness. Many existing\nworks propose neural attention networks to aggregate neighbors and place higher\nweights on specific subsets of users for recommendation. However, the\nneighborhood information is not necessarily always informative, and the noises\nin the neighborhood can negatively affect the model performance. To address\nthis issue, we propose a novel neighborhood-based recommender, where a hybrid\ngated network is designed to automatically separate similar neighbors from\ndissimilar (noisy) ones, and aggregate those similar neighbors to comprise\nneighborhood representations. The confidence in the neighborhood is also\naddressed by putting higher weights on the neighborhood representations if we\nare confident with the neighborhood information, and vice versa. In addition, a\nuser-neighbor component is proposed to explicitly regularize user-neighbor\nproximity in the latent space. These two components are combined into a unified\nmodel to complement each other for the recommendation task. Extensive\nexperiments on three publicly available datasets show that the proposed model\nconsistently outperforms state-of-the-art neighborhood-based recommenders. We\nalso study different variants of the proposed model to justify the underlying\nintuition of the proposed hybrid gated network and user-neighbor modeling\ncomponents.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 08:06:16 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ma", "Jingwei", ""], ["Wen", "Jiahui", ""], ["Zhang", "Panpan", ""], ["Zhang", "Guangda", ""], ["Li", "Xue", ""]]}, {"id": "2010.08591", "submitter": "Sumit Kumar", "authors": "Smita Pallavi, Raj Ratn Pranesh, Sumit Kumar", "title": "A Conglomerate of Multiple OCR Table Detection and Extraction", "comments": "For ICDAR proceedings, see https://panel.waset.org/abstracts/127575", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information representation as tables are compact and concise method that\neases searching, indexing, and storage requirements. Extracting and cloning\ntables from parsable documents is easier and widely used, however industry\nstill faces challenge in detecting and extracting tables from OCR documents or\nimages. This paper proposes an algorithm that detects and extracts multiple\ntables from OCR document. The algorithm uses a combination of image processing\ntechniques, text recognition and procedural coding to identify distinct tables\nin same image and map the text to appropriate corresponding cell in dataframe\nwhich can be stored as Comma-separated values, Database, Excel and multiple\nother usable formats.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:56:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Pallavi", "Smita", ""], ["Pranesh", "Raj Ratn", ""], ["Kumar", "Sumit", ""]]}, {"id": "2010.08593", "submitter": "Suraj Kothawade", "authors": "Suraj Kothawade, Jiten Girdhar, Chandrashekhar Lavania, Rishabh Iyer", "title": "Deep Submodular Networks for Extractive Data Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Models are increasingly becoming prevalent in summarization problems\n(e.g. document, video and images) due to their ability to learn complex feature\ninteractions and representations. However, they do not model characteristics\nsuch as diversity, representation, and coverage, which are also very important\nfor summarization tasks. On the other hand, submodular functions naturally\nmodel these characteristics because of their diminishing returns property. Most\napproaches for modelling and learning submodular functions rely on very simple\nmodels, such as weighted mixtures of submodular functions. Unfortunately, these\nmodels only learn the relative importance of the different submodular functions\n(such as diversity, representation or importance), but cannot learn more\ncomplex feature representations, which are often required for state-of-the-art\nperformance. We propose Deep Submodular Networks (DSN), an end-to-end learning\nframework that facilitates the learning of more complex features and richer\nfunctions, crafted for better modelling of all aspects of summarization. The\nDSN framework can be used to learn features appropriate for summarization from\nscratch. We demonstrate the utility of DSNs on both generic and query focused\nimage-collection summarization, and show significant improvement over the\nstate-of-the-art. In particular, we show that DSNs outperform simple mixture\nmodels using off the shelf features. Secondly, we also show that just using\nfour submodular functions in a DSN with end-to-end learning performs comparably\nto the state-of-the-art mixture model with a hand-crafted set of 594 components\nand outperforms other methods for image collection summarization.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:06:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kothawade", "Suraj", ""], ["Girdhar", "Jiten", ""], ["Lavania", "Chandrashekhar", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2010.08652", "submitter": "Jian Ni", "authors": "Jian Ni and Taesun Moon and Parul Awasthy and Radu Florian", "title": "Cross-Lingual Relation Extraction with Transformers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) is one of the most important tasks in information\nextraction, as it provides essential information for many NLP applications. In\nthis paper, we propose a cross-lingual RE approach that does not require any\nhuman annotation in a target language or any cross-lingual resources. Building\nupon unsupervised cross-lingual representation learning frameworks, we develop\nseveral deep Transformer based RE models with a novel encoding scheme that can\neffectively encode both entity location and entity type information. Our RE\nmodels, when trained with English data, outperform several deep neural network\nbased English RE models. More importantly, our models can be applied to perform\nzero-shot cross-lingual RE, achieving the state-of-the-art cross-lingual RE\nperformance on two datasets (68-89% of the accuracy of the supervised\ntarget-language RE model). The high cross-lingual transfer efficiency without\nrequiring additional training data or cross-lingual resources shows that our RE\nmodels are especially useful for low-resource languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 22:23:37 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ni", "Jian", ""], ["Moon", "Taesun", ""], ["Awasthy", "Parul", ""], ["Florian", "Radu", ""]]}, {"id": "2010.08679", "submitter": "Assaf Eisenman", "authors": "Assaf Eisenman, Kiran Kumar Matam, Steven Ingram, Dheevatsa Mudigere,\n  Raghuraman Krishnamoorthi, Krishnakumar Nair, Misha Smelyanskiy, Murali\n  Annavaram", "title": "Check-N-Run: A Checkpointing System for Training Deep Learning\n  Recommendation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Checkpoints play an important role in training long running machine learning\n(ML) models. Checkpoints take a snapshot of an ML model and store it in a\nnon-volatile memory so that they can be used to recover from failures to ensure\nrapid training progress. In addition, they are used for online training to\nimprove inference prediction accuracy with continuous learning. Given the large\nand ever increasing model sizes, checkpoint frequency is often bottlenecked by\nthe storage write bandwidth and capacity. When checkpoints are maintained on\nremote storage, as is the case with many industrial settings, they are also\nbottlenecked by network bandwidth. We present Check-N-Run, a scalable\ncheckpointing system for training large ML models at Facebook. While\nCheck-N-Run is applicable to long running ML jobs, we focus on checkpointing\nrecommendation models which are currently the largest ML models with Terabytes\nof model size. Check-N-Run uses two primary techniques to address the size and\nbandwidth challenges. First, it applies incremental checkpointing, which tracks\nand checkpoints the modified part of the model. Incremental checkpointing is\nparticularly valuable in the context of recommendation models where only a\nfraction of the model (stored as embedding tables) is updated on each\niteration. Second, Check-N-Run leverages quantization techniques to\nsignificantly reduce the checkpoint size, without degrading training accuracy.\nThese techniques allow Check-N-Run to reduce the required write bandwidth by\n6-17x and the required capacity by 2.5-8x on real-world models at Facebook, and\nthereby significantly improve checkpoint capabilities while reducing the total\ncost of ownership.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 00:45:55 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:36:01 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Eisenman", "Assaf", ""], ["Matam", "Kiran Kumar", ""], ["Ingram", "Steven", ""], ["Mudigere", "Dheevatsa", ""], ["Krishnamoorthi", "Raghuraman", ""], ["Nair", "Krishnakumar", ""], ["Smelyanskiy", "Misha", ""], ["Annavaram", "Murali", ""]]}, {"id": "2010.08737", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Pavlos Avgoustinakis, Giorgos Kordopatis-Zilos, Symeon Papadopoulos,\n  Andreas L. Symeonidis, Ioannis Kompatsiaris", "title": "Audio-based Near-Duplicate Video Retrieval with Audio Similarity\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of audio-based near-duplicate video\nretrieval. We propose the Audio Similarity Learning (AuSiL) approach that\neffectively captures temporal patterns of audio similarity between video pairs.\nFor the robust similarity calculation between two videos, we first extract\nrepresentative audio-based video descriptors by leveraging transfer learning\nbased on a Convolutional Neural Network (CNN) trained on a large scale dataset\nof audio events, and then we calculate the similarity matrix derived from the\npairwise similarity of these descriptors. The similarity matrix is subsequently\nfed to a CNN network that captures the temporal structures existing within its\ncontent. We train our network following a triplet generation process and\noptimizing the triplet loss function. To evaluate the effectiveness of the\nproposed approach, we have manually annotated two publicly available video\ndatasets based on the audio duplicity between their videos. The proposed\napproach achieves very competitive results compared to three state-of-the-art\nmethods. Also, unlike the competing methods, it is very robust to the retrieval\nof audio duplicates generated with speed transformations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 08:12:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 12:33:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Avgoustinakis", "Pavlos", ""], ["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Symeonidis", "Andreas L.", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2010.08768", "submitter": "Fatima Haouari", "authors": "Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed", "title": "ArCOV19-Rumors: Arabic COVID-19 Twitter Dataset for Misinformation\n  Detection", "comments": "This work was accepted at the Sixth Arabic Natural Language\n  Processing Workshop (EACL/WANLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ArCOV19-Rumors, an Arabic COVID-19 Twitter dataset\nfor misinformation detection composed of tweets containing claims from 27th\nJanuary till the end of April 2020. We collected 138 verified claims, mostly\nfrom popular fact-checking websites, and identified 9.4K relevant tweets to\nthose claims. Tweets were manually-annotated by veracity to support research on\nmisinformation detection, which is one of the major problems faced during a\npandemic. ArCOV19-Rumors supports two levels of misinformation detection over\nTwitter: verifying free-text claims (called claim-level verification) and\nverifying claims expressed in tweets (called tweet-level verification). Our\ndataset covers, in addition to health, claims related to other topical\ncategories that were influenced by COVID-19, namely, social, politics, sports,\nentertainment, and religious. Moreover, we present benchmarking results for\ntweet-level verification on the dataset. We experimented with SOTA models of\nversatile approaches that either exploit content, user profiles features,\ntemporal features and propagation structure of the conversational threads for\ntweet verification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 11:21:40 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 20:26:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haouari", "Fatima", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Elsayed", "Tamer", ""]]}, {"id": "2010.08777", "submitter": "Pengshuai Li", "authors": "Pengshuai Li, Xinsong Zhang, Weijia Jia and Wei Zhao", "title": "Active Testing: An Unbiased Evaluation Method for Distantly Supervised\n  Relation Extraction", "comments": "accepted to appear at Findings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant supervision has been a widely used method for neural relation\nextraction for its convenience of automatically labeling datasets. However,\nexisting works on distantly supervised relation extraction suffer from the low\nquality of test set, which leads to considerable biased performance evaluation.\nThese biases not only result in unfair evaluations but also mislead the\noptimization of neural relation extraction. To mitigate this problem, we\npropose a novel evaluation method named active testing through utilizing both\nthe noisy test set and a few manual annotations. Experiments on a widely used\nbenchmark show that our proposed approach can yield approximately unbiased\nevaluations for distantly supervised relation extractors.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 12:29:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Pengshuai", ""], ["Zhang", "Xinsong", ""], ["Jia", "Weijia", ""], ["Zhao", "Wei", ""]]}, {"id": "2010.08865", "submitter": "Thanh Tran", "authors": "Thanh Tran, Yifan Hu, Changwei Hu, Kevin Yen, Fei Tan, Kyumin Lee,\n  Serim Park", "title": "HABERTOR: An Efficient and Effective Deep Hatespeech Detector", "comments": null, "journal-ref": "EMNLP 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our HABERTOR model for detecting hatespeech in large scale\nuser-generated content. Inspired by the recent success of the BERT model, we\npropose several modifications to BERT to enhance the performance on the\ndownstream hatespeech classification task. HABERTOR inherits BERT's\narchitecture, but is different in four aspects: (i) it generates its own\nvocabularies and is pre-trained from the scratch using the largest scale\nhatespeech dataset; (ii) it consists of Quaternion-based factorized components,\nresulting in a much smaller number of parameters, faster training and\ninferencing, as well as less memory usage; (iii) it uses our proposed\nmulti-source ensemble heads with a pooling layer for separate input sources, to\nfurther enhance its effectiveness; and (iv) it uses a regularized adversarial\ntraining with our proposed fine-grained and adaptive noise magnitude to enhance\nits robustness. Through experiments on the large-scale real-world hatespeech\ndataset with 1.4M annotated comments, we show that HABERTOR works better than\n15 state-of-the-art hatespeech detection methods, including fine-tuning\nLanguage Models. In particular, comparing with BERT, our HABERTOR is 4~5 times\nfaster in the training/inferencing phase, uses less than 1/3 of the memory, and\nhas better performance, even though we pre-train it by using less than 1% of\nthe number of words. Our generalizability analysis shows that HABERTOR\ntransfers well to other unseen hatespeech datasets and is a more efficient and\neffective alternative to BERT for the hatespeech classification.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:10:08 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Tran", "Thanh", ""], ["Hu", "Yifan", ""], ["Hu", "Changwei", ""], ["Yen", "Kevin", ""], ["Tan", "Fei", ""], ["Lee", "Kyumin", ""], ["Park", "Serim", ""]]}, {"id": "2010.08874", "submitter": "Andreas Schreiber", "authors": "Andreas Schreiber", "title": "Visualization of Contributions to Open-Source Projects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We want to analyze visually, to what extend team members and external\ndevelopers contribute to open-source projects. This gives a high-level\nimpression about collaboration in that projects. We achieve this by recording\nprovenance of the development process and use graph drawing on the resulting\nprovenance graph. Our graph drawings show, which developers are jointly changed\nthe same files -- and to what extent -- which we show at Germany's COVID-19\nexposure notification app 'Corona-Warn-App'.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:51:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Schreiber", "Andreas", ""]]}, {"id": "2010.08883", "submitter": "Sai Sharath Japa", "authors": "Sai Sharath Japa and Rekabdar Banafsheh", "title": "Question Answering over Knowledge Base using Language Model Embeddings", "comments": null, "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9206698", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Base, represents facts about the world, often in some form of\nsubsumption ontology, rather than implicitly, embedded in procedural code, the\nway a conventional computer program does. While there is a rapid growth in\nknowledge bases, it poses a challenge of retrieving information from them.\nKnowledge Base Question Answering is one of the promising approaches for\nextracting substantial knowledge from Knowledge Bases. Unlike web search,\nQuestion Answering over a knowledge base gives accurate and concise results,\nprovided that natural language questions can be understood and mapped precisely\nto an answer in the knowledge base. However, some of the existing\nembedding-based methods for knowledge base question answering systems ignore\nthe subtle correlation between the question and the Knowledge Base (e.g.,\nentity types, relation paths, and context) and suffer from the Out Of\nVocabulary problem. In this paper, we focused on using a pre-trained language\nmodel for the Knowledge Base Question Answering task. Firstly, we used Bert\nbase uncased for the initial experiments. We further fine-tuned these\nembeddings with a two-way attention mechanism from the knowledge base to the\nasked question and from the asked question to the knowledge base answer\naspects. Our method is based on a simple Convolutional Neural Network\narchitecture with a Multi-Head Attention mechanism to represent the asked\nquestion dynamically in multiple aspects. Our experimental results show the\neffectiveness and the superiority of the Bert pre-trained language model\nembeddings for question answering systems on knowledge bases over other\nwell-known embedding methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 22:59:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Japa", "Sai Sharath", ""], ["Banafsheh", "Rekabdar", ""]]}, {"id": "2010.09029", "submitter": "Xinyi Zhou", "authors": "Chen Yang, Xinyi Zhou, Reza Zafarani", "title": "CHECKED: Chinese COVID-19 Fake News Dataset", "comments": "Accepted to Social Network Analysis and Mining (SNAM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  COVID-19 has impacted all lives. To maintain social distancing and avoiding\nexposure, works and lives have gradually moved online. Under this trend, social\nmedia usage to obtain COVID-19 news has increased. Also, misinformation on\nCOVID-19 is frequently spread on social media. In this work, we develop\nCHECKED, the first Chinese dataset on COVID-19 misinformation. CHECKED provides\na total 2,104 verified microblogs related to COVID-19 from December 2019 to\nAugust 2020, identified by using a specific list of keywords. Correspondingly,\nCHECKED includes 1,868,175 reposts, 1,185,702 comments, and 56,852,736 likes\nthat reveal how these verified microblogs are spread and reacted on Weibo. The\ndataset contains a rich set of multimedia information for each microblog\nincluding ground-truth label, textual, visual, temporal, and network\ninformation. Extensive experiments have been conducted to analyze CHECKED data\nand to provide benchmark results for well-established methods when predicting\nfake news using CHECKED. We hope that CHECKED can facilitate studies that\ntarget misinformation on coronavirus. The dataset is available at\nhttps://github.com/cyang03/CHECKED.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:52:27 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 02:17:47 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yang", "Chen", ""], ["Zhou", "Xinyi", ""], ["Zafarani", "Reza", ""]]}, {"id": "2010.09113", "submitter": "Amrita Bhattacharjee", "authors": "Amrita Bhattacharjee, Kai Shu, Min Gao, Huan Liu", "title": "Disinformation in the Online Information Ecosystem: Detection,\n  Mitigation and Challenges", "comments": "A Chinese version of this manuscript has been submitted to the\n  Journal of Computer Research and Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase in access to internet and the subsequent growth in\nthe population of online social media users, the quality of information posted,\ndisseminated and consumed via these platforms is an issue of growing concern. A\nlarge fraction of the common public turn to social media platforms and in\ngeneral the internet for news and even information regarding highly concerning\nissues such as COVID-19 symptoms. Given that the online information ecosystem\nis extremely noisy, fraught with misinformation and disinformation, and often\ncontaminated by malicious agents spreading propaganda, identifying genuine and\ngood quality information from disinformation is a challenging task for humans.\nIn this regard, there is a significant amount of ongoing research in the\ndirections of disinformation detection and mitigation. In this survey, we\ndiscuss the online disinformation problem, focusing on the recent 'infodemic'\nin the wake of the coronavirus pandemic. We then proceed to discuss the\ninherent challenges in disinformation research, and then elaborate on the\ncomputational and interdisciplinary approaches towards mitigation of\ndisinformation, after a short overview of the various directions explored in\ndetection efforts.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 21:44:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bhattacharjee", "Amrita", ""], ["Shu", "Kai", ""], ["Gao", "Min", ""], ["Liu", "Huan", ""]]}, {"id": "2010.09139", "submitter": "Benjamin Adams", "authors": "Benjamin Adams and Mark Gahegan", "title": "We Need to Rethink How We Describe and Organize Spatial Information:\n  Instrumenting and Observing the Community of Users to Improve Data\n  Description and Discovery", "comments": "6 pages, 2 figures", "journal-ref": "GEOProcessing 2016 : The Eighth International Conference on\n  Advanced Geographic Information Systems, Applications, and Services (2016),\n  131-136", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Spatial Data Infrastructure or Cyber Infrastructure, the description of\ngeographic data semantics is intended to support data discovery, reuse and\nintegration. In the vast majority of cases the producers of these data generate\ndescriptions based on particular understandings of what uses the data are good\nfor. This producer-oriented perspective means that the descriptions often do\nnot help to answer the question of whether a data set is of use for a consumer\nwho might want to apply it in a different context. In this paper, we discuss\nthe role geographic information observatories can play in providing an\ninfrastructure for observing the context of data use by consumers. These\nobservations of data pragmatics lead to operational statistical methods that\nwill support better fitness-for-use assessment. Finally, we highlight some of\nthe challenges to building these observatories, and briefly discuss strategies\nto address those challenges.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 23:21:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Adams", "Benjamin", ""], ["Gahegan", "Mark", ""]]}, {"id": "2010.09157", "submitter": "Ryoma Sato", "authors": "Ryoma Sato, Makoto Yamada, Hisashi Kashima", "title": "Poincare: Recommending Publication Venues via Treatment Effect\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a publication venue for an academic paper is a crucial step in the\nresearch process. However, in many cases, decisions are based on the experience\nof researchers, which often leads to suboptimal results. Although some existing\nmethods recommend publication venues, they just recommend venues where a paper\nis likely to be published. In this study, we aim to recommend publication\nvenues from a different perspective. We estimate the number of citations a\npaper will receive if the paper is published in each venue and recommend the\nvenue where the paper has the most potential impact. However, there are two\nchallenges to this task. First, a paper is published in only one venue, and\nthus, we cannot observe the number of citations the paper would receive if the\npaper were published in another venue. Secondly, the contents of a paper and\nthe publication venue are not statistically independent; that is, there exist\nselection biases in choosing publication venues. In this paper, we propose to\nuse a causal inference method to estimate the treatment effects of choosing a\npublication venue effectively and to recommend venues based on the potential\ninfluence of papers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 00:50:48 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2010.09189", "submitter": "Sheng Zhang", "authors": "Ye Liu, Sheng Zhang, Rui Song, Suo Feng, Yanghua Xiao", "title": "Knowledge-guided Open Attribute Value Extraction with Reinforcement\n  Learning", "comments": "EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open attribute value extraction for emerging entities is an important but\nchallenging task. A lot of previous works formulate the problem as a\n\\textit{question-answering} (QA) task. While the collections of articles from\nweb corpus provide updated information about the emerging entities, the\nretrieved texts can be noisy, irrelevant, thus leading to inaccurate answers.\nEffectively filtering out noisy articles as well as bad answers is the key to\nimproving extraction accuracy. Knowledge graph (KG), which contains rich, well\norganized information about entities, provides a good resource to address the\nchallenge. In this work, we propose a knowledge-guided reinforcement learning\n(RL) framework for open attribute value extraction. Informed by relevant\nknowledge in KG, we trained a deep Q-network to sequentially compare extracted\nanswers to improve extraction accuracy. The proposed framework is applicable to\ndifferent information extraction system. Our experimental results show that our\nmethod outperforms the baselines by 16.5 - 27.8\\%.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:28:27 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Ye", ""], ["Zhang", "Sheng", ""], ["Song", "Rui", ""], ["Feng", "Suo", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2010.09249", "submitter": "Albert Weichselbraun", "authors": "Albert Weichselbraun and Philipp Kuntschik and Sandro H\\\"orler", "title": "Improving Company Valuations with Automated Knowledge Discovery,\n  Extraction and Fusion", "comments": "English translation of the article: \"Optimierung von\n  Unternehmensbewertungen durch automatisierte Wissensidentifikation,\n  -extraktion und -integration\". Information - Wissenschaft und Praxis 71\n  (5-6):1-5, https://doi.org/10.1515/iwp-2020-2119", "journal-ref": null, "doi": "10.1515/iwp-2020-2119", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing company valuations within the domain of biotechnology, pharmacy\nand medical technology is a challenging task, especially when considering the\nunique set of risks biotech start-ups face when entering new markets. Companies\nspecialized in global valuation services, therefore, combine valuation models\nand past experience with heterogeneous metrics and indicators that provide\ninsights into a company's performance. This paper illustrates how automated\nknowledge discovery, extraction and data fusion can be used to (i) obtain\nadditional indicators that provide insights into the success of a company's\nproduct development efforts, and (ii) support labor-intensive data curation\nprocesses. We apply deep web knowledge acquisition methods to identify and\nharvest data on clinical trials that is hidden behind proprietary search\ninterfaces and integrate the extracted data into the industry partner's company\nvaluation ontology. In addition, focused Web crawls and shallow semantic\nparsing yield information on the company's key personnel and respective contact\ndata, notifying domain experts of relevant changes that get then incorporated\ninto the industry partner's company data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 06:33:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Weichselbraun", "Albert", ""], ["Kuntschik", "Philipp", ""], ["H\u00f6rler", "Sandro", ""]]}, {"id": "2010.09393", "submitter": "Yusuke Kawamoto", "authors": "Natasha Fernandes, Yusuke Kawamoto, Takao Murakami", "title": "Locality Sensitive Hashing with Extended Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended differential privacy, a generalization of standard differential\nprivacy (DP) using a general metric, has been widely studied to provide\nrigorous privacy guarantees while keeping high utility. However, existing works\non extended DP are limited to few metrics such as the Euclidean metric.\nConsequently, they have only a small number of applications such as\nlocation-based services and document processing. In this paper, we propose a\ncouple of mechanisms providing extended DP with a different metric: angular\ndistance (or cosine distance). Our mechanisms are based on locality sensitive\nhashing (LSH), which can be applied to the angular distance and work well for\npersonal data in a high-dimensional space. We theoretically analyze the privacy\nproperties of our mechanisms, and prove extended DP for input data by taking\ninto account that LSH preserves the original metric only approximately. We\napply our mechanisms to friend matching based on high-dimensional personal data\nwith angular distance in the local model, and evaluate our mechanisms using two\nreal datasets. We show that LDP requires a very large privacy budget and that\nRAPPOR does not work in this application. Then we show that our mechanisms\nenable friend matching with high utility and rigorous privacy guarantees based\non extended DP.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 11:30:51 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:35:28 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 03:26:13 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 12:58:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Fernandes", "Natasha", ""], ["Kawamoto", "Yusuke", ""], ["Murakami", "Takao", ""]]}, {"id": "2010.09426", "submitter": "Ishita Doshi", "authors": "Ishita Doshi, Dhritiman Das, Ashish Bhutani, Rajeev Kumar, Rushi\n  Bhatt, Niranjan Balasubramanian", "title": "LANNS: A Web-Scale Approximate Nearest Neighbor Lookup System", "comments": "10 pages, 9 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search (NNS) has a wide range of applications in information\nretrieval, computer vision, machine learning, databases, and other areas.\nExisting state-of-the-art algorithm for nearest neighbor search, Hierarchical\nNavigable Small World Networks(HNSW), is unable to scale to large datasets of\n100M records in high dimensions. In this paper, we propose LANNS, an end-to-end\nplatform for Approximate Nearest Neighbor Search, which scales for web-scale\ndatasets. Library for Large Scale Approximate Nearest Neighbor Search (LANNS)\nis deployed in multiple production systems for identifying topK ($100 \\leq topK\n\\leq 200$) approximate nearest neighbors with a latency of a few milliseconds\nper query, high throughput of 2.5k Queries Per Second (QPS) on a single node,\non large ($\\sim$180M data points) high dimensional (50-2048 dimensional)\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 12:36:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Doshi", "Ishita", ""], ["Das", "Dhritiman", ""], ["Bhutani", "Ashish", ""], ["Kumar", "Rajeev", ""], ["Bhatt", "Rushi", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2010.09474", "submitter": "Lixi Zhou", "authors": "Lixi Zhou, Zijie Wang, Amitabh Das, Jia Zou", "title": "It's the Best Only When It Fits You Most: Finding Related Models for\n  Serving Based on Dynamic Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent, deep learning has become the most popular direction in machine\nlearning and artificial intelligence. However, preparation of training data is\noften a bottleneck in the lifecycle of deploying a deep learning model for\nproduction or research. Reusing models for inferencing a dataset can greatly\nsave the human costs required for training data creation. Although there exist\na number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub,\nmost of these systems require model uploaders to manually specify the details\nof each model and model downloaders to screen keyword search results for\nselecting a model. They are in lack of an automatic model searching tool. This\npaper proposes an end-to-end process of searching related models for serving\nbased on the similarity of the target dataset and the training datasets of the\navailable models. While there exist many similarity measurements, we study how\nto efficiently apply these metrics without pair-wise comparison and compare the\neffectiveness of these metrics. We find that our proposed adaptivity\nmeasurement which is based on Jensen-Shannon (JS) divergence, is an effective\nmeasurement, and its computation can be significantly accelerated by using the\ntechnique of locality sensitive hashing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 22:52:13 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhou", "Lixi", ""], ["Wang", "Zijie", ""], ["Das", "Amitabh", ""], ["Zou", "Jia", ""]]}, {"id": "2010.09495", "submitter": "Bingqing Yu", "authors": "Bingqing Yu and Jacopo Tagliabue", "title": "Blending Search and Discovery: Tag-Based Query Refinement with\n  Contextual Reinforcement Learning", "comments": "accepted at EComNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle tag-based query refinement as a mobile-friendly alternative to\nstandard facet search. We approach the inference challenge with reinforcement\nlearning, and propose a deep contextual bandit that can be efficiently scaled\nin a multi-tenant SaaS scenario.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:40:35 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yu", "Bingqing", ""], ["Tagliabue", "Jacopo", ""]]}, {"id": "2010.09600", "submitter": "Dalton Schutte", "authors": "Rui Zhang, Dimitar Hristovski, Dalton Schutte, Andrej Kastrin, Marcelo\n  Fiszman, Halil Kilicoglu", "title": "Drug Repurposing for COVID-19 via Knowledge Graph Completion", "comments": "47 pages, 3 figures, Accepted to Journal of Biomedical Informatics", "journal-ref": null, "doi": "10.1016/j.jbi.2021.103696", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To discover candidate drugs to repurpose for COVID-19 using\nliterature-derived knowledge and knowledge graph completion methods. Methods:\nWe propose a novel, integrative, and neural network-based literature-based\ndiscovery (LBD) approach to identify drug candidates from both PubMed and\nCOVID-19-focused research literature. Our approach relies on semantic triples\nextracted using SemRep (via SemMedDB). We identified an informative subset of\nsemantic triples using filtering rules and an accuracy classifier developed on\na BERT variant, and used this subset to construct a knowledge graph. Five SOTA,\nneural knowledge graph completion algorithms were used to predict drug\nrepurposing candidates. The models were trained and assessed using a time\nslicing approach and the predicted drugs were compared with a list of drugs\nreported in the literature and evaluated in clinical trials. These models were\ncomplemented by a discovery pattern-based approach. Results: Accuracy\nclassifier based on PubMedBERT achieved the best performance (F1= 0.854) in\nclassifying semantic predications. Among five knowledge graph completion\nmodels, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs\nlinked to COVID-19 in the literature were identified, as well as some candidate\ndrugs that have not yet been studied. Discovery patterns enabled generation of\nplausible hypotheses regarding the relationships between the candidate drugs\nand COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB\n203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated\nhydroxytoluene) with their mechanistic explanations were further discussed.\nConclusion: We show that an LBD approach can be feasible for discovering drug\ncandidates for COVID-19, and for generating mechanistic explanations. Our\napproach can be generalized to other diseases as well as to other clinical\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 15:30:51 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 17:23:14 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhang", "Rui", ""], ["Hristovski", "Dimitar", ""], ["Schutte", "Dalton", ""], ["Kastrin", "Andrej", ""], ["Fiszman", "Marcelo", ""], ["Kilicoglu", "Halil", ""]]}, {"id": "2010.09797", "submitter": "Dara Bahri", "authors": "Dara Bahri, Che Zheng, Yi Tay, Donald Metzler, Andrew Tomkins", "title": "Surprise: Result List Truncation via Extreme Value Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in information retrieval has largely been centered around ranking and\nrelevance: given a query, return some number of results ordered by relevance to\nthe user. The problem of result list truncation, or where to truncate the\nranked list of results, however, has received less attention despite being\ncrucial in a variety of applications. Such truncation is a balancing act\nbetween the overall relevance, or usefulness of the results, with the user cost\nof processing more results. Result list truncation can be challenging because\nrelevance scores are often not well-calibrated. This is particularly true in\nlarge-scale IR systems where documents and queries are embedded in the same\nmetric space and a query's nearest document neighbors are returned during\ninference. Here, relevance is inversely proportional to the distance between\nthe query and candidate document, but what distance constitutes relevance\nvaries from query to query and changes dynamically as more documents are added\nto the index. In this work, we propose Surprise scoring, a statistical method\nthat leverages the Generalized Pareto distribution that arises in extreme value\ntheory to produce interpretable and calibrated relevance scores at query time\nusing nothing more than the ranked scores. We demonstrate its effectiveness on\nthe result list truncation task across image, text, and IR datasets and compare\nit to both classical and recent baselines. We draw connections to hypothesis\ntesting and $p$-values.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:15:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Bahri", "Dara", ""], ["Zheng", "Che", ""], ["Tay", "Yi", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2010.09803", "submitter": "Jie Zhao", "authors": "Jie Zhao, Huan Sun", "title": "Adversarial Training for Code Retrieval with Question-Description\n  Relevance Regularization", "comments": "Accepted to Findings of EMNLP 2020. 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code retrieval is a key task aiming to match natural and programming\nlanguages. In this work, we propose adversarial learning for code retrieval,\nthat is regularized by question-description relevance. First, we adapt a simple\nadversarial learning technique to generate difficult code snippets given the\ninput question, which can help the learning of code retrieval that faces\nbi-modal and data-scarce challenges. Second, we propose to leverage\nquestion-description relevance to regularize adversarial learning, such that a\ngenerated code snippet should contribute more to the code retrieval training\nloss, only if its paired natural language description is predicted to be less\nrelevant to the user given question. Experiments on large-scale code retrieval\ndatasets of two programming languages show that our adversarial learning method\nis able to improve the performance of state-of-the-art models. Moreover, using\nan additional duplicate question prediction model to regularize adversarial\nlearning further improves the performance, and this is more effective than\nusing the duplicated questions in strong multi-task learning baselines\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:32:03 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 05:49:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhao", "Jie", ""], ["Sun", "Huan", ""]]}, {"id": "2010.09892", "submitter": "Anna Zaitsev", "authors": "Sam Clark and Anna Zaitsev", "title": "Understanding YouTube Communities via Subscription-based Channel\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  YouTube is an important source of news and entertainment worldwide, but the\nscale makes it challenging to study the ideas and topics being discussed on the\nplatform. This paper presents new methods to discover and classify YouTube\nchannels which enable the analysis of communities and categories on the\nplatform using orders of magnitude more channels than have been used in\nprevious studies. Instead of using channel and video data as features for\nclassification as other researchers have, these methods use a self-supervised\nlearning approach that leverages the public subscription pages of commenters.\nWe test the classification method on the task of predicting the political lean\nof YouTube news channels and find that it outperforms the previous best model\non the task. Further experiments also show that there are important advantages\nto using commenter subscriptions to discover channels. The subscription data,\nalong with an iterative approach, is applied to discover, to our current\nunderstanding, the most comprehensive set of English language socio-political\nYouTube channels yet to be analyzed. We experiment with predicting more fine\ngrained political tags for channels using a previously annotated dataset and\nfind that our model performs better than the average individual human reviewer\nfor most of the top tags. This fine grained political tag model is then applied\nto the newly discovered English language socio-political channels to create a\nnew dataset to analyze the amount of traffic going to different political\ncontent. The data shows that some tags, such as \"Partisan Right\" and\n\"Conspiracy\", are significantly under represented when looking only at the most\npopular socio-political channels. Through the use of our methods, we are able\nto get a much more accurate picture of the size of these communities on\nYouTube.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 22:00:04 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Clark", "Sam", ""], ["Zaitsev", "Anna", ""]]}, {"id": "2010.09927", "submitter": "Arvind Srikantan", "authors": "Karthik Radhakrishnan, Arvind Srikantan, Xi Victoria Lin", "title": "ColloQL: Robust Cross-Domain Text-to-SQL Over Search Queries", "comments": "IntEx-SemPar Workshop at EMNLP 2020, 12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating natural language utterances to executable queries is a helpful\ntechnique in making the vast amount of data stored in relational databases\naccessible to a wider range of non-tech-savvy end users. Prior work in this\narea has largely focused on textual input that is linguistically correct and\nsemantically unambiguous. However, real-world user queries are often succinct,\ncolloquial, and noisy, resembling the input of a search engine. In this work,\nwe introduce data augmentation techniques and a sampling-based content-aware\nBERT model (ColloQL) to achieve robust text-to-SQL modeling over natural\nlanguage search (NLS) questions. Due to the lack of evaluation data, we curate\na new dataset of NLS questions and demonstrate the efficacy of our approach.\nColloQL's superior performance extends to well-formed text, achieving 84.9%\n(logical) and 90.7% (execution) accuracy on the WikiSQL dataset, making it, to\nthe best of our knowledge, the highest performing model that does not use\nexecution guided decoding.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 23:53:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Radhakrishnan", "Karthik", ""], ["Srikantan", "Arvind", ""], ["Lin", "Xi Victoria", ""]]}, {"id": "2010.09934", "submitter": "Chengzhi Zhang", "authors": "Yingyi Zhang and Chengzhi Zhang", "title": "Enhancing Keyphrase Extraction from Microblogs using Human Reading Time", "comments": null, "journal-ref": "Journal of the Association for Information Science and\n  Technology,2021", "doi": "10.1002/ASI.24430", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The premise of manual keyphrase annotation is to read the corresponding\ncontent of an annotated object. Intuitively, when we read, more important words\nwill occupy a longer reading time. Hence, by leveraging human reading time, we\ncan find the salient words in the corresponding content. However, previous\nstudies on keyphrase extraction ignore human reading features. In this article,\nwe aim to leverage human reading time to extract keyphrases from microblog\nposts. There are two main tasks in this study. One is to determine how to\nmeasure the time spent by a human on reading a word. We use eye fixation\ndurations extracted from an open source eye-tracking corpus (OSEC). Moreover,\nwe propose strategies to make eye fixation duration more effective on keyphrase\nextraction. The other task is to determine how to integrate human reading time\ninto keyphrase extraction models. We propose two novel neural network models.\nThe first is a model in which the human reading time is used as the ground\ntruth of the attention mechanism. In the second model, we use human reading\ntime as the external feature. Quantitative and qualitative experiments show\nthat our proposed models yield better performance than the baseline models on\ntwo microblog datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:18:44 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 11:24:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yingyi", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2010.10137", "submitter": "Xinyu Ma", "authors": "Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji and Xueqi\n  Cheng", "title": "PROP: Pre-training with Representative Words Prediction for Ad-hoc\n  Retrieval", "comments": "Accepted by WSDM2021", "journal-ref": null, "doi": "10.1145/3437963.3441777", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently pre-trained language representation models such as BERT have shown\ngreat success when fine-tuned on downstream tasks including information\nretrieval (IR). However, pre-training objectives tailored for ad-hoc retrieval\nhave not been well explored. In this paper, we propose Pre-training with\nRepresentative wOrds Prediction (PROP) for ad-hoc retrieval. PROP is inspired\nby the classical statistical language model for IR, specifically the query\nlikelihood model, which assumes that the query is generated as the piece of\ntext representative of the \"ideal\" document. Based on this idea, we construct\nthe representative words prediction (ROP) task for pre-training. Given an input\ndocument, we sample a pair of word sets according to the document language\nmodel, where the set with higher likelihood is deemed as more representative of\nthe document. We then pre-train the Transformer model to predict the pairwise\npreference between the two word sets, jointly with the Masked Language Model\n(MLM) objective. By further fine-tuning on a variety of representative\ndownstream ad-hoc retrieval tasks, PROP achieves significant improvements over\nbaselines without pre-training or with other pre-training methods. We also show\nthat PROP can achieve exciting performance under both the zero- and\nlow-resource IR settings. The code and pre-trained models are available at\nhttps://github.com/Albert-Ma/PROP.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:04:56 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 12:30:20 GMT"}, {"version": "v3", "created": "Sun, 27 Dec 2020 12:22:35 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ma", "Xinyu", ""], ["Guo", "Jiafeng", ""], ["Zhang", "Ruqing", ""], ["Fan", "Yixing", ""], ["Ji", "Xiang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2010.10176", "submitter": "Markus J. Hofmann", "authors": "Markus J. Hofmann, Lara M\\\"uller, Andre R\\\"olke, Ralph Radach and\n  Chris Biemann", "title": "Individual corpora predict fast memory retrieval during reading", "comments": "Proceedings of the 6th workshop on Cognitive Aspects of the Lexicon\n  (CogALex-VI), Barcelona, Spain, December 12, 2020; accepted manuscript; 11\n  pages, 2 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The corpus, from which a predictive language model is trained, can be\nconsidered the experience of a semantic system. We recorded everyday reading of\ntwo participants for two months on a tablet, generating individual corpus\nsamples of 300/500K tokens. Then we trained word2vec models from individual\ncorpora and a 70 million-sentence newspaper corpus to obtain individual and\nnorm-based long-term memory structure. To test whether individual corpora can\nmake better predictions for a cognitive task of long-term memory retrieval, we\ngenerated stimulus materials consisting of 134 sentences with uncorrelated\nindividual and norm-based word probabilities. For the subsequent eye tracking\nstudy 1-2 months later, our regression analyses revealed that individual, but\nnot norm-corpus-based word probabilities can account for first-fixation\nduration and first-pass gaze duration. Word length additionally affected gaze\nduration and total viewing duration. The results suggest that corpora\nrepresentative for an individual's longterm memory structure can better explain\nreading performance than a norm corpus, and that recently acquired information\nis lexically accessed rapidly.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:18:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hofmann", "Markus J.", ""], ["M\u00fcller", "Lara", ""], ["R\u00f6lke", "Andre", ""], ["Radach", "Ralph", ""], ["Biemann", "Chris", ""]]}, {"id": "2010.10252", "submitter": "Marco Wrzalik", "authors": "Marco Wrzalik and Dirk Krechel", "title": "CoRT: Complementary Rankings from Transformers", "comments": "NAACL-HLT 2021, Long Paper", "journal-ref": "Proceedings of the 2021 Conference of the North American Chapter\n  of the Association for Computational Linguistics: Human Language Technologies\n  (pp. 4194-4204). Anthology ID: 2021.naacl-main.331", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent approaches towards neural information retrieval mitigate their\ncomputational costs by using a multi-stage ranking pipeline. In the first\nstage, a number of potentially relevant candidates are retrieved using an\nefficient retrieval model such as BM25. Although BM25 has proven decent\nperformance as a first-stage ranker, it tends to miss relevant passages. In\nthis context we propose CoRT, a simple neural first-stage ranking model that\nleverages contextual representations from pretrained language models such as\nBERT to complement term-based ranking functions while causing no significant\ndelay at query time. Using the MS MARCO dataset, we show that CoRT\nsignificantly increases the candidate recall by complementing BM25 with missing\ncandidates. Consequently, we find subsequent re-rankers achieve superior\nresults with less candidates. We further demonstrate that passage retrieval\nusing CoRT can be realized with surprisingly low latencies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:28:27 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:15:31 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wrzalik", "Marco", ""], ["Krechel", "Dirk", ""]]}, {"id": "2010.10276", "submitter": "Paul Magron", "authors": "Paul Magron, C\\'edric F\\'evotte", "title": "Leveraging the structure of musical preference in content-aware music\n  recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art music recommendation systems are based on collaborative\nfiltering, which predicts a user's interest from his listening habits and\nsimilarities with other users' profiles. These approaches are agnostic to the\nsong content, and therefore face the cold-start problem: they cannot recommend\nnovel songs without listening history. To tackle this issue, content-aware\nrecommendation incorporates information about the songs that can be used for\nrecommending new items. Most methods falling in this category exploit either\nuser-annotated tags, acoustic features or deeply-learned features.\nConsequently, these content features do not have a clear musical meaning, thus\nthey are not necessarily relevant from a musical preference perspective. In\nthis work, we propose instead to leverage a model of musical preference which\noriginates from the field of music psychology. From low-level acoustic features\nwe extract three factors (arousal, valence and depth), which have been shown\nappropriate for describing musical taste. Then we integrate those into a\ncollaborative filtering framework for content-aware music recommendation.\nExperiments conducted on large-scale data show that this approach is able to\naddress the cold-start problem, while using a compact and meaningful set of\nmusical features.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:46:08 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 15:37:32 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Magron", "Paul", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "2010.10333", "submitter": "Wenchang Ma", "authors": "Wenchang Ma, Ryuichi Takanobu, Minghao Tu, Minlie Huang", "title": "Bridging the Gap between Conversational Reasoning and Interactive\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been growing interests in building a conversational recommender\nsystem, where the system simultaneously interacts with the user and explores\nthe user's preference throughout conversational interactions. Recommendation\nand conversation were usually treated as two separate modules with limited\ninformation exchange in existing works, which hinders the capability of both\nsystems: (1) dialog merely incorporated recommendation entities without being\nguided by an explicit recommendation-oriented policy; (2) recommendation\nutilized dialog only as a form of interaction instead of improving\nrecommendation effectively. To address the above issues, we propose a novel\nrecommender dialog model: CR-Walker. In order to view the two separate systems\nwithin a unified framework, we seek high-level mapping between hierarchical\ndialog acts and multi-hop knowledge graph reasoning. The model walks on a\nlarge-scale knowledge graph to form a reasoning tree at each turn, then mapped\nto dialog acts to guide response generation. With such a mapping mechanism as a\nbridge between recommendation and conversation, our framework maximizes the\nmutual benefit between two systems: dialog as an enhancement to recommendation\nquality and explainability, recommendation as a goal and enrichment to dialog\nsemantics. Quantitative evaluation shows that our model excels in conversation\ninformativeness and recommendation effectiveness, at the same time explainable\non the policy level.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 14:53:22 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ma", "Wenchang", ""], ["Takanobu", "Ryuichi", ""], ["Tu", "Minghao", ""], ["Huang", "Minlie", ""]]}, {"id": "2010.10386", "submitter": "Spyretta Leivaditi", "authors": "Spyretta Leivaditi, Julien Rossi, Evangelos Kanoulas", "title": "A Benchmark for Lease Contract Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting entities and other useful information from legal contracts is an\nimportant task whose automation can help legal professionals perform contract\nreviews more efficiently and reduce relevant risks. In this paper, we tackle\nthe problem of detecting two different types of elements that play an important\nrole in a contract review, namely entities and red flags. The latter are terms\nor sentences that indicate that there is some danger or other potentially\nproblematic situation for one or more of the signing parties. We focus on\nsupporting the review of lease agreements, a contract type that has received\nlittle attention in the legal information extraction literature, and we define\nthe types of entities and red flags needed for that task. We release a new\nbenchmark dataset of 179 lease agreement documents that we have manually\nannotated with the entities and red flags they contain, and which can be used\nto train and test relevant extraction algorithms. Finally, we release a new\nlanguage model, called ALeaseBERT, pre-trained on this dataset and fine-tuned\nfor the detection of the aforementioned elements, providing a baseline for\nfurther research\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:50:50 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Leivaditi", "Spyretta", ""], ["Rossi", "Julien", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2010.10409", "submitter": "Faegheh Hasibi", "authors": "Emma J. Gerritse and Faegheh Hasibi and Arjen P. de Vries", "title": "Bias in Conversational Search: The Double-Edged Sword of the\n  Personalized Knowledge Graph", "comments": null, "journal-ref": null, "doi": "10.1145/3409256.3409834", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational AI systems are being used in personal devices, providing users\nwith highly personalized content. Personalized knowledge graphs (PKGs) are one\nof the recently proposed methods to store users' information in a structured\nform and tailor answers to their liking. Personalization, however, is prone to\namplifying bias and contributing to the echo-chamber phenomenon. In this paper,\nwe discuss different types of biases in conversational search systems, with the\nemphasis on the biases that are related to PKGs. We review existing definitions\nof bias in the literature: people bias, algorithm bias, and a combination of\nthe two, and further propose different strategies for tackling these biases for\nconversational search systems. Finally, we discuss methods for measuring bias\nand evaluating user satisfaction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:14:33 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Gerritse", "Emma J.", ""], ["Hasibi", "Faegheh", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "2010.10442", "submitter": "Yue Shang", "authors": "Yunjiang Jiang, Yue Shang, Ziyang Liu, Hongwei Shen, Yun Xiao, Wei\n  Xiong, Sulong Xu, Weipeng Yan and Di Jin", "title": "BERT2DNN: BERT Distillation with Massive Unlabeled Data for Online\n  E-Commerce Search", "comments": "10 pages, 7 figures, to appear in ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance has significant impact on user experience and business profit for\ne-commerce search platform. In this work, we propose a data-driven framework\nfor search relevance prediction, by distilling knowledge from BERT and related\nmulti-layer Transformer teacher models into simple feed-forward networks with\nlarge amount of unlabeled data. The distillation process produces a student\nmodel that recovers more than 97\\% test accuracy of teacher models on new\nqueries, at a serving cost that's several magnitude lower (latency 150x lower\nthan BERT-Base and 15x lower than the most efficient BERT variant, TinyBERT).\nThe applications of temperature rescaling and teacher model stacking further\nboost model accuracy, without increasing the student model complexity.\n  We present experimental results on both in-house e-commerce search relevance\ndata as well as a public data set on sentiment analysis from the GLUE\nbenchmark. The latter takes advantage of another related public data set of\nmuch larger scale, while disregarding its potentially noisy labels. Embedding\nanalysis and case study on the in-house data further highlight the strength of\nthe resulting model. By making the data processing and model training source\ncode public, we hope the techniques presented here can help reduce energy\nconsumption of the state of the art Transformer models and also level the\nplaying field for small organizations lacking access to cutting edge machine\nlearning hardwares.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 16:56:04 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Jiang", "Yunjiang", ""], ["Shang", "Yue", ""], ["Liu", "Ziyang", ""], ["Shen", "Hongwei", ""], ["Xiao", "Yun", ""], ["Xiong", "Wei", ""], ["Xu", "Sulong", ""], ["Yan", "Weipeng", ""], ["Jin", "Di", ""]]}, {"id": "2010.10469", "submitter": "Jingtao Zhan", "authors": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma", "title": "Learning To Retrieve: How to Train a Dense Retrieval Model Effectively\n  and Efficiently", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking has always been one of the top concerns in information retrieval\nresearch. For decades, lexical matching signal has dominated the ad-hoc\nretrieval process, but it also has inherent defects, such as the vocabulary\nmismatch problem. Recently, Dense Retrieval (DR) technique has been proposed to\nalleviate these limitations by capturing the deep semantic relationship between\nqueries and documents. The training of most existing Dense Retrieval models\nrelies on sampling negative instances from the corpus to optimize a pairwise\nloss function. Through investigation, we find that this kind of training\nstrategy is biased and fails to optimize full retrieval performance effectively\nand efficiently. To solve this problem, we propose a Learning To Retrieve\n(LTRe) training technique. LTRe constructs the document index beforehand. At\neach training iteration, it performs full retrieval without negative sampling\nand then updates the query representation model parameters. Through this\nprocess, it teaches the DR model how to retrieve relevant documents from the\nentire corpus instead of how to rerank a potentially biased sample of\ndocuments. Experiments in both passage retrieval and document retrieval tasks\nshow that: 1) in terms of effectiveness, LTRe significantly outperforms all\ncompetitive sparse and dense baselines. It even gains better performance than\nthe BM25-BERT cascade system under reasonable latency constraints. 2) in terms\nof training efficiency, compared with the previous state-of-the-art DR method,\nLTRe provides more than 170x speed-up in the training process. Training with a\ncompressed index further saves computing resources with minor performance loss.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:29:25 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zhan", "Jingtao", ""], ["Mao", "Jiaxin", ""], ["Liu", "Yiqun", ""], ["Zhang", "Min", ""], ["Ma", "Shaoping", ""]]}, {"id": "2010.10508", "submitter": "Zhenan Fan", "authors": "Zhenan Fan, Halyun Jeong, Babhru Joshi, Michael P. Friedlander", "title": "Polar Deconvolution of Mixed Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The signal demixing problem seeks to separate the superposition of multiple\nsignals into its constituent components. This paper provides an algorithm and\ntheoretical analysis that guarantees recovery of the individual signals from\nundersampled and noisy observations of the superposition. In particular, the\ntheory of polar convolution of convex sets and guage functions is applied to\nderive guarantees for a two-stage approach that first decompresses and\nsubsequently deconvolves the observations. If the measurements are random and\nthe noise is bounded, this approach stably recovers low-complexity and\nmutually-incoherent signals with high probability and with optimal sample\ncomplexity. An efficient algorithm is given, based on level-set and\nconditional-gradient methods, which solves at each stage a convex optimization\nwith sublinear iteration complexity. Numerical experiments on both real and\nsynthetic data confirm the theory and effeciency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 04:51:18 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 01:22:02 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Fan", "Zhenan", ""], ["Jeong", "Halyun", ""], ["Joshi", "Babhru", ""], ["Friedlander", "Michael P.", ""]]}, {"id": "2010.10581", "submitter": "Gregory Coppola", "authors": "Gregory Coppola", "title": "A Hub-and-Spoke Model for Content-Moderation-at-Scale on an\n  Information-Sharing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most expensive parts of maintaining a modern information-sharing\nplatform (e.g., web search, social network) is the task of\ncontent-moderation-at-scale. Content moderation is the binary task of\ndetermining whether or not a given user-created message meets the editorial\nteam's content guidelines for the site. The challenge is that the number of\nmessages to check scales with the number of users, which is much larger than\nthe number of moderator-employees working for the given platform.\n  We show how content moderation can be achieved significantly more cheaply\nthan before, in the special case where all messages are public, by effectively\nplatformizing the task of content moderation. Our approach is to use a\nhub-and-spoke model. The hub is the core editorial team delegated by the\nmanagement of the given platform. The spokes are the individual users. The\nratings of the editorial team create the labels for a statistical learning\nalgorithm, while the ratings of the users are used as features.\n  We have implemented a primitive version of this algorithm into our\nopen-source DimensionRank code base, found at \"thinkdifferentagain.art\".\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:37:24 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Coppola", "Gregory", ""]]}, {"id": "2010.10685", "submitter": "Gregory Coppola", "authors": "Gregory Coppola", "title": "Fact-Checking at Scale with DimensionRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important problem that has emerged after twenty years of popular\ninternet usage is that of fact-checking at scale. This problem is experienced\nacutely in both of the major internet application platform types, web search\nand social media.\n  We offer a working definition of what a \"platform\" is. We critically\ndeconstruct what we call the \"PolitiFact\" model of fact checking, and show it\nto be inherently inferior for fact-checking at scale to a platform-b ased\nsolution.\n  Our central contribution is to show how to effectively platformize the\nproblem of fact-checking at scale. We show how a two-dimensional rating system,\nwith dimensions agreement and hotness allows us to create information-seeking\nqueries not possible with the on e-dimensional rating system predominating on\nexisting platforms. And, we show that, underlying our user-friendly\nuser-interface, lies a system that allows the creation of formal proofs in the\npropositional calculus.\n  Our algorithm is implemented in our open-source DimensionRank software\npackage available at \"https://thinkdifferentagain.art\".\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 00:39:04 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Coppola", "Gregory", ""]]}, {"id": "2010.10755", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Ying Sheng, Nguyen Vo, Sandeep Tata", "title": "FreeDOM: A Transferable Neural Architecture for Structured Information\n  Extraction on Web Documents", "comments": "in Proc. of KDD 2020 (Research Track). Figure 5 updated", "journal-ref": null, "doi": "10.1145/3394486.3403153", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured data from HTML documents is a long-studied problem with\na broad range of applications like augmenting knowledge bases, supporting\nfaceted search, and providing domain-specific experiences for key verticals\nlike shopping and movies. Previous approaches have either required a small\nnumber of examples for each target site or relied on carefully handcrafted\nheuristics built over visual renderings of websites. In this paper, we present\na novel two-stage neural approach, named FreeDOM, which overcomes both these\nlimitations. The first stage learns a representation for each DOM node in the\npage by combining both the text and markup information. The second stage\ncaptures longer range distance and semantic relatedness using a relational\nneural network. By combining these stages, FreeDOM is able to generalize to\nunseen sites after training on a small number of seed sites from that vertical\nwithout requiring expensive hand-crafted features over visual renderings of the\npage. Through experiments on a public dataset with 8 different verticals, we\nshow that FreeDOM beats the previous state of the art by nearly 3.7 F1 points\non average without requiring features over rendered pages or expensive\nhand-crafted features.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 04:20:13 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Sheng", "Ying", ""], ["Vo", "Nguyen", ""], ["Tata", "Sandeep", ""]]}, {"id": "2010.10783", "submitter": "Jiancan Wu", "authors": "Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, Jianxun\n  Lian, and Xing Xie", "title": "Self-supervised Graph Learning for Recommendation", "comments": "11 pages, 7 figures, 5 tables. Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462862", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Representation learning on user-item graph for recommendation has evolved\nfrom using single ID or interaction history to exploiting higher-order\nneighbors. This leads to the success of graph convolution networks (GCNs) for\nrecommendation such as PinSage and LightGCN. Despite effectiveness, we argue\nthat they suffer from two limitations: (1) high-degree nodes exert larger\nimpact on the representation learning, deteriorating the recommendations of\nlow-degree (long-tail) items; and (2) representations are vulnerable to noisy\ninteractions, as the neighborhood aggregation scheme further enlarges the\nimpact of observed edges.\n  In this work, we explore self-supervised learning on user-item graph, so as\nto improve the accuracy and robustness of GCNs for recommendation. The idea is\nto supplement the classical supervised task of recommendation with an auxiliary\nself-supervised task, which reinforces node representation learning via\nself-discrimination. Specifically, we generate multiple views of a node,\nmaximizing the agreement between different views of the same node compared to\nthat of other nodes. We devise three operators to generate the views -- node\ndropout, edge dropout, and random walk -- that change the graph structure in\ndifferent manners. We term this new learning paradigm as\n\\textit{Self-supervised Graph Learning} (SGL), implementing it on the\nstate-of-the-art model LightGCN. Through theoretical analyses, we find that SGL\nhas the ability of automatically mining hard negatives. Empirical studies on\nthree benchmark datasets demonstrate the effectiveness of SGL, which improves\nthe recommendation accuracy, especially on long-tail items, and the robustness\nagainst interaction noises. Our implementations are available at\n\\url{https://github.com/wujcan/SGL}.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 06:35:26 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:58:32 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 02:15:00 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 11:56:37 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wu", "Jiancan", ""], ["Wang", "Xiang", ""], ["Feng", "Fuli", ""], ["He", "Xiangnan", ""], ["Chen", "Liang", ""], ["Lian", "Jianxun", ""], ["Xie", "Xing", ""]]}, {"id": "2010.10784", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting\n  Chen, Lichan Hong, Ed H. Chi", "title": "Learning to Embed Categorical Features without Embedding Tables for\n  Recommendation", "comments": "Accepted to KDD'21, Research Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding learning of categorical features (e.g. user/item IDs) is at the\ncore of various recommendation models including matrix factorization and neural\ncollaborative filtering. The standard approach creates an embedding table where\neach row represents a dedicated embedding vector for every unique feature\nvalue. However, this method fails to efficiently handle high-cardinality\nfeatures and unseen feature values (e.g. new video ID) that are prevalent in\nreal-world recommendation systems. In this paper, we propose an alternative\nembedding framework Deep Hash Embedding (DHE), replacing embedding tables by a\ndeep embedding network to compute embeddings on the fly. DHE first encodes the\nfeature value to a unique identifier vector with multiple hashing functions and\ntransformations, and then applies a DNN to convert the identifier vector to an\nembedding. The encoding module is deterministic, non-learnable, and free of\nstorage, while the embedding network is updated during the training time to\nlearn embedding generation. Empirical results show that DHE achieves comparable\nAUC against the standard one-hot full embedding, with smaller model sizes. Our\nwork sheds light on the design of DNN-based alternative embedding schemes for\ncategorical features without using embedding table lookup.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 06:37:28 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 06:31:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Cheng", "Derek Zhiyuan", ""], ["Yao", "Tiansheng", ""], ["Yi", "Xinyang", ""], ["Chen", "Ting", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""]]}, {"id": "2010.10789", "submitter": "Weizhen Qi", "authors": "Weizhen Qi, Yeyun Gong, Yu Yan, Jian Jiao, Bo Shao, Ruofei Zhang,\n  Houqiang Li, Nan Duan, Ming Zhou", "title": "ProphetNet-Ads: A Looking Ahead Strategy for Generative Retrieval Models\n  in Sponsored Search Engine", "comments": "Accepted to NLPCC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a sponsored search engine, generative retrieval models are recently\nproposed to mine relevant advertisement keywords for users' input queries.\nGenerative retrieval models generate outputs token by token on a path of the\ntarget library prefix tree (Trie), which guarantees all of the generated\noutputs are legal and covered by the target library. In actual use, we found\nseveral typical problems caused by Trie-constrained searching length. In this\npaper, we analyze these problems and propose a looking ahead strategy for\ngenerative retrieval models named ProphetNet-Ads. ProphetNet-Ads improves the\nretrieval ability by directly optimizing the Trie-constrained searching space.\nWe build a dataset from a real-word sponsored search engine and carry out\nexperiments to analyze different generative retrieval models. Compared with\nTrie-based LSTM generative retrieval model proposed recently, our single model\nresult and integrated result improve the recall by 15.58\\% and 18.8\\%\nrespectively with beam size 5. Case studies further demonstrate how these\nproblems are alleviated by ProphetNet-Ads clearly.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:03:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Qi", "Weizhen", ""], ["Gong", "Yeyun", ""], ["Yan", "Yu", ""], ["Jiao", "Jian", ""], ["Shao", "Bo", ""], ["Zhang", "Ruofei", ""], ["Li", "Houqiang", ""], ["Duan", "Nan", ""], ["Zhou", "Ming", ""]]}, {"id": "2010.10817", "submitter": "Chengzhi Zhang", "authors": "Yuzhuo Wang, Chengzhi Zhang", "title": "Using the Full-text Content of Academic Articles to Identify and\n  Evaluate Algorithm Entities in the Domain of Natural Language Processing", "comments": null, "journal-ref": "Journal of Informetrics,2020", "doi": "10.1016/j.joi.2020.101091", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, the advancement, improvement, and application of\nalgorithms in academic research have played an important role in promoting the\ndevelopment of different disciplines. Academic papers in various disciplines,\nespecially computer science, contain a large number of algorithms. Identifying\nthe algorithms from the full-text content of papers can determine popular or\nclassical algorithms in a specific field and help scholars gain a comprehensive\nunderstanding of the algorithms and even the field. To this end, this article\ntakes the field of natural language processing (NLP) as an example and\nidentifies algorithms from academic papers in the field. A dictionary of\nalgorithms is constructed by manually annotating the contents of papers, and\nsentences containing algorithms in the dictionary are extracted through\ndictionary-based matching. The number of articles mentioning an algorithm is\nused as an indicator to analyze the influence of that algorithm. Our results\nreveal the algorithm with the highest influence in NLP papers and show that\nclassification algorithms represent the largest proportion among the\nhigh-impact algorithms. In addition, the evolution of the influence of\nalgorithms reflects the changes in research tasks and topics in the field, and\nthe changes in the influence of different algorithms show different trends. As\na preliminary exploration, this paper conducts an analysis of the impact of\nalgorithms mentioned in the academic text, and the results can be used as\ntraining data for the automatic extraction of large-scale algorithms in the\nfuture. The methodology in this paper is domain-independent and can be applied\nto other domains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 08:24:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wang", "Yuzhuo", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2010.10932", "submitter": "Sungchul Choi", "authors": "Jaewoong Choi, Sion Jang, Jaeyoung Kim, Jiho Lee, Janghyeok Yoona,\n  Sungchul Choi", "title": "Deep learning-based citation recommendation system for patents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we address the challenges in developing a deep learning-based\nautomatic patent citation recommendation system. Although deep learning-based\nrecommendation systems have exhibited outstanding performance in various\ndomains (such as movies, products, and paper citations), their validity in\npatent citations has not been investigated, owing to the lack of a freely\navailable high-quality dataset and relevant benchmark model. To solve these\nproblems, we present a novel dataset called PatentNet that includes textual\ninformation and metadata for approximately 110,000 patents from the Google Big\nQuery service. Further, we propose strong benchmark models considering the\nsimilarity of textual information and metadata (such as cooperative patent\nclassification code). Compared with existing recommendation methods, the\nproposed benchmark method achieved a mean reciprocal rank of 0.2377 on the test\nset, whereas the existing state-of-the-art recommendation method achieved\n0.2073.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:18:21 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Choi", "Jaewoong", ""], ["Jang", "Sion", ""], ["Kim", "Jaeyoung", ""], ["Lee", "Jiho", ""], ["Yoona", "Janghyeok", ""], ["Choi", "Sungchul", ""]]}, {"id": "2010.11060", "submitter": "Yitong Ji", "authors": "Yitong Ji, Aixin Sun, Jie Zhang, Chenliang Li", "title": "A Critical Study on Data Leakage in Recommender System Offline\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In academic research, recommender models are often evaluated on offline\ndatasets. The offline dataset is first split to training and test instances.\nAll training instances are then modelled in a user-item interaction matrix\nwhich can be used to train recommender models. Many such offline evaluations\nignore the global timeline in the data, which leads to \"data leakage\": a model\nlearns from future data to predict a current value, making the evaluation\nunrealistic. In this paper, we evaluate the impact of \"data leakage\" using two\nwidely adopted baseline models, BPR and NeuMF, on four popular offline datasets\n- MovieLens-25M, Yelp, Amazon-music, and Amazon-electronic. We show that\naccessing to different amount of future data may improve or deteriorate a\nmodel's recommendation accuracy. That is, ignoring global timeline in offline\nevaluation makes the performance among recommendation models not comparable. We\nshare our understanding of these observations and highlight the importance of\npreserving the global timeline. We also call for a revisit of recommender\nsystem offline evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:09:20 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 12:24:14 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ji", "Yitong", ""], ["Sun", "Aixin", ""], ["Zhang", "Jie", ""], ["Li", "Chenliang", ""]]}, {"id": "2010.11066", "submitter": "Chenyu You", "authors": "Chenyu You, Nuo Chen, Yuexian Zou", "title": "Contextualized Attention-based Knowledge Transfer for Spoken\n  Conversational Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken conversational question answering (SCQA) requires machines to model\ncomplex dialogue flow given the speech utterances and text corpora. Different\nfrom traditional text question answering (QA) tasks, SCQA involves audio signal\nprocessing, passage comprehension, and contextual understanding. However, ASR\nsystems introduce unexpected noisy signals to the transcriptions, which result\nin performance degradation on SCQA. To overcome the problem, we propose CADNet,\na novel contextualized attention-based distillation approach, which applies\nboth cross-attention and self-attention to obtain ASR-robust contextualized\nembedding representations of the passage and dialogue history for performance\nimprovements. We also introduce the spoken conventional knowledge distillation\nframework to distill the ASR-robust knowledge from the estimated probabilities\nof the teacher model to the student. We conduct extensive experiments on the\nSpoken-CoQA dataset and demonstrate that our approach achieves remarkable\nperformance in this task.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:17:18 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 02:17:26 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 20:04:11 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 16:32:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["You", "Chenyu", ""], ["Chen", "Nuo", ""], ["Zou", "Yuexian", ""]]}, {"id": "2010.11067", "submitter": "Chenyu You", "authors": "Chenyu You, Nuo Chen, Yuexian Zou", "title": "Knowledge Distillation for Improved Accuracy in Spoken Question\n  Answering", "comments": "To appear in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken question answering (SQA) is a challenging task that requires the\nmachine to fully understand the complex spoken documents. Automatic speech\nrecognition (ASR) plays a significant role in the development of QA systems.\nHowever, the recent work shows that ASR systems generate highly noisy\ntranscripts, which critically limit the capability of machine comprehension on\nthe SQA task. To address the issue, we present a novel distillation framework.\nSpecifically, we devise a training strategy to perform knowledge distillation\n(KD) from spoken documents and written counterparts. Our work makes a step\ntowards distilling knowledge from the language model as a supervision signal to\nlead to better student accuracy by reducing the misalignment between automatic\nand manual transcriptions. Experiments demonstrate that our approach\noutperforms several state-of-the-art language models on the Spoken-SQuAD\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 15:18:01 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 02:16:42 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 02:26:27 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["You", "Chenyu", ""], ["Chen", "Nuo", ""], ["Zou", "Yuexian", ""]]}, {"id": "2010.11386", "submitter": "Jheng-Hong Yang", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin", "title": "Distilling Dense Representations for Ranking using Tightly-Coupled\n  Teachers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to ranking with dense representations that applies\nknowledge distillation to improve the recently proposed late-interaction\nColBERT model. Specifically, we distill the knowledge from ColBERT's expressive\nMaxSim operator for computing relevance scores into a simple dot product, thus\nenabling single-step ANN search. Our key insight is that during distillation,\ntight coupling between the teacher model and the student model enables more\nflexible distillation strategies and yields better learned representations. We\nempirically show that our approach improves query latency and greatly reduces\nthe onerous storage requirements of ColBERT, while only making modest\nsacrifices in terms of effectiveness. By combining our dense representations\nwith sparse representations derived from document expansion, we are able to\napproach the effectiveness of a standard cross-encoder reranker using BERT that\nis orders of magnitude slower.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 02:26:01 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Lin", "Jimmy", ""]]}, {"id": "2010.11401", "submitter": "Chenghao Liu", "authors": "Jianwen Yin, Chenghao Liu, Weiqing Wang, Jianling Sun, Steven C.H. Hoi", "title": "Learning Transferrable Parameters for Long-tailed Sequential User\n  Behavior Modeling", "comments": "Accepted by KDD2020", "journal-ref": null, "doi": "10.1145/3394486.3403078", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential user behavior modeling plays a crucial role in online\nuser-oriented services, such as product purchasing, news feed consumption, and\nonline advertising. The performance of sequential modeling heavily depends on\nthe scale and quality of historical behaviors. However, the number of user\nbehaviors inherently follows a long-tailed distribution, which has been seldom\nexplored. In this work, we argue that focusing on tail users could bring more\nbenefits and address the long tails issue by learning transferrable parameters\nfrom both optimization and feature perspectives. Specifically, we propose a\ngradient alignment optimizer and adopt an adversarial training scheme to\nfacilitate knowledge transfer from the head to the tail. Such methods can also\ndeal with the cold-start problem of new users. Moreover, it could be directly\nadaptive to various well-established sequential models. Extensive experiments\non four real-world datasets verify the superiority of our framework compared\nwith the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:12:02 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 13:36:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yin", "Jianwen", ""], ["Liu", "Chenghao", ""], ["Wang", "Weiqing", ""], ["Sun", "Jianling", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2010.11419", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Xiaohan Li, Ziwei Fan, Stephen Guo, Kannan Achan, and\n  Philip S. Yu", "title": "Basket Recommendation with Multi-Intent Translation Graph Neural Network", "comments": "Accepted to IEEE Bigdata 2020. Code is available online at\n  https://github.com/JimLiu96/MITGNN", "journal-ref": "978-1-7281-6251-5/20/\\$31.00~\\copyright2020 IEEE", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of basket recommendation~(BR) is to recommend a ranking list of\nitems to the current basket. Existing methods solve this problem by assuming\nthe items within the same basket are correlated by one semantic relation, thus\noptimizing the item embeddings. However, this assumption breaks when there\nexist multiple intents within a basket. For example, assuming a basket contains\n\\{\\textit{bread, cereal, yogurt, soap, detergent}\\} where \\{\\textit{bread,\ncereal, yogurt}\\} are correlated through the \"breakfast\" intent, while\n\\{\\textit{soap, detergent}\\} are of \"cleaning\" intent, ignoring multiple\nrelations among the items spoils the ability of the model to learn the\nembeddings. To resolve this issue, it is required to discover the intents\nwithin the basket. However, retrieving a multi-intent pattern is rather\nchallenging, as intents are latent within the basket. Additionally, intents\nwithin the basket may also be correlated. Moreover, discovering a multi-intent\npattern requires modeling high-order interactions, as the intents across\ndifferent baskets are also correlated. To this end, we propose a new framework\nnamed as \\textbf{M}ulti-\\textbf{I}ntent \\textbf{T}ranslation \\textbf{G}raph\n\\textbf{N}eural \\textbf{N}etwork~({\\textbf{MITGNN}}). MITGNN models $T$ intents\nas tail entities translated from one corresponding basket embedding via $T$\nrelation vectors. The relation vectors are learned through multi-head\naggregators to handle user and item information. Additionally, MITGNN\npropagates multiple intents across our defined basket graph to learn the\nembeddings of users and items by aggregating neighbors. Extensive experiments\non two real-world datasets prove the effectiveness of our proposed model on\nboth transductive and inductive BR. The code is available online at\nhttps://github.com/JimLiu96/MITGNN.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 03:52:00 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Liu", "Zhiwei", ""], ["Li", "Xiaohan", ""], ["Fan", "Ziwei", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""], ["Yu", "Philip S.", ""]]}, {"id": "2010.11512", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski, Oriol Nieto, Matthew McCallum, Minz Won, Sergio\n  Oramas, Erik Schmidt", "title": "Mood Classification Using Listening Data", "comments": "Appears in Proc. of the International Society for Music Information\n  Retrieval Conference 2020 (ISMIR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mood of a song is a highly relevant feature for exploration and\nrecommendation in large collections of music. These collections tend to require\nautomatic methods for predicting such moods. In this work, we show that\nlistening-based features outperform content-based ones when classifying moods:\nembeddings obtained through matrix factorization of listening data appear to be\nmore informative of a track mood than embeddings based on its audio content. To\ndemonstrate this, we compile a subset of the Million Song Dataset, totalling\n67k tracks, with expert annotations of 188 different moods collected from\nAllMusic. Our results on this novel dataset not only expose the limitations of\ncurrent audio-based models, but also aim to foster further reproducible\nresearch on this timely topic.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 08:13:56 GMT"}], "update_date": "2020-10-24", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Nieto", "Oriol", ""], ["McCallum", "Matthew", ""], ["Won", "Minz", ""], ["Oramas", "Sergio", ""], ["Schmidt", "Erik", ""]]}, {"id": "2010.11557", "submitter": "Francisco J. Escribano", "authors": "S. Zurita-Zurita, Francisco J. Escribano, J. S\\'aez-Landete, J.A.\n  Rodr\\'iguez-Manfredi", "title": "Denoising Atmospheric Temperature Measurements Taken by the Mars Science\n  Laboratory on the Martian Surface", "comments": "10 pages, 7 figures, accepted for publication at IEEE Transactions on\n  Instrumentation and Measurement", "journal-ref": null, "doi": "10.1109/TIM.2020.3034986", "report-no": null, "categories": "eess.SP astro-ph.EP cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present article we analyze data from two temperature sensors of the\nMars Science Laboratory, which has been active in Mars since August 2012.\nTemperature measurements received from the rover are noisy and must be\nprocessed and validated before being delivered to the scientific community.\nCurrently, a simple Moving Average (MA) filter is used to perform signal\ndenoising. The application of this basic method relies on the assumption that\nthe noise is stationary and statistically independent from the underlying\nstructure of the signal, an arguable assumption in this kind of harsh\nenvironment. In this paper, we analyze the application of two alternative\nmethods to process the temperature sensor measurements: the Discrete Wavelet\nTransform (DWT) and the Hilbert-Huang Transform (HHT). We consider two\ndifferent datasets, one belonging to the current Martian measurement campaigns,\nand the other to the Thermal Vacuum Tests. The processing of these datasets\nallows to separate the random noise from the interference created by other\nsystems. The experiments show that the MA filter may provide useful results\nunder given circumstances. However, the proposed methods allow a better fitting\nfor all the realistic scenarios, while providing the possibility to identify\nand analyze other interesting signal features and artifacts that could be later\nstudied and classified. The large amount of data to be processed makes\ncomputational efficiency an important requirement in this mission. Considering\nthe computational cost and the filtering performance, we propose the method\nbased on DWT as more suitable for this application.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 09:30:13 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 08:32:23 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zurita-Zurita", "S.", ""], ["Escribano", "Francisco J.", ""], ["S\u00e1ez-Landete", "J.", ""], ["Rodr\u00edguez-Manfredi", "J. A.", ""]]}, {"id": "2010.11634", "submitter": "Alberto Natali", "authors": "Alberto Natali, Mario Coutino, Elvin Isufi and Geert Leus", "title": "Online Time-Varying Topology Identification via Prediction-Correction\n  Algorithms", "comments": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal processing and machine learning algorithms for data supported over\ngraphs, require the knowledge of the graph topology. Unless this information is\ngiven by the physics of the problem (e.g., water supply networks, power grids),\nthe topology has to be learned from data. Topology identification is a\nchallenging task, as the problem is often ill-posed, and becomes even harder\nwhen the graph structure is time-varying. In this paper, we address the problem\nof dynamic topology identification by building on recent results from\ntime-varying optimization, devising a general-purpose online algorithm\noperating in non-stationary environments. Because of its iteration-constrained\nnature, the proposed approach exhibits an intrinsic temporal-regularization of\nthe graph topology without explicitly enforcing it. As a case-study, we\nspecialize our method to the Gaussian graphical model (GGM) problem and\ncorroborate its performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 12:02:27 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 15:45:53 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Natali", "Alberto", ""], ["Coutino", "Mario", ""], ["Isufi", "Elvin", ""], ["Leus", "Geert", ""]]}, {"id": "2010.11786", "submitter": "Benjamin Ricaud", "authors": "Benjamin Ricaud, Nicolas Aspert and Volodymyr Miz", "title": "Spikyball sampling: Exploring large networks via an inhomogeneous\n  filtered diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying real-world networks such as social networks or web networks is a\nchallenge. These networks often combine a complex, highly connected structure\ntogether with a large size. We propose a new approach for large scale networks\nthat is able to automatically sample user-defined relevant parts of a network.\nStarting from a few selected places in the network and a reduced set of\nexpansion rules, the method adopts a filtered breadth-first search approach,\nthat expands through edges and nodes matching these properties. Moreover, the\nexpansion is performed over a random subset of neighbors at each step to\nmitigate further the overwhelming number of connections that may exist in large\ngraphs. This carries the image of a \"spiky\" expansion. We show that this\napproach generalize previous exploration sampling methods, such as Snowball or\nForest Fire and extend them. We demonstrate its ability to capture groups of\nnodes with high interactions while discarding weakly connected nodes that are\noften numerous in social networks and may hide important structures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 15:01:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ricaud", "Benjamin", ""], ["Aspert", "Nicolas", ""], ["Miz", "Volodymyr", ""]]}, {"id": "2010.11910", "submitter": "Sungkyun Chang", "authors": "Sungkyun Chang, Donmoon Lee, Jeongsoo Park, Hyungui Lim, Kyogu Lee,\n  Karam Ko, Yoonchang Han", "title": "Neural Audio Fingerprint for High-specific Audio Retrieval based on\n  Contrastive Learning", "comments": "ICASSP 2021 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing audio fingerprinting systems have limitations to be used for\nhigh-specific audio retrieval at scale. In this work, we generate a\nlow-dimensional representation from a short unit segment of audio, and couple\nthis fingerprint with a fast maximum inner-product search. To this end, we\npresent a contrastive learning framework that derives from the segment-level\nsearch objective. Each update in training uses a batch consisting of a set of\npseudo labels, randomly selected original samples, and their augmented\nreplicas. These replicas can simulate the degrading effects on original audio\nsignals by applying small time offsets and various types of distortions, such\nas background noise and room/microphone impulse responses. In the segment-level\nsearch task, where the conventional audio fingerprinting systems used to fail,\nour system using 10x smaller storage has shown promising results. Our code and\ndataset are available at \\url{https://mimbres.github.io/neural-audio-fp/}.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:44:40 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 07:36:03 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 09:07:02 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 08:23:59 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chang", "Sungkyun", ""], ["Lee", "Donmoon", ""], ["Park", "Jeongsoo", ""], ["Lim", "Hyungui", ""], ["Lee", "Kyogu", ""], ["Ko", "Karam", ""], ["Han", "Yoonchang", ""]]}, {"id": "2010.11930", "submitter": "Rodrigo Nogueira", "authors": "Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, Jimmy Lin", "title": "Scientific Claim Verification with VERT5ERINI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the adaptation of a pretrained sequence-to-sequence model\nto the task of scientific claim verification in the biomedical domain. We\npropose VERT5ERINI that exploits T5 for abstract retrieval, sentence selection\nand label prediction, which are three critical sub-tasks of claim verification.\nWe evaluate our pipeline on SCIFACT, a newly curated dataset that requires\nmodels to not just predict the veracity of claims but also provide relevant\nsentences from a corpus of scientific literature that support this decision.\nEmpirically, our pipeline outperforms a strong baseline in each of the three\nsteps. Finally, we show VERT5ERINI's ability to generalize to two new datasets\nof COVID-19 claims using evidence from the ever-expanding CORD-19 corpus.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:56:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pradeep", "Ronak", ""], ["Ma", "Xueguang", ""], ["Nogueira", "Rodrigo", ""], ["Lin", "Jimmy", ""]]}, {"id": "2010.12174", "submitter": "Rubungo Andre Niyongabo", "authors": "Rubungo Andre Niyongabo and Hong Qu and Julia Kreutzer and Li Huang", "title": "KINNEWS and KIRNEWS: Benchmarking Cross-Lingual Text Classification for\n  Kinyarwanda and Kirundi", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in text classification has been focused on high-resource\nlanguages such as English and Chinese. For low-resource languages, amongst them\nmost African languages, the lack of well-annotated data and effective\npreprocessing, is hindering the progress and the transfer of successful\nmethods. In this paper, we introduce two news datasets (KINNEWS and KIRNEWS)\nfor multi-class classification of news articles in Kinyarwanda and Kirundi, two\nlow-resource African languages. The two languages are mutually intelligible,\nbut while Kinyarwanda has been studied in Natural Language Processing (NLP) to\nsome extent, this work constitutes the first study on Kirundi. Along with the\ndatasets, we provide statistics, guidelines for preprocessing, and monolingual\nand cross-lingual baseline models. Our experiments show that training\nembeddings on the relatively higher-resourced Kinyarwanda yields successful\ncross-lingual transfer to Kirundi. In addition, the design of the created\ndatasets allows for a wider use in NLP beyond text classification in future\nstudies, such as representation learning, cross-lingual learning with more\ndistant languages, or as base for new annotations for tasks such as parsing,\nPOS tagging, and NER. The datasets, stopwords, and pre-trained embeddings are\npublicly available at https://github.com/Andrews2017/KINNEWS-and-KIRNEWS-Corpus .\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:37:42 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Niyongabo", "Rubungo Andre", ""], ["Qu", "Hong", ""], ["Kreutzer", "Julia", ""], ["Huang", "Li", ""]]}, {"id": "2010.12175", "submitter": "Yahya Sattar", "authors": "Yahya Sattar and Zubair Khalid", "title": "Estimation of Groundwater Storage Variations in Indus River Basin using\n  GRACE Data", "comments": null, "journal-ref": "IEEE ICASSP, 2021", "doi": null, "report-no": null, "categories": "eess.SP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The depletion and variations of groundwater storage~(GWS) are of critical\nimportance for sustainable groundwater management. In this work, we use Gravity\nRecovery and Climate Experiment (GRACE) to estimate variations in the\nterrestrial water storage~(TWS) and use it in conjunction with the Global Land\nData Assimilation System~(GLDAS) data to extract GWS variations over time for\nIndus river basin~(IRB). We present a data processing framework that processes\nand combines these data-sets to provide an estimate of GWS changes. We also\npresent the design of a band-limited optimally concentrated window function for\nspatial localization of the data in the region of interest. We construct the\nso-called optimal window for the IRB region and use it in our processing\nframework to analyze the GWS variations from 2005 to 2015. Our analysis reveals\nthe expected seasonal variations in GWS and signifies groundwater depletion on\naverage over the time period. Our proposed processing framework can be used to\nanalyze spatio-temporal variations in TWS and GWS for any region of interest.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 05:40:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sattar", "Yahya", ""], ["Khalid", "Zubair", ""]]}, {"id": "2010.12256", "submitter": "Jinbo Song", "authors": "Jinbo Song and Chao Chang and Fei Sun and Xinbo Song and Peng Jiang", "title": "NGAT4Rec: Neighbor-Aware Graph Attention Network For Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning informative representations (aka. embeddings) of users and items is\nthe core of modern recommender systems. Previous works exploit user-item\nrelationships of one-hop neighbors in the user-item interaction graph to\nimprove the quality of representation. Recently, the research of Graph Neural\nNetwork (GNN) for recommendation considers the implicit collaborative\ninformation of multi-hop neighbors to enrich the representation. However, most\nworks of GNN for recommendation systems do not consider the relational\ninformation which implies the expression differences of different neighbors in\nthe neighborhood explicitly. The influence of each neighboring item to the\nrepresentation of the user's preference can be represented by the correlation\nbetween the item and neighboring items of the user. Symmetrically, for a given\nitem, the correlation between one neighboring user and neighboring users can\nreflect the strength of signal about the item's characteristic. To modeling the\nimplicit correlations of neighbors in graph embedding aggregating, we propose a\nNeighbor-Aware Graph Attention Network for recommendation task, termed\nNGAT4Rec. It employs a novel neighbor-aware graph attention layer that assigns\ndifferent neighbor-aware attention coefficients to different neighbors of a\ngiven node by computing the attention among these neighbors pairwisely. Then\nNGAT4Rec aggregates the embeddings of neighbors according to the corresponding\nneighbor-aware attention coefficients to generate next layer embedding for\nevery node. Furthermore, we combine more neighbor-aware graph attention layer\nto gather the influential signals from multi-hop neighbors. We remove feature\ntransformation and nonlinear activation that proved to be useless on\ncollaborative filtering. Extensive experiments on three benchmark datasets show\nthat our model outperforms various state-of-the-art models consistently.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:37:43 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 10:13:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Song", "Jinbo", ""], ["Chang", "Chao", ""], ["Sun", "Fei", ""], ["Song", "Xinbo", ""], ["Jiang", "Peng", ""]]}, {"id": "2010.12284", "submitter": "Yong Liu Stephen", "authors": "Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong\n  Zhang, Aixin Sun, Chunyan Miao", "title": "Pre-training Graph Transformer with Multimodal Side Information for\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Side information of items, e.g., images and text description, has shown to be\neffective in contributing to accurate recommendations. Inspired by the recent\nsuccess of pre-training models on natural language and images, we propose a\npre-training strategy to learn item representations by considering both item\nside information and their relationships. We relate items by common user\nactivities, e.g., co-purchase, and construct a homogeneous item graph. This\ngraph provides a unified view of item relations and their associated side\ninformation in multimodality. We develop a novel sampling algorithm named\nMCNSampling to select contextual neighbors for each item. The proposed\nPre-trained Multimodal Graph Transformer (PMGT) learns item representations\nwith two objectives: 1) graph structure reconstruction, and 2) masked node\nfeature reconstruction. Experimental results on real datasets demonstrate that\nthe proposed PMGT model effectively exploits the multimodality side information\nto achieve better accuracies in downstream tasks including item recommendation,\nitem classification, and click-through ratio prediction. We also report a case\nstudy of testing the proposed PMGT model in an online setting with 600 thousand\nusers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 10:30:24 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 10:39:57 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Liu", "Yong", ""], ["Yang", "Susen", ""], ["Lei", "Chenyi", ""], ["Wang", "Guoxin", ""], ["Tang", "Haihong", ""], ["Zhang", "Juyong", ""], ["Sun", "Aixin", ""], ["Miao", "Chunyan", ""]]}, {"id": "2010.12363", "submitter": "Kaito Ariu", "authors": "Kaito Ariu, Narae Ryu, Se-Young Yun, Alexandre Prouti\\`ere", "title": "Regret in Online Recommendation Systems", "comments": "Advances in Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a theoretical analysis of recommendation systems in an\nonline setting, where items are sequentially recommended to users over time. In\neach round, a user, randomly picked from a population of $m$ users, requests a\nrecommendation. The decision-maker observes the user and selects an item from a\ncatalogue of $n$ items. Importantly, an item cannot be recommended twice to the\nsame user. The probabilities that a user likes each item are unknown. The\nperformance of the recommendation algorithm is captured through its regret,\nconsidering as a reference an Oracle algorithm aware of these probabilities. We\ninvestigate various structural assumptions on these probabilities: we derive\nfor each structure regret lower bounds, and devise algorithms achieving these\nlimits. Interestingly, our analysis reveals the relative weights of the\ndifferent components of regret: the component due to the constraint of not\npresenting the same item twice to the same user, that due to learning the\nchances users like items, and finally that arising when learning the underlying\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:48:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ariu", "Kaito", ""], ["Ryu", "Narae", ""], ["Yun", "Se-Young", ""], ["Prouti\u00e8re", "Alexandre", ""]]}, {"id": "2010.12370", "submitter": "Sara Abdollahi", "authors": "Sara Abdollahi, Simon Gottschalk, Elena Demidova", "title": "EventKG+Click: A Dataset of Language-specific Event-centric User\n  Interaction Traces", "comments": "In Proceedings of the 1st International Workshop on Cross-lingual\n  Event-centric Open Analytics co-located with the 17th Extended Semantic Web\n  Conference (ESWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing need to analyse event-centric cross-lingual information calls\nfor innovative user interaction models that assist users in crossing the\nlanguage barrier. However, datasets that reflect user interaction traces in\ncross-lingual settings required to train and evaluate the user interaction\nmodels are mostly missing. In this paper, we present the EventKG+Click dataset\nthat aims to facilitate the creation and evaluation of such interaction models.\nEventKG+Click builds upon the event-centric EventKG knowledge graph and\nlanguage-specific information on user interactions with events, entities, and\ntheir relations derived from the Wikipedia clickstream.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:59:19 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Abdollahi", "Sara", ""], ["Gottschalk", "Simon", ""], ["Demidova", "Elena", ""]]}, {"id": "2010.12533", "submitter": "Mariana Noguti", "authors": "Mariana Y. Noguti, Eduardo Vellasques, Luiz S. Oliveira", "title": "Legal Document Classification: An Application to Law Area Prediction of\n  Petitions to Public Prosecution Service", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207211", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increased interest in the application of\nNatural Language Processing (NLP) to legal documents. The use of convolutional\nand recurrent neural networks along with word embedding techniques have\npresented promising results when applied to textual classification problems,\nsuch as sentiment analysis and topic segmentation of documents. This paper\nproposes the use of NLP techniques for textual classification, with the purpose\nof categorizing the descriptions of the services provided by the Public\nProsecutor's Office of the State of Paran\\'a to the population in one of the\nareas of law covered by the institution. Our main goal is to automate the\nprocess of assigning petitions to their respective areas of law, with a\nconsequent reduction in costs and time associated with such process while\nallowing the allocation of human resources to more complex tasks. In this\npaper, we compare different approaches to word representations in the\naforementioned task: including document-term matrices and a few different word\nembeddings. With regards to the classification models, we evaluated three\ndifferent families: linear models, boosted trees and neural networks. The best\nresults were obtained with a combination of Word2Vec trained on a\ndomain-specific corpus and a Recurrent Neural Network (RNN) architecture (more\nspecifically, LSTM), leading to an accuracy of 90\\% and F1-Score of 85\\% in the\nclassification of eighteen categories (law areas).\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 18:05:37 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Noguti", "Mariana Y.", ""], ["Vellasques", "Eduardo", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "2010.12537", "submitter": "Haoyu Dong", "authors": "Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, Dongmei\n  Zhang", "title": "TUTA: Tree-based Transformers for Generally Structured Table\n  Pre-training", "comments": "KDD'21", "journal-ref": null, "doi": "10.1145/3447548.3467434", "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables are widely used with various structures to organize and present data.\nRecent attempts on table understanding mainly focus on relational tables, yet\noverlook to other common table structures. In this paper, we propose TUTA, a\nunified pre-training architecture for understanding generally structured\ntables. Noticing that understanding a table requires spatial, hierarchical, and\nsemantic information, we enhance transformers with three novel structure-aware\nmechanisms. First, we devise a unified tree-based structure, called a\nbi-dimensional coordinate tree, to describe both the spatial and hierarchical\ninformation of generally structured tables. Upon this, we propose tree-based\nattention and position embedding to better capture the spatial and hierarchical\ninformation. Moreover, we devise three progressive pre-training objectives to\nenable representations at the token, cell, and table levels. We pre-train TUTA\non a wide range of unlabeled web and spreadsheet tables and fine-tune it on two\ncritical tasks in the field of table structure understanding: cell type\nclassification and table type classification. Experiments show that TUTA is\nhighly effective, achieving state-of-the-art on five widely-studied datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:22:31 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 06:56:14 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 13:20:11 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 01:18:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Zhiruo", ""], ["Dong", "Haoyu", ""], ["Jia", "Ran", ""], ["Li", "Jia", ""], ["Fu", "Zhiyi", ""], ["Han", "Shi", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2010.12539", "submitter": "Kunal Sawarkar", "authors": "Kunal Sawarkar, Sanket Jain", "title": "Dynamically Tie the Right Offer to the Right Customer in\n  Telecommunications Industry", "comments": "Published in 2011 RESEARCH COUNCIL JOURNAL, An Annual Publication\n  From The Direct Marketing Association Research Council-\n  https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiitNPbyr7sAhXyct8KHfSpDsEQFjABegQIAxAC&url=https%3A%2F%2Fthedma.org%2Fwp-content%2Fuploads%2F2011-Analytics-Journal.pdf&usg=AOvVaw1XhGZBELMr3-_HAPhysVPp", "journal-ref": "Published in 2011 RESEARCH COUNCIL JOURNAL, An Annual Publication\n  From The Direct Marketing Association Research Council", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a successful business, engaging in an effective campaign is a key task\nfor marketers. Most previous studies used various mathematical models to\nsegment customers without considering the correlation between customer\nsegmentation and a campaign. This work presents a conceptual model by studying\nthe significant campaign-dependent variables of customer targeting in customer\nsegmentation context. In this way, the processes of customer segmentation and\ntargeting thus can be linked and solved together. The outcomes of customer\nsegmentation of this study could be more meaningful and relevant for marketers.\nThis investigation applies a customer life time value (LTV) model to assess the\nfitness between targeted customer groups and marketing strategies. To integrate\ncustomer segmentation and customer targeting, this work uses the genetic\nalgorithm (GA) to determine the optimized marketing strategy. Later, we suggest\nusing C&RT (Classification and Regression Tree) in SPSS PASW Modeler as the\nreplacement to Genetic Algorithm technique to accomplish these results. We also\nsuggest using LOSSYCOUNTING and Counting Bloom Filter to dynamically design the\nright and up-to-date offer to the right customer.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 16:44:51 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sawarkar", "Kunal", ""], ["Jain", "Sanket", ""]]}, {"id": "2010.12540", "submitter": "Mohamed Maher Mr", "authors": "Mohamed Maher (1), Perseverance Munga Ngoy (1), Aleksandrs Rebriks\n  (1), Cagri Ozcinar (1), Josue Cuevas (3), Rajasekhar Sanagavarapu (3),\n  Gholamreza Anbarjafari (1 and 2) ((1) iCV Lab, University of Tartu, Tartu,\n  Estonia, (2) Faculty of Engineering, Hasan Kalyoncu University, Gaziantep,\n  Turkey, (3) Rakuten Inc., Big Data Department, Machine Learning Group, Tokyo,\n  Japan)", "title": "Comprehensive Empirical Evaluation of Deep Learning Approaches for\n  Session-based Recommendation in E-Commerce", "comments": "48 pages, 17 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting sales of e-commerce services is guaranteed once users find more\nmatching items to their interests in a short time. Consequently, recommendation\nsystems have become a crucial part of any successful e-commerce services.\nAlthough various recommendation techniques could be used in e-commerce, a\nconsiderable amount of attention has been drawn to session-based recommendation\nsystems during the recent few years. This growing interest is due to the\nsecurity concerns in collecting personalized user behavior data, especially\nafter the recent general data protection regulations. In this work, we present\na comprehensive evaluation of the state-of-the-art deep learning approaches\nused in the session-based recommendation. In session-based recommendation, a\nrecommendation system counts on the sequence of events made by a user within\nthe same session to predict and endorse other items that are more likely to\ncorrelate with his/her preferences. Our extensive experiments investigate\nbaseline techniques (\\textit{e.g.,} nearest neighbors and pattern mining\nalgorithms) and deep learning approaches (\\textit{e.g.,} recurrent neural\nnetworks, graph neural networks, and attention-based networks). Our evaluations\nshow that advanced neural-based models and session-based nearest neighbor\nalgorithms outperform the baseline techniques in most of the scenarios.\nHowever, we found that these models suffer more in case of long sessions when\nthere exists drift in user interests, and when there is no enough data to model\ndifferent items correctly during training. Our study suggests that using hybrid\nmodels of different approaches combined with baseline algorithms could lead to\nsubstantial results in session-based recommendations based on dataset\ncharacteristics. We also discuss the drawbacks of current session-based\nrecommendation algorithms and further open research directions in this field.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 17:22:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Maher", "Mohamed", "", "1 and 2"], ["Ngoy", "Perseverance Munga", "", "1 and 2"], ["Rebriks", "Aleksandrs", "", "1 and 2"], ["Ozcinar", "Cagri", "", "1 and 2"], ["Cuevas", "Josue", "", "1 and 2"], ["Sanagavarapu", "Rajasekhar", "", "1 and 2"], ["Anbarjafari", "Gholamreza", "", "1 and 2"]]}, {"id": "2010.12637", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Comparative analysis of word embeddings in assessing semantic similarity\n  of complex sentences", "comments": "14 pages, 6 figures, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic textual similarity is one of the open research challenges in the\nfield of Natural Language Processing. Extensive research has been carried out\nin this field and near-perfect results are achieved by recent transformer-based\nmodels in existing benchmark datasets like the STS dataset and the SICK\ndataset. In this paper, we study the sentences in these datasets and analyze\nthe sensitivity of various word embeddings with respect to the complexity of\nthe sentences. We build a complex sentences dataset comprising of 50 sentence\npairs with associated semantic similarity values provided by 15 human\nannotators. Readability analysis is performed to highlight the increase in\ncomplexity of the sentences in the existing benchmark datasets and those in the\nproposed dataset. Further, we perform a comparative analysis of the performance\nof various word embeddings and language models on the existing benchmark\ndatasets and the proposed dataset. The results show the increase in complexity\nof the sentences has a significant impact on the performance of the embedding\nmodels resulting in a 10-20% decrease in Pearson's and Spearman's correlation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 19:55:11 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 03:44:56 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 21:15:24 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2010.12647", "submitter": "Changfeng Yu", "authors": "Changfeng Yu, Cheng Zhang and Jie Wang", "title": "Extracting Body Text from Academic PDF Documents for Text Mining", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate extraction of body text from PDF-formatted academic documents is\nessential in text-mining applications for deeper semantic understandings. The\nobjective is to extract complete sentences in the body text into a txt file\nwith the original sentence flow and paragraph boundaries. Existing tools for\nextracting text from PDF documents would often mix body and nonbody texts. We\ndevise and implement a system called PDFBoT to detect multiple-column layouts\nusing a line-sweeping technique, remove nonbody text using computed text\nfeatures and syntactic tagging in backward traversal, and align the remaining\ntext back to sentences and paragraphs. We show that PDFBoT is highly accurate\nwith average F1 scores of, respectively, 0.99 on extracting sentences, 0.96 on\nextracting paragraphs, and 0.98 on removing text on tables, figures, and charts\nover a corpus of PDF documents randomly selected from arXiv.org across multiple\nacademic disciplines.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 20:18:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yu", "Changfeng", ""], ["Zhang", "Cheng", ""], ["Wang", "Jie", ""]]}, {"id": "2010.12674", "submitter": "Thomas Schoegje", "authors": "Thomas Schoegje, Chris Kamphuis, Koen Dercksen, Djoerd Hiemstra, Toine\n  Pieters, Arjen de Vries", "title": "Exploring task-based query expansion at the TREC-COVID track", "comments": "Update version 2: Improved title Update version 3: corrected\n  terminology hyponym -> hypernym in two instances Documents our participation\n  to the TREC-COVID track. Contains 16 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how to generate effective queries based on search tasks. Our\napproach has three main steps: 1) identify search tasks based on research\ngoals, 2) manually classify search queries according to those tasks, and 3)\ncompare three methods to improve search rankings based on the task context. The\nmost promising approach is based on expanding the user's query terms using task\nterms, which slightly improved the NDCG@20 scores over a BM25 baseline. Further\nimprovements might be gained if we can identify more specific search tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:16:33 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 11:10:44 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:10:33 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Schoegje", "Thomas", ""], ["Kamphuis", "Chris", ""], ["Dercksen", "Koen", ""], ["Hiemstra", "Djoerd", ""], ["Pieters", "Toine", ""], ["de Vries", "Arjen", ""]]}, {"id": "2010.12794", "submitter": "Zihan Wang", "authors": "Zihan Wang and Dheeraj Mekala and Jingbo Shang", "title": "X-Class: Text Classification with Extremely Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore to conduct text classification with extremely weak\nsupervision, i.e., only relying on the surface text of class names. This is a\nmore challenging setting than the seed-driven weak supervision, which allows a\nfew seed words per class. We opt to attack this problem from a representation\nlearning perspective -- ideal document representations should lead to very\nclose results between clustering and the desired classification. In particular,\none can classify the same corpus differently (e.g., based on topics and\nlocations), so document representations must be adaptive to the given class\nnames. We propose a novel framework X-Class to realize it. Specifically, we\nfirst estimate comprehensive class representations by incrementally adding the\nmost similar word to each class until inconsistency appears. Following a\ntailored mixture of class attention mechanisms, we obtain the document\nrepresentation via a weighted average of contextualized token representations.\nWe then cluster and align the documents to classes with the prior of each\ndocument assigned to its nearest class. Finally, we pick the most confident\ndocuments from each cluster to train a text classifier. Extensive experiments\ndemonstrate that X-Class can rival and even outperform seed-driven weakly\nsupervised methods on 7 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:09:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Zihan", ""], ["Mekala", "Dheeraj", ""], ["Shang", "Jingbo", ""]]}, {"id": "2010.12798", "submitter": "Xavier Thomas", "authors": "Xavier Thomas", "title": "Content-Based Personalized Recommender System Using Entity Embeddings", "comments": "2 Pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are a class of machine learning algorithms that provide\nrelevant recommendations to a user based on the user's interaction with similar\nitems or based on the content of the item. In settings where the content of the\nitem is to be preserved, a content-based approach would be beneficial. This\npaper aims to highlight the advantages of the content-based approach through\nlearned embeddings and leveraging these advantages to provide better and\npersonalised movie recommendations based on user preferences to various movie\nfeatures such as genre and keyword tags.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:25:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Thomas", "Xavier", ""]]}, {"id": "2010.12800", "submitter": "Xinliang (Frederick) Zhang", "authors": "Xinliang Frederick Zhang, Heming Sun, Xiang Yue, Emmett Jesrani, Simon\n  Lin, Huan Sun", "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSpecifically, similar to a standard FAQ dataset, COUGH consists of three parts:\nFAQ Bank, User Query Bank and Annotated Relevance Set. FAQ Bank contains ~16K\nFAQ items scraped from 55 credible websites (e.g., CDC and WHO). For\nevaluation, we introduce User Query Bank and Annotated Relevance Set, where the\nformer contains 1201 human-paraphrased queries while the latter contains ~32\nhuman-annotated FAQ items for each query. We analyze COUGH by testing different\nFAQ retrieval models built on top of BM25 and BERT, among which the best model\nachieves 0.29 under P@5, indicating that the dataset presents a great challenge\nfor future research. Our dataset is freely available at\nhttps://github.com/sunlab-osu/covid-faq.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:30:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Xinliang Frederick", ""], ["Sun", "Heming", ""], ["Yue", "Xiang", ""], ["Jesrani", "Emmett", ""], ["Lin", "Simon", ""], ["Sun", "Huan", ""]]}, {"id": "2010.12803", "submitter": "Zheda Mai", "authors": "Zheda Mai, Ga Wu, Kai Luo, Scott Sanner", "title": "Attentive Autoencoders for Multifaceted Preference Learning in One-class\n  Collaborative Filtering", "comments": "Accepted at ICDMW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing One-Class Collaborative Filtering (OC-CF) algorithms estimate a\nuser's preference as a latent vector by encoding their historical interactions.\nHowever, users often show diverse interests, which significantly increases the\nlearning difficulty. In order to capture multifaceted user preferences,\nexisting recommender systems either increase the encoding complexity or extend\nthe latent representation dimension. Unfortunately, these changes inevitably\nlead to increased training difficulty and exacerbate scalability issues. In\nthis paper, we propose a novel and efficient CF framework called Attentive\nMulti-modal AutoRec (AMA) that explicitly tracks multiple facets of user\npreferences. Specifically, we extend the Autoencoding-based recommender AutoRec\nto learn user preferences with multi-modal latent representations, where each\nmode captures one facet of a user's preferences. By leveraging the attention\nmechanism, each observed interaction can have different contributions to the\npreference facets. Through extensive experiments on three real-world datasets,\nwe show that AMA is competitive with state-of-the-art models under the OC-CF\nsetting. Also, we demonstrate how the proposed model improves interpretability\nby providing explanations using the attention mechanism.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 06:35:44 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mai", "Zheda", ""], ["Wu", "Ga", ""], ["Luo", "Kai", ""], ["Sanner", "Scott", ""]]}, {"id": "2010.12837", "submitter": "Fuyu Lv", "authors": "Fuyu Lv, Mengxue Li, Tonglei Guo, Changlong Yu, Fei Sun, Taiwei Jin,\n  Hong Wen, Guli Lin, Keping Yang, Wilfred Ng", "title": "XDM: Improving Sequential Deep Matching with Unclicked User Behaviors\n  for Recommender System", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based sequential recommender systems have recently attracted\nincreasing attention from both academia and industry. Most of industrial\nEmbedding-Based Retrieval (EBR) system for recommendation share the similar\nideas with sequential recommenders. Among them, how to comprehensively capture\nsequential user interest is a fundamental problem. However, most existing\nsequential recommendation models take as input clicked or purchased behavior\nsequences from user-item interactions. This leads to incomprehensive user\nrepresentation and sub-optimal model performance, since they ignore the\ncomplete user behavior exposure data, i.e., items impressed yet unclicked by\nusers. In this work, we attempt to incorporate and model those unclicked item\nsequences using a new learning approach in order to explore better sequential\nrecommendation technique. An efficient triplet metric learning algorithm is\nproposed to appropriately learn the representation of unclicked items. Our\nmethod can be simply integrated with existing sequential recommendation models\nby a confidence fusion network and further gain better user representation. The\noffline experimental results based on real-world E-commerce data demonstrate\nthe effectiveness and verify the importance of unclicked items in sequential\nrecommendation. Moreover we deploy our new model (named XDM) into EBR of\nrecommender system at Taobao, outperforming the deployed previous generation\nSDM.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:37:04 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 02:50:06 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 08:03:30 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lv", "Fuyu", ""], ["Li", "Mengxue", ""], ["Guo", "Tonglei", ""], ["Yu", "Changlong", ""], ["Sun", "Fei", ""], ["Jin", "Taiwei", ""], ["Wen", "Hong", ""], ["Lin", "Guli", ""], ["Yang", "Keping", ""], ["Ng", "Wilfred", ""]]}, {"id": "2010.12908", "submitter": "Xiang Ling", "authors": "Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei Ma, Fangli\n  Xu, Alex X. Liu, Chunming Wu, Shouling Ji", "title": "Deep Graph Matching and Searching for Semantic Code Retrieval", "comments": "Accepted by ACM Transactions on Knowledge Discovery from Data (ACM\n  TKDD)", "journal-ref": "ACM Trans. Knowl. Discov. Data 15, 5 (2021), 1-21", "doi": "10.1145/3447571", "report-no": null, "categories": "cs.AI cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code retrieval is to find the code snippet from a large corpus of source code\nrepositories that highly matches the query of natural language description.\nRecent work mainly uses natural language processing techniques to process both\nquery texts (i.e., human natural language) and code snippets (i.e., machine\nprogramming language), however neglecting the deep structured features of query\ntexts and source codes, both of which contain rich semantic information. In\nthis paper, we propose an end-to-end deep graph matching and searching (DGMS)\nmodel based on graph neural networks for the task of semantic code retrieval.\nTo this end, we first represent both natural language query texts and\nprogramming language code snippets with the unified graph-structured data, and\nthen use the proposed graph matching and searching model to retrieve the best\nmatching code snippet. In particular, DGMS not only captures more structural\ninformation for individual query texts or code snippets but also learns the\nfine-grained similarity between them by cross-attention based semantic matching\noperations. We evaluate the proposed DGMS model on two public code retrieval\ndatasets with two representative programming languages (i.e., Java and Python).\nExperiment results demonstrate that DGMS significantly outperforms\nstate-of-the-art baseline models by a large margin on both datasets. Moreover,\nour extensive ablation studies systematically investigate and illustrate the\nimpact of each part of DGMS.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 14:16:50 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 16:38:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Ling", "Xiang", ""], ["Wu", "Lingfei", ""], ["Wang", "Saizhuo", ""], ["Pan", "Gaoning", ""], ["Ma", "Tengfei", ""], ["Xu", "Fangli", ""], ["Liu", "Alex X.", ""], ["Wu", "Chunming", ""], ["Ji", "Shouling", ""]]}, {"id": "2010.13100", "submitter": "Minsoo Rhu", "authors": "Youngeun Kwon, Yunjae Lee, Minsoo Rhu", "title": "Tensor Casting: Co-Designing Algorithm-Architecture for Personalized\n  Recommendation Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are one of the most widely deployed machine\nlearning (ML) workload serviced from cloud datacenters. As such, architectural\nsolutions for high-performance recommendation inference have recently been the\ntarget of several prior literatures. Unfortunately, little have been explored\nand understood regarding the training side of this emerging ML workload. In\nthis paper, we first perform a detailed workload characterization study on\ntraining recommendations, root-causing sparse embedding layer training as one\nof the most significant performance bottlenecks. We then propose our\nalgorithm-architecture co-design called Tensor Casting, which enables the\ndevelopment of a generic accelerator architecture for tensor gather-scatter\nthat encompasses all the key primitives of training embedding layers. When\nprototyped on a real CPU-GPU system, Tensor Casting provides 1.9-21x\nimprovements in training throughput compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 12:04:32 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kwon", "Youngeun", ""], ["Lee", "Yunjae", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2010.13128", "submitter": "Mokanarangan Thayaparan", "authors": "Mokanarangan Thayaparan, Marco Valentino, Andr\\'e Freitas", "title": "ExplanationLP: Abductive Reasoning for Explainable Science Question\n  Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for answering and explaining multiple-choice\nscience questions by reasoning on grounding and abstract inference chains. This\npaper frames question answering as an abductive reasoning problem, constructing\nplausible explanations for each choice and then selecting the candidate with\nthe best explanation as the final answer. Our system, ExplanationLP, elicits\nexplanations by constructing a weighted graph of relevant facts for each\ncandidate answer and extracting the facts that satisfy certain structural and\nsemantic constraints. To extract the explanations, we employ a linear\nprogramming formalism designed to select the optimal subgraph. The graphs'\nweighting function is composed of a set of parameters, which we fine-tune to\noptimize answer selection performance. We carry out our experiments on the\nWorldTree and ARC-Challenge corpus to empirically demonstrate the following\nconclusions: (1) Grounding-Abstract inference chains provides the semantic\ncontrol to perform explainable abductive reasoning (2) Efficiency and\nrobustness in learning with a fewer number of parameters by outperforming\ncontemporary explainable and transformer-based approaches in a similar setting\n(3) Generalisability by outperforming SOTA explainable approaches on general\nscience question sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 14:49:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2010.13273", "submitter": "Yuyang Dong", "authors": "Yuyang Dong, Kunihiro Takeoka, Chuan Xiao, Masafumi Oyamada", "title": "Efficient Joinable Table Discovery in Data Lakes: A High-Dimensional\n  Similarity-Based Approach", "comments": "Full version of paper in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding joinable tables in data lakes is key procedure in many applications\nsuch as data integration, data augmentation, data analysis, and data market.\nTraditional approaches that find equi-joinable tables are unable to deal with\nmisspellings and different formats, nor do they capture any semantic joins. In\nthis paper, we propose PEXESO, a framework for joinable table discovery in data\nlakes. We embed textual values as high-dimensional vectors and join columns\nunder similarity predicates on high-dimensional vectors, hence to address the\nlimitations of equi-join approaches and identify more meaningful results. To\nefficiently find joinable tables with similarity, we propose a block-and-verify\nmethod that utilizes pivot-based filtering. A partitioning technique is\ndeveloped to cope with the case when the data lake is large and the index\ncannot fit in main memory. An experimental evaluation on real datasets shows\nthat our solution identifies substantially more tables than equi-joins and\noutperforms other similarity-based options, and the join results are useful in\ndata enrichment for machine learning tasks. The experiments also demonstrate\nthe efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 01:39:35 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 22:51:28 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 13:28:05 GMT"}, {"version": "v4", "created": "Mon, 29 Mar 2021 05:10:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dong", "Yuyang", ""], ["Takeoka", "Kunihiro", ""], ["Xiao", "Chuan", ""], ["Oyamada", "Masafumi", ""]]}, {"id": "2010.13352", "submitter": "Zhaopeng Xing", "authors": "Zhaopeng Xing, Xiaojun (Jenny) Yuan, Lisa Vizer", "title": "The Age-related Differences in Web Information Search Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Older adults' need for quality health information has never been more\ncritical as during the COVID-19 pandemic. Yet, they are susceptible to the\nwide-spread misinformation disseminated through search engines and social\nmedia. To build a search-related behavioral profile of older adults, this\narticle surveys the empirical research on age-related differences in query\nformulation, search strategies, information evaluation, and susceptibility to\nmisinformation effects. It also decomposes the mechanisms (i.e., cognitive\nchanges, development goal shift) and moderators (i.e., search task and\ninterface design) of such differences. To inform the design of information\nsystems to improve older adults' information search experience, we discuss\nopportunities for future research.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:37:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xing", "Zhaopeng", "", "Jenny"], ["Xiaojun", "", "", "Jenny"], ["Yuan", "", ""], ["Vizer", "Lisa", ""]]}, {"id": "2010.13373", "submitter": "Shi Pu", "authors": "Shi Pu and Yijiang He and Zheng Li and Mao Zheng", "title": "Multimodal Topic Learning for Video Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facilitated by deep neural networks, video recommendation systems have made\nsignificant advances. Existing video recommendation systems directly exploit\nfeatures from different modalities (e.g., user personal data, user behavior\ndata, video titles, video tags, and visual contents) to input deep neural\nnetworks, while expecting the networks to online mine user-preferred topics\nimplicitly from these features. However, the features lacking semantic topic\ninformation limits accurate recommendation generation. In addition, feature\ncrosses using visual content features generate high dimensionality features\nthat heavily downgrade the online computational efficiency of networks. In this\npaper, we explicitly separate topic generation from recommendation generation,\npropose a multimodal topic learning algorithm to exploit three modalities\n(i.e., tags, titles, and cover images) for generating video topics offline. The\ntopics generated by the proposed algorithm serve as semantic topic features to\nfacilitate preference scope determination and recommendation generation.\nFurthermore, we use the semantic topic features instead of visual content\nfeatures to effectively reduce online computational cost. Our proposed\nalgorithm has been deployed in the Kuaibao information streaming platform.\nOnline and offline evaluation results show that our proposed algorithm performs\nfavorably.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 07:02:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Pu", "Shi", ""], ["He", "Yijiang", ""], ["Li", "Zheng", ""], ["Zheng", "Mao", ""]]}, {"id": "2010.13442", "submitter": "Markus Schmid", "authors": "Markus L. Schmid and Nicole Schweikardt", "title": "A Purely Regular Approach to Non-Regular Core Spanners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regular spanners (characterised by vset-automata) are closed under the\nalgebraic operations of union, join and projection, and have desirable\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\nfunctionality of the query language AQL used in IBM's SystemT) additionally\nneed string equality selections and it has been shown by Freydenberger and\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\ncomplexity and even undecidability of the typical problems in static analysis\nand query evaluation. We propose an alternative approach to core spanners: by\nincorporating the string-equality selections directly into the regular language\nthat represents the underlying regular spanner (instead of treating it as an\nalgebraic operation on the table extracted by the regular spanner), we obtain a\nfragment of core spanners that, while having slightly weaker expressive power\nthan the full class of core spanners, arguably still covers the intuitive\napplications of string equality selections for information extraction and has\nmuch better upper complexity bounds of the typical problems in static analysis\nand query evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:27:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Schmid", "Markus L.", ""], ["Schweikardt", "Nicole", ""]]}, {"id": "2010.13447", "submitter": "Timo Breuer", "authors": "Timo Breuer, Nicola Ferro, Norbert Fuhr, Maria Maistro, Tetsuya Sakai,\n  Philipp Schaer, Ian Soboroff", "title": "How to Measure the Reproducibility of System-oriented IR Experiments", "comments": "SIGIR2020 Full Conference Paper", "journal-ref": null, "doi": "10.1145/3397271.3401036", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicability and reproducibility of experimental results are primary\nconcerns in all the areas of science and IR is not an exception. Besides the\nproblem of moving the field towards more reproducible experimental practices\nand protocols, we also face a severe methodological issue: we do not have any\nmeans to assess when reproduced is reproduced. Moreover, we lack any\nreproducibility-oriented dataset, which would allow us to develop such methods.\nTo address these issues, we compare several measures to objectively quantify to\nwhat extent we have replicated or reproduced a system-oriented IR experiment.\nThese measures operate at different levels of granularity, from the\nfine-grained comparison of ranked lists, to the more general comparison of the\nobtained effects and significant differences. Moreover, we also develop a\nreproducibility-oriented dataset, which allows us to validate our measures and\nwhich can also be used to develop future measures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:36:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Breuer", "Timo", ""], ["Ferro", "Nicola", ""], ["Fuhr", "Norbert", ""], ["Maistro", "Maria", ""], ["Sakai", "Tetsuya", ""], ["Schaer", "Philipp", ""], ["Soboroff", "Ian", ""]]}, {"id": "2010.13556", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiusi Chen, Yu Meng, Jiawei Han", "title": "Hierarchical Metadata-Aware Document Categorization under Weak\n  Supervision", "comments": "9 pages; Accepted to WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorizing documents into a given label hierarchy is intuitively appealing\ndue to the ubiquity of hierarchical topic structures in massive text corpora.\nAlthough related studies have achieved satisfying performance in fully\nsupervised hierarchical document classification, they usually require massive\nhuman-annotated training data and only utilize text information. However, in\nmany domains, (1) annotations are quite expensive where very few training\nsamples can be acquired; (2) documents are accompanied by metadata information.\nHence, this paper studies how to integrate the label hierarchy, metadata, and\ntext signals for document categorization under weak supervision. We develop\nHiMeCat, an embedding-based generative framework for our task. Specifically, we\npropose a novel joint representation learning module that allows simultaneous\nmodeling of category dependencies, metadata information and textual semantics,\nand we introduce a data augmentation module that hierarchically synthesizes\ntraining documents to complement the original, small-scale training set. Our\nexperiments demonstrate a consistent improvement of HiMeCat over competitive\nbaselines and validate the contribution of our representation learning and data\naugmentation modules.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 13:07:56 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 02:02:39 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Yu", ""], ["Chen", "Xiusi", ""], ["Meng", "Yu", ""], ["Han", "Jiawei", ""]]}, {"id": "2010.13658", "submitter": "Baosong Yang", "authors": "Tianchi Bi and Liang Yao and Baosong Yang and Haibo Zhang and Weihua\n  Luo and Boxing Chen", "title": "Constraint Translation Candidates: A Bridge between Neural Query\n  Translation and Cross-lingual Information Retrieval", "comments": "SIGIR eCom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query translation (QT) is a key component in cross-lingual information\nretrieval system (CLIR). With the help of deep learning, neural machine\ntranslation (NMT) has shown promising results on various tasks. However, NMT is\ngenerally trained with large-scale out-of-domain data rather than in-domain\nquery translation pairs. Besides, the translation model lacks a mechanism at\nthe inference time to guarantee the generated words to match the search index.\nThe two shortages of QT result in readable texts for human but inadequate\ncandidates for the downstream retrieval task. In this paper, we propose a novel\napproach to alleviate these problems by limiting the open target vocabulary\nsearch space of QT to a set of important words mined from search index\ndatabase. The constraint translation candidates are employed at both of\ntraining and inference time, thus guiding the translation model to learn and\ngenerate well performing target queries. The proposed methods are exploited and\nexamined in a real-word CLIR system--Aliexpress e-Commerce search engine.\nExperimental results demonstrate that our approach yields better performance on\nboth translation quality and retrieval accuracy than the strong NMT baseline.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:27:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bi", "Tianchi", ""], ["Yao", "Liang", ""], ["Yang", "Baosong", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Chen", "Boxing", ""]]}, {"id": "2010.13659", "submitter": "Baosong Yang", "authors": "Liang Yao and Baosong Yang and Haibo Zhang and Weihua Luo and Boxing\n  Chen", "title": "Exploiting Neural Query Translation into Cross Lingual Information\n  Retrieval", "comments": "SIGIR eCom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial role in cross-language information retrieval (CLIR), query\ntranslation has three main challenges: 1) the adequacy of translation; 2) the\nlack of in-domain parallel training data; and 3) the requisite of low latency.\nTo this end, existing CLIR systems mainly exploit statistical-based machine\ntranslation (SMT) rather than the advanced neural machine translation (NMT),\nlimiting the further improvements on both translation and retrieval quality. In\nthis paper, we investigate how to exploit neural query translation model into\nCLIR system. Specifically, we propose a novel data augmentation method that\nextracts query translation pairs according to user clickthrough data, thus to\nalleviate the problem of domain-adaptation in NMT. Then, we introduce an\nasynchronous strategy which is able to leverage the advantages of the real-time\nin SMT and the veracity in NMT. Experimental results reveal that the proposed\napproach yields better retrieval quality than strong baselines and can be well\napplied into a real-world CLIR system, i.e. Aliexpress e-Commerce search\nengine. Readers can examine and test their cases on our website:\nhttps://aliexpress.com .\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:28:19 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yao", "Liang", ""], ["Yang", "Baosong", ""], ["Zhang", "Haibo", ""], ["Luo", "Weihua", ""], ["Chen", "Boxing", ""]]}, {"id": "2010.13684", "submitter": "Lloyd Montgomery", "authors": "Colin Werner, Lloyd Montgomery, Sanja Dodos, Gabriel Tapuc, Diksha\n  Sharma, Daniela Damian", "title": "How angry are your customers? Sentiment analysis of support tickets that\n  escalate", "comments": "8 pages, 0 figures, 6 tables, accepted at the 2018 1st International\n  Workshop on Affective Computing for Requirements Engineering (AffectRE) at RE", "journal-ref": null, "doi": "10.1109/AffectRE.2018.00006", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software support ticket escalations can be an extremely costly burden for\nsoftware organizations all over the world. Consequently, there exists an\ninterest in researching how to better enable support analysts to handle such\nescalations. In order to do so, we need to develop tools to reliably predict\nif, and when, a support ticket becomes a candidate for escalation. This paper\nexplores the use of sentiment analysis tools on customer-support analyst\nconversations to find indicators of when a particular support ticket may be\nescalated. The results of this research indicate a considerable difference in\nthe sentiment between escalated support tickets and non-escalated support\ntickets. Thus, this preliminary research provides us with the necessary\ninformation to further investigate how we can reliably predict support ticket\nescalations, and subsequently to provide insight to support analysts to better\nenable them to handle support tickets that may be escalated.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:00:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Werner", "Colin", ""], ["Montgomery", "Lloyd", ""], ["Dodos", "Sanja", ""], ["Tapuc", "Gabriel", ""], ["Sharma", "Diksha", ""], ["Damian", "Daniela", ""]]}, {"id": "2010.14013", "submitter": "Yitong Meng", "authors": "Yitong Meng, Jie Liu, Xiao Yan and James Cheng", "title": "The item selection problem for user cold-start recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a new user just signs up on a website, we usually have no information\nabout him/her, i.e. no interaction with items, no user profile and no social\nlinks with other users. Under such circumstances, we still expect our\nrecommender systems could attract the users at the first time so that the users\ndecide to stay on the website and become active users. This problem falls into\nnew user cold-start category and it is crucial to the development and even\nsurvival of a company. Existing works on user cold-start recommendation either\nrequire additional user efforts, e.g. setting up an interview process, or make\nuse of side information [10] such as user demographics, locations, social\nrelations, etc. However, users may not be willing to take the interview and\nside information on cold-start users is usually not available. Therefore, we\nconsider a pure cold-start scenario where neither interaction nor side\ninformation is available and no user effort is required. Studying this setting\nis also important for the initialization of other cold-start solutions, such as\ninitializing the first few questions of an interview.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 02:48:26 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Meng", "Yitong", ""], ["Liu", "Jie", ""], ["Yan", "Xiao", ""], ["Cheng", "James", ""]]}, {"id": "2010.14049", "submitter": "Wen-Ting Tseng", "authors": "Wen-Ting Tseng, Tien-Hong Lo, Yung-Chang Hsu and Berlin Chen", "title": "Effective FAQ Retrieval and Question Matching With Unsupervised\n  Knowledge Injection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently asked question (FAQ) retrieval, with the purpose of providing\ninformation on frequent questions or concerns, has far-reaching applications in\nmany areas, where a collection of question-answer (Q-A) pairs compiled a priori\ncan be employed to retrieve an appropriate answer in response to a user\\u2019s\nquery that is likely to reoccur frequently. To this end, predominant approaches\nto FAQ retrieval typically rank question-answer pairs by considering either the\nsimilarity between the query and a question (q-Q), the relevance between the\nquery and the associated answer of a question (q-A), or combining the clues\ngathered from the q-Q similarity measure and the q-A relevance measure. In this\npaper, we extend this line of research by combining the clues gathered from the\nq-Q similarity measure and the q-A relevance measure and meanwhile injecting\nextra word interaction information, distilled from a generic (open domain)\nknowledge base, into a contextual language model for inferring the q-A\nrelevance. Furthermore, we also explore to capitalize on domain-specific\ntopically-relevant relations between words in an unsupervised manner, acting as\na surrogate to the supervised domain-specific knowledge base information. As\nsuch, it enables the model to equip sentence representations with the knowledge\nabout domain-specific and topically-relevant relations among words, thereby\nproviding a better q-A relevance measure. We evaluate variants of our approach\non a publicly-available Chinese FAQ dataset, and further apply and\ncontextualize it to a large-scale question-matching task, which aims to search\nquestions from a QA dataset that have a similar intent as an input query.\nExtensive experimental results on these two datasets confirm the promising\nperformance of the proposed approach in relation to some state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 05:03:34 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tseng", "Wen-Ting", ""], ["Lo", "Tien-Hong", ""], ["Hsu", "Yung-Chang", ""], ["Chen", "Berlin", ""]]}, {"id": "2010.14057", "submitter": "Debasish Chakroborti", "authors": "Debasish Chakroborti", "title": "An Intermediate Data-driven Methodology for Scientific Workflow\n  Management System to Support Reusability", "comments": "Preprint. arXiv admin note: text overlap with arXiv:2010.04880", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis first we propose an intermediate data management scheme for a\nSWfMS. In our second attempt, we explored the possibilities and introduced an\nautomatic recommendation technique for a SWfMS from real-world workflow data\n(i.e Galaxy [1] workflows) where our investigations show that the proposed\ntechnique can facilitate 51% of workflow building in a SWfMS by reusing\nintermediate data of previous workflows and can reduce 74% execution time of\nworkflow buildings in a SWfMS. Later we propose an adaptive version of our\ntechnique by considering the states of tools in a SWfMS, which shows around 40%\nreusability for workflows. Consequently, in our fourth study, We have done\nseveral experiments for analyzing the performance and exploring the\neffectiveness of the technique in a SWfMS for various environments. The\ntechnique is introduced to emphasize on storing cost reduction, increase data\nreusability, and faster workflow execution, to the best of our knowledge, which\nis the first of its kind. Detail architecture and evaluation of the technique\nare presented in this thesis. We believe our findings and developed system will\ncontribute significantly to the research domain of SWfMSs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:27:25 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chakroborti", "Debasish", ""]]}, {"id": "2010.14171", "submitter": "Xavier Favory", "authors": "Xavier Favory, Konstantinos Drossos, Tuomas Virtanen, Xavier Serra", "title": "Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio\n  and Tags", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Self-supervised audio representation learning offers an attractive\nalternative for obtaining generic audio embeddings, capable to be employed into\nvarious downstream tasks. Published approaches that consider both audio and\nwords/tags associated with audio do not employ text processing models that are\ncapable to generalize to tags unknown during training. In this work we propose\na method for learning audio representations using an audio autoencoder (AAE), a\ngeneral word embeddings model (WEM), and a multi-head self-attention (MHA)\nmechanism. MHA attends on the output of the WEM, providing a contextualized\nrepresentation of the tags associated with the audio, and we align the output\nof MHA with the output of the encoder of AAE using a contrastive loss. We\njointly optimize AAE and MHA and we evaluate the audio representations (i.e.\nthe output of the encoder of AAE) by utilizing them in three different\ndownstream tasks, namely sound, music genre, and music instrument\nclassification. Our results show that employing multi-head self-attention with\nmultiple heads in the tag-based network can induce better learned audio\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 10:13:17 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Favory", "Xavier", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""], ["Serra", "Xavier", ""]]}, {"id": "2010.14282", "submitter": "Sundong Kim", "authors": "Sundong Kim and Tung-Duong Mai and Thi Nguyen Duc Khanh and Sungwon\n  Han and Sungwon Park and Karandeep Singh and Meeyoung Cha", "title": "Take a Chance: Managing the Exploitation-Exploration Dilemma in Customs\n  Fraud Detection via Online Active Learning", "comments": "10 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Continual labeling of training examples is a costly task in supervised\nlearning. Active learning strategies mitigate this cost by identifying\nunlabeled data that are considered the most useful for training a predictive\nmodel. However, sample selection via active learning may lead to an\nexploitation-exploration dilemma. In online settings, profitable items can be\nneglected when uncertain items are annotated instead. To illustrate this\ndilemma, we study a human-in-the-loop customs selection scenario where an\nAI-based system supports customs officers by providing a set of imports to be\ninspected. If the inspected items are fraud, officers levy extra duties, and\nthese items will be used as additional training data for the next iterations.\nInspecting highly suspicious items will inevitably lead to additional customs\nrevenue, yet they may not give any extra knowledge to customs officers. On the\nother hand, inspecting uncertain items will help customs officers to acquire\nnew knowledge, which will be used as supplementary training resources to update\ntheir selection systems. Through years of customs selection simulation, we show\nthat some exploration is needed to cope with the domain shift, and our hybrid\nstrategy of selecting fraud and uncertain items will eventually outperform the\nperformance of the exploitation strategy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 13:31:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kim", "Sundong", ""], ["Mai", "Tung-Duong", ""], ["Khanh", "Thi Nguyen Duc", ""], ["Han", "Sungwon", ""], ["Park", "Sungwon", ""], ["Singh", "Karandeep", ""], ["Cha", "Meeyoung", ""]]}, {"id": "2010.14395", "submitter": "Fei Sun", "authors": "Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Bolin Ding, Bin\n  Cui", "title": "Contrastive Learning for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation methods play a crucial role in modern recommender\nsystems because of their ability to capture a user's dynamic interest from\nher/his historical interactions. Despite their success, we argue that these\napproaches usually rely on the sequential prediction task to optimize the huge\namounts of parameters. They usually suffer from the data sparsity problem,\nwhich makes it difficult for them to learn high-quality user representations.\nTo tackle that, inspired by recent advances of contrastive learning techniques\nin the computer version, we propose a novel multi-task model called\n\\textbf{C}ontrastive \\textbf{L}earning for \\textbf{S}equential\n\\textbf{Rec}ommendation~(\\textbf{CL4SRec}). CL4SRec not only takes advantage of\nthe traditional next item prediction task but also utilizes the contrastive\nlearning framework to derive self-supervision signals from the original user\nbehavior sequences. Therefore, it can extract more meaningful user patterns and\nfurther encode the user representation effectively. In addition, we propose\nthree data augmentation approaches to construct self-supervision signals.\nExtensive experiments on four public datasets demonstrate that CL4SRec achieves\nstate-of-the-art performance over existing baselines by inferring better user\nrepresentations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:57:37 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 13:10:27 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Xu", ""], ["Sun", "Fei", ""], ["Liu", "Zhaoyang", ""], ["Wu", "Shiwen", ""], ["Gao", "Jinyang", ""], ["Ding", "Bolin", ""], ["Cui", "Bin", ""]]}, {"id": "2010.14464", "submitter": "Lukasz Borchmann", "authors": "{\\L}ukasz Borchmann, Dawid Jurkiewicz, Filip Grali\\'nski, Tomasz\n  G\\'orecki", "title": "Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel method of finding a fragment in a long temporal\nsequence similar to the set of shorter sequences. We are the first to propose\nan algorithm for such a search that does not rely on computing the average\nsequence from query examples. Instead, we use query examples as is, utilizing\nall of them simultaneously. The introduced method based on the Dynamic Time\nWarping (DTW) technique is suited explicitly for few-shot query-by-example\nretrieval tasks. We evaluate it on two different few-shot problems from the\nfield of Natural Language Processing. The results show it either outperforms\nbaselines and previous approaches or achieves comparable results when a low\nnumber of examples is available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:23:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Borchmann", "\u0141ukasz", ""], ["Jurkiewicz", "Dawid", ""], ["Grali\u0144ski", "Filip", ""], ["G\u00f3recki", "Tomasz", ""]]}, {"id": "2010.14531", "submitter": "Tim Draws", "authors": "Tim Draws, Nava Tintarev, Ujwal Gadiraju, Alessandro Bozzon, Benjamin\n  Timmermans", "title": "Assessing Viewpoint Diversity in Search Results Using Ranking Fairness\n  Metrics", "comments": null, "journal-ref": "ACM SIGKDD Explorations Newsletter, vol. 23, iss. 1, p. 50-58,\n  2021", "doi": "10.1145/3468507.3468515", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way pages are ranked in search results influences whether the users of\nsearch engines are exposed to more homogeneous, or rather to more diverse\nviewpoints. However, this viewpoint diversity is not trivial to assess. In this\npaper we use existing and novel ranking fairness metrics to evaluate viewpoint\ndiversity in search result rankings. We conduct a controlled simulation study\nthat shows how ranking fairness metrics can be used for viewpoint diversity,\nhow their outcome should be interpreted, and which metric is most suitable\ndepending on the situation. This paper lays out important ground work for\nfuture research to measure and assess viewpoint diversity in real search result\nrankings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:04:34 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 16:19:43 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Draws", "Tim", ""], ["Tintarev", "Nava", ""], ["Gadiraju", "Ujwal", ""], ["Bozzon", "Alessandro", ""], ["Timmermans", "Benjamin", ""]]}, {"id": "2010.14570", "submitter": "Manojkumar Rangasamy Kannadasan", "authors": "Shubhangi Tandon, Saratchandra Indrakanti, Amit Jaiswal, Svetlana\n  Strunjas, Manojkumar Rangasamy Kannadasan", "title": "Addressing Purchase-Impression Gap through a Sequential Re-ranker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale eCommerce platforms such as eBay carry a wide variety of\ninventory and provide several buying choices to online shoppers. It is critical\nfor eCommerce search engines to showcase in the top results the variety and\nselection of inventory available, specifically in the context of the various\nbuying intents that may be associated with a search query. Search rankers are\nmost commonly powered by learning-to-rank models which learn the preference\nbetween items during training. However, they score items independent of other\nitems at runtime. Although the items placed at top of the results by such\nscoring functions may be independently optimal, they can be sub-optimal as a\nset. This may lead to a mismatch between the ideal distribution of items in the\ntop results vs what is actually impressed. In this paper, we present methods to\naddress the purchase-impression gap observed in top search results on eCommerce\nsites. We establish the ideal distribution of items based on historic shopping\npatterns. We then present a sequential reranker that methodically reranks top\nsearch results produced by a conventional pointwise scoring ranker. The\nreranker produces a reordered list by sequentially selecting candidates trading\noff between their independent relevance and potential to address the\npurchase-impression gap by utilizing specially constructed features that\ncapture impression distribution of items already added to a reranked list. The\nsequential reranker enables addressing purchase impression gap with respect to\nmultiple item aspects. Early version of the reranker showed promising lifts in\nconversion and engagement metrics at eBay. Based on experiments on randomly\nsampled validation datasets, we observe that the reranking methodology\npresented produces around 10% reduction in purchase-impression gap at an\naverage for the top 20 results, while making improvements to conversion\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 19:26:51 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Tandon", "Shubhangi", ""], ["Indrakanti", "Saratchandra", ""], ["Jaiswal", "Amit", ""], ["Strunjas", "Svetlana", ""], ["Kannadasan", "Manojkumar Rangasamy", ""]]}, {"id": "2010.14596", "submitter": "Niranda Perera", "authors": "Niranda Perera, Vibhatha Abeykoon, Chathura Widanage, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Pulasthi Wickramasinghe, Ahmet Uyar,\n  Hasara Maithree, Damitha Lenadora, and Geoffrey Fox", "title": "A Fast, Scalable, Universal Approach For Distributed Data Aggregations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era of Big Data, data engineering has transformed into an\nessential field of study across many branches of science. Advancements in\nArtificial Intelligence (AI) have broadened the scope of data engineering and\nopened up new applications in both enterprise and research communities.\nAggregations (also termed reduce in functional programming) are an integral\nfunctionality in these applications. They are traditionally aimed at generating\nmeaningful information on large data-sets, and today, they are being used for\nengineering more effective features for complex AI models. Aggregations are\nusually carried out on top of data abstractions such as tables/ arrays and are\ncombined with other operations such as grouping of values. There are frameworks\nthat excel in the said domains individually. But, we believe that there is an\nessential requirement for a data analytics tool that can universally integrate\nwith existing frameworks, and thereby increase the productivity and efficiency\nof the entire data analytics pipeline. Cylon endeavors to fulfill this void. In\nthis paper, we present Cylon's fast and scalable aggregation operations\nimplemented on top of a distributed in-memory table structure that universally\nintegrates with existing frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 20:39:21 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 02:10:12 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Perera", "Niranda", ""], ["Abeykoon", "Vibhatha", ""], ["Widanage", "Chathura", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Maithree", "Hasara", ""], ["Lenadora", "Damitha", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2010.14707", "submitter": "Yang Qian", "authors": "Yang Qian, Yuanchun Jiang, Yidong Chai, Yezheng Liu, Jiansha Sun", "title": "TopicModel4J: A Java Package for Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models provide a flexible and principled framework for exploring hidden\nstructure in high-dimensional co-occurrence data and are commonly used natural\nlanguage processing (NLP) of text. In this paper, we design and implement a\nJava package, TopicModel4J, which contains 13 kinds of representative\nalgorithms for fitting topic models. The TopicModel4J in the Java programming\nenvironment provides an easy-to-use interface for data analysts to run the\nalgorithms, and allow to easily input and output data. In addition, this\npackage provides a few unstructured text preprocessing techniques, such as\nsplitting textual data into words, lowercasing the words, preforming\nlemmatization and removing the useless characters, URLs and stop words.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:33:41 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Qian", "Yang", ""], ["Jiang", "Yuanchun", ""], ["Chai", "Yidong", ""], ["Liu", "Yezheng", ""], ["Sun", "Jiansha", ""]]}, {"id": "2010.14709", "submitter": "Yihao Chen", "authors": "Yihao Chen, Alexander Lerch", "title": "Melody-Conditioned Lyrics Generation with SeqGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lyrics generation has received attention from both music and AI\ncommunities for years. Early rule-based approaches have~---due to increases in\ncomputational power and evolution in data-driven models---~mostly been replaced\nwith deep-learning-based systems. Many existing approaches, however, either\nrely heavily on prior knowledge in music and lyrics writing or oversimplify the\ntask by largely discarding melodic information and its relationship with the\ntext. We propose an end-to-end melody-conditioned lyrics generation system\nbased on Sequence Generative Adversarial Networks (SeqGAN), which generates a\nline of lyrics given the corresponding melody as the input. Furthermore, we\ninvestigate the performance of the generator with an additional input\ncondition: the theme or overarching topic of the lyrics to be generated. We\nshow that the input conditions have no negative impact on the evaluation\nmetrics while enabling the network to produce more meaningful results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:35:40 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Yihao", ""], ["Lerch", "Alexander", ""]]}, {"id": "2010.14848", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, Eric Nyberg", "title": "Flexible retrieval with NMSLIB and FlexNeuART", "comments": null, "journal-ref": "2nd EMNLP Workshop for Natural Language Processing Open Source\n  Software (NLP-OSS), 2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to introduce to the NLP community an existing k-NN search\nlibrary NMSLIB, a new retrieval toolkit FlexNeuART, as well as their\nintegration capabilities. NMSLIB, while being one the fastest k-NN search\nlibraries, is quite generic and supports a variety of distance/similarity\nfunctions. Because the library relies on the distance-based structure-agnostic\nalgorithms, it can be further extended by adding new distances. FlexNeuART is a\nmodular, extendible and flexible toolkit for candidate generation in IR and QA\napplications, which supports mixing of classic and neural ranking signals.\nFlexNeuART can efficiently retrieve mixed dense and sparse representations\n(with weights learned from training data), which is achieved by extending\nNMSLIB. In that, other retrieval systems work with purely sparse\nrepresentations (e.g., Lucene), purely dense representations (e.g., FAISS and\nAnnoy), or only perform mixing at the re-ranking stage.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:51:56 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 05:26:03 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "2010.15008", "submitter": "Ankur Kulkarni", "authors": "Anuj S. Vora, Ankur A. Kulkarni", "title": "Optimal Questionnaires for Screening of Strategic Agents", "comments": "Longer version of our paper submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the COVID-$19$ pandemic the health authorities at airports and train\nstations try to screen and identify the travellers possibly exposed to the\nvirus. However, many individuals avoid getting tested and hence may misreport\ntheir travel history. This is a challenge for the health authorities who wish\nto ascertain the truly susceptible cases in spite of this strategic\nmisreporting. We investigate the problem of questioning travellers to classify\nthem for further testing when the travellers are strategic or are unwilling to\nreveal their travel histories. We show there are fundamental limits to how many\ntravel histories the health authorities can recover.% can be correctly\nclassified by any probing mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:35:00 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Vora", "Anuj S.", ""], ["Kulkarni", "Ankur A.", ""]]}, {"id": "2010.15363", "submitter": "Tianxin Wei", "authors": "Tianxin Wei, Fuli Feng, Jiawei Chen, Ziwei Wu, Jinfeng Yi, Xiangnan He", "title": "Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias\n  in Recommender System", "comments": "To Appear in SIGKDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467289", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general aim of the recommender system is to provide personalized\nsuggestions to users, which is opposed to suggesting popular items. However,\nthe normal training paradigm, i.e., fitting a recommender model to recover the\nuser behavior data with pointwise or pairwise loss, makes the model biased\ntowards popular items. This results in the terrible Matthew effect, making\npopular items be more frequently recommended and become even more popular.\nExisting work addresses this issue with Inverse Propensity Weighting (IPW),\nwhich decreases the impact of popular items on the training and increases the\nimpact of long-tail items. Although theoretically sound, IPW methods are highly\nsensitive to the weighting strategy, which is notoriously difficult to tune. In\nthis work, we explore the popularity bias issue from a novel and fundamental\nperspective -- cause-effect. We identify that popularity bias lies in the\ndirect effect from the item node to the ranking score, such that an item's\nintrinsic property is the cause of mistakenly assigning it a higher ranking\nscore. To eliminate popularity bias, it is essential to answer the\ncounterfactual question that what the ranking score would be if the model only\nuses item property. To this end, we formulate a causal graph to describe the\nimportant cause-effect relations in the recommendation process. During\ntraining, we perform multi-task learning to achieve the contribution of each\ncause; during testing, we perform counterfactual inference to remove the effect\nof item popularity. Remarkably, our solution amends the learning process of\nrecommendation which is agnostic to a wide range of models -- it can be easily\nimplemented in existing methods. We demonstrate it on Matrix Factorization (MF)\nand LightGCN [20]. Experiments on five real-world datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:02:31 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:23:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wei", "Tianxin", ""], ["Feng", "Fuli", ""], ["Chen", "Jiawei", ""], ["Wu", "Ziwei", ""], ["Yi", "Jinfeng", ""], ["He", "Xiangnan", ""]]}, {"id": "2010.15620", "submitter": "Yikun Xian", "authors": "Yikun Xian, Zuohui Fu, Handong Zhao, Yingqiang Ge, Xu Chen, Qiaoying\n  Huang, Shijie Geng, Zhou Qin, Gerard de Melo, S. Muthukrishnan, Yongfeng\n  Zhang", "title": "CAFE: Coarse-to-Fine Neural Symbolic Reasoning for Explainable\n  Recommendation", "comments": "Accepted in CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412038", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research explores incorporating knowledge graphs (KG) into e-commerce\nrecommender systems, not only to achieve better recommendation performance, but\nmore importantly to generate explanations of why particular decisions are made.\nThis can be achieved by explicit KG reasoning, where a model starts from a user\nnode, sequentially determines the next step, and walks towards an item node of\npotential interest to the user. However, this is challenging due to the huge\nsearch space, unknown destination, and sparse signals over the KG, so\ninformative and effective guidance is needed to achieve a satisfactory\nrecommendation quality. To this end, we propose a CoArse-to-FinE neural\nsymbolic reasoning approach (CAFE). It first generates user profiles as coarse\nsketches of user behaviors, which subsequently guide a path-finding process to\nderive reasoning paths for recommendations as fine-grained predictions. User\nprofiles can capture prominent user behaviors from the history, and provide\nvaluable signals about which kinds of path patterns are more likely to lead to\npotential items of interest for the user. To better exploit the user profiles,\nan improved path-finding algorithm called Profile-guided Path Reasoning (PPR)\nis also developed, which leverages an inventory of neural symbolic reasoning\nmodules to effectively and efficiently find a batch of paths over a large-scale\nKG. We extensively experiment on four real-world benchmarks and observe\nsubstantial gains in the recommendation performance compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:08:07 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Xian", "Yikun", ""], ["Fu", "Zuohui", ""], ["Zhao", "Handong", ""], ["Ge", "Yingqiang", ""], ["Chen", "Xu", ""], ["Huang", "Qiaoying", ""], ["Geng", "Shijie", ""], ["Qin", "Zhou", ""], ["de Melo", "Gerard", ""], ["Muthukrishnan", "S.", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2010.15670", "submitter": "Boyu Zhang", "authors": "Boyu Zhang, Anis Zaman, Rupam Acharyya, Ehsan Hoque, Vincent Silenzio,\n  Henry Kautz", "title": "Detecting Individuals with Depressive Disorder fromPersonal Google\n  Search and YouTube History Logs", "comments": "Machine Learning in Public Health (MLPH) at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depressive disorder is one of the most prevalent mental illnesses among the\nglobal population. However, traditional screening methods require exacting\nin-person interviews and may fail to provide immediate interventions. In this\nwork, we leverage ubiquitous personal longitudinal Google Search and YouTube\nengagement logs to detect individuals with depressive disorder. We collected\nGoogle Search and YouTube history data and clinical depression evaluation\nresults from $212$ participants ($99$ of them suffered from moderate to severe\ndepressions). We then propose a personalized framework for classifying\nindividuals with and without depression symptoms based on mutual-exciting point\nprocess that captures both the temporal and semantic aspects of online\nactivities. Our best model achieved an average F1 score of $0.77 \\pm 0.04$ and\nan AUC ROC of $0.81 \\pm 0.02$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 04:40:18 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Zhang", "Boyu", ""], ["Zaman", "Anis", ""], ["Acharyya", "Rupam", ""], ["Hoque", "Ehsan", ""], ["Silenzio", "Vincent", ""], ["Kautz", "Henry", ""]]}, {"id": "2010.15711", "submitter": "Jaehyeok Han", "authors": "Jaehyeok Han, Jieon Kim, Sangjin Lee", "title": "5W1H-based Expression for the Effective Sharing of Information in\n  Digital Forensic Investigations", "comments": "This paper was presented at the Sixteenth Annual IFIP WG 11.9\n  International Conference on Digital Forensics, Delhi, India, in January 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital forensic investigation is used in various areas related to digital\ndevices including the cyber crime. This is an investigative process using many\ntechniques, which have implemented as tools. The types of files covered by the\ndigital forensic investigation are wide and varied, however, there is no way to\nexpress the results into a standardized format. The standardization are\ndifferent by types of device, file system, or application. Different outputs\nmake it time-consuming and difficult to share information and to implement\nintegration. In addition, it could weaken cyber security. Thus, it is important\nto define normalization and to present data in the same format. In this paper,\na 5W1H-based expression for information sharing for effective digital forensic\ninvestigation is proposed to analyze digital forensic information using six\nquestions--what, who, where, when, why and how. Based on the 5W1H-based\nexpression, digital information from different types of files is converted and\nrepresented in the same format of outputs. As the 5W1H is the basic writing\nprinciple, application of the 5W1H-based expression on the case studies shows\nthat this expression enhances clarity and correctness for information sharing.\nFurthermore, in the case of security incidents, this expression has an\nadvantage in being compatible with STIX.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 00:44:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Han", "Jaehyeok", ""], ["Kim", "Jieon", ""], ["Lee", "Sangjin", ""]]}, {"id": "2010.15879", "submitter": "Maciej Besta", "authors": "Maciej Besta, Dimitri Stanojevic, Tijana Zivic, Jagpreet Singh,\n  Maurice Hoerold, Torsten Hoefler", "title": "Log(Graph): A Near-Optimal High-Performance Graph Representation", "comments": null, "journal-ref": "Proceedings of the 27th International Conference on Parallel\n  Architectures and Compilation (PACT'18), 2018", "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's graphs used in domains such as machine learning or social network\nanalysis may contain hundreds of billions of edges. Yet, they are not\nnecessarily stored efficiently, and standard graph representations such as\nadjacency lists waste a significant number of bits while graph compression\nschemes such as WebGraph often require time-consuming decompression. To address\nthis, we propose Log(Graph): a graph representation that combines high\ncompression ratios with very low-overhead decompression to enable cheaper and\nfaster graph processing. The key idea is to encode a graph so that the parts of\nthe representation approach or match the respective storage lower bounds. We\ncall our approach \"graph logarithmization\" because these bounds are usually\nlogarithmic. Our high-performance Log(Graph) implementation based on modern\nbitwise operations and state-of-the-art succinct data structures achieves high\ncompression ratios as well as performance. For example, compared to the tuned\nGraph Algorithm Processing Benchmark Suite (GAPBS), it reduces graph sizes by\n20-35% while matching GAPBS' performance or even delivering speedups due to\nreducing amounts of transferred data. It approaches the compression ratio of\nthe established WebGraph compression library while enabling speedups of up to\nmore than 2x. Log(Graph) can improve the design of various graph processing\nengines or libraries on single NUMA nodes as well as distributed-memory\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:41:08 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Stanojevic", "Dimitri", ""], ["Zivic", "Tijana", ""], ["Singh", "Jagpreet", ""], ["Hoerold", "Maurice", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2010.15924", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "How Many Pages? Paper Length Prediction from the Metadata", "comments": "5 pages, 6 tables. Published in proceedings of NLPIR 2020, the 4th\n  International Conference on Natural Language Processing and Information\n  Retrieval, Seoul, Korea", "journal-ref": null, "doi": "10.1145/3443279.3443305", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict the length of a scientific paper may be helpful in\nnumerous situations. This work defines the paper length prediction task as a\nregression problem and reports several experimental results using popular\nmachine learning models. We also create a huge dataset of publication metadata\nand the respective lengths in number of pages. The dataset will be freely\navailable and is intended to foster research in this domain. As future work, we\nwould like to explore more advanced regressors based on neural networks and big\npretrained language models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 20:28:24 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 15:21:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2010.15982", "submitter": "Yin Zhang", "authors": "Yin Zhang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Lichan\n  Hong, Ed H. Chi", "title": "A Model of Two Tales: Dual Transfer Learning Framework for Improved\n  Long-tail Item Recommendation", "comments": "Accepted by WWW 2021 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly skewed long-tail item distribution is very common in recommendation\nsystems. It significantly hurts model performance on tail items. To improve\ntail-item recommendation, we conduct research to transfer knowledge from head\nitems to tail items, leveraging the rich user feedback in head items and the\nsemantic connections between head and tail items. Specifically, we propose a\nnovel dual transfer learning framework that jointly learns the knowledge\ntransfer from both model-level and item-level: 1. The model-level knowledge\ntransfer builds a generic meta-mapping of model parameters from few-shot to\nmany-shot model. It captures the implicit data augmentation on the model-level\nto improve the representation learning of tail items. 2. The item-level\ntransfer connects head and tail items through item-level features, to ensure a\nsmooth transfer of meta-mapping from head items to tail items. The two types of\ntransfers are incorporated to ensure the learned knowledge from head items can\nbe well applied for tail item representation learning in the long-tail\ndistribution settings. Through extensive experiments on two benchmark datasets,\nresults show that our proposed dual transfer learning framework significantly\noutperforms other state-of-the-art methods for tail item recommendation in hit\nratio and NDCG. It is also very encouraging that our framework further improves\nhead items and overall performance on top of the gains on tail items.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 22:56:04 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 19:22:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Yin", ""], ["Cheng", "Derek Zhiyuan", ""], ["Yao", "Tiansheng", ""], ["Yi", "Xinyang", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""]]}, {"id": "2010.16030", "submitter": "Minz Won", "authors": "Minz Won, Sergio Oramas, Oriol Nieto, Fabien Gouyon, Xavier Serra", "title": "Multimodal Metric Learning for Tag-based Music Retrieval", "comments": "5 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tag-based music retrieval is crucial to browse large-scale music libraries\nefficiently. Hence, automatic music tagging has been actively explored, mostly\nas a classification task, which has an inherent limitation: a fixed vocabulary.\nOn the other hand, metric learning enables flexible vocabularies by using\npretrained word embeddings as side information. Also, metric learning has\nalready proven its suitability for cross-modal retrieval tasks in other domains\n(e.g., text-to-image) by jointly learning a multimodal embedding space. In this\npaper, we investigate three ideas to successfully introduce multimodal metric\nlearning for tag-based music retrieval: elaborate triplet sampling, acoustic\nand cultural music information, and domain-specific word embeddings. Our\nexperimental results show that the proposed ideas enhance the retrieval system\nquantitatively, and qualitatively. Furthermore, we release the MSD500, a subset\nof the Million Song Dataset (MSD) containing 500 cleaned tags, 7 manually\nannotated tag categories, and user taste profiles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 02:46:28 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Won", "Minz", ""], ["Oramas", "Sergio", ""], ["Nieto", "Oriol", ""], ["Gouyon", "Fabien", ""], ["Serra", "Xavier", ""]]}, {"id": "2010.16037", "submitter": "Mohamed Trabelsi", "authors": "Mohamed Trabelsi, Jin Cao, Jeff Heflin", "title": "Semantic Labeling Using a Deep Contextualized Language Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating schema labels automatically for column values of data tables has\nmany data science applications such as schema matching, and data discovery and\nlinking. For example, automatically extracted tables with missing headers can\nbe filled by the predicted schema labels which significantly minimizes human\neffort. Furthermore, the predicted labels can reduce the impact of inconsistent\nnames across multiple data tables. Understanding the connection between column\nvalues and contextual information is an important yet neglected aspect as\npreviously proposed methods treat each column independently. In this paper, we\npropose a context-aware semantic labeling method using both the column values\nand context. Our new method is based on a new setting for semantic labeling,\nwhere we sequentially predict labels for an input table with missing headers.\nWe incorporate both the values and context of each data column using the\npre-trained contextualized language model, BERT, that has achieved significant\nimprovements in multiple natural language processing tasks. To our knowledge,\nwe are the first to successfully apply BERT to solve the semantic labeling\ntask. We evaluate our approach using two real-world datasets from different\ndomains, and we demonstrate substantial improvements in terms of evaluation\nmetrics over state-of-the-art feature-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 03:04:22 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Trabelsi", "Mohamed", ""], ["Cao", "Jin", ""], ["Heflin", "Jeff", ""]]}, {"id": "2010.16218", "submitter": "Claudia Schulz", "authors": "Claudia Schulz and Josh Levy-Kramer and Camille Van Assel and Miklos\n  Kepes and Nils Hammerla", "title": "Biomedical Concept Relatedness -- A large EHR-based benchmark", "comments": "Accepted for publication at the 28th International Conference on\n  Computational Linguistics (COLING 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising application of AI to healthcare is the retrieval of information\nfrom electronic health records (EHRs), e.g. to aid clinicians in finding\nrelevant information for a consultation or to recruit suitable patients for a\nstudy. This requires search capabilities far beyond simple string matching,\nincluding the retrieval of concepts (diagnoses, symptoms, medications, etc.)\nrelated to the one in question. The suitability of AI methods for such\napplications is tested by predicting the relatedness of concepts with known\nrelatedness scores. However, all existing biomedical concept relatedness\ndatasets are notoriously small and consist of hand-picked concept pairs. We\nopen-source a novel concept relatedness benchmark overcoming these issues: it\nis six times larger than existing datasets and concept pairs are chosen based\non co-occurrence in EHRs, ensuring their relevance for the application of\ninterest. We present an in-depth analysis of our new dataset and compare it to\nexisting ones, highlighting that it is not only larger but also complements\nexisting datasets in terms of the types of concepts included. Initial\nexperiments with state-of-the-art embedding methods show that our dataset is a\nchallenging new benchmark for testing concept relatedness models.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 12:20:18 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Schulz", "Claudia", ""], ["Levy-Kramer", "Josh", ""], ["Van Assel", "Camille", ""], ["Kepes", "Miklos", ""], ["Hammerla", "Nils", ""]]}, {"id": "2010.16310", "submitter": "Kleanthis Avramidis", "authors": "Kleanthis Avramidis, Athanasia Zlatintsi, Christos Garoufis, Petros\n  Maragos", "title": "Multiscale Fractal Analysis on EEG Signals for Music-Induced Emotion\n  Recognition", "comments": "5 pages, 3 figures, 3 tables, submitted to the European Signal\n  Processing Conference (EUSIPCO) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Recognition from EEG signals has long been researched as it can\nassist numerous medical and rehabilitative applications. However, their complex\nand noisy structure has proven to be a serious barrier for traditional modeling\nmethods. In this paper, we employ multifractal analysis to examine the behavior\nof EEG signals in terms of presence of fluctuations and the degree of\nfragmentation along their major frequency bands, for the task of emotion\nrecognition. In order to extract emotion-related features we utilize two novel\nalgorithms for EEG analysis, based on Multiscale Fractal Dimension and\nMultifractal Detrended Fluctuation Analysis. The proposed feature extraction\nmethods perform efficiently, surpassing some widely used baseline features on\nthe competitive DEAP dataset, indicating that multifractal analysis could serve\nas basis for the development of robust models for affective state recognition.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:01:05 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 11:29:13 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Avramidis", "Kleanthis", ""], ["Zlatintsi", "Athanasia", ""], ["Garoufis", "Christos", ""], ["Maragos", "Petros", ""]]}, {"id": "2010.16313", "submitter": "Shigehiko Schamoni", "authors": "Toshitaka Kuwa and Shigehiko Schamoni and Stefan Riezler", "title": "Embedding Meta-Textual Information for Improved Learning to Rank", "comments": "Accepted as a long paper at COLING 2020, Barcelona, Spain", "journal-ref": null, "doi": "10.18653/v1/2020.coling-main.487", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural approaches to learning term embeddings have led to improved\ncomputation of similarity and ranking in information retrieval (IR). So far\nneural representation learning has not been extended to meta-textual\ninformation that is readily available for many IR tasks, for example, patent\nclasses in prior-art retrieval, topical information in Wikipedia articles, or\nproduct categories in e-commerce data. We present a framework that learns\nembeddings for meta-textual categories, and optimizes a pairwise ranking\nobjective for improved matching based on combined embeddings of textual and\nmeta-textual information. We show considerable gains in an experimental\nevaluation on cross-lingual retrieval in the Wikipedia domain for three\nlanguage pairs, and in the Patent domain for one language pair. Our results\nemphasize that the mode of combining different types of information is crucial\nfor model improvement.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 15:06:08 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kuwa", "Toshitaka", ""], ["Schamoni", "Shigehiko", ""], ["Riezler", "Stefan", ""]]}, {"id": "2010.16413", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Robert Leaman, Alexis Allot, Ling Luo, Chih-Hsuan Wei,\n  Shankai Yan, Zhiyong Lu", "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic\n  with Natural Language Processing (NLP)", "comments": "51 pages, 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 22:10:43 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 18:22:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Qingyu", ""], ["Leaman", "Robert", ""], ["Allot", "Alexis", ""], ["Luo", "Ling", ""], ["Wei", "Chih-Hsuan", ""], ["Yan", "Shankai", ""], ["Lu", "Zhiyong", ""]]}]