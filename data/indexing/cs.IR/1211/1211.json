[{"id": "1211.0320", "submitter": "Rami Al-Rfou'", "authors": "Rami Al-Rfou', William Jannen, Nikhil Patwardhan", "title": "TrackMeNot-so-good-after-all", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TrackMeNot is a Firefox plugin with laudable intentions - protecting your\nprivacy. By issuing a customizable stream of random search queries on its\nusers' behalf, TrackMeNot surmises that enough search noise will prevent its\nusers' true query profiles from being discerned. However, we find that\nclustering queries by semantic relatedness allows us to disentangle a\nnontrivial subset of true user queries from TrackMeNot issued noise.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 22:16:57 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Al-Rfou'", "Rami", ""], ["Jannen", "William", ""], ["Patwardhan", "Nikhil", ""]]}, {"id": "1211.0390", "submitter": "Mohammad Allahbakhsh", "authors": "Mohammad Allahbakhsh, Aleksandar Ignjatovic", "title": "Rating through Voting: An Iterative Method for Robust Rating", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TPDS.2013.215", "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce an iterative voting algorithm and then use it to\nobtain a rating method which is very robust against collusion attacks as well\nas random and biased raters. Unlike the previous iterative methods, our method\nis not based on comparing submitted evaluations to an approximation of the\nfinal rating scores, and it entirely decouples credibility assessment of the\ncast evaluations from the ranking itself. The convergence of our algorithm\nrelies on the existence of a fixed point of a continuous mapping which is also\na stationary point of a constrained optimization objective. We have implemented\nand tested our rating method using both simulated data as well as real world\ndata. In particular, we have applied our method to movie evaluations obtained\nfrom MovieLens and compared our results with IMDb and Rotten Tomatoes movie\nrating sites. Not only are the ratings provided by our system very close to\nIMDb rating scores, but when we differ from the IMDb ratings, the direction of\nsuch differences is essentially always towards the ratings provided by the\ncritics in Rotten Tomatoes. Our tests demonstrate high efficiency of our\nmethod, especially for very large online rating systems, for which trust\nmanagement is both of the highest importance and one of the most challenging\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 08:16:53 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Allahbakhsh", "Mohammad", ""], ["Ignjatovic", "Aleksandar", ""]]}, {"id": "1211.0689", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Enhancing Invenio Digital Library With An External Relevance Ranking\n  Engine", "comments": "70 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invenio is a comprehensive web-based free digital library software suite\noriginally developed at CERN. In order to improve its information retrieval and\nword similarity ranking capabilities, the goal of this thesis is to enhance\nInvenio by bridging it with modern external information retrieval systems. In\nthe first part a comparison of various information retrieval systems such as\nSolr and Xapian is made. In the second part a system-independent bridge for\nword similarity ranking is designed and implemented. Subsequently, Solr and\nXapian are integrated in Invenio via adapters to the bridge. In the third part\nscalability tests are performed. Finally, a future outlook is briefly\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2012 14:50:16 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1211.0963", "submitter": "Mohammad Allahbakhsh", "authors": "Mohammad Allahbakhsh, Aleksandar Ignjatovic, Boualem Benatallah,\n  Seyed-Mehdi-Reza Beheshti, Norman Foo, Elisa Bertino", "title": "Detecting, Representing and Querying Collusion in Online Rating Systems", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201220", "categories": "cs.CR cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online rating systems are subject to malicious behaviors mainly by posting\nunfair rating scores. Users may try to individually or collaboratively promote\nor demote a product. Collaborating unfair rating 'collusion' is more damaging\nthan individual unfair rating. Although collusion detection in general has been\nwidely studied, identifying collusion groups in online rating systems is less\nstudied and needs more investigation. In this paper, we study impact of\ncollusion in online rating systems and asses their susceptibility to collusion\nattacks. The proposed model uses a frequent itemset mining algorithm to detect\ncandidate collusion groups. Then, several indicators are used for identifying\ncollusion groups and for estimating how damaging such colluding groups might\nbe. Also, we propose an algorithm for finding possible collusive subgroup\ninside larger groups which are not identified as collusive. The model has been\nimplemented and we present results of experimental evaluation of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 07:50:05 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Allahbakhsh", "Mohammad", ""], ["Ignjatovic", "Aleksandar", ""], ["Benatallah", "Boualem", ""], ["Beheshti", "Seyed-Mehdi-Reza", ""], ["Foo", "Norman", ""], ["Bertino", "Elisa", ""]]}, {"id": "1211.1107", "submitter": "S. K. Sahay", "authors": "R.K. Roul and S.K. Sahay", "title": "An effective web document clustering for information retrieval", "comments": "11 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "IJCSMR, 2012, Vol. 1, No. 3, p. 481", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size of web has increased exponentially over the past few years with\nthousands of documents related to a subject available to the user. With this\nmuch amount of information available, it is not possible to take the full\nadvantage of the World Wide Web without having a proper framework to search\nthrough the available data. This requisite organization can be done in many\nways. In this paper we introduce a combine approach to cluster the web pages\nwhich first finds the frequent sets and then clusters the documents. These\nfrequent sets are generated by using Frequent Pattern growth technique. Then by\napplying Fuzzy C- Means algorithm on it, we found clusters having documents\nwhich are highly related and have similar features. We used Gensim package to\nimplement our approach because of its simplicity and robust nature. We have\ncompared our results with the combine approach of (Frequent Pattern growth,\nK-means) and (Frequent Pattern growth, Cosine_Similarity). Experimental results\nshow that our approach is more efficient then the above two combine approach\nand can handles more efficiently the serious limitation of traditional Fuzzy\nC-Means algorithm, which is sensitiveto initial centroid and the number of\nclusters to be formed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 04:39:30 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Roul", "R. K.", ""], ["Sahay", "S. K.", ""]]}, {"id": "1211.1780", "submitter": "Iyad Abu Doush Dr.", "authors": "Iyad Abu Doush, Faisal Alkhateeb, Eslam Al Maghayreh, Izzat Alsmadi,\n  Samer Samarah", "title": "Annotations, Collaborative Tagging, and Searching Mathematics in\n  E-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new framework for adding semantics into e-learning\nsystem. The proposed approach relies on two principles. The first principle is\nthe automatic addition of semantic information when creating the mathematical\ncontents. The second principle is the collaborative tagging and annotation of\nthe e-learning contents and the use of an ontology to categorize the e-learning\ncontents. The proposed system encodes the mathematical contents using\npresentation MathML with RDFa annotations. The system allows students to\nhighlight and annotate specific parts of the e-learning contents. The objective\nis to add meaning into the e-learning contents, to add relationships between\ncontents, and to create a framework to facilitate searching the contents. This\nsemantic information can be used to answer semantic queries (e.g., SPARQL) to\nretrieve information request of a user. This work is implemented as an embedded\ncode into Moodle e-learning system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 07:53:43 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Doush", "Iyad Abu", ""], ["Alkhateeb", "Faisal", ""], ["Maghayreh", "Eslam Al", ""], ["Alsmadi", "Izzat", ""], ["Samarah", "Samer", ""]]}, {"id": "1211.1861", "submitter": "Mohamed Firdhous", "authors": "Mohamed Firdhous", "title": "Automating Legal Research through Data Mining", "comments": "8 pages, 11 figures, published in (IJACSA) International Journal of\n  Advanced Computer Science and Applications. arXiv admin note: text overlap\n  with wikipedia entry on text mining", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA) Vol. 1, No 6, December 2010, pp. 9-16", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term legal research generally refers to the process of identifying and\nretrieving appropriate information necessary to support legal decision making\nfrom past case records. At present, the process is mostly manual, but some\ntraditional technologies such as keyword searching are commonly used to speed\nthe process up. But a keyword search is not a comprehensive search to cater to\nthe requirements of legal research as the search result includes too many false\nhits in terms of irrelevant case records. Hence the present generic tools\ncannot be used to automate legal research.\n  This paper presents a framework which was developed by combining several Text\nMining techniques to automate the process overcoming the difficulties in the\nexisting methods. Further, the research also identifies the possible\nenhancements that could be done to enhance the effectiveness of the framework.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 14:19:49 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Firdhous", "Mohamed", ""]]}, {"id": "1211.2741", "submitter": "Venkateshwara Prasad Tangirala", "authors": "Kamlesh Sharma, S. V. A. V. Prasad and T. V. Prasad", "title": "A Hindi Speech Actuated Computer Interface for Web Search", "comments": "7 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications 3(10), 2012, 147-152", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at increasing system simplicity and flexibility, an audio evoked based\nsystem was developed by integrating simplified headphone and user-friendly\nsoftware design. This paper describes a Hindi Speech Actuated Computer\nInterface for Web search (HSACIWS), which accepts spoken queries in Hindi\nlanguage and provides the search result on the screen. This system recognizes\nspoken queries by large vocabulary continuous speech recognition (LVCSR),\nretrieves relevant document by text retrieval, and provides the search result\non the Web by the integration of the Web and the voice systems. The LVCSR in\nthis system showed enough performance levels for speech with acoustic and\nlanguage models derived from a query corpus with target contents.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 19:17:34 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Sharma", "Kamlesh", ""], ["Prasad", "S. V. A. V.", ""], ["Prasad", "T. V.", ""]]}, {"id": "1211.2854", "submitter": "Eya Ben Ahmed", "authors": "Wahiba Ben Abdessalem Karaa Nouha Mhimdi", "title": "Using ontology for resume annotation", "comments": "9 pages", "journal-ref": "International Journal of Metadata, Semantics and Ontologies\n  (IJMSO), 2011 Vol. 6 No. 3/4", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employers collect a large number of resumes from job portals, or from the\ncompany's own website. These documents are used for an automated selection of\ncandidates satisfying the requirements and therefore reducing recruitment\ncosts. Various approaches for process documents have already been developed for\nrecruitment. In this paper we present an approach based on semantic annotation\nof resumes for e-recruitment process. The most important task consists on\nmodelling the semantic content of these documents using ontology. The ontology\nis built taking into account the most significant components of resumes\ninspired from the structure of EUROPASS CV. This ontology is thereafter used to\nannotate automatically the resumes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 23:47:47 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Mhimdi", "Wahiba Ben Abdessalem Karaa Nouha", ""]]}, {"id": "1211.2891", "submitter": "Lior Rokach", "authors": "Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar", "title": "Boosting Simple Collaborative Filtering Models Using Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the effect of applying ensemble learning to the\nperformance of collaborative filtering methods. We present several systematic\napproaches for generating an ensemble of collaborative filtering models based\non a single collaborative filtering algorithm (single-model or homogeneous\nensemble). We present an adaptation of several popular ensemble techniques in\nmachine learning for the collaborative filtering domain, including bagging,\nboosting, fusion and randomness injection. We evaluate the proposed approach on\nseveral types of collaborative filtering base models: k- NN, matrix\nfactorization and a neighborhood matrix factorization model. Empirical\nevaluation shows a prediction improvement compared to all base CF algorithms.\nIn particular, we show that the performance of an ensemble of simple (weak) CF\nmodels such as k-NN is competitive compared with a single strong CF model (such\nas matrix factorization) while requiring an order of magnitude less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 05:30:36 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Bar", "Ariel", ""], ["Rokach", "Lior", ""], ["Shani", "Guy", ""], ["Shapira", "Bracha", ""], ["Schclar", "Alon", ""]]}, {"id": "1211.3200", "submitter": "Mohammad Allahbakhsh", "authors": "Mohammad Allahbakhsh, Aleksandar Ignjatovic, Boualem Benatallah,\n  Seyed-Mehdi-Reza Beheshti, Norman Foo, Elisa Bertino", "title": "An Analytic Approach to People Evaluation in Crowdsourcing Systems", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": "UNSW-CSE-TR-201204", "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worker selection is a significant and challenging issue in crowdsourcing\nsystems. Such selection is usually based on an assessment of the reputation of\nthe individual workers participating in such systems. However, assessing the\ncredibility and adequacy of such calculated reputation is a real challenge. In\nthis paper, we propose an analytic model which leverages the values of the\ntasks completed, the credibility of the evaluators of the results of the tasks\nand time of evaluation of the results of these tasks in order to calculate an\naccurate and credible reputation rank of participating workers and fairness\nrank for evaluators. The model has been implemented and experimentally\nvalidated.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 04:25:57 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Allahbakhsh", "Mohammad", ""], ["Ignjatovic", "Aleksandar", ""], ["Benatallah", "Boualem", ""], ["Beheshti", "Seyed-Mehdi-Reza", ""], ["Foo", "Norman", ""], ["Bertino", "Elisa", ""]]}, {"id": "1211.3402", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Genetic Optimization of Keywords Subset in the Classification Analysis\n  of Texts Authorship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The genetic selection of keywords set, the text frequencies of which are\nconsidered as attributes in text classification analysis, has been analyzed.\nThe genetic optimization was performed on a set of words, which is the fraction\nof the frequency dictionary with given frequency limits. The frequency\ndictionary was formed on the basis of analyzed text array of texts of English\nfiction. As the fitness function which is minimized by the genetic algorithm,\nthe error of nearest k neighbors classifier was used. The obtained results show\nhigh precision and recall of texts classification by authorship categories on\nthe basis of attributes of keywords set which were selected by the genetic\nalgorithm from the frequency dictionary.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 20:04:51 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1211.3497", "submitter": "Prabath Abeysiriwardana", "authors": "Prabath Chaminda Abeysiriwardana, Saluka R Kodituwakku", "title": "Ontology Based Information Extraction for Disease Intelligence", "comments": "Disease Intelligence, Disease Ontology, Information Extraction,\n  Semantic Web", "journal-ref": "International Journal of Research in Computer Science, 2 (6): pp.\n  7-19, November 2012. doi:10.7815/ijorcs.26.2012.051", "doi": "10.7815/ijorcs.26.2012.051", "report-no": null, "categories": "cs.AI cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease Intelligence (DI) is based on the acquisition and aggregation of\nfragmented knowledge of diseases at multiple sources all over the world to\nprovide valuable information to doctors, researchers and information seeking\ncommunity. Some diseases have their own characteristics changed rapidly at\ndifferent places of the world and are reported on documents as unrelated and\nheterogeneous information which may be going unnoticed and may not be quickly\navailable. This research presents an Ontology based theoretical framework in\nthe context of medical intelligence and country/region. Ontology is designed\nfor storing information about rapidly spreading and changing diseases with\nincorporating existing disease taxonomies to genetic information of both humans\nand infectious organisms. It further maps disease symptoms to diseases and drug\neffects to disease symptoms. The machine understandable disease ontology\nrepresented as a website thus allows the drug effects to be evaluated on\ndisease symptoms and exposes genetic involvements in the human diseases.\nInfectious agents which have no known place in an existing classification but\nhave data on genetics would still be identified as organisms through the\nintelligence of this system. It will further facilitate researchers on the\nsubject to try out different solutions for curing diseases.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 05:33:28 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Abeysiriwardana", "Prabath Chaminda", ""], ["Kodituwakku", "Saluka R", ""]]}, {"id": "1211.4266", "submitter": "Ryan Rossi", "authors": "David F. Gleich and Ryan A. Rossi", "title": "A Dynamical System for PageRank with Time-Dependent Teleportation", "comments": "arXiv admin note: substantial text overlap with arXiv:1203.6098", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dynamical system that captures changes to the network centrality\nof nodes as external interest in those nodes vary. We derive this system by\nadding time-dependent teleportation to the PageRank score. The result is not a\nsingle set of importance scores, but rather a time-dependent set. These can be\nconverted into ranked lists in a variety of ways, for instance, by taking the\nlargest change in the importance score. For an interesting class of the dynamic\nteleportation functions, we derive closed form solutions for the dynamic\nPageRank vector. The magnitude of the deviation from a static PageRank vector\nis given by a PageRank problem with complex-valued teleportation parameters.\nMoreover, these dynamical systems are easy to evaluate. We demonstrate the\nutility of dynamic teleportation on both the article graph of Wikipedia, where\nthe external interest information is given by the number of hourly visitors to\neach page, and the Twitter social network, where external interest is the\nnumber of tweets per month. For these problems, we show that using information\nfrom the dynamical system helps improve a prediction task and identify trends\nin the data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 22:51:34 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Gleich", "David F.", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1211.4370", "submitter": "Elahe Moghimi", "authors": "Elahe Moghimi Hanjani and Mahdi Javanmard", "title": "An Algorithm for Optimized Searching using NON-Overlapping Iterative\n  Neighbor intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have attempted in this paper to reduce the number of checked condition\nthrough saving frequency of the tandem replicated words, and also using\nnon-overlapping iterative neighbor intervals on plane sweep algorithm. The\nessential idea of non-overlapping iterative neighbor search in a document lies\nin focusing the search not on the full space of solutions but on a smaller\nsubspace considering non-overlapping intervals defined by the solutions.\nSubspace is defined by the range near the specified minimum keyword. We\nrepeatedly pick a range up and flip the unsatisfied keywords, so the relevant\nranges are detected. The proposed method tries to improve the plane sweep\nalgorithm by efficiently calculating the minimal group of words and enumerating\nintervals in a document which contain the minimum frequency keyword. It\ndecreases the number of comparison and creates the best state of optimized\nsearch algorithm especially in a high volume of data. Efficiency and\nreliability are also increased compared to the previous modes of the technical\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 11:49:44 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Hanjani", "Elahe Moghimi", ""], ["Javanmard", "Mahdi", ""]]}, {"id": "1211.4521", "submitter": "Tyler Clemons Mr", "authors": "Tyler Clemons, S. M. Faisal, Shirish Tatikonda, Charu Aggarawl, and\n  Srinivasan Parthasarathy", "title": "Hash in a Flash: Hash Tables for Solid State Devices", "comments": "16 pages 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, information retrieval algorithms have taken center stage for\nextracting important data in ever larger datasets. Advances in hardware\ntechnology have lead to the increasingly wide spread use of flash storage\ndevices. Such devices have clear benefits over traditional hard drives in terms\nof latency of access, bandwidth and random access capabilities particularly\nwhen reading data. There are however some interesting trade-offs to consider\nwhen leveraging the advanced features of such devices. On a relative scale\nwriting to such devices can be expensive. This is because typical flash devices\n(NAND technology) are updated in blocks. A minor update to a given block\nrequires the entire block to be erased, followed by a re-writing of the block.\nOn the other hand, sequential writes can be two orders of magnitude faster than\nrandom writes. In addition, random writes are degrading to the life of the\nflash drive, since each block can support only a limited number of erasures.\nTF-IDF can be implemented using a counting hash table. In general, hash tables\nare a particularly challenging case for the flash drive because this data\nstructure is inherently dependent upon the randomness of the hash function, as\nopposed to the spatial locality of the data. This makes it difficult to avoid\nthe random writes incurred during the construction of the counting hash table\nfor TF-IDF. In this paper, we will study the design landscape for the\ndevelopment of a hash table for flash storage devices. We demonstrate how to\neffectively design a hash table with two related hash functions, one of which\nexhibits a data placement property with respect to the other. Specifically, we\nfocus on three designs based on this general philosophy and evaluate the\ntrade-offs among them along the axes of query performance, insert and update\ntimes and I/O time through an implementation of the TF-IDF algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 17:55:01 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Clemons", "Tyler", ""], ["Faisal", "S. M.", ""], ["Tatikonda", "Shirish", ""], ["Aggarawl", "Charu", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1211.4709", "submitter": "Manjula Shenoy K", "authors": "Manjula Shenoy.K, K.C.Shet, U.Dinesh Acharya", "title": "A New Similarity Measure for Taxonomy Based on Edge Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new similarity measure based on edge counting in a\ntaxonomy like WorldNet or Ontology. Measurement of similarity between text\nsegments or concepts is very useful for many applications like information\nretrieval, ontology matching, text mining, and question answering and so on.\nSeveral measures have been developed for measuring similarity between two\nconcepts: out of these we see that the measure given by Wu and Palmer [1] is\nsimple, and gives good performance. Our measure is based on their measure but\nstrengthens it. Wu and Palmer [1] measure has a disadvantage that it does not\nconsider how far the concepts are semantically. In our measure we include the\nshortest path between the concepts and the depth of whole taxonomy together\nwith the distances used in Wu and Palmer [1]. Also the measure has following\ndisadvantage i.e. in some situations, the similarity of two elements of an IS-A\nontology contained in the neighborhood exceeds the similarity value of two\nelements contained in the same hierarchy. Our measure introduces a penalization\nfactor for this case based upon shortest length between the concepts and depth\nof whole taxonomy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 10:53:22 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["K", "Manjula Shenoy.", ""], ["Shet", "K. C.", ""], ["Acharya", "U. Dinesh", ""]]}, {"id": "1211.4929", "submitter": "Trung Nguyen", "authors": "Trung V. Nguyen and Alice H. Oh", "title": "Summarizing Reviews with Variable-length Syntactic Patterns and Topic\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel summarization framework for reviews of products and\nservices by selecting informative and concise text segments from the reviews.\nOur method consists of two major steps. First, we identify five frequently\noccurring variable-length syntactic patterns and use them to extract candidate\nsegments. Then we use the output of a joint generative sentiment topic model to\nfilter out the non-informative segments. We verify the proposed method with\nquantitative and qualitative experiments. In a quantitative study, our approach\noutperforms previous methods in producing informative segments and summaries\nthat capture aspects of products and services as expressed in the\nuser-generated pros and cons lists. Our user study with ninety users resonates\nwith this result: individual segments extracted and filtered by our method are\nrated as more useful by users compared to previous approaches by users.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 03:59:06 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Nguyen", "Trung V.", ""], ["Oh", "Alice H.", ""]]}, {"id": "1211.5353", "submitter": "Roberto Konow", "authors": "Roberto Konow and Gonzalo Navarro", "title": "Faster Compact Top-k Document Retrieval", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optimal index solving top-k document retrieval [Navarro and Nekrich,\nSODA12] takes O(m + k) time for a pattern of length m, but its space is at\nleast 80n bytes for a collection of n symbols. We reduce it to 1.5n to 3n\nbytes, with O(m+(k+log log n) log log n) time, on typical texts. The index is\nup to 25 times faster than the best previous compressed solutions, and requires\nat most 5% more space in practice (and in some cases as little as one half).\nApart from replacing classical by compressed data structures, our main idea is\nto replace suffix tree sampling by frequency thresholding to achieve\ncompression.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 18:58:27 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Konow", "Roberto", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1211.5492", "submitter": "Mohammad Soleymani", "authors": "Mohammad Soleymani and Martha Larson and Thierry Pun and Alan Hanjalic", "title": "Corpus Development for Affective Video Indexing", "comments": "Manuscript published", "journal-ref": "IEEE Transactions on Multimedia 16(4):1075-1089, 2014", "doi": "10.1109/TMM.2014.2305573", "report-no": null, "categories": "cs.MM cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective video indexing is the area of research that develops techniques to\nautomatically generate descriptions of video content that encode the emotional\nreactions which the video content evokes in viewers. This paper provides a set\nof corpus development guidelines based on state-of-the-art practice intended to\nsupport researchers in this field. Affective descriptions can be used for video\nsearch and browsing systems offering users affective perspectives. The paper is\nmotivated by the observation that affective video indexing has yet to fully\nprofit from the standard corpora (data sets) that have benefited conventional\nforms of video indexing. Affective video indexing faces unique challenges,\nsince viewer-reported affective reactions are difficult to assess. Moreover\naffect assessment efforts must be carefully designed in order to both cover the\ntypes of affective responses that video content evokes in viewers and also\ncapture the stable and consistent aspects of these responses. We first present\nbackground information on affect and multimedia and related work on affective\nmultimedia indexing, including existing corpora. Three dimensions emerge as\ncritical for affective video corpora, and form the basis for our proposed\nguidelines: the context of viewer response, personal variation among viewers,\nand the effectiveness and efficiency of corpus creation. Finally, we present\nexamples of three recent corpora and discuss how these corpora make progressive\nsteps towards fulfilling the guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 13:06:25 GMT"}, {"version": "v2", "created": "Tue, 19 Aug 2014 10:11:02 GMT"}, {"version": "v3", "created": "Fri, 28 Nov 2014 10:52:25 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Soleymani", "Mohammad", ""], ["Larson", "Martha", ""], ["Pun", "Thierry", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1211.5723", "submitter": "Neelamadhab Padhy Padhy", "authors": "Neelamadhab Padhy, Dr. Pragnyaban Mishra, and Rasmita Panigrahi", "title": "The Survey of Data Mining Applications And Feature Scope", "comments": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol.2, No.3, June 2012, 16 pages, 1 table", "journal-ref": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol.2, No.3,page no-43 June 2012 ,DOI :\n  10.5121/ijcseit.2012.2303", "doi": "10.5121/ijcseit.2012.2303", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we have focused a variety of techniques, approaches and\ndifferent areas of the research which are helpful and marked as the important\nfield of data mining Technologies. As we are aware that many Multinational\ncompanies and large organizations are operated in different places of the\ndifferent countries.Each place of operation may generate large volumes of data.\nCorporate decision makers require access from all such sources and take\nstrategic decisions.The data warehouse is used in the significant business\nvalue by improving the effectiveness of managerial decision-making. In an\nuncertain and highly competitive business environment, the value of strategic\ninformation systems such as these are easily recognized however in todays\nbusiness environment,efficiency or speed is not the only key for\ncompetitiveness.This type of huge amount of data are available in the form of\ntera-topeta-bytes which has drastically changed in the areas of science and\nengineering.To analyze,manage and make a decision of such type of huge amount\nof data we need techniques called the data mining which will transforming in\nmany fields.This paper imparts more number of applications of the data mining\nand also focuses scope of the data mining which will helpful in the further\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 04:08:07 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Padhy", "Neelamadhab", ""], ["Mishra", "Dr. Pragnyaban", ""], ["Panigrahi", "Rasmita", ""]]}, {"id": "1211.5766", "submitter": "Abdelmalek Amine", "authors": "Reda Mohamed Hamou, Abdelmalek Amine, Ahmed Chaouki Lokbani, Michel\n  Simonet", "title": "Visualization and clustering by 3D cellular automata: Application to\n  unstructured data", "comments": "10 pages, 8 figures", "journal-ref": "International Journal Of Data Mining And Emerging Technologies.\n  2-1 (2012) 15-25", "doi": "10.5958/j.2249-3212.2.1.003", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the limited performance of 2D cellular automata in terms of space when\nthe number of documents increases and in terms of visualization clusters, our\nmotivation was to experiment these cellular automata by increasing the size to\nview the impact of size on quality of results. The representation of textual\ndata was carried out by a vector model whose components are derived from the\noverall balancing of the used corpus, Term Frequency Inverse Document Frequency\n(TF-IDF). The WorldNet thesaurus has been used to address the problem of the\nlemmatization of the words because the representation used in this study is\nthat of the bags of words. Another independent method of the language was used\nto represent textual records is that of the n-grams. Several measures of\nsimilarity have been tested. To validate the classification we have used two\nmeasures of assessment based on the recall and precision (f-measure and\nentropy). The results are promising and confirm the idea to increase the\ndimension to the problem of the spatiality of the classes. The results obtained\nin terms of purity class (i.e. the minimum value of entropy) shows that the\nnumber of documents over longer believes the results are better for 3D cellular\nautomata, which was not obvious to the 2D dimension. In terms of spatial\nnavigation, cellular automata provide very good 3D performance visualization\nthan 2D cellular automata.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2012 14:24:01 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Hamou", "Reda Mohamed", ""], ["Amine", "Abdelmalek", ""], ["Lokbani", "Ahmed Chaouki", ""], ["Simonet", "Michel", ""]]}, {"id": "1211.5877", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution, Shahrul Azman Noah", "title": "A Methodology to Extract Social Network from the Web Snippet", "comments": "7 pages, draft to conference: ICOCSIM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has been chosen as a basic infrastructure to gain the social\nstructure information, through the social network extraction, from all over the\nworld. However, most of the web documents are unstructured and lack of\nsemantics. Moreover, that network is subject to all kinds of changes and\ndynamics, and a network can be very complex due to the large number of nodes\nand links Web contains. In this paper, we discuss a methodology that meant to\nassists in extracting and modeling the social network from Web snippet. As the\nmanual social network extraction of web documents is impractical and\nunscalable, and fully automated extraction are still at the very early stage to\nbe implemented, we proposed a (semi)-automatic extraction based on the\nsuperficial methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 08:13:39 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""], ["Noah", "Shahrul Azman", ""]]}, {"id": "1211.5986", "submitter": "Rui Vilela-Mendes", "authors": "Carlos Aguirre and R. Vilela Mendes", "title": "Signal recognition and adapted filtering by non-commutative tomography", "comments": "19 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1107.0929", "journal-ref": "IET Signal Processing 8 (2014) 67 - 75", "doi": null, "report-no": null, "categories": "physics.data-an cs.IR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomograms, a generalization of the Radon transform to arbitrary pairs of\nnon-commuting operators, are positive bilinear transforms with a rigorous\nprobabilistic interpretation which provide a full characterization of the\nsignal and are robust in the presence of noise. Tomograms based on the\ntime-frequency operator pair, were used in the past for component separation\nand denoising. Here we show how, by the construction of an operator pair\nadapted to the signal, meaningful information with good time resolution is\nextracted even in very noisy situations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 15:27:42 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Aguirre", "Carlos", ""], ["Mendes", "R. Vilela", ""]]}, {"id": "1211.6159", "submitter": "Manuel Rojas", "authors": "Manuel Rojas", "title": "A semantic association page rank algorithm for web search engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of Semantic Web search engines retrieve information by focusing\non the use of concepts and relations restricted to the query provided by the\nuser. By trying to guess the implicit meaning between these concepts and\nrelations, probabilities are calculated to give the pages a score for ranking.\nIn this study, I propose a relation-based page rank algorithm to be used as a\nSemantic Web search engine. Relevance is measured as the probability of finding\nthe connections made by the user at the time of the query, as well as the\ninformation contained in the base knowledge of the Semantic Web environment. By\nthe use of \"virtual links\" between the concepts in a page, which are obtained\nfrom the knowledge base, we can connect concepts and components of a page and\nincrease the probability score for a better ranking. By creating these\nconnections, this study also looks to eliminate the possibility of getting\nresults equal to zero, and to provide a tie-breaker solution when two or more\npages obtain the same score.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 23:16:02 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Rojas", "Manuel", ""]]}, {"id": "1211.6166", "submitter": "Tao Zhu", "authors": "Tao Zhu, David Phipps, Adam Pridgen, Jedidiah R. Crandall, Dan S.\n  Wallach", "title": "Tracking and Quantifying Censorship on a Chinese Microblogging Site", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present measurements and analysis of censorship on Weibo, a popular\nmicroblogging site in China. Since we were limited in the rate at which we\ncould download posts, we identified users likely to participate in sensitive\ntopics and recursively followed their social contacts. We also leveraged new\nnatural language processing techniques to pick out trending topics despite the\nuse of neologisms, named entities, and informal language usage in Chinese\nsocial media. We found that Weibo dynamically adapts to the changing interests\nof its users through multiple layers of filtering. The filtering includes both\nretroactively searching posts by keyword or repost links to delete them, and\nrejecting posts as they are posted. The trend of sensitive topics is\nshort-lived, suggesting that the censorship is effective in stopping the\n\"viral\" spread of sensitive issues. We also give evidence that sensitive topics\nin Weibo only scarcely propagate beyond a core of sensitive posters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 23:54:27 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Zhu", "Tao", ""], ["Phipps", "David", ""], ["Pridgen", "Adam", ""], ["Crandall", "Jedidiah R.", ""], ["Wallach", "Dan S.", ""]]}, {"id": "1211.6321", "submitter": "Guo Zhang", "authors": "Guo Zhang, Ying Ding, Sta\\v{s}a Milojevi\\'c", "title": "Citation content analysis (cca): A framework for syntactic and semantic\n  analysis of citation content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.IT math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for Citation Content Analysis (CCA), for\nsyntactic and semantic analysis of citation content that can be used to better\nanalyze the rich sociocultural context of research behavior. The framework\ncould be considered the next generation of citation analysis. This paper\nbriefly reviews the history and features of content analysis in traditional\nsocial sciences, and its previous application in Library and Information\nScience. Based on critical discussion of the theoretical necessity of a new\nmethod as well as the limits of citation analysis, the nature and purposes of\nCCA are discussed, and potential procedures to conduct CCA, including\nprinciples to identify the reference scope, a two-dimensional (citing and\ncited) and two-modular (syntactic and semantic modules) codebook, are provided\nand described. Future works and implications are also suggested.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 15:07:43 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Zhang", "Guo", ""], ["Ding", "Ying", ""], ["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "1211.6799", "submitter": "Lilian Weng", "authors": "Lilian Weng and Filippo Menczer", "title": "Context Visualization for Social Bookmark Management", "comments": "11 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present the design of a new social bookmark manager, named GalViz, as part\nof the interface of the GiveA-Link system. Unlike the interfaces of traditional\nsocial tagging tools, which usually display information in a list view, GalViz\nvisualizes tags, resources, social links, and social context in an interactive\nnetwork, combined with the tag cloud. Evaluations through a scenario case study\nand log analysis provide evidence of the effectiveness of our design.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 02:34:28 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Weng", "Lilian", ""], ["Menczer", "Filippo", ""]]}, {"id": "1211.7133", "submitter": "Brian Thompson", "authors": "Graham Cormode, Qiang Ma, S. Muthukrishnan, Brian Thompson", "title": "Socializing the h-index", "comments": "5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of bibliometric measures have been proposed to quantify the impact\nof researchers and their work. The h-index is a notable and widely-used example\nwhich aims to improve over simple metrics such as raw counts of papers or\ncitations. However, a limitation of this measure is that it considers authors\nin isolation and does not account for contributions through a collaborative\nteam. To address this, we propose a natural variant that we dub the Social\nh-index. The idea is to redistribute the h-index score to reflect an\nindividual's impact on the research community. In addition to describing this\nnew measure, we provide examples, discuss its properties, and contrast with\nother measures.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 01:15:31 GMT"}, {"version": "v2", "created": "Tue, 7 May 2013 19:11:53 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Cormode", "Graham", ""], ["Ma", "Qiang", ""], ["Muthukrishnan", "S.", ""], ["Thompson", "Brian", ""]]}, {"id": "1211.7232", "submitter": "Giannis Haralabopoulos", "authors": "Giannis Haralabopoulos, Ioannis Anagnostopoulos", "title": "Real Time Enhanced Random Sampling of Online Social Networks", "comments": "16 Pages, 9 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social graphs can be easily extracted from Online Social Networks. However\nthese networks are getting larger from day to day. Sampling methods used to\nevaluate graph information cannot accurately extract graph properties.\nFurthermore Social Networks are limiting the access to their data, making the\ncrawling process even harder. A novel approach on Random Sampling is proposed,\nconsidering both limitation and resources. We evaluate this proposal with 4\ndifferent settings on 5 different Test Graphs, crawled directly from Twitter.\nThrough comparing the results we observe the pros and cons of its method as\nwell as their resource allocation. Concluding we present their best area of\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 12:54:01 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 19:00:26 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Haralabopoulos", "Giannis", ""], ["Anagnostopoulos", "Ioannis", ""]]}]