[{"id": "2002.00206", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Edgar Meij and Krisztian Balog and Ridho Reinanda", "title": "Novel Entity Discovery from Web Tables", "comments": "Proceedings of The Web Conference 2020 (WWW '20), 2020", "journal-ref": null, "doi": "10.1145/3366423.3380205", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with any sort of knowledge base (KB) one has to make sure it is\nas complete and also as up-to-date as possible. Both tasks are non-trivial as\nthey require recall-oriented efforts to determine which entities and\nrelationships are missing from the KB. As such they require a significant\namount of labor. Tables on the Web, on the other hand, are abundant and have\nthe distinct potential to assist with these tasks. In particular, we can\nleverage the content in such tables to discover new entities, properties, and\nrelationships. Because web tables typically only contain raw textual content we\nfirst need to determine which cells refer to which known entities---a task we\ndub table-to-KB matching. This first task aims to infer table semantics by\nlinking table cells and heading columns to elements of a KB. Then second task\nbuilds upon these linked entities and properties to not only identify novel\nones in the same table but also to bootstrap their type and additional\nrelationships. We refer to this process as novel entity discovery and, to the\nbest of our knowledge, it is the first endeavor on mining the unlinked cells in\nweb tables. Our method identifies not only out-of-KB (``novel'') information\nbut also novel aliases for in-KB (``known'') entities. When evaluated using\nthree purpose-built test collections, we find that our proposed approaches\nobtain a marked improvement in terms of precision over our baselines whilst\nkeeping recall stable.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 13:24:03 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Shuo", ""], ["Meij", "Edgar", ""], ["Balog", "Krisztian", ""], ["Reinanda", "Ridho", ""]]}, {"id": "2002.00207", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog", "title": "Web Table Extraction, Retrieval and Augmentation: A Survey", "comments": "ACM Transactions on Intelligent Systems and Technology. 11(2):\n  Article 13, January 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables are a powerful and popular tool for organizing and manipulating data.\nA vast number of tables can be found on the Web, which represents a valuable\nknowledge resource. The objective of this survey is to synthesize and present\ntwo decades of research on web tables. In particular, we organize existing\nliterature into six main categories of information access tasks: table\nextraction, table interpretation, table search, question answering, knowledge\nbase augmentation, and table augmentation. For each of these tasks, we identify\nand describe seminal approaches, present relevant resources, and point out\ninterdependencies among the different tasks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 13:25:43 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:46:58 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "2002.00251", "submitter": "Alexander Schindler", "authors": "Alexander Schindler", "title": "Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with\n  Visual Computing for Improved Music Video Analysis", "comments": "Dissertation at TU Wien", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis combines audio-analysis with computer vision to approach Music\nInformation Retrieval (MIR) tasks from a multi-modal perspective. This thesis\nfocuses on the information provided by the visual layer of music videos and how\nit can be harnessed to augment and improve tasks of the MIR research domain.\nThe main hypothesis of this work is based on the observation that certain\nexpressive categories such as genre or theme can be recognized on the basis of\nthe visual content alone, without the sound being heard. This leads to the\nhypothesis that there exists a visual language that is used to express mood or\ngenre. In a further consequence it can be concluded that this visual\ninformation is music related and thus should be beneficial for the\ncorresponding MIR tasks such as music genre classification or mood recognition.\nA series of comprehensive experiments and evaluations are conducted which are\nfocused on the extraction of visual information and its application in\ndifferent MIR tasks. A custom dataset is created, suitable to develop and test\nvisual features which are able to represent music related information.\nEvaluations range from low-level visual features to high-level concepts\nretrieved by means of Deep Convolutional Neural Networks. Additionally, new\nvisual features are introduced capturing rhythmic visual patterns. In all of\nthese experiments the audio-based results serve as benchmark for the visual and\naudio-visual approaches. The experiments are conducted for three MIR tasks\nArtist Identification, Music Genre Classification and Cross-Genre\nClassification. Experiments show that an audio-visual approach harnessing\nhigh-level semantic information gained from visual concept detection,\noutperforms audio-only genre-classification accuracy by 16.43%.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:57:14 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Schindler", "Alexander", ""]]}, {"id": "2002.00467", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman and Ilya Markov and Maarten de Rijke", "title": "Safe Exploration for Optimizing Contextual Bandits", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit problems are a natural fit for many information retrieval\ntasks, such as learning to rank, text classification, recommendation, etc.\nHowever, existing learning methods for contextual bandit problems have one of\ntwo drawbacks: they either do not explore the space of all possible document\nrankings (i.e., actions) and, thus, may miss the optimal ranking, or they\npresent suboptimal rankings to a user and, thus, may harm the user experience.\nWe introduce a new learning method for contextual bandit problems, Safe\nExploration Algorithm (SEA), which overcomes the above drawbacks. SEA starts by\nusing a baseline (or production) ranking system (i.e., policy), which does not\nharm the user experience and, thus, is safe to execute, but has suboptimal\nperformance and, thus, needs to be improved. Then SEA uses counterfactual\nlearning to learn a new policy based on the behavior of the baseline policy.\nSEA also uses high-confidence off-policy evaluation to estimate the performance\nof the newly learned policy. Once the performance of the newly learned policy\nis at least as good as the performance of the baseline policy, SEA starts using\nthe new policy to execute new actions, allowing it to actively explore\nfavorable regions of the action space. This way, SEA never performs worse than\nthe baseline policy and, thus, does not harm the user experience, while still\nexploring the action space and, thus, being able to find an optimal policy. Our\nexperiments using text classification and document retrieval confirm the above\nby comparing SEA (and a boundless variant called BSEA) to online and offline\nlearning methods for contextual bandit problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 19:18:22 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Jagerman", "Rolf", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2002.00571", "submitter": "Liu Yang", "authors": "Liu Yang, Minghui Qiu, Chen Qu, Cen Chen, Jiafeng Guo, Yongfeng Zhang,\n  W. Bruce Croft, Haiqing Chen", "title": "IART: Intent-aware Response Ranking with Transformers in\n  Information-seeking Conversation Systems", "comments": "Accepted by WWW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal assistant systems, such as Apple Siri, Google Assistant, Amazon\nAlexa, and Microsoft Cortana, are becoming ever more widely used. Understanding\nuser intent such as clarification questions, potential answers and user\nfeedback in information-seeking conversations is critical for retrieving good\nresponses. In this paper, we analyze user intent patterns in\ninformation-seeking conversations and propose an intent-aware neural response\nranking model \"IART\", which refers to \"Intent-Aware Ranking with Transformers\".\nIART is built on top of the integration of user intent modeling and language\nrepresentation learning with the Transformer architecture, which relies\nentirely on a self-attention mechanism instead of recurrent nets. It\nincorporates intent-aware utterance attention to derive an importance weighting\nscheme of utterances in conversation context with the aim of better\nconversation history understanding. We conduct extensive experiments with three\ninformation-seeking conversation data sets including both standard benchmarks\nand commercial data. Our proposed model outperforms all baseline methods with\nrespect to a variety of metrics. We also perform case studies and analysis of\nlearned user intent and its impact on response ranking in information-seeking\nconversations to provide interpretation of results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 05:59:52 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Yang", "Liu", ""], ["Qiu", "Minghui", ""], ["Qu", "Chen", ""], ["Chen", "Cen", ""], ["Guo", "Jiafeng", ""], ["Zhang", "Yongfeng", ""], ["Croft", "W. Bruce", ""], ["Chen", "Haiqing", ""]]}, {"id": "2002.00734", "submitter": "Andrew Krizhanovsky A", "authors": "A. Smirnov, T. Levashova, A. Karpov, I. Kipyatkova, A. Ronzhin, A.\n  Krizhanovsky, N. Krizhanovsky", "title": "Analysis of the quotation corpus of the Russian Wiktionary", "comments": "12 pages, 3 tables, 5 figures, published in the journal (preprint)", "journal-ref": "Research in Computing Science, Vol. 56, pp. 101-112, 2012", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The quantitative evaluation of quotations in the Russian Wiktionary was\nperformed using the developed Wiktionary parser. It was found that the number\nof quotations in the dictionary is growing fast (51.5 thousands in 2011, 62\nthousands in 2012). These quotations were extracted and saved in the relational\ndatabase of a machine-readable dictionary. For this database, tables related to\nthe quotations were designed. A histogram of distribution of quotations of\nliterary works written in different years was built. It was made an attempt to\nexplain the characteristics of the histogram by associating it with the years\nof the most popular and cited (in the Russian Wiktionary) writers of the\nnineteenth century. It was found that more than one-third of all the quotations\n(the example sentences) contained in the Russian Wiktionary are taken by the\neditors of a Wiktionary entry from the Russian National Corpus.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 12:30:17 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Smirnov", "A.", ""], ["Levashova", "T.", ""], ["Karpov", "A.", ""], ["Kipyatkova", "I.", ""], ["Ronzhin", "A.", ""], ["Krizhanovsky", "A.", ""], ["Krizhanovsky", "N.", ""]]}, {"id": "2002.00741", "submitter": "Jibang Wu", "authors": "Jibang Wu, Renqin Cai, Hongning Wang", "title": "D\\'ej\\`a vu: A Contextualized Temporal Attention Mechanism for\n  Sequential Recommendation", "comments": "Key Words: Sequential Recommendation, Self-attention mechanism,\n  Temporal Recommendation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting users' preferences based on their sequential behaviors in history\nis challenging and crucial for modern recommender systems. Most existing\nsequential recommendation algorithms focus on transitional structure among the\nsequential actions, but largely ignore the temporal and context information,\nwhen modeling the influence of a historical event to current prediction.\n  In this paper, we argue that the influence from the past events on a user's\ncurrent action should vary over the course of time and under different context.\nThus, we propose a Contextualized Temporal Attention Mechanism that learns to\nweigh historical actions' influence on not only what action it is, but also\nwhen and how the action took place. More specifically, to dynamically calibrate\nthe relative input dependence from the self-attention mechanism, we deploy\nmultiple parameterized kernel functions to learn various temporal dynamics, and\nthen use the context information to determine which of these reweighing kernels\nto follow for each input. In empirical evaluations on two large public\nrecommendation datasets, our model consistently outperformed an extensive set\nof state-of-the-art sequential recommendation methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 20:27:42 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Wu", "Jibang", ""], ["Cai", "Renqin", ""], ["Wang", "Hongning", ""]]}, {"id": "2002.00747", "submitter": "Maartje ter Hoeve", "authors": "Maartje ter Hoeve, Robert Sim, Elnaz Nouri, Adam Fourney, Maarten de\n  Rijke, Ryen W. White", "title": "Conversations with Documents. An Exploration of Document-Centered\n  Assistance", "comments": "Accepted as full paper at CHIIR 2020; 9 pages + Appendix", "journal-ref": null, "doi": "10.1145/3343413.3377971", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of conversational assistants has become more prevalent in helping\npeople increase their productivity. Document-centered assistance, for example\nto help an individual quickly review a document, has seen less significant\nprogress, even though it has the potential to tremendously increase a user's\nproductivity. This type of document-centered assistance is the focus of this\npaper. Our contributions are three-fold: (1) We first present a survey to\nunderstand the space of document-centered assistance and the capabilities\npeople expect in this scenario. (2) We investigate the types of queries that\nusers will pose while seeking assistance with documents, and show that\ndocument-centered questions form the majority of these queries. (3) We present\na set of initial machine learned models that show that (a) we can accurately\ndetect document-centered questions, and (b) we can build reasonably accurate\nmodels for answering such questions. These positive results are encouraging,\nand suggest that even greater results may be attained with continued study of\nthis interesting and novel problem space. Our findings have implications for\nthe design of intelligent systems to support task completion via natural\ninteractions with documents.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:10:11 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["ter Hoeve", "Maartje", ""], ["Sim", "Robert", ""], ["Nouri", "Elnaz", ""], ["Fourney", "Adam", ""], ["de Rijke", "Maarten", ""], ["White", "Ryen W.", ""]]}, {"id": "2002.00761", "submitter": "Ahmed El-Kishky", "authors": "Ahmed El-Kishky, Francisco Guzm\\'an", "title": "Massively Multilingual Document Alignment with Cross-lingual\n  Sentence-Mover's Distance", "comments": "In Proceedings of AACL-IJCNLP, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document alignment aims to identify pairs of documents in two distinct\nlanguages that are of comparable content or translations of each other. Such\naligned data can be used for a variety of NLP tasks from training cross-lingual\nrepresentations to mining parallel data for machine translation. In this paper\nwe develop an unsupervised scoring function that leverages cross-lingual\nsentence embeddings to compute the semantic distance between documents in\ndifferent languages. These semantic distances are then used to guide a document\nalignment algorithm to properly pair cross-lingual web documents across a\nvariety of low, mid, and high-resource language pairs. Recognizing that our\nproposed scoring function and other state of the art methods are\ncomputationally intractable for long web documents, we utilize a more tractable\ngreedy algorithm that performs comparably. We experimentally demonstrate that\nour distance metric performs better alignment than current baselines\noutperforming them by 7% on high-resource language pairs, 15% on mid-resource\nlanguage pairs, and 22% on low-resource language pairs.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 05:14:16 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 05:26:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["El-Kishky", "Ahmed", ""], ["Guzm\u00e1n", "Francisco", ""]]}, {"id": "2002.00844", "submitter": "Peijie Sun", "authors": "Le Wu, Junwei Li, Peijie Sun, Richang Hong, Yong Ge, Meng Wang", "title": "DiffNet++: A Neural Influence and Interest Diffusion Network for Social\n  Recommendation", "comments": "This paper has been accepted by IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommendation has emerged to leverage social connections among users\nfor predicting users' unknown preferences, which could alleviate the data\nsparsity issue in collaborative filtering based recommendation. Early\napproaches relied on utilizing each user's first-order social neighbors'\ninterests for better user modeling and failed to model the social influence\ndiffusion process from the global social network structure. Recently, we\npropose a preliminary work of a neural influence diffusion network (i.e.,\nDiffNet) for social recommendation (Diffnet), which models the recursive social\ndiffusion process to capture the higher-order relationships for each user.\nHowever, we argue that, as users play a central role in both user-user social\nnetwork and user-item interest network, only modeling the influence diffusion\nprocess in the social network would neglect the users' latent collaborative\ninterests in the user-item interest network. In this paper, we propose\nDiffNet++, an improved algorithm of DiffNet that models the neural influence\ndiffusion and interest diffusion in a unified framework. By reformulating the\nsocial recommendation as a heterogeneous graph with social network and interest\nnetwork as input, DiffNet++ advances DiffNet by injecting these two network\ninformation for user embedding learning at the same time. This is achieved by\niteratively aggregating each user's embedding from three aspects: the user's\nprevious embedding, the influence aggregation of social neighbors from the\nsocial network, and the interest aggregation of item neighbors from the\nuser-item interest network. Furthermore, we design a multi-level attention\nnetwork that learns how to attentively aggregate user embeddings from these\nthree aspects. Finally, extensive experimental results on two real-world\ndatasets clearly show the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 08:45:34 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 22:55:40 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 22:20:14 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 09:34:56 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Wu", "Le", ""], ["Li", "Junwei", ""], ["Sun", "Peijie", ""], ["Hong", "Richang", ""], ["Ge", "Yong", ""], ["Wang", "Meng", ""]]}, {"id": "2002.01071", "submitter": "Karam Abdulahhad", "authors": "Karam Abdulahhad", "title": "Concept Embedding for Information Retrieval", "comments": "6 pages", "journal-ref": null, "doi": "10.1007/978-3-319-76941-7_45", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts are used to solve the term-mismatch problem. However, we need an\neffective similarity measure between concepts. Word embedding presents a\npromising solution. We present in this study three approaches to build concepts\nvectors based on words vectors. We use a vector-based measure to estimate\ninter-concepts similarity. Our experiments show promising results. Furthermore,\nwords and concepts become comparable. This could be used to improve conceptual\nindexing process.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 09:18:56 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Abdulahhad", "Karam", ""]]}, {"id": "2002.01077", "submitter": "Sunshine Chong", "authors": "Sunshine Chong, Andr\\'es Abeliuk", "title": "Quantifying the Effects of Recommendation Systems", "comments": "8 pages, 6 figures, accepted into the National Symposium of IEEE Big\n  Data 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems today exert a strong influence on consumer behavior\nand individual perceptions of the world. By using collaborative filtering (CF)\nmethods to create recommendations, it generates a continuous feedback loop in\nwhich user behavior becomes magnified in the algorithmic system. Popular items\nget recommended more frequently, creating the bias that affects and alters user\npreferences. In order to visualize and compare the different biases, we will\nanalyze the effects of recommendation systems and quantify the inequalities\nresulting from them.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 01:21:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Chong", "Sunshine", ""], ["Abeliuk", "Andr\u00e9s", ""]]}, {"id": "2002.01264", "submitter": "Yu Zhou", "authors": "Yu Zhou, Xinying Yang, Taolue Chen, Zhiqiu Huang, Xiaoxing Ma, Harald\n  Gall", "title": "Boosting API Recommendation with Implicit Feedback", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers often need to use appropriate APIs to program efficiently, but it\nis usually a difficult task to identify the exact one they need from a vast of\ncandidates. To ease the burden, a multitude of API recommendation approaches\nhave been proposed. However, most of the currently available API recommenders\ndo not support the effective integration of users' feedback into the\nrecommendation loop. In this paper, we propose a framework, BRAID (Boosting\nRecommendAtion with Implicit FeeDback), which leverages learning-to-rank and\nactive learning techniques to boost recommendation performance. By exploiting\nusers' feedback information, we train a learning-to-rank model to re-rank the\nrecommendation results. In addition, we speed up the feedback learning process\nwith active learning. Existing query-based API recommendation approaches can be\nplugged into BRAID. We select three state-of-the-art API recommendation\napproaches as baselines to demonstrate the performance enhancement of BRAID\nmeasured by Hit@k (Top-k), MAP, and MRR. Empirical experiments show that, with\nacceptable overheads, the recommendation performance improves steadily and\nsubstantially with the increasing percentage of feedback data, comparing with\nthe baselines.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 12:51:59 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 05:26:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhou", "Yu", ""], ["Yang", "Xinying", ""], ["Chen", "Taolue", ""], ["Huang", "Zhiqiu", ""], ["Ma", "Xiaoxing", ""], ["Gall", "Harald", ""]]}, {"id": "2002.01447", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "A Prototype of Serverless Lucene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a working prototype that adapts Lucene, the world's most\npopular and most widely deployed open-source search library, to operate within\na serverless environment in the cloud. Although the serverless search concept\nis not new, this work represents a substantial improvement over a previous\nimplementation in eliminating most custom code and in enabling interactive\nsearch. While there remain limitations to the design, it nevertheless\nchallenges conventional thinking about search architectures for particular\noperating points.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:20:06 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "2002.01554", "submitter": "Miklas S. Kristoffersen", "authors": "Miklas S. Kristoffersen, Sven E. Shepstone, Zheng-Hua Tan", "title": "Context-Aware Recommendations for Televisions Using Deep Embeddings with\n  Relaxed N-Pairs Loss Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies context-aware recommendations in the television domain by\nproposing a deep learning-based method for learning joint context-content\nembeddings (JCCE). The method builds on recent developments within\nrecommendations using latent representations and deep metric learning, in order\nto effectively represent contextual settings of viewing situations as well as\navailable content in a shared latent space. This embedding space is used for\nexploring relevant content in various viewing settings by applying an N -pairs\nloss objective as well as a relaxed variant introduced in this paper.\nExperiments on two datasets confirm the recommendation ability of JCCE,\nachieving improvements when compared to state-of-the-art methods. Further\nexperiments display useful structures in the learned embeddings that can be\nused to gain valuable knowledge of underlying variables in the relationship\nbetween contextual settings and content properties.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 21:51:26 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "2002.01726", "submitter": "Qi Yang", "authors": "Qi Yang, Aleksandr Farseev, Andrey Filchenkov", "title": "I Know Where You Are Coming From: On the Impact of Social Media Sources\n  on AI Model Performance", "comments": "AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, social networks play a crucial role in human everyday life and no\nlonger purely associated with spare time spending. In fact, instant\ncommunication with friends and colleagues has become an essential component of\nour daily interaction giving a raise of multiple new social network types\nemergence. By participating in such networks, individuals generate a multitude\nof data points that describe their activities from different perspectives and,\nfor example, can be further used for applications such as personalized\nrecommendation or user profiling. However, the impact of the different social\nmedia networks on machine learning model performance has not been studied\ncomprehensively yet. Particularly, the literature on modeling multi-modal data\nfrom multiple social networks is relatively sparse, which had inspired us to\ntake a deeper dive into the topic in this preliminary study. Specifically, in\nthis work, we will study the performance of different machine learning models\nwhen being learned on multi-modal data from different social networks. Our\ninitial experimental results reveal that social network choice impacts the\nperformance and the proper selection of data source is crucial.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 11:10:44 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Yang", "Qi", ""], ["Farseev", "Aleksandr", ""], ["Filchenkov", "Andrey", ""]]}, {"id": "2002.01792", "submitter": "Hardik Joshi Mr.", "authors": "Dr. Jyoti Pareek, Hardik Joshi, Krunal Chauhan, Rushikesh Patel", "title": "Experiments with Different Indexing Techniques for Text Retrieval tasks\n  on Gujarati Language using Bag of Words Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents results of various experiments carried out to improve\ntext retrieval of gujarati text documents. Text retrieval involves searching\nand ranking of text documents for a given set of query terms. We have tested\nvarious retrieval models that uses bag-of-words approach. Bag-of-words approach\nis a traditional approach that is being used till date where the text document\nis represented as collection of words. Measures like frequency count, inverse\ndocument frequency etc. are used to signify and rank relevant documents for\nuser queries. Different ranking models have been used to quantify ranking\nperformance using the metric of mean average precision. Gujarati is a\nmorphologically rich language, we have compared techniques like stop word\nremoval, stemming and frequent case generation against baseline to measure the\nimprovements in information retrieval tasks. Most of the techniques are\nlanguage dependent and requires development of language specific tools. We used\nplain unprocessed word index as the baseline, we have seen significant\nimprovements in comparison of MAP values after applying different indexing\ntechniques when compared to the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:58:29 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Pareek", "Dr. Jyoti", ""], ["Joshi", "Hardik", ""], ["Chauhan", "Krunal", ""], ["Patel", "Rushikesh", ""]]}, {"id": "2002.01793", "submitter": "Yacov Hel-Or", "authors": "Inbal Lav, Shai Avidan, Yoram Singer, Yacov Hel-Or", "title": "Proximity Preserving Binary Code using Signed Graph-Cut", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence , Feb. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a binary embedding framework, called Proximity Preserving Code\n(PPC), which learns similarity and dissimilarity between data points to create\na compact and affinity-preserving binary code. This code can be used to apply\nfast and memory-efficient approximation to nearest-neighbor searches. Our\nframework is flexible, enabling different proximity definitions between data\npoints. In contrast to previous methods that extract binary codes based on\nunsigned graph partitioning, our system models the attractive and repulsive\nforces in the data by incorporating positive and negative graph weights. The\nproposed framework is shown to boil down to finding the minimal cut of a signed\ngraph, a problem known to be NP-hard. We offer an efficient approximation and\nachieve superior results by constructing the code bit after bit. We show that\nthe proposed approximation is superior to the commonly used spectral methods\nwith respect to both accuracy and complexity. Thus, it is useful for many other\nproblems that can be translated into signed graph cut.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 13:58:41 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Lav", "Inbal", ""], ["Avidan", "Shai", ""], ["Singer", "Yoram", ""], ["Hel-Or", "Yacov", ""]]}, {"id": "2002.01846", "submitter": "Elad Kravi Mr.", "authors": "Elad Kravi, Benny Kimelfeld, Yaron Kanza, Roi Reichart", "title": "Geosocial Location Classification: Associating Type to Places Based on\n  Geotagged Social-Media Posts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associating type to locations can be used to enrich maps and can serve a\nplethora of geospatial applications. An automatic method to do so could make\nthe process less expensive in terms of human labor, and faster to react to\nchanges. In this paper we study the problem of Geosocial Location\nClassification, where the type of a site, e.g., a building, is discovered based\non social-media posts. Our goal is to correctly associate a set of messages\nposted in a small radius around a given location with the corresponding\nlocation type, e.g., school, church, restaurant or museum. We explore two\napproaches to the problem: (a) a pipeline approach, where each message is first\nclassified, and then the location associated with the message set is inferred\nfrom the individual message labels; and (b) a joint approach where the\nindividual messages are simultaneously processed to yield the desired location\ntype. We tested the two approaches over a dataset of geotagged tweets. Our\nresults demonstrate the superiority of the joint approach. Moreover, we show\nthat due to the unique structure of the problem, where weakly-related messages\nare jointly processed to yield a single final label, linear classifiers\noutperform deep neural network alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 16:09:52 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 11:16:46 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Kravi", "Elad", ""], ["Kimelfeld", "Benny", ""], ["Kanza", "Yaron", ""], ["Reichart", "Roi", ""]]}, {"id": "2002.01854", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Markus Zlabinger, Allan Hanbury", "title": "Interpretable & Time-Budget-Constrained Contextualization for Re-Ranking", "comments": "Accepted at ECAI 2020 (full paper). arXiv admin note: text overlap\n  with arXiv:1912.01385", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines operate under a strict time constraint as a fast response is\nparamount to user satisfaction. Thus, neural re-ranking models have a limited\ntime-budget to re-rank documents. Given the same amount of time, a faster\nre-ranking model can incorporate more documents than a less efficient one,\nleading to a higher effectiveness. To utilize this property, we propose TK\n(Transformer-Kernel): a neural re-ranking model for ad-hoc search using an\nefficient contextualization mechanism. TK employs a very small number of\nTransformer layers (up to three) to contextualize query and document word\nembeddings. To score individual term interactions, we use a document-length\nenhanced kernel-pooling, which enables users to gain insight into the model. TK\noffers an optimal ratio between effectiveness and efficiency: under realistic\ntime constraints (max. 200 ms per query) TK achieves the highest effectiveness\nin comparison to BERT and other re-ranking models. We demonstrate this on three\nlarge-scale ranking collections: MSMARCO-Passage, MSMARCO-Document, and TREC\nCAR. In addition, to gain insight into TK, we perform a clustered query\nanalysis of TK's results, highlighting its strengths and weaknesses on queries\nwith different types of information need and we show how to interpret the cause\nof ranking differences of two documents by comparing their internal scores.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:42:54 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "2002.01861", "submitter": "Jimmy Lin", "authors": "Ruixue Zhang, Wei Yang, Luyun Lin, Zhengkai Tu, Yuqing Xie, Zihang Fu,\n  Yuhao Xie, Luchen Tan, Kun Xiong, Jimmy Lin", "title": "Rapid Adaptation of BERT for Information Extraction on Domain-Specific\n  Business Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques for automatically extracting important content elements from\nbusiness documents such as contracts, statements, and filings have the\npotential to make business operations more efficient. This problem can be\nformulated as a sequence labeling task, and we demonstrate the adaption of BERT\nto two types of business documents: regulatory filings and property lease\nagreements. There are aspects of this problem that make it easier than\n\"standard\" information extraction tasks and other aspects that make it more\ndifficult, but on balance we find that modest amounts of annotated data (less\nthan 100 documents) are sufficient to achieve reasonable accuracy. We integrate\nour models into an end-to-end cloud platform that provides both an easy-to-use\nannotation interface as well as an inference interface that allows users to\nupload documents and inspect model outputs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 16:45:44 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Ruixue", ""], ["Yang", "Wei", ""], ["Lin", "Luyun", ""], ["Tu", "Zhengkai", ""], ["Xie", "Yuqing", ""], ["Fu", "Zihang", ""], ["Xie", "Yuhao", ""], ["Tan", "Luchen", ""], ["Xiong", "Kun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2002.01984", "submitter": "Wlodek Zadrozny", "authors": "Sai Krishna Telukuntla, Aditya Kapri and Wlodek Zadrozny", "title": "UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B,\n  Phase-B", "comments": "28 pages, 8 figures. This is an expanded version of our submission to\n  2019 BioAsq competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we detail our submission to the 2019, 7th year, BioASQ\ncompetition. We present our approach for Task-7b, Phase B, Exact Answering\nTask. These Question Answering (QA) tasks include Factoid, Yes/No, List Type\nQuestion answering. Our system is based on a contextual word embedding model.\nWe have used a Bidirectional Encoder Representations from Transformers(BERT)\nbased system, fined tuned for biomedical question answering task using BioBERT.\nIn the third test batch set, our system achieved the highest MRR score for\nFactoid Question Answering task. Also, for List type question answering task\nour system achieved the highest recall score in the fourth test batch set.\nAlong with our detailed approach, we present the results for our submissions,\nand also highlight identified downsides for our current approach and ways to\nimprove them in our future experiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:43:14 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Telukuntla", "Sai Krishna", ""], ["Kapri", "Aditya", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "2002.02070", "submitter": "Habeeb Hooshmand", "authors": "Habeeb Hooshmand, James Caverlee", "title": "Understanding Car-Speak: Replacing Humans in Dealerships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large portion of the car-buying experience in the United States involves\ninteractions at a car dealership. At the dealership, the car-buyer relays their\nneeds to a sales representative. However, most car-buyers are only have an\nabstract description of the vehicle they need. Therefore, they are only able to\ndescribe their ideal car in \"car-speak\". Car-speak is abstract language that\npertains to a car's physical attributes. In this paper, we define car-speak. We\nalso aim to curate a reasonable data set of car-speak language. Finally, we\ntrain several classifiers in order to classify car-speak.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 02:10:33 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Hooshmand", "Habeeb", ""], ["Caverlee", "James", ""]]}, {"id": "2002.02126", "submitter": "Kuan Deng", "authors": "Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang and Meng\n  Wang", "title": "LightGCN: Simplifying and Powering Graph Convolution Network for\n  Recommendation", "comments": "Accepted by SIGIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolution Network (GCN) has become new state-of-the-art for\ncollaborative filtering. Nevertheless, the reasons of its effectiveness for\nrecommendation are not well understood. Existing work that adapts GCN to\nrecommendation lacks thorough ablation analyses on GCN, which is originally\ndesigned for graph classification tasks and equipped with many neural network\noperations. However, we empirically find that the two most common designs in\nGCNs -- feature transformation and nonlinear activation -- contribute little to\nthe performance of collaborative filtering. Even worse, including them adds to\nthe difficulty of training and degrades recommendation performance.\n  In this work, we aim to simplify the design of GCN to make it more concise\nand appropriate for recommendation. We propose a new model named LightGCN,\nincluding only the most essential component in GCN -- neighborhood aggregation\n-- for collaborative filtering. Specifically, LightGCN learns user and item\nembeddings by linearly propagating them on the user-item interaction graph, and\nuses the weighted sum of the embeddings learned at all layers as the final\nembedding. Such simple, linear, and neat model is much easier to implement and\ntrain, exhibiting substantial improvements (about 16.0\\% relative improvement\non average) over Neural Graph Collaborative Filtering (NGCF) -- a\nstate-of-the-art GCN-based recommender model -- under exactly the same\nexperimental setting. Further analyses are provided towards the rationality of\nthe simple LightGCN from both analytical and empirical perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 06:53:42 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:42:54 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 11:07:46 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 04:20:53 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["He", "Xiangnan", ""], ["Deng", "Kuan", ""], ["Wang", "Xiang", ""], ["Li", "Yan", ""], ["Zhang", "Yongdong", ""], ["Wang", "Meng", ""]]}, {"id": "2002.02333", "submitter": "Li Weng", "authors": "Li Weng, Lingzhi Ye, Jiangmin Tian, Jiuwen Cao, and Jianzhong Wang", "title": "Random VLAD based Deep Hashing for Efficient Image Retrieval", "comments": "10 pages, 17 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hash algorithms generate compact binary representations that can be\nquickly matched by Hamming distance, thus become an efficient solution for\nlarge-scale image retrieval. This paper proposes RV-SSDH, a deep image hash\nalgorithm that incorporates the classical VLAD (vector of locally aggregated\ndescriptors) architecture into neural networks. Specifically, a novel neural\nnetwork component is formed by coupling a random VLAD layer with a latent hash\nlayer through a transform layer. This component can be combined with\nconvolutional layers to realize a hash algorithm. We implement RV-SSDH as a\npoint-wise algorithm that can be efficiently trained by minimizing\nclassification error and quantization loss. Comprehensive experiments show this\nnew architecture significantly outperforms baselines such as NetVLAD and SSDH,\nand offers a cost-effective trade-off in the state-of-the-art. In addition, the\nproposed random VLAD layer leads to satisfactory accuracy with low complexity,\nthus shows promising potentials as an alternative to NetVLAD.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:22:44 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Weng", "Li", ""], ["Ye", "Lingzhi", ""], ["Tian", "Jiangmin", ""], ["Cao", "Jiuwen", ""], ["Wang", "Jianzhong", ""]]}, {"id": "2002.02375", "submitter": "Andre Lamurias", "authors": "Andre Lamurias, Diana Sousa and Francisco M. Couto", "title": "Generating Biomedical Question Answering Corpora from Q&A forums", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3020868", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is a natural language processing task that aims at\nobtaining relevant answers to user questions. While some progress has been made\nin this area, biomedical questions are still a challenge to most QA approaches,\ndue to the complexity of the domain and limited availability of training sets.\nWe present a method to automatically extract question-article pairs from Q\\&A\nweb forums, which can be used for document retrieval, a crucial step of most QA\nsystems. The proposed framework extracts from selected forums the questions and\nthe respective answers that contain citations. This way, QA systems based on\ndocument retrieval can be developed and evaluated using the question-article\npairs annotated by users of these forums. We generated the BiQA corpus by\napplying our framework to three forums, obtaining 7,453 questions and 14,239\nquestion-article pairs. We evaluated how the number of articles associated with\neach question and the number of votes on each answer affects the performance of\nbaseline document retrieval approaches. Also, we demonstrated that the articles\ngiven as answers are significantly similar to the questions and trained a\nstate-of-the-art deep learning model that obtained similar performance to using\na dataset manually annotated by experts. The proposed framework can be used to\nupdate the BiQA corpus from the same forums as new posts are made, and from\nother forums that support their answers with documents. The BiQA corpus and the\nframework used to generate it are available at\n\\url{https://github.com/lasigeBioTM/BiQA}.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:23:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 10:45:27 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Lamurias", "Andre", ""], ["Sousa", "Diana", ""], ["Couto", "Francisco M.", ""]]}, {"id": "2002.02557", "submitter": "Amila Silva", "authors": "Amila Silva and Pei-Chi Lo and Ee-Peng Lim", "title": "JPLink: On Linking Jobs to Vocational Interest Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking job seekers with relevant jobs requires matching based on not only\nskills, but also personality types. Although the Holland Code also known as\nRIASEC has frequently been used to group people by their suitability for six\ndifferent categories of occupations, the RIASEC category labels of individual\njobs are often not found in job posts. This is attributed to significant manual\nefforts required for assigning job posts with RIASEC labels. To cope with\nassigning massive number of jobs with RIASEC labels, we propose JPLink, a\nmachine learning approach using the text content in job titles and job\ndescriptions. JPLink exploits domain knowledge available in an\noccupation-specific knowledge base known as O*NET to improve feature\nrepresentation of job posts. To incorporate relative ranking of RIASEC labels\nof each job, JPLink proposes a listwise loss function inspired by learning to\nrank. Both our quantitative and qualitative evaluations show that JPLink\noutperforms conventional baselines. We conduct an error analysis on JPLink's\npredictions to show that it can uncover label errors in existing job posts.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 23:56:46 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Silva", "Amila", ""], ["Lo", "Pei-Chi", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2002.02712", "submitter": "Moritz Schubotz", "authors": "Andre Greiner-Petter, Moritz Schubotz, Fabian Mueller, Corinna\n  Breitinger, Howard S. Cohl, Akiko Aizawa, Bela Gipp", "title": "Discovering Mathematical Objects of Interest -- A Study of Mathematical\n  Notations", "comments": "Proceedings of The Web Conference 2020 (WWW'20), April 20--24, 2020,\n  Taipei, Taiwan", "journal-ref": null, "doi": "10.1145/3366423.3380218", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical notation, i.e., the writing system used to communicate concepts\nin mathematics, encodes valuable information for a variety of information\nsearch and retrieval systems. Yet, mathematical notations remain mostly\nunutilized by today's systems. In this paper, we present the first in-depth\nstudy on the distributions of mathematical notation in two large scientific\ncorpora: the open access arXiv (2.5B mathematical objects) and the mathematical\nreviewing service for pure and applied mathematics zbMATH (61M mathematical\nobjects). Our study lays a foundation for future research projects on\nmathematical information retrieval for large scientific corpora. Further, we\ndemonstrate the relevance of our results to a variety of use-cases. For\nexample, to assist semantic extraction systems, to improve scientific search\nengines, and to facilitate specialized math recommendation systems. The\ncontributions of our presented research are as follows: (1) we present the\nfirst distributional analysis of mathematical formulae on arXiv and zbMATH; (2)\nwe retrieve relevant mathematical objects for given textual search queries\n(e.g., linking $P_{n}^{(\\alpha, \\beta)}\\!\\left(x\\right)$ with `Jacobi\npolynomial'); (3) we extend zbMATH's search engine by providing relevant\nmathematical formulae; and (4) we exemplify the applicability of the results by\npresenting auto-completion for math inputs as the first contribution to math\nrecommendation systems. To expedite future research projects, we have made\navailable our source code and data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:57:00 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 18:20:13 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 10:57:45 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Greiner-Petter", "Andre", ""], ["Schubotz", "Moritz", ""], ["Mueller", "Fabian", ""], ["Breitinger", "Corinna", ""], ["Cohl", "Howard S.", ""], ["Aizawa", "Akiko", ""], ["Gipp", "Bela", ""]]}, {"id": "2002.02755", "submitter": "Shubham Vatsal", "authors": "Shubham Vatsal, Naresh Purre, Sukumar Moharana, Gopi Ramena, Debi\n  Prasanna Mohanty", "title": "On-Device Information Extraction from SMS using Hybrid Hierarchical\n  Classification", "comments": "to be published in IEEE ICSC 2020 proceedings", "journal-ref": null, "doi": "10.1109/ICSC.2020.00036", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cluttering of SMS inbox is one of the serious problems that users today face\nin the digital world where every online login, transaction, along with\npromotions generate multiple SMS. This problem not only prevents users from\nsearching and navigating messages efficiently but often results in users\nmissing out the relevant information associated with the corresponding SMS like\noffer codes, payment reminders etc. In this paper, we propose a unique\narchitecture to organize and extract the appropriate information from SMS and\nfurther display it in an intuitive template. In the proposed architecture, we\nuse a Hybrid Hierarchical Long Short Term Memory (LSTM)-Convolutional Neural\nNetwork (CNN) to categorize SMS into multiple classes followed by a set of\nentity parsers used to extract the relevant information from the classified\nmessage. The architecture using its preprocessing techniques not only takes\ninto account the enormous variations observed in SMS data but also makes it\nefficient for its on-device (mobile phone) functionalities in terms of\ninference timing and size.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:24:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Vatsal", "Shubham", ""], ["Purre", "Naresh", ""], ["Moharana", "Sukumar", ""], ["Ramena", "Gopi", ""], ["Mohanty", "Debi Prasanna", ""]]}, {"id": "2002.02775", "submitter": "Yingcheng Sun", "authors": "Yingcheng Sun and Kenneth Loparo", "title": "Context Aware Image Annotation in Active Learning", "comments": "arXiv admin note: text overlap with arXiv:1508.07647, arXiv:1207.3809\n  by other authors", "journal-ref": "2019 19th Industrial Conference on Data Mining", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image annotation for active learning is labor-intensive. Various automatic\nand semi-automatic labeling methods are proposed to save the labeling cost, but\na reduction in the number of labeled instances does not guarantee a reduction\nin cost because the queries that are most valuable to the learner may be the\nmost difficult or ambiguous cases, and therefore the most expensive for an\noracle to label accurately. In this paper, we try to solve this problem by\nusing image metadata to offer the oracle more clues about the image during\nannotation process. We propose a Context Aware Image Annotation Framework\n(CAIAF) that uses image metadata as similarity metric to cluster images into\ngroups for annotation. We also present useful metadata information as context\nfor each image on the annotation interface. Experiments show that it reduces\nthat annotation cost with CAIAF compared to the conventional framework, while\nmaintaining a high classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:57:05 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Sun", "Yingcheng", ""], ["Loparo", "Kenneth", ""]]}, {"id": "2002.02879", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and Pranjul Yadav and Khoa Doan and S. Sathiya\n  Keerthi", "title": "Targeted display advertising: the case of preferential attachment", "comments": "IEEE BigData 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An average adult is exposed to hundreds of digital advertisements daily\n(https://www.mediadynamicsinc.com/uploads/files/PR092214-Note-only-150-Ads-2mk.pdf),\nmaking the digital advertisement industry a classic example of a\nbig-data-driven platform. As such, the ad-tech industry relies on historical\nengagement logs (clicks or purchases) to identify potentially interested users\nfor the advertisement campaign of a partner (a seller who wants to target users\nfor its products). The number of advertisements that are shown for a partner,\nand hence the historical campaign data available for a partner depends upon the\nbudget constraints of the partner. Thus, enough data can be collected for the\nhigh-budget partners to make accurate predictions, while this is not the case\nwith the low-budget partners. This skewed distribution of the data leads to\n\"preferential attachment\" of the targeted display advertising platforms towards\nthe high-budget partners. In this paper, we develop \"domain-adaptation\"\napproaches to address the challenge of predicting interested users for the\npartners with insufficient data, i.e., the tail partners. Specifically, we\ndevelop simple yet effective approaches that leverage the similarity among the\npartners to transfer information from the partners with sufficient data to\ncold-start partners, i.e., partners without any campaign data. Our approaches\nreadily adapt to the new campaign data by incremental fine-tuning, and hence\nwork at varying points of a campaign, and not just the cold-start. We present\nan experimental analysis on the historical logs of a major display advertising\nplatform (https://www.criteo.com/). Specifically, we evaluate our approaches\nacross 149 partners, at varying points of their campaigns. Experimental results\nshow that the proposed approaches outperform the other \"domain-adaptation\"\napproaches at different time points of the campaigns.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:23:17 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Manchanda", "Saurav", ""], ["Yadav", "Pranjul", ""], ["Doan", "Khoa", ""], ["Keerthi", "S. Sathiya", ""]]}, {"id": "2002.03124", "submitter": "Diego Monti", "authors": "Andrea Fiandro, Giorgio Crepaldi, Diego Monti, Giuseppe Rizzo and\n  Maurizio Morisio", "title": "Predict your Click-out: Modeling User-Item Interactions and Session\n  Actions in an Ensemble Learning Fashion", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the solution of the POLINKS team to the RecSys Challenge\n2019 that focuses on the task of predicting the last click-out in a\nsession-based interaction. We propose an ensemble approach comprising a matrix\nfactorization for modeling the interaction user-item, and a session-aware\nlearning model implemented with a recurrent neural network. This method appears\nto be effective in predicting the last click-out scoring a 0.60277 of Mean\nReciprocal Rank on the local test set.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 09:04:32 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Fiandro", "Andrea", ""], ["Crepaldi", "Giorgio", ""], ["Monti", "Diego", ""], ["Rizzo", "Giuseppe", ""], ["Morisio", "Maurizio", ""]]}, {"id": "2002.03182", "submitter": "Zafaryab Rasool", "authors": "Zafaryab Rasool, Rui Zhou, Lu Chen, Chengfei Liu, Jiajie Xu", "title": "Index-based Solutions for Efficient Density Peak Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density Peak Clustering (DPC), a popular density-based clustering approach,\nhas received considerable attention from the research community primarily due\nto its simplicity and fewer-parameter requirement. However, the resultant\nclusters obtained using DPC are influenced by the sensitive parameter $d_c$,\nwhich depends on data distribution and requirements of different users.\nBesides, the original DPC algorithm requires visiting a large number of\nobjects, making it slow. To this end, this paper investigates index-based\nsolutions for DPC. Specifically, we propose two list-based index methods viz.\n(i) a simple List Index, and (ii) an advanced Cumulative Histogram Index.\nEfficient query algorithms are proposed for these indices which significantly\navoids irrelevant comparisons at the cost of space. For memory-constrained\nsystems, we further introduce an approximate solution to the above indices\nwhich allows substantial reduction in the space cost, provided that slight\ninaccuracies are admissible. Furthermore, owing to considerably lower memory\nrequirements of existing tree-based index structures, we also present effective\npruning techniques and efficient query algorithms to support DPC using the\npopular Quadtree Index and R-tree Index. Finally, we practically evaluate all\nthe above indices and present the findings and results, obtained from a set of\nextensive experiments on six synthetic and real datasets. The experimental\ninsights obtained can help to guide in selecting a befitting index.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 15:22:37 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:08:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Rasool", "Zafaryab", ""], ["Zhou", "Rui", ""], ["Chen", "Lu", ""], ["Liu", "Chengfei", ""], ["Xu", "Jiajie", ""]]}, {"id": "2002.03203", "submitter": "Yingcheng Sun", "authors": "Yingcheng Sun and Richard Kolacinski and Kenneth Loparo", "title": "Eliminating Search Intent Bias in Learning to Rank", "comments": null, "journal-ref": "2020 IEEE 14th International Conference on Semantic Computing\n  (ICSC)", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through data has proven to be a valuable resource for improving\nsearch-ranking quality. Search engines can easily collect click data, but\nbiases introduced in the data can make it difficult to use the data\neffectively. In order to measure the effects of biases, many click models have\nbeen proposed in the literature. However, none of the models can explain the\nobservation that users with different search intent (e.g., informational,\nnavigational, etc.) have different click behaviors. In this paper, we study how\ndifferences in user search intent can influence click activities and determined\nthat there exists a bias between user search intent and the relevance of the\ndocument relevance. Based on this observation, we propose a search intent bias\nhypothesis that can be applied to most existing click models to improve their\nability to learn unbiased relevance. Experimental results demonstrate that\nafter adopting the search intent hypothesis, click models can better interpret\nuser clicks and substantially improve retrieval performance.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 17:07:37 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 23:11:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sun", "Yingcheng", ""], ["Kolacinski", "Richard", ""], ["Loparo", "Kenneth", ""]]}, {"id": "2002.03222", "submitter": "Yue Zhao", "authors": "Yue Zhao and Xueying Ding and Jianing Yang and Haoping Bai", "title": "SUOD: Toward Scalable Unsupervised Outlier Detection", "comments": "In AAAI-20 Workshop on Artificial Intelligence for Cyber Security\n  (AICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a key field of machine learning for identifying abnormal\ndata objects. Due to the high expense of acquiring ground truth, unsupervised\nmodels are often chosen in practice. To compensate for the unstable nature of\nunsupervised algorithms, practitioners from high-stakes fields like finance,\nhealth, and security, prefer to build a large number of models for further\ncombination and analysis. However, this poses scalability challenges in\nhigh-dimensional large datasets. In this study, we propose a three-module\nacceleration framework called SUOD to expedite the training and prediction with\na large number of unsupervised detection models. SUOD's Random Projection\nmodule can generate lower subspaces for high-dimensional datasets while\nreserving their distance relationship. Balanced Parallel Scheduling module can\nforecast the training and prediction cost of models with high confidence---so\nthe task scheduler could assign nearly equal amount of taskload among workers\nfor efficient parallelization. SUOD also comes with a Pseudo-supervised\nApproximation module, which can approximate fitted unsupervised models by lower\ntime complexity supervised regressors for fast prediction on unseen data. It\nmay be considered as an unsupervised model knowledge distillation process.\nNotably, all three modules are independent with great flexibility to \"mix and\nmatch\"; a combination of modules can be chosen based on use cases. Extensive\nexperiments on more than 30 benchmark datasets have shown the efficacy of SUOD,\nand a comprehensive future development plan is also presented.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 19:38:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhao", "Yue", ""], ["Ding", "Xueying", ""], ["Yang", "Jianing", ""], ["Bai", "Haoping", ""]]}, {"id": "2002.03259", "submitter": "Nidhika Yadav", "authors": "Nidhika Yadav, Niladri Chatterjee", "title": "Rough Set based Aggregate Rank Measure & its Application to Supervised\n  Multi Document Summarization", "comments": "The paper proposes a novel Rough Set based technique to compute rank\n  in a decision system. This is further evaluated on the problem of Supervised\n  Text Summarization. The paper contains 9 pages, illustrative examples,\n  theoretical properties, and experimental evaluations on standard datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most problems in Machine Learning cater to classification and the objects of\nuniverse are classified to a relevant class. Ranking of classified objects of\nuniverse per decision class is a challenging problem. We in this paper propose\na novel Rough Set based membership called Rank Measure to solve to this\nproblem. It shall be utilized for ranking the elements to a particular class.\nIt differs from Pawlak Rough Set based membership function which gives an\nequivalent characterization of the Rough Set based approximations. It becomes\nparamount to look beyond the traditional approach of computing memberships\nwhile handling inconsistent, erroneous and missing data that is typically\npresent in real world problems. This led us to propose the aggregate Rank\nMeasure. The contribution of the paper is three fold. Firstly, it proposes a\nRough Set based measure to be utilized for numerical characterization of within\nclass ranking of objects. Secondly, it proposes and establish the properties of\nRank Measure and aggregate Rank Measure based membership. Thirdly, we apply the\nconcept of membership and aggregate ranking to the problem of supervised Multi\nDocument Summarization wherein first the important class of sentences are\ndetermined using various supervised learning techniques and are post processed\nusing the proposed ranking measure. The results proved to have significant\nimprovement in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 01:03:25 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yadav", "Nidhika", ""], ["Chatterjee", "Niladri", ""]]}, {"id": "2002.03461", "submitter": "Piotr Koniusz", "authors": "Xianjing Wang, Flora D. Salim, Yongli Ren, Piotr Koniusz", "title": "Relation Embedding for Personalised POI Recommendation", "comments": "12 pages, 3 figures, Accepted in the 24th Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-47426-3_5", "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-Interest (POI) recommendation is one of the most important\nlocation-based services helping people discover interesting venues or services.\nHowever, the extreme user-POI matrix sparsity and the varying spatio-temporal\ncontext pose challenges for POI systems, which affects the quality of POI\nrecommendations. To this end, we propose a translation-based relation embedding\nfor POI recommendation. Our approach encodes the temporal and geographic\ninformation, as well as semantic contents effectively in a low-dimensional\nrelation space by using Knowledge Graph Embedding techniques. To further\nalleviate the issue of user-POI matrix sparsity, a combined matrix\nfactorization framework is built on a user-POI graph to enhance the inference\nof dynamic personal interests by exploiting the side-information. Experiments\non two real-world datasets demonstrate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 22:26:52 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:40:48 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Wang", "Xianjing", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Koniusz", "Piotr", ""]]}, {"id": "2002.03773", "submitter": "Kashif Ahmad", "authors": "Kashif Ahmad, Syed Zohaib, Nicola Conci and Ala Al-Fuqaha", "title": "Deriving Emotions and Sentiments from Visual Content: A Disaster\n  Analysis Use Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis aims to extract and express a person's perception,\nopinions and emotions towards an entity, object, product and a service,\nenabling businesses to obtain feedback from the consumers. The increasing\npopularity of the social networks and users' tendency towards sharing their\nfeelings, expressions and opinions in text, visual and audio content has opened\nnew opportunities and challenges in sentiment analysis. While sentiment\nanalysis of text streams has been widely explored in the literature, sentiment\nanalysis of images and videos is relatively new. This article introduces visual\nsentiment analysis and contrasts it with textual sentiment analysis with\nemphasis on the opportunities and challenges in this nascent research area. We\nalso propose a deep visual sentiment analyzer for disaster-related images as a\nuse-case, covering different aspects of visual sentiment analysis starting from\ndata collection, annotation, model selection, implementation and evaluations.\nWe believe such rigorous analysis will provide a baseline for future research\nin the domain.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 08:48:52 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ahmad", "Kashif", ""], ["Zohaib", "Syed", ""], ["Conci", "Nicola", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "2002.03932", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, Sanjiv Kumar", "title": "Pre-training Tasks for Embedding-based Large-scale Retrieval", "comments": "Accepted by ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the large-scale query-document retrieval problem: given a query\n(e.g., a question), return the set of relevant documents (e.g., paragraphs\ncontaining the answer) from a large document corpus. This problem is often\nsolved in two steps. The retrieval phase first reduces the solution space,\nreturning a subset of candidate documents. The scoring phase then re-ranks the\ndocuments. Critically, the retrieval algorithm not only desires high recall but\nalso requires to be highly efficient, returning candidates in time sublinear to\nthe number of documents. Unlike the scoring phase witnessing significant\nadvances recently due to the BERT-style pre-training tasks on cross-attention\nmodels, the retrieval phase remains less well studied. Most previous works rely\non classic Information Retrieval (IR) methods such as BM-25 (token matching +\nTF-IDF weights). These models only accept sparse handcrafted features and can\nnot be optimized for different downstream tasks of interest. In this paper, we\nconduct a comprehensive study on the embedding-based retrieval models. We show\nthat the key ingredient of learning a strong embedding-based Transformer model\nis the set of pre-training tasks. With adequately designed paragraph-level\npre-training tasks, the Transformer models can remarkably improve over the\nwidely-used BM-25 as well as embedding models without Transformers. The\nparagraph-level pre-training tasks we studied are Inverse Cloze Task (ICT),\nBody First Selection (BFS), Wiki Link Prediction (WLP), and the combination of\nall three.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:44:00 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Yu", "Felix X.", ""], ["Chang", "Yin-Wen", ""], ["Yang", "Yiming", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2002.04187", "submitter": "Zhengxin Li", "authors": "Zhengxin Li", "title": "Exact Indexing of Time Series under Dynamic Time Warping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic time warping (DTW) is a robust similarity measure of time series.\nHowever, it does not satisfy triangular inequality and has high computational\ncomplexity, severely limiting its applications in similarity search on\nlarge-scale datasets. Usually, we resort to lower bounding distances to speed\nup similarity search under DTW. Unfortunately, there is still a lack of an\neffective lower bounding distance that can measure unequal-length time series\nand has desirable tightness. In the paper, we propose a novel lower bounding\ndistance LB_Keogh+, which is a seamless combination of sequence extension and\nLB_Keogh. It can be used for unequal-length sequences and has low computational\ncomplexity. Besides, LB_Keogh+ can extend sequences to an arbitrary suitable\nlength, without significantly reducing tightness. Next, based on LB_Keogh+, an\nexact index of time series under DTW is devised. Then, we introduce several\ntheorems and complete the relevant proofs to guarantee no false dismissals in\nour similarity search. Finally, extensive experiments are conducted on\nreal-world datasets. Experimental results indicate that our proposed method can\nperform similarity search of unequal-length sequences with high tightness and\ngood pruning power.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 03:34:48 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Li", "Zhengxin", ""]]}, {"id": "2002.04208", "submitter": "Yi Han", "authors": "Yi Han, Shanika Karunasekera, Christopher Leckie", "title": "Image Analysis Enhanced Event Detection from Geo-tagged Tweet Streams", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events detected from social media streams often include early signs of\naccidents, crimes or disasters. Therefore, they can be used by related parties\nfor timely and efficient response. Although significant progress has been made\non event detection from tweet streams, most existing methods have not\nconsidered the posted images in tweets, which provide richer information than\nthe text, and potentially can be a reliable indicator of whether an event\noccurs or not. In this paper, we design an event detection algorithm that\ncombines textual, statistical and image information, following an unsupervised\nmachine learning approach. Specifically, the algorithm starts with semantic and\nstatistical analyses to obtain a list of tweet clusters, each of which\ncorresponds to an event candidate, and then performs image analysis to separate\nevents from non-events---a convolutional autoencoder is trained for each\ncluster as an anomaly detector, where a part of the images are used as the\ntraining data and the remaining images are used as the test instances. Our\nexperiments on multiple datasets verify that when an event occurs, the mean\nreconstruction errors of the training and test images are much closer, compared\nwith the case where the candidate is a non-event cluster. Based on this\nfinding, the algorithm rejects a candidate if the difference is larger than a\nthreshold. Experimental results over millions of tweets demonstrate that this\nimage analysis enhanced approach can significantly increase the precision with\nminimum impact on the recall.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 05:15:00 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Han", "Yi", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""]]}, {"id": "2002.04279", "submitter": "Tom\\'a\\v{s} Folt\\'ynek Ph.D.", "authors": "Tom\\'a\\v{s} Folt\\'ynek, Dita Dlabolov\\'a, Alla Anohina-Naumeca, Salim\n  Raz{\\i}, J\\'ulius Kravjar, Laima Kamzola, Jean Guerrero-Dib, \\\"Ozg\\\"ur\n  \\c{C}elik, Debora Weber-Wulff", "title": "Testing of Support Tools for Plagiarism Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a general belief that software must be able to easily do things that\nhumans find difficult. Since finding sources for plagiarism in a text is not an\neasy task, there is a wide-spread expectation that it must be simple for\nsoftware to determine if a text is plagiarized or not. Software cannot\ndetermine plagiarism, but it can work as a support tool for identifying some\ntext similarity that may constitute plagiarism. But how well do the various\nsystems work? This paper reports on a collaborative test of 15 web-based\ntext-matching systems that can be used when plagiarism is suspected. It was\nconducted by researchers from seven countries using test material in eight\ndifferent languages, evaluating the effectiveness of the systems on\nsingle-source and multi-source documents. A usability examination was also\nperformed. The sobering results show that although some systems can indeed help\nidentify some plagiarized content, they clearly do not find all plagiarism and\nat times also identify non-plagiarized material as problematic.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 09:51:23 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Folt\u00fdnek", "Tom\u00e1\u0161", ""], ["Dlabolov\u00e1", "Dita", ""], ["Anohina-Naumeca", "Alla", ""], ["Raz\u0131", "Salim", ""], ["Kravjar", "J\u00falius", ""], ["Kamzola", "Laima", ""], ["Guerrero-Dib", "Jean", ""], ["\u00c7elik", "\u00d6zg\u00fcr", ""], ["Weber-Wulff", "Debora", ""]]}, {"id": "2002.04302", "submitter": "David Pelta", "authors": "David A. Pelta and Jose L. Verdegay and Maria T. Lamata and Carlos\n  Cruz Corona", "title": "Trust dynamics and user attitudes on recommendation errors: preliminary\n  results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GT cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence based systems may be used as digital nudging\ntechniques that can steer or coerce users to make decisions not always aligned\nwith their true interests. When such systems properly address the issues of\nFairness, Accountability, Transparency, and Ethics, then the trust of the user\nin the system would just depend on the system's output. The aim of this paper\nis to propose a model for exploring how good and bad recommendations affect the\noverall trust in an idealized recommender system that issues recommendations\nover a resource with limited capacity. The impact of different users attitudes\non trust dynamics is also considered. Using simulations, we ran a large set of\nexperiments that allowed to observe that: 1) under certain circumstances, all\nthe users ended accepting the recommendations; and 2) the user attitude\n(controlled by a single parameter balancing the gain/loss of trust after a\ngood/bad recommendation) has a great impact in the trust dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 10:52:53 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Pelta", "David A.", ""], ["Verdegay", "Jose L.", ""], ["Lamata", "Maria T.", ""], ["Corona", "Carlos Cruz", ""]]}, {"id": "2002.04513", "submitter": "Sandro Tsang Dr", "authors": "Sandro Tsang", "title": "An experiment exploring the theoretical and methodological challenges in\n  developing a semi-automated approach to analysis of small-N qualitative data", "comments": "Page 2: \"qualitative\" research (QR); Page 3 and Appendix: Cited the\n  second and third authors of a paper; Page 4: Redundant citation of author\n  names; Page 8: Replaced wordclouds with higher resolution ones (cited\n  facilities on page 3); Page 13: TM is a big-data/\"quantitative\" method and\n  replaced \"supplementary material\" with \"appendix\"; Showed author names after\n  see or cf. on pages 4, 5, 6 and 15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper experiments with designing a semi-automated qualitative data\nanalysis (QDA) algorithm to analyse 20 transcripts by using freeware.\nText-mining (TM) and QDA were guided by frequency and association measures,\nbecause these statistics remain robust when the sample size is small. The\nrefined TM algorithm split the text into various sizes based on a manually\nrevised dictionary. This lemmatisation approach may reflect the context of the\ntext better than uniformly tokenising the text into one single size. TM results\nwere used for initial coding. Code repacking was guided by association measures\nand external data to implement a general inductive QDA approach. The\ninformation retrieved by TM and QDA was depicted in subgraphs for comparisons.\nThe analyses were completed in 6-7 days. Both algorithms retrieved contextually\nconsistent and relevant information. However, the QDA algorithm retrieved more\nspecific information than TM alone. The QDA algorithm does not strictly comply\nwith the convention of TM or of QDA, but becomes a more efficient, systematic\nand transparent text analysis approach than a conventional QDA approach.\nScaling up QDA to reliably discover knowledge from text was exactly the\nresearch purpose. This paper also sheds light on understanding the relations\nbetween information technologies, theory and methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 17:55:19 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 19:10:29 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Tsang", "Sandro", ""]]}, {"id": "2002.04608", "submitter": "Mike Kuehne", "authors": "Michael Kuehne and Marius Radu", "title": "Constructing a Highlight Classifier with an Attention Based LSTM Neural\n  Network", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data is being produced in larger quantities than ever before in human\nhistory. It's only natural to expect a rise in demand for technology that aids\nhumans in sifting through and analyzing this inexhaustible supply of\ninformation. This need exists in the market research industry, where large\namounts of consumer research data is collected through video recordings. At\npresent, the standard method for analyzing video data is human labor. Market\nresearchers manually review the vast majority of consumer research video in\norder to identify relevant portions - highlights. The industry state of the art\nturnaround ratio is 2.2 - for every hour of video content 2.2 hours of manpower\nare required. In this study we present a novel approach for NLP-based highlight\nidentification and extraction based on a supervised learning model that aides\nmarket researchers in sifting through their data. Our approach hinges on a\nmanually curated user-generated highlight clips constructed from long and\nshort-form video data. The problem is best suited for an NLP approach due to\nthe availability of video transcription. We evaluate multiple classes of\nmodels, from gradient boosting to recurrent neural networks, comparing their\nperformance in extraction and identification of highlights. The best performing\nmodels are then evaluated using four sampling methods designed to analyze\ndocuments much larger than the maximum input length of the classifiers. We\nreport very high performances for the standalone classifiers, ROC AUC scores in\nthe range 0.93-0.94, but observe a significant drop in effectiveness when\nevaluated on large documents. Based on our results we suggest combinations of\nmodels/sampling algorithms for various use cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:18:31 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Kuehne", "Michael", ""], ["Radu", "Marius", ""]]}, {"id": "2002.04689", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Ond\\v{r}ej Bojar", "title": "Two Huge Title and Keyword Generation Corpora of Research Articles", "comments": "9 pages, 8 tables. Published in proceedings of LREC 2020, the 12th\n  International Conference on Language Resources and Evaluation, Marseille,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in sequence-to-sequence learning with neural networks\nhave considerably improved the quality of automatically generated text\nsummaries and document keywords, stipulating the need for even bigger training\ncorpora. Metadata of research articles are usually easy to find online and can\nbe used to perform research on various tasks. In this paper, we introduce two\nhuge datasets for text summarization (OAGSX) and keyword generation (OAGKX)\nresearch, containing 34 million and 23 million records, respectively. The data\nwere retrieved from the Open Academic Graph which is a network of research\nprofiles and publications. We carefully processed each record and also tried\nseveral extractive and abstractive methods of both tasks to create performance\nbaselines for other researchers. We further illustrate the performance of those\nmethods previewing their outputs. In the near future, we would like to apply\ntopic modeling on the two sets to derive subsets of research articles from more\nspecific disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:17:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "2002.05063", "submitter": "Alessandro Antonucci", "authors": "Francesca Mangili and Denis Broggini and Alessandro Antonucci and\n  Marco Alberti and Lorenzo Cimasoni", "title": "A Bayesian Approach to Conversational Recommendation Systems", "comments": "Accepted for oral presentation at the \\emph{AAAI 2020 Workshop on\n  Interactive and Conversational Recommendation Systems} (WICRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conversational recommendation system based on a Bayesian\napproach. A probability mass function over the items is updated after any\ninteraction with the user, with information-theoretic criteria optimally\nshaping the interaction and deciding when the conversation should be terminated\nand the most probable item consequently recommended. Dedicated elicitation\ntechniques for the prior probabilities of the parameters modeling the\ninteractions are derived from basic structural judgements. Such prior\ninformation can be combined with historical data to discriminate items with\ndifferent recommendation histories. A case study based on the application of\nthis approach to \\emph{stagend.com}, an online platform for booking\nentertainers, is finally discussed together with an empirical analysis showing\nthe advantages in terms of recommendation quality and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:59:31 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Mangili", "Francesca", ""], ["Broggini", "Denis", ""], ["Antonucci", "Alessandro", ""], ["Alberti", "Marco", ""], ["Cimasoni", "Lorenzo", ""]]}, {"id": "2002.05527", "submitter": "Sridhama Prakhya", "authors": "Sridhama Prakhya, Deepak P", "title": "Unsupervised Separation of Native and Loanwords for Malayalam and Telugu", "comments": "submitted to Natural Language Engineering; 22 pages; 4 figures. This\n  is an extended version of a conference paper (arXiv:1803.09641) that has been\n  enriched with substantive new content, with significant extensions on both\n  the method modeling and the experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quite often, words from one language are adopted within a different language\nwithout translation; these words appear in transliterated form in text written\nin the latter language. This phenomenon is particularly widespread within\nIndian languages where many words are loaned from English. In this paper, we\naddress the task of identifying loanwords automatically and in an unsupervised\nmanner, from large datasets of words from agglutinative Dravidian languages. We\ntarget two specific languages from the Dravidian family, viz., Malayalam and\nTelugu. Based on familiarity with the languages, we outline an observation that\nnative words in both these languages tend to be characterized by a much more\nversatile stem - stem being a shorthand to denote the subword sequence formed\nby the first few characters of the word - than words that are loaned from other\nlanguages. We harness this observation to build an objective function and an\niterative optimization formulation to optimize for it, yielding a scoring of\neach word's nativeness in the process. Through an extensive empirical analysis\nover real-world datasets from both Malayalam and Telugu, we illustrate the\neffectiveness of our method in quantifying nativeness effectively over\navailable baselines for the task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 04:01:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Prakhya", "Sridhama", ""], ["P", "Deepak", ""]]}, {"id": "2002.05606", "submitter": "Ali Erkan", "authors": "Ali Erkan and Tunga Gungor", "title": "Sentiment Analysis Using Averaged Weighted Word Vector Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People use the world wide web heavily to share their experience with entities\nsuch as products, services, or travel destinations. Texts that provide online\nfeedback in the form of reviews and comments are essential to make consumer\ndecisions. These comments create a valuable source that may be used to measure\nsatisfaction related to products or services. Sentiment analysis is the task of\nidentifying opinions expressed in such text fragments. In this work, we develop\ntwo methods that combine different types of word vectors to learn and estimate\npolarity of reviews. We develop average review vectors from word vectors and\nadd weights to this review vectors using word frequencies in positive and\nnegative sensitivity-tagged reviews. We applied the methods to several datasets\nfrom different domains that are used as standard benchmarks for sentiment\nanalysis. We ensemble the techniques with each other and existing methods, and\nwe make a comparison with the approaches in the literature. The results show\nthat the performances of our approaches outperform the state-of-the-art success\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:30:34 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Erkan", "Ali", ""], ["Gungor", "Tunga", ""]]}, {"id": "2002.05607", "submitter": "Zheng Chen", "authors": "Zheng Chen, Xing Fan, Yuan Ling, Lambert Mathias, Chenlei Guo", "title": "Pre-Training for Query Rewriting in A Spoken Language Understanding\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Query rewriting (QR) is an increasingly important technique to reduce\ncustomer friction caused by errors in a spoken language understanding pipeline,\nwhere the errors originate from various sources such as speech recognition\nerrors, language understanding errors or entity resolution errors. In this\nwork, we first propose a neural-retrieval based approach for query rewriting.\nThen, inspired by the wide success of pre-trained contextual language\nembeddings, and also as a way to compensate for insufficient QR training data,\nwe propose a language-modeling (LM) based approach to pre-train query\nembeddings on historical user conversation data with a voice assistant. In\naddition, we propose to use the NLU hypotheses generated by the language\nunderstanding system to augment the pre-training. Our experiments show\npre-training provides rich prior information and help the QR task achieve\nstrong performance. We also show joint pre-training with NLU hypotheses has\nfurther benefit. Finally, after pre-training, we find a small set of rewrite\npairs is enough to fine-tune the QR model to outperform a strong baseline by\nfull training on all QR training data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:31:50 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Zheng", ""], ["Fan", "Xing", ""], ["Ling", "Yuan", ""], ["Mathias", "Lambert", ""], ["Guo", "Chenlei", ""]]}, {"id": "2002.05653", "submitter": "Zheng Chen", "authors": "Zheng Chen, Sadid A. Hasan, Joey Liu, Vivek Datla, Md Shamsuzzaman,\n  Hafiz Khan, Mohammad S Sorower, Gabe Mankovich, Rob van Ommering, Nevenka\n  Dimitrova", "title": "An Ontology-driven Treatment Article Retrieval System for Precision\n  Oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an ontology-driven treatment article retrieval system\ndeveloped and experimented using the data and ground truths provided by the\nTREC 2017 precision medicine track. The key aspects of our system include:\nmeaningful integration of various disease, gene, and drug name ontologies,\ntraining of a novel perceptron model for article relevance labeling, a ranking\nmodule that considers additional factors such as journal impact and article\npublication year, and comprehensive query matching rules. Experimental results\ndemonstrate that our proposed system considerably outperforms the results of\nthe best participating system of the TREC 2017 precision medicine challenge.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:35:02 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Zheng", ""], ["Hasan", "Sadid A.", ""], ["Liu", "Joey", ""], ["Datla", "Vivek", ""], ["Shamsuzzaman", "Md", ""], ["Khan", "Hafiz", ""], ["Sorower", "Mohammad S", ""], ["Mankovich", "Gabe", ""], ["van Ommering", "Rob", ""], ["Dimitrova", "Nevenka", ""]]}, {"id": "2002.05753", "submitter": "Michinari Momma", "authors": "Michinari Momma, Alireza Bagheri Garakani, Nanxun Ma, Yi Sun", "title": "Multi-objective Ranking via Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an Augmented Lagrangian based method to\nincorporate the multiple objectives (MO) in a search ranking algorithm.\nOptimizing MOs is an essential and realistic requirement for building ranking\nmodels in production. The proposed method formulates MO in constrained\noptimization and solves the problem in the popular Boosting framework -- a\nnovel contribution of our work. Furthermore, we propose a procedure to set up\nall optimization parameters in the problem. The experimental results show that\nthe method successfully achieves MO criteria much more efficiently than\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 19:30:32 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Momma", "Michinari", ""], ["Garakani", "Alireza Bagheri", ""], ["Ma", "Nanxun", ""], ["Sun", "Yi", ""]]}, {"id": "2002.06098", "submitter": "Ramy E. Ali", "authors": "Ramy E. Ali", "title": "Consistency Analysis of Replication-Based Probabilistic Key-Value Stores", "comments": null, "journal-ref": "IEEE International Conference on Communications IEEE International\n  Conference on Communications (ICC), 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial quorum systems are widely used in distributed key-value stores due to\ntheir latency benefits at the expense of providing weaker consistency\nguarantees. The probabilistically bounded staleness framework (PBS) studied the\nlatency-consistency trade-off of Dynamo-style partial quorum systems through\nMonte Carlo event-based simulations. In this paper, we study the\nlatency-consistency trade-off for such systems analytically and derive a\nclosed-form expression for the inconsistency probability. Our approach allows\nfine-tuning of latency and consistency guarantees in key-value stores, which is\nintractable using Monte Carlo event-based simulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:09:47 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 00:48:09 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 19:05:23 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 19:42:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ali", "Ramy E.", ""]]}, {"id": "2002.06125", "submitter": "Raul Lima", "authors": "Raul de Ara\\'ujo Lima and Simone Diniz Junqueira Barbosa", "title": "VisMaker: a Question-Oriented Visualization Recommender System for Data\n  Exploration", "comments": "14 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly rapid growth of data production and the consequent need to\nexplore data to obtain answers to the most varied questions have promoted the\ndevelopment of tools to facilitate the manipulation and construction of data\nvisualizations. However, building useful data visualizations is not a trivial\ntask: it may involve a large number of subtle decisions that require experience\nfrom their designer. In this paper, we present VisMaker, a visualization\nrecommender tool that uses a set of rules to present visualization\nrecommendations organized and described through questions, in order to\nfacilitate the understanding of the recommendations and assisting the visual\nexploration process. We carried out two studies comparing our tool with Voyager\n2 and analyzed some aspects of the use of tools. We collected feedback from\nparticipants to identify the advantages and disadvantages of our recommendation\napproach. As a result, we gathered comments to help improve the development of\ntools in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:56:59 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Lima", "Raul de Ara\u00fajo", ""], ["Barbosa", "Simone Diniz Junqueira", ""]]}, {"id": "2002.06144", "submitter": "Rapha\\\"el Barman", "authors": "Rapha\\\"el Barman, Maud Ehrmann, Simon Clematide, Sofia Ares Oliveira,\n  Fr\\'ed\\'eric Kaplan", "title": "Combining Visual and Textual Features for Semantic Segmentation of\n  Historical Newspapers", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, HistoInformatics,\n  HistoInformatics (January 19, 2021) jdmdh:7097", "doi": "10.46298/jdmdh.6107", "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The massive amounts of digitized historical documents acquired over the last\ndecades naturally lend themselves to automatic processing and exploration.\nResearch work seeking to automatically process facsimiles and extract\ninformation thereby are multiplying with, as a first essential step, document\nlayout analysis. If the identification and categorization of segments of\ninterest in document images have seen significant progress over the last years\nthanks to deep learning techniques, many challenges remain with, among others,\nthe use of finer-grained segmentation typologies and the consideration of\ncomplex, heterogeneous documents such as historical newspapers. Besides, most\napproaches consider visual features only, ignoring textual signal. In this\ncontext, we introduce a multimodal approach for the semantic segmentation of\nhistorical newspapers that combines visual and textual features. Based on a\nseries of experiments on diachronic Swiss and Luxembourgish newspapers, we\ninvestigate, among others, the predictive power of visual and textual features\nand their capacity to generalize across time and sources. Results show\nconsistent improvement of multimodal models in comparison to a strong visual\nbaseline, as well as better robustness to high material variance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:56:18 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 11:59:18 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 07:45:29 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 16:56:29 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Barman", "Rapha\u00ebl", ""], ["Ehrmann", "Maud", ""], ["Clematide", "Simon", ""], ["Oliveira", "Sofia Ares", ""], ["Kaplan", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.06205", "submitter": "Avi Caciularu", "authors": "Oren Barkan, Avi Caciularu, Ori Katz and Noam Koenigstein", "title": "Attentive Item2Vec: Neural Attentive User Representations", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization methods for recommender systems tend to represent users as a\nsingle latent vector. However, user behavior and interests may change in the\ncontext of the recommendations that are presented to the user. For example, in\nthe case of movie recommendations, it is usually true that earlier user data is\nless informative than more recent data. However, it is possible that a certain\nearly movie may become suddenly more relevant in the presence of a popular\nsequel movie. This is just a single example of a variety of possible\ndynamically altering user interests in the presence of a potential new\nrecommendation. In this work, we present Attentive Item2vec (AI2V) - a novel\nattentive version of Item2vec (I2V). AI2V employs a context-target attention\nmechanism in order to learn and capture different characteristics of user\nhistorical behavior (context) with respect to a potential recommended item\n(target). The attentive context-target mechanism enables a final neural\nattentive user representation. We demonstrate the effectiveness of AI2V on\nseveral datasets, where it is shown to outperform other baselines.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 15:22:47 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 13:41:55 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 18:25:43 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Barkan", "Oren", ""], ["Caciularu", "Avi", ""], ["Katz", "Ori", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2002.06275", "submitter": "Wenhao Lu", "authors": "Wenhao Lu, Jian Jiao, Ruofei Zhang", "title": "TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for\n  Efficient Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models like BERT have achieved great success in a wide\nvariety of NLP tasks, while the superior performance comes with high demand in\ncomputational resources, which hinders the application in low-latency IR\nsystems. We present TwinBERT model for effective and efficient retrieval, which\nhas twin-structured BERT-like encoders to represent query and document\nrespectively and a crossing layer to combine the embeddings and produce a\nsimilarity score. Different from BERT, where the two input sentences are\nconcatenated and encoded together, TwinBERT decouples them during encoding and\nproduces the embeddings for query and document independently, which allows\ndocument embeddings to be pre-computed offline and cached in memory. Thereupon,\nthe computation left for run-time is from the query encoding and query-document\ncrossing only. This single change can save large amount of computation time and\nresources, and therefore significantly improve serving efficiency. Moreover, a\nfew well-designed network layers and training strategies are proposed to\nfurther reduce computational cost while at the same time keep the performance\nas remarkable as BERT model. Lastly, we develop two versions of TwinBERT for\nretrieval and relevance tasks correspondingly, and both of them achieve close\nor on-par performance to BERT-Base model.\n  The model was trained following the teacher-student framework and evaluated\nwith data from one of the major search engines. Experimental results showed\nthat the inference time was significantly reduced and was firstly controlled\naround 20ms on CPUs while at the same time the performance gain from fine-tuned\nBERT-Base model was mostly retained. Integration of the models into production\nsystems also demonstrated remarkable improvements on relevance metrics with\nnegligible influence on latency.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:44:36 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lu", "Wenhao", ""], ["Jiao", "Jian", ""], ["Zhang", "Ruofei", ""]]}, {"id": "2002.06397", "submitter": "Wei Hu", "authors": "Ermei Cao and Difeng Wang and Jiacheng Huang and Wei Hu", "title": "Open Knowledge Enrichment for Long-tail Entities", "comments": "Accepted by the 29th International World Wide Web Conference (WWW\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) have gradually become a valuable asset for many AI\napplications. While many current KBs are quite large, they are widely\nacknowledged as incomplete, especially lacking facts of long-tail entities,\ne.g., less famous persons. Existing approaches enrich KBs mainly on completing\nmissing links or filling missing values. However, they only tackle a part of\nthe enrichment problem and lack specific considerations regarding long-tail\nentities. In this paper, we propose a full-fledged approach to knowledge\nenrichment, which predicts missing properties and infers true facts of\nlong-tail entities from the open Web. Prior knowledge from popular entities is\nleveraged to improve every enrichment step. Our experiments on the synthetic\nand real-world datasets and comparison with related work demonstrate the\nfeasibility and superiority of the approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 15:25:44 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:42:41 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Cao", "Ermei", ""], ["Wang", "Difeng", ""], ["Huang", "Jiacheng", ""], ["Hu", "Wei", ""]]}, {"id": "2002.06406", "submitter": "Michael F\\\"arber", "authors": "Michael F\\\"arber, Ashwath Sampath", "title": "HybridCite: A Hybrid Model for Context-Aware Citation Recommendation", "comments": "to be published in the Proceedings of the ACM/IEEE Joint Conference\n  on Digital Libraries (JCDL '20)", "journal-ref": null, "doi": "10.1145/3383583.3398534", "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation recommendation systems aim to recommend citations for either a\ncomplete paper or a small portion of text called a citation context. The\nprocess of recommending citations for citation contexts is called local\ncitation recommendation and is the focus of this paper. Firstly, we develop\ncitation recommendation approaches based on embeddings, topic modeling, and\ninformation retrieval techniques. We combine, for the first time to the best of\nour knowledge, the best-performing algorithms into a semi-genetic hybrid\nrecommender system for citation recommendation. We evaluate the single\napproaches and the hybrid approach offline based on several data sets, such as\nthe Microsoft Academic Graph (MAG) and the MAG in combination with arXiv and\nACL. We further conduct a user study for evaluating our approaches online. Our\nevaluation results show that a hybrid model containing embedding and\ninformation retrieval-based components outperforms its individual components\nand further algorithms by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 16:19:55 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 16:49:44 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["F\u00e4rber", "Michael", ""], ["Sampath", "Ashwath", ""]]}, {"id": "2002.06450", "submitter": "Manni Singh", "authors": "Manni Singh, David Weston, Mark Levene", "title": "Supervised Phrase-boundary Embeddings", "comments": "12 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new word embedding model, called SPhrase, that incorporates\nsupervised phrase information. Our method modifies traditional word embeddings\nby ensuring that all target words in a phrase have exactly the same context. We\ndemonstrate that including this information within a context window produces\nsuperior embeddings for both intrinsic evaluation tasks and downstream\nextrinsic tasks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 21:05:07 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Singh", "Manni", ""], ["Weston", "David", ""], ["Levene", "Mark", ""]]}, {"id": "2002.06561", "submitter": "Li Shen", "authors": "Enneng Yang, Xin Xin, Li Shen and Guibing Guo", "title": "Generalized Embedding Machines for Recommender Systems", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization machine (FM) is an effective model for feature-based\nrecommendation which utilizes inner product to capture second-order feature\ninteractions. However, one of the major drawbacks of FM is that it couldn't\ncapture complex high-order interaction signals. A common solution is to change\nthe interaction function, such as stacking deep neural networks on the top of\nFM. In this work, we propose an alternative approach to model high-order\ninteraction signals in the embedding level, namely Generalized Embedding\nMachine (GEM). The embedding used in GEM encodes not only the information from\nthe feature itself but also the information from other correlated features.\nUnder such situation, the embedding becomes high-order. Then we can incorporate\nGEM with FM and even its advanced variants to perform feature interactions.\nMore specifically, in this paper we utilize graph convolution networks (GCN) to\ngenerate high-order embeddings. We integrate GEM with several FM-based models\nand conduct extensive experiments on two real-world datasets. The results\ndemonstrate significant improvement of GEM over corresponding baselines.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 12:03:18 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Yang", "Enneng", ""], ["Xin", "Xin", ""], ["Shen", "Li", ""], ["Guo", "Guibing", ""]]}, {"id": "2002.06585", "submitter": "Vinicius Woloszyn", "authors": "Vinicius Woloszyn, Felipe Schaeffer, Beliza Boniatti, Eduardo Cortes,\n  Salar Mohtaj, Sebastian M\\\"oller", "title": "Untrue.News: A New Search Engine For Fake Stories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we demonstrate Untrue News, a new search engine for fake\nstories. Untrue News is easy to use and offers useful features such as: a) a\nmulti-language option combining fake stories from different countries and\nlanguages around the same subject or person; b) an user privacy protector,\navoiding the filter bubble by employing a bias-free ranking scheme; and c) a\ncollaborative platform that fosters the development of new tools for fighting\ndisinformation. Untrue News relies on Elasticsearch, a new scalable analytic\nsearch engine based on the Lucene library that provides near real-time results.\nWe demonstrate two key scenarios: the first related to a politician - looking\nhow the categories are shown for different types of fake stories - and a second\nrelated to a refugee - showing the multilingual tool. A prototype of Untrue\nNews is accessible via http://untrue.news\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 14:32:22 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Woloszyn", "Vinicius", ""], ["Schaeffer", "Felipe", ""], ["Boniatti", "Beliza", ""], ["Cortes", "Eduardo", ""], ["Mohtaj", "Salar", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2002.06612", "submitter": "Zahra Abbasiantaeb", "authors": "Zahra Abbasiantaeb and Saeedeh Momtazi", "title": "Text-based Question Answering from Information Retrieval and Deep Neural\n  Network Perspectives: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based Question Answering (QA) is a challenging task which aims at\nfinding short concrete answers for users' questions. This line of research has\nbeen widely studied with information retrieval techniques and has received\nincreasing attention in recent years by considering deep neural network\napproaches. Deep learning approaches, which are the main focus of this paper,\nprovide a powerful technique to learn multiple layers of representations and\ninteraction between questions and texts. In this paper, we provide a\ncomprehensive overview of different models proposed for the QA task, including\nboth traditional information retrieval perspective, and more recent deep neural\nnetwork perspective. We also introduce well-known datasets for the task and\npresent available results from the literature to have a comparison between\ndifferent techniques.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 16:24:39 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:27:09 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Abbasiantaeb", "Zahra", ""], ["Momtazi", "Saeedeh", ""]]}, {"id": "2002.06854", "submitter": "Diego Antognini", "authors": "Diego Antognini, Boi Faltings", "title": "HotelRec: a Novel Very Large-Scale Hotel Recommendation Dataset", "comments": "7 pages, 3 figure, 5 tables. Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, recommender systems are an inevitable part of everyone's daily digital\nroutine and are present on most internet platforms. State-of-the-art deep\nlearning-based models require a large number of data to achieve their best\nperformance. Many datasets fulfilling this criterion have been proposed for\nmultiple domains, such as Amazon products, restaurants, or beers. However,\nworks and datasets in the hotel domain are limited: the largest hotel review\ndataset is below the million samples. Additionally, the hotel domain suffers\nfrom a higher data sparsity than traditional recommendation datasets and\ntherefore, traditional collaborative-filtering approaches cannot be applied to\nsuch data. In this paper, we propose HotelRec, a very large-scale hotel\nrecommendation dataset, based on TripAdvisor, containing 50 million reviews. To\nthe best of our knowledge, HotelRec is the largest publicly available dataset\nin the hotel domain (50M versus 0.9M) and additionally, the largest\nrecommendation dataset in a single domain and with textual reviews (50M versus\n22M). We release HotelRec for further research:\nhttps://github.com/Diego999/HotelRec.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 09:30:52 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Antognini", "Diego", ""], ["Faltings", "Boi", ""]]}, {"id": "2002.06862", "submitter": "Taro Langner", "authors": "Taro Langner, Robin Strand, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Large-scale biometry with interpretable neural network regression on UK\n  Biobank body MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a large-scale medical examination, the UK Biobank study has successfully\nimaged more than 32,000 volunteer participants with magnetic resonance imaging\n(MRI). Each scan is linked to extensive metadata, providing a comprehensive\nmedical survey of imaged anatomy and related health states. Despite its\npotential for research, this vast amount of data presents a challenge to\nestablished methods of evaluation, which often rely on manual input. To date,\nthe range of reference values for cardiovascular and metabolic risk factors is\ntherefore incomplete. In this work, neural networks were trained for\nimage-based regression to infer various biological metrics from the\nneck-to-knee body MRI automatically. The approach requires no manual\nintervention or direct access to reference segmentations for training. The\nexamined fields span 64 variables derived from anthropometric measurements,\ndual-energy X-ray absorptiometry (DXA), atlas-based segmentations, and\ndedicated liver scans. With the ResNet50, the standardized framework achieves a\nclose fit to the target values (median R^2 > 0.97) in cross-validation.\nInterpretation of aggregated saliency maps suggests that the network correctly\ntargets specific body regions and limbs, and learned to emulate different\nmodalities. On several body composition metrics, the quality of the predictions\nis within the range of variability observed between established gold standard\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 09:47:58 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:23:26 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 08:59:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Langner", "Taro", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2002.06923", "submitter": "Xavier Bost", "authors": "Xavier Bost (LIA), Vincent Labatut (LIA), Georges Linares (LIA)", "title": "Serial Speakers: a Dataset of TV Series", "comments": null, "journal-ref": "12th International Conference on Language Resources and Evaluation\n  (LREC 2020), p.4256-4264, May 2020, Marseille, France", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over a decade, TV series have been drawing increasing interest, both from\nthe audience and from various academic fields. But while most viewers are\nhooked on the continuous plots of TV serials, the few annotated datasets\navailable to researchers focus on standalone episodes of classical TV series.\nWe aim at filling this gap by providing the multimedia/speech processing\ncommunities with Serial Speakers, an annotated dataset of 161 episodes from\nthree popular American TV serials: Breaking Bad, Game of Thrones and House of\nCards. Serial Speakers is suitable both for investigating multimedia retrieval\nin realistic use case scenarios, and for addressing lower level speech related\ntasks in especially challenging conditions. We publicly release annotations for\nevery speech turn (boundaries, speaker) and scene boundary, along with\nannotations for shot boundaries, recurring shots, and interacting speakers in a\nsubset of episodes. Because of copyright restrictions, the textual content of\nthe speech turns is encrypted in the public version of the dataset, but we\nprovide the users with a simple online tool to recover the plain text from\ntheir own subtitle files.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 12:51:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bost", "Xavier", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "2002.06961", "submitter": "Michael F\\\"arber", "authors": "Michael F\\\"arber, Adam Jatowt", "title": "Citation Recommendation: Approaches and Datasets", "comments": "to be published in the International Journal on Digital Libraries", "journal-ref": null, "doi": "10.1007/s00799-020-00288-2", "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation recommendation describes the task of recommending citations for a\ngiven text. Due to the overload of published scientific works in recent years\non the one hand, and the need to cite the most appropriate publications when\nwriting scientific texts on the other hand, citation recommendation has emerged\nas an important research topic. In recent years, several approaches and\nevaluation data sets have been presented. However, to the best of our\nknowledge, no literature survey has been conducted explicitly on citation\nrecommendation. In this article, we give a thorough introduction into automatic\ncitation recommendation research. We then present an overview of the approaches\nand data sets for citation recommendation and identify differences and\ncommonalities using various dimensions. Last but not least, we shed light on\nthe evaluation methods, and outline general challenges in the evaluation and\nhow to meet them. We restrict ourselves to citation recommendation for\nscientific publications, as this document type has been studied the most in\nthis area. However, many of the observations and discussions included in this\nsurvey are also applicable to other types of text, such as news articles and\nencyclopedic articles.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:59:50 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 08:01:27 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["F\u00e4rber", "Michael", ""], ["Jatowt", "Adam", ""]]}, {"id": "2002.06987", "submitter": "Wei Deng", "authors": "Wei Deng and Junwei Pan and Tian Zhou and Deguang Kong and Aaron\n  Flores and Guang Lin", "title": "DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR\n  Predictions in Ad Serving", "comments": "Accepted by WSDM 2021; Source code:\n  https://github.com/WayneDW/DeepLight_Deep-Lightweight-Feature-Interactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a crucial task in online display\nadvertising. The embedding-based neural networks have been proposed to learn\nboth explicit feature interactions through a shallow component and deep feature\ninteractions using a deep neural network (DNN) component. These sophisticated\nmodels, however, slow down the prediction inference by at least hundreds of\ntimes. To address the issue of significantly increased serving delay and high\nmemory usage for ad serving in production, this paper presents\n\\emph{DeepLight}: a framework to accelerate the CTR predictions in three\naspects: 1) accelerate the model inference via explicitly searching informative\nfeature interactions in the shallow component; 2) prune redundant layers and\nparameters at intra-layer and inter-layer level in the DNN component; 3)\npromote the sparsity of the embedding layer to preserve the most discriminant\nsignals. By combining the above efforts, the proposed approach accelerates the\nmodel inference by 46X on Criteo dataset and 27X on Avazu dataset without any\nloss on the prediction accuracy. This paves the way for successfully deploying\ncomplicated embedding-based neural networks in production for ad serving.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 14:51:31 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 01:46:08 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 22:13:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Deng", "Wei", ""], ["Pan", "Junwei", ""], ["Zhou", "Tian", ""], ["Kong", "Deguang", ""], ["Flores", "Aaron", ""], ["Lin", "Guang", ""]]}, {"id": "2002.07143", "submitter": "James Dunham", "authors": "James Dunham and Jennifer Melot and Dewey Murdick", "title": "Identifying the Development and Application of Artificial Intelligence\n  in Scientific Text", "comments": "This revision expands our analysis in Section 5. We predict and\n  evaluate labels for publications in Microsoft Academic Graph and Digital\n  Science Dimensions, in addition to Clarivate Web of Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a strategy for identifying the universe of research publications\nrelevant to the application and development of artificial intelligence. The\napproach leverages the arXiv corpus of scientific preprints, in which authors\nchoose subject tags for their papers from a set defined by editors. We compose\na functional definition of AI relevance by learning these subjects from paper\nmetadata, and then inferring the arXiv-subject labels of papers in larger\ncorpora: Clarivate Web of Science, Digital Science Dimensions, and Microsoft\nAcademic Graph. This yields predictive classification $F_1$ scores between .75\nand .86 for Natural Language Processing (cs.CL), Computer Vision (cs.CV), and\nRobotics (cs.RO). For a single model that learns these and four other\nAI-relevant subjects (cs.AI, cs.LG, stat.ML, and cs.MA), we see precision of\n.83 and recall of .85. We evaluate the out-of-domain performance of our\nclassifiers against other sources of topic information and predictions from\nalternative methods. We find that a supervised solution can generalize to\nidentify publications that belong to the high-level fields of study represented\non arXiv. This offers a method for identifying AI-relevant publications that\nupdates at the pace of research output, without reliance on subject-matter\nexperts for query development or labeling.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 18:58:59 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 15:35:17 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Dunham", "James", ""], ["Melot", "Jennifer", ""], ["Murdick", "Dewey", ""]]}, {"id": "2002.07397", "submitter": "Kun Zhou", "authors": "Kun Zhou and Wayne Xin Zhao and Yutao Zhu and Ji-Rong Wen and Jingsong\n  Yu", "title": "Improving Multi-Turn Response Selection Models with Complementary\n  Last-Utterance Selection by Instance Weighting", "comments": "12 pages. Accepted by PAKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain retrieval-based dialogue systems require a considerable amount of\ntraining data to learn their parameters. However, in practice, the negative\nsamples of training data are usually selected from an unannotated conversation\ndata set at random. The generated training data is likely to contain noise and\naffect the performance of the response selection models. To address this\ndifficulty, we consider utilizing the underlying correlation in the data\nresource itself to derive different kinds of supervision signals and reduce the\ninfluence of noisy data. More specially, we consider a main-complementary task\npair. The main task (\\ie our focus) selects the correct response given the last\nutterance and context, and the complementary task selects the last utterance\ngiven the response and context. The key point is that the output of the\ncomplementary task is used to set instance weights for the main task. We\nconduct extensive experiments in two public datasets and obtain significant\nimprovement in both datasets. We also investigate the variant of our approach\nin multiple aspects, and the results have verified the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:29:01 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhou", "Kun", ""], ["Zhao", "Wayne Xin", ""], ["Zhu", "Yutao", ""], ["Wen", "Ji-Rong", ""], ["Yu", "Jingsong", ""]]}, {"id": "2002.07590", "submitter": "Bharath K P", "authors": "Manas Jain, Shruthi Narayan, Pratibha Balaji, Bharath K P, Abhijit\n  Bhowmick, Karthik R, Rajesh Kumar Muthu", "title": "Speech Emotion Recognition using Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.SD", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this project, we aim to classify the speech taken as one of the four\nemotions namely, sadness, anger, fear and happiness. The samples that have been\ntaken to complete this project are taken from Linguistic Data Consortium (LDC)\nand UGA database. The important characteristics determined from the samples are\nenergy, pitch, MFCC coefficients, LPCC coefficients and speaker rate. The\nclassifier used to classify these emotional states is Support Vector Machine\n(SVM) and this is done using two classification strategies: One against All\n(OAA) and Gender Dependent Classification. Furthermore, a comparative analysis\nhas been conducted between the two and LPCC and MFCC algorithms as well.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:56:56 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Jain", "Manas", ""], ["Narayan", "Shruthi", ""], ["Balaji", "Pratibha", ""], ["P", "Bharath K", ""], ["Bhowmick", "Abhijit", ""], ["R", "Karthik", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "2002.07651", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "Listwise Learning to Rank with Deep Q-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to Rank is the problem involved with ranking a sequence of documents\nbased on their relevance to a given query. Deep Q-Learning has been shown to be\na useful method for training an agent in sequential decision making. In this\npaper, we show that DeepQRank, our deep q-learning to rank agent, demonstrates\nperformance that can be considered state-of-the-art. Though less\ncomputationally efficient than a supervised learning approach such as linear\nregression, our agent has fewer limitations in terms of which format of data it\ncan use for training and evaluation. We run our algorithm against Microsoft's\nLETOR listwise dataset and achieve an NDCG@1 (ranking accuracy in the range\n[0,1]) of 0.5075, narrowly beating out the leading supervised learning model,\nSVMRank (0.4958).\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 22:45:56 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "2002.07696", "submitter": "Oren Barkan", "authors": "Oren Barkan, Ori Katz, Noam Koenigstein", "title": "Neural Attentive Multiview Machines", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in multiview representation learning is finding the\noptimal combination of views with respect to the specific task at hand. To this\nend, we introduce NAM: a Neural Attentive Multiview machine that learns\nmultiview item representations and similarity by employing a novel attention\nmechanism. NAM harnesses multiple information sources and automatically\nquantifies their relevancy with respect to a supervised task. Finally, a very\npractical advantage of NAM is its robustness to the case of dataset with\nmissing views. We demonstrate the effectiveness of NAM for the task of movies\nand app recommendations. Our evaluations indicate that NAM outperforms single\nview models as well as alternative multiview methods on item recommendations\ntasks, including cold-start scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:21:46 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Barkan", "Oren", ""], ["Katz", "Ori", ""], ["Koenigstein", "Noam", ""]]}, {"id": "2002.07786", "submitter": "Masoud Mansoury", "authors": "Masoud Mansoury and Himan Abdollahpouri and Jessie Smith and Arman\n  Dehpanah and Mykola Pechenizkiy and Bamshad Mobasher", "title": "Investigating Potential Factors Associated with Gender Discrimination in\n  Collaborative Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of personalized recommendation technologies has raised\nconcerns about discrepancies in their recommendation performance across\ndifferent genders, age groups, and racial or ethnic populations. This varying\ndegree of performance could impact users' trust in the system and may pose\nlegal and ethical issues in domains where fairness and equity are critical\nconcerns, like job recommendation. In this paper, we investigate several\npotential factors that could be associated with discriminatory performance of a\nrecommendation algorithm for women versus men. We specifically study several\ncharacteristics of user profiles and analyze their possible associations with\ndisparate behavior of the system towards different genders. These\ncharacteristics include the anomaly in rating behavior, the entropy of users'\nprofiles, and the users' profile size. Our experimental results on a public\ndataset using four recommendation algorithms show that, based on all the three\nmentioned factors, women get less accurate recommendations than men indicating\nan unfair nature of recommendation algorithms across genders.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:30:17 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Mansoury", "Masoud", ""], ["Abdollahpouri", "Himan", ""], ["Smith", "Jessie", ""], ["Dehpanah", "Arman", ""], ["Pechenizkiy", "Mykola", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2002.07877", "submitter": "Subhadip Maji", "authors": "Subhadip Maji and Smarajit Bose", "title": "CBIR using features derived by Deep Learning", "comments": "18 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Content Based Image Retrieval (CBIR) System, the task is to retrieve\nsimilar images from a large database given a query image. The usual procedure\nis to extract some useful features from the query image, and retrieve images\nwhich have similar set of features. For this purpose, a suitable similarity\nmeasure is chosen, and images with high similarity scores are retrieved.\nNaturally the choice of these features play a very important role in the\nsuccess of this system, and high level features are required to reduce the\nsemantic gap.\n  In this paper, we propose to use features derived from pre-trained network\nmodels from a deep-learning convolution network trained for a large image\nclassification problem. This approach appears to produce vastly superior\nresults for a variety of databases, and it outperforms many contemporary CBIR\nsystems. We analyse the retrieval time of the method, and also propose a\npre-clustering of the database based on the above-mentioned features which\nyields comparable results in a much shorter time in most of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:26:32 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Maji", "Subhadip", ""], ["Bose", "Smarajit", ""]]}, {"id": "2002.07993", "submitter": "Wei Zhang", "authors": "Wen Wang, Wei Zhang, Shukai Liu, Qi Liu, Bo Zhang, Leyu Lin, Hongyuan\n  Zha", "title": "Beyond Clicks: Modeling Multi-Relational Item Graph for Session-Based\n  Target Behavior Prediction", "comments": "Short paper with 7 pages, WWW'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session-based target behavior prediction aims to predict the next item to be\ninteracted with specific behavior types (e.g., clicking). Although existing\nmethods for session-based behavior prediction leverage powerful representation\nlearning approaches to encode items' sequential relevance in a low-dimensional\nspace, they suffer from several limitations. Firstly, they focus on only\nutilizing the same type of user behavior for prediction, but ignore the\npotential of taking other behavior data as auxiliary information. This is\nparticularly crucial when the target behavior is sparse but important (e.g.,\nbuying or sharing an item). Secondly, item-to-item relations are modeled\nseparately and locally in one behavior sequence, and they lack a principled way\nto globally encode these relations more effectively. To overcome these\nlimitations, we propose a novel Multi-relational Graph Neural Network model for\nSession-based target behavior Prediction, namely MGNN-SPred for short.\nSpecifically, we build a Multi-Relational Item Graph (MRIG) based on all\nbehavior sequences from all sessions, involving target and auxiliary behavior\ntypes. Based on MRIG, MGNN-SPred learns global item-to-item relations and\nfurther obtains user preferences w.r.t. current target and auxiliary behavior\nsequences, respectively. In the end, MGNN-SPred leverages a gating mechanism to\nadaptively fuse user representations for predicting next item interacted with\ntarget behavior. The extensive experiments on two real-world datasets\ndemonstrate the superiority of MGNN-SPred by comparing with state-of-the-art\nsession-based prediction methods, validating the benefits of leveraging\nauxiliary behavior and learning item-to-item relations over MRIG.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 03:52:48 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 04:04:38 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 12:26:52 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Wen", ""], ["Zhang", "Wei", ""], ["Liu", "Shukai", ""], ["Liu", "Qi", ""], ["Zhang", "Bo", ""], ["Lin", "Leyu", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2002.08025", "submitter": "Minghong Fang", "authors": "Minghong Fang, Neil Zhenqiang Gong, Jia Liu", "title": "Influence Function based Data Poisoning Attacks to Top-N Recommender\n  Systems", "comments": "Accepted by WWW 2020; This is technical report version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system is an essential component of web services to engage users.\nPopular recommender systems model user preferences and item properties using a\nlarge amount of crowdsourced user-item interaction data, e.g., rating scores;\nthen top-$N$ items that match the best with a user's preference are recommended\nto the user. In this work, we show that an attacker can launch a data poisoning\nattack to a recommender system to make recommendations as the attacker desires\nvia injecting fake users with carefully crafted user-item interaction data.\nSpecifically, an attacker can trick a recommender system to recommend a target\nitem to as many normal users as possible. We focus on matrix factorization\nbased recommender systems because they have been widely deployed in industry.\nGiven the number of fake users the attacker can inject, we formulate the\ncrafting of rating scores for the fake users as an optimization problem.\nHowever, this optimization problem is challenging to solve as it is a\nnon-convex integer programming problem. To address the challenge, we develop\nseveral techniques to approximately solve the optimization problem. For\ninstance, we leverage influence function to select a subset of normal users who\nare influential to the recommendations and solve our formulated optimization\nproblem based on these influential users. Our results show that our attacks are\neffective and outperform existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 06:41:51 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 20:45:44 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 21:24:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fang", "Minghong", ""], ["Gong", "Neil Zhenqiang", ""], ["Liu", "Jia", ""]]}, {"id": "2002.08500", "submitter": "Jos\\'e Everardo Bessa Maia", "authors": "Jos\\'e E. B. Maia and Gild\\'acio J. de A. S\\'a", "title": "Processing topical queries on images of historical newspaper pages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical newspapers are a source of research for the human and social\nsciences. However, these image collections are difficult to read by machine due\nto the low quality of the print, the lack of standardization of the pages in\naddition to the low quality photograph of some files. This paper presents the\nprocessing model of a topic navigation system in historical newspaper page\nimages. The general procedure consists of four modules which are: segmentation\nof text sub-images and text extraction, preprocessing and representation,\ninduced topic extraction and representation, and document viewing and retrieval\ninterface. The algorithmic and technological approaches of each module are\ndescribed and the initial test results about a collection covering a range of\n28 years are presented.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 00:02:07 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Maia", "Jos\u00e9 E. B.", ""], ["S\u00e1", "Gild\u00e1cio J. de A.", ""]]}, {"id": "2002.08530", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin,\n  Lichan Hong, Ed H. Chi", "title": "Learning Multi-granular Quantized Embeddings for Large-Vocab Categorical\n  Features in Recommender Systems", "comments": "longer version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system models often represent various sparse features like users,\nitems, and categorical features via embeddings. A standard approach is to map\neach unique feature value to an embedding vector. The size of the produced\nembedding table grows linearly with the size of the vocabulary. Therefore, a\nlarge vocabulary inevitably leads to a gigantic embedding table, creating two\nsevere problems: (i) making model serving intractable in resource-constrained\nenvironments; (ii) causing overfitting problems. In this paper, we seek to\nlearn highly compact embeddings for large-vocab sparse features in recommender\nsystems (recsys). First, we show that the novel Differentiable Product\nQuantization (DPQ) approach can generalize to recsys problems. In addition, to\nbetter handle the power-law data distribution commonly seen in recsys, we\npropose a Multi-Granular Quantized Embeddings (MGQE) technique which learns\nmore compact embeddings for infrequent items. We seek to provide a new angle to\nimprove recommendation performance with compact model sizes. Extensive\nexperiments on three recommendation tasks and two datasets show that we can\nachieve on par or better performance, with only ~20% of the original model\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:01:44 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 02:38:13 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Cheng", "Derek Zhiyuan", ""], ["Chen", "Ting", ""], ["Yi", "Xinyang", ""], ["Lin", "Dong", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""]]}, {"id": "2002.08575", "submitter": "Yuanyuan Jin", "authors": "Yuanyuan Jin, Wei Zhang, Xiangnan He, Xinyu Wang and Xiaoling Wang", "title": "Syndrome-aware Herb Recommendation with Multi-Graph Convolution Network", "comments": "Accepted by ICDE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herb recommendation plays a crucial role in the therapeutic process of\nTraditional Chinese Medicine(TCM), which aims to recommend a set of herbs to\ntreat the symptoms of a patient. While several machine learning methods have\nbeen developed for herb recommendation, they are limited in modeling only the\ninteractions between herbs and symptoms, and ignoring the intermediate process\nof syndrome induction. When performing TCM diagnostics, an experienced doctor\ntypically induces syndromes from the patient's symptoms and then suggests herbs\nbased on the induced syndromes. As such, we believe the induction of syndromes,\nan overall description of the symptoms, is important for herb recommendation\nand should be properly handled. However, due to the ambiguity and complexity of\nsyndrome induction, most prescriptions lack the explicit ground truth of\nsyndromes. In this paper, we propose a new method that takes the implicit\nsyndrome induction process into account for herb recommendation. Given a set of\nsymptoms to treat, we aim to generate an overall syndrome representation by\neffectively fusing the embeddings of all the symptoms in the set, to mimic how\na doctor induces the syndromes. Towards symptom embedding learning, we\nadditionally construct a symptom-symptom graph from the input prescriptions for\ncapturing the relations between symptoms; we then build graph convolution\nnetworks(GCNs) on both symptom-symptom and symptom-herb graphs to learn symptom\nembedding. Similarly, we construct a herb-herb graph and build GCNs on both\nherb-herb and symptom-herb graphs to learn herb embedding, which is finally\ninteracted with the syndrome representation to predict the scores of herbs. In\nthis way, more comprehensive representations can be obtained. We conduct\nextensive experiments on a public TCM dataset, showing significant improvements\nover state-of-the-art herb recommendation methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 05:56:53 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Jin", "Yuanyuan", ""], ["Zhang", "Wei", ""], ["He", "Xiangnan", ""], ["Wang", "Xinyu", ""], ["Wang", "Xiaoling", ""]]}, {"id": "2002.08577", "submitter": "Yinan Zhang", "authors": "Yinan Zhang, Parikshit Sondhi, Anjan Goswami, ChengXiang Zhai", "title": "Towards a Soft Faceted Browsing Scheme for Information Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faceted browsing is a commonly supported feature of user interfaces for\naccess to information. Existing interfaces generally treat facet values\nselected by a user as hard filters and respond to the user by only displaying\ninformation items strictly satisfying the filters and in their original ranking\norder. We propose a novel alternative strategy for faceted browsing, called\nsoft faceted browsing, where the system also includes some possibly relevant\nitems outside the selected filter in a non-intrusive way and re-ranks the items\nto better satisfy the user's information need. Such a soft faceted browsing\nstrategy can be beneficial when the user does not have a very confident and\nstrict preference for the selected facet values, and is especially appropriate\nfor applications such as e-commerce search where the user would like to explore\na larger space before finalizing a purchasing decision. We propose a\nprobabilistic framework for modeling and solving the soft faceted browsing\nproblem, and apply the framework to study the case of facet filter selection in\ne-commerce search engines. Preliminary experiment results demonstrate the soft\nfaceted browsing scheme is better than the traditional faceted browsing scheme\nin terms of its efficiency in helping users navigate in the information space.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 06:01:57 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zhang", "Yinan", ""], ["Sondhi", "Parikshit", ""], ["Goswami", "Anjan", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "2002.08762", "submitter": "Konstantinos Bougiatiotis", "authors": "K. Bougiatiotis, R. Fasoulis, F. Aisopos, A. Nentidis, G. Paliouras", "title": "Guiding Graph Embeddings using Path-Ranking Methods for Error Detection\n  innoisy Knowledge Graphs", "comments": "9 pages, 2 figures. To appear in GCLR 2021: AAAI 2021 Workshop on\n  Graphs and more Complex structures for Learning and Reasonin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays Knowledge Graphs constitute a mainstream approach for the\nrepresentation of relational information on big heterogeneous data, however,\nthey may contain a big amount of imputed noise when constructed automatically.\nTo address this problem, different error detection methodologies have been\nproposed, mainly focusing on path ranking and representation learning. This\nwork presents various mainstream approaches and proposes a hybrid and modular\nmethodology for the task. We compare different methods on two benchmarks and\none real-world biomedical publications dataset, showcasing the potential of our\napproach and providing insights on graph embeddings when dealing with noisy\nKnowledge Graphs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 11:04:11 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 20:43:10 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bougiatiotis", "K.", ""], ["Fasoulis", "R.", ""], ["Aisopos", "F.", ""], ["Nentidis", "A.", ""], ["Paliouras", "G.", ""]]}, {"id": "2002.09026", "submitter": "Jianyu Fan", "authors": "Jianyu Fan, Eric Nichols, Daniel Tompkins, Ana Elisa Mendez Mendez,\n  Benjamin Elizalde, and Philippe Pasquier", "title": "Multi-label Sound Event Retrieval Using a Deep Learning-based Siamese\n  Structure with a Pairwise Presence Matrix", "comments": "Paper accepted for 45th International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic recordings of soundscapes often have multiple sound events\nco-occurring, such as car horns, engine and human voices. Sound event retrieval\nis a type of content-based search aiming at finding audio samples, similar to\nan audio query based on their acoustic or semantic content. State of the art\nsound event retrieval models have focused on single-label audio recordings,\nwith only one sound event occurring, rather than on multi-label audio\nrecordings (i.e., multiple sound events occur in one recording). To address\nthis latter problem, we propose different Deep Learning architectures with a\nSiamese-structure and a Pairwise Presence Matrix. The networks are trained and\nevaluated using the SONYC-UST dataset containing both single- and multi-label\nsoundscape recordings. The performance results show the effectiveness of our\nproposed model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 21:33:07 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Fan", "Jianyu", ""], ["Nichols", "Eric", ""], ["Tompkins", "Daniel", ""], ["Mendez", "Ana Elisa Mendez", ""], ["Elizalde", "Benjamin", ""], ["Pasquier", "Philippe", ""]]}, {"id": "2002.09102", "submitter": "Yisong Miao", "authors": "Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong,\n  Min-Yen Kan, Tat-Seng Chua", "title": "Estimation-Action-Reflection: Towards Deep Interaction Between\n  Conversational and Recommender Systems", "comments": "WSDM '20 oral, 9 pages, 3 figures", "journal-ref": "In Proceedings of the 13th International Conference on Web Search\n  and Data Mining, pp. 304-312. 2020", "doi": "10.1145/3336191.3371769", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are embracing conversational technologies to obtain user\npreferences dynamically, and to overcome inherent limitations of their static\nmodels. A successful Conversational Recommender System (CRS) requires proper\nhandling of interactions between conversation and recommendation. We argue that\nthree fundamental problems need to be solved: 1) what questions to ask\nregarding item attributes, 2) when to recommend items, and 3) how to adapt to\nthe users' online feedback. To the best of our knowledge, there lacks a unified\nframework that addresses these problems.\n  In this work, we fill this missing interaction framework gap by proposing a\nnew CRS framework named Estimation-Action-Reflection, or EAR, which consists of\nthree stages to better converse with users. (1) Estimation, which builds\npredictive models to estimate user preference on both items and item\nattributes; (2) Action, which learns a dialogue policy to determine whether to\nask attributes or recommend items, based on Estimation stage and conversation\nhistory; and (3) Reflection, which updates the recommender model when a user\nrejects the recommendations made by the Action stage. We present two\nconversation scenarios on binary and enumerated questions, and conduct\nextensive experiments on two datasets from Yelp and LastFM, for each scenario,\nrespectively. Our experiments demonstrate significant improvements over the\nstate-of-the-art method CRM [32], corresponding to fewer conversation turns and\na higher level of recommendation hits.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 02:56:17 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Lei", "Wenqiang", ""], ["He", "Xiangnan", ""], ["Miao", "Yisong", ""], ["Wu", "Qingyun", ""], ["Hong", "Richang", ""], ["Kan", "Min-Yen", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2002.09693", "submitter": "Haoxing Lin", "authors": "Haoxing Lin and Weijia Jia and Yongjian You and Yiping Sun", "title": "Interpretable Crowd Flow Prediction with Spatial-Temporal Self-Attention", "comments": "7pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd flow prediction has been increasingly investigated in intelligent urban\ncomputing field as a fundamental component of urban management system. The most\nchallenging part of predicting crowd flow is to measure the complicated\nspatial-temporal dependencies. A prevalent solution employed in current methods\nis to divide and conquer the spatial and temporal information by various\narchitectures (e.g., CNN/GCN, LSTM). However, this strategy has two\ndisadvantages: (1) the sophisticated dependencies are also divided and\ntherefore partially isolated; (2) the spatial-temporal features are transformed\ninto latent representations when passing through different architectures,\nmaking it hard to interpret the predicted crowd flow. To address these issues,\nwe propose a Spatial-Temporal Self-Attention Network (STSAN) with an ST\nencoding gate that calculates the entire spatial-temporal representation with\npositional and time encodings and therefore avoids dividing the dependencies.\nFurthermore, we develop a Multi-aspect attention mechanism that applies scaled\ndot-product attention over spatial-temporal information and measures the\nattention weights that explicitly indicate the dependencies. Experimental\nresults on traffic and mobile data demonstrate that the proposed method reduces\ninflow and outflow RMSE by 16% and 8% on the Taxi-NYC dataset compared to the\nSOTA baselines.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 12:43:11 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lin", "Haoxing", ""], ["Jia", "Weijia", ""], ["You", "Yongjian", ""], ["Sun", "Yiping", ""]]}, {"id": "2002.09841", "submitter": "Chao Wang", "authors": "Chao Wang, Hengshu Zhu, Chen Zhu, Chuan Qin, Hui Xiong", "title": "SetRank: A Setwise Bayesian Approach for Collaborative Ranking from\n  Implicit Feedback", "comments": "This paper has been accepted in AAAI'20", "journal-ref": "The Thirty-Fourth AAAI Conference on Artificial Intelligenc\n  (AAAI'20), New York, New York, USA, 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of online recommender systems has a focus on\ncollaborative ranking from implicit feedback, such as user clicks and\npurchases. Different from explicit ratings, which reflect graded user\npreferences, the implicit feedback only generates positive and unobserved\nlabels. While considerable efforts have been made in this direction, the\nwell-known pairwise and listwise approaches have still been limited by various\nchallenges. Specifically, for the pairwise approaches, the assumption of\nindependent pairwise preference is not always held in practice. Also, the\nlistwise approaches cannot efficiently accommodate \"ties\" due to the\nprecondition of the entire list permutation. To this end, in this paper, we\npropose a novel setwise Bayesian approach for collaborative ranking, namely\nSetRank, to inherently accommodate the characteristics of implicit feedback in\nrecommender system. Specifically, SetRank aims at maximizing the posterior\nprobability of novel setwise preference comparisons and can be implemented with\nmatrix factorization and neural networks. Meanwhile, we also present the\ntheoretical analysis of SetRank to show that the bound of excess risk can be\nproportional to $\\sqrt{M/N}$, where $M$ and $N$ are the numbers of items and\nusers, respectively. Finally, extensive experiments on four real-world datasets\nclearly validate the superiority of SetRank compared with various\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 06:40:48 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Chao", ""], ["Zhu", "Hengshu", ""], ["Zhu", "Chen", ""], ["Qin", "Chuan", ""], ["Xiong", "Hui", ""]]}, {"id": "2002.09901", "submitter": "Pravesh Koirala", "authors": "Pravesh Koirala and Aman Shakya", "title": "A Nepali Rule Based Stemmer and its performance on different NLP\n  applications", "comments": "5 pages, 2 figures, 3 tables", "journal-ref": "Proceedings of the 4th International IT Conference on ICT with\n  Smart Computing and 9th National Students' Conference on Information\n  Technology, (NaSCoIT 2018), Kathmandu, Nepal, ISSN No 2505-1075, pp. 16\n  (December 2018)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is an integral part of Natural Language Processing (NLP). It's a\npreprocessing step in almost every NLP application. Arguably, the most\nimportant usage of stemming is in Information Retrieval (IR). While there are\nlots of work done on stemming in languages like English, Nepali stemming has\nonly a few works. This study focuses on creating a Rule Based stemmer for\nNepali text. Specifically, it is an affix stripping system that identifies two\ndifferent class of suffixes in Nepali grammar and strips them separately. Only\na single negativity prefix (Na) is identified and stripped. This study focuses\non a number of techniques like exception word identification, morphological\nnormalization and word transformation to increase stemming performance. The\nstemmer is tested intrinsically using Paice's method and extrinsically on a\nbasic tf-idf based IR system and an elementary news topic classifier using\nMultinomial Naive Bayes Classifier. The difference in performance of these\nsystems with and without using the stemmer is analysed.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 13:33:04 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Koirala", "Pravesh", ""], ["Shakya", "Aman", ""]]}, {"id": "2002.10016", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh (1), Ebrahim Ansari (1 and 2), Parvin Razzaghi (1\n  and 3), Akbar Karimi (4) ((1) Institute for Advanced Studies in Basic\n  Sciences (IASBS), Zanjan, Iran, (2) Faculty of Mathematics and Physics,\n  Institute of Formal and Applied Linguistics, Charles University, Czechia, (3)\n  Institute for Research in Fundamental Sciences (IPM), Tehran, Iran, (4) IMP\n  Lab, Department of Engineering and Architecture, University of Parma, Parma,\n  Italy)", "title": "Deep Multimodal Image-Text Embeddings for Automatic Cross-Media\n  Retrieval", "comments": "6 pages and 2 figures, Learn more about this project at\n  https://iasbs.ac.ir/~ansari/deeptwitter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of matching images and sentences by learning a\nvisual-textual embedding space for cross-modal retrieval. Finding such a space\nis a challenging task since the features and representations of text and image\nare not comparable. In this work, we introduce an end-to-end deep multimodal\nconvolutional-recurrent network for learning both vision and language\nrepresentations simultaneously to infer image-text similarity. The model learns\nwhich pairs are a match (positive) and which ones are a mismatch (negative)\nusing a hinge-based triplet ranking. To learn about the joint representations,\nwe leverage our newly extracted collection of tweets from Twitter. The main\ncharacteristic of our dataset is that the images and tweets are not\nstandardized the same as the benchmarks. Furthermore, there can be a higher\nsemantic correlation between the pictures and tweets contrary to benchmarks in\nwhich the descriptions are well-organized. Experimental results on MS-COCO\nbenchmark dataset show that our model outperforms certain methods presented\npreviously and has competitive performance compared to the state-of-the-art.\nThe code and dataset have been made available publicly.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 23:58:04 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Khojasteh", "Hadi Abdi", "", "1 and 2"], ["Ansari", "Ebrahim", "", "1 and 2"], ["Razzaghi", "Parvin", "", "1\n  and 3"], ["Karimi", "Akbar", ""]]}, {"id": "2002.10181", "submitter": "Gong Cheng", "authors": "Shuxin Li, Gong Cheng, Chengkai Li", "title": "Relaxing Relationship Queries on Graph Data", "comments": "16 pages, accepted to JoWS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many domains we have witnessed the need to search a large entity-relation\ngraph for direct and indirect relationships between a set of entities specified\nin a query. A search result, called a semantic association (SA), is typically a\ncompact (e.g., diameter-constrained) connected subgraph containing all the\nquery entities. For this problem of SA search, efficient algorithms exist but\nwill return empty results if some query entities are distant in the graph. To\nreduce the occurrence of failing query and provide alternative results, we\nstudy the problem of query relaxation in the context of SA search. Simply\nrelaxing the compactness constraint will sacrifice the compactness of an SA,\nand more importantly, may lead to performance issues and be impracticable.\nInstead, we focus on removing the smallest number of entities from the original\nfailing query, to form a maximum successful sub-query which minimizes the loss\nof result quality caused by relaxation. We prove that verifying the success of\na sub-query turns into finding an entity (called a certificate) that satisfies\na distance-based condition about the query entities. To efficiently find a\ncertificate of the success of a maximum sub-query, we propose a best-first\nsearch algorithm that leverages distance-based estimation to effectively prune\nthe search space. We further improve its performance by adding two fine-grained\nheuristics: one based on degree and the other based on distance. Extensive\nexperiments over popular RDF datasets demonstrate the efficiency of our\nalgorithm, which is more scalable than baselines.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:55:24 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Li", "Shuxin", ""], ["Cheng", "Gong", ""], ["Li", "Chengkai", ""]]}, {"id": "2002.10198", "submitter": "Wei Ye", "authors": "Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang, Shikun\n  Zhang", "title": "Leveraging Code Generation to Improve Code Retrieval and Summarization\n  via Dual Learning", "comments": "Published at The Web Conference (WWW) 2020, full paper", "journal-ref": null, "doi": "10.1145/3366423.3380295", "report-no": null, "categories": "cs.IR cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summarization generates brief natural language description given a\nsource code snippet, while code retrieval fetches relevant source code given a\nnatural language query. Since both tasks aim to model the association between\nnatural language and programming language, recent studies have combined these\ntwo tasks to improve their performance. However, researchers have yet been able\nto effectively leverage the intrinsic connection between the two tasks as they\ntrain these tasks in a separate or pipeline manner, which means their\nperformance can not be well balanced. In this paper, we propose a novel\nend-to-end model for the two tasks by introducing an additional code generation\ntask. More specifically, we explicitly exploit the probabilistic correlation\nbetween code summarization and code generation with dual learning, and utilize\nthe two encoders for code summarization and code generation to train the code\nretrieval task via multi-task learning. We have carried out extensive\nexperiments on an existing dataset of SQL and Python, and results show that our\nmodel can significantly improve the results of the code retrieval task over\nthe-state-of-art models, as well as achieve competitive performance in terms of\nBLEU score for the code summarization task.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:26:11 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 08:49:11 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ye", "Wei", ""], ["Xie", "Rui", ""], ["Zhang", "Jinglei", ""], ["Hu", "Tianxiang", ""], ["Wang", "Xiaoyin", ""], ["Zhang", "Shikun", ""]]}, {"id": "2002.10233", "submitter": "Yanan Sun", "authors": "Yanan Sun, Ziyao Ren, Gary G. Yen, Bing Xue, Mengjie Zhang and\n  Jiancheng Lv", "title": "ArcText: A Unified Text Approach to Describing Convolutional Neural\n  Network Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superiority of Convolutional Neural Networks (CNNs) largely relies on\ntheir architectures that are often manually crafted with extensive human\nexpertise. Unfortunately, such kind of domain knowledge is not necessarily\nowned by each of the users interested. Data mining on existing CNN can discover\nuseful patterns and fundamental sub-comments from their architectures,\nproviding researchers with strong prior knowledge to design proper CNN\narchitectures when they have no expertise in CNNs. There have been various\nstate-of-the-art data mining algorithms at hand, while there is only rare work\nthat has been done for the mining. One of the main reasons is the gap between\nCNN architectures and data mining algorithms. Specifically, the current CNN\narchitecture descriptions cannot be exactly vectorized to the input of data\nmining algorithms. In this paper, we propose a unified approach, named ArcText,\nto describing CNN architectures based on text. Particularly, four different\nunits and an ordering method have been elaborately designed in ArcText, to\nuniquely describe the same architecture with sufficient information. Also, the\nresulted description can be exactly converted back to the corresponding CNN\narchitecture. ArcText bridges the gap between CNN architectures and data mining\nresearchers, and has the potentiality to be utilized to wider scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 17:17:16 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:59:39 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 08:17:06 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 08:43:12 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Sun", "Yanan", ""], ["Ren", "Ziyao", ""], ["Yen", "Gary G.", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2002.10241", "submitter": "Sujoy Chatterjee", "authors": "Sujoy Chatterjee, Nicolas Pasquier, Simon Nanty, Maria A. Zuluaga", "title": "Multi-objective Consensus Clustering Framework for Flight Search\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the travel industry, online customers book their travel itinerary\naccording to several features, like cost and duration of the travel or the\nquality of amenities. To provide personalized recommendations for travel\nsearches, an appropriate segmentation of customers is required. Clustering\nensemble approaches were developed to overcome well-known problems of classical\nclustering approaches, that each rely on a different theoretical model and can\nthus identify in the data space only clusters corresponding to this model.\nClustering ensemble approaches combine multiple clustering results, each from a\ndifferent algorithmic configuration, for generating more robust consensus\nclusters corresponding to agreements between initial clusters. We present a new\nclustering ensemble multi-objective optimization-based framework developed for\nanalyzing Amadeus customer search data and improve personalized\nrecommendations. This framework optimizes diversity in the clustering ensemble\nsearch space and automatically determines an appropriate number of clusters\nwithout requiring user's input. Experimental results compare the efficiency of\nthis approach with other existing approaches on Amadeus customer search data in\nterms of internal (Adjusted Rand Index) and external (Amadeus business metric)\nvalidations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 03:56:02 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 14:41:59 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Chatterjee", "Sujoy", ""], ["Pasquier", "Nicolas", ""], ["Nanty", "Simon", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2002.10294", "submitter": "Fateh Boucenna", "authors": "Fateh Boucenna", "title": "Semantic, Efficient, and Secure Search over Encrypted Cloud Data", "comments": "180 pages, PhD Thesis, University of Sciences and Technology Houari\n  Boumediene (USTHB) Algiers Algeria, searchable encryption, cloud computing,\n  semantic search, homomorphic encryption, data privacy, weighting formula", "journal-ref": null, "doi": null, "report-no": "01/2020-D/INF", "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies and individuals demand more and more storage space and computing\npower. For this purpose, several new technologies have been designed and\nimplemented, such as the cloud computing. This technology provides its users\nwith storage space and computing power according to their needs in a flexible\nand personalized way. However, the outsourced data such as emails, electronic\nhealth records, and company reports are sensitive and confidential. Therefore,\nIt is primordial to protect the outsourced data against possible external\nattacks and the cloud server itself. That is why it is highly recommended to\nencrypt the sensitive data before being outsourced to a remote server. To\nperform searches over outsourced data, it is no longer possible to exploit\ntraditional search engines given that these data are encrypted. Consequently,\nlots of searchable encryption (SE) schemes have been proposed in the\nliterature. Three major research axes of searchable encryption area have been\nstudied in the literature. The first axis consists in ensuring the security of\nthe search approach. Indeed, the search process should be performed without\ndecryption any data and without causing any sensitive information leakage. The\nsecond axis consists in studying the search performance. In fact, the encrypted\nindexes are less efficient than the plaintext indexes, which makes the\nsearchable encryption schemes very slow in practice. More the approach is\nsecure, less it is efficient, thus, the challenge consists in finding the best\ncompromise between security and performance. Finally, the third research axis\nconsists in the quality of the returned results in terms of relevance and\nrecall. The problem is that the encryption of the index causes the degradation\nof the recall and the precision. Therefore, the goal is to propose a technique\nthat is able to obtain almost the same result obtained in the traditional\nsearch.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:53:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Boucenna", "Fateh", ""]]}, {"id": "2002.10782", "submitter": "Martin Potthast", "authors": "Wei-Fan Chen, Shahbaz Syed, Benno Stein, Matthias Hagen, Martin\n  Potthast", "title": "Abstractive Snippet Generation", "comments": "Accepted by WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380206", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An abstractive snippet is an originally created piece of text to summarize a\nweb page on a search engine results page. Compared to the conventional\nextractive snippets, which are generated by extracting phrases and sentences\nverbatim from a web page, abstractive snippets circumvent copyright issues;\neven more interesting is the fact that they open the door for personalization.\nAbstractive snippets have been evaluated as equally powerful in terms of user\nacceptance and expressiveness---but the key question remains: Can abstractive\nsnippets be automatically generated with sufficient quality?\n  This paper introduces a new approach to abstractive snippet generation: We\nidentify the first two large-scale sources for distant supervision, namely\nanchor contexts and web directories. By mining the entire ClueWeb09 and\nClueWeb12 for anchor contexts and by utilizing the DMOZ Open Directory Project,\nwe compile the Webis Abstractive Snippet Corpus 2020, comprising more than 3.5\nmillion triples of the form $\\langle$query, snippet, document$\\rangle$ as\ntraining examples, where the snippet is either an anchor context or a web\ndirectory description in lieu of a genuine query-biased abstractive snippet of\nthe web document. We propose a bidirectional abstractive snippet generation\nmodel and assess the quality of both our corpus and the generated abstractive\nsnippets with standard measures, crowdsourcing, and in comparison to the state\nof the art. The evaluation shows that our novel data sources along with the\nproposed model allow for producing usable query-biased abstractive snippets\nwhile minimizing text reuse.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:36:17 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 22:50:12 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Wei-Fan", ""], ["Syed", "Shahbaz", ""], ["Stein", "Benno", ""], ["Hagen", "Matthias", ""], ["Potthast", "Martin", ""]]}, {"id": "2002.10943", "submitter": "Balaji Ganesan", "authors": "Lingraj S Vannur, Balaji Ganesan, Lokesh Nagalapatti, Hima Patel, MN\n  Thippeswamy", "title": "Data Augmentation for Personal Knowledge Base Population", "comments": "8 pages, 9 figures, 6 tables. under review. arXiv admin note: text\n  overlap with arXiv:2001.08013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold start knowledge base population (KBP) is the problem of populating a\nknowledge base from unstructured documents. While artificial neural networks\nhave led to significant improvements in the different tasks that are part of\nKBP, the overall F1 of the end-to-end system remains quite low. This problem is\nmore acute in personal knowledge bases, which present additional challenges\nwith regard to data protection, fairness and privacy. In this work, we present\na system that uses rule based annotators and a graph neural network for missing\nlink prediction, to populate a more complete, fair and diverse knowledge base\nfrom the TACRED dataset.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 07:39:55 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 06:31:40 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Vannur", "Lingraj S", ""], ["Ganesan", "Balaji", ""], ["Nagalapatti", "Lokesh", ""], ["Patel", "Hima", ""], ["Thippeswamy", "MN", ""]]}, {"id": "2002.11188", "submitter": "Sakib Ahmed", "authors": "Sakib Ahmed (1), Touseef Saleh Bin Ahmed (1), Sumaiya Jafreen (1),\n  Jannatul Tajrin (1) and Jia Uddin (1) ((1) BRAC University)", "title": "IoT Based Real Time Noise Mapping System for Urban Sound Pollution Study", "comments": "Appendix by Sakib Ahmed Accepted as Conference Paper at ICIEV and\n  icIVPR, 2018, Student Conference on Informatics, Electronics & Vision\n  (SCIEV): Paper ID 175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of a system that enables real time data\nvisualization via a webapp regarding sound intensity using multiple node\ndevices connected through internet. The prototypes were realized using\nATmega328 (Arduino Nano) and ESP8266 hardware modules, NodeMCU Arduino wrapper\nlibrary, Google maps and firebase API along with JavaScript webapp. System\narchitecture is such that multiple node devices will be installed in different\nlocations of the target area. On each node device, an Arduino Nano interfaced\nwith a Sound Sensor measures the ambient sound intensity and ESP8266 Wi-Fi\nmodule transmits the data to a database via web API. On the webapp, it plots\nall the real-time data from the devices over Google maps according to the\nlocations of the node devices. The logged data that is collected can then be\nused to carry out researches regarding sound pollution in targeted areas.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 21:53:06 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ahmed", "Sakib", "", "BRAC University"], ["Ahmed", "Touseef Saleh Bin", "", "BRAC University"], ["Jafreen", "Sumaiya", "", "BRAC University"], ["Tajrin", "Jannatul", "", "BRAC University"], ["Uddin", "Jia", "", "BRAC University"]]}, {"id": "2002.11252", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu,\n  Jiliang Tang", "title": "AutoEmb: Automated Embedding Dimensionality Search in Streaming\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based recommender systems (DLRSs) often have embedding layers,\nwhich are utilized to lessen the dimensionality of categorical variables (e.g.\nuser/item identifiers) and meaningfully transform them in the low-dimensional\nspace. The majority of existing DLRSs empirically pre-define a fixed and\nunified dimension for all user/item embeddings. It is evident from recent\nresearches that different embedding sizes are highly desired for different\nusers/items according to their popularity. However, manually selecting\nembedding sizes in recommender systems can be very challenging due to the large\nnumber of users/items and the dynamic nature of their popularity. Thus, in this\npaper, we propose an AutoML based end-to-end framework (AutoEmb), which can\nenable various embedding dimensions according to the popularity in an automated\nand dynamic manner. To be specific, we first enhance a typical DLRS to allow\nvarious embedding dimensions; then we propose an end-to-end differentiable\nframework that can automatically select different embedding dimensions\naccording to user/item popularity; finally we propose an AutoML based\noptimization algorithm in a streaming recommendation setting. The experimental\nresults based on widely used benchmark datasets demonstrate the effectiveness\nof the AutoEmb framework.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:36:01 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 22:15:55 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Wang", "Chong", ""], ["Chen", "Ming", ""], ["Zheng", "Xudong", ""], ["Liu", "Xiaobing", ""], ["Tang", "Jiliang", ""]]}, {"id": "2002.11589", "submitter": "Carolyn Kim", "authors": "Carolyn Kim, Mohsen Bayati", "title": "Recommendation on a Budget: Column Space Recovery from Partially\n  Observed Entries with Random or Active Sampling", "comments": "A shorter version is accepted to AISTATS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze alternating minimization for column space recovery of a partially\nobserved, approximately low rank matrix with a growing number of columns and a\nfixed budget of observations per column. In this work, we prove that if the\nbudget is greater than the rank of the matrix, column space recovery succeeds\n-- as the number of columns grows, the estimate from alternating minimization\nconverges to the true column space with probability tending to one. From our\nproof techniques, we naturally formulate an active sampling strategy for\nchoosing entries of a column that is theoretically and empirically (on\nsynthetic and real data) better than the commonly studied uniformly random\nsampling strategy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:17:05 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 00:14:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kim", "Carolyn", ""], ["Bayati", "Mohsen", ""]]}, {"id": "2002.11590", "submitter": "Emilio Leonardi", "authors": "Evgenia Christoforou, Alessandro Nordio, Alberto Tarable, Emilio\n  Leonardi", "title": "Ranking a set of objects: a graph based least-square approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of ranking $N$ objects starting from a set of noisy\npairwise comparisons provided by a crowd of equal workers. We assume that\nobjects are endowed with intrinsic qualities and that the probability with\nwhich an object is preferred to another depends only on the difference between\nthe qualities of the two competitors. We propose a class of non-adaptive\nranking algorithms that rely on a least-squares optimization criterion for the\nestimation of qualities. Such algorithms are shown to be asymptotically optimal\n(i.e., they require $O(\\frac{N}{\\epsilon^2}\\log \\frac{N}{\\delta})$ comparisons\nto be $(\\epsilon, \\delta)$-PAC). Numerical results show that our schemes are\nvery efficient also in many non-asymptotic scenarios exhibiting a performance\nsimilar to the maximum-likelihood algorithm. Moreover, we show how they can be\nextended to adaptive schemes and test them on real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:19:09 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Christoforou", "Evgenia", ""], ["Nordio", "Alessandro", ""], ["Tarable", "Alberto", ""], ["Leonardi", "Emilio", ""]]}, {"id": "2002.11844", "submitter": "Paul Sheridan", "authors": "Paul Sheridan and Mikael Onsj\\\"o", "title": "Fisher's exact test explains a popular metric in information retrieval", "comments": "26 pages, 4 figures, 1 tables, minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term frequency-inverse document frequency, or tf-idf for short, is a\nnumerical measure that is widely used in information retrieval to quantify the\nimportance of a term of interest in one out of many documents. While tf-idf was\noriginally proposed as a heuristic, much work has been devoted over the years\nto placing it on a solid theoretical foundation. Following in this tradition,\nwe here advance the first justification for tf-idf that is grounded in\nstatistical hypothesis testing. More precisely, we first show that the\none-tailed version of Fisher's exact test, also known as the hypergeometric\ntest, corresponds well with a common tf-idf variant on selected real-data\ninformation retrieval tasks. We then set forth a mathematical argument that\nsuggests the tf-idf variant approximates the negative logarithm of the\none-tailed Fisher's exact test P-value (i.e., a hypergeometric distribution\ntail probability). The Fisher's exact test interpretation of this common tf-idf\nvariant furnishes the working statistician with a ready explanation of tf-idf's\nlong-established effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 23:50:34 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 22:52:45 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Sheridan", "Paul", ""], ["Onsj\u00f6", "Mikael", ""]]}, {"id": "2002.11890", "submitter": "Bo Peng", "authors": "Bo Peng, Zhiyun Ren, Srinivasan Parthasarathy and Xia Ning", "title": "HAM: Hybrid Associations Models for Sequential Recommendation", "comments": "This paper has been accepted by IEEE Transactions on Knowledge and\n  Data Engineering (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation aims to identify and recommend the next few items\nfor a user that the user is most likely to purchase/review, given the user's\npurchase/rating trajectories. It becomes an effective tool to help users select\nfavorite items from a variety of options. In this manuscript, we developed\nhybrid associations models (HAM) to generate sequential recommendations using\nthree factors: 1) users' long-term preferences, 2) sequential, high-order and\nlow-order association patterns in the users' most recent purchases/ratings, and\n3) synergies among those items. HAM uses simplistic pooling to represent a set\nof items in the associations, and element-wise product to represent item\nsynergies of arbitrary orders. We compared HAM models with the most recent,\nstate-of-the-art methods on six public benchmark datasets in three different\nexperimental settings. Our experimental results demonstrate that HAM models\nsignificantly outperform the state of the art in all the experimental settings,\nwith an improvement as much as 46.6%. In addition, our run-time performance\ncomparison in testing demonstrates that HAM models are much more efficient than\nthe state-of-the-art methods, and are able to achieve significant speedup as\nmuch as 139.7 folds.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:04:16 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 21:09:36 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 14:14:41 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Peng", "Bo", ""], ["Ren", "Zhiyun", ""], ["Parthasarathy", "Srinivasan", ""], ["Ning", "Xia", ""]]}, {"id": "2002.12021", "submitter": "Felicitas L\\\"offler", "authors": "Felicitas L\\\"offler, Valentin Wesp, Birgitta K\\\"onig-Ries, Friederike\n  Klan", "title": "Dataset Search In Biodiversity Research: Do Metadata In Data\n  Repositories Reflect Scholarly Information Needs?", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0246099", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing amount of research data provides the opportunity to link and\nintegrate data to create novel hypotheses, to repeat experiments or to compare\nrecent data to data collected at a different time or place. However, recent\nstudies have shown that retrieving relevant data for data reuse is a\ntime-consuming task in daily research practice. In this study, we explore what\nhampers dataset retrieval in biodiversity research, a field that produces a\nlarge amount of heterogeneous data. We analyze the primary source in dataset\nsearch - metadata - and determine if they reflect scholarly search interests.\nWe examine if metadata standards provide elements corresponding to search\ninterests, we inspect if selected data repositories use metadata standards\nrepresenting scholarly interests, and we determine how many fields of the\nmetadata standards used are filled. To determine search interests in\nbiodiversity research, we gathered 169 questions that researchers aimed to\nanswer with the help of retrieved data, identified biological entities and\ngrouped them into 13 categories. Our findings indicate that environments,\nmaterials and chemicals, species, biological and chemical processes, locations,\ndata parameters and data types are important search interests in biodiversity\nresearch. The comparison with existing metadata standards shows that\ndomain-specific standards cover search interests quite well, whereas general\nstandards do not explicitly contain elements that reflect search interests. We\ninspect metadata from five large data repositories. Our results confirm that\nmetadata currently poorly reflect search interests in biodiversity research.\nFrom these findings, we derive recommendations for researchers and data\nrepositories how to bridge the gap between search interest and metadata\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:32:47 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["L\u00f6ffler", "Felicitas", ""], ["Wesp", "Valentin", ""], ["K\u00f6nig-Ries", "Birgitta", ""], ["Klan", "Friederike", ""]]}, {"id": "2002.12277", "submitter": "Meshal Alfarhood", "authors": "Meshal Alfarhood and Jianlin Cheng", "title": "CATA++: A Collaborative Dual Attentive Autoencoder Method for\n  Recommending Scientific Articles", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3029722", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems today have become an essential component of any\ncommercial website. Collaborative filtering approaches, and Matrix\nFactorization (MF) techniques in particular, are widely used in recommender\nsystems. However, the natural data sparsity problem limits their performance\nwhere users generally interact with very few items in the system. Consequently,\nmultiple hybrid models were proposed recently to optimize MF performance by\nincorporating additional contextual information in its learning process.\nAlthough these models improve the recommendation quality, there are two primary\naspects for further improvements: (1) multiple models focus only on some\nportion of the available contextual information and neglect other portions; (2)\nlearning the feature space of the side contextual information needs to be\nfurther enhanced. In this paper, we introduce a Collaborative Dual Attentive\nAutoencoder (CATA++) for recommending scientific articles. CATA++ utilizes an\narticle's content and learns its latent space via two parallel autoencoders. We\nemploy the attention mechanism to capture the most related parts of information\nin order to make more relevant recommendations. Extensive experiments on three\nreal-world datasets have shown that our dual-way learning strategy has\nsignificantly improved the MF performance in comparison with other\nstate-of-the-art MF-based models using various experimental evaluations. The\nsource code of our methods is available at:\nhttps://github.com/jianlin-cheng/CATA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 17:35:46 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 09:20:15 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Alfarhood", "Meshal", ""], ["Cheng", "Jianlin", ""]]}, {"id": "2002.12312", "submitter": "Liwei Wu", "authors": "Liwei Wu", "title": "Advances in Collaborative Filtering and Ranking", "comments": "PhD Dissertation 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation, we cover some recent advances in collaborative\nfiltering and ranking. In chapter 1, we give a brief introduction of the\nhistory and the current landscape of collaborative filtering and ranking;\nchapter 2 we first talk about pointwise collaborative filtering problem with\ngraph information, and how our proposed new method can encode very deep graph\ninformation which helps four existing graph collaborative filtering algorithms;\nchapter 3 is on the pairwise approach for collaborative ranking and how we\nspeed up the algorithm to near-linear time complexity; chapter 4 is on the new\nlistwise approach for collaborative ranking and how the listwise approach is a\nbetter choice of loss for both explicit and implicit feedback over pointwise\nand pairwise loss; chapter 5 is about the new regularization technique\nStochastic Shared Embeddings (SSE) we proposed for embedding layers and how it\nis both theoretically sound and empirically effectively for 6 different tasks\nacross recommendation and natural language processing; chapter 6 is how we\nintroduce personalization for the state-of-the-art sequential recommendation\nmodel with the help of SSE, which plays an important role in preventing our\npersonalized model from overfitting to the training data; chapter 7, we\nsummarize what we have achieved so far and predict what the future directions\ncan be; chapter 8 is the appendix to all the chapters.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:30:47 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wu", "Liwei", ""]]}, {"id": "2002.12456", "submitter": "Rom\\`an R. Zapatrin", "authors": "Roman Zapatrin", "title": "Quantifying daseinisation using Shannon entropy", "comments": "LaTeX, 8 pages, no figures", "journal-ref": "WSEAS Transactions on Systems, ISSN / E-ISSN: 1109-2777 /\n  2224-2678, Volume 19, 2020, Art. #12, pp. 82-85", "doi": "10.37394/23202.2020.19.12", "report-no": null, "categories": "quant-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topos formalism for quantum mechanics is interpreted in a broader,\ninformation retrieval perspective. Contexts, its basic components, are treated\nas sources of information. Their interplay, called daseinisation, defined in\npurely logical terms, is reformulated in terms of two relations: exclusion and\npreclusion of queries. Then, broadening these options, daseinisation becomes a\ncharacteristic of proximity of contexts; to quantify it numerically, Shannon\nentropy is used.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:00:25 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zapatrin", "Roman", ""]]}, {"id": "2002.12528", "submitter": "Yinxiao Li", "authors": "Yinxiao Li", "title": "Handling Position Bias for Unbiased Learning to Rank in Hotels Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, search ranking and recommendation systems rely on a lot of data to\ntrain machine learning models such as Learning-to-Rank (LTR) models to rank\nresults for a given query, and implicit user feedbacks (e.g. click data) have\nbecome the dominant source of data collection due to its abundance and low\ncost, especially for major Internet companies. However, a drawback of this data\ncollection approach is the data could be highly biased, and one of the most\nsignificant biases is the position bias, where users are biased towards\nclicking on higher ranked results. In this work, we will investigate the\nmarginal importance of properly handling the position bias in an online test\nenvironment in Tripadvisor Hotels search. We propose an empirically effective\nmethod of handling the position bias that fully leverages the user action data.\nWe take advantage of the fact that when user clicks a result, he has almost\ncertainly observed all the results above, and the propensities of the results\nbelow the clicked result will be estimated by a simple but effective position\nbias model. The online A/B test results show that this method leads to an\nimproved search ranking model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 03:48:42 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Li", "Yinxiao", ""]]}, {"id": "2002.12612", "submitter": "Francesco Pierri", "authors": "Francesco Pierri, Carlo Piccardi, Stefano Ceri", "title": "A multi-layer approach to disinformation detection on Twitter", "comments": "A revised version of this pre-print has been published on EPJ Data\n  Science with the title \"A multi-layer approach to disinformation detection in\n  US and Italian news spreading on Twitter\"", "journal-ref": "Published version on EPJ Data Science (\"A multi-layer approach to\n  disinformation detection in US and Italian news spreading on Twitter\") Dec\n  2020", "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of classifying news articles pertaining to\ndisinformation vs mainstream news by solely inspecting their diffusion\nmechanisms on Twitter. Our technique is inherently simple compared to existing\ntext-based approaches, as it allows to by-pass the multiple levels of\ncomplexity which are found in news content (e.g. grammar, syntax, style). We\nemploy a multi-layer representation of Twitter diffusion networks, and we\ncompute for each layer a set of global network features which quantify\ndifferent aspects of the sharing process. Experimental results with two\nlarge-scale datasets, corresponding to diffusion cascades of news shared\nrespectively in the United States and Italy, show that a simple Logistic\nRegression model is able to classify disinformation vs mainstream networks with\nhigh accuracy (AUROC up to 94%), also when considering the political bias of\ndifferent sources in the classification task. We also highlight differences in\nthe sharing patterns of the two news domains which appear to be\ncountry-independent. We believe that our network-based approach provides useful\ninsights which pave the way to the future development of a system to detect\nmisleading and harmful information spreading on social media.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:25:53 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 08:23:47 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Pierri", "Francesco", ""], ["Piccardi", "Carlo", ""], ["Ceri", "Stefano", ""]]}, {"id": "2002.12683", "submitter": "Jie Gao", "authors": "Jie Gao, Sooji Han, Xingyi Song, Fabio Ciravegna", "title": "RP-DNN: A Tweet level propagation context based deep neural networks for\n  early rumor detection in Social Media", "comments": "Manuscript accepted for publication at The LREC 2020 Proceedings. The\n  International Conference on Language Resources and Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early rumor detection (ERD) on social media platform is very challenging when\nlimited, incomplete and noisy information is available. Most of the existing\nmethods have largely worked on event-level detection that requires the\ncollection of posts relevant to a specific event and relied only on\nuser-generated content. They are not appropriate to detect rumor sources in the\nvery early stages, before an event unfolds and becomes widespread. In this\npaper, we address the task of ERD at the message level. We present a novel\nhybrid neural network architecture, which combines a task-specific\ncharacter-based bidirectional language model and stacked Long Short-Term Memory\n(LSTM) networks to represent textual contents and social-temporal contexts of\ninput source tweets, for modelling propagation patterns of rumors in the early\nstages of their development. We apply multi-layered attention models to jointly\nlearn attentive context embeddings over multiple context inputs. Our\nexperiments employ a stringent leave-one-out cross-validation (LOO-CV)\nevaluation setup on seven publicly available real-life rumor event data sets.\nOur models achieve state-of-the-art(SoA) performance for detecting unseen\nrumors on large augmented data which covers more than 12 events and 2,967\nrumors. An ablation study is conducted to understand the relative contribution\nof each component of our proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:44:34 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 10:47:53 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Gao", "Jie", ""], ["Han", "Sooji", ""], ["Song", "Xingyi", ""], ["Ciravegna", "Fabio", ""]]}]