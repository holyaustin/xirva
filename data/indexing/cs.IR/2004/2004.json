[{"id": "2004.00061", "submitter": "Marco Valentino", "authors": "Marco Valentino, Mokanarangan Thayaparan, Andr\\'e Freitas", "title": "Unification-based Reconstruction of Multi-hop Explanations for Science\n  Questions", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for reconstructing multi-hop\nexplanations in science Question Answering (QA). While existing approaches for\nmulti-hop reasoning build explanations considering each question in isolation,\nwe propose a method to leverage explanatory patterns emerging in a corpus of\nscientific explanations. Specifically, the framework ranks a set of atomic\nfacts by integrating lexical relevance with the notion of unification power,\nestimated analysing explanations for similar questions in the corpus.\n  An extensive evaluation is performed on the Worldtree corpus, integrating\nk-NN clustering and Information Retrieval (IR) techniques. We present the\nfollowing conclusions: (1) The proposed method achieves results competitive\nwith Transformers, yet being orders of magnitude faster, a feature that makes\nit scalable to large explanatory corpora (2) The unification-based mechanism\nhas a key role in reducing semantic drift, contributing to the reconstruction\nof many hops explanations (6 or more facts) and the ranking of complex\ninference facts (+12.0 Mean Average Precision) (3) Crucially, the constructed\nexplanations can support downstream QA models, improving the accuracy of BERT\nby up to 10% overall.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:07:51 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 09:32:05 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Valentino", "Marco", ""], ["Thayaparan", "Mokanarangan", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2004.00071", "submitter": "Nidhi Rastogi", "authors": "Nidhi Rastogi and Mohammed J. Zaki", "title": "Personal Health Knowledge Graphs for Patients", "comments": "3 pages, workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing patient data analytics platforms fail to incorporate information\nthat has context, is personal, and topical to patients. For a recommendation\nsystem to give a suitable response to a query or to derive meaningful insights\nfrom patient data, it should consider personal information about the patient's\nhealth history, including but not limited to their preferences, locations, and\nlife choices that are currently applicable to them. In this review paper, we\ncritique existing literature in this space and also discuss the various\nresearch challenges that come with designing, building, and operationalizing a\npersonal health knowledge graph (PHKG) for patients.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:35:14 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 10:40:21 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Rastogi", "Nidhi", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "2004.00150", "submitter": "Mohammed Ibrahim", "authors": "Mohammed Ibrahim, Susan Gauch, Omar Salman, Mohammed Alqahatani", "title": "Enriching Consumer Health Vocabulary Using Enhanced GloVe Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-Access and Collaborative Consumer Health Vocabulary (OAC CHV, or CHV for\nshort), is a collection of medical terms written in plain English. It provides\na list of simple, easy, and clear terms that laymen prefer to use rather than\nan equivalent professional medical term. The National Library of Medicine (NLM)\nhas integrated and mapped the CHV terms to their Unified Medical Language\nSystem (UMLS). These CHV terms mapped to 56000 professional concepts on the\nUMLS. We found that about 48% of these laymen's terms are still jargon and\nmatched with the professional terms on the UMLS. In this paper, we present an\nenhanced word embedding technique that generates new CHV terms from a\nconsumer-generated text. We downloaded our corpus from a healthcare social\nmedia and evaluated our new method based on iterative feedback to word\nembedding using ground truth built from the existing CHV terms. Our feedback\nalgorithm outperformed unmodified GLoVe and new CHV terms have been detected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 22:50:24 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 18:02:10 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ibrahim", "Mohammed", ""], ["Gauch", "Susan", ""], ["Salman", "Omar", ""], ["Alqahatani", "Mohammed", ""]]}, {"id": "2004.00197", "submitter": "Lei Zhu", "authors": "Tong Wang, Lei Zhu, Zhiyong Cheng, Jingjing Li, and Huaxiang Zhang", "title": "Task-adaptive Asymmetric Deep Cross-modal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised cross-modal hashing aims to embed the semantic correlations of\nheterogeneous modality data into the binary hash codes with discriminative\nsemantic labels. Because of its advantages on retrieval and storage efficiency,\nit is widely used for solving efficient cross-modal retrieval. However,\nexisting researches equally handle the different tasks of cross-modal\nretrieval, and simply learn the same couple of hash functions in a symmetric\nway for them. Under such circumstance, the uniqueness of different cross-modal\nretrieval tasks are ignored and sub-optimal performance may be brought.\nMotivated by this, we present a Task-adaptive Asymmetric Deep Cross-modal\nHashing (TA-ADCMH) method in this paper. It can learn task-adaptive hash\nfunctions for two sub-retrieval tasks via simultaneous modality representation\nand asymmetric hash learning. Unlike previous cross-modal hashing approaches,\nour learning framework jointly optimizes semantic preserving that transforms\ndeep features of multimedia data into binary hash codes, and the semantic\nregression which directly regresses query modality representation to explicit\nlabel. With our model, the binary codes can effectively preserve semantic\ncorrelations across different modalities, meanwhile, adaptively capture the\nquery semantics. The superiority of TA-ADCMH is proved on two standard datasets\nfrom many aspects.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 02:09:20 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wang", "Tong", ""], ["Zhu", "Lei", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Zhang", "Huaxiang", ""]]}, {"id": "2004.00267", "submitter": "Noemi Mauro", "authors": "Liliana Ardissono, Matteo Delsanto, Maurizio Lucenteforte, Noemi\n  Mauro, Adriano Savoca and Daniele Scanu", "title": "Map-Based Visualization of 2D/3D Spatial Data via Stylization and Tuning\n  of Information Emphasis", "comments": null, "journal-ref": "Proceedings of the 2018 International Conference on Advanced\n  Visual Interfaces (AVI 2018)", "doi": "10.1145/3206505.3206516", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Geographical Information search, map visualization can challenge the user\nbecause results can consist of a large set of heterogeneous items, increasing\nvisual complexity. We propose a novel visualization model to address this\nissue. Our model represents results as markers, or as geometric objects, on\n2D/3D layers, using stylized and highly colored shapes to enhance their\nvisibility. Moreover, the model supports interactive information filtering in\nthe map by enabling the user to focus on different data categories, using\ntransparency sliders to tune the opacity, and thus the emphasis, of the\ncorresponding data items. A test with users provided positive results\nconcerning the efficacy of the model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 07:48:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Ardissono", "Liliana", ""], ["Delsanto", "Matteo", ""], ["Lucenteforte", "Maurizio", ""], ["Mauro", "Noemi", ""], ["Savoca", "Adriano", ""], ["Scanu", "Daniele", ""]]}, {"id": "2004.00291", "submitter": "Axel Mascaro Mr", "authors": "Axel Mascaro and Christophe Rey", "title": "Recommandation ontologique multicrit\\`ere pour la m\\'etrologie", "comments": "9 pages, in French, for conference RJCIA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matchmaking and information ranking are helping process for users, by\noffering them the best answers possible at their request. When there is no\nexact answer, giving them the closest proposition available is an efficient\nupgrade of that helping process. With a reasearch platform on metrology as a\nframework, we will discuss about ranking with knowledge representation, with an\napproach based on Description Logic, ontologies and multricriteria comparison.\nWe present a reasonning to compare each proposition with the other, with\nsemantic and syntaxic difference, by troncating the information in distinct\ncomponent.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:54:55 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 08:51:07 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 09:59:17 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2020 11:36:41 GMT"}, {"version": "v5", "created": "Mon, 25 May 2020 05:56:17 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Mascaro", "Axel", ""], ["Rey", "Christophe", ""]]}, {"id": "2004.00293", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono, Laura Di Rocco, Michela Bertolotto and\n  Giovanna Guerrini", "title": "Impact of Semantic Granularity on Geographic Information Search Support", "comments": null, "journal-ref": "2018 IEEE/WIC/ACM International Conference on Web Intelligence\n  (WI)", "doi": "10.1109/WI.2018.00-73", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Information Retrieval research has used semantics to provide accurate\nsearch results, but the analysis of conceptual abstraction has mainly focused\non information integration. We consider session-based query expansion in\nGeographical Information Retrieval, and investigate the impact of semantic\ngranularity (i.e., specificity of concepts representation) on the suggestion of\nrelevant types of information to search for. We study how different levels of\ndetail in knowledge representation influence the capability of guiding the user\nin the exploration of a complex information space. A comparative analysis of\nthe performance of a query expansion model, using three spatial ontologies\ndefined at different semantic granularity levels, reveals that a fine-grained\nrepresentation enhances recall. However, precision depends on how closely the\nontologies match the way people conceptualize and verbally describe the\ngeographic space.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:58:25 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Di Rocco", "Laura", ""], ["Bertolotto", "Michela", ""], ["Guerrini", "Giovanna", ""]]}, {"id": "2004.00362", "submitter": "Kisor Sahu Dr.", "authors": "Ajay K. Gogineni, S. Swayamjyoti, Devadatta Sahoo, Kisor K. Sahu, Raj\n  kishore", "title": "Multi-Class classification of vulnerabilities in Smart Contracts using\n  AWD-LSTM, with pre-trained encoder inspired from natural language processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vulnerability detection and safety of smart contracts are of paramount\nimportance because of their immutable nature. Symbolic tools like OYENTE and\nMAIAN are typically used for vulnerability prediction in smart contracts. As\nthese tools are computationally expensive, they are typically used to detect\nvulnerabilities until some predefined invocation depth. These tools require\nmore search time as the invocation depth increases. Since the number of smart\ncontracts is increasing exponentially, it is difficult to analyze the contracts\nusing these traditional tools. Recently a machine learning technique called\nLong Short Term Memory (LSTM) has been used for binary classification, i.e., to\npredict whether a smart contract is vulnerable or not. This technique requires\nnearly constant search time as the invocation depth increases. In the present\narticle, we have shown a multi-class classification, where we classify a smart\ncontract in Suicidal, Prodigal, Greedy, or Normal categories. We used Average\nStochastic Gradient Descent Weight-Dropped LSTM (AWD-LSTM), which is a variant\nof LSTM, to perform classification. We reduced the class imbalance (a large\nnumber of normal contracts as compared to other categories) by considering only\nthe distinct opcode combination for normal contracts. We have achieved a\nweighted average Fbeta score of 90.0%. Hence, such techniques can be used to\nanalyze a large number of smart contracts and help to improve the security of\nthese contracts.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 20:48:09 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gogineni", "Ajay K.", ""], ["Swayamjyoti", "S.", ""], ["Sahoo", "Devadatta", ""], ["Sahu", "Kisor K.", ""], ["kishore", "Raj", ""]]}, {"id": "2004.00384", "submitter": "Dongdong Yang", "authors": "Dongdong Yang, Kevin Dyer, Senzhang Wang", "title": "Interpretable Deep Learning Model for Online Multi-touch Attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, users may be exposed to a range of different\nadvertising campaigns, such as natural search or referral or organic search,\nbefore leading to a final transaction. Estimating the contribution of\nadvertising campaigns on the user's journey is very meaningful and crucial. A\nmarketer could observe each customer's interaction with different marketing\nchannels and modify their investment strategies accordingly. Existing methods\nincluding both traditional last-clicking methods and recent data-driven\napproaches for the multi-touch attribution (MTA) problem lack enough\ninterpretation on why the methods work. In this paper, we propose a novel model\ncalled DeepMTA, which combines deep learning model and additive feature\nexplanation model for interpretable online multi-touch attribution. DeepMTA\nmainly contains two parts, the phased-LSTMs based conversion prediction model\nto catch different time intervals, and the additive feature attribution model\ncombined with shaley values. Additive feature attribution is explanatory that\ncontains a linear function of binary variables. As the first interpretable deep\nlearning model for MTA, DeepMTA considers three important features in the\ncustomer journey: event sequence order, event frequency and time-decay effect\nof the event. Evaluation on a real dataset shows the proposed conversion\nprediction model achieves 91\\% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 23:21:40 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yang", "Dongdong", ""], ["Dyer", "Kevin", ""], ["Wang", "Senzhang", ""]]}, {"id": "2004.00387", "submitter": "Yang Gao", "authors": "Yang Gao, Yi-Fan Li, Yu Lin, Hang Gao, Latifur Khan", "title": "Deep Learning on Knowledge Graph for Recommender System: A Survey", "comments": "6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in research have demonstrated the effectiveness of knowledge\ngraphs (KG) in providing valuable external knowledge to improve recommendation\nsystems (RS). A knowledge graph is capable of encoding high-order relations\nthat connect two objects with one or multiple related attributes. With the help\nof the emerging Graph Neural Networks (GNN), it is possible to extract both\nobject characteristics and relations from KG, which is an essential factor for\nsuccessful recommendations. In this paper, we provide a comprehensive survey of\nthe GNN-based knowledge-aware deep recommender systems. Specifically, we\ndiscuss the state-of-the-art frameworks with a focus on their core component,\ni.e., the graph embedding module, and how they address practical recommendation\nissues such as scalability, cold-start and so on. We further summarize the\ncommonly-used benchmark datasets, evaluation metrics as well as open-source\ncodes. Finally, we conclude the survey and propose potential research\ndirections in this rapidly growing field.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 22:53:14 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gao", "Yang", ""], ["Li", "Yi-Fan", ""], ["Lin", "Yu", ""], ["Gao", "Hang", ""], ["Khan", "Latifur", ""]]}, {"id": "2004.00430", "submitter": "Vithya Yogarajan", "authors": "Vithya Yogarajan, Jacob Montiel, Tony Smith, Bernhard Pfahringer", "title": "Seeing The Whole Patient: Using Multi-Label Medical Text Classification\n  Techniques to Enhance Predictions of Medical Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning-based multi-label medical text classifications can be used\nto enhance the understanding of the human body and aid the need for patient\ncare. We present a broad study on clinical natural language processing\ntechniques to maximise a feature representing text when predicting medical\ncodes on patients with multi-morbidity. We present results of multi-label\nmedical text classification problems with 18, 50 and 155 labels. We compare\nseveral variations to embeddings, text tagging, and pre-processing. For\nimbalanced data we show that labels which occur infrequently, benefit the most\nfrom additional features incorporated in embeddings. We also show that high\ndimensional embeddings pre-trained using health-related data present a\nsignificant improvement in a multi-label setting, similarly to the way they\nimprove performance for binary classification. High dimensional embeddings from\nthis research are made available for public use.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 02:19:30 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yogarajan", "Vithya", ""], ["Montiel", "Jacob", ""], ["Smith", "Tony", ""], ["Pfahringer", "Bernhard", ""]]}, {"id": "2004.00646", "submitter": "Dietmar Jannach", "authors": "Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen", "title": "A Survey on Conversational Recommender Systems", "comments": "35 pages, 5 figures", "journal-ref": "ACM Computing Surveys, Volume 54, Issue 5, 2021", "doi": "10.1145/3453154", "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are software applications that help users to find items\nof interest in situations of information overload. Current research often\nassumes a one-shot interaction paradigm, where the users' preferences are\nestimated based on past observed behavior and where the presentation of a\nranked list of suggestions is the main, one-directional form of user\ninteraction. Conversational recommender systems (CRS) take a different approach\nand support a richer set of interactions. These interactions can, for example,\nhelp to improve the preference elicitation process or allow the user to ask\nquestions about the recommendations and to give feedback. The interest in CRS\nhas significantly increased in the past few years. This development is mainly\ndue to the significant progress in the area of natural language processing, the\nemergence of new voice-controlled home assistants, and the increased use of\nchatbot technology. With this paper, we provide a detailed survey of existing\napproaches to conversational recommendation. We categorize these approaches in\nvarious dimensions, e.g., in terms of the supported user intents or the\nknowledge they use in the background. Moreover, we discuss technological\napproaches, review how CRS are evaluated, and finally identify a number of gaps\nthat deserve more research in the future.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:00:47 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 06:16:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jannach", "Dietmar", ""], ["Manzoor", "Ahtsham", ""], ["Cai", "Wanling", ""], ["Chen", "Li", ""]]}, {"id": "2004.00698", "submitter": "Erik Quintanilla", "authors": "Erik Quintanilla, Yogesh Rawat, Andrey Sakryukin, Mubarak Shah, Mohan\n  Kankanhalli", "title": "Adversarial Learning for Personalized Tag Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently seen great progress in image classification due to the\nsuccess of deep convolutional neural networks and the availability of\nlarge-scale datasets. Most of the existing work focuses on single-label image\nclassification. However, there are usually multiple tags associated with an\nimage. The existing works on multi-label classification are mainly based on lab\ncurated labels. Humans assign tags to their images differently, which is mainly\nbased on their interests and personal tagging behavior. In this paper, we\naddress the problem of personalized tag recommendation and propose an\nend-to-end deep network which can be trained on large-scale datasets. The\nuser-preference is learned within the network in an unsupervised way where the\nnetwork performs joint optimization for user-preference and visual encoding. A\njoint training of user-preference and visual encoding allows the network to\nefficiently integrate the visual preference with tagging behavior for a better\nuser recommendation. In addition, we propose the use of adversarial learning,\nwhich enforces the network to predict tags resembling user-generated tags. We\ndemonstrate the effectiveness of the proposed model on two different\nlarge-scale and publicly available datasets, YFCC100M and NUS-WIDE. The\nproposed method achieves significantly better performance on both the datasets\nwhen compared to the baselines and other state-of-the-art methods. The code is\npublicly available at https://github.com/vyzuer/ALTReco.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 20:41:41 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Quintanilla", "Erik", ""], ["Rawat", "Yogesh", ""], ["Sakryukin", "Andrey", ""], ["Shah", "Mubarak", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "2004.01646", "submitter": "Bo Peng", "authors": "Bo Peng, Zhiyun Ren, Srinivasan Parthasarathy and Xia Ning", "title": "M2: Mixed Models with Preferences, Popularities and Transitions for\n  Next-Basket Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-basket recommendation considers the problem of recommending a set of\nitems into the next basket that users will purchase as a whole. In this paper,\nwe develop a novel mixed model with preferences, popularities and transitions\n(M2) for next-basket recommendation. This method explicitly models three\nimportant factors in next-basket generation process: 1) users' general\npreferences, 2) items' global popularities and 3) transition patterns among\nitems. We also propose a simple encoder-decoder based framework (ed-Trans) to\nbetter model the transition patterns among items. We compared M2 with 5\nstate-of-the-art next-basket recommendation methods on 4 public benchmark\ndatasets. Our experimental results demonstrate that M2 significantly\noutperforms the state-of-the-art methods on all the datasets, with an\nimprovement as much as 19.0% at recall@5. We also compared M2 with these\nbaseline methods in recommending the second next and third next baskets. Our\nexperimental results demonstrate that M2 could consistently outperform the\nbaseline methods in all these tasks, with an improvement as much as 14.4% at\nrecall@5. In addition, we conducted a comprehensive ablation study to verify\nthe effects of the different factors. The results show that learning all the\nfactors together could significantly improve the recommendation performance\ncompared to learning each of them alone. The results also show that ed-Trans in\nlearning item transitions among baskets could outperform recurrent neural\nnetwork-based methods on the benchmark datasets, with an improvement as much as\n20.4% at recall@5. We also have a thorough discussion on various experimental\nprotocols and evaluation metrics for next-basket recommendation evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:11:26 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 15:32:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Peng", "Bo", ""], ["Ren", "Zhiyun", ""], ["Parthasarathy", "Srinivasan", ""], ["Ning", "Xia", ""]]}, {"id": "2004.01862", "submitter": "Pengtao Xie", "authors": "Yuxiao Liang, Pengtao Xie", "title": "Identifying Radiological Findings Related to COVID-19 from Medical\n  Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has infected more than one million\nindividuals all over the world and caused more than 55,000 deaths, as of April\n3 in 2020. Radiological findings are important sources of information in\nguiding the diagnosis and treatment of COVID-19. However, the existing studies\non how radiological findings are correlated with COVID-19 are conducted\nseparately by different hospitals, which may be inconsistent or even\nconflicting due to population bias. To address this problem, we develop natural\nlanguage processing methods to analyze a large collection of COVID-19\nliterature containing study reports from hospitals all over the world,\nreconcile these results, and draw unbiased and universally-sensible conclusions\nabout the correlation between radiological findings and COVID-19. We apply our\nmethod to the CORD-19 dataset and successfully extract a set of radiological\nfindings that are closely tied to COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:33:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liang", "Yuxiao", ""], ["Xie", "Pengtao", ""]]}, {"id": "2004.01907", "submitter": "Dianbo Sui", "authors": "Dianbo Sui, Yubo Chen, Binjie Mao, Delai Qiu, Kang Liu and Jun Zhao", "title": "Knowledge Guided Metric Learning for Few-Shot Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of deep-learning-based text classification models relies heavily\non a huge amount of annotation data, which is difficult to obtain. When the\nlabeled data is scarce, models tend to struggle to achieve satisfactory\nperformance. However, human beings can distinguish new categories very\nefficiently with few examples. This is mainly due to the fact that human beings\ncan leverage knowledge obtained from relevant tasks. Inspired by human\nintelligence, we propose to introduce external knowledge into few-shot learning\nto imitate human knowledge. A novel parameter generator network is investigated\nto this end, which is able to use the external knowledge to generate relation\nnetwork parameters. Metrics can be transferred among tasks when equipped with\nthese generated parameters, so that similar tasks use similar metrics while\ndifferent tasks use different metrics. Through experiments, we demonstrate that\nour method outperforms the state-of-the-art few-shot text classification\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 10:56:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sui", "Dianbo", ""], ["Chen", "Yubo", ""], ["Mao", "Binjie", ""], ["Qiu", "Delai", ""], ["Liu", "Kang", ""], ["Zhao", "Jun", ""]]}, {"id": "2004.02023", "submitter": "Rishiraj Saha Roy", "authors": "Asia J. Biega, Jana Schmidt, Rishiraj Saha Roy", "title": "Towards Query Logs for Privacy Studies: On Deriving Search Queries from\n  Questions", "comments": "ECIR 2020 Short Paper", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_14", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating verbose information needs into crisp search queries is a\nphenomenon that is ubiquitous but hardly understood. Insights into this process\ncould be valuable in several applications, including synthesizing large\nprivacy-friendly query logs from public Web sources which are readily available\nto the academic research community. In this work, we take a step towards\nunderstanding query formulation by tapping into the rich potential of community\nquestion answering (CQA) forums. Specifically, we sample natural language (NL)\nquestions spanning diverse themes from the Stack Exchange platform, and conduct\na large-scale conversion experiment where crowdworkers submit search queries\nthey would use when looking for equivalent information. We provide a careful\nanalysis of this data, accounting for possible sources of bias during\nconversion, along with insights into user-specific linguistic patterns and\nsearch behaviors. We release a dataset of 7,000 question-query pairs from this\nstudy to facilitate further research on query understanding.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:24:52 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 18:53:21 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 10:16:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Biega", "Asia J.", ""], ["Schmidt", "Jana", ""], ["Roy", "Rishiraj Saha", ""]]}, {"id": "2004.02085", "submitter": "Sabber Ahamed", "authors": "Sabber Ahamed and Manar Samad", "title": "Information Mining for COVID-19 Research From a Large Volume of\n  Scientific Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The year 2020 has seen an unprecedented COVID-19 pandemic due to the outbreak\nof a novel strain of coronavirus in 180 countries. In a desperate effort to\ndiscover new drugs and vaccines for COVID-19, many scientists are working\naround the clock. Their valuable time and effort may benefit from\ncomputer-based mining of a large volume of health science literature that is a\ntreasure trove of information. In this paper, we have developed a graph-based\nmodel using abstracts of 10,683 scientific articles to find key information on\nthree topics: transmission, drug types, and genome research related to\ncoronavirus. A subgraph is built for each of the three topics to extract more\ntopic-focused information. Within each subgraph, we use a betweenness\ncentrality measurement to rank order the importance of keywords related to\ndrugs, diseases, pathogens, hosts of pathogens, and biomolecules. The results\nreveal intriguing information about antiviral drugs (Chloroquine, Amantadine,\nDexamethasone), pathogen-hosts (pigs, bats, macaque, cynomolgus), viral\npathogens (zika, dengue, malaria, and several viruses in the coronaviridae\nvirus family), and proteins and therapeutic mechanisms (oligonucleotide,\ninterferon, glycoprotein) in connection with the core topic of coronavirus. The\ncategorical summary of these keywords and topics may be a useful reference to\nexpedite and recommend new and alternative directions for COVID-19 research.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 03:51:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ahamed", "Sabber", ""], ["Samad", "Manar", ""]]}, {"id": "2004.02167", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov, Kishor Patil, Gugan Thoppe", "title": "Change Rate Estimation and Optimal Freshness in Web Page Crawling", "comments": "This paper has been accepted to the 13th EAI International Conference\n  on Performance Evaluation Methodologies and Tools, VALUETOOLS'20, May 18--20,\n  2020, Tsukuba, Japan. This is the author version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For providing quick and accurate results, a search engine maintains a local\nsnapshot of the entire web. And, to keep this local cache fresh, it employs a\ncrawler for tracking changes across various web pages. However, finite\nbandwidth availability and server restrictions impose some constraints on the\ncrawling frequency. Consequently, the ideal crawling rates are the ones that\nmaximise the freshness of the local cache and also respect the above\nconstraints. Azar et al. 2018 recently proposed a tractable algorithm to solve\nthis optimisation problem. However, they assume the knowledge of the exact page\nchange rates, which is unrealistic in practice. We address this issue here.\nSpecifically, we provide two novel schemes for online estimation of page change\nrates. Both schemes only need partial information about the page change\nprocess, i.e., they only need to know if the page has changed or not since the\nlast crawled instance. For both these schemes, we prove convergence and, also,\nderive their convergence rates. Finally, we provide some numerical experiments\nto compare the performance of our proposed estimators with the existing ones\n(e.g., MLE).\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 11:48:38 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Patil", "Kishor", ""], ["Thoppe", "Gugan", ""]]}, {"id": "2004.02184", "submitter": "Hossein A. Rahmani", "authors": "Mahdi Dehghan, Hossein A. Rahmani, Ahmad Ali Abin, Viet-Vu Vu", "title": "Mining Shape of Expertise: A Novel Approach Based on Convolutional\n  Neural Network", "comments": "IP&M 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert finding addresses the task of retrieving and ranking talented people\non the subject of user query. It is a practical issue in the Community Question\nAnswering networks. Recruiters looking for knowledgeable people for their job\npositions are the most important clients of expert finding systems. In addition\nto employee expertise, the cost of hiring new staff is another significant\nconcern for organizations. An efficient solution to cope with this concern is\nto hire T-shaped experts that are cost-effective. In this study, we have\nproposed a new deep model for T-shaped experts finding based on Convolutional\nNeural Networks. The proposed model tries to match queries and users by\nextracting local and position-invariant features from their corresponding\ndocuments. In other words, it detects users' shape of expertise by learning\npatterns from documents of users and queries simultaneously. The proposed model\ncontains two parallel CNN's that extract latent vectors of users and queries\nbased on their corresponding documents and join them together in the last layer\nto match queries with users. Experiments on a large subset of Stack Overflow\ndocuments indicate the effectiveness of the proposed method against baselines\nin terms of NDCG, MRR, and ERR evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 12:44:26 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Dehghan", "Mahdi", ""], ["Rahmani", "Hossein A.", ""], ["Abin", "Ahmad Ali", ""], ["Vu", "Viet-Vu", ""]]}, {"id": "2004.02256", "submitter": "K.R. Chowdhary", "authors": "K. R. Chowdhary", "title": "Natural language processing for word sense disambiguation and\n  information extraction", "comments": "150 pages, PhD Thesis", "journal-ref": null, "doi": null, "report-no": "cse-mbm-krc-p-thesis-04", "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research work deals with Natural Language Processing (NLP) and\nextraction of essential information in an explicit form. The most common among\nthe information management strategies is Document Retrieval (DR) and\nInformation Filtering. DR systems may work as combine harvesters, which bring\nback useful material from the vast fields of raw material. With large amount of\npotentially useful information in hand, an Information Extraction (IE) system\ncan then transform the raw material by refining and reducing it to a germ of\noriginal text. A Document Retrieval system collects the relevant documents\ncarrying the required information, from the repository of texts. An IE system\nthen transforms them into information that is more readily digested and\nanalyzed. It isolates relevant text fragments, extracts relevant information\nfrom the fragments, and then arranges together the targeted information in a\ncoherent framework. The thesis presents a new approach for Word Sense\nDisambiguation using thesaurus. The illustrative examples supports the\neffectiveness of this approach for speedy and effective disambiguation. A\nDocument Retrieval method, based on Fuzzy Logic has been described and its\napplication is illustrated. A question-answering system describes the operation\nof information extraction from the retrieved text documents. The process of\ninformation extraction for answering a query is considerably simplified by\nusing a Structured Description Language (SDL) which is based on cardinals of\nqueries in the form of who, what, when, where and why. The thesis concludes\nwith the presentation of a novel strategy based on Dempster-Shafer theory of\nevidential reasoning, for document retrieval and information extraction. This\nstrategy permits relaxation of many limitations, which are inherent in Bayesian\nprobabilistic approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 17:13:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chowdhary", "K. R.", ""]]}, {"id": "2004.02330", "submitter": "Pietro Ghezzi", "authors": "Arthur Cassa Macedo (1), Andr\\'e Oliveira Vilela de Faria (1),\n  Isabella Bizzi (2), Fabr\\'icio A. Moreira (1), Alessandro Colasanti (3) and\n  Pietro Ghezzi (3) ((1) Faculdade de Medicina, Universidade Federal de Minas\n  Gerais, Belo Horizonte, Brazil, (2) Universidade Federal do Rio Grande do\n  Sul, Porto Alegre, Brazil, (3) Brighton & Sussex Medical School, Brighton,\n  United Kingdom)", "title": "Online information on medical cannabis may rise unrealistic expectations\n  and downplay potential side effects", "comments": "22 pages, 1 table, 1 supplementary table, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing literature on the potential medical uses of Cannabis\nsativa and cannabinoid compounds. Although these have only been approved by\nregulatory agencies for few indications, there is a hype about their possible\nbenefits in a variety of conditions and a large market in the wellness\nindustry. As in many cases patients search for information on cannabis products\nonline, we have analyzed the information on medical cannabis available on the\nInternet. Analyzing 176 webpages returned by a search engine, we found that\nmore than half of them were news websites. Pain, epilepsy and multiple\nsclerosis were the most frequently therapeutic areas mentioned by the webpages,\nwhich did not always match those for which there is regulatory approval.\nInformation was also incomplete, with only 22% of the webpages mentioning\npotential side effects. Health portal websites provided the most complete\ninformation. On average, 80% of webpages had a neutral stance on the potential\nbenefits of medical cannabis, with commercial websites having more frequently a\npositive stance (67%). We conclude that the information that can be found\nonline could raise unrealistic expectations regarding therapeutic areas for\nwhich science-based evidence is often still weak.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:08:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Macedo", "Arthur Cassa", ""], ["de Faria", "Andr\u00e9 Oliveira Vilela", ""], ["Bizzi", "Isabella", ""], ["Moreira", "Fabr\u00edcio A.", ""], ["Colasanti", "Alessandro", ""], ["Ghezzi", "Pietro", ""]]}, {"id": "2004.02340", "submitter": "Junliang Yu", "authors": "Junliang Yu, Hongzhi Yin, Jundong Li, Min Gao, Zi Huang, Lizhen Cui", "title": "Enhancing Social Recommendation with Adversarial Graph Convolutional\n  Networks", "comments": "Accepted by TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social recommender systems are expected to improve recommendation quality by\nincorporating social information when there is little user-item interaction\ndata. However, recent reports from industry show that social recommender\nsystems consistently fail in practice. According to the negative findings, the\nfailure is attributed to: (1) A majority of users only have a very limited\nnumber of neighbors in social networks and can hardly benefit from social\nrelations; (2) Social relations are noisy but they are indiscriminately used;\n(3) Social relations are assumed to be universally applicable to multiple\nscenarios while they are actually multi-faceted and show heterogeneous\nstrengths in different scenarios. Most existing social recommendation models\nonly consider the homophily in social networks and neglect these drawbacks. In\nthis paper we propose a deep adversarial framework based on graph convolutional\nnetworks (GCN) to address these problems. Concretely, for (1) and (2), a\nGCN-based autoencoder is developed to augment the relation data by encoding\nhigh-order and complex connectivity patterns, and meanwhile is optimized\nsubject to the constraint of reconstructing the social profile to guarantee the\nvalidity of the identified neighborhood. After obtaining enough purified social\nrelations for each user, a GCN-based attentive social recommendation module is\ndesigned to address (3) by capturing the heterogeneous strengths of social\nrelations. Finally, we adopt adversarial training to unify all the components\nby playing a Minimax game and ensure a coordinated effort to enhance\nrecommendation performance. Extensive experiments on multiple open datasets\ndemonstrate the superiority of our framework and the ablation study confirms\nthe importance and effectiveness of each component.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:32:39 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 07:27:50 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 16:41:20 GMT"}, {"version": "v4", "created": "Fri, 23 Oct 2020 17:23:33 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yu", "Junliang", ""], ["Yin", "Hongzhi", ""], ["Li", "Jundong", ""], ["Gao", "Min", ""], ["Huang", "Zi", ""], ["Cui", "Lizhen", ""]]}, {"id": "2004.02349", "submitter": "Jonathan Herzig", "authors": "Jonathan Herzig, Pawe{\\l} Krzysztof Nowak, Thomas M\\\"uller, Francesco\n  Piccinno, Julian Martin Eisenschlos", "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering natural language questions over tables is usually seen as a\nsemantic parsing task. To alleviate the collection cost of full logical forms,\none popular approach focuses on weak supervision consisting of denotations\ninstead of logical forms. However, training semantic parsers from weak\nsupervision poses difficulties, and in addition, the generated logical forms\nare only used as an intermediate step prior to retrieving the denotation. In\nthis paper, we present TAPAS, an approach to question answering over tables\nwithout generating logical forms. TAPAS trains from weak supervision, and\npredicts the denotation by selecting table cells and optionally applying a\ncorresponding aggregation operator to such selection. TAPAS extends BERT's\narchitecture to encode tables as input, initializes from an effective joint\npre-training of text segments and tables crawled from Wikipedia, and is trained\nend-to-end. We experiment with three different semantic parsing datasets, and\nfind that TAPAS outperforms or rivals semantic parsing models by improving\nstate-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with\nthe state-of-the-art on WIKISQL and WIKITQ, but with a simpler model\narchitecture. We additionally find that transfer learning, which is trivial in\nour setting, from WIKISQL to WIKITQ, yields 48.7 accuracy, 4.2 points above the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 23:18:37 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 15:09:48 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Herzig", "Jonathan", ""], ["Nowak", "Pawe\u0142 Krzysztof", ""], ["M\u00fcller", "Thomas", ""], ["Piccinno", "Francesco", ""], ["Eisenschlos", "Julian Martin", ""]]}, {"id": "2004.02607", "submitter": "Tomas Kulvicius", "authors": "Tomas Kulvicius, Irene Markelic, Minija Tamosiunaite and Florentin\n  W\\\"org\\\"otter", "title": "Semantic Image Search for Robotic Applications", "comments": null, "journal-ref": "22nd International Workshop on Robotics in Alpe-Adria-Danube\n  Region (RAAD 2013), September 11-13, 2013, Portoroz, Slovenia", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization in robotics is one of the most important problems. New\ngeneralization approaches use internet databases in order to solve new tasks.\nModern search engines can return a large amount of information according to a\nquery within milliseconds. However, not all of the returned information is task\nrelevant, partly due to the problem of polysemes. Here we specifically address\nthe problem of object generalization by using image search. We suggest a\nbi-modal solution, combining visual and textual information, based on the\nobservation that humans use additional linguistic cues to demarcate intended\nword meaning. We evaluate the quality of our approach by comparing it to human\nlabelled data and find that, on average, our approach leads to improved results\nin comparison to Google searches, and that it can treat the problem of\npolysemes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:09:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kulvicius", "Tomas", ""], ["Markelic", "Irene", ""], ["Tamosiunaite", "Minija", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""]]}, {"id": "2004.02620", "submitter": "Ciro Javier Diaz Penedo", "authors": "Ciro Javier Diaz Penedo and Lucas Leonardo Silveira Costa", "title": "Grouping headlines", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we deal with the problem of grouping in headlines of the\nnewspaper ABC (Australian Bro-adcasting Corporation) using unsupervised machine\nlearning techniques. We present and discuss the results on the clusters found\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 04:03:56 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Penedo", "Ciro Javier Diaz", ""], ["Costa", "Lucas Leonardo Silveira", ""]]}, {"id": "2004.02621", "submitter": "Mustafa Abdool", "authors": "Mustafa Abdool, Malay Haldar, Prashant Ramanathan, Tyler Sax, Lanbo\n  Zhang, Aamir Mansawala, Shulin Yang, Thomas Legrand", "title": "Managing Diversity in Airbnb Search", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the long-standing questions in search systems is the role of diversity\nin results. From a product perspective, showing diverse results provides the\nuser with more choice and should lead to an improved experience. However, this\nintuition is at odds with common machine learning approaches to ranking which\ndirectly optimize the relevance of each individual item without a holistic view\nof the result set. In this paper, we describe our journey in tackling the\nproblem of diversity for Airbnb search, starting from heuristic based\napproaches and concluding with a novel deep learning solution that produces an\nembedding of the entire query context by leveraging Recurrent Neural Networks\n(RNNs). We hope our lessons learned will prove useful to others and motivate\nfurther research in this area.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 20:54:45 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Abdool", "Mustafa", ""], ["Haldar", "Malay", ""], ["Ramanathan", "Prashant", ""], ["Sax", "Tyler", ""], ["Zhang", "Lanbo", ""], ["Mansawala", "Aamir", ""], ["Yang", "Shulin", ""], ["Legrand", "Thomas", ""]]}, {"id": "2004.03027", "submitter": "Yumo Xu", "authors": "Yumo Xu and Mirella Lapata", "title": "Query Focused Multi-Document Summarization with Distant Supervision", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of better modeling query-cluster interactions to\nfacilitate query focused multi-document summarization (QFS). Due to the lack of\ntraining data, existing work relies heavily on retrieval-style methods for\nestimating the relevance between queries and text segments. In this work, we\nleverage distant supervision from question answering where various resources\nare available to more explicitly capture the relationship between queries and\ndocuments. We propose a coarse-to-fine modeling framework which introduces\nseparate modules for estimating whether segments are relevant to the query,\nlikely to contain an answer, and central. Under this framework, a trained\nevidence estimator further discerns which retrieved segments might answer the\nquery for final selection in the summary. We demonstrate that our framework\noutperforms strong comparison systems on standard QFS benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:35:19 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "2004.03181", "submitter": "Leo Bouscarrat", "authors": "L\\'eo Bouscarrat (QARMA, TALEP), Antoine Bonnefoy, C\\'ecile Capponi\n  (LIF, QARMA), Carlos Ramisch (TALEP)", "title": "Multilingual enrichment of disease biomedical ontologies", "comments": null, "journal-ref": "2nd workshop on MultilingualBIO: Multilingual Biomedical Text\n  Processing, May 2020, Marseille, France", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating biomedical ontologies is an important challenge, but doing it\nmanually requires much time and money. We study the possibility to use\nopen-source knowledge bases to translate biomedical ontologies. We focus on two\naspects: coverage and quality. We look at the coverage of two biomedical\nontologies focusing on diseases with respect to Wikidata for 9 European\nlanguages (Czech, Dutch, English, French, German, Italian, Polish, Portuguese\nand Spanish) for both ontologies, plus Arabic, Chinese and Russian for the\nsecond one. We first use direct links between Wikidata and the studied\nontologies and then use second-order links by going through other intermediate\nontologies. We then compare the quality of the translations obtained thanks to\nWikidata with a commercial machine translation tool, here Google Cloud\nTranslation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:04:21 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bouscarrat", "L\u00e9o", "", "QARMA, TALEP"], ["Bonnefoy", "Antoine", "", "LIF, QARMA"], ["Capponi", "C\u00e9cile", "", "LIF, QARMA"], ["Ramisch", "Carlos", "", "TALEP"]]}, {"id": "2004.03397", "submitter": "Iztok Fister", "authors": "Iztok Fister Jr., Karin Fister, Iztok Fister", "title": "Discovering associations in COVID-19 related research papers", "comments": "arXiv admin note: text overlap with arXiv:2003.00348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A COVID-19 pandemic has already proven itself to be a global challenge. It\nproves how vulnerable humanity can be. It has also mobilized researchers from\ndifferent sciences and different countries in the search for a way to fight\nthis potentially fatal disease. In line with this, our study analyses the\nabstracts of papers related to COVID-19 and coronavirus-related-research using\nassociation rule text mining in order to find the most interestingness words,\non the one hand, and relationships between them on the other. Then, a method,\ncalled information cartography, was applied for extracting structured knowledge\nfrom a huge amount of association rules. On the basis of these methods, the\npurpose of our study was to show how researchers have responded in similar\nepidemic/pandemic situations throughout history.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:52:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Fister", "Iztok", "Jr."], ["Fister", "Karin", ""], ["Fister", "Iztok", ""]]}, {"id": "2004.03455", "submitter": "Francesco Sovrano", "authors": "Francesco Sovrano, Monica Palmirani, Fabio Vitali", "title": "Deep Learning Based Multi-Label Text Classification of UNGA Resolutions", "comments": "10 pages, 10 figures, accepted paper at ICEGOV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this research is to produce a useful software for United\nNations (UN), that could help to speed up the process of qualifying the UN\ndocuments following the Sustainable Development Goals (SDGs) in order to\nmonitor the progresses at the world level to fight poverty, discrimination,\nclimate changes. In fact human labeling of UN documents would be a daunting\ntask given the size of the impacted corpus. Thus, automatic labeling must be\nadopted at least as a first step of a multi-phase process to reduce the overall\neffort of cataloguing and classifying. Deep Learning (DL) is nowadays one of\nthe most powerful tools for state-of-the-art (SOTA) AI for this task, but very\noften it comes with the cost of an expensive and error-prone preparation of a\ntraining-set. In the case of multi-label text classification of domain-specific\ntext it seems that we cannot effectively adopt DL without a big-enough\ndomain-specific training-set. In this paper, we show that this is not always\ntrue. In fact we propose a novel method that is able, through statistics like\nTF-IDF, to exploit pre-trained SOTA DL models (such as the Universal Sentence\nEncoder) without any need for traditional transfer learning or any other\nexpensive training procedure. We show the effectiveness of our method in a\nlegal context, by classifying UN Resolutions according to their most related\nSDGs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:54:38 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sovrano", "Francesco", ""], ["Palmirani", "Monica", ""], ["Vitali", "Fabio", ""]]}, {"id": "2004.03461", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Lukas Stankevi\\v{c}ius and Mantas Luko\\v{s}evi\\v{c}ius", "title": "Testing pre-trained Transformer models for Lithuanian news clustering", "comments": "Submission accepted at https://ivus.ktu.edu/", "journal-ref": "Proceedings of the Information Society and University Studies\n  2020, pp. 46-53, vol. 2698, CEUR, Kaunas, 2020, ISSN: 1613-0073", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent introduction of Transformer deep learning architecture made\nbreakthroughs in various natural language processing tasks. However,\nnon-English languages could not leverage such new opportunities with the\nEnglish text pre-trained models. This changed with research focusing on\nmultilingual models, where less-spoken languages are the main beneficiaries. We\ncompare pre-trained multilingual BERT, XLM-R, and older learned text\nrepresentation methods as encodings for the task of Lithuanian news clustering.\nOur results indicate that publicly available pre-trained multilingual\nTransformer models can be fine-tuned to surpass word vectors but still score\nmuch lower than specially trained doc2vec embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 14:41:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Stankevi\u010dius", "Lukas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""]]}, {"id": "2004.03589", "submitter": "Piji Li", "authors": "Piji Li, Lidong Bing, Zhongyu Wei, Wai Lam", "title": "Salience Estimation with Multi-Attention Learning for Abstractive Text\n  Summarization", "comments": "11 pages, @CUHK. arXiv admin note: text overlap with\n  arXiv:1803.11070, arXiv:1708.00625", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanism plays a dominant role in the sequence generation models\nand has been used to improve the performance of machine translation and\nabstractive text summarization. Different from neural machine translation, in\nthe task of text summarization, salience estimation for words, phrases or\nsentences is a critical component, since the output summary is a distillation\nof the input text. Although the typical attention mechanism can conduct text\nfragment selection from the input text conditioned on the decoder states, there\nis still a gap to conduct direct and effective salience detection. To bring\nback direct salience estimation for summarization with neural networks, we\npropose a Multi-Attention Learning framework which contains two new attention\nlearning components for salience estimation: supervised attention learning and\nunsupervised attention learning. We regard the attention weights as the\nsalience information, which means that the semantic units with large attention\nvalue will be more important. The context information obtained based on the\nestimated salience is incorporated with the typical attention mechanism in the\ndecoder to conduct summary generation. Extensive experiments on some benchmark\ndatasets in different languages demonstrate the effectiveness of the proposed\nframework for the task of abstractive summarization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:38:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Piji", ""], ["Bing", "Lidong", ""], ["Wei", "Zhongyu", ""], ["Lam", "Wai", ""]]}, {"id": "2004.03621", "submitter": "Robin Brochier", "authors": "Robin Brochier, Antoine Gourru, Adrien Guille and Julien Velcin", "title": "New Datasets and a Benchmark of Document Network Embedding Methods for\n  Scientific Expert Finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scientific literature is growing faster than ever. Finding an expert in a\nparticular scientific domain has never been as hard as today because of the\nincreasing amount of publications and because of the ever growing diversity of\nexpertise fields. To tackle this challenge, automatic expert finding algorithms\nrely on the vast scientific heterogeneous network to match textual queries with\npotential expert candidates. In this direction, document network embedding\nmethods seem to be an ideal choice for building representations of the\nscientific literature. Citation and authorship links contain major\ncomplementary information to the textual content of the publications. In this\npaper, we propose a benchmark for expert finding in document networks by\nleveraging data extracted from a scientific citation network and three\nscientific question & answer websites. We compare the performances of several\nalgorithms on these different sources of data and further study the\napplicability of embedding methods on an expert finding task.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:01:16 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Brochier", "Robin", ""], ["Gourru", "Antoine", ""], ["Guille", "Adrien", ""], ["Velcin", "Julien", ""]]}, {"id": "2004.03636", "submitter": "Jun Chen", "authors": "Jun Chen, Robert Hoehndorf, Mohamed Elhoseiny and Xiangliang Zhang", "title": "Efficient long-distance relation extraction with DG-SpanBERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In natural language processing, relation extraction seeks to rationally\nunderstand unstructured text. Here, we propose a novel SpanBERT-based graph\nconvolutional network (DG-SpanBERT) that extracts semantic features from a raw\nsentence using the pre-trained language model SpanBERT and a graph\nconvolutional network to pool latent features. Our DG-SpanBERT model inherits\nthe advantage of SpanBERT on learning rich lexical features from large-scale\ncorpus. It also has the ability to capture long-range relations between\nentities due to the usage of GCN on dependency tree. The experimental results\nshow that our model outperforms other existing dependency-based and\nsequence-based models and achieves a state-of-the-art performance on the TACRED\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 18:21:47 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chen", "Jun", ""], ["Hoehndorf", "Robert", ""], ["Elhoseiny", "Mohamed", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2004.03661", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang and Marcel Worring", "title": "Query-controllable Video Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When video collections become huge, how to explore both within and across\nvideos efficiently is challenging. Video summarization is one of the ways to\ntackle this issue. Traditional summarization approaches limit the effectiveness\nof video exploration because they only generate one fixed video summary for a\ngiven input video independent of the information need of the user. In this\nwork, we introduce a method which takes a text-based query as input and\ngenerates a video summary corresponding to it. We do so by modeling video\nsummarization as a supervised learning problem and propose an end-to-end deep\nlearning based method for query-controllable video summarization to generate a\nquery-dependent video summary. Our proposed method consists of a video summary\ncontroller, video summary generator, and video summary output module. To foster\nthe research of query-controllable video summarization and conduct our\nexperiments, we introduce a dataset that contains frame-based relevance score\nlabels. Based on our experimental result, it shows that the text-based query\nhelps control the video summary. It also shows the text-based query improves\nour model performance. Our code and dataset:\nhttps://github.com/Jhhuangkay/Query-controllable-Video-Summarization.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:35:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Worring", "Marcel", ""]]}, {"id": "2004.03688", "submitter": "Juan Banda", "authors": "Juan M. Banda, Ramya Tekumalla, Guanyu Wang, Jingyuan Yu, Tuo Liu,\n  Yuning Ding, Katya Artemova, Elena Tutubalina, Gerardo Chowell", "title": "A large-scale COVID-19 Twitter chatter dataset for open scientific\n  research -- an international collaboration", "comments": "8 pages, 1 figure 2 table. Update: new version of paper with\n  up-to-date statistics and new co-authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the COVID-19 pandemic continues its march around the world, an\nunprecedented amount of open data is being generated for genetics and\nepidemiological research. The unparalleled rate at which many research groups\naround the world are releasing data and publications on the ongoing pandemic is\nallowing other scientists to learn from local experiences and data generated in\nthe front lines of the COVID-19 pandemic. However, there is a need to integrate\nadditional data sources that map and measure the role of social dynamics of\nsuch a unique world-wide event into biomedical, biological, and epidemiological\nanalyses. For this purpose, we present a large-scale curated dataset of over\n152 million tweets, growing daily, related to COVID-19 chatter generated from\nJanuary 1st to April 4th at the time of writing. This open dataset will allow\nresearchers to conduct a number of research projects relating to the emotional\nand mental responses to social distancing measures, the identification of\nsources of misinformation, and the stratified measurement of sentiment towards\nthe pandemic in near real time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:25:26 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 16:20:38 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Banda", "Juan M.", ""], ["Tekumalla", "Ramya", ""], ["Wang", "Guanyu", ""], ["Yu", "Jingyuan", ""], ["Liu", "Tuo", ""], ["Ding", "Yuning", ""], ["Artemova", "Katya", ""], ["Tutubalina", "Elena", ""], ["Chowell", "Gerardo", ""]]}, {"id": "2004.03728", "submitter": "Hengtong Zhang", "authors": "Hengtong Zhang, Yaliang Li, Bolin Ding, Jing Gao", "title": "Practical Data Poisoning Attack against Next-Item Recommendation", "comments": null, "journal-ref": "Proceedings of The Web Conference 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommendation systems make use of a variety of information sources to\nprovide users the items that users are potentially interested in. However, due\nto the openness of the online platform, recommendation systems are vulnerable\nto data poisoning attacks. Existing attack approaches are either based on\nsimple heuristic rules or designed against specific recommendations approaches.\nThe former often suffers unsatisfactory performance, while the latter requires\nstrong knowledge of the target system. In this paper, we focus on a general\nnext-item recommendation setting and propose a practical poisoning attack\napproach named LOKI against blackbox recommendation systems. The proposed LOKI\nutilizes the reinforcement learning algorithm to train the attack agent, which\ncan be used to generate user behavior samples for data poisoning. In real-world\nrecommendation systems, the cost of retraining recommendation models is high,\nand the interaction frequency between users and a recommendation system is\nrestricted.Given these real-world restrictions, we propose to let the agent\ninteract with a recommender simulator instead of the target recommendation\nsystem and leverage the transferability of the generated adversarial samples to\npoison the target system. We also propose to use the influence function to\nefficiently estimate the influence of injected samples on the recommendation\nresults, without re-training the models within the simulator. Extensive\nexperiments on two datasets against four representative recommendation models\nshow that the proposed LOKI achieves better attacking performance than existing\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:04:52 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhang", "Hengtong", ""], ["Li", "Yaliang", ""], ["Ding", "Bolin", ""], ["Gao", "Jing", ""]]}, {"id": "2004.03774", "submitter": "Manqing Dong", "authors": "Manqing Dong, Feng Yuan, Lina Yao, Xianzhi Wang, Xiwei Xu and Liming\n  Zhu", "title": "Survey for Trust-aware Recommender Systems: A Deep Learning Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A significant remaining challenge for existing recommender systems is that\nusers may not trust the recommender systems for either lack of explanation or\ninaccurate recommendation results. Thus, it becomes critical to embrace a\ntrustworthy recommender system. This survey provides a systemic summary of\nthree categories of trust-aware recommender systems: social-aware recommender\nsystems that leverage users' social relationships; robust recommender systems\nthat filter untruthful noises (e.g., spammers and fake information) or enhance\nattack resistance; explainable recommender systems that provide explanations of\nrecommended items. We focus on the work based on deep learning techniques, an\nemerging area in the recommendation research.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 02:11:55 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 00:45:13 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Dong", "Manqing", ""], ["Yuan", "Feng", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2004.03788", "submitter": "Yue Zhou", "authors": "Yue Zhou, Yan Zhang, JingTao Yao", "title": "Satirical News Detection with Semantic Feature Extraction and\n  Game-theoretic Rough Sets", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satirical news detection is an important yet challenging task to prevent\nspread of misinformation. Many feature based and end-to-end neural nets based\nsatirical news detection systems have been proposed and delivered promising\nresults. Existing approaches explore comprehensive word features from satirical\nnews articles, but lack semantic metrics using word vectors for tweet form\nsatirical news. Moreover, the vagueness of satire and news parody determines\nthat a news tweet can hardly be classified with a binary decision, that is,\nsatirical or legitimate. To address these issues, we collect satirical and\nlegitimate news tweets, and propose a semantic feature based approach. Features\nare extracted by exploring inconsistencies in phrases, entities, and between\nmain and relative clauses. We apply game-theoretic rough set model to detect\nsatirical news, in which probabilistic thresholds are derived by game\nequilibrium and repetition learning mechanism. Experimental results on the\ncollected dataset show the robustness and improvement of the proposed approach\ncompared with Pawlak rough set model and SVM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 03:22:21 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Zhou", "Yue", ""], ["Zhang", "Yan", ""], ["Yao", "JingTao", ""]]}, {"id": "2004.03815", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xun Wang, Leimin Zhang, Chaoxi Xu, Gang Yang, Xirong Li", "title": "Feature Re-Learning with Data Augmentation for Video Relevance\n  Prediction", "comments": "accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": "10.1109/TKDE.2019.2947442", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the relevance between two given videos with respect to their\nvisual content is a key component for content-based video recommendation and\nretrieval. Thanks to the increasing availability of pre-trained image and video\nconvolutional neural network models, deep visual features are widely used for\nvideo content representation. However, as how two videos are relevant is\ntask-dependent, such off-the-shelf features are not always optimal for all\ntasks. Moreover, due to varied concerns including copyright, privacy and\nsecurity, one might have access to only pre-computed video features rather than\noriginal videos. We propose in this paper feature re-learning for improving\nvideo relevance prediction, with no need of revisiting the original video\ncontent. In particular, re-learning is realized by projecting a given deep\nfeature into a new space by an affine transformation. We optimize the\nre-learning process by a novel negative-enhanced triplet ranking loss. In order\nto generate more training data, we propose a new data augmentation strategy\nwhich works directly on frame-level and video-level features. Extensive\nexperiments in the context of the Hulu Content-based Video Relevance Prediction\nChallenge 2018 justify the effectiveness of the proposed method and its\nstate-of-the-art performance for content-based video relevance prediction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:22:41 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Dong", "Jianfeng", ""], ["Wang", "Xun", ""], ["Zhang", "Leimin", ""], ["Xu", "Chaoxi", ""], ["Yang", "Gang", ""], ["Li", "Xirong", ""]]}, {"id": "2004.03822", "submitter": "Leonhard Hennig", "authors": "Johannes Kirschnick, Philippe Thomas, Roland Roller, and Leonhard\n  Hennig", "title": "SIA: A Scalable Interoperable Annotation Server for Biomedical Named\n  Entities", "comments": "11 pages, 2 figures, published in Journal of Cheminformatics", "journal-ref": "J Cheminform 10, 63 (2018)", "doi": "10.1186/s13321-018-0319-2", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years showed a strong increase in biomedical sciences and an inherent\nincrease in publication volume. Extraction of specific information from these\nsources requires highly sophisticated text mining and information extraction\ntools. However, the integration of freely available tools into customized\nworkflows is often cumbersome and difficult. We describe SIA (Scalable\nInteroperable Annotation Server), our contribution to the BeCalm-Technical\ninteroperability and performance of annotation servers (BeCalm-TIPS) task, a\nscalable, extensible, and robust annotation service. The system currently\ncovers six named entity types (i.e., Chemicals, Diseases, Genes, miRNA,\nMutations, and Organisms) and is freely available under Apache 2.0 license at\nhttps://github.com/Erechtheus/sia.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 05:44:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Kirschnick", "Johannes", ""], ["Thomas", "Philippe", ""], ["Roller", "Roland", ""], ["Hennig", "Leonhard", ""]]}, {"id": "2004.03925", "submitter": "Bhavya Ahuja Grover Ms.", "authors": "Nikhil Kumar Rajput, Bhavya Ahuja Grover and Vipin Kumar Rathi", "title": "Word frequency and sentiment analysis of twitter messages during\n  Coronavirus pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus pandemic has taken the world by storm as also the social\nmedia. As the awareness about the ailment increased, so did messages, videos\nand posts acknowledging its presence. The social networking site, Twitter,\ndemonstrated similar effect with the number of posts related to coronavirus\nshowing an unprecedented growth in a very short span of time. This paper\npresents a statistical analysis of the twitter messages related to this disease\nposted since January 2020. Two types of empirical studies have been performed.\nThe first is on word frequency and the second on sentiments of the individual\ntweet messages. Inspection of the word frequency is useful in characterizing\nthe patterns or trends in the words used on the site. This would also reflect\non the psychology of the twitter users at this critical juncture. Unigram,\nbigram and trigram frequencies have been modeled by power law distribution. The\nresults have been validated by Sum of Square Error (SSE), R2 and Root Mean\nSquare Error (RMSE). High values of R2 and low values of SSE and RMSE lay the\ngrounds for the goodness of fit of this model. Sentiment analysis has been\nconducted to understand the general attitudes of the twitter users at this\ntime. Both tweets by general public and WHO were part of the corpus. The\nresults showed that the majority of the tweets had a positive polarity and only\nabout 15% were negative.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 10:45:08 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rajput", "Nikhil Kumar", ""], ["Grover", "Bhavya Ahuja", ""], ["Rathi", "Vipin Kumar", ""]]}, {"id": "2004.03985", "submitter": "Xavier Favory", "authors": "Xavier Favory, Frederic Font and Xavier Serra", "title": "Search Result Clustering in Collaborative Sound Collections", "comments": "8 pages, 4 figures, Proceedings of the 2020 International Conference\n  on Multimedia Retrieval (ICMR 20), June 8-11, 2020, Dublin, Ireland. ACM,\n  NewYork, NY, USA, 8 pages", "journal-ref": null, "doi": "10.1145/3372278.3390691", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large size of nowadays' online multimedia databases makes retrieving\ntheir content a difficult and time-consuming task. Users of online sound\ncollections typically submit search queries that express a broad intent, often\nmaking the system return large and unmanageable result sets. Search Result\nClustering is a technique that organises search-result content into coherent\ngroups, which allows users to identify useful subsets in their results.\nObtaining coherent and distinctive clusters that can be explored with a\nsuitable interface is crucial for making this technique a useful complement of\ntraditional search engines. In our work, we propose a graph-based approach\nusing audio features for clustering diverse sound collections obtained when\nquerying large online databases. We propose an approach to assess the\nperformance of different features at scale, by taking advantage of the metadata\nassociated with each sound. This analysis is complemented with an evaluation\nusing ground-truth labels from manually annotated datasets. We show that using\na confidence measure for discarding inconsistent clusters improves the quality\nof the partitions. After identifying the most appropriate features for\nclustering, we conduct an experiment with users performing a sound design task,\nin order to evaluate our approach and its user interface. A qualitative\nanalysis is carried out including usability questionnaires and semi-structured\ninterviews. This provides us with valuable new insights regarding the features\nthat promote efficient interaction with the clusters.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 13:08:17 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Favory", "Xavier", ""], ["Font", "Frederic", ""], ["Serra", "Xavier", ""]]}, {"id": "2004.04225", "submitter": "Bennett Kleinberg", "authors": "Bennett Kleinberg, Isabelle van der Vegt, Maximilian Mozes", "title": "Measuring Emotions in the COVID-19 Real World Worry Dataset", "comments": "Accepted to ACL 2020 COVID-19 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic is having a dramatic impact on societies and economies\naround the world. With various measures of lockdowns and social distancing in\nplace, it becomes important to understand emotional responses on a large scale.\nIn this paper, we present the first ground truth dataset of emotional responses\nto COVID-19. We asked participants to indicate their emotions and express these\nin text. This resulted in the Real World Worry Dataset of 5,000 texts (2,500\nshort + 2,500 long texts). Our analyses suggest that emotional responses\ncorrelated with linguistic measures. Topic modeling further revealed that\npeople in the UK worry about their family and the economic situation.\nTweet-sized texts functioned as a call for solidarity, while longer texts shed\nlight on worries and concerns. Using predictive modeling approaches, we were\nable to approximate the emotional responses of participants from text within\n14% of their actual value. We encourage others to use the dataset and improve\nhow we can use automated methods to learn about emotional responses and worries\nabout an urgent problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:52:14 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:57:23 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Kleinberg", "Bennett", ""], ["van der Vegt", "Isabelle", ""], ["Mozes", "Maximilian", ""]]}, {"id": "2004.04256", "submitter": "Muhammad Ammad-Ud-Din Ph.D.", "authors": "Adrian Flanagan, Were Oyomno, Alexander Grigorievskiy, Kuan Eeik Tan,\n  Suleiman A. Khan, and Muhammad Ammad-Ud-Din", "title": "Federated Multi-view Matrix Factorization for Personalized\n  Recommendations", "comments": "16 pages, 3 figures, 5 tables, submitted to a conference", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2020. Lecture Notes in Computer Science, Springer, Cham", "doi": "10.1007/978-3-030-67661-2_20", "report-no": "12458", "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the federated multi-view matrix factorization method that\nextends the federated learning framework to matrix factorization with multiple\ndata sources. Our method is able to learn the multi-view model without\ntransferring the user's personal data to a central server. As far as we are\naware this is the first federated model to provide recommendations using\nmulti-view matrix factorization. The model is rigorously evaluated on three\ndatasets on production settings. Empirical validation confirms that federated\nmulti-view matrix factorization outperforms simpler methods that do not take\ninto account the multi-view structure of the data, in addition, it demonstrates\nthe usefulness of the proposed method for the challenging prediction tasks of\ncold-start federated recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:07:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Flanagan", "Adrian", ""], ["Oyomno", "Were", ""], ["Grigorievskiy", "Alexander", ""], ["Tan", "Kuan Eeik", ""], ["Khan", "Suleiman A.", ""], ["Ammad-Ud-Din", "Muhammad", ""]]}, {"id": "2004.04596", "submitter": "Berry De Bruijn", "authors": "Dave Carter, Marta Stojanovic, Philip Hachey, Kevin Fournier, Simon\n  Rodier, Yunli Wang, Berry de Bruijn", "title": "Global Public Health Surveillance using Media Reports: Redesigning GPHIN", "comments": "5 pages, 1 figure. To be published in \"Ebook Series: Studies in\n  Health Technology and Informatics -- Proceedings of Medical Informatics\n  Europe 2020 (MIE 2020) -- IOS Press\"", "journal-ref": null, "doi": "10.3233/SHTI200280", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global public health surveillance relies on reporting structures and\ntransmission of trustworthy health reports. But in practice, these processes\nmay not always be fast enough, or are hindered by procedural, technical, or\npolitical barriers. GPHIN, the Global Public Health Intelligence Network, was\ndesigned in the late 1990s to scour mainstream news for health events, as that\ntravels faster and more freely. This paper outlines the next generation of\nGPHIN, which went live in 2017, and reports on design decisions underpinning\nits new functions and innovations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:34:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Carter", "Dave", ""], ["Stojanovic", "Marta", ""], ["Hachey", "Philip", ""], ["Fournier", "Kevin", ""], ["Rodier", "Simon", ""], ["Wang", "Yunli", ""], ["de Bruijn", "Berry", ""]]}, {"id": "2004.04816", "submitter": "Guanhua Zhang", "authors": "Bing Bai, Guanhua Zhang, Ye Lin, Hao Li, Kun Bai, Bo Luo", "title": "CSRN: Collaborative Sequential Recommendation Networks for News\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, news apps have taken over the popularity of paper-based media,\nproviding a great opportunity for personalization. Recurrent Neural Network\n(RNN)-based sequential recommendation is a popular approach that utilizes\nusers' recent browsing history to predict future items. This approach is\nlimited that it does not consider the societal influences of news consumption,\ni.e., users may follow popular topics that are constantly changing, while\ncertain hot topics might be spreading only among specific groups of people.\nSuch societal impact is difficult to predict given only users' own reading\nhistories. On the other hand, the traditional User-based Collaborative\nFiltering (UserCF) makes recommendations based on the interests of the\n\"neighbors\", which provides the possibility to supplement the weaknesses of\nRNN-based methods. However, conventional UserCF only uses a single similarity\nmetric to model the relationships between users, which is too coarse-grained\nand thus limits the performance. In this paper, we propose a framework of deep\nneural networks to integrate the RNN-based sequential recommendations and the\nkey ideas from UserCF, to develop Collaborative Sequential Recommendation\nNetworks (CSRNs). Firstly, we build a directed co-reading network of users, to\ncapture the fine-grained topic-specific similarities between users in a vector\nspace. Then, the CSRN model encodes users with RNNs, and learns to attend to\nneighbors and summarize what news they are reading at the moment. Finally, news\narticles are recommended according to both the user's own state and the\nsummarized state of the neighbors. Experiments on two public datasets show that\nthe proposed model outperforms the state-of-the-art approaches significantly.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 13:25:21 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Bai", "Bing", ""], ["Zhang", "Guanhua", ""], ["Lin", "Ye", ""], ["Li", "Hao", ""], ["Bai", "Kun", ""], ["Luo", "Bo", ""]]}, {"id": "2004.04959", "submitter": "Rui Zhao", "authors": "Rui Zhao, Kecheng Zheng, Zheng-jun Zha", "title": "Stacked Convolutional Deep Encoding Network for Video-Text Retrieval", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing dominant approaches for cross-modal video-text retrieval task are to\nlearn a joint embedding space to measure the cross-modal similarity. However,\nthese methods rarely explore long-range dependency inside video frames or\ntextual words leading to insufficient textual and visual details. In this\npaper, we propose a stacked convolutional deep encoding network for video-text\nretrieval task, which considers to simultaneously encode long-range and\nshort-range dependency in the videos and texts. Specifically, a multi-scale\ndilated convolutional (MSDC) block within our approach is able to encode\nshort-range temporal cues between video frames or text words by adopting\ndifferent scales of kernel size and dilation size of convolutional layer. A\nstacked structure is designed to expand the receptive fields by repeatedly\nadopting the MSDC block, which further captures the long-range relations\nbetween these cues. Moreover, to obtain more robust textual representations, we\nfully utilize the powerful language model named Transformer in two stages:\npretraining phrase and fine-tuning phrase. Extensive experiments on two\ndifferent benchmark datasets (MSR-VTT, MSVD) show that our proposed method\noutperforms other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 09:18:12 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhao", "Rui", ""], ["Zheng", "Kecheng", ""], ["Zha", "Zheng-jun", ""]]}, {"id": "2004.05001", "submitter": "Ivan P Yamshchikov", "authors": "Ivan P. Yamshchikov, Viacheslav Shibaev, Nikolay Khlebnikov, Alexey\n  Tikhonov", "title": "Style-transfer and Paraphrase: Looking for a Sensible Semantic\n  Similarity Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of such natural language processing tasks as style\ntransfer, paraphrase, and machine translation often calls for the use of\nsemantic similarity metrics. In recent years a lot of methods to measure the\nsemantic similarity of two short texts were developed. This paper provides a\ncomprehensive analysis for more than a dozen of such methods. Using a new\ndataset of fourteen thousand sentence pairs human-labeled according to their\nsemantic similarity, we demonstrate that none of the metrics widely used in the\nliterature is close enough to human judgment in these tasks. A number of\nrecently proposed metrics provide comparable results, yet Word Mover Distance\nis shown to be the most reasonable solution to measure semantic similarity in\nreformulated texts at the moment.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 11:52:06 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:10:24 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 21:58:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yamshchikov", "Ivan P.", ""], ["Shibaev", "Viacheslav", ""], ["Khlebnikov", "Nikolay", ""], ["Tikhonov", "Alexey", ""]]}, {"id": "2004.05125", "submitter": "Jimmy Lin", "authors": "Edwin Zhang, Nikhil Gupta, Rodrigo Nogueira, Kyunghyun Cho, and Jimmy\n  Lin", "title": "Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research\n  Dataset: Preliminary Thoughts and Lessons Learned", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Neural Covidex, a search engine that exploits the latest\nneural ranking architectures to provide information access to the COVID-19 Open\nResearch Dataset curated by the Allen Institute for AI. This web application\nexists as part of a suite of tools that we have developed over the past few\nweeks to help domain experts tackle the ongoing global pandemic. We hope that\nimproved information access capabilities to the scientific literature can\ninform evidence-based decision making and insight generation. This paper\ndescribes our initial efforts and offers a few thoughts about lessons we have\nlearned along the way.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 17:12:29 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Edwin", ""], ["Gupta", "Nikhil", ""], ["Nogueira", "Rodrigo", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.05476", "submitter": "Maite Taboada", "authors": "Varada Kolhatkar, Nithum Thain, Jeffrey Sorensen, Lucas Dixon and\n  Maite Taboada", "title": "Classifying Constructive Comments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the Constructive Comments Corpus (C3), comprised of 12,000\nannotated news comments, intended to help build new tools for online\ncommunities to improve the quality of their discussions. We define constructive\ncomments as high-quality comments that make a contribution to the conversation.\nWe explain the crowd worker annotation scheme and define a taxonomy of\nsub-characteristics of constructiveness. The quality of the annotation scheme\nand the resulting dataset is evaluated using measurements of inter-annotator\nagreement, expert assessment of a sample, and by the constructiveness\nsub-characteristics, which we show provide a proxy for the general\nconstructiveness concept. We provide models for constructiveness trained on C3\nusing both feature-based and a variety of deep learning approaches and\ndemonstrate that these models capture general rather than topic- or\ndomain-specific characteristics of constructiveness, through domain adaptation\nexperiments. We examine the role that length plays in our models, as comment\nlength could be easily gamed if models depend heavily upon this feature. By\nexamining the errors made by each model and their distribution by length, we\nshow that the best performing models are less correlated with comment\nlength.The constructiveness corpus and our experiments pave the way for a\nmoderation tool focused on promoting comments that make a contribution, rather\nthan only filtering out undesirable content.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:05:52 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 22:23:15 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 04:28:42 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 03:14:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kolhatkar", "Varada", ""], ["Thain", "Nithum", ""], ["Sorensen", "Jeffrey", ""], ["Dixon", "Lucas", ""], ["Taboada", "Maite", ""]]}, {"id": "2004.05716", "submitter": "Zhi Liu", "authors": "Zhi Liu, Yan Huang, Jing Gao, Li Chen, Dong Li", "title": "Large-scale Real-time Personalized Similar Product Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar product recommendation is one of the most common scenes in\ne-commerce. Many recommendation algorithms such as item-to-item Collaborative\nFiltering are working on measuring item similarities. In this paper, we\nintroduce our real-time personalized algorithm to model product similarity and\nreal-time user interests. We also introduce several other baseline algorithms\nincluding an image-similarity-based method, item-to-item collaborative\nfiltering, and item2vec, and compare them on our large-scale real-world\ne-commerce dataset. The algorithms which achieve good offline results are also\ntested on the online e-commerce website. Our personalized method achieves a 10%\nimprovement on the add-cart number in the real-world e-commerce scenario.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 23:16:14 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Liu", "Zhi", ""], ["Huang", "Yan", ""], ["Gao", "Jing", ""], ["Chen", "Li", ""], ["Li", "Dong", ""]]}, {"id": "2004.05755", "submitter": "Yufei Tian", "authors": "Yufei Tian, Jianfei Yu, Jing Jiang", "title": "Aspect and Opinion Aware Abstractive Review Summarization with\n  Reinforced Hard Typed Decoder", "comments": null, "journal-ref": null, "doi": "10.1145/3357384.3358142", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study abstractive review summarization.Observing that\nreview summaries often consist of aspect words, opinion words and context\nwords, we propose a two-stage reinforcement learning approach, which first\npredicts the output word type from the three types, and then leverages the\npredicted word type to generate the final word distribution.Experimental\nresults on two Amazon product review datasets demonstrate that our method can\nconsistently outperform several strong baseline approaches based on ROUGE\nscores.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 03:35:29 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Tian", "Yufei", ""], ["Yu", "Jianfei", ""], ["Jiang", "Jing", ""]]}, {"id": "2004.05861", "submitter": "Maram Hasanain", "authors": "Fatima Haouari, Maram Hasanain, Reem Suwaileh, Tamer Elsayed", "title": "ArCOV-19: The First Arabic COVID-19 Twitter Dataset with Propagation\n  Networks", "comments": "This work was accepted at the Sixth Arabic Natural Language\n  Processing Workshop (EACL/WANLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ArCOV-19, an Arabic COVID-19 Twitter dataset that\nspans one year, covering the period from 27th of January 2020 till 31st of\nJanuary 2021. ArCOV-19 is the first publicly-available Arabic Twitter dataset\ncovering COVID-19 pandemic that includes about 2.7M tweets alongside the\npropagation networks of the most-popular subset of them (i.e., most-retweeted\nand -liked). The propagation networks include both retweets and conversational\nthreads (i.e., threads of replies). ArCOV-19 is designed to enable research\nunder several domains including natural language processing, information\nretrieval, and social computing. Preliminary analysis shows that ArCOV-19\ncaptures rising discussions associated with the first reported cases of the\ndisease as they appeared in the Arab world. In addition to the source tweets\nand propagation networks, we also release the search queries and\nlanguage-independent crawler used to collect the tweets to encourage the\ncuration of similar datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 10:49:53 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 07:09:53 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 16:14:53 GMT"}, {"version": "v4", "created": "Sat, 13 Mar 2021 23:14:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Haouari", "Fatima", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Elsayed", "Tamer", ""]]}, {"id": "2004.05976", "submitter": "Brendan Hoover", "authors": "Brendan Hoover, Gil Bohrer, Jerod Merkle, Jennifer A. Miller", "title": "A Digital Ecosystem for Animal Movement Science: Making animal movement\n  datasets, data-linkage techniques, methods, and environmental layers easier\n  to find, interpret, and analyze", "comments": "Permission was not granted by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movement is a fundamental aspect of animal life and plays a crucial role in\ndetermining the structure of population dynamics, communities, ecosystems, and\ndiversity. In recent years, the recording of animal movements via GPS collars,\ncamera traps, acoustic sensors, and citizen science, along with the abundance\nof environmental and other ancillary data used by researchers to contextualize\nthose movements, has reached a level of volume, velocity, and variety that puts\nmovement ecology research in the realm of big data science. That data growth\nhas spawned increasingly complex methods for movement analysis. Consequently,\nanimal ecologists need a greater understanding of technical skills such as\nstatistics, geographic information systems (GIS), remote sensing, and coding.\nTherefore, collaboration has become increasingly crucial, as research requires\nboth domain knowledge and technical expertise. Datasets of animal movement and\nenvironmental data are typically available in repositories run by government\nagencies, universities, and non-governmental organizations (NGOs) with methods\ndescribed in scientific journals. However, there is little connectivity between\nthese entities. The construction of a digital ecosystem for animal movement\nscience is critically important right now. The digital ecosystem represents a\nsetting where movement data, environmental layers, and analysis methods are\ndiscoverable and available for efficient storage, manipulation, and analysis.\nWe argue that such a system which will help mature the field of movement\necology by engendering collaboration, facilitating replication, expanding the\nspatiotemporal range of potential analyses, and limiting redundancy in method\ndevelopment. We describe the key components of the digital ecosystem, the\ncritical challenges that would need addressing, as well as potential solutions\nto those challenges.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 14:52:06 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 22:11:39 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hoover", "Brendan", ""], ["Bohrer", "Gil", ""], ["Merkle", "Jerod", ""], ["Miller", "Jennifer A.", ""]]}, {"id": "2004.06059", "submitter": "Huajie Shao", "authors": "Huajie Shao, Dachun Sun, Jiahao Wu, Zecheng Zhang, Aston Zhang,\n  Shuochao Yao, Shengzhong Liu, Tianshi Wang, Chao Zhang, Tarek Abdelzaher", "title": "paper2repo: GitHub Repository Recommendation for Academic Papers", "comments": null, "journal-ref": "The Web Conference 2020 (WWW)", "doi": "10.1145/3366423.3380145", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GitHub has become a popular social application platform, where a large number\nof users post their open source projects. In particular, an increasing number\nof researchers release repositories of source code related to their research\npapers in order to attract more people to follow their work. Motivated by this\ntrend, we describe a novel item-item cross-platform recommender system,\n$\\textit{paper2repo}$, that recommends relevant repositories on GitHub that\nmatch a given paper in an academic search system such as Microsoft Academic.\nThe key challenge is to identify the similarity between an input paper and its\nrelated repositories across the two platforms, $\\textit{without the benefit of\nhuman labeling}$. Towards that end, paper2repo integrates text encoding and\nconstrained graph convolutional networks (GCN) to automatically learn and map\nthe embeddings of papers and repositories into the same space, where proximity\noffers the basis for recommendation. To make our method more practical in real\nlife systems, labels used for model training are computed automatically from\nfeatures of user actions on GitHub. In machine learning, such automatic\nlabeling is often called {\\em distant supervision\\/}. To the authors'\nknowledge, this is the first distant-supervised cross-platform (paper to\nrepository) matching system. We evaluate the performance of paper2repo on\nreal-world data sets collected from GitHub and Microsoft Academic. Results\ndemonstrate that it outperforms other state of the art recommendation methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:33:25 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Shao", "Huajie", ""], ["Sun", "Dachun", ""], ["Wu", "Jiahao", ""], ["Zhang", "Zecheng", ""], ["Zhang", "Aston", ""], ["Yao", "Shuochao", ""], ["Liu", "Shengzhong", ""], ["Wang", "Tianshi", ""], ["Zhang", "Chao", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "2004.06165", "submitter": "Xiujun Li", "authors": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei\n  Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao", "title": "Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks", "comments": "ECCV 2020, Code and pre-trained models are released:\n  https://github.com/microsoft/Oscar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pre-training methods of learning cross-modal representations on\nimage-text pairs are becoming popular for vision-language tasks. While existing\nmethods simply concatenate image region features and text features as input to\nthe model to be pre-trained and use self-attention to learn image-text semantic\nalignments in a brute force manner, in this paper, we propose a new learning\nmethod Oscar (Object-Semantics Aligned Pre-training), which uses object tags\ndetected in images as anchor points to significantly ease the learning of\nalignments. Our method is motivated by the observation that the salient objects\nin an image can be accurately detected, and are often mentioned in the paired\ntext. We pre-train an Oscar model on the public corpus of 6.5 million\ntext-image pairs, and fine-tune it on downstream tasks, creating new\nstate-of-the-arts on six well-established vision-language understanding and\ngeneration tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:18:10 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 03:29:46 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 04:57:31 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 01:18:25 GMT"}, {"version": "v5", "created": "Sun, 26 Jul 2020 00:46:46 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Xiujun", ""], ["Yin", "Xi", ""], ["Li", "Chunyuan", ""], ["Zhang", "Pengchuan", ""], ["Hu", "Xiaowei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Hu", "Houdong", ""], ["Dong", "Li", ""], ["Wei", "Furu", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2004.06174", "submitter": "Ralf L\\\"ammel", "authors": "Ralf L\\\"ammel, Alvin Kerber, and Liane Praza", "title": "Understanding What Software Engineers Are Working on -- The Work-Item\n  Prediction Challenge", "comments": "This paper appears in Proceedings of 28th International Conference on\n  Program Comprehension, ICPC 2020. The subject of the paper is covered by the\n  first author's keynote at the same conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what a software engineer (a developer, an incident responder, a\nproduction engineer, etc.) is working on is a challenging problem -- especially\nwhen considering the more complex software engineering workflows in\nsoftware-intensive organizations: i) engineers rely on a multitude (perhaps\nhundreds) of loosely integrated tools; ii) engineers engage in concurrent and\nrelatively long running workflows; ii) infrastructure (such as logging) is not\nfully aware of work items; iv) engineering processes (e.g., for incident\nresponse) are not explicitly modeled. In this paper, we explain the\ncorresponding 'work-item prediction challenge' on the grounds of representative\nscenarios, report on related efforts at Facebook, discuss some lessons learned,\nand review related work to call to arms to leverage, advance, and combine\ntechniques from program comprehension, mining software repositories, process\nmining, and machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 19:59:36 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["L\u00e4mmel", "Ralf", ""], ["Kerber", "Alvin", ""], ["Praza", "Liane", ""]]}, {"id": "2004.06201", "submitter": "Yi Tay", "authors": "Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew\n  Tomkins", "title": "Reverse Engineering Configurations of Neural Text Generation Models", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper seeks to develop a deeper understanding of the fundamental\nproperties of neural text generations models. The study of artifacts that\nemerge in machine generated text as a result of modeling choices is a nascent\nresearch area. Previously, the extent and degree to which these artifacts\nsurface in generated text has not been well studied. In the spirit of better\nunderstanding generative text models and their artifacts, we propose the new\ntask of distinguishing which of several variants of a given model generated a\npiece of text, and we conduct an extensive suite of diagnostic tests to observe\nwhether modeling choices (e.g., sampling methods, top-$k$ probabilities, model\narchitectures, etc.) leave detectable artifacts in the text they generate. Our\nkey finding, which is backed by a rigorous set of experiments, is that such\nartifacts are present and that different modeling choices can be inferred by\nobserving the generated text alone. This suggests that neural text generators\nmay be more sensitive to various modeling choices than previously thought.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 21:02:44 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tay", "Yi", ""], ["Bahri", "Dara", ""], ["Zheng", "Che", ""], ["Brunk", "Clifford", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2004.06222", "submitter": "Murthy Devarakonda", "authors": "Ashwin Karthik Ambalavanan, Murthy Devarakonda", "title": "Cascade Neural Ensemble for Identifying Scientifically Sound Articles", "comments": "11 pages, 4 figures, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A significant barrier to conducting systematic reviews and\nmeta-analysis is efficiently finding scientifically sound relevant articles.\nTypically, less than 1% of articles match this requirement which leads to a\nhighly imbalanced task. Although feature-engineered and early neural networks\nmodels were studied for this task, there is an opportunity to improve the\nresults.\n  Methods: We framed the problem of filtering articles as a classification\ntask, and trained and tested several ensemble architectures of SciBERT, a\nvariant of BERT pre-trained on scientific articles, on a manually annotated\ndataset of about 50K articles from MEDLINE. Since scientifically sound articles\nare identified through a multi-step process we proposed a novel cascade\nensemble analogous to the selection process. We compared the performance of the\ncascade ensemble with a single integrated model and other types of ensembles as\nwell as with results from previous studies.\n  Results: The cascade ensemble architecture achieved 0.7505 F measure, an\nimpressive 49.1% error rate reduction, compared to a CNN model that was\npreviously proposed and evaluated on a selected subset of the 50K articles. On\nthe full dataset, the cascade ensemble achieved 0.7639 F measure, resulting in\nan error rate reduction of 19.7% compared to the best performance reported in a\nprevious study that used the full dataset.\n  Conclusion: Pre-trained contextual encoder neural networks (e.g. SciBERT)\nperform better than the models studied previously and manually created search\nfilters in filtering for scientifically sound relevant articles. The superior\nperformance achieved by the cascade ensemble is a significant result that\ngeneralizes beyond this task and the dataset, and is analogous to query\noptimization in IR and databases.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:23:04 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ambalavanan", "Ashwin Karthik", ""], ["Devarakonda", "Murthy", ""]]}, {"id": "2004.06223", "submitter": "Guilherme Ramos", "authors": "Joao Saude and Guilherme Ramos and Ludovico Boratto and Carlos Caleiro", "title": "A Robust Reputation-based Group Ranking System and its Resistance to\n  Bribery", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of online reviews and opinions and its growing influence on\npeople's behavior and decisions, boosted the interest to extract meaningful\ninformation from this data deluge. Hence, crowdsourced ratings of products and\nservices gained a critical role in business and governments. Current\nstate-of-the-art solutions rank the items with an average of the ratings\nexpressed for an item, with a consequent lack of personalization for the users,\nand the exposure to attacks and spamming/spurious users. Using these ratings to\ngroup users with similar preferences might be useful to present users with\nitems that reflect their preferences and overcome those vulnerabilities. In\nthis paper, we propose a new reputation-based ranking system, utilizing\nmultipartite rating subnetworks, which clusters users by their similarities\nusing three measures, two of them based on Kolmogorov complexity. We also study\nits resistance to bribery and how to design optimal bribing strategies. Our\nsystem is novel in that it reflects the diversity of preferences by (possibly)\nassigning distinct rankings to the same item, for different groups of users. We\nprove the convergence and efficiency of the system. By testing it on synthetic\nand real data, we see that it copes better with spamming/spurious users, being\nmore robust to attacks than state-of-the-art approaches. Also, by clustering\nusers, the effect of bribery in the proposed multipartite ranking system is\ndimmed, comparing to the bipartite case.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:28:29 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 14:00:45 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Saude", "Joao", ""], ["Ramos", "Guilherme", ""], ["Boratto", "Ludovico", ""], ["Caleiro", "Carlos", ""]]}, {"id": "2004.06389", "submitter": "Dwaipayan Roy", "authors": "Suraj Agrawal, Dwaipayan Roy, Mandar Mitra", "title": "Tag Embedding Based Personalized Point Of Interest Recommendation System", "comments": "21 pages LNCS format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized Point of Interest recommendation is very helpful for satisfying\nusers' needs at new places. In this article, we propose a tag embedding based\nmethod for Personalized Recommendation of Point Of Interest. We model the\nrelationship between tags corresponding to Point Of Interest. The model\nprovides representative embedding corresponds to a tag in a way that related\ntags will be closer. We model Point of Interest-based on tag embedding and also\nmodel the users (user profile) based on the Point Of Interest rated by them.\nfinally, we rank the user's candidate Point Of Interest based on cosine\nsimilarity between user's embedding and Point of Interest's embedding. Further,\nwe find the parameters required to model user by discrete optimizing over\ndifferent measures (like ndcg@5, MRR, ...). We also analyze the result while\nconsidering the same parameters for all users and individual parameters for\neach user. Along with it we also analyze the effect on the result while\nchanging the dataset to model the relationship between tags. Our method also\nminimizes the privacy leak issue. We used TREC Contextual Suggestion 2016 Phase\n2 dataset and have significant improvement over all the measures on the state\nof the art method. It improves ndcg@5 by 12.8%, p@5 by 4.3%, and MRR by 7.8%,\nwhich shows the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:51:20 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Agrawal", "Suraj", ""], ["Roy", "Dwaipayan", ""], ["Mitra", "Mandar", ""]]}, {"id": "2004.06390", "submitter": "Ruiming Tang", "authors": "Yichao Wang, Xiangyu Zhang, Zhirong Liu, Zhenhua Dong, Xinhua Feng,\n  Ruiming Tang, Xiuqiang He", "title": "Personalized Re-ranking for Improving Diversity in Live Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of industrial recommender systems are normally suggesteda list of items\nat one time. Ideally, such list-wise recommendationshould provide diverse and\nrelevant options to the users. However, in practice, list-wise recommendation\nis implemented as top-N recommendation. Top-N recommendation selects the first\nN items from candidates to display. The list is generated by a ranking\nfunction, which is learned from labeled data to optimize accuracy.However,\ntop-N recommendation may lead to suboptimal, as it focuses on accuracy of each\nindividual item independently and overlooks mutual influence between items.\nTherefore, we propose a personalized re-ranking model for improving diversity\nof the recommendation list in real recommender systems. The proposed re-ranking\nmodel can be easily deployed as a follow-up component after any existing\nranking function. The re-ranking model improves the diversity by employing\npersonalized Determinental Point Process (DPP). DPP has been applied in some\nrecommender systems to improve the diversity and increase the user\nengagement.However, DPP does not take into account the fact that users may have\nindividual propensities to the diversity. To overcome such limitation, our\nre-ranking model proposes a personalized DPP to model the trade-off between\naccuracy and diversity for each individual user. We implement and deploy the\npersonalized DPP model on alarge scale industrial recommender system.\nExperimental results on both offline and online demonstrate the efficiency of\nour proposed re-ranking model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:51:57 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 02:58:25 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wang", "Yichao", ""], ["Zhang", "Xiangyu", ""], ["Liu", "Zhirong", ""], ["Dong", "Zhenhua", ""], ["Feng", "Xinhua", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2004.06391", "submitter": "Tehmina Amjad Dr.", "authors": "Muhammad Shoaib, Ali Daud, Tehmina Amjad", "title": "Author Name Disambiguation in Bibliographic Databases: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is a challenging and hot research area in the field of\nInformation Systems since last decade. Author Name Disambiguation (AND) in\nBibliographic Databases (BD) like DBLP , Citeseer , and Scopus is a specialized\nfield of entity resolution. Given many citations of underlying authors, the AND\ntask is to find which citations belong to the same author. In this survey, we\nstart with three basic AND problems, followed by need for solution and\nchallenges. A generic, five-step framework is provided for handling AND issues.\nThese steps are; (1) Preparation of dataset (2) Selection of publication\nattributes (3) Selection of similarity metrics (4) Selection of models and (5)\nClustering Performance evaluation. Categorization and elaboration of similarity\nmetrics and methods are also provided. Finally, future directions and\nrecommendations are given for this dynamic area of research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 09:54:57 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Shoaib", "Muhammad", ""], ["Daud", "Ali", ""], ["Amjad", "Tehmina", ""]]}, {"id": "2004.06651", "submitter": "Chaoyang Wang", "authors": "Chaoyang Wang and Zhiqiang Guo and Jianjun Li and Peng Pan and Guohui\n  Li", "title": "A Text-based Deep Reinforcement Learning Framework for Interactive\n  Recommendation", "comments": "Accepted by ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to its nature of learning from dynamic interactions and planning for\nlong-run performance, reinforcement learning (RL) recently has received much\nattention in interactive recommender systems (IRSs). IRSs usually face the\nlarge discrete action space problem, which makes most of the existing RL-based\nrecommendation methods inefficient. Moreover, data sparsity is another\nchallenging problem that most IRSs are confronted with. While the textual\ninformation like reviews and descriptions is less sensitive to sparsity,\nexisting RL-based recommendation methods either neglect or are not suitable for\nincorporating textual information. To address these two problems, in this\npaper, we propose a Text-based Deep Deterministic Policy Gradient framework\n(TDDPG-Rec) for IRSs. Specifically, we leverage textual information to map\nitems and users into a feature space, which greatly alleviates the sparsity\nproblem. Moreover, we design an effective method to construct an action\ncandidate set. By the policy vector dynamically learned from TDDPG-Rec that\nexpresses the user's preference, we can select actions from the candidate set\neffectively. Through experiments on three public datasets, we demonstrate that\nTDDPG-Rec achieves state-of-the-art performance over several baselines in a\ntime-efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:46:01 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 15:32:01 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 02:26:05 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 13:03:21 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Chaoyang", ""], ["Guo", "Zhiqiang", ""], ["Li", "Jianjun", ""], ["Pan", "Peng", ""], ["Li", "Guohui", ""]]}, {"id": "2004.06721", "submitter": "Daniel Torres-Salinas Dr", "authors": "Daniel Torres-Salinas", "title": "Daily growth rate of scientific production on Covid-19. Analysis in\n  databases and open access repositories", "comments": "in Spanish", "journal-ref": null, "doi": "10.3145/epi.2020.mar.15", "report-no": null, "categories": "cs.DL cs.IR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community is facing one of its greatest challenges in solving\na global health problem: COVID-19 pandemic. This situation has generated an\nunprecedented volume of publications. What is the volume, in terms of\npublications, of research on COVID-19? The general objective of this research\nwork is to obtain a global vision of the daily growth of scientific production\non COVID-19 in different databases (Dimensions, Web of Science Core Collection,\nScopus-Elsevier, Pubmed and eight repositories). In relation to the results\nobtained, Dimensions indexes a total of 9435 publications (69% with peer review\nand 2677 preprints) well above Scopus (1568) and WoS (718). This is a classic\nbiliometric phenomenon of exponential growth (R2 = 0.92). The global growth\nrate is 500 publications and the production doubles every 15 days. In the case\nof Pubmed the weekly growth is around 1000 publications. Of the eight\nrepositories analysed, Pubmed Central, Medrxiv and SSRN are the leaders.\nDespite their enormous contribution, the journals continue to be the core of\nscientific communication. Finally, it has been established that three out of\nevery four publications on the COVID-19 are available in open access. The\ninformation explosion demands a serious and coordinated response from\ninformation professionals, which places us at the centre of the information\npandemic.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:30:01 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Torres-Salinas", "Daniel", ""]]}, {"id": "2004.06747", "submitter": "Carlos-Emiliano Gonz\\'alez-Gallardo", "authors": "Carlos-Emiliano Gonz\\'alez-Gallardo, Eric SanJuan, Juan-Manuel\n  Torres-Moreno", "title": "Extending Text Informativeness Measures to Passage Interestingness\n  Evaluation (Language Model vs. Word Embedding)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Standard informativeness measures used to evaluate Automatic Text\nSummarization mostly rely on n-gram overlapping between the automatic summary\nand the reference summaries. These measures differ from the metric they use\n(cosine, ROUGE, Kullback-Leibler, Logarithm Similarity, etc.) and the bag of\nterms they consider (single words, word n-grams, entities, nuggets, etc.).\nRecent word embedding approaches offer a continuous alternative to discrete\napproaches based on the presence/absence of a text unit. Informativeness\nmeasures have been extended to Focus Information Retrieval evaluation involving\na user's information need represented by short queries. In particular for the\ntask of CLEF-INEX Tweet Contextualization, tweet contents have been considered\nas queries. In this paper we define the concept of Interestingness as a\ngeneralization of Informativeness, whereby the information need is diverse and\nformalized as an unknown set of implicit queries. We then study the ability of\nstate of the art Informativeness measures to cope with this generalization.\nLately we show that with this new framework, standard word embeddings\noutperforms discrete measures only on uni-grams, however bi-grams seems to be a\nkey point of interestingness evaluation. Lastly we prove that the CLEF-INEX\nTweet Contextualization 2012 Logarithm Similarity measure provides best\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 18:22:48 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Gonz\u00e1lez-Gallardo", "Carlos-Emiliano", ""], ["SanJuan", "Eric", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "2004.06774", "submitter": "Firoj Alam", "authors": "Firoj Alam, Hassan Sajjad, Muhammad Imran and Ferda Ofli", "title": "CrisisBench: Benchmarking Crisis-related Social Media Datasets for\n  Humanitarian Information Processing", "comments": "Accepted in ICWSM-2021, Twitter datasets, Textual content, Natural\n  disasters, Crisis Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Time-critical analysis of social media streams is important for humanitarian\norganizations for planing rapid response during disasters. The \\textit{crisis\ninformatics} research community has developed several techniques and systems\nfor processing and classifying big crisis-related data posted on social media.\nHowever, due to the dispersed nature of the datasets used in the literature\n(e.g., for training models), it is not possible to compare the results and\nmeasure the progress made towards building better models for crisis informatics\ntasks. In this work, we attempt to bridge this gap by combining various\nexisting crisis-related datasets. We consolidate eight human-annotated datasets\nand provide 166.1k and 141.5k tweets for \\textit{informativeness} and\n\\textit{humanitarian} classification tasks, respectively. We believe that the\nconsolidated dataset will help train more sophisticated models. Moreover, we\nprovide benchmarks for both binary and multiclass classification tasks using\nseveral deep learning architecrures including, CNN, fastText, and transformers.\nWe make the dataset and scripts available at:\nhttps://crisisnlp.qcri.org/crisis_datasets_benchmarks.html\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 19:51:04 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 09:48:46 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 08:58:43 GMT"}, {"version": "v4", "created": "Sat, 17 Apr 2021 16:10:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Alam", "Firoj", ""], ["Sajjad", "Hassan", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2004.06793", "submitter": "Toktam Amanzadeh Oghaz", "authors": "Toktam A. Oghaz, Ece C. Mutlu, Jasser Jasser, Niloofar Yousefi, Ivan\n  Garibay", "title": "Probabilistic Model of Narratives Over Topical Trends in Social Media: A\n  Discrete Time Model", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3372923.3404790", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social media platforms are turning into the prime source of news and\nnarratives about worldwide events. However,a systematic summarization-based\nnarrative extraction that can facilitate communicating the main underlying\nevents is lacking. To address this issue, we propose a novel event-based\nnarrative summary extraction framework. Our proposed framework is designed as a\nprobabilistic topic model, with categorical time distribution, followed by\nextractive text summarization. Our topic model identifies topics' recurrence\nover time with a varying time resolution. This framework not only captures the\ntopic distributions from the data, but also approximates the user activity\nfluctuations over time. Furthermore, we define significance-dispersity\ntrade-off (SDT) as a comparison measure to identify the topic with the highest\nlifetime attractiveness in a timestamped corpus. We evaluate our model on a\nlarge corpus of Twitter data, including more than one million tweets in the\ndomain of the disinformation campaigns conducted against the White Helmets of\nSyria. Our results indicate that the proposed framework is effective in\nidentifying topical trends, as well as extracting narrative summaries from text\ncorpus with timestamped data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 20:18:21 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Oghaz", "Toktam A.", ""], ["Mutlu", "Ece C.", ""], ["Jasser", "Jasser", ""], ["Yousefi", "Niloofar", ""], ["Garibay", "Ivan", ""]]}, {"id": "2004.06842", "submitter": "Chien-Chun Ni", "authors": "Chien-Chun Ni, Kin Sum Liu, Nicolas Torzec", "title": "Layered Graph Embedding for Entity Recommendation using Wikipedia in the\n  Yahoo! Knowledge Graph", "comments": "8 pages, 4 figures, 8 tables. To be appeared in Wiki Workshop 2020,\n  Companion Proceedings of the Web Conference 2020(WWW 20 Companion), Taipei,\n  Taiwan", "journal-ref": null, "doi": "10.1145/3366424.3383570", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe an embedding-based entity recommendation framework\nfor Wikipedia that organizes Wikipedia into a collection of graphs layered on\ntop of each other, learns complementary entity representations from their\ntopology and content, and combines them with a lightweight learning-to-rank\napproach to recommend related entities on Wikipedia. Through offline and online\nevaluations, we show that the resulting embeddings and recommendations perform\nwell in terms of quality and user engagement. Balancing simplicity and quality,\nthis framework provides default entity recommendations for English and other\nlanguages in the Yahoo! Knowledge Graph, which Wikipedia is a core subset of.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 00:49:27 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ni", "Chien-Chun", ""], ["Liu", "Kin Sum", ""], ["Torzec", "Nicolas", ""]]}, {"id": "2004.07123", "submitter": "Seif Ben Chaabene", "authors": "Seif Ben Chaabene", "title": "Nouvelles repr\\'esentations concises exactes des motifs rares", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until a present, the majority of work in data mining were interested in the\nextraction of the frequent itemsets and the generation of the frequent\nassociation rules from these itemsets. Sometimes, the frequent of associations\nrules can revealed not-interesting in the direction where a frequent behavior\nis in general a normal behavior in the database. These last years, some work\nwas focused on the exploitation and the extraction of rare itemset and shows\nthem interest. However, the very important size of those itemset was the\nhandicap of algorithms that exploit the rare pattern. In order to relieve this\nproblem, the present report proposes two exact concise representations of the\nrare itemset, one based on the minimal generators and the other based on the\nclosed itemset. In this context, we introduce two new algorithms called GMRare\nand MFRare which extract these two exact concise representations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:35:33 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Chaabene", "Seif Ben", ""]]}, {"id": "2004.07171", "submitter": "Adrien Ycart", "authors": "Adrien Ycart, Lele Liu, Emmanouil Benetos, Marcus T. Pearce", "title": "Musical Features for Automatic Music Transcription Evaluation", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report gives a detailed, formal description of the features\nintroduced in the paper: Adrien Ycart, Lele Liu, Emmanouil Benetos and Marcus\nT. Pearce. \"Investigating the Perceptual Validity of Evaluation Metrics for\nAutomatic Piano Music Transcription\", Transactions of the International Society\nfor Music Information Retrieval (TISMIR), Accepted, 2020.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 15:56:45 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Ycart", "Adrien", ""], ["Liu", "Lele", ""], ["Benetos", "Emmanouil", ""], ["Pearce", "Marcus T.", ""]]}, {"id": "2004.07286", "submitter": "Jay Tenenbaum", "authors": "Haim Kaplan, Jay Tenenbaum", "title": "Locality Sensitive Hashing for Set-Queries, Motivated by Group\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality Sensitive Hashing (LSH) is an effective method to index a set of\npoints such that we can efficiently find the nearest neighbors of a query\npoint. We extend this method to our novel Set-query LSH (SLSH), such that it\ncan find the nearest neighbors of a set of points, given as a query.\n  Let $ s(x,y) $ be the similarity between two points $ x $ and $ y $. We\ndefine a similarity between a set $ Q$ and a point $ x $ by aggregating the\nsimilarities $ s(p,x) $ for all $ p\\in Q $. For example, we can take $ s(p,x) $\nto be the angular similarity between $ p $ and $ x $ (i.e., $1-{\\angle\n(x,p)}/{\\pi}$), and aggregate by arithmetic or geometric averaging, or taking\nthe lowest similarity.\n  We develop locality sensitive hash families and data structures for a large\nset of such arithmetic and geometric averaging similarities, and analyze their\ncollision probabilities. We also establish an analogous framework and hash\nfamilies for distance functions. Specifically, we give a structure for the\neuclidean distance aggregated by either averaging or taking the maximum.\n  We leverage SLSH to solve a geometric extension of the approximate near\nneighbors problem. In this version, we consider a metric for which the unit\nball is an ellipsoid and its orientation is specified with the query.\n  An important application that motivates our work is group recommendation\nsystems. Such a system embeds movies and users in the same feature space, and\nthe task of recommending a movie for a group to watch together, translates to a\nset-query $ Q $ using an appropriate similarity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:41:54 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 07:45:19 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 20:45:03 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kaplan", "Haim", ""], ["Tenenbaum", "Jay", ""]]}, {"id": "2004.07352", "submitter": "Ralf L\\\"ammel", "authors": "John Ahlgren, Maria Eugenia Berezin, Kinga Bojarczuk, Elena Dulskyte,\n  Inna Dvortsova, Johann George, Natalija Gucevska, Mark Harman, Shan He, Ralf\n  L\\\"ammel, Erik Meijer, Silvia Sapora, and Justin Spahr-Summers", "title": "Ownership at Large -- Open Problems and Challenges in Ownership\n  Management", "comments": "Author order is alphabetical. Contact author: Ralf L\\\"ammel\n  (rlaemmel@acm.org). The subject of the paper is covered by the contact\n  author's keynote at the same conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-intensive organizations rely on large numbers of software assets of\ndifferent types, e.g., source-code files, tables in the data warehouse, and\nsoftware configurations. Who is the most suitable owner of a given asset\nchanges over time, e.g., due to reorganization and individual function changes.\nNew forms of automation can help suggest more suitable owners for any given\nasset at a given point in time. By such efforts on ownership health,\naccountability of ownership is increased. The problem of finding the most\nsuitable owners for an asset is essentially a program comprehension problem:\nhow do we automatically determine who would be best placed to understand,\nmaintain, evolve (and thereby assume ownership of) a given asset. This paper\nintroduces the Facebook Ownesty system, which uses a combination of ultra large\nscale data mining and machine learning and has been deployed at Facebook as\npart of the company's ownership management approach. Ownesty processes many\nmillions of software assets (e.g., source-code files) and it takes into account\nworkflow and organizational aspects. The paper sets out open problems and\nchallenges on ownership for the research community with advances expected from\nthe fields of software engineering, programming languages, and machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:26:19 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Ahlgren", "John", ""], ["Berezin", "Maria Eugenia", ""], ["Bojarczuk", "Kinga", ""], ["Dulskyte", "Elena", ""], ["Dvortsova", "Inna", ""], ["George", "Johann", ""], ["Gucevska", "Natalija", ""], ["Harman", "Mark", ""], ["He", "Shan", ""], ["L\u00e4mmel", "Ralf", ""], ["Meijer", "Erik", ""], ["Sapora", "Silvia", ""], ["Spahr-Summers", "Justin", ""]]}, {"id": "2004.07493", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang,\n  Prashant Shiralkar, Xiang Ren", "title": "TriggerNER: Learning with Entity Triggers as Explanations for Named\n  Entity Recognition", "comments": "Accepted to the ACL 2020. Project page:\n  https://inklab.usc.edu/TriggerNER/ (Fixed a few typos and added a new\n  figure.)", "journal-ref": "Proc. of ACL 2020, page 8503--8511", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural models for named entity recognition (NER) in a new domain\noften requires additional human annotations (e.g., tens of thousands of labeled\ninstances) that are usually expensive and time-consuming to collect. Thus, a\ncrucial research question is how to obtain supervision in a cost-effective way.\nIn this paper, we introduce \"entity triggers,\" an effective proxy of human\nexplanations for facilitating label-efficient learning of NER models. An entity\ntrigger is defined as a group of words in a sentence that helps to explain why\nhumans would recognize an entity in the sentence.\n  We crowd-sourced 14k entity triggers for two well-studied NER datasets. Our\nproposed model, Trigger Matching Network, jointly learns trigger\nrepresentations and soft matching module with self-attention such that can\ngeneralize to unseen sentences easily for tagging. Our framework is\nsignificantly more cost-effective than the traditional neural NER frameworks.\nExperiments show that using only 20% of the trigger-annotated sentences results\nin a comparable performance as using 70% of conventional annotated sentences.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 07:27:43 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 10:06:37 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 07:43:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 01:10:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["Lee", "Dong-Ho", ""], ["Shen", "Ming", ""], ["Moreno", "Ryan", ""], ["Huang", "Xiao", ""], ["Shiralkar", "Prashant", ""], ["Ren", "Xiang", ""]]}, {"id": "2004.07609", "submitter": "Khalid Aloufi K.", "authors": "Khalid S. Aloufi, Abdulrahman A. Alsewari", "title": "Toward Efficient Web Publishing with Provenance of Information Using\n  Trusty URIs: Applying the proposed model with the Quran", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research presents a methodology for trusting the provenance of data on\nthe web. The implication is that data does not change after publication and the\nsource of the data is stable. There are different data that should not change\nover time, such as published information in books and similar documents as well\nas news or events reported on the web. If the data change after publication on\nthe web, the web pages that reference the unstable data will lose points of\ninterest or link to different resources. With the current move to linked data\nand the semantic web, this is becoming a greater obstacle to be solved. This\nresearch presents a methodology for establishing trusted information using an\nencoded reference of the data embedded in its URI, which creates a stable\nreference of the data and a method for ensuring its provenance stability. After\napplying the methodology, the results showed that the methodology is highly\napplicable and has no overhead cost over the loading time. The novel solution\ncan be applied directly to any data portals or web content management systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 11:38:14 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Aloufi", "Khalid S.", ""], ["Alsewari", "Abdulrahman A.", ""]]}, {"id": "2004.08068", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen, Chaoran Huang, Lina Yao, Xianzhi Wang, Wei Liu, Wenjie\n  Zhang", "title": "Knowledge-guided Deep Reinforcement Learning for Interactive\n  Recommendation", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207010", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive recommendation aims to learn from dynamic interactions between\nitems and users to achieve responsiveness and accuracy. Reinforcement learning\nis inherently advantageous for coping with dynamic environments and thus has\nattracted increasing attention in interactive recommendation research. Inspired\nby knowledge-aware recommendation, we proposed Knowledge-Guided deep\nReinforcement learning (KGRL) to harness the advantages of both reinforcement\nlearning and knowledge graphs for interactive recommendation. This model is\nimplemented upon the actor-critic network framework. It maintains a local\nknowledge network to guide decision-making and employs the attention mechanism\nto capture long-term semantics between items. We have conducted comprehensive\nexperiments in a simulated online environment with six public real-world\ndatasets and demonstrated the superiority of our model over several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 05:26:47 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chen", "Xiaocong", ""], ["Huang", "Chaoran", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Liu", "Wei", ""], ["Zhang", "Wenjie", ""]]}, {"id": "2004.08123", "submitter": "Mathis Linger", "authors": "Mathis Linger and Mhamed Hajaiej", "title": "Batch Clustering for Multilingual News Streaming", "comments": "7 pages, 2 figures", "journal-ref": "Proceedings of Text2Story - Third Workshop on Narrative Extraction\n  From Texts co-located with 42nd European Conference on Information Retrieval\n  (ECIR 2020) Lisbon, Portugal, April 14th, 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, digital news articles are widely available, published by various\neditors and often written in different languages. This large volume of diverse\nand unorganized information makes human reading very difficult or almost\nimpossible. This leads to a need for algorithms able to arrange high amount of\nmultilingual news into stories. To this purpose, we extend previous works on\nTopic Detection and Tracking, and propose a new system inspired from newsLens.\nWe process articles per batch, looking for monolingual local topics which are\nthen linked across time and languages. Here, we introduce a novel \"replaying\"\nstrategy to link monolingual local topics into stories. Besides, we propose new\nfine tuned multilingual embedding using SBERT to create crosslingual stories.\nOur system gives monolingual state-of-the-art results on dataset of Spanish and\nGerman news and crosslingual state-of-the-art results on English, Spanish and\nGerman news.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:59:13 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Linger", "Mathis", ""], ["Hajaiej", "Mhamed", ""]]}, {"id": "2004.08145", "submitter": "Zhiwei Gao", "authors": "Zhiwei Gao, Shuntaro Yada, Shoko Wakamiya and Eiji Aramaki", "title": "NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the outbreak of coronavirus disease 2019 (COVID-19) in the late 2019,\nit has affected over 200 countries and billions of people worldwide. This has\naffected the social life of people owing to enforcements, such as \"social\ndistancing\" and \"stay at home.\" This has resulted in an increasing interaction\nthrough social media. Given that social media can bring us valuable information\nabout COVID-19 at a global scale, it is important to share the data and\nencourage social media studies against COVID-19 or other infectious diseases.\nTherefore, we have released a multilingual dataset of social media posts\nrelated to COVID-19, consisting of microblogs in English and Japanese from\nTwitter and those in Chinese from Weibo. The data cover microblogs from January\n20, 2020, to March 24, 2020. This paper also provides a quantitative as well as\nqualitative analysis of these datasets by creating daily word clouds as an\nexample of text-mining analysis. The dataset is now available on Github. This\ndataset can be analyzed in a multitude of ways and is expected to help in\nefficient communication of precautions related to COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 09:48:14 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gao", "Zhiwei", ""], ["Yada", "Shuntaro", ""], ["Wakamiya", "Shoko", ""], ["Aramaki", "Eiji", ""]]}, {"id": "2004.08204", "submitter": "Tam Tran-The", "authors": "Tam Tran-The", "title": "Modeling Institutional Credit Risk with Financial News", "comments": "Accepted to the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Credit risk management, the practice of mitigating losses by understanding\nthe adequacy of a borrower's capital and loan loss reserves, has long been\nimperative to any financial institution's long-term sustainability and growth.\nMassMutual is no exception. The company is keen on effectively monitoring\ndowngrade risk, or the risk associated with the event when credit rating of a\ncompany deteriorates. Current work in downgrade risk modeling depends on\nmultiple variations of quantitative measures provided by third-party rating\nagencies and risk management consultancy companies. As these structured\nnumerical data become increasingly commoditized among institutional investors,\nthere has been a wide push into using alternative sources of data, such as\nfinancial news, earnings call transcripts, or social media content, to possibly\ngain a competitive edge in the industry. The volume of qualitative information\nor unstructured text data has exploded in the past decades and is now available\nfor due diligence to supplement quantitative measures of credit risk. This\npaper proposes a predictive downgrade model using solely news data represented\nby neural network embeddings. The model standalone achieves an Area Under the\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent. The\noutput probability from this news model, as an additional feature, improves the\nperformance of our benchmark model using only quantitative measures by more\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\nalso indicates that news articles related to our predicted downgrade events are\nspecially relevant and high-quality in our business context.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 17:52:07 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Tran-The", "Tam", ""]]}, {"id": "2004.08257", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Elias K\\\"arle and Dieter Fensel", "title": "Duplication Detection in Knowledge Graphs: Literature and Tools", "comments": "Submitted to EKAW 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, an increasing amount of knowledge graphs (KGs) have been\ncreated as a means to store cross-domain knowledge and billion of facts, which\nare the basis of costumers' applications like search engines. However, KGs\ninevitably have inconsistencies such as duplicates that might generate\nconflicting property values. Duplication detection (DD) aims to identify\nduplicated entities and resolve their conflicting property values effectively\nand efficiently. In this paper, we perform a literature review on DD methods\nand tools, and an evaluation of them. Our main contributions are a performance\nevaluation of DD tools in KGs, improvement suggestions, and a DD workflow to\nsupport future development of DD tools, which are based on desirable features\ndetected through this study.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 14:12:40 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Huaman", "Elwin", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "2004.08301", "submitter": "Koujin Takeda", "authors": "Hiroki Kitano, Koujin Takeda", "title": "Belief Propagation for Maximum Coverage on Weighted Bipartite Graph and\n  Application to Text Summarization", "comments": "4 pages, 4 figures", "journal-ref": "J. Phys. Soc. Jpn. 89, 043801 (2020)", "doi": "10.7566/JPSJ.89.043801", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study text summarization from the viewpoint of maximum coverage problem.\nIn graph theory, the task of text summarization is regarded as maximum coverage\nproblem on bipartite graph with weighted nodes. In recent study,\nbelief-propagation based algorithm for maximum coverage on unweighted graph was\nproposed using the idea of statistical mechanics. We generalize it to weighted\ngraph for text summarization. Then we apply our algorithm to weighted biregular\nrandom graph for verification of maximum coverage performance. We also apply it\nto bipartite graph representing real document in open text dataset, and check\nthe performance of text summarization. As a result, our algorithm exhibits\nbetter performance than greedy-type algorithm in some setting of text\nsummarization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 05:50:20 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Kitano", "Hiroki", ""], ["Takeda", "Koujin", ""]]}, {"id": "2004.08333", "submitter": "Alireza Borjali", "authors": "Alireza Borjali, Martin Magneli, David Shin, Henrik Malchau, Orhun K.\n  Muratoglu, Kartik M. Varadarajan", "title": "Natural Language Processing with Deep Learning for Medical Adverse Event\n  Detection from Free-Text Medical Narratives: A Case Study of Detecting Total\n  Hip Replacement Dislocation", "comments": null, "journal-ref": null, "doi": "10.1016/j.compbiomed.2020.104140", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and timely detection of medical adverse events (AEs) from free-text\nmedical narratives is challenging. Natural language processing (NLP) with deep\nlearning has already shown great potential for analyzing free-text data, but\nits application for medical AE detection has been limited. In this study we\nproposed deep learning based NLP (DL-NLP) models for efficient and accurate hip\ndislocation AE detection following total hip replacement from standard\n(radiology notes) and non-standard (follow-up telephone notes) free-text\nmedical narratives. We benchmarked these proposed models with a wide variety of\ntraditional machine learning based NLP (ML-NLP) models, and also assessed the\naccuracy of International Classification of Diseases (ICD) and Current\nProcedural Terminology (CPT) codes in capturing these hip dislocation AEs in a\nmulti-center orthopaedic registry. All DL-NLP models out-performed all of the\nML-NLP models, with a convolutional neural network (CNN) model achieving the\nbest overall performance (Kappa = 0.97 for radiology notes, and Kappa = 1.00\nfor follow-up telephone notes). On the other hand, the ICD/CPT codes of the\npatients who sustained a hip dislocation AE were only 75.24% accurate, showing\nthe potential of the proposed model to be used in largescale orthopaedic\nregistries for accurate and efficient hip dislocation AE detection to improve\nthe quality of care and patient outcome.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 16:25:36 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 18:54:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Borjali", "Alireza", ""], ["Magneli", "Martin", ""], ["Shin", "David", ""], ["Malchau", "Henrik", ""], ["Muratoglu", "Orhun K.", ""], ["Varadarajan", "Kartik M.", ""]]}, {"id": "2004.08476", "submitter": "Shuguang Han", "authors": "Shuguang Han, Xuanhui Wang, Mike Bendersky, Marc Najork", "title": "Learning-to-Rank with BERT in TF-Ranking", "comments": "6 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a machine learning algorithm for document (re)ranking,\nin which queries and documents are firstly encoded using BERT [1], and on top\nof that a learning-to-rank (LTR) model constructed with TF-Ranking (TFR) [2] is\napplied to further optimize the ranking performance. This approach is proved to\nbe effective in a public MS MARCO benchmark [3]. Our first two submissions\nachieve the best performance for the passage re-ranking task [4], and the\nsecond best performance for the passage full-ranking task as of April 10, 2020\n[5]. To leverage the lately development of pre-trained language models, we\nrecently integrate RoBERTa [6] and ELECTRA [7]. Our latest submissions improve\nour previously state-of-the-art re-ranking performance by 4.3% [8], and achieve\nthe third best performance for the full-ranking task [9] as of June 8, 2020.\nBoth of them demonstrate the effectiveness of combining ranking losses with\nBERT representations for document ranking.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:45:44 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 05:06:01 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 23:16:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Han", "Shuguang", ""], ["Wang", "Xuanhui", ""], ["Bendersky", "Mike", ""], ["Najork", "Marc", ""]]}, {"id": "2004.08519", "submitter": "Noriyoshi Sukegawa", "authors": "Naoki Nishimura, Noriyoshi Sukegawa, Yuichi Takano, Jiro Iwanaga", "title": "Predicting Online Item-choice Behavior: A Shape-restricted Regression\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the relationship between user pageview (PV) histories and\ntheir item-choice behavior on an e-commerce website. We focus on PV sequences,\nwhich represent time series of the number of PVs for each user--item pair. We\npropose a shape-restricted optimization model that accurately estimates\nitem-choice probabilities for all possible PV sequences. This model imposes\nmonotonicity constraints on item-choice probabilities by exploiting partial\norders for PV sequences, according to the recency and frequency of a user's\nprevious PVs. To improve the computational efficiency of our optimization\nmodel, we devise efficient algorithms for eliminating all redundant constraints\naccording to the transitivity of the partial orders. Experimental results using\nreal-world clickstream data demonstrate that our method achieves higher\nprediction performance than that of a state-of-the-art optimization model and\ncommon machine learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 04:12:40 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 13:18:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nishimura", "Naoki", ""], ["Sukegawa", "Noriyoshi", ""], ["Takano", "Yuichi", ""], ["Iwanaga", "Jiro", ""]]}, {"id": "2004.08731", "submitter": "Brent Biseda", "authors": "Brent Biseda and Katie Mo", "title": "Enhancing Pharmacovigilance with Drug Reviews and Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores whether the use of drug reviews and social media could be\nleveraged as potential alternative sources for pharmacovigilance of adverse\ndrug reactions (ADRs). We examined the performance of BERT alongside two\nvariants that are trained on biomedical papers, BioBERT7, and clinical notes,\nClinical BERT8. A variety of 8 different BERT models were fine-tuned and\ncompared across three different tasks in order to evaluate their relative\nperformance to one another in the ADR tasks. The tasks include sentiment\nclassification of drug reviews, presence of ADR in twitter postings, and named\nentity recognition of ADRs in twitter postings. BERT demonstrates its\nflexibility with high performance across all three different pharmacovigilance\nrelated tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 23:35:24 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Biseda", "Brent", ""], ["Mo", "Katie", ""]]}, {"id": "2004.08762", "submitter": "Cheng Feng", "authors": "Cheng Feng, Xiao Liang, Daniel Schneegass, PengWei Tian", "title": "RelSen: An Optimization-based Framework for Simultaneously Sensor\n  Reliability Monitoring and Data Cleaning", "comments": "accepted in CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.IR cs.NI eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent advances in the Internet of Things (IoT) technology have led to a\nsurge on the popularity of sensing applications. As a result, people\nincreasingly rely on information obtained from sensors to make decisions in\ntheir daily life. Unfortunately, in most sensing applications, sensors are\nknown to be error-prone and their measurements can become misleading at any\nunexpected time. Therefore, in order to enhance the reliability of sensing\napplications, apart from the physical phenomena/processes of interest, we\nbelieve it is also highly important to monitor the reliability of sensors and\nclean the sensor data before analysis on them being conducted. Existing studies\noften regard sensor reliability monitoring and sensor data cleaning as separate\nproblems. In this work, we propose RelSen, a novel optimization-based framework\nto address the two problems simultaneously via utilizing the mutual dependence\nbetween them. Furthermore, RelSen is not application-specific as its\nimplementation assumes a minimal prior knowledge of the process dynamics under\nmonitoring. This significantly improves its generality and applicability in\npractice. In our experiments, we apply RelSen on an outdoor air pollution\nmonitoring system and a condition monitoring system for a cement rotary kiln.\nExperimental results show that our framework can timely identify unreliable\nsensors and remove sensor measurement errors caused by three types of most\ncommonly observed sensor faults.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 03:52:25 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 09:59:10 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 13:23:25 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Feng", "Cheng", ""], ["Liang", "Xiao", ""], ["Schneegass", "Daniel", ""], ["Tian", "PengWei", ""]]}, {"id": "2004.08851", "submitter": "Chandan Biswas", "authors": "Chandan Biswas, Debasis Ganguly and Ujjwal Bhattacharya", "title": "Approximate Nearest Neighbour Search on Privacy-aware Encoding of User\n  Locations to Identify Susceptible Infections in Simulated Epidemics", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amidst an increasing number of infected cases during the Covid-19 pandemic,\nit is essential to trace, as early as possible, the susceptible people who\nmight have been infected by the disease due to their close proximity with\npeople who were tested positive for the virus. This early contact tracing is\nlikely to limit the rate of spread of the infection within a locality. In this\npaper, we investigate how effectively and efficiently can such a list of\nsusceptible people be found given a list of infected persons and their\nlocations. To address this problem from an information retrieval (search)\nperspective, we represent the location of each person at each time instant as a\npoint in a vector space. By using the locations of the given list of infected\npersons as queries, we investigate the feasibility of applying approximate\nnearest neighbour (ANN) based indexing and retrieval approaches to obtain a\nlist of top-k suspected users in real-time. Since leveraging information from\ntrue user location data can lead to security and privacy concerns, we also\ninvestigate what effects does distance-preserving encoding methods have on the\neffectiveness of the ANN methods. Experiments conducted on real and synthetic\ndatasets demonstrate that the top-k retrieved lists of susceptible users\nretrieved with existing ANN approaches (KD-tree and HNSW) yield satisfactory\nprecision and recall values, thus indicating that ANN approaches can\npotentially be applied in practice to facilitate real-time contact tracing even\nunder the presence of imposed privacy constraints.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 13:34:16 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:44:19 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Biswas", "Chandan", ""], ["Ganguly", "Debasis", ""], ["Bhattacharya", "Ujjwal", ""]]}, {"id": "2004.09036", "submitter": "Yefei Zha", "authors": "Yefei Zha, Ruobing Li, Hui Lin", "title": "Gated Convolutional Bidirectional Attention-based Model for Off-topic\n  Spoken Response Detection", "comments": "ACL2020 long paper", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.56", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-topic spoken response detection, the task aiming at predicting whether a\nresponse is off-topic for the corresponding prompt, is important for an\nautomated speaking assessment system. In many real-world educational\napplications, off-topic spoken response detectors are required to achieve high\nrecall for off-topic responses not only on seen prompts but also on prompts\nthat are unseen during training. In this paper, we propose a novel approach for\noff-topic spoken response detection with high off-topic recall on both seen and\nunseen prompts. We introduce a new model, Gated Convolutional Bidirectional\nAttention-based Model (GCBiA), which applies bi-attention mechanism and\nconvolutions to extract topic words of prompts and key-phrases of responses,\nand introduces gated unit and residual connections between major layers to\nbetter represent the relevance of responses and prompts. Moreover, a new\nnegative sampling method is proposed to augment training data. Experiment\nresults demonstrate that our novel approach can achieve significant\nimprovements in detecting off-topic responses with extremely high on-topic\nrecall, for both seen and unseen prompts.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 03:16:06 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 02:12:17 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 02:22:50 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 07:08:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zha", "Yefei", ""], ["Li", "Ruobing", ""], ["Lin", "Hui", ""]]}, {"id": "2004.09167", "submitter": "Akshay Smit", "authors": "Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y. Ng,\n  Matthew P. Lungren", "title": "CheXbert: Combining Automatic Labelers and Expert Annotations for\n  Accurate Radiology Report Labeling Using BERT", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of labels from radiology text reports enables large-scale\ntraining of medical imaging models. Existing approaches to report labeling\ntypically rely either on sophisticated feature engineering based on medical\ndomain knowledge or manual annotations by experts. In this work, we introduce a\nBERT-based approach to medical image report labeling that exploits both the\nscale of available rule-based systems and the quality of expert annotations. We\ndemonstrate superior performance of a biomedically pretrained BERT model first\ntrained on annotations of a rule-based labeler and then finetuned on a small\nset of expert annotations augmented with automated backtranslation. We find\nthat our final model, CheXbert, is able to outperform the previous best\nrules-based labeler with statistical significance, setting a new SOTA for\nreport labeling on one of the largest datasets of chest x-rays.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:46:40 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 05:32:06 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 20:30:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Smit", "Akshay", ""], ["Jain", "Saahil", ""], ["Rajpurkar", "Pranav", ""], ["Pareek", "Anuj", ""], ["Ng", "Andrew Y.", ""], ["Lungren", "Matthew P.", ""]]}, {"id": "2004.09180", "submitter": "Evangelos Pournaras", "authors": "Thomas Asikis, Johannes Klinglmayr, Dirk Helbing, Evangelos Pournaras", "title": "How Value-Sensitive Design Can Empower Sustainable Consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a so-called overpopulated world, sustainable consumption is of existential\nimportance.However, the expanding spectrum of product choices and their\nproduction complexity challenge consumers to make informed and value-sensitive\ndecisions. Recent approaches based on (personalized) psychological manipulation\nare often intransparent, potentially privacy-invasive and inconsistent with\n(informational) self-determination. In contrast, responsible consumption based\non informed choices currently requires reasoning to an extent that tends to\noverwhelm human cognitive capacity. As a result, a collective shift towards\nsustainable consumption remains a grand challenge. Here we demonstrate a novel\npersonal shopping assistant implemented as a smart phone app that supports a\nvalue-sensitive design and leverages sustainability awareness, using experts'\nknowledge and \"wisdom of the crowd\" for transparent product information and\nexplainable product ratings. Real-world field experiments in two supermarkets\nconfirm higher sustainability awareness and a bottom-up behavioral shift\ntowards more sustainable consumption. These results encourage novel business\nmodels for retailers and producers, ethically aligned with consumer preferences\nand with higher sustainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:11:20 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:16:28 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 07:43:13 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 14:56:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Asikis", "Thomas", ""], ["Klinglmayr", "Johannes", ""], ["Helbing", "Dirk", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2004.09327", "submitter": "Peter Hillmann", "authors": "Peter Hillmann, Frank Tietze, Gabi Dreo Rodosek", "title": "Tracemax: A Novel Single Packet IP Traceback Strategy for Data-Flow\n  Analysis", "comments": "Keywords: Computer network management, IP networks, IP packet,\n  Traceback, Packet trace, Denial of Service", "journal-ref": "Local Computer Networks 2015", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of the exact path that packets are routed on in the\nnetwork is quite a challenge. This paper presents a novel, efficient traceback\nstrategy named Tracemax in context of a defense system against distributed\ndenial of service (DDoS) attacks. A single packet can be directly traced over\nmany more hops than the current existing techniques allow. In combination with\na defense system it differentiates between multiple connections. It aims to\nletting non-malicious connections pass while bad ones get thwarted. The novel\nconcept allows detailed analyses of the traffic and the transmission path\nthrough the network. The strategy can effectively reduce the effect of common\nbandwidth and resource consumption attacks, foster early warning and prevention\nas well as higher the availability of the network services for the wanted\ncustomers.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:18:39 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Hillmann", "Peter", ""], ["Tietze", "Frank", ""], ["Rodosek", "Gabi Dreo", ""]]}, {"id": "2004.09338", "submitter": "Venky Soundararajan", "authors": "FNU Shweta, Karthik Murugadoss, Samir Awasthi, AJ Venkatakrishnan,\n  Arjun Puranik, Martin Kang, Brian W. Pickering, John C. O'Horo, Philippe R.\n  Bauer, Raymund R. Razonable, Paschalis Vergidis, Zelalem Temesgen, Stacey\n  Rizza, Maryam Mahmood, Walter R. Wilson, Douglas Challener, Praveen Anand,\n  Matt Liebers, Zainab Doctor, Eli Silvert, Hugo Solomon, Tyler Wagner, Gregory\n  J. Gores, Amy W. Williams, John Halamka, Venky Soundararajan, Andrew D.\n  Badley", "title": "Augmented Curation of Unstructured Clinical Notes from a Massive EHR\n  System Reveals Specific Phenotypic Signature of Impending COVID-19 Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the temporal dynamics of COVID-19 patient phenotypes is\nnecessary to derive fine-grained resolution of pathophysiology. Here we use\nstate-of-the-art deep neural networks over an institution-wide machine\nintelligence platform for the augmented curation of 15.8 million clinical notes\nfrom 30,494 patients subjected to COVID-19 PCR diagnostic testing. By\ncontrasting the Electronic Health Record (EHR)-derived clinical phenotypes of\nCOVID-19-positive (COVIDpos, n=635) versus COVID-19-negative (COVIDneg,\nn=29,859) patients over each day of the week preceding the PCR testing date, we\nidentify anosmia/dysgeusia (37.4-fold), myalgia/arthralgia (2.6-fold), diarrhea\n(2.2-fold), fever/chills (2.1-fold), respiratory difficulty (1.9-fold), and\ncough (1.8-fold) as significantly amplified in COVIDpos over COVIDneg patients.\nThe specific combination of cough and diarrhea has a 3.2-fold amplification in\nCOVIDpos patients during the week prior to PCR testing, and along with\nanosmia/dysgeusia, constitutes the earliest EHR-derived signature of COVID-19\n(4-7 days prior to typical PCR testing date). This study introduces an\nAugmented Intelligence platform for the real-time synthesis of institutional\nknowledge captured in EHRs. The platform holds tremendous potential for scaling\nup curation throughput, with minimal need for retraining underlying neural\nnetworks, thus promising EHR-powered early diagnosis for a broad spectrum of\ndiseases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:10:46 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 19:39:16 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Shweta", "FNU", ""], ["Murugadoss", "Karthik", ""], ["Awasthi", "Samir", ""], ["Venkatakrishnan", "AJ", ""], ["Puranik", "Arjun", ""], ["Kang", "Martin", ""], ["Pickering", "Brian W.", ""], ["O'Horo", "John C.", ""], ["Bauer", "Philippe R.", ""], ["Razonable", "Raymund R.", ""], ["Vergidis", "Paschalis", ""], ["Temesgen", "Zelalem", ""], ["Rizza", "Stacey", ""], ["Mahmood", "Maryam", ""], ["Wilson", "Walter R.", ""], ["Challener", "Douglas", ""], ["Anand", "Praveen", ""], ["Liebers", "Matt", ""], ["Doctor", "Zainab", ""], ["Silvert", "Eli", ""], ["Solomon", "Hugo", ""], ["Wagner", "Tyler", ""], ["Gores", "Gregory J.", ""], ["Williams", "Amy W.", ""], ["Halamka", "John", ""], ["Soundararajan", "Venky", ""], ["Badley", "Andrew D.", ""]]}, {"id": "2004.09424", "submitter": "Keping Bi", "authors": "Keping Bi, Qingyao Ai, W. Bruce Croft", "title": "Learning a Fine-Grained Review-based Transformer Model for Personalized\n  Product Search", "comments": "To appear in SIGIR'2021", "journal-ref": null, "doi": "10.1145/3404835.3462911", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search has been a crucial entry point to serve people shopping\nonline. Most existing personalized product models follow the paradigm of\nrepresenting and matching user intents and items in the semantic space, where\nfiner-grained matching is totally discarded and the ranking of an item cannot\nbe explained further than just user/item level similarity. In addition, while\nsome models in existing studies have created dynamic user representations based\non search context, their representations for items are static across all search\nsessions. This makes every piece of information about the item always equally\nimportant in representing the item during matching with various user intents.\nAware of the above limitations, we propose a review-based transformer model\n(RTM) for personalized product search, which encodes the sequence of query,\nuser reviews, and item reviews with a transformer architecture. RTM conducts\nreview-level matching between the user and item, where each review has a\ndynamic effect according to the context in the sequence. This makes it possible\nto identify useful reviews to explain the scoring. Experimental results show\nthat RTM significantly outperforms state-of-the-art personalized product search\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:29:55 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 18:16:25 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 18:19:31 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bi", "Keping", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2004.09721", "submitter": "Viet Trinh", "authors": "Viet Trinh, Vikrant More, Samira Zare, and Sheideh Homayon", "title": "Quarantine Deceiving Yelp's Users by Detecting Unreliable Rating Reviews", "comments": "5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews have become a valuable and significant resource, for not only\nconsumers but companies, in decision making. In the absence of a trusted\nsystem, highly popular and trustworthy internet users will be assumed as\nmembers of the trusted circle. In this paper, we describe our focus on\nquarantining deceiving Yelp's users that employ both review spike detection\n(RSD) algorithm and spam detection technique in bridging review networks (BRN),\non extracted key features. We found that more than 80% of Yelp's accounts are\nunreliable, and more than 80% of highly-rated businesses are subject to\nspamming.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:44:10 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Trinh", "Viet", ""], ["More", "Vikrant", ""], ["Zare", "Samira", ""], ["Homayon", "Sheideh", ""]]}, {"id": "2004.09906", "submitter": "Jaber Kakar", "authors": "Jaber Kakar and Aydin Sezgin", "title": "Robust Interference Management for SISO Systems with Multiple\n  Over-the-Air Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the over-the-air computation of sums.\nSpecifically, we wish to compute $M\\geq 2$ sums\n$s_m=\\sum_{k\\in\\mathcal{D}m}x_k$ over a shared complex-valued MAC at once with\nminimal mean-squared error ($\\mathsf{MSE}$). Finding appropriate Tx-Rx scaling\nfactors balance between a low error in the computation of $s_n$ and the\ninterference induced by it in the computation of other sums $s_m$, $m\\neq n$.\nIn this paper, we are interested in designing an optimal Tx-Rx scaling policy\nthat minimizes the mean-squared error $\\max_{m\\in[1:M]}\\mathsf{MSE}_m$ subject\nto a Tx power constraint with maximum power $P$. We show that an optimal design\nof the Tx-Rx scaling policy $\\left(\\bar{\\mathbf{a}},\\bar{\\mathbf{b}}\\right)$\ninvolves optimizing (a) their phases and (b) their absolute values in order to\n(i) decompose the computation of $M$ sums into, respectively, $M_R$ and $M_I$\n($M=M_R+M_I$) calculations over real and imaginary part of the Rx signal and\n(ii) to minimize the computation over each part -- real and imaginary --\nindividually. The primary focus of this paper is on (b). We derive conditions\n(i) on the feasibility of the optimization problem and (ii) on the Tx-Rx\nscaling policy of a local minimum for $M_w=2$ computations over the real\n($w=R$) or the imaginary ($w=I$) part. Extensive simulations over a single Rx\nchain for $M_w=2$ show that the level of interference in terms of $\\Delta\nD=|\\mathcal{D}_2|-|\\mathcal{D}_1|$ plays an important role on the ergodic\nworst-case $\\mathsf{MSE}$. At very high $\\mathsf{SNR}$, typically only the\nsensor with the weakest channel transmits with full power while all remaining\nsensors transmit with less to limit the interference. Interestingly, we observe\nthat due to residual interference, the ergodic worst-case $\\mathsf{MSE}$ is not\nvanishing; rather, it converges to $\\frac{|\\mathcal{D}_1||\\mathcal{D}_2|}{K}$\nas $\\mathsf{SNR}\\rightarrow\\infty$.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 11:15:26 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kakar", "Jaber", ""], ["Sezgin", "Aydin", ""]]}, {"id": "2004.09980", "submitter": "David Graus", "authors": "Feng Lu, Anca Dumitrache, David Graus", "title": "Beyond Optimizing for Clicks: Incorporating Editorial Values in News\n  Recommendation", "comments": "To appear in UMAP 2020", "journal-ref": null, "doi": "10.1145/3340631.3394864", "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the uptake of algorithmic personalization in the news domain, news\norganizations increasingly trust automated systems with previously considered\neditorial responsibilities, e.g., prioritizing news to readers. In this paper\nwe study an automated news recommender system in the context of a news\norganization's editorial values. We conduct and present two online studies with\na news recommender system, which span one and a half months and involve over\n1,200 users. In our first study we explore how our news recommender steers\nreading behavior in the context of editorial values such as serendipity,\ndynamism, diversity, and coverage. Next, we present an intervention study where\nwe extend our news recommender to steer our readers to more dynamic reading\nbehavior. We find that (i) our recommender system yields more diverse reading\nbehavior and yields a higher coverage of articles compared to non-personalized\neditorial rankings, and (ii) we can successfully incorporate dynamism in our\nrecommender system as a re-ranking method, effectively steering our readers to\nmore dynamic articles without hurting our recommender system's accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:24:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lu", "Feng", ""], ["Dumitrache", "Anca", ""], ["Graus", "David", ""]]}, {"id": "2004.10009", "submitter": "Lianwei Wu", "authors": "Lianwei Wu and Yuan Rao", "title": "Adaptive Interaction Fusion Networks for Fake News Detection", "comments": "Accepted at the 24th European Conference on Artificial Intelligence\n  (ECAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of existing methods for fake news detection universally focus on\nlearning and fusing various features for detection. However, the learning of\nvarious features is independent, which leads to a lack of cross-interaction\nfusion between features on social media, especially between posts and comments.\nGenerally, in fake news, there are emotional associations and semantic\nconflicts between posts and comments. How to represent and fuse the\ncross-interaction between both is a key challenge. In this paper, we propose\nAdaptive Interaction Fusion Networks (AIFN) to fulfill cross-interaction fusion\namong features for fake news detection. In AIFN, to discover semantic\nconflicts, we design gated adaptive interaction networks (GAIN) to capture\nadaptively similar semantics and conflicting semantics between posts and\ncomments. To establish feature associations, we devise semantic-level fusion\nself-attention networks (SFSN) to enhance semantic correlations and fusion\namong features. Extensive experiments on two real-world datasets, i.e.,\nRumourEval and PHEME, demonstrate that AIFN achieves the state-of-the-art\nperformance and boosts accuracy by more than 2.05% and 1.90%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:51:03 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wu", "Lianwei", ""], ["Rao", "Yuan", ""]]}, {"id": "2004.10035", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Leveraging Cognitive Search Patterns to Enhance Automated Natural\n  Language Retrieval Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search of information in large text repositories has been plagued by the\nso-called document-query vocabulary gap, i.e. the semantic discordance between\nthe contents in the stored document entities on the one hand and the human\nquery on the other hand. Over the past two decades, a significant body of works\nhas advanced technical retrieval prowess while several studies have shed light\non issues pertaining to human search behavior. We believe that these efforts\nshould be conjoined, in the sense that automated retrieval systems have to\nfully emulate human search behavior and thus consider the procedure according\nto which users incrementally enhance their initial query. To this end,\ncognitive reformulation patterns that mimic user search behaviour are\nhighlighted and enhancement terms which are statistically collocated with or\nlexical-semantically related to the original terms adopted in the retrieval\nprocess. We formalize the application of these patterns by considering a query\nconceptual representation and introducing a set of operations allowing to\noperate modifications on the initial query. A genetic algorithm-based weighting\nprocess allows placing emphasis on terms according to their conceptual\nrole-type. An experimental evaluation on real-world datasets against relevance,\nlanguage, conceptual and knowledge-based models is conducted. We also show,\nwhen compared to language and relevance models, a better performance in terms\nof mean average precision than a word embedding-based model instantiation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:13:33 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.10100", "submitter": "Shohei Hisada", "authors": "Shohei Hisada, Taichi Murayama, Kota Tsubouchi, Sumio Fujita, Shuntaro\n  Yada, Shoko Wakamiya, and Eiji Aramaki", "title": "Syndromic surveillance using search query logs and user location\n  information from smartphones against COVID-19 clusters in Japan", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-020-75771-6", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [Background] Two clusters of coronavirus disease 2019 (COVID-19) were\nconfirmed in Hokkaido, Japan in February 2020. To capture the clusters, this\nstudy employs Web search query logs and user location information from\nsmartphones. [Material and Methods] First, we anonymously identified smartphone\nusers who used a Web search engine (Yahoo! JAPAN Search) for the COVID-19 or\nits symptoms via its companion application for smartphones (Yahoo Japan App).\nWe regard these searchers as Web searchers who are suspicious of their own\nCOVID-19 infection (WSSCI). Second, we extracted the location of the WSSCI via\nthe smartphone application. The spatio-temporal distribution of the number of\nWSSCI are compared with the actual location of the known two clusters. [Result\nand Discussion] Before the early stage of the cluster development, we could\nconfirm several WSSCI, which demonstrated the basic feasibility of our\nWSSCI-based approach. However, it is accurate only in the early stage, and it\nwas biased after the public announcement of the cluster development. For the\ncase where the other cluster-related resources, such as fine-grained population\nstatistics, are not available, the proposed metric would be helpful to catch\nthe hint of emerging clusters.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 15:21:30 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hisada", "Shohei", ""], ["Murayama", "Taichi", ""], ["Tsubouchi", "Kota", ""], ["Fujita", "Sumio", ""], ["Yada", "Shuntaro", ""], ["Wakamiya", "Shoko", ""], ["Aramaki", "Eiji", ""]]}, {"id": "2004.10220", "submitter": "Andriy Mulyar", "authors": "Andriy Mulyar and Bridget T. McInnes", "title": "MT-Clinical BERT: Scaling Clinical Information Extraction with Multitask\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes contain an abundance of important but not-readily accessible\ninformation about patients. Systems to automatically extract this information\nrely on large amounts of training data for which their exists limited resources\nto create. Furthermore, they are developed dis-jointly; meaning that no\ninformation can be shared amongst task-specific systems. This bottle-neck\nunnecessarily complicates practical application, reduces the performance\ncapabilities of each individual solution and associates the engineering debt of\nmanaging multiple information extraction systems. We address these challenges\nby developing Multitask-Clinical BERT: a single deep learning model that\nsimultaneously performs eight clinical tasks spanning entity extraction, PHI\nidentification, language entailment and similarity by sharing representations\namongst tasks. We find our single system performs competitively with all\nstate-the-art task-specific systems while also benefiting from massive\ncomputational benefits at inference.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:04:08 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mulyar", "Andriy", ""], ["McInnes", "Bridget T.", ""]]}, {"id": "2004.10225", "submitter": "Long Chen", "authors": "Long Chen, Hanjia Lyu, Tongyu Yang, Yu Wang, Jiebo Luo", "title": "In the Eyes of the Beholder: Analyzing Social Media Use of Neutral and\n  Controversial Terms for COVID-19", "comments": "8 pages, 6 figures, updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the COVID-19 pandemic, \"Chinese Virus\" emerged as a controversial term\nfor coronavirus. To some, it may seem like a neutral term referring to the\nphysical origin of the virus. To many others, however, the term is in fact\nattaching ethnicity to the virus. While both arguments appear reasonable,\nquantitative analysis of the term's real-world usage is lacking to shed light\non the issues behind the controversy. In this paper, we attempt to fill this\ngap. To model the substantive difference of tweets with controversial terms and\nthose with non-controversial terms, we apply topic modeling and LIWC-based\nsentiment analysis. To test whether \"Chinese Virus\" and \"COVID-19\" are\ninterchangeable, we formulate it as a classification task, mask out these\nterms, and classify them using the state-of-the-art transformer models. Our\nexperiments consistently show that the term \"Chinese Virus\" is associated with\ndifferent substantive topics and sentiment compared with \"COVID-19\" and that\nthe two terms are easily distinguishable by looking at their context.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 18:15:45 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 05:12:36 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 14:18:38 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Chen", "Long", ""], ["Lyu", "Hanjia", ""], ["Yang", "Tongyu", ""], ["Wang", "Yu", ""], ["Luo", "Jiebo", ""]]}, {"id": "2004.10265", "submitter": "Nicholas Vincent", "authors": "Nicholas Vincent and Brent Hecht", "title": "A Deeper Investigation of the Importance of Wikipedia Links to the\n  Success of Search Engines", "comments": "This is a pre-print of a paper accepted to the non-archival track of\n  the WikiWorkshop at the Web Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of work has highlighted the important role that Wikipedia's\nvolunteer-created content plays in helping search engines achieve their core\ngoal of addressing the information needs of millions of people. In this paper,\nwe report the results of an investigation into the incidence of Wikipedia links\nin search engine results pages (SERPs). Our results extend prior work by\nconsidering three U.S. search engines, simulating both mobile and desktop\ndevices, and using a spatial analysis approach designed to study modern SERPs\nthat are no longer just \"ten blue links\". We find that Wikipedia links are\nextremely common in important search contexts, appearing in 67-84% of all SERPs\nfor common and trending queries, but less often for medical queries.\nFurthermore, we observe that Wikipedia links often appear in \"Knowledge Panel\"\nSERP elements and are in positions visible to users without scrolling, although\nWikipedia appears less in prominent positions on mobile devices. Our findings\nreinforce the complementary notions that (1) Wikipedia content and research has\nmajor impact outside of the Wikipedia domain and (2) powerful technologies like\nsearch engines are highly reliant on free content created by volunteers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:58:28 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Vincent", "Nicholas", ""], ["Hecht", "Brent", ""]]}, {"id": "2004.10393", "submitter": "Qiang Dong", "authors": "Qiang Dong, Quan Yuan, Yang-Bo Shi", "title": "Alleviating the recommendation bias via rank aggregation", "comments": "published on Physica A: Statistical Mechanics and its Applications,\n  2019, volume 534, article ID: 122073", "journal-ref": "Physica A: Statistical Mechanics and its Applications, 2019,\n  534:122073", "doi": "10.1016/j.physa.2019.122073", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary goal of a recommender system is often known as \"helping users\nfind relevant items\", and a lot of recommendation algorithms are proposed\naccordingly. However, these accuracy-oriented methods usually suffer the\nproblem of recommendation bias on popular items, which is not welcome to not\nonly users but also item providers. To alleviate the recommendation bias\nproblem, we propose a generic rank aggregation framework for the recommendation\nresults of an existing algorithm, in which the user- and item-oriented ranking\nresults are linearly aggregated together, with a parameter controlling the\nweight of the latter ranking process. Experiment results of a typical algorithm\non two real-world data sets show that, this framework is effective to improve\nthe recommendation fairness of any existing accuracy-oriented algorithms, while\navoiding significant accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 04:33:20 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Dong", "Qiang", ""], ["Yuan", "Quan", ""], ["Shi", "Yang-Bo", ""]]}, {"id": "2004.10410", "submitter": "Joeran Beel", "authors": "Mark Grennan, Joeran Beel", "title": "Synthetic vs. Real Reference Strings for Citation Parsing, and the\n  Importance of Re-training and Out-Of-Sample Data for Meaningful Evaluations:\n  Experiments with GROBID, GIANT and Cora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation parsing, particularly with deep neural networks, suffers from a lack\nof training data as available datasets typically contain only a few thousand\ntraining instances. Manually labelling citation strings is very time-consuming,\nhence synthetically created training data could be a solution. However, as of\nnow, it is unknown if synthetically created reference-strings are suitable to\ntrain machine learning algorithms for citation parsing. To find out, we train\nGrobid, which uses Conditional Random Fields, with a) human-labelled reference\nstrings from 'real' bibliographies and b) synthetically created reference\nstrings from the GIANT dataset. We find that both synthetic and organic\nreference strings are equally suited for training Grobid (F1 = 0.74). We\nadditionally find that retraining Grobid has a notable impact on its\nperformance, for both synthetic and real data (+30% in F1). Having as many\ntypes of labelled fields as possible during training also improves\neffectiveness, even if these fields are not available in the evaluation data\n(+13.5% F1). We conclude that synthetic data is suitable for training (deep)\ncitation parsing models. We further suggest that in future evaluations of\nreference parsers both evaluation data similar and dissimilar to the training\ndata should be used for more meaningful evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 06:34:36 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 14:36:52 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Grennan", "Mark", ""], ["Beel", "Joeran", ""]]}, {"id": "2004.10624", "submitter": "Angrosh Mandya", "authors": "Angrosh Mandya, Danushka Bollegala and Frans Coenen", "title": "Contextualised Graph Attention for Improved Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a contextualized graph attention network that combines\nedge features and multiple sub-graphs for improving relation extraction. A\nnovel method is proposed to use multiple sub-graphs to learn rich node\nrepresentations in graph-based networks. To this end multiple sub-graphs are\nobtained from a single dependency tree. Two types of edge features are\nproposed, which are effectively combined with GAT and GCN models to apply for\nrelation extraction. The proposed model achieves state-of-the-art performance\non Semeval 2010 Task 8 dataset, achieving an F1-score of 86.3.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:04:52 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Mandya", "Angrosh", ""], ["Bollegala", "Danushka", ""], ["Coenen", "Frans", ""]]}, {"id": "2004.10816", "submitter": "Majid Asgari-Bidhendi", "authors": "Majid Asgari-Bidhendi, Farzane Fakhrian and Behrouz Minaei-Bidgoli", "title": "ParsEL 1.0: Unsupervised Entity Linking in Persian Social Media Texts", "comments": "8 pages, 3 figures. ParsEL service (source code is available in\n  github)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, social media data has exponentially increased, which can be\nenumerated as one of the largest data repositories in the world. A large\nportion of this social media data is natural language text. However, the\nnatural language is highly ambiguous due to exposure to the frequent\noccurrences of entities, which have polysemous words or phrases. Entity linking\nis the task of linking the entity mentions in the text to their corresponding\nentities in a knowledge base. Recently, FarsBase, a Persian knowledge graph,\nhas been introduced containing almost half a million entities. In this paper,\nwe propose an unsupervised Persian Entity Linking system, the first entity\nlinking system specially focused on the Persian language, which utilizes\ncontext-dependent and context-independent features. For this purpose, we also\npublish the first entity linking corpus of the Persian language containing\n67,595 words that have been crawled from social media texts of some popular\nchannels in the Telegram messenger. The output of the proposed method is 86.94%\nf-score for the Persian language, which is comparable with the similar\nstate-of-the-art methods in the English language.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:34:13 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Asgari-Bidhendi", "Majid", ""], ["Fakhrian", "Farzane", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2004.10878", "submitter": "Sujit Bhattacharya Professor", "authors": "Sujit Bhattacharya and Shubham Singh", "title": "Visible Insights of the Invisible Pandemic: A Scientometric, Altmetric\n  and Topic Trend Analysis", "comments": "21 pages, 4 Figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent SARS-COV-2 virus outbreak has created an unprecedented global\nhealth crisis! The disease is showing alarming trends with the number of people\ngetting infected with this disease, new cases and death rate are all\nhighlighting the need to control this disease at the earliest. The strategy now\nfor the governments around the globe is how to limit the spread of the virus\nuntil the research community develops treatment/drug or vaccination against the\nvirus. The outbreak of this disease has unsurprisingly led to huge volume of\nresearch within a short period of time surrounding this disease. It has also\nled to aggressive social media activity on twitter, Facebook, dedicated blogs,\nnews reports and other online sites actively involved in discussing about the\nvarious aspects of and related to this disease. It becomes a useful and\nchallenging exercise to draw from this huge volume of research, the key papers\nthat form the research front, its influence in the research community, and\nother important research insights. Similarly, it becomes important to discern\nthe key issues that influence the society concerning this disease. The paper is\nmotivated by this. It attempts to distinguish which are the most influential\npapers, the key knowledge base and major topics surrounding the research\ncovered by COVID-19. Further it attempts to capture the society's perception by\ndiscerning key topics that are trending online. The study concludes by\nhighlighting the implications of this study.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 21:53:15 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Bhattacharya", "Sujit", ""], ["Singh", "Shubham", ""]]}, {"id": "2004.11045", "submitter": "Amir Vakili Tahami", "authors": "Amir Vakili Tahami, Kamyar Ghajar, Azadeh Shakery", "title": "Distilling Knowledge for Fast Retrieval-based Chat-bots", "comments": "Accepted for publication in the 43rd International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response retrieval is a subset of neural ranking in which a model selects a\nsuitable response from a set of candidates given a conversation history.\nRetrieval-based chat-bots are typically employed in information seeking\nconversational systems such as customer support agents. In order to make\npairwise comparisons between a conversation history and a candidate response,\ntwo approaches are common: cross-encoders performing full self-attention over\nthe pair and bi-encoders encoding the pair separately. The former gives better\nprediction quality but is too slow for practical use. In this paper, we propose\na new cross-encoder architecture and transfer knowledge from this model to a\nbi-encoder model using distillation. This effectively boosts bi-encoder\nperformance at no cost during inference time. We perform a detailed analysis of\nthis approach on three response retrieval datasets.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 09:41:37 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tahami", "Amir Vakili", ""], ["Ghajar", "Kamyar", ""], ["Shakery", "Azadeh", ""]]}, {"id": "2004.11081", "submitter": "Mohammed Belkhatir", "authors": "Mohammed Maree, Mohammed Belkhatir", "title": "Coupling semantic and statistical techniques for dynamically enriching\n  web ontologies", "comments": null, "journal-ref": "Journal Intelligent Information Systems 2013", "doi": "10.1007/s10844-012-0233-4", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the Semantic Web technology, the use of ontologies to\nstore and retrieve information covering several domains has increased. However,\nvery few ontologies are able to cope with the ever-growing need of frequently\nupdated semantic information or specific user requirements in specialized\ndomains. As a result, a critical issue is related to the unavailability of\nrelational information between concepts, also coined missing background\nknowledge. One solution to address this issue relies on the manual enrichment\nof ontologies by domain experts which is however a time consuming and costly\nprocess, hence the need for dynamic ontology enrichment. In this paper we\npresent an automatic coupled statistical/semantic framework for dynamically\nenriching large-scale generic ontologies from the World Wide Web. Using the\nmassive amount of information encoded in texts on the Web as a corpus, missing\nbackground knowledge can therefore be discovered through a combination of\nsemantic relatedness measures and pattern acquisition techniques and\nsubsequently exploited. The benefits of our approach are: (i) proposing the\ndynamic enrichment of large-scale generic ontologies with missing background\nknowledge, and thus, enabling the reuse of such knowledge, (ii) dealing with\nthe issue of costly ontological manual enrichment by domain experts.\nExperimental results in a precision-based evaluation setting demonstrate the\neffectiveness of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:21:30 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Maree", "Mohammed", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11083", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Coupled intrinsic and extrinsic human language resource-based query\n  expansion", "comments": null, "journal-ref": "Knowledge & Information Systems 2018", "doi": "10.1007/s10115-018-1267-x", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor information retrieval performance has often been attributed to the\nquery-document vocabulary mismatch problem which is defined as the difficulty\nfor human users to formulate precise natural language queries that are in line\nwith the vocabulary of the documents deemed relevant to a specific search goal.\nTo alleviate this problem, query expansion processes are applied in order to\nspawn and integrate additional terms to an initial query. This requires\naccurate identification of main query concepts to ensure the intended search\ngoal is duly emphasized and relevant expansion concepts are extracted and\nincluded in the enriched query. Natural language queries have intrinsic\nlinguistic properties such as parts-of-speech labels and grammatical relations\nwhich can be utilized in determining the intended search goal. Additionally,\nextrinsic language-based resources such as ontologies are needed to suggest\nexpansion concepts semantically coherent with the query content. We present\nhere a query expansion framework which capitalizes on both linguistic\ncharacteristics of user queries and ontology resources for query constituent\nencoding, expansion concept extraction and concept weighting. A thorough\nempirical evaluation on real-world datasets validates our approach against\nunigram language model, relevance model and a sequential dependence based\ntechnique.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:22:38 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11093", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "Natural language technology and query expansion: issues,\n  state-of-the-art and perspectives", "comments": null, "journal-ref": "J Intell Inf Syst 2012", "doi": "10.1007/s10844-011-0174-3", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of an abundance of knowledge sources has spurred a large\namount of effort in the development and enhancement of Information Retrieval\ntechniques. Users information needs are expressed in natural language and\nsuccessful retrieval is very much dependent on the effective communication of\nthe intended purpose. Natural language queries consist of multiple linguistic\nfeatures which serve to represent the intended search goal. Linguistic\ncharacteristics that cause semantic ambiguity and misinterpretation of queries\nas well as additional factors such as the lack of familiarity with the search\nenvironment affect the users ability to accurately represent their information\nneeds, coined by the concept intention gap. The latter directly affects the\nrelevance of the returned search results which may not be to the users\nsatisfaction and therefore is a major issue impacting the effectiveness of\ninformation retrieval systems. Central to our discussion is the identification\nof the significant constituents that characterize the query intent and their\nenrichment through the addition of meaningful terms, phrases or even latent\nrepresentations, either manually or automatically to capture their intended\nmeaning. Specifically, we discuss techniques to achieve the enrichment and in\nparticular those utilizing the information gathered from statistical processing\nof term dependencies within a document corpus or from external knowledge\nsources such as ontologies. We lay down the anatomy of a generic linguistic\nbased query expansion framework and propose its module-based decomposition,\ncovering topical issues from query processing, information retrieval,\ncomputational linguistics and ontology engineering. For each of the modules we\nreview state-of-the-art solutions in the literature categorized and analyzed\nunder the light of the techniques used.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:39:07 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.11131", "submitter": "Mukund Srinath", "authors": "Mukund Srinath, Shomir Wilson, C. Lee Giles", "title": "Privacy at Scale: Introducing the PrivaSeer Corpus of Web Privacy\n  Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisations disclose their privacy practices by posting privacy policies on\ntheir website. Even though users often care about their digital privacy, they\noften don't read privacy policies since they require a significant investment\nin time and effort. Although natural language processing can help in privacy\npolicy understanding, there has been a lack of large scale privacy policy\ncorpora that could be used to analyse, understand, and simplify privacy\npolicies. Thus, we create PrivaSeer, a corpus of over one million English\nlanguage website privacy policies, which is significantly larger than any\npreviously available corpus. We design a corpus creation pipeline which\nconsists of crawling the web followed by filtering documents using language\ndetection, document classification, duplicate and near-duplication removal, and\ncontent extraction. We investigate the composition of the corpus and show\nresults from readability tests, document similarity, keyphrase extraction, and\nexplored the corpus through topic modeling.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 13:21:00 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Srinath", "Mukund", ""], ["Wilson", "Shomir", ""], ["Giles", "C. Lee", ""]]}, {"id": "2004.11141", "submitter": "Mirko Polato", "authors": "Tommaso Carraro, Mirko Polato, Fabio Aiolli", "title": "Conditioned Variational Autoencoder for top-N item recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Conditioned Variational Autoencoder (C-VAE) for\nconstrained top-N item recommendation where the recommended items must satisfy\na given condition. The proposed model architecture is similar to a standard VAE\nin which the condition vector is fed into the encoder. The constrained ranking\nis learned during training thanks to a new reconstruction loss that takes the\ninput condition into account. We show that our model generalizes the\nstate-of-the-art Mult-VAE collaborative filtering model. Moreover, we provide\ninsights on what C-VAE learns in the latent space, providing a human-friendly\ninterpretation. Experimental results underline the potential of C-VAE in\nproviding accurate recommendations under constraints. Finally, the performed\nanalyses suggest that C-VAE can be used in other recommendation scenarios, such\nas context-aware recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 22:29:34 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 16:15:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Carraro", "Tommaso", ""], ["Polato", "Mirko", ""], ["Aiolli", "Fabio", ""]]}, {"id": "2004.11339", "submitter": "Jimmy Lin", "authors": "Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil Gupta, Phuong Cam,\n  Kyunghyun Cho, Jimmy Lin", "title": "Rapidly Bootstrapping a Question Answering Dataset for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CovidQA, the beginnings of a question answering dataset\nspecifically designed for COVID-19, built by hand from knowledge gathered from\nKaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is\nthe first publicly available resource of its type, and intended as a stopgap\nmeasure for guiding research until more substantial evaluation resources become\navailable. While this dataset, comprising 124 question-article pairs as of the\npresent version 0.1 release, does not have sufficient examples for supervised\nmachine learning, we believe that it can be helpful for evaluating the\nzero-shot or transfer capabilities of existing models on topics specifically\nrelated to COVID-19. This paper describes our methodology for constructing the\ndataset and presents the effectiveness of a number of baselines, including\nterm-based techniques and various transformer-based models. The dataset is\navailable at http://covidqa.ai/\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:35:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tang", "Raphael", ""], ["Nogueira", "Rodrigo", ""], ["Zhang", "Edwin", ""], ["Gupta", "Nikhil", ""], ["Cam", "Phuong", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2004.11464", "submitter": "Jocelyn Mazarura", "authors": "Jocelyn Mazarura, Alta de Waal and Pieter de Villiers", "title": "A Gamma-Poisson Mixture Topic Model for Short Text", "comments": "26 pages, 14 Figures, to be published in Mathematical Problems in\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most topic models are constructed under the assumption that documents follow\na multinomial distribution. The Poisson distribution is an alternative\ndistribution to describe the probability of count data. For topic modelling,\nthe Poisson distribution describes the number of occurrences of a word in\ndocuments of fixed length. The Poisson distribution has been successfully\napplied in text classification, but its application to topic modelling is not\nwell documented, specifically in the context of a generative probabilistic\nmodel. Furthermore, the few Poisson topic models in literature are admixture\nmodels, making the assumption that a document is generated from a mixture of\ntopics. In this study, we focus on short text. Many studies have shown that the\nsimpler assumption of a mixture model fits short text better. With mixture\nmodels, as opposed to admixture models, the generative assumption is that a\ndocument is generated from a single topic. One topic model, which makes this\none-topic-per-document assumption, is the Dirichlet-multinomial mixture model.\nThe main contributions of this work are a new Gamma-Poisson mixture model, as\nwell as a collapsed Gibbs sampler for the model. The benefit of the collapsed\nGibbs sampler derivation is that the model is able to automatically select the\nnumber of topics contained in the corpus. The results show that the\nGamma-Poisson mixture model performs better than the Dirichlet-multinomial\nmixture model at selecting the number of topics in labelled corpora.\nFurthermore, the Gamma-Poisson mixture produces better topic coherence scores\nthan the Dirichlet-multinomial mixture model, thus making it a viable option\nfor the challenging task of topic modelling of short text.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 21:13:53 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mazarura", "Jocelyn", ""], ["de Waal", "Alta", ""], ["de Villiers", "Pieter", ""]]}, {"id": "2004.11529", "submitter": "Yong Liu Stephen", "authors": "Susen Yang, Yong Liu, Yonghui Xu, Chunyan Miao, Min Wu, Juyong Zhang", "title": "Contextualized Graph Attention Network for Recommendation with Item\n  Knowledge Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) have recently been applied to exploit knowledge\ngraph (KG) for recommendation. Existing GNN-based methods explicitly model the\ndependency between an entity and its local graph context in KG (i.e., the set\nof its first-order neighbors), but may not be effective in capturing its\nnon-local graph context (i.e., the set of most related high-order neighbors).\nIn this paper, we propose a novel recommendation framework, named\nContextualized Graph Attention Network (CGAT), which can explicitly exploit\nboth local and non-local graph context information of an entity in KG.\nSpecifically, CGAT captures the local context information by a user-specific\ngraph attention mechanism, considering a user's personalized preferences on\nentities. Moreover, CGAT employs a biased random walk sampling process to\nextract the non-local context of an entity, and utilizes a Recurrent Neural\nNetwork (RNN) to model the dependency between the entity and its non-local\ncontextual entities. To capture the user's personalized preferences on items,\nan item-specific attention mechanism is also developed to model the dependency\nbetween a target item and the contextual items extracted from the user's\nhistorical behaviors. Experimental results on real datasets demonstrate the\neffectiveness of CGAT, compared with state-of-the-art KG-based recommendation\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 04:27:50 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Yang", "Susen", ""], ["Liu", "Yong", ""], ["Xu", "Yonghui", ""], ["Miao", "Chunyan", ""], ["Wu", "Min", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.11588", "submitter": "Yong Liu Stephen", "authors": "Yong Liu, Susen Yang, Yinan Zhang, Chunyan Miao, Zaiqing Nie, Juyong\n  Zhang", "title": "Learning Hierarchical Review Graph Representations for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The user review data have been demonstrated to be effective in solving\ndifferent recommendation problems. Previous review-based recommendation methods\nusually employ sophisticated compositional models, such as Recurrent Neural\nNetworks (RNN) and Convolutional Neural Networks (CNN), to learn semantic\nrepresentations from the review data for recommendation. However, these methods\nmainly capture the local dependency between neighbouring words in a word\nwindow, and they treat each review equally. Therefore, they may not be\neffective in capturing the global dependency between words, and tend to be\neasily biased by noise review information. In this paper, we propose a novel\nreview-based recommendation model, named Review Graph Neural Network (RGNN).\nSpecifically, RGNN builds a specific review graph for each individual\nuser/item, which provides a global view about the user/item properties to help\nweaken the biases caused by noise review information. A type-aware graph\nattention mechanism is developed to learn semantic embeddings of words.\nMoreover, a personalized graph pooling operator is proposed to learn\nhierarchical representations of the review graph to form the semantic\nrepresentation for each user/item. We compared RGNN with state-of-the-art\nreview-based recommendation approaches on two real-world datasets. The\nexperimental results indicate that RGNN consistently outperforms baseline\nmethods, in terms of Mean Square Error (MSE).\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 08:01:10 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 00:59:29 GMT"}, {"version": "v3", "created": "Sun, 24 Jan 2021 07:42:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Yong", ""], ["Yang", "Susen", ""], ["Zhang", "Yinan", ""], ["Miao", "Chunyan", ""], ["Nie", "Zaiqing", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.11662", "submitter": "Ya-Hui An", "authors": "Ya-Hui An and Qiang Dong and Quan Yuan and Chao Wang", "title": "Improving Recommendation Diversity by Highlighting the ExTrA Fabricated\n  Experts", "comments": null, "journal-ref": "in IEEE Access, vol. 8, pp. 64422-64433, 2020", "doi": "10.1109/ACCESS.2020.2984365", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, recommender systems (RSes) are becoming increasingly important to\nindividual users and business marketing, especially in the online e-commerce\nscenarios. However, while the majority of recommendation algorithms proposed in\nthe literature have focused their efforts on improving prediction accuracy,\nother important aspects of recommendation quality, such as diversity of\nrecommendations, have been more or less overlooked. In the latest decade,\nrecommendation diversity has drawn more research attention, especially in the\nmodels based on user-item bipartite networks. In this paper, we introduce a\nfamily of approaches to extract fabricated experts from users in RSes, named as\nthe Expert Tracking Approaches (ExTrA for short), and explore the capability of\nthese fabricated experts in improving the recommendation diversity, by\nhighlighting them in a well-known bipartite network-based method, called the\nMass Diffusion (MD for short) model. These ExTrA-based models are compared with\ntwo state-of-the-art MD-improved models HHP and BHC, with respect to\nrecommendation accuracy and diversity. Comprehensive empirical results on three\nreal-world datasets MovieLens, Netflix and RYM show that, our proposed\nExTrA-based models can achieve significant diversity gain while maintain\ncomparable level of recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:26:40 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["An", "Ya-Hui", ""], ["Dong", "Qiang", ""], ["Yuan", "Quan", ""], ["Wang", "Chao", ""]]}, {"id": "2004.11694", "submitter": "Navedanjum Ansari Mr", "authors": "Navedanjum Ansari, Rajesh Sharma", "title": "Identifying Semantically Duplicate Questions Using Data Science\n  Approach: A Quora Case Study", "comments": "11 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying semantically identical questions on, Question and Answering\nsocial media platforms like Quora is exceptionally significant to ensure that\nthe quality and the quantity of content are presented to users, based on the\nintent of the question and thus enriching overall user experience. Detecting\nduplicate questions is a challenging problem because natural language is very\nexpressive, and a unique intent can be conveyed using different words, phrases,\nand sentence structuring. Machine learning and deep learning methods are known\nto have accomplished superior results over traditional natural language\nprocessing techniques in identifying similar texts. In this paper, taking Quora\nfor our case study, we explored and applied different machine learning and deep\nlearning techniques on the task of identifying duplicate questions on Quora's\ndataset. By using feature engineering, feature importance techniques, and\nexperimenting with seven selected machine learning classifiers, we demonstrated\nthat our models outperformed previous studies on this task. Xgboost model with\ncharacter level term frequency and inverse term frequency is our best machine\nlearning model that has also outperformed a few of the Deep learning baseline\nmodels. We applied deep learning techniques to model four different deep neural\nnetworks of multiple layers consisting of Glove embeddings, Long Short Term\nMemory, Convolution, Max pooling, Dense, Batch Normalization, Activation\nfunctions, and model merge. Our deep learning models achieved better accuracy\nthan machine learning models. Three out of four proposed architectures\noutperformed the accuracy from previous machine learning and deep learning\nresearch work, two out of four models outperformed accuracy from previous deep\nlearning study on Quora's question pair dataset, and our best model achieved\naccuracy of 85.82% which is close to Quora state of the art accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 19:39:58 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ansari", "Navedanjum", ""], ["Sharma", "Rajesh", ""]]}, {"id": "2004.11695", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Rita Orji, Hucheng Huang", "title": "Deep Sentiment Classification and Topic Discovery on Novel Coronavirus\n  or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet forums and public social media, such as online healthcare forums,\nprovide a convenient channel for users (people/patients) concerned about health\nissues to discuss and share information with each other. In late December 2019,\nan outbreak of a novel coronavirus (infection from which results in the disease\nnamed COVID-19) was reported, and, due to the rapid spread of the virus in\nother parts of the world, the World Health Organization declared a state of\nemergency. In this paper, we used automated extraction of COVID-19 related\ndiscussions from social media and a natural language process (NLP) method based\non topic modeling to uncover various issues related to COVID-19 from public\nopinions. Moreover, we also investigate how to use LSTM recurrent neural\nnetwork for sentiment classification of COVID-19 comments. Our findings shed\nlight on the importance of using public opinions and suitable computational\ntechniques to understand issues surrounding COVID-19 and to guide related\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 16:29:13 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Orji", "Rita", ""], ["Huang", "Hucheng", ""]]}, {"id": "2004.11699", "submitter": "Sasan Harifi", "authors": "Mohammad Moradi, Elham Ghanbari, Mehrdad Maeen, Sasan Harifi", "title": "An approach based on Combination of Features for automatic news\n  retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, according to the increasingly increasing information, the\nimportance of its presentation is also increasing. The internet has become one\nof the main sources of information for users and their favorite topics. It also\nprovides access to more information. Understanding this information is very\nimportant for providing the best set of information resources for users.\nContent providers now need a precise and efficient way to retrieve news with\nthe least human help. Data mining has led to the emergence of new methods for\ndetecting related and unrelated documents. Although the conceptual relationship\nbetween documents may be negligible, it is important to provide useful\ninformation and relevant content to users. In this paper, a new approach based\non the Combination of Features (CoF) for information retrieval operations is\nintroduced. Along with introducing this new approach, we proposed a dataset by\nidentifying the most commonly used keywords in documents and using the most\nappropriate documents to help them with the abundance of vocabulary. Then,\nusing the proposed approach, techniques of text categorization, evaluation\ncriteria and ranking algorithms, the data were analyzed and examined. The\nevaluation results show that using the combination of features approach\nimproves the quality and effects on efficient ranking.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 10:11:09 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Moradi", "Mohammad", ""], ["Ghanbari", "Elham", ""], ["Maeen", "Mehrdad", ""], ["Harifi", "Sasan", ""]]}, {"id": "2004.11706", "submitter": "Sanjay Kumar Sonbhadra Mr.", "authors": "Sanjay Kumar Sonbhadra, Sonali Agarwal and P. Nagabhushan", "title": "Target specific mining of COVID-19 scholarly articles using one-class\n  approach", "comments": null, "journal-ref": "Chaos, Solitons and Fractals, 2020", "doi": "10.1016/j.chaos.2020.110155", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several research articles have been published in the field\nof corona-virus caused diseases like severe acute respiratory syndrome (SARS),\nmiddle east respiratory syndrome (MERS) and COVID-19. In the presence of\nnumerous research articles, extracting best-suited articles is time-consuming\nand manually impractical. The objective of this paper is to extract the\nactivity and trends of corona-virus related research articles using machine\nlearning approaches. The COVID-19 open research dataset (CORD-19) is used for\nexperiments, whereas several target-tasks along with explanations are defined\nfor classification, based on domain knowledge. Clustering techniques are used\nto create the different clusters of available articles, and later the task\nassignment is performed using parallel one-class support vector machines\n(OCSVMs). Experiments with original and reduced features validate the\nperformance of the approach. It is evident that the k-means clustering\nalgorithm, followed by parallel OCSVMs, outperforms other methods for both\noriginal and reduced feature space.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 12:39:54 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 13:31:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""], ["Nagabhushan", "P.", ""]]}, {"id": "2004.11718", "submitter": "Shoujin Wang", "authors": "Shoujin Wang, Liang Hu, Yan Wang, Xiangnan He, Quan Z. Sheng, Mehmet\n  Orgun, Longbing Cao, Nan Wang, Francesco Ricci, Philip S. Yu", "title": "Graph Learning Approaches to Recommender Systems: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the fast development of the emerging topic of\nGraph Learning based Recommender Systems (GLRS). GLRS mainly employ the\nadvanced graph learning approaches to model users' preferences and intentions\nas well as items' characteristics and popularity for Recommender Systems (RS).\nDifferently from conventional RS, including content based filtering and\ncollaborative filtering, GLRS are built on simple or complex graphs where\nvarious objects, e.g., users, items, and attributes, are explicitly or\nimplicitly connected. With the rapid development of graph learning, exploring\nand exploiting homogeneous or heterogeneous relations in graphs is a promising\ndirection for building advanced RS. In this paper, we provide a systematic\nreview of GLRS, on how they obtain the knowledge from graphs to improve the\naccuracy, reliability and explainability for recommendations. First, we\ncharacterize and formalize GLRS, and then summarize and categorize the key\nchallenges in this new research area. Then, we survey the most recent and\nimportant developments in the area. Finally, we share some new research\ndirections in this vibrant area.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:13:58 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wang", "Shoujin", ""], ["Hu", "Liang", ""], ["Wang", "Yan", ""], ["He", "Xiangnan", ""], ["Sheng", "Quan Z.", ""], ["Orgun", "Mehmet", ""], ["Cao", "Longbing", ""], ["Wang", "Nan", ""], ["Ricci", "Francesco", ""], ["Yu", "Philip S.", ""]]}, {"id": "2004.11759", "submitter": "Jibril Frej", "authors": "Jibril Frej and Phillipe Mulhem and Didier Schwab and Jean-Pierre\n  Chevallet", "title": "Learning Term Discrimination", "comments": "Accepted to ACM SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Document indexing is a key component for efficient information retrieval\n(IR). After preprocessing steps such as stemming and stop-word removal,\ndocument indexes usually store term-frequencies (tf). Along with tf (that only\nreflects the importance of a term in a document), traditional IR models use\nterm discrimination values (TDVs) such as inverse document frequency (idf) to\nfavor discriminative terms during retrieval. In this work, we propose to learn\nTDVs for document indexing with shallow neural networks that approximate\ntraditional IR ranking functions such as TF-IDF and BM25. Our proposal\noutperforms, both in terms of nDCG and recall, traditional approaches, even\nwith few positively labelled query-document pairs as learning data. Our learned\nTDVs, when used to filter out terms of the vocabulary that have zero\ndiscrimination value, allow to both significantly lower the memory footprint of\nthe inverted index and speed up the retrieval process (BM25 is up to 3~times\nfaster), without degrading retrieval quality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:00:50 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:59:16 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 08:15:04 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Frej", "Jibril", ""], ["Mulhem", "Phillipe", ""], ["Schwab", "Didier", ""], ["Chevallet", "Jean-Pierre", ""]]}, {"id": "2004.11894", "submitter": "Rezarta Islamaj", "authors": "Rezarta Islamaj, Dongseop Kwon, Sun Kim, Zhiyong Lu", "title": "TeamTat: a collaborative text annotation tool", "comments": "11 pages, 4 figures, NAR Web Server Issue 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually annotated data is key to developing text-mining and\ninformation-extraction algorithms. However, human annotation requires\nconsiderable time, effort and expertise. Given the rapid growth of biomedical\nliterature, it is paramount to build tools that facilitate speed and maintain\nexpert quality. While existing text annotation tools may provide user-friendly\ninterfaces to domain experts, limited support is available for image display,\nproject management, and multi-user team annotation. In response, we developed\nTeamTat (teamtat.org), a web-based annotation tool (local setup available),\nequipped to manage team annotation projects engagingly and efficiently. TeamTat\nis a novel tool for managing multi-user, multi-label document annotation,\nreflecting the entire production life cycle. Project managers can specify\nannotation schema for entities and relations and select annotator(s) and\ndistribute documents anonymously to prevent bias. Document input format can be\nplain text, PDF or BioC, (uploaded locally or automatically retrieved from\nPubMed or PMC), and output format is BioC with inline annotations. TeamTat\ndisplays figures from the full text for the annotators convenience. Multiple\nusers can work on the same document independently in their workspaces, and the\nteam manager can track task completion. TeamTat provides corpus-quality\nassessment via inter-annotator agreement statistics, and a user-friendly\ninterface convenient for annotation review and inter-annotator disagreement\nresolution to improve corpus quality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:58:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Islamaj", "Rezarta", ""], ["Kwon", "Dongseop", ""], ["Kim", "Sun", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2004.11980", "submitter": "Rishiraj Saha Roy", "authors": "Rishiraj Saha Roy, Avishek Anand", "title": "Question Answering over Curated and Open Web Sources", "comments": "SIGIR 2020 Tutorial", "journal-ref": null, "doi": "10.1145/3397271.3401421", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last few years have seen an explosion of research on the topic of\nautomated question answering (QA), spanning the communities of information\nretrieval, natural language processing, and artificial intelligence. This\ntutorial would cover the highlights of this really active period of growth for\nQA to give the audience a grasp over the families of algorithms that are\ncurrently being used. We partition research contributions by the underlying\nsource from where answers are retrieved: curated knowledge graphs, unstructured\ntext, or hybrid corpora. We choose this dimension of partitioning as it is the\nmost discriminative when it comes to algorithm design. Other key dimensions are\ncovered within each sub-topic: like the complexity of questions addressed, and\ndegrees of explainability and interactivity introduced in the systems. We would\nconclude the tutorial with the most promising emerging trends in the expanse of\nQA, that would help new entrants into this field make the best decisions to\ntake the community forward. Much has changed in the community since the last\ntutorial on QA in SIGIR 2016, and we believe that this timely overview will\nindeed benefit a large number of conference participants.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:35:11 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 18:30:58 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 20:42:15 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 11:36:15 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Roy", "Rishiraj Saha", ""], ["Anand", "Avishek", ""]]}, {"id": "2004.12034", "submitter": "Mohammed Belkhatir", "authors": "M. Maree, A. Kmail, M. Belkhatir", "title": "Analysis & Shortcomings of E-Recruitment Systems: Towards a\n  Semantics-based Approach Addressing Knowledge Incompleteness and Limited\n  Domain Coverage", "comments": null, "journal-ref": "J. Inf. Sci. 45(6) (2019)", "doi": "10.1177/0165551518811449", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of the Internet has led to introducing new methods for\ne-recruitment and human resources management. These methods aim to\nsystematically address the limitations of conventional recruitment procedures\nthrough incorporating natural language processing tools and semantics-based\nmethods. In this context, for a given job post, applicant resumes (usually\nuploaded as free-text unstructured documents in different formats such as .pdf,\n.doc, or .rtf) are matched/screened out using the conventional keyword-based\nmodel enriched by additional resources such as occupational categories and\nsemantics-based techniques. Employing these techniques has proved to be\neffective in reducing the cost, time, and efforts required in traditional\nrecruitment and candidate selection methods. However, the skill gap, i.e. the\npropensity to precisely detect and extract relevant skills in applicant resumes\nand job posts, and the hidden semantic dimensions encoded in applicant resumes\nstill form a major obstacle for e-recruitment systems. This is due to the fact\nthat resources exploited by current e-recruitment systems are obtained from\ngeneric domain-independent sources, therefore resulting in knowledge\nincompleteness and the lack of domain coverage. In this paper, we review\nstate-of-the-art e-recruitment approaches and highlight recent advancements in\nthis domain. An e-recruitment framework addressing current shortcomings through\nthe use of multiple cooperative semantic resources, feature extraction\ntechniques and skill relatedness measures is detailed. An instantiation of the\nproposed framework is proposed and an experimental validation using a\nreal-world recruitment dataset from two employment portals demonstrates the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:25:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Maree", "M.", ""], ["Kmail", "A.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2004.12038", "submitter": "Mohammed Belkhatir", "authors": "M. Belkhatir", "title": "Fuzzy Logic Based Integration of Web Contextual Linguistic Structures\n  for Enriching Conceptual Visual Representations", "comments": null, "journal-ref": null, "doi": "10.1109/TETCI.2018.2849417", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the difficulty of automatically mapping visual features with semantic\ndescriptors, state-of-the-art frameworks have exhibited poor performance in\nterms of coverage and effectiveness for indexing the visual content. This\nprompted us to investigate the use of both the Web as a large information\nsource from where to extract relevant contextual linguistic information and\nbimodal visual-textual indexing as a technique to enrich the vocabulary of\nindex concepts. Our proposal is based on the Signal/Semantic approach for\nmultimedia indexing which generates multi-facetted conceptual representations\nof the visual content. We propose to enrich these image representations with\nconcepts automatically extracted from the visual contextual information. We\nspecifically target the integration of semantic concepts which are more\nspecific than the initial index concepts since they represent the visual\ncontent with greater accuracy and precision. Also, we aim to correct the faulty\nindexes resulting from the automatic semantic tagging. Experimentally, the\ndetails of the prototyping are given and the presented technique is tested in a\nWeb-scale evaluation on 30 queries representing elaborate image scenes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:49:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Belkhatir", "M.", ""]]}, {"id": "2004.12118", "submitter": "Feng Liu", "authors": "Feng Liu, Weiwen Liu, Xutao Li, Yunming Ye", "title": "Inter-sequence Enhanced Framework for Personalized Sequential\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the sequential correlation of users' historical interactions is\nessential in sequential recommendation. However, the majority of the approaches\nmainly focus on modeling the \\emph{intra-sequence} item correlation within each\nindividual sequence but neglect the \\emph{inter-sequence} item correlation\nacross different user interaction sequences. Though several studies have been\naware of this issue, their method is either simple or implicit. To make better\nuse of such information, we propose an inter-sequence enhanced framework for\nthe Sequential Recommendation (ISSR). In ISSR, both inter-sequence and\nintra-sequence item correlation are considered. Firstly, we equip graph neural\nnetworks in the inter-sequence correlation encoder to capture the high-order\nitem correlation from the user-item bipartite graph and the item-item graph.\nThen, based on the inter-sequence correlation encoder, we build GRU network and\nattention network in the intra-sequence correlation encoder to model the item\nsequential correlation within each individual sequence and temporal dynamics\nfor predicting users' preferences over candidate items. Additionally, we\nconduct extensive experiments on three real-world datasets. The experimental\nresults demonstrate the superiority of ISSR over many state-of-the-art methods\nand the effectiveness of the inter-sequence correlation encoder.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 11:57:00 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 03:23:44 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liu", "Feng", ""], ["Liu", "Weiwen", ""], ["Li", "Xutao", ""], ["Ye", "Yunming", ""]]}, {"id": "2004.12161", "submitter": "Yutao Ma", "authors": "Liwei Huang, Yutao Ma, Yanbo Liu, Keqing He", "title": "DAN-SNR: A Deep Attentive Network for Social-Aware Next\n  Point-of-Interest Recommendation", "comments": "25 pages, 7 figures, and 6 tables", "journal-ref": "ACM Transactions on Internet Technology, 2021, 21(1): 2:1-2:27", "doi": "10.1145/3430504", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next (or successive) point-of-interest (POI) recommendation has attracted\nincreasing attention in recent years. Most of the previous studies attempted to\nincorporate the spatiotemporal information and sequential patterns of user\ncheck-ins into recommendation models to predict the target user's next move.\nHowever, none of these approaches utilized the social influence of each user's\nfriends. In this study, we discuss a new topic of next POI recommendation and\npresent a deep attentive network for social-aware next POI recommendation\ncalled DAN-SNR. In particular, the DAN-SNR makes use of the self-attention\nmechanism instead of the architecture of recurrent neural networks to model\nsequential influence and social influence in a unified manner. Moreover, we\ndesign and implement two parallel channels to capture short-term user\npreference and long-term user preference as well as social influence,\nrespectively. By leveraging multi-head self-attention, the DAN-SNR can model\nlong-range dependencies between any two historical check-ins efficiently and\nweigh their contributions to the next destination adaptively. Also, we carried\nout a comprehensive evaluation using large-scale real-world datasets collected\nfrom two popular location-based social networks, namely Gowalla and Brightkite.\nExperimental results indicate that the DAN-SNR outperforms seven competitive\nbaseline approaches regarding recommendation performance and is of high\nefficiency among six neural-network- and attention-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:52:11 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Huang", "Liwei", ""], ["Ma", "Yutao", ""], ["Liu", "Yanbo", ""], ["He", "Keqing", ""]]}, {"id": "2004.12184", "submitter": "Ganesh Bagler Dr", "authors": "Nirav Diwan, Devansh Batra and Ganesh Bagler", "title": "A Named Entity Based Approach to Model Recipes", "comments": "36th IEEE International Conference on Data Engineering (ICDE 2020),\n  DECOR Workshop; 6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cooking recipes follow a structure which can be modelled very\nwell if the rules and semantics of the different sections of the recipe text\nare analyzed and represented accurately. We propose a structure that can\naccurately represent the recipe as well as a pipeline to infer the best\nrepresentation of the recipe in this uniform structure. The Ingredients section\nin a recipe typically lists down the ingredients required and corresponding\nattributes such as quantity, temperature, and processing state. This can be\nmodelled by defining these attributes and their values. The physical entities\nwhich make up a recipe can be broadly classified into utensils, ingredients and\ntheir combinations that are related by cooking techniques. The instruction\nsection lists down a series of events in which a cooking technique or process\nis applied upon these utensils and ingredients. We model these relationships in\nthe form of tuples. Thus, using a combination of these methods we model cooking\nrecipe in the dataset RecipeDB to show the efficacy of our method. This mined\ninformation model can have several applications which include translating\nrecipes between languages, determining similarity between recipes, generation\nof novel recipes and estimation of the nutritional profile of recipes. For the\npurpose of recognition of ingredient attributes, we train the Named Entity\nRelationship (NER) models and analyze the inferences with the help of K-Means\nclustering. Our model presented with an F1 score of 0.95 across all datasets.\nWe use a similar NER tagging model for labelling cooking techniques (F1 score =\n0.88) and utensils (F1 score = 0.90) within the instructions section. Finally,\nwe determine the temporal sequence of relationships between ingredients,\nutensils and cooking techniques for modeling the instruction steps.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:37:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Diwan", "Nirav", ""], ["Batra", "Devansh", ""], ["Bagler", "Ganesh", ""]]}, {"id": "2004.12212", "submitter": "Lior Sidi", "authors": "Lior Sidi and Hadar Klein", "title": "Neural Network-Based Collaborative Filtering for Question Sequencing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  E-Learning systems (ELS) and Intelligent Tutoring Systems (ITS) play a\nsignificant part in today's education programs. Sequencing questions is the art\nof generating a personalized quiz for a target learner. A personalized test\nwill enrich the learner's experience and will contribute to a more effective\nand efficient learning process. In this paper, we used the Neural Collaborative\nFiltering (NCF) model to generate question sequencing and compare it to a\npair-wise memory-based question sequencing algorithm - EduRank. The NCF model\nshowed significantly better ranking results than the EduRank model with an\nAverage precision correlation score of 0.85 compared to 0.8.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:15:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sidi", "Lior", ""], ["Klein", "Hadar", ""]]}, {"id": "2004.12213", "submitter": "Wenying Ji", "authors": "Yitong Li, Wenying Ji, Simaan M. AbouRizk", "title": "Automated Abstraction of Operation Processes from Unstructured Text for\n  Simulation Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction of operation processes is a fundamental step for simulation\nmodeling. To reliably abstract an operation process, modelers rely on text\ninformation to study and understand details of operations. Aiming at reducing\nmodelers' interpretation load and ensuring the reliability of the abstracted\ninformation, this research proposes a systematic methodology to automate the\nabstraction of operation processes. The methodology applies rule-based\ninformation extraction to automatically extract operation process-related\ninformation from unstructured text and creates graphical representations of\noperation processes using the extracted information. To demonstrate the\napplicability and feasibility of the proposed methodology, a text description\nof an earthmoving operation is used to create its corresponding graphical\nrepresentation. Overall, this research enhances the state-of-the-art simulation\nmodeling through achieving automated abstraction of operation processes, which\nlargely reduces modelers' interpretation load and ensures the reliability of\nthe abstracted operation processes.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:18:23 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 22:47:02 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 14:34:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Yitong", ""], ["Ji", "Wenying", ""], ["AbouRizk", "Simaan M.", ""]]}, {"id": "2004.12247", "submitter": "Arda Akdemir", "authors": "Arda Akdemir and Tetsuo Shibuya and Tunga G\\\"ung\\\"or", "title": "Hierarchical Multi Task Learning with Subword Contextual Embeddings for\n  Languages with Rich Morphology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphological information is important for many sequence labeling tasks in\nNatural Language Processing (NLP). Yet, existing approaches rely heavily on\nmanual annotations or external software to capture this information. In this\nstudy, we propose using subword contextual embeddings to capture the\nmorphological information for languages with rich morphology. In addition, we\nincorporate these embeddings in a hierarchical multi-task setting which is not\nemployed before, to the best of our knowledge. Evaluated on Dependency Parsing\n(DEP) and Named Entity Recognition (NER) tasks, which are shown to benefit\ngreatly from morphological information, our final model outperforms previous\nstate-of-the-art models on both tasks for the Turkish language. Besides, we\nshow a net improvement of 18.86% and 4.61% F-1 over the previously proposed\nmulti-task learner in the same setting for the DEP and the NER tasks,\nrespectively. Empirical results for five different MTL settings show that\nincorporating subword contextual embeddings brings significant improvements for\nboth tasks. In addition, we observed that multi-task learning consistently\nimproves the performance of the DEP component.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 22:55:56 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Akdemir", "Arda", ""], ["Shibuya", "Tetsuo", ""], ["G\u00fcng\u00f6r", "Tunga", ""]]}, {"id": "2004.12297", "submitter": "Liu Yang", "authors": "Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork", "title": "Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical\n  Encoder for Long-Form Document Matching", "comments": "Accepted as a full paper in CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3411908", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many natural language processing and information retrieval problems can be\nformalized as the task of semantic matching. Existing work in this area has\nbeen largely focused on matching between short texts (e.g., question\nanswering), or between a short and a long text (e.g., ad-hoc retrieval).\nSemantic matching between long-form documents, which has many important\napplications like news recommendation, related article recommendation and\ndocument clustering, is relatively less explored and needs more research\neffort. In recent years, self-attention based models like Transformers and BERT\nhave achieved state-of-the-art performance in the task of text matching. These\nmodels, however, are still limited to short text like a few sentences or one\nparagraph due to the quadratic computational complexity of self-attention with\nrespect to input text length. In this paper, we address the issue by proposing\nthe Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for\nlong-form document matching. Our model contains several innovations to adapt\nself-attention models for longer text input. In order to better capture\nsentence level semantic relations within a document, we pre-train the model\nwith a novel masked sentence block language modeling task in addition to the\nmasked word language modeling task used by BERT. Our experimental results on\nseveral benchmark datasets for long-form document matching show that our\nproposed SMITH model outperforms the previous state-of-the-art models including\nhierarchical attention, multi-depth attention-based hierarchical recurrent\nneural network, and BERT. Comparing to BERT based baselines, our model is able\nto increase maximum input text length from 512 to 2048. We will open source a\nWikipedia based benchmark dataset, code and a pre-trained checkpoint to\naccelerate future research on long-form document matching.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:04:08 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 01:48:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Yang", "Liu", ""], ["Zhang", "Mingyang", ""], ["Li", "Cheng", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2004.12302", "submitter": "Canwen Xu", "authors": "Canwen Xu and Jiaxin Pei and Hongtao Wu and Yiyu Liu and Chenliang Li", "title": "MATINF: A Jointly Labeled Large-Scale Dataset for Classification,\n  Question Answering and Summarization", "comments": "Accepted as a long paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, large-scale datasets have vastly facilitated the development in\nnearly all domains of Natural Language Processing. However, there is currently\nno cross-task dataset in NLP, which hinders the development of multi-task\nlearning. We propose MATINF, the first jointly labeled large-scale dataset for\nclassification, question answering and summarization. MATINF contains 1.07\nmillion question-answer pairs with human-labeled categories and user-generated\nquestion descriptions. Based on such rich information, MATINF is applicable for\nthree major NLP tasks, including classification, question answering, and\nsummarization. We benchmark existing methods and a novel multi-task baseline\nover MATINF to inspire further research. Our comprehensive comparison and\nexperiments over MATINF and other datasets demonstrate the merits held by\nMATINF.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:43:15 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 06:11:55 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Xu", "Canwen", ""], ["Pei", "Jiaxin", ""], ["Wu", "Hongtao", ""], ["Liu", "Yiyu", ""], ["Li", "Chenliang", ""]]}, {"id": "2004.12307", "submitter": "Arindam Pal", "authors": "Paheli Bhattacharya, Kripabandhu Ghosh, Arindam Pal, Saptarshi Ghosh", "title": "Methods for Computing Legal Document Similarity: A Comparative Study", "comments": "This paper was published at the LDA 2019 workshop in the JURIX 2019\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing similarity between two legal documents is an important and\nchallenging task in the domain of Legal Information Retrieval. Finding similar\nlegal documents has many applications in downstream tasks, including prior-case\nretrieval, recommendation of legal articles, and so on. Prior works have\nproposed two broad ways of measuring similarity between legal documents -\nanalyzing the precedent citation network, and measuring similarity based on\ntextual content similarity measures. But there has not been a comprehensive\ncomparison of these existing methods on a common platform. In this paper, we\nperform the first systematic analysis of the existing methods. In addition, we\nexplore two promising new similarity computation methods - one text-based and\nthe other based on network embeddings, which have not been considered till now.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:26:04 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Ghosh", "Kripabandhu", ""], ["Pal", "Arindam", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2004.12316", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao", "title": "Towards Persona-Based Empathetic Conversational Models", "comments": "Accepted to EMNLP 2020 (A new dataset is proposed:\n  https://github.com/zhongpeixiang/PEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empathetic conversational models have been shown to improve user satisfaction\nand task outcomes in numerous domains. In Psychology, persona has been shown to\nbe highly correlated to personality, which in turn influences empathy. In\naddition, our empirical analysis also suggests that persona plays an important\nrole in empathetic conversations. To this end, we propose a new task towards\npersona-based empathetic conversations and present the first empirical study on\nthe impact of persona on empathetic responding. Specifically, we first present\na novel large-scale multi-domain dataset for persona-based empathetic\nconversations. We then propose CoBERT, an efficient BERT-based response\nselection model that obtains the state-of-the-art performance on our dataset.\nFinally, we conduct extensive experiments to investigate the impact of persona\non empathetic responding. Notably, our results show that persona improves\nempathetic responding more when CoBERT is trained on empathetic conversations\nthan non-empathetic ones, establishing an empirical link between persona and\nempathy in human conversations.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:51:01 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:55:05 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 03:40:56 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 06:48:24 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2020 08:23:51 GMT"}, {"version": "v6", "created": "Mon, 5 Oct 2020 09:21:06 GMT"}, {"version": "v7", "created": "Thu, 19 Nov 2020 11:00:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhong", "Peixiang", ""], ["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Liu", "Yong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2004.12331", "submitter": "Rui Wang", "authors": "Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye,\n  Haiyang Xu", "title": "Neural Topic Modeling with Bidirectional Adversarial Training", "comments": "To appear at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a surge of interests of using neural topic models\nfor automatic topic extraction from text, since they avoid the complicated\nmathematical derivations for model inference as in traditional topic models\nsuch as Latent Dirichlet Allocation (LDA). However, these models either\ntypically assume improper prior (e.g. Gaussian or Logistic Normal) over latent\ntopic space or could not infer topic distribution for a given document. To\naddress these limitations, we propose a neural topic modeling approach, called\nBidirectional Adversarial Topic (BAT) model, which represents the first attempt\nof applying bidirectional adversarial training for neural topic modeling. The\nproposed BAT builds a two-way projection between the document-topic\ndistribution and the document-word distribution. It uses a generator to capture\nthe semantic patterns from texts and an encoder for topic inference.\nFurthermore, to incorporate word relatedness information, the Bidirectional\nAdversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT. To\nverify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are\nused in our experiments. The experimental results show that BAT and\nGaussian-BAT obtain more coherent topics, outperforming several competitive\nbaselines. Moreover, when performing text clustering based on the extracted\ntopics, our models outperform all the baselines, with more significant\nimprovements achieved by Gaussian-BAT where an increase of near 6\\% is observed\nin accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 09:41:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Rui", ""], ["Hu", "Xuemeng", ""], ["Zhou", "Deyu", ""], ["He", "Yulan", ""], ["Xiong", "Yuxuan", ""], ["Ye", "Chenchen", ""], ["Xu", "Haiyang", ""]]}, {"id": "2004.12563", "submitter": "Xuan Wang", "authors": "Xuan Wang, Weili Liu, Aabhas Chauhan, Yingjun Guan, Jiawei Han", "title": "Automatic Textual Evidence Mining in COVID-19 Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We created this EVIDENCEMINER system for automatic textual evidence mining in\nCOVID-19 literature. EVIDENCEMINER is a web-based system that lets users query\na natural language statement and automatically retrieves textual evidence from\na background corpora for life sciences. It is constructed in a completely\nautomated way without any human effort for training data annotation.\nEVIDENCEMINER is supported by novel data-driven methods for distantly\nsupervised named entity recognition and open information extraction. The named\nentities and meta-patterns are pre-computed and indexed offline to support fast\nonline evidence retrieval. The annotation results are also highlighted in the\noriginal document for better visualization. EVIDENCEMINER also includes\nanalytic functionalities such as the most frequent entity and relation\nsummarization.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 03:14:59 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 00:51:16 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 19:12:36 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Wang", "Xuan", ""], ["Liu", "Weili", ""], ["Chauhan", "Aabhas", ""], ["Guan", "Yingjun", ""], ["Han", "Jiawei", ""]]}, {"id": "2004.12602", "submitter": "Qiang Liu", "authors": "Qiang Liu and Zhaocheng Liu and Haoli Zhang", "title": "An Empirical Study on Feature Discretization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with continuous numeric features, we usually adopt feature\ndiscretization. In this work, to find the best way to conduct feature\ndiscretization, we present some theoretical analysis, in which we focus on\nanalyzing correctness and robustness of feature discretization. Then, we\npropose a novel discretization method called Local Linear Encoding (LLE).\nExperiments on two numeric datasets show that, LLE can outperform conventional\ndiscretization method with much fewer model parameters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 06:50:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Qiang", ""], ["Liu", "Zhaocheng", ""], ["Zhang", "Haoli", ""]]}, {"id": "2004.12628", "submitter": "Sven Hertling", "authors": "Jan Portisch, Sven Hertling, Heiko Paulheim", "title": "Visual Analysis of Ontology Matching Results with the MELT Dashboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo, we introduce MELT Dashboard, an interactive Web user interface\nfor ontology alignment evaluation which is created with the existing Matching\nEvaLuation Toolkit (MELT). Compared to existing, static evaluation interfaces\nin the ontology matching domain, our dashboard allows for interactive\nself-service analyses such as a drill down into the matcher performance for\ndata type properties or into the performance of matchers within a certain\nconfidence threshold. In addition, the dashboard offers detailed group\nevaluation capabilities that allow for the application in broad evaluation\ncampaigns such as the Ontology Alignment Evaluation Initiative (OAEI).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:11:00 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Portisch", "Jan", ""], ["Hertling", "Sven", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2004.12733", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono and Federica Cena", "title": "Personalized Recommendation of PoIs to People with Autism", "comments": null, "journal-ref": "Proceedings of the 28th ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP 2020)", "doi": "10.1145/3340631.3394845", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The suggestion of Points of Interest to people with Autism Spectrum Disorder\n(ASD) challenges recommender systems research because these users' perception\nof places is influenced by idiosyncratic sensory aversions which can mine their\nexperience by causing stress and anxiety. Therefore, managing individual\npreferences is not enough to provide these people with suitable\nrecommendations. In order to address this issue, we propose a Top-N\nrecommendation model that combines the user's idiosyncratic aversions with\nher/his preferences in a personalized way to suggest the most compatible and\nlikable Points of Interest for her/him. We are interested in finding a\nuser-specific balance of compatibility and interest within a recommendation\nmodel that integrates heterogeneous evaluation criteria to appropriately take\nthese aspects into account. We tested our model on both ASD and \"neurotypical\"\npeople. The evaluation results show that, on both groups, our model outperforms\nin accuracy and ranking capability the recommender systems based on item\ncompatibility, on user preferences, or which integrate these two aspects by\nmeans of a uniform evaluation model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:04:58 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Cena", "Federica", ""]]}, {"id": "2004.12832", "submitter": "Omar Khattab", "authors": "Omar Khattab and Matei Zaharia", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late\n  Interaction over BERT", "comments": "Accepted at SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in Natural Language Understanding (NLU) is driving fast-paced\nadvances in Information Retrieval (IR), largely owed to fine-tuning deep\nlanguage models (LMs) for document ranking. While remarkably effective, the\nranking models based on these LMs increase computational cost by orders of\nmagnitude over prior approaches, particularly as they must feed each\nquery-document pair through a massive neural network to compute a single\nrelevance score. To tackle this, we present ColBERT, a novel ranking model that\nadapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT\nintroduces a late interaction architecture that independently encodes the query\nand the document using BERT and then employs a cheap yet powerful interaction\nstep that models their fine-grained similarity. By delaying and yet retaining\nthis fine-granular interaction, ColBERT can leverage the expressiveness of deep\nLMs while simultaneously gaining the ability to pre-compute document\nrepresentations offline, considerably speeding up query processing. Beyond\nreducing the cost of re-ranking the documents retrieved by a traditional model,\nColBERT's pruning-friendly interaction mechanism enables leveraging\nvector-similarity indexes for end-to-end retrieval directly from a large\ndocument collection. We extensively evaluate ColBERT using two recent passage\nsearch datasets. Results show that ColBERT's effectiveness is competitive with\nexisting BERT-based models (and outperforms every non-BERT baseline), while\nexecuting two orders-of-magnitude faster and requiring four orders-of-magnitude\nfewer FLOPs per query.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:21:03 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 05:28:21 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Khattab", "Omar", ""], ["Zaharia", "Matei", ""]]}, {"id": "2004.12835", "submitter": "Ivan P Yamshchikov", "authors": "Igor Samenko, Alexey Tikhonov, Ivan P. Yamshchikov", "title": "Synonyms and Antonyms: Embedded Conflict", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since modern word embeddings are motivated by a distributional hypothesis and\nare, therefore, based on local co-occurrences of words, it is only to be\nexpected that synonyms and antonyms can have very similar embeddings. Contrary\nto this widespread assumption, this paper shows that modern embeddings contain\ninformation that distinguishes synonyms and antonyms despite small cosine\nsimilarities between corresponding vectors. This information is encoded in the\ngeometry of the embeddings and could be extracted with a manifold learning\nprocedure or {\\em contrasting map}. Such a map is trained on a small labeled\nsubset of the data and can produce new empeddings that explicitly highlight\nspecific semantic attributes of the word. The new embeddings produced by the\nmap are shown to improve the performance on downstream tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:33:37 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Samenko", "Igor", ""], ["Tikhonov", "Alexey", ""], ["Yamshchikov", "Ivan P.", ""]]}, {"id": "2004.13003", "submitter": "Tian Shi", "authors": "Tian Shi, Xuchao Zhang, Ping Wang, Chandan K. Reddy", "title": "Corpus-level and Concept-based Explanations for Interpretable Document\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using attention weights to identify information that is important for models'\ndecision-making is a popular approach to interpret attention-based neural\nnetworks. This is commonly realized in practice through the generation of a\nheat-map for every single document based on attention weights. However, this\ninterpretation method is fragile, and easy to find contradictory examples. In\nthis paper, we propose a corpus-level explanation approach, which aims to\ncapture causal relationships between keywords and model predictions via\nlearning the importance of keywords for predicted labels across a training\ncorpus based on attention weights. Based on this idea, we further propose a\nconcept-based explanation method that can automatically learn higher-level\nconcepts and their importance to model prediction tasks. Our concept-based\nexplanation method is built upon a novel Abstraction-Aggregation Network, which\ncan automatically cluster important keywords during an end-to-end training\nprocess. We apply these methods to the document classification task and show\nthat they are powerful in extracting semantically meaningful keywords and\nconcepts. Our consistency analysis results based on an attention-based Na\\\"ive\nBayes classifier also demonstrate these keywords and concepts are important for\nmodel predictions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:54:17 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 21:48:26 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 04:50:32 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 03:22:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Shi", "Tian", ""], ["Zhang", "Xuchao", ""], ["Wang", "Ping", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2004.13005", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Amro El-Jaroudi, William Hartmann, Damianos Karakos,\n  Lingjun Zhao", "title": "Cross-lingual Information Retrieval with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple neural language models have been developed recently, e.g., BERT and\nXLNet, and achieved impressive results in various NLP tasks including sentence\nclassification, question answering and document ranking. In this paper, we\nexplore the use of the popular bidirectional language model, BERT, to model and\nlearn the relevance between English queries and foreign-language documents in\nthe task of cross-lingual information retrieval. A deep relevance matching\nmodel based on BERT is introduced and trained by finetuning a pretrained\nmultilingual BERT model with weak supervision, using home-made CLIR training\ndata derived from parallel corpora. Experimental results of the retrieval of\nLithuanian documents against short English queries show that our model is\neffective and outperforms the competitive baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 23:32:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Jiang", "Zhuolin", ""], ["El-Jaroudi", "Amro", ""], ["Hartmann", "William", ""], ["Karakos", "Damianos", ""], ["Zhao", "Lingjun", ""]]}, {"id": "2004.13007", "submitter": "Mar\\'ia N. Moreno Garc\\'ia", "authors": "Diego S\\'anchez-Moreno, Vivian F. L\\'opez Batista, M. Dolores Mu\\~noz\n  Vicente, Ana B. Gil Gonz\\'alez and Mar\\'ia N. Moreno-Garc\\'ia", "title": "A session-based song recommendation approach involving user\n  characterization along the play power-law distribution", "comments": "Accepted in Complexity (ISSN: 1099-0526)", "journal-ref": null, "doi": "10.1155/2020/7309453", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, streaming music platforms have become very popular mainly\ndue to the huge number of songs these systems make available to users. This\nenormous availability means that recommendation mechanisms that help users to\nselect the music they like need to be incorporated. However, developing\nreliable recommender systems in the music field involves dealing with many\nproblems, some of which are generic and widely studied in the literature, while\nothers are specific to this application domain and are therefore less\nwell-known. This work is focused on two important issues that have not received\nmuch attention: managing gray-sheep users and obtaining implicit ratings. The\nfirst one is usually addressed by resorting to content information that is\noften difficult to obtain. The other drawback is related to the sparsity\nproblem that arises when there are obstacles to gather explicit ratings. In\nthis work, the referred shortcomings are addressed by means of a recommendation\napproach based on the users' streaming sessions. The method is aimed at\nmanaging the well-known power-law probability distribution representing the\nlistening behavior of users. This proposal improves the recommendation\nreliability of collaborative filtering methods while reducing the complexity of\nthe procedures used so far to deal with the gray-sheep problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 07:17:03 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["S\u00e1nchez-Moreno", "Diego", ""], ["Batista", "Vivian F. L\u00f3pez", ""], ["Vicente", "M. Dolores Mu\u00f1oz", ""], ["Gonz\u00e1lez", "Ana B. Gil", ""], ["Moreno-Garc\u00eda", "Mar\u00eda N.", ""]]}, {"id": "2004.13012", "submitter": "Dara Bahri", "authors": "Dara Bahri, Yi Tay, Che Zheng, Donald Metzler, Andrew Tomkins", "title": "Choppy: Cut Transformer For Ranked List Truncation", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in information retrieval has traditionally focused on ranking and\nrelevance: given a query, return some number of results ordered by relevance to\nthe user. However, the problem of determining how many results to return, i.e.\nhow to optimally truncate the ranked result list, has received less attention\ndespite being of critical importance in a range of applications. Such\ntruncation is a balancing act between the overall relevance, or usefulness of\nthe results, with the user cost of processing more results. In this work, we\npropose Choppy, an assumption-free model based on the widely successful\nTransformer architecture, to the ranked list truncation problem. Needing\nnothing more than the relevance scores of the results, the model uses a\npowerful multi-head attention mechanism to directly optimize any user-defined\nIR metric. We show Choppy improves upon recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 00:52:49 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bahri", "Dara", ""], ["Tay", "Yi", ""], ["Zheng", "Che", ""], ["Metzler", "Donald", ""], ["Tomkins", "Andrew", ""]]}, {"id": "2004.13078", "submitter": "Iyiola E. Olatunji", "authors": "Iyiola E. Olatunji, Xin Li, Wai Lam", "title": "Context-aware Helpfulness Prediction for Online Product Reviews", "comments": "Published as a proceeding paper in AIRS 2019", "journal-ref": null, "doi": "10.1007/978-3-030-42835-8_6", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling and prediction of review helpfulness has become more predominant due\nto proliferation of e-commerce websites and online shops. Since the\nfunctionality of a product cannot be tested before buying, people often rely on\ndifferent kinds of user reviews to decide whether or not to buy a product.\nHowever, quality reviews might be buried deep in the heap of a large amount of\nreviews. Therefore, recommending reviews to customers based on the review\nquality is of the essence. Since there is no direct indication of review\nquality, most reviews use the information that ''X out of Y'' users found the\nreview helpful for obtaining the review quality. However, this approach\nundermines helpfulness prediction because not all reviews have statistically\nabundant votes. In this paper, we propose a neural deep learning model that\npredicts the helpfulness score of a review. This model is based on\nconvolutional neural network (CNN) and a context-aware encoding mechanism which\ncan directly capture relationships between words irrespective of their distance\nin a long sequence. We validated our model on human annotated dataset and the\nresult shows that our model significantly outperforms existing models for\nhelpfulness prediction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 18:19:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Olatunji", "Iyiola E.", ""], ["Li", "Xin", ""], ["Lam", "Wai", ""]]}, {"id": "2004.13115", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Yin Li, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma,\n  Sumaya Almanee", "title": "Obscure: Information-Theoretically Secure, Oblivious, and Verifiable\n  Aggregation Queries on Secret-Shared Outsourced Data -- Full Version", "comments": "A preliminary version of this work was accepted in VLDB 2019. This\n  version has been accepted in IEEE Transactions on Knowledge and Data\n  Engineering (TKDE). The final published version of this paper may differ from\n  this accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite exciting progress on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. We develop a\ncommunication-efficient and information-theoretically secure system, entitled\nObscure for aggregation queries with conjunctive or disjunctive predicates,\nusing secret-sharing. Obscure is strongly secure (i.e., secure regardless of\nthe computational-capabilities of an adversary) and prevents the network, as\nwell as, the (adversarial) servers to learn the user's queries, results, or the\ndatabase. In addition, Obscure provides additional security features, such as\nhiding access-patterns (i.e., hiding the identity of the tuple satisfying a\nquery) and hiding query-patterns (i.e., hiding which two queries are\nidentical). Also, Obscure does not require any communication between any two\nservers that store the secret-shared data before/during/after the query\nexecution. Moreover, our techniques deal with the secret-shared data that is\noutsourced by a single or multiple database owners, as well as, allows a user,\nwhich may not be the database owner, to execute the query over secret-shared\ndata. We further develop (non-mandatory) privacy-preserving result verification\nalgorithms that detect malicious behaviors, and experimentally validate the\nefficiency of Obscure on large datasets, the size of which prior approaches of\nsecret-sharing or multi-party computation systems have not scaled to.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:27:21 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Gupta", "Peeyush", ""], ["Li", "Yin", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Almanee", "Sumaya", ""]]}, {"id": "2004.13117", "submitter": "Magdalena Kaiser", "authors": "Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum", "title": "Conversational Question Answering over Passages by Leveraging Word\n  Proximity Networks", "comments": "SIGIR 2020 Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) over text passages is a problem of long-standing\ninterest in information retrieval. Recently, the conversational setting has\nattracted attention, where a user asks a sequence of questions to satisfy her\ninformation needs around a topic. While this setup is a natural one and similar\nto humans conversing with each other, it introduces two key research\nchallenges: understanding the context left implicit by the user in follow-up\nquestions, and dealing with ad hoc question formulations. In this work, we\ndemonstrate CROWN (Conversational passage ranking by Reasoning Over Word\nNetworks): an unsupervised yet effective system for conversational QA with\npassage responses, that supports several modes of context propagation over\nmultiple turns. To this end, CROWN first builds a word proximity network (WPN)\nfrom large corpora to store statistically significant term co-occurrences. At\nanswering time, passages are ranked by a combination of their similarity to the\nquestion, and coherence of query terms within: these factors are measured by\nreading off node and edge weights from the WPN. CROWN provides an interface\nthat is both intuitive for end-users, and insightful for experts for\nreconfiguration to individual setups. CROWN was evaluated on TREC CAsT data,\nwhere it achieved above-median performance in a pool of neural methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:30:47 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 22:57:08 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 15:21:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kaiser", "Magdalena", ""], ["Roy", "Rishiraj Saha", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2004.13121", "submitter": "Amir Javadpour", "authors": "Amir Javadpour, Samira Rezaei, Kuan-Ching Li and Guojun Wang", "title": "A Scalable Feature Selection and Opinion Miner Using Whale Optimization\n  Algorithm", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-15-4828-4_20", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fast-growing volume of text documents and reviews in recent years,\ncurrent analyzing techniques are not competent enough to meet the users' needs.\nUsing feature selection techniques not only support to understand data better\nbut also lead to higher speed and also accuracy. In this article, the Whale\nOptimization algorithm is considered and applied to the search for the optimum\nsubset of features. As known, F-measure is a metric based on precision and\nrecall that is very popular in comparing classifiers. For the evaluation and\ncomparison of the experimental results, PART, random tree, random forest, and\nRBF network classification algorithms have been applied to the different number\nof features. Experimental results show that the random forest has the best\naccuracy on 500 features. Keywords: Feature selection, Whale Optimization\nalgorithm, Selecting optimal, Classification algorithm\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 01:08:45 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Javadpour", "Amir", ""], ["Rezaei", "Samira", ""], ["Li", "Kuan-Ching", ""], ["Wang", "Guojun", ""]]}, {"id": "2004.13136", "submitter": "Ahmad Ababneh Dr", "authors": "Ahmad Hussein Ababneh, Joan Lu, Qiang Xu", "title": "The Effect of the Multi-Layer Text Summarization Model on the Efficiency\n  and Relevancy of the Vector Space-based Information Retrieval", "comments": null, "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS), Vol. 18, No. 3, March 2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive upload of text on the internet creates a huge inverted index in\ninformation retrieval systems, which hurts their efficiency. The purpose of\nthis research is to measure the effect of the Multi-Layer Similarity model of\nthe automatic text summarization on building an informative and condensed\ninvert index in the IR systems. To achieve this purpose, we summarized a\nconsiderable number of documents using the Multi-Layer Similarity model, and we\nbuilt the inverted index from the automatic summaries that were generated from\nthis model. A series of experiments were held to test the performance in terms\nof efficiency and relevancy. The experiments include comparisons with three\nexisting text summarization models; the Jaccard Coefficient Model, the Vector\nSpace Model, and the Latent Semantic Analysis model. The experiments examined\nthree groups of queries with manual and automatic relevancy assessment. The\npositive effect of the Multi-Layer Similarity in the efficiency of the IR\nsystem was clear without noticeable loss in the relevancy results. However, the\nevaluation showed that the traditional statistical models without semantic\ninvestigation failed to improve the information retrieval efficiency. Comparing\nwith the previous publications that addressed the use of summaries as a source\nof the index, the relevancy assessment of our work was higher, and the\nMulti-Layer Similarity retrieval constructed an inverted index that was 58%\nsmaller than the main corpus inverted index.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 00:18:14 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ababneh", "Ahmad Hussein", ""], ["Lu", "Joan", ""], ["Xu", "Qiang", ""]]}, {"id": "2004.13138", "submitter": "Jinghui Lu", "authors": "Jinghui Lu and Brian MacNamee", "title": "Investigating the Effectiveness of Representations Based on Pretrained\n  Transformer-based Language Models in Active Learning for Labelling Text\n  Datasets", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.03505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning has been shown to be an effective way to alleviate some of\nthe effort required in utilising large collections of unlabelled data for\nmachine learning tasks without needing to fully label them. The representation\nmechanism used to represent text documents when performing active learning,\nhowever, has a significant influence on how effective the process will be.\nWhile simple vector representations such as bag-of-words and embedding-based\nrepresentations based on techniques such as word2vec have been shown to be an\neffective way to represent documents during active learning, the emergence of\nrepresentation mechanisms based on the pre-trained transformer-based neural\nnetwork models popular in natural language processing research (e.g. BERT)\noffer a promising, and as yet not fully explored, alternative. This paper\ndescribes a comprehensive evaluation of the effectiveness of representations\nbased on pre-trained transformer-based language models for active learning.\nThis evaluation shows that transformer-based models, especially BERT-like\nmodels, that have not yet been widely used in active learning, achieve a\nsignificant improvement over more commonly used vector representations like\nbag-of-words or other classical word embeddings like word2vec. This paper also\ninvestigates the effectiveness of representations based on variants of BERT\nsuch as Roberta, Albert as well as comparing the effectiveness of the [CLS]\ntoken representation and the aggregated representation that can be generated\nusing BERT-like models. Finally, we propose an approach Adaptive Tuning Active\nLearning. Our experiments show that the limited label information acquired in\nactive learning can not only be used for training a classifier but can also\nadaptively improve the embeddings generated by the BERT-like language models as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:37:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Lu", "Jinghui", ""], ["MacNamee", "Brian", ""]]}, {"id": "2004.13139", "submitter": "Yang Sun", "authors": "Yang Sun, Fajie Yuan, Min Yang, Guoao Wei, Zhou Zhao, and Duo Liu", "title": "A Generic Network Compression Framework for Sequential Recommender\n  Systems", "comments": "Accepted by SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommender systems (SRS) have become the key technology in\ncapturing user's dynamic interests and generating high-quality recommendations.\nCurrent state-of-the-art sequential recommender models are typically based on a\nsandwich-structured deep neural network, where one or more middle (hidden)\nlayers are placed between the input embedding layer and output softmax layer.\nIn general, these models require a large number of parameters (such as using a\nlarge embedding dimension or a deep network architecture) to obtain their\noptimal performance. Despite the effectiveness, at some point, further\nincreasing model size may be harder for model deployment in resource-constraint\ndevices, resulting in longer responding time and larger memory footprint. To\nresolve the issues, we propose a compressed sequential recommendation\nframework, termed as CpRec, where two generic model shrinking techniques are\nemployed. Specifically, we first propose a block-wise adaptive decomposition to\napproximate the input and softmax matrices by exploiting the fact that items in\nSRS obey a long-tailed distribution. To reduce the parameters of the middle\nlayers, we introduce three layer-wise parameter sharing schemes. We instantiate\nCpRec using deep convolutional neural network with dilated kernels given\nconsideration to both recommendation accuracy and efficiency. By the extensive\nablation studies, we demonstrate that the proposed CpRec can achieve up to\n4$\\sim$8 times compression rates in real-world SRS datasets. Meanwhile, CpRec\nis faster during training\\inference, and in most cases outperforms its\nuncompressed counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:40:55 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 08:10:16 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 03:16:13 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 14:49:16 GMT"}, {"version": "v5", "created": "Tue, 26 May 2020 06:25:41 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Sun", "Yang", ""], ["Yuan", "Fajie", ""], ["Yang", "Min", ""], ["Wei", "Guoao", ""], ["Zhao", "Zhou", ""], ["Liu", "Duo", ""]]}, {"id": "2004.13157", "submitter": "Fernando Diaz", "authors": "Fernando Diaz and Bhaskar Mitra and Michael D. Ekstrand and Asia J.\n  Biega and Ben Carterette", "title": "Evaluating Stochastic Rankings with Expected Exposure", "comments": "In Proceedings of the 29th ACM International Conference on\n  Information & Knowledge Management (CIKM '20). Association for Computing\n  Machinery, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3340531.3411962", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of \\emph{expected exposure} as the average attention\nranked items receive from users over repeated samples of the same query.\nFurthermore, we advocate for the adoption of the principle of equal expected\nexposure: given a fixed information need, no item should receive more or less\nexpected exposure than any other item of the same relevance grade. We argue\nthat this principle is desirable for many retrieval objectives and scenarios,\nincluding topical diversity and fair ranking. Leveraging user models from\nexisting retrieval metrics, we propose a general evaluation methodology based\non expected exposure and draw connections to related metrics in information\nretrieval evaluation. Importantly, this methodology relaxes classic information\nretrieval assumptions, allowing a system, in response to a query, to produce a\n\\emph{distribution over rankings} instead of a single fixed ranking. We study\nthe behavior of the expected exposure metric and stochastic rankers across a\nvariety of information access conditions, including \\emph{ad hoc} retrieval and\nrecommendation. We believe that measuring and optimizing expected exposure\nmetrics using randomization opens a new area for retrieval algorithm\ndevelopment and progress.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:29:56 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 20:26:09 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Diaz", "Fernando", ""], ["Mitra", "Bhaskar", ""], ["Ekstrand", "Michael D.", ""], ["Biega", "Asia J.", ""], ["Carterette", "Ben", ""]]}, {"id": "2004.13255", "submitter": "Yaushian Wang", "authors": "Yau-Shian Wang and Hung-Yi Lee and Yun-Nung Chen", "title": "Learning Interpretable and Discrete Representations with Adversarial\n  Training for Unsupervised Text Classification", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continuous representations from unlabeled textual data has been\nincreasingly studied for benefiting semi-supervised learning. Although it is\nrelatively easier to interpret discrete representations, due to the difficulty\nof training, learning discrete representations for unlabeled textual data has\nnot been widely explored. This work proposes TIGAN that learns to encode texts\ninto two disentangled representations, including a discrete code and a\ncontinuous noise, where the discrete code represents interpretable topics, and\nthe noise controls the variance within the topics. The discrete code learned by\nTIGAN can be used for unsupervised text classification. Compared to other\nunsupervised baselines, the proposed TIGAN achieves superior performance on six\ndifferent corpora. Also, the performance is on par with a recently proposed\nweakly-supervised text classification method. The extracted topical words for\nrepresenting latent topics show that TIGAN learns coherent and highly\ninterpretable topics.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:53:59 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Wang", "Yau-Shian", ""], ["Lee", "Hung-Yi", ""], ["Chen", "Yun-Nung", ""]]}, {"id": "2004.13313", "submitter": "Zhuyun Dai", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "title": "Modularized Transfomer-based Ranking Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent innovations in Transformer-based ranking models have advanced the\nstate-of-the-art in information retrieval. However, these Transformers are\ncomputationally expensive, and their opaque hidden states make it hard to\nunderstand the ranking process. In this work, we modularize the Transformer\nranker into separate modules for text representation and interaction. We show\nhow this design enables substantially faster ranking using offline pre-computed\nrepresentations and light-weight online interactions. The modular design is\nalso easier to interpret and sheds light on the ranking process in Transformer\nrankers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:55:42 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 21:22:36 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 22:55:25 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2004.13401", "submitter": "Shilin Qu", "authors": "Shilin Qu, Fajie Yuan, Guibing Guo, Liguang Zhang, Wei Wei", "title": "CmnRec: Sequential Recommendations with Chunk-accelerated Memory Network", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Memory-based Neural Recommenders (MNR) have demonstrated superior\npredictive accuracy in the task of sequential recommendations, particularly for\nmodeling long-term item dependencies. However, typical MNR requires complex\nmemory access operations, i.e., both writing and reading via a controller\n(e.g., RNN) at every time step. Those frequent operations will dramatically\nincrease the network training time, resulting in the difficulty in being\ndeployed on industrial-scale recommender systems. In this paper, we present a\nnovel general Chunk framework to accelerate MNR significantly. Specifically,\nour framework divides proximal information units into chunks, and performs\nmemory access at certain time steps, whereby the number of memory operations\ncan be greatly reduced. We investigate two ways to implement effective\nchunking, i.e., PEriodic Chunk (PEC) and Time-Sensitive Chunk (TSC), to\npreserve and recover important recurrent signals in the sequence. Since\nchunk-accelerated MNR models take into account more proximal information units\nthan that from a single timestep, it can remove the influence of noise in the\nitem sequence to a large extent, and thus improve the stability of MNR. In this\nway, the proposed chunk mechanism can lead to not only faster training and\nprediction, but even slightly better results. The experimental results on three\nreal-world datasets (weishi, ml-10M and ml-latest) show that our chunk\nframework notably reduces the running time (e.g., with up to 7x for training &\n10x for inference on ml-latest) of MNR, and meantime achieves competitive\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 10:13:00 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Qu", "Shilin", ""], ["Yuan", "Fajie", ""], ["Guo", "Guibing", ""], ["Zhang", "Liguang", ""], ["Wei", "Wei", ""]]}, {"id": "2004.13481", "submitter": "Mohammed Belkhatir", "authors": "Bhawani Selvaretnam, Mohammed Belkhatir", "title": "A Linguistically Driven Framework for Query Expansion via Grammatical\n  Constituent Highlighting and Role-Based Concept Weighting", "comments": "arXiv admin note: text overlap with arXiv:2004.11083", "journal-ref": null, "doi": "10.1016/j.ipm.2015.04.002", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a linguistically-motivated query expansion\nframework that recognizes and en-codes significant query constituents that\ncharacterize query intent in order to improve retrieval performance.\nConcepts-of-Interest are recognized as the core concepts that represent the\ngist of the search goal whilst the remaining query constituents which serve to\nspecify the search goal and complete the query structure are classified as\ndescriptive, relational or structural. Acknowledging the need to form\nsemantically-associated base pairs for the purpose of extracting related\npotential expansion concepts, an algorithm which capitalizes on syntactical\ndependencies to capture relationships between adjacent and non-adjacent query\nconcepts is proposed. Lastly, a robust weighting scheme that duly emphasizes\nthe importance of query constituents based on their linguistic role within the\nexpanded query is presented. We demonstrate improvements in retrieval\neffectiveness in terms of increased mean average precision (MAP) garnered by\nthe proposed linguistic-based query expansion framework through experimentation\non the TREC ad hoc test collections.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 01:43:00 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Selvaretnam", "Bhawani", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2004.13486", "submitter": "Bhaskar Mitra", "authors": "Emine Yilmaz, Nick Craswell, Bhaskar Mitra and Daniel Campos", "title": "On the Reliability of Test Collections for Evaluating Systems of\n  Different Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning based models are increasingly being used for information\nretrieval (IR), a major challenge is to ensure the availability of test\ncollections for measuring their quality. Test collections are generated based\non pooling results of various retrieval systems, but until recently this did\nnot include deep learning systems. This raises a major challenge for reusable\nevaluation: Since deep learning based models use external resources (e.g. word\nembeddings) and advanced representations as opposed to traditional methods that\nare mainly based on lexical similarity, they may return different types of\nrelevant document that were not identified in the original pooling. If so, test\ncollections constructed using traditional methods are likely to lead to biased\nand unfair evaluation results for deep learning (neural) systems. This paper\nuses simulated pooling to test the fairness and reusability of test\ncollections, showing that pooling based on traditional systems only can lead to\nbiased evaluation of deep learning systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:22:26 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Yilmaz", "Emine", ""], ["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Campos", "Daniel", ""]]}, {"id": "2004.13574", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Tao Yang, Huazheng Wang, Jiaxin Mao", "title": "Unbiased Learning to Rank: Online or Offline?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to obtain an unbiased ranking model by learning to rank with biased user\nfeedback is an important research question for IR. Existing work on unbiased\nlearning to rank (ULTR) can be broadly categorized into two groups -- the\nstudies on unbiased learning algorithms with logged data, namely the\n\\textit{offline} unbiased learning, and the studies on unbiased parameters\nestimation with real-time user interactions, namely the \\textit{online}\nlearning to rank. While their definitions of \\textit{unbiasness} are different,\nthese two types of ULTR algorithms share the same goal -- to find the best\nmodels that rank documents based on their intrinsic relevance or utility.\nHowever, most studies on offline and online unbiased learning to rank are\ncarried in parallel without detailed comparisons on their background theories\nand empirical performance. In this paper, we formalize the task of unbiased\nlearning to rank and show that existing algorithms for offline unbiased\nlearning and online learning to rank are just the two sides of the same coin.\nWe evaluate six state-of-the-art ULTR algorithms and find that most of them can\nbe used in both offline settings and online environments with or without minor\nmodifications. Further, we analyze how different offline and online learning\nparadigms would affect the theoretical foundation and empirical effectiveness\nof each algorithm on both synthetic and real search data. Our findings could\nprovide important insights and guideline for choosing and deploying ULTR\nalgorithms in practice.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 15:01:33 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:26:49 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 16:55:23 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Ai", "Qingyao", ""], ["Yang", "Tao", ""], ["Wang", "Huazheng", ""], ["Mao", "Jiaxin", ""]]}, {"id": "2004.13820", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Evolution of Semantic Similarity -- A Survey", "comments": "29 pages, 5 figures, submitted to \"ACM Computing Survey\"", "journal-ref": "ACM Computing Surveys 54(2):1-37 (2021)", "doi": "10.1145/3440755", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the semantic similarity between text data is one of the\nchallenging and open research problems in the field of Natural Language\nProcessing (NLP). The versatility of natural language makes it difficult to\ndefine rule-based methods for determining semantic similarity measures. In\norder to address this issue, various semantic similarity methods have been\nproposed over the years. This survey article traces the evolution of such\nmethods, categorizing them based on their underlying principles as\nknowledge-based, corpus-based, deep neural network-based methods, and hybrid\nmethods. Discussing the strengths and weaknesses of each method, this survey\nprovides a comprehensive view of existing systems in place, for new researchers\nto experiment and develop innovative ideas to address the issue of semantic\nsimilarity.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 22:07:39 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 15:57:06 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2004.13851", "submitter": "Siqi Liu", "authors": "Siqi Liu", "title": "Sentiment Analysis of Yelp Reviews: A Comparison of Techniques and\n  Models", "comments": "7 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use over 350,000 Yelp reviews on 5,000 restaurants to perform an ablation\nstudy on text preprocessing techniques. We also compare the effectiveness of\nseveral machine learning and deep learning models on predicting user sentiment\n(negative, neutral, or positive). For machine learning models, we find that\nusing binary bag-of-word representation, adding bi-grams, imposing minimum\nfrequency constraints and normalizing texts have positive effects on model\nperformance. For deep learning models, we find that using pre-trained word\nembeddings and capping maximum length often boost model performance. Finally,\nusing macro F1 score as our comparison metric, we find simpler models such as\nLogistic Regression and Support Vector Machine to be more effective at\npredicting sentiments than more complex models such as Gradient Boosting, LSTM\nand BERT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:50:49 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Liu", "Siqi", ""]]}, {"id": "2004.13852", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Jun Ma, Xin Luna Dong", "title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product\n  Categories", "comments": "Accepted to ACL 2020 (Long Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured knowledge from product profiles is crucial for various\napplications in e-Commerce. State-of-the-art approaches for knowledge\nextraction were each designed for a single category of product, and thus do not\napply to real-life e-Commerce scenarios, which often contain thousands of\ndiverse categories. This paper proposes TXtract, a taxonomy-aware knowledge\nextraction model that applies to thousands of product categories organized in a\nhierarchical taxonomy. Through category conditional self-attention and\nmulti-task learning, our approach is both scalable, as it trains a single model\nfor thousands of categories, and effective, as it extracts category-specific\nattribute values. Experiments on products from a taxonomy with 4,000 categories\nshow that TXtract outperforms state-of-the-art approaches by up to 10% in F1\nand 15% in coverage across all categories.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 03:02:09 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 14:54:13 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Ma", "Jun", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2004.13921", "submitter": "Wen Yu Kon", "authors": "Wen Yu Kon, Charles Ci Wen Lim", "title": "Provably-secure symmetric private information retrieval with quantum\n  cryptography", "comments": "19 pages", "journal-ref": "Entropy 23, 54 (2021)", "doi": "10.3390/e23010054", "report-no": null, "categories": "quant-ph cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private information retrieval (PIR) is a database query protocol that\nprovides user privacy, in that the user can learn a particular entry of the\ndatabase of his interest but his query would be hidden from the data centre.\nSymmetric private information retrieval (SPIR) takes PIR further by\nadditionally offering database privacy, where the user cannot learn any\nadditional entries of the database. Unconditionally secure SPIR solutions with\nmultiple databases are known classically, but are unrealistic because they\nrequire long shared secret keys between the parties for secure communication\nand shared randomness in the protocol. Here, we propose using quantum key\ndistribution (QKD) instead for a practical implementation, which can realise\nboth the secure communication and shared randomness requirements. We prove that\nQKD maintains the security of the SPIR protocol and that it is also secure\nagainst any external eavesdropper. We also show how such a classical-quantum\nsystem could be implemented practically, using the example of a two-database\nSPIR protocol with keys generated by measurement device-independent QKD.\nThrough key rate calculations, we show that such an implementation is feasible\nat the metropolitan level with current QKD technology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 02:08:10 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 02:43:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kon", "Wen Yu", ""], ["Lim", "Charles Ci Wen", ""]]}, {"id": "2004.13969", "submitter": "Luyu Gao", "authors": "Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Benjamin Van Durme,\n  Jamie Callan", "title": "Complementing Lexical Retrieval with Semantic Residual Embedding", "comments": "ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CLEAR, a retrieval model that seeks to complement\nclassical lexical exact-match models such as BM25 with semantic matching\nsignals from a neural embedding matching model. CLEAR explicitly trains the\nneural embedding to encode language structures and semantics that lexical\nretrieval fails to capture with a novel residual-based embedding learning\nmethod. Empirical evaluations demonstrate the advantages of CLEAR over\nstate-of-the-art retrieval models, and that it can substantially improve the\nend-to-end accuracy and efficiency of reranking pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:10:02 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 17:13:03 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 06:19:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Chen", "Tongfei", ""], ["Fan", "Zhen", ""], ["Van Durme", "Benjamin", ""], ["Callan", "Jamie", ""]]}, {"id": "2004.13972", "submitter": "Avishek Anand", "authors": "Jaspreet Singh, Zhenye Wang, Megha Khosla, and Avishek Anand", "title": "Valid Explanations for Learning to Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-to-rank (LTR) is a class of supervised learning techniques that\napply to ranking problems dealing with a large number of features.\n  The popularity and widespread application of LTR models in prioritizing\ninformation in a variety of domains makes their scrutability vital in today's\nlandscape of fair and transparent learning systems. However, limited work\nexists that deals with interpreting the decisions of learning systems that\noutput rankings. In this paper we propose a model agnostic local explanation\nmethod that seeks to identify a small subset of input features as explanation\nto a ranking decision. We introduce new notions of validity and completeness of\nexplanations specifically for rankings, based on the presence or absence of\nselected features, as a way of measuring goodness. We devise a novel\noptimization problem to maximize validity directly and propose greedy\nalgorithms as solutions. In extensive quantitative experiments we show that our\napproach outperforms other model agnostic explanation approaches across\npointwise, pairwise and listwise LTR models in validity while not compromising\non completeness.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:21:56 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 13:31:40 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 15:46:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Singh", "Jaspreet", ""], ["Wang", "Zhenye", ""], ["Khosla", "Megha", ""], ["Anand", "Avishek", ""]]}, {"id": "2004.14054", "submitter": "Nicola Tonellotto", "authors": "I. Mele, C. I. Muntean, F. M. Nardini, R. Perego, N. Tonellotto, O.\n  Frieder", "title": "Topic Propagation in Conversational Search", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a conversational context, a user expresses her multi-faceted information\nneed as a sequence of natural-language questions, i.e., utterances. Starting\nfrom a given topic, the conversation evolves through user utterances and system\nreplies. The retrieval of documents relevant to a given utterance in a\nconversation is challenging due to ambiguity of natural language and to the\ndifficulty of detecting possible topic shifts and semantic relationships among\nutterances. We adopt the 2019 TREC Conversational Assistant Track (CAsT)\nframework to experiment with a modular architecture performing: (i) topic-aware\nutterance rewriting, (ii) retrieval of candidate passages for the rewritten\nutterances, and (iii) neural-based re-ranking of candidate passages. We present\na comprehensive experimental evaluation of the architecture assessed in terms\nof traditional IR metrics at small cutoffs. Experimental results show the\neffectiveness of our techniques that achieve an improvement up to 0.28 (+93%)\nfor P@1 and 0.19 (+89.9%) for nDCG@3 w.r.t. the CAsT baseline.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:06:00 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Mele", "I.", ""], ["Muntean", "C. I.", ""], ["Nardini", "F. M.", ""], ["Perego", "R.", ""], ["Tonellotto", "N.", ""], ["Frieder", "O.", ""]]}, {"id": "2004.14162", "submitter": "Pengjie Ren", "authors": "Pengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof\n  Monz, and Maarten de Rijke", "title": "Conversations with Search Engines: SERP-based Conversational Response\n  Generation", "comments": "published in TOIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of answering complex information needs\nby conversing conversations with search engines, in the sense that users can\nexpress their queries in natural language, and directly receivethe information\nthey need from a short system response in a conversational manner. Recently,\nthere have been some attempts towards a similar goal, e.g., studies on\nConversational Agents (CAs) and Conversational Search (CS). However, they\neither do not address complex information needs, or they are limited to the\ndevelopment of conceptual frameworks and/or laboratory-based user studies.\n  We pursue two goals in this paper: (1) the creation of a suitable dataset,\nthe Search as a Conversation (SaaC) dataset, for the development of pipelines\nfor conversations with search engines, and (2) the development of\nastate-of-the-art pipeline for conversations with search engines, the\nConversations with Search Engines (CaSE), using this dataset. SaaC is built\nbased on a multi-turn conversational search dataset, where we further employ\nworkers from a crowdsourcing platform to summarize each relevant passage into a\nshort, conversational response. CaSE enhances the state-of-the-art by\nintroducing a supporting token identification module and aprior-aware pointer\ngenerator, which enables us to generate more accurate responses.\n  We carry out experiments to show that CaSE is able to outperform strong\nbaselines. We also conduct extensive analyses on the SaaC dataset to show where\nthere is room for further improvement beyond CaSE. Finally, we release the SaaC\ndataset and the code for CaSE and all models used for comparison to facilitate\nfuture research on this topic.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:07:53 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 06:40:31 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Kanoulas", "Evangelos", ""], ["Monz", "Christof", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2004.14176", "submitter": "Emeka Ogbuju Mr", "authors": "Emeka Ogbuju and Moses Onyesolu", "title": "Development of a General Purpose Sentiment Lexicon for Igbo Language", "comments": "Accepted and presented at the Widening Natural Language Processing\n  (WiNLP) workshop, co-located with the Association for Computational\n  Linguistics (ACL) conference 2019 in Florence, Italy. See\n  https://www.winlp.org/wp-content/uploads/2019/final_papers/103_Paper.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are publicly available general purpose sentiment lexicons in some high\nresource languages but very few exist in the low resource languages. This makes\nit difficult to directly perform sentiment analysis tasks in such languages.\nThe objective of this work is to create a general purpose sentiment lexicon for\nthe Igbo language that can determine the sentiment of documents written in the\nIgbo language without having to translate it to the English language. The\nmaterial used was an automatically translated lexicon by Liu and the manual\naddition of Igbo native words. The result of this work is a general purpose\nlexicon called IgboSentilex. The performance was tested on the BBC Igbo news\nchannel. It returned an average polarity agreement of 95.75 percent with other\ngeneral purpose sentiment lexicons.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 22:10:34 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Ogbuju", "Emeka", ""], ["Onyesolu", "Moses", ""]]}, {"id": "2004.14201", "submitter": "Ruize Wang", "authors": "Ruize Wang, Duyu Tang, Nan Duan, Wanjun Zhong, Zhongyu Wei, Xuanjing\n  Huang, Daxin Jiang, Ming Zhou", "title": "Leveraging Declarative Knowledge in Text and First-Order Logic for\n  Fine-Grained Propaganda Detection", "comments": "Accepted as a long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection of propagandistic text fragments in news articles.\nInstead of merely learning from input-output datapoints in training data, we\nintroduce an approach to inject declarative knowledge of fine-grained\npropaganda techniques. Specifically, we leverage the declarative knowledge\nexpressed in both first-order logic and natural language. The former refers to\nthe logical consistency between coarse- and fine-grained predictions, which is\nused to regularize the training process with propositional Boolean expressions.\nThe latter refers to the literal definition of each propaganda technique, which\nis utilized to get class representations for regularizing the model parameters.\nWe conduct experiments on Propaganda Techniques Corpus, a large manually\nannotated dataset for fine-grained propaganda detection. Experiments show that\nour method achieves superior performance, demonstrating that leveraging\ndeclarative knowledge can help the model to make more accurate predictions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 13:46:15 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 13:08:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Ruize", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Zhong", "Wanjun", ""], ["Wei", "Zhongyu", ""], ["Huang", "Xuanjing", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""]]}, {"id": "2004.14245", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola\n  Tonellotto, Nazli Goharian, Ophir Frieder", "title": "Expansion via Prediction of Importance with Contextualization", "comments": "Accepted at SIGIR 2020 (short)", "journal-ref": null, "doi": "10.1145/3397271.3401262", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of relevance with little textual context is a primary\nchallenge in passage retrieval. We address this problem with a\nrepresentation-based ranking approach that: (1) explicitly models the\nimportance of each term using a contextualized language model; (2) performs\npassage expansion by propagating the importance to similar terms; and (3)\ngrounds the representations in the lexicon, making them interpretable. Passage\nrepresentations can be pre-computed at index time to reduce query-time latency.\nWe call our approach EPIC (Expansion via Prediction of Importance with\nContextualization). We show that EPIC significantly outperforms prior\nimportance-modeling and document expansion approaches. We also observe that the\nperformance is additive with the current leading first-stage retrieval methods,\nfurther narrowing the gap between inexpensive and cost-prohibitive passage\nranking approaches. Specifically, EPIC achieves a MRR@10 of 0.304 on the\nMS-MARCO passage ranking dataset with 78ms average query latency on commodity\nhardware. We also find that the latency is further reduced to 68ms by pruning\ndocument representations, with virtually no difference in effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 14:52:09 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 21:26:14 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["MacAvaney", "Sean", ""], ["Nardini", "Franco Maria", ""], ["Perego", "Raffaele", ""], ["Tonellotto", "Nicola", ""], ["Goharian", "Nazli", ""], ["Frieder", "Ophir", ""]]}, {"id": "2004.14255", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola\n  Tonellotto, Nazli Goharian, Ophir Frieder", "title": "Efficient Document Re-Ranking for Transformers by Precomputing Term\n  Representations", "comments": "Accepted at SIGIR 2020 (long)", "journal-ref": null, "doi": "10.1145/3397271.3401093", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep pretrained transformer networks are effective at various ranking tasks,\nsuch as question answering and ad-hoc document ranking. However, their\ncomputational expenses deem them cost-prohibitive in practice. Our proposed\napproach, called PreTTR (Precomputing Transformer Term Representations),\nconsiderably reduces the query-time latency of deep transformer networks (up to\na 42x speedup on web document ranking) making these networks more practical to\nuse in a real-time ranking scenario. Specifically, we precompute part of the\ndocument term representations at indexing time (without a query), and merge\nthem with the query representation at query time to compute the final ranking\nscore. Due to the large size of the token representations, we also propose an\neffective approach to reduce the storage requirement by training a compression\nlayer to match attention scores. Our compression technique reduces the storage\nrequired up to 95% and it can be applied without a substantial degradation in\nranking performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:04:22 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 17:11:32 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["MacAvaney", "Sean", ""], ["Nardini", "Franco Maria", ""], ["Perego", "Raffaele", ""], ["Tonellotto", "Nicola", ""], ["Goharian", "Nazli", ""], ["Frieder", "Ophir", ""]]}, {"id": "2004.14265", "submitter": "Epaminondas Kapetanios", "authors": "Epaminondas Kapetanios, Vijayan Sugumaran, and Anastassia Angelopoulou", "title": "Exploring the Suitability of Semantic Spaces as Word Association Models\n  for the Extraction of Semantic Relationships", "comments": "10 pages, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Given the recent advances and progress in Natural Language Processing (NLP),\nextraction of semantic relationships has been at the top of the research agenda\nin the last few years. This work has been mainly motivated by the fact that\nbuilding knowledge graphs (KG) and bases (KB), as a key ingredient of\nintelligent applications, is a never-ending challenge, since new knowledge\nneeds to be harvested while old knowledge needs to be revised. Currently,\napproaches towards relation extraction from text are dominated by neural models\npracticing some sort of distant (weak) supervision in machine learning from\nlarge corpora, with or without consulting external knowledge sources. In this\npaper, we empirically study and explore the potential of a novel idea of using\nclassical semantic spaces and models, e.g., Word Embedding, generated for\nextracting word association, in conjunction with relation extraction\napproaches. The goal is to use these word association models to reinforce\ncurrent relation extraction approaches. We believe that this is a first attempt\nof this kind and the results of the study should shed some light on the extent\nto which these word association models can be used as well as the most\npromising types of relationships to be considered for extraction.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:25:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kapetanios", "Epaminondas", ""], ["Sugumaran", "Vijayan", ""], ["Angelopoulou", "Anastassia", ""]]}, {"id": "2004.14269", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola\n  Tonellotto, Nazli Goharian, Ophir Frieder", "title": "Training Curricula for Open Domain Answer Re-Ranking", "comments": "Accepted at SIGIR 2020 (long)", "journal-ref": null, "doi": "10.1145/3397271.3401094", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In precision-oriented tasks like answer ranking, it is more important to rank\nmany relevant answers highly than to retrieve all relevant answers. It follows\nthat a good ranking strategy would be to learn how to identify the easiest\ncorrect answers first (i.e., assign a high ranking score to answers that have\ncharacteristics that usually indicate relevance, and a low ranking score to\nthose with characteristics that do not), before incorporating more complex\nlogic to handle difficult cases (e.g., semantic matching or reasoning). In this\nwork, we apply this idea to the training of neural answer rankers using\ncurriculum learning. We propose several heuristics to estimate the difficulty\nof a given training sample. We show that the proposed heuristics can be used to\nbuild a training curriculum that down-weights difficult samples early in the\ntraining process. As the training process progresses, our approach gradually\nshifts to weighting all samples equally, regardless of difficulty. We present a\ncomprehensive evaluation of our proposed idea on three answer ranking datasets.\nResults show that our approach leads to superior performance of two leading\nneural ranking architectures, namely BERT and ConvKNRM, using both pointwise\nand pairwise losses. When applied to a BERT-based ranker, our method yields up\nto a 4% improvement in MRR and a 9% improvement in P@1 (compared to the model\ntrained without a curriculum). This results in models that can achieve\ncomparable performance to more expensive state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:30:06 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 14:20:44 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["MacAvaney", "Sean", ""], ["Nardini", "Franco Maria", ""], ["Perego", "Raffaele", ""], ["Tonellotto", "Nicola", ""], ["Goharian", "Nazli", ""], ["Frieder", "Ophir", ""]]}, {"id": "2004.14294", "submitter": "Jurek Leonhardt", "authors": "Jurek Leonhardt, Avishek Anand, Megha Khosla", "title": "Boilerplate Removal using a Neural Sequence Labeling Model", "comments": "WWW20 Demo paper", "journal-ref": null, "doi": "10.1145/3366424.3383547", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of main content from web pages is an important task for\nnumerous applications, ranging from usability aspects, like reader views for\nnews articles in web browsers, to information retrieval or natural language\nprocessing. Existing approaches are lacking as they rely on large amounts of\nhand-crafted features for classification. This results in models that are\ntailored to a specific distribution of web pages, e.g. from a certain time\nframe, but lack in generalization power. We propose a neural sequence labeling\nmodel that does not rely on any hand-crafted features but takes only the HTML\ntags and words that appear in a web page as input. This allows us to present a\nbrowser extension which highlights the content of arbitrary web pages directly\nwithin the browser using our model. In addition, we create a new, more current\ndataset to show that our model is able to adapt to changes in the structure of\nweb pages and outperform the state-of-the-art model.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 08:06:59 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Leonhardt", "Jurek", ""], ["Anand", "Avishek", ""], ["Khosla", "Megha", ""]]}, {"id": "2004.14443", "submitter": "Johny Moreira", "authors": "Johny Moreira, Chaina Oliveira, David Mac\\^edo, Cleber Zanchettin,\n  Luciano Barbosa", "title": "Distantly-Supervised Neural Relation Extraction with Side Information\n  using BERT", "comments": "2020 International Joint Conference on Neural Networks (IJCNN)", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9206648", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) consists in categorizing the relationship between\nentities in a sentence. A recent paradigm to develop relation extractors is\nDistant Supervision (DS), which allows the automatic creation of new datasets\nby taking an alignment between a text corpus and a Knowledge Base (KB). KBs can\nsometimes also provide additional information to the RE task. One of the\nmethods that adopt this strategy is the RESIDE model, which proposes a\ndistantly-supervised neural relation extraction using side information from\nKBs. Considering that this method outperformed state-of-the-art baselines, in\nthis paper, we propose a related approach to RESIDE also using additional side\ninformation, but simplifying the sentence encoding with BERT embeddings.\nThrough experiments, we show the effectiveness of the proposed method in Google\nDistant Supervision and Riedel datasets concerning the BGWA and RESIDE baseline\nmethods. Although Area Under the Curve is decreased because of unbalanced\ndatasets, P@N results have shown that the use of BERT as sentence encoding\nallows superior performance to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:29:10 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 21:45:15 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 20:30:34 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Moreira", "Johny", ""], ["Oliveira", "Chaina", ""], ["Mac\u00eado", "David", ""], ["Zanchettin", "Cleber", ""], ["Barbosa", "Luciano", ""]]}, {"id": "2004.14503", "submitter": "Ji Ma", "authors": "Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall and Ryan McDonald", "title": "Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic\n  Question Generation", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major obstacle to the wide-spread adoption of neural retrieval models is\nthat they require large supervised training sets to surpass traditional\nterm-based techniques, which are constructed from raw corpora. In this paper,\nwe propose an approach to zero-shot learning for passage retrieval that uses\nsynthetic question generation to close this gap. The question generation system\nis trained on general domain data, but is applied to documents in the targeted\ndomain. This allows us to create arbitrarily large, yet noisy, question-passage\nrelevance pairs that are domain specific. Furthermore, when this is coupled\nwith a simple hybrid term-neural model, first-stage retrieval performance can\nbe improved further. Empirically, we show that this is an effective strategy\nfor building neural passage retrieval models in the absence of large training\ncorpora. Depending on the domain, this technique can even approach the accuracy\nof supervised models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:21:31 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 13:29:55 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 16:04:12 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ma", "Ji", ""], ["Korotkov", "Ivan", ""], ["Yang", "Yinfei", ""], ["Hall", "Keith", ""], ["McDonald", "Ryan", ""]]}, {"id": "2004.14592", "submitter": "Chongyang Tao", "authors": "Jiayi Zhang, Chongyang Tao, Zhenjing Xu, Qiaojing Xie, Wei Chen, Rui\n  Yan", "title": "EnsembleGAN: Adversarial Learning for Retrieval-Generation Ensemble\n  Model on Short-Text Conversation", "comments": "10 pages, SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating qualitative responses has always been a challenge for\nhuman-computer dialogue systems. Existing dialogue systems generally derive\nfrom either retrieval-based or generative-based approaches, both of which have\ntheir own pros and cons. Despite the natural idea of an ensemble model of the\ntwo, existing ensemble methods only focused on leveraging one approach to\nenhance another, we argue however that they can be further mutually enhanced\nwith a proper training strategy. In this paper, we propose ensembleGAN, an\nadversarial learning framework for enhancing a retrieval-generation ensemble\nmodel in open-domain conversation scenario. It consists of a\nlanguage-model-like generator, a ranker generator, and one ranker\ndiscriminator. Aiming at generating responses that approximate the ground-truth\nand receive high ranking scores from the discriminator, the two generators\nlearn to generate improved highly relevant responses and competitive unobserved\ncandidates respectively, while the discriminative ranker is trained to identify\ntrue responses from adversarial ones, thus featuring the merits of both\ngenerator counterparts. The experimental results on a large short-text\nconversation data demonstrate the effectiveness of the ensembleGAN by the\namelioration on both human and automatic evaluation metrics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:59:12 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Zhang", "Jiayi", ""], ["Tao", "Chongyang", ""], ["Xu", "Zhenjing", ""], ["Xie", "Qiaojing", ""], ["Chen", "Wei", ""], ["Yan", "Rui", ""]]}, {"id": "2004.14641", "submitter": "Raffaele Perego", "authors": "Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele\n  Perego, Salvatore Trani", "title": "Query-level Early Exit for Additive Learning-to-Rank Ensembles", "comments": "Accepted at SIGIR 2020 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine ranking pipelines are commonly based on large ensembles of\nmachine-learned decision trees. The tight constraints on query response time\nrecently motivated researchers to investigate algorithms to make faster the\ntraversal of the additive ensemble or to early terminate the evaluation of\ndocuments that are unlikely to be ranked among the top-k. In this paper, we\ninvestigate the novel problem of \\textit{query-level early exiting}, aimed at\ndeciding the profitability of early stopping the traversal of the ranking\nensemble for all the candidate documents to be scored for a query, by simply\nreturning a ranking based on the additive scores computed by a limited portion\nof the ensemble. Besides the obvious advantage on query latency and throughput,\nwe address the possible positive impact of query-level early exiting on ranking\neffectiveness. To this end, we study the actual contribution of incremental\nportions of the tree ensemble to the ranking of the top-k documents scored for\na given query. Our main finding is that queries exhibit different behaviors as\nscores are accumulated during the traversal of the ensemble and that\nquery-level early stopping can remarkably improve ranking quality. We present a\nreproducible and comprehensive experimental evaluation, conducted on two public\ndatasets, showing that query-level early exiting achieves an overall gain of up\nto 7.5% in terms of NDCG@10 with a speedup of the scoring process of up to\n2.2x.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 08:59:45 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lucchese", "Claudio", ""], ["Nardini", "Franco Maria", ""], ["Orlando", "Salvatore", ""], ["Perego", "Raffaele", ""], ["Trani", "Salvatore", ""]]}, {"id": "2004.14652", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Shayne Longpre, Zhucheng Tu, Raviteja Anantha", "title": "Question Rewriting for Conversational Question Answering", "comments": "Version accepted to WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational question answering (QA) requires the ability to correctly\ninterpret a question in the context of previous conversation turns. We address\nthe conversational QA task by decomposing it into question rewriting and\nquestion answering subtasks. The question rewriting (QR) subtask is\nspecifically designed to reformulate ambiguous questions, which depend on the\nconversational context, into unambiguous questions that can be correctly\ninterpreted outside of the conversational context. We introduce a\nconversational QA architecture that sets the new state of the art on the TREC\nCAsT 2019 passage retrieval dataset. Moreover, we show that the same QR model\nimproves QA performance on the QuAC dataset with respect to answer span\nextraction, which is the next step in QA after passage retrieval. Our\nevaluation results indicate that the QR model we proposed achieves near\nhuman-level performance on both datasets and the gap in performance on the\nend-to-end conversational QA task is attributed mostly to the errors in QA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:27:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 11:25:55 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 09:22:49 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Longpre", "Shayne", ""], ["Tu", "Zhucheng", ""], ["Anantha", "Raviteja", ""]]}, {"id": "2004.14691", "submitter": "Suat Mercan", "authors": "Suat Mercan, Mumin Cebe, Ege Tekiner, Kemal Akkaya, Melissa Chang and\n  Selcuk Uluagac", "title": "A Cost-efficient IoT Forensics Framework with Blockchain", "comments": null, "journal-ref": null, "doi": "10.1109/ICBC48266.2020.9169397", "report-no": null, "categories": "cs.CR cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT devices have been adopted widely in the last decade which enabled\ncollection of various data from different environments. The collected data is\ncrucial in certain applications where IoT devices generate data for critical\ninfrastructure or systems whose failure may result in catastrophic results.\nSpecifically, for such critical applications, data storage poses challenges\nsince the data may be compromised during the storage and the integrity might be\nviolated without being noticed. In such cases, integrity and data provenance\nare required in order to be able to detect the source of any incident and prove\nit in legal cases if there is a dispute with the involved parties. To address\nthese issues, blockchain provides excellent opportunities since it can protect\nthe integrity of the data thanks to its distributed structure. However, it\ncomes with certain costs as storing huge amount of data in a public blockchain\nwill come with significant transaction fees. In this paper, we propose a highly\ncost effective and reliable digital forensics framework by exploiting multiple\ninexpensive blockchain networks as a temporary storage before the data is\ncommitted to Ethereum. To reduce Ethereum costs,we utilize Merkle trees which\nhierarchically stores hashes of the collected event data from IoT devices. We\nevaluated the approach on popular blockchains such as EOS, Stellar, and\nEthereum by presenting a cost and security analysis. The results indicate that\nwe can achieve significant cost savings without compromising the integrity of\nthe data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 11:11:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Mercan", "Suat", ""], ["Cebe", "Mumin", ""], ["Tekiner", "Ege", ""], ["Akkaya", "Kemal", ""], ["Chang", "Melissa", ""], ["Uluagac", "Selcuk", ""]]}, {"id": "2004.14714", "submitter": "Jiarui Jin", "authors": "Jiarui Jin, Yuchen Fang, Weinan Zhang, Kan Ren, Guorui Zhou, Jian Xu,\n  Yong Yu, Jun Wang, Xiaoqiang Zhu, Kun Gai", "title": "A Deep Recurrent Survival Model for Unbiased Ranking", "comments": "SIGIR 2020. arXiv admin note: text overlap with arXiv:1809.05818 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Position bias is a critical problem in information retrieval when dealing\nwith implicit yet biased user feedback data. Unbiased ranking methods typically\nrely on causality models and debias the user feedback through inverse\npropensity weighting. While practical, these methods still suffer from two\nmajor problems. First, when inferring a user click, the impact of the\ncontextual information, such as documents that have been examined, is often\nignored. Second, only the position bias is considered but other issues resulted\nfrom user browsing behaviors are overlooked. In this paper, we propose an\nend-to-end Deep Recurrent Survival Ranking (DRSR), a unified framework to\njointly model user's various behaviors, to (i) consider the rich contextual\ninformation in the ranking list; and (ii) address the hidden issues underlying\nuser behaviors, i.e., to mine observe pattern in queries without any click\n(non-click queries), and to model tracking logs which cannot truly reflect the\nuser browsing intents (untrusted observation). Specifically, we adopt a\nrecurrent neural network to model the contextual information and estimates the\nconditional likelihood of user feedback at each position. We then incorporate\nsurvival analysis techniques with the probability chain rule to mathematically\nrecover the unbiased joint probability of one user's various behaviors. DRSR\ncan be easily incorporated with both point-wise and pair-wise learning\nobjectives. The extensive experiments over two large-scale industrial datasets\ndemonstrate the significant performance gains of our model comparing with the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:07:52 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 10:14:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Jin", "Jiarui", ""], ["Fang", "Yuchen", ""], ["Zhang", "Weinan", ""], ["Ren", "Kan", ""], ["Zhou", "Guorui", ""], ["Xu", "Jian", ""], ["Yu", "Yong", ""], ["Wang", "Jun", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "2004.14734", "submitter": "Shaowen Peng", "authors": "Shaowen Peng, Tsunenori Mine", "title": "A Robust Hierarchical Graph Convolutional Network Model for\n  Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Network (GCN) has achieved great success and has been\napplied in various fields including recommender systems. However, GCN still\nsuffers from many issues such as training difficulties, over-smoothing,\nvulnerable to adversarial attacks, etc. Distinct from current GCN-based methods\nwhich simply employ GCN for recommendation, in this paper we are committed to\nbuild a robust GCN model for collaborative filtering. Firstly, we argue that\nrecursively incorporating messages from different order neighborhood mixes\ndistinct node messages indistinguishably, which increases the training\ndifficulty; instead we choose to separately aggregate different order neighbor\nmessages with a simple GCN model which has been shown effective; then we\naccumulate them together in a hierarchical way without introducing additional\nmodel parameters. Secondly, we propose a solution to alleviate over-smoothing\nby randomly dropping out neighbor messages at each layer, which also well\nprevents over-fitting and enhances the robustness. Extensive experiments on\nthree real-world datasets demonstrate the effectiveness and robustness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:50:39 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Peng", "Shaowen", ""], ["Mine", "Tsunenori", ""]]}, {"id": "2004.14969", "submitter": "Baoxu Shi", "authors": "Baoxu Shi, Shan Li, Jaewon Yang, Mustafa Emre Kazdagli, Qi He", "title": "Learning to Ask Screening Questions for Job Postings", "comments": "10 pages, to appear in SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At LinkedIn, we want to create economic opportunity for everyone in the\nglobal workforce. A critical aspect of this goal is matching jobs with\nqualified applicants. To improve hiring efficiency and reduce the need to\nmanually screening each applicant, we develop a new product where recruiters\ncan ask screening questions online so that they can filter qualified candidates\neasily. To add screening questions to all $20$M active jobs at LinkedIn, we\npropose a new task that aims to automatically generate screening questions for\na given job posting. To solve the task of generating screening questions, we\ndevelop a two-stage deep learning model called Job2Questions, where we apply a\ndeep learning model to detect intent from the text description, and then rank\nthe detected intents by their importance based on other contextual features.\nSince this is a new product with no historical data, we employ deep transfer\nlearning to train complex models with limited training data. We launched the\nscreening question product and our AI models to LinkedIn users and observed\nsignificant impact in the job marketplace. During our online A/B test, we\nobserved $+53.10\\%$ screening question suggestion acceptance rate, $+22.17\\%$\njob coverage, $+190\\%$ recruiter-applicant interaction, and $+11$ Net Promoter\nScore. In sum, the deployed Job2Questions model helps recruiters to find\nqualified applicants and job seekers to find jobs they are qualified for.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:18:17 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Shi", "Baoxu", ""], ["Li", "Shan", ""], ["Yang", "Jaewon", ""], ["Kazdagli", "Mustafa Emre", ""], ["He", "Qi", ""]]}]