[{"id": "1609.00161", "submitter": "Frederic Prost", "authors": "Frederic Prost and Jisang Yoon", "title": "Parallel Clustering of Graphs for Anonymization and Recommender Systems", "comments": "submitted to VLDB'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is widely used in many data analysis applications. In this\npaper we propose several parallel graph clustering algorithms based on Monte\nCarlo simulations and expectation maximization in the context of stochastic\nblock models. We apply those algorithms to the specific problems of recommender\nsystems and social network anonymization. We compare the experimental results\nto previous propositions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 09:43:29 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 08:06:38 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Prost", "Frederic", ""], ["Yoon", "Jisang", ""]]}, {"id": "1609.00464", "submitter": "Trey Grainger", "authors": "Trey Grainger, Khalifeh AlJadda, Mohammed Korayem, Andries Smith", "title": "The Semantic Knowledge Graph: A compact, auto-generated model for\n  real-time traversal and ranking of any relationship within a domain", "comments": "Accepted for publication in 2016 IEEE 3rd International Conference on\n  Data Science and Advanced Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new kind of knowledge representation and mining system\nwhich we are calling the Semantic Knowledge Graph. At its heart, the Semantic\nKnowledge Graph leverages an inverted index, along with a complementary\nuninverted index, to represent nodes (terms) and edges (the documents within\nintersecting postings lists for multiple terms/nodes). This provides a layer of\nindirection between each pair of nodes and their corresponding edge, enabling\nedges to materialize dynamically from underlying corpus statistics. As a\nresult, any combination of nodes can have edges to any other nodes materialize\nand be scored to reveal latent relationships between the nodes. This provides\nnumerous benefits: the knowledge graph can be built automatically from a\nreal-world corpus of data, new nodes - along with their combined edges - can be\ninstantly materialized from any arbitrary combination of preexisting nodes\n(using set operations), and a full model of the semantic relationships between\nall entities within a domain can be represented and dynamically traversed using\na highly compact representation of the graph. Such a system has widespread\napplications in areas as diverse as knowledge modeling and reasoning, natural\nlanguage processing, anomaly detection, data cleansing, semantic search,\nanalytics, data classification, root cause analysis, and recommendations\nsystems. The main contribution of this paper is the introduction of a novel\nsystem - the Semantic Knowledge Graph - which is able to dynamically discover\nand score interesting relationships between any arbitrary combination of\nentities (words, phrases, or extracted concepts) through dynamically\nmaterializing nodes and edges from a compact graphical representation built\nautomatically from a corpus of data representative of a knowledge domain.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 04:26:54 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 15:06:45 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Grainger", "Trey", ""], ["AlJadda", "Khalifeh", ""], ["Korayem", "Mohammed", ""], ["Smith", "Andries", ""]]}, {"id": "1609.00511", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hosein Azarbonyad, Jaap Kamps, Maarten Marx", "title": "Generalized Group Profiling for Content Customization", "comments": "Short paper (4 pages) published in proceedings of ACM SIGIR\n  Conference on Human Information Interaction and Retrieval (CHIIR'16)", "journal-ref": null, "doi": "10.1145/2854946.2855003", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ongoing debate on personalization, adapting results to the unique\nuser exploiting a user's personal history, versus customization, adapting\nresults to a group profile sharing one or more characteristics with the user at\nhand. Personal profiles are often sparse, due to cold start problems and the\nfact that users typically search for new items or information, necessitating to\nback-off to customization, but group profiles often suffer from accidental\nfeatures brought in by the unique individual contributing to the group. In this\npaper we propose a generalized group profiling approach that teases apart the\nexact contribution of the individual user level and the \"abstract\" group level\nby extracting a latent model that captures all, and only, the essential\nfeatures of the whole group. Our main findings are the followings. First, we\npropose an efficient way of group profiling which implicitly eliminates the\ngeneral and specific features from users' models in a group and takes out the\nabstract model representing the whole group. Second, we employ the resulting\nmodels in the task of contextual suggestion. We analyse different grouping\ncriteria and we find that group-based suggestions improve the customization.\nThird, we see that the granularity of groups affects the quality of group\nprofiling. We observe that grouping approach should compromise between the\nlevel of customization and groups' size.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 09:14:07 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Azarbonyad", "Hosein", ""], ["Kamps", "Jaap", ""], ["Marx", "Maarten", ""]]}, {"id": "1609.00514", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hosein Azarbonyad, Jaap Kamps, Maarten Marx", "title": "On Horizontal and Vertical Separation in Hierarchical Text\n  Classification", "comments": "Full paper (10 pages) accepted for publication in proceedings of ACM\n  SIGIR International Conference on the Theory of Information Retrieval\n  (ICTIR'16)", "journal-ref": null, "doi": "10.1145/2970398.2970408", "report-no": null, "categories": "cs.IR cs.CL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchy is a common and effective way of organizing data and representing\ntheir relationships at different levels of abstraction. However, hierarchical\ndata dependencies cause difficulties in the estimation of \"separable\" models\nthat can distinguish between the entities in the hierarchy. Extracting\nseparable models of hierarchical entities requires us to take their relative\nposition into account and to consider the different types of dependencies in\nthe hierarchy. In this paper, we present an investigation of the effect of\nseparability in text-based entity classification and argue that in hierarchical\nclassification, a separation property should be established between entities\nnot only in the same layer, but also in different layers. Our main findings are\nthe followings. First, we analyse the importance of separability on the data\nrepresentation in the task of classification and based on that, we introduce a\n\"Strong Separation Principle\" for optimizing expected effectiveness of\nclassifiers decision based on separation property. Second, we present\nHierarchical Significant Words Language Models (HSWLM) which capture all, and\nonly, the essential features of hierarchical entities according to their\nrelative position in the hierarchy resulting in horizontally and vertically\nseparable models. Third, we validate our claims on real-world data and\ndemonstrate that how HSWLM improves the accuracy of classification and how it\nprovides transferable models over time. Although discussions in this paper\nfocus on the classification problem, the models are applicable to any\ninformation access tasks on data that has, or can be mapped to, a hierarchical\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 09:21:33 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Azarbonyad", "Hosein", ""], ["Kamps", "Jaap", ""], ["Marx", "Maarten", ""]]}, {"id": "1609.00543", "submitter": "Richard Oentaryo", "authors": "Richard Jayadi Oentaryo, Arinto Murdopo, Philips Kokoh Prasetyo,\n  Ee-Peng Lim", "title": "On Profiling Bots in Social Media", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-47880-7_6", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of social media platforms such as Twitter has led to the\nproliferation of automated bots, creating both opportunities and challenges in\ninformation dissemination, user engagements, and quality of services. Past\nworks on profiling bots had been focused largely on malicious bots, with the\nassumption that these bots should be removed. In this work, however, we find\nmany bots that are benign, and propose a new, broader categorization of bots\nbased on their behaviors. This includes broadcast, consumption, and spam bots.\nTo facilitate comprehensive analyses of bots and how they compare to human\naccounts, we develop a systematic profiling framework that includes a rich set\nof features and classifier bank. We conduct extensive experiments to evaluate\nthe performances of different classifiers under varying time windows, identify\nthe key features of bots, and infer about bots in a larger Twitter population.\nOur analysis encompasses more than 159K bot and human (non-bot) accounts in\nTwitter. The results provide interesting insights on the behavioral traits of\nboth benign and malicious bots.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 10:47:28 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Oentaryo", "Richard Jayadi", ""], ["Murdopo", "Arinto", ""], ["Prasetyo", "Philips Kokoh", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "1609.00552", "submitter": "Aleksandr Chuklin", "authors": "Aleksandr Chuklin and Maarten de Rijke", "title": "Incorporating Clicks, Attention and Satisfaction into a Search Engine\n  Result Page Evaluation Model", "comments": "CIKM2016, Proceedings of the 25th ACM International Conference on\n  Information and Knowledge Management. 2016", "journal-ref": null, "doi": "10.1145/2983323.2983829", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern search engine result pages often provide immediate value to users and\norganize information in such a way that it is easy to navigate. The core\nranking function contributes to this and so do result snippets, smart\norganization of result blocks and extensive use of one-box answers or side\npanels. While they are useful to the user and help search engines to stand out,\nsuch features present two big challenges for evaluation. First, the presence of\nsuch elements on a search engine result page (SERP) may lead to the absence of\nclicks, which is, however, not related to dissatisfaction, so-called \"good\nabandonments.\" Second, the non-linear layout and visual difference of SERP\nitems may lead to non-trivial patterns of user attention, which is not captured\nby existing evaluation metrics.\n  In this paper we propose a model of user behavior on a SERP that jointly\ncaptures click behavior, user attention and satisfaction, the CAS model, and\ndemonstrate that it gives more accurate predictions of user actions and\nself-reported satisfaction than existing models based on clicks alone. We use\nthe CAS model to build a novel evaluation metric that can be applied to\nnon-linear SERP layouts and that can account for the utility that users obtain\ndirectly on a SERP. We demonstrate that this metric shows better agreement with\nuser-reported satisfaction than conventional evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 11:37:01 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Chuklin", "Aleksandr", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1609.00683", "submitter": "Alessandro Checco", "authors": "Alessandro Checco, Gianluca Demartini", "title": "Pairwise, Magnitude, or Stars: What's the Best Way for Crowds to Rate?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare three popular techniques of rating content: the ubiquitous five\nstar rating, the less used pairwise comparison, and the recently introduced (in\ncrowdsourcing) magnitude estimation approach. Each system has specific\nadvantages and disadvantages, in terms of required user effort, achievable user\npreference prediction accuracy and number of ratings required.\n  We design an experiment where the three techniques are compared in an\nunbiased way. We collected 39'000 ratings on a popular crowdsourcing platform,\nallowing us to release a dataset that will be useful for many related studies\non user rating techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 17:50:53 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Checco", "Alessandro", ""], ["Demartini", "Gianluca", ""]]}, {"id": "1609.00689", "submitter": "Christina Lioma Assoc. Prof", "authors": "Niels Dalum Hansen and Christina Lioma and K{\\aa}re M{\\o}lbak", "title": "Ensemble Learned Vaccination Uptake Prediction using Web Search Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that uses ensemble learning to combine clinical and\nweb-mined time-series data in order to predict future vaccination uptake. The\nclinical data is official vaccination registries, and the web data is query\nfrequencies collected from Google Trends. Experiments with official vaccine\nrecords show that our method predicts vaccination uptake effectively (4.7 Root\nMean Squared Error). Whereas performance is best when combining clinical and\nweb data, using solely web data yields comparative performance. To our\nknowledge, this is the first study to predict vaccination uptake using web data\n(with and without clinical data).\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 18:23:22 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Hansen", "Niels Dalum", ""], ["Lioma", "Christina", ""], ["M\u00f8lbak", "K\u00e5re", ""]]}, {"id": "1609.00799", "submitter": "Minh-Tien Nguyen", "authors": "Danilo S. Carvalho, Minh-Tien Nguyen, Tran Xuan Chien and Minh Le\n  Nguyen", "title": "Lexical-Morphological Modeling for Legal Text Analysis", "comments": "16 pages, 5 figures, Lecture notes in computer science: New Frontiers\n  in Artificial Intelligence, 2016/03", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the Competition on Legal Information Extraction/Entailment\n(COLIEE), we propose a method comprising the necessary steps for finding\nrelevant documents to a legal question and deciding on textual entailment\nevidence to provide a correct answer. The proposed method is based on the\ncombination of several lexical and morphological characteristics, to build a\nlanguage model and a set of features for Machine Learning algorithms. We\nprovide a detailed study on the proposed method performance and failure cases,\nindicating that it is competitive with state-of-the-art approaches on Legal\nInformation Retrieval and Question Answering, while not needing extensive\ntraining data nor depending on expert produced knowledge. The proposed method\nachieved significant results in the competition, indicating a substantial level\nof adequacy for the tasks addressed.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 07:24:08 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Carvalho", "Danilo S.", ""], ["Nguyen", "Minh-Tien", ""], ["Chien", "Tran Xuan", ""], ["Nguyen", "Minh Le", ""]]}, {"id": "1609.00969", "submitter": "Christina Lioma Assoc. Prof", "authors": "Casper Petersen and Jakob Grue Simonsen and Kalervo Jarvelin and\n  Christina Lioma", "title": "Adaptive Distributional Extensions to DFR Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence From Randomness (DFR) ranking models assume that informative terms\nare distributed in a corpus differently than non-informative terms. Different\nstatistical models (e.g. Poisson, geometric) are used to model the distribution\nof non-informative terms, producing different DFR models. An informative term\nis then detected by measuring the divergence of its distribution from the\ndistribution of non-informative terms. However, there is little empirical\nevidence that the distributions of non-informative terms used in DFR actually\nfit current datasets. Practically this risks providing a poor separation\nbetween informative and non-informative terms, thus compromising the\ndiscriminative power of the ranking model. We present a novel extension to DFR,\nwhich first detects the best-fitting distribution of non-informative terms in a\ncollection, and then adapts the ranking computation to this best-fitting\ndistribution. We call this model Adaptive Distributional Ranking (ADR) because\nit adapts the ranking to the statistics of the specific dataset being processed\neach time. Experiments on TREC data show ADR to outperform DFR models (and\ntheir extensions) and be comparable in performance to a query likelihood\nlanguage model (LM).\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 17:55:39 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Petersen", "Casper", ""], ["Simonsen", "Jakob Grue", ""], ["Jarvelin", "Kalervo", ""], ["Lioma", "Christina", ""]]}, {"id": "1609.00992", "submitter": "Nhien-An Le-Khac", "authors": "Maarten Banerveld, Nhien-An Le-Khac, Tahar Kechadi", "title": "Performance Evaluation of a Natural Language Processing approach applied\n  in White Collar crime investigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today world we are confronted with increasing amounts of information every\nday coming from a large variety of sources. People and co-operations are\nproducing data on a large scale, and since the rise of the internet, e-mail and\nsocial media the amount of produced data has grown exponentially. From a law\nenforcement perspective we have to deal with these huge amounts of data when a\ncriminal investigation is launched against an individual or company. Relevant\nquestions need to be answered like who committed the crime, who were involved,\nwhat happened and on what time, who were communicating and about what? Not only\nthe amount of available data to investigate has increased enormously, but also\nthe complexity of this data has increased. When these communication patterns\nneed to be combined with for instance a seized financial administration or\ncorporate document shares a complex investigation problem arises. Recently,\ncriminal investigators face a huge challenge when evidence of a crime needs to\nbe found in the Big Data environment where they have to deal with large and\ncomplex datasets especially in financial and fraud investigations. To tackle\nthis problem, a financial and fraud investigation unit of a European country\nhas developed a new tool named LES that uses Natural Language Processing (NLP)\ntechniques to help criminal investigators handle large amounts of textual\ninformation in a more efficient and faster way. In this paper, we present\nbriefly this tool and we focus on the evaluation its performance in terms of\nthe requirements of forensic investigation: speed, smarter and easier for\ninvestigators. In order to evaluate this LES tool, we use different performance\nmetrics. We also show experimental results of our evaluation with large and\ncomplex datasets from real-world application.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 21:23:22 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Banerveld", "Maarten", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1609.01331", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Zhi Zhang, Xiaobo Ren, Haohong Wang, Zhihai He", "title": "Joint Audio-Video Fingerprint Media Retrieval Using Rate-Coverage\n  Optimization", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a joint audio-video fingerprint Automatic Content\nRecognition (ACR) technology for media retrieval. The problem is focused on how\nto balance the query accuracy and the size of fingerprint, and how to allocate\nthe bits of the fingerprint to video frames and audio frames to achieve the\nbest query accuracy. By constructing a novel concept called Coverage, which is\nhighly correlated to the query accuracy, we are able to form a rate-coverage\nmodel to translate the original problem into an optimization problem that can\nbe resolved by dynamic programming. To the best of our knowledge, this is the\nfirst work that uses joint audio-video fingerprint ACR technology for media\nretrieval with a theoretical problem formulation. Experimental results indicate\nthat compared to reference algorithms, the proposed method has up to 25% query\naccuracy improvement while using 60% overall bit-rates, and 25% bit-rate\nreduction while achieving 85% accuracy, and it significantly outperforms the\nsolution with single audio or video source fingerprint.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 21:25:44 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Ning", "Guanghan", ""], ["Zhang", "Zhi", ""], ["Ren", "Xiaobo", ""], ["Wang", "Haohong", ""], ["He", "Zhihai", ""]]}, {"id": "1609.01357", "submitter": "Khushnood Abbas", "authors": "Khushnood Abbas, Mingsheng Shang, Cai Shi-Min, Xiaoyu Shi", "title": "Identifying emerging influential Nodes in evolving networks: Exploiting\n  strength of weak nodes", "comments": "4 figures 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying emerging influential or popular node/item in future on network is\na current interest of the researchers. Most of previous works focus on\nidentifying leaders in time evolving networks on the basis of network structure\nor node's activity separate way. In this paper, we have proposed a hybrid model\nwhich considers both, node's structural centrality and recent activity of nodes\ntogether. We consider that the node is active when it is receiving more links\nin a given recent time window, rather than in the whole past life of the node.\nFurthermore our model is flexible to implement structural rank such as PageRank\nand webpage click information as activity of the node. For testing the\nperformance of our model, we adopt the PageRank algorithm and linear\npreferential attachment based model as the baseline methods. Experiments on\nthree real data sets (i.e Movielens, Netflix and Facebook wall post data set),\nwe found that our model shows better performance in terms of finding the\nemerging influential nodes that were not popular in past.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 00:56:45 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Abbas", "Khushnood", ""], ["Shang", "Mingsheng", ""], ["Shi-Min", "Cai", ""], ["Shi", "Xiaoyu", ""]]}, {"id": "1609.01415", "submitter": "Aravind Sesagiri Raamkumar", "authors": "Aravind Sesagiri Raamkumar, Schubert Foo, Natalie Pang", "title": "A Framework for Scientific Paper Retrieval and Recommender Systems", "comments": "Technical Report, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval (IR) and recommender systems (RS) have been employed\nfor addressing search tasks executed during literature review and the overall\nscholarly communication lifecycle. Majority of the studies have concentrated on\nalgorithm design for improving the accuracy and usefulness of these systems.\nContextual elements related to the scholarly tasks have been largely ignored.\nIn this paper, we propose a framework called the Scientific Paper Recommender\nand Retrieval Framework (SPRRF) that combines aspects of user role modeling and\nuser-interface features with IR/RS components. The framework is based on eight\nemergent themes identified from participants feedback in a user evaluation\nstudy conducted with a prototype assistive system. 119 researchers participated\nin the study for evaluating the prototype system that provides recommendations\nfor two literature review and one manuscript writing tasks. This holistic\nframework is meant to guide future studies in this area.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 07:36:00 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Raamkumar", "Aravind Sesagiri", ""], ["Foo", "Schubert", ""], ["Pang", "Natalie", ""]]}, {"id": "1609.01574", "submitter": "Siddhartha Jonnalagadda", "authors": "Prakash Reddy Putta, John J. Dzak III, Siddhartha R. Jonnalagadda", "title": "Automatically extracting, ranking and visually summarizing the\n  treatments for a disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinicians are expected to have up-to-date and broad knowledge of disease\ntreatment options for a patient. Online health knowledge resources contain a\nwealth of information. However, because of the time investment needed to\ndisseminate and rank pertinent information, there is a need to summarize the\ninformation in a more concise format. Our aim of the study is to provide\nclinicians with a concise overview of popular treatments for a given disease\nusing information automatically computed from Medline abstracts. We analyzed\nthe treatments of two disorders - Atrial Fibrillation and Congestive Heart\nFailure. We calculated the precision, recall, and f-scores of our two ranking\nmethods to measure the accuracy of the results. For Atrial Fibrillation\ndisorder, maximum f-score for the New Treatments weighing method is 0.611,\nwhich occurs at 60 treatments. For Congestive Heart Failure disorder, maximum\nf-score for the New Treatments weighing method is 0.503, which occurs at 80\ntreatments.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 14:30:18 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Putta", "Prakash Reddy", ""], ["Dzak", "John J.", "III"], ["Jonnalagadda", "Siddhartha R.", ""]]}, {"id": "1609.01597", "submitter": "Siddhartha Jonnalagadda", "authors": "Kalpana Raja, Andrew J Sauer, Ravi P Garg, Melanie R Klerer,\n  Siddhartha R Jonnalagadda", "title": "A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical\n  Knowledge Summarization: Combining Concept Extraction, Vector Similarity and\n  Query Expansion for High Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel information retrieval methods to identify citations relevant to a\nclinical topic can overcome the knowledge gap existing between the primary\nliterature (MEDLINE) and online clinical knowledge resources such as UpToDate.\nSearching the MEDLINE database directly or with query expansion methods returns\na large number of citations that are not relevant to the query. The current\nstudy presents a citation retrieval system that retrieves citations for\nevidence-based clinical knowledge summarization. This approach combines query\nexpansion, concept-based screening algorithm, and concept-based vector\nsimilarity. We also propose an information extraction framework for automated\nconcept (Population, Intervention, Comparison, and Disease) extraction. We\nevaluated our proposed system on all topics (as queries) available from\nUpToDate for two diseases, heart failure (HF) and atrial fibrillation (AFib).\nThe system achieved an overall F-score of 41.2% on HF topics and 42.4% on AFib\ntopics on a gold standard of citations available in UpToDate. This is\nsignificantly high when compared to a query-expansion based baseline (F-score\nof 1.3% on HF and 2.2% on AFib) and a system that uses query expansion with\ndisease hyponyms and journal names, concept-based screening, and term-based\nvector similarity system (F-score of 37.5% on HF and 39.5% on AFib). Evaluating\nthe system with top K relevant citations, where K is the number of citations in\nthe gold standard achieved a much higher overall F-score of 69.9% on HF topics\nand 75.1% on AFib topics. In addition, the system retrieved up to 18 new\nrelevant citations per topic when tested on ten HF and six AFib clinical\ntopics.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 15:10:39 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Raja", "Kalpana", ""], ["Sauer", "Andrew J", ""], ["Garg", "Ravi P", ""], ["Klerer", "Melanie R", ""], ["Jonnalagadda", "Siddhartha R", ""]]}, {"id": "1609.01962", "submitter": "Arkaitz Zubiaga", "authors": "Michal Lukasik, Kalina Bontcheva, Trevor Cohn, Arkaitz Zubiaga, Maria\n  Liakata, Rob Procter", "title": "Using Gaussian Processes for Rumour Stance Classification in Social\n  Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media tend to be rife with rumours while new reports are released\npiecemeal during breaking news. Interestingly, one can mine multiple reactions\nexpressed by social media users in those situations, exploring their stance\ntowards rumours, ultimately enabling the flagging of highly disputed rumours as\nbeing potentially false. In this work, we set out to develop an automated,\nsupervised classifier that uses multi-task learning to classify the stance\nexpressed in each individual tweet in a rumourous conversation as either\nsupporting, denying or questioning the rumour. Using a classifier based on\nGaussian Processes, and exploring its effectiveness on two datasets with very\ndifferent characteristics and varying distributions of stances, we show that\nour approach consistently outperforms competitive baseline classifiers. Our\nclassifier is especially effective in estimating the distribution of different\ntypes of stance associated with a given rumour, which we set forth as a desired\ncharacteristic for a rumour-tracking system that will warn both ordinary users\nof Twitter and professional news practitioners when a rumour is being rebutted.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 12:33:02 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Lukasik", "Michal", ""], ["Bontcheva", "Kalina", ""], ["Cohn", "Trevor", ""], ["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1609.02171", "submitter": "Alessandro Checco", "authors": "Rehab K. Qarout, Alessandro Checco, and Gianluca Demartini", "title": "The Effect of Class Imbalance and Order on Crowdsourced Relevance\n  Judgments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the effect on crowd worker efficiency and\neffectiveness of the dominance of one class in the data they process. We aim at\nunderstanding if there is any positive or negative bias in workers seeing many\nnegative examples in the identification of positive labels. To test our\nhypothesis, we design an experiment where crowd workers are asked to judge the\nrelevance of documents presented in different orders. Our findings indicate\nthat there is a significant improvement in the quality of relevance judgements\nwhen presenting relevant results before the non-relevant ones.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 12:07:57 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Qarout", "Rehab K.", ""], ["Checco", "Alessandro", ""], ["Demartini", "Gianluca", ""]]}, {"id": "1609.02451", "submitter": "Francisco  Couto", "authors": "Diogo Goncalves and Miguel Costa and Francisco M. Couto", "title": "A Flexible Recommendation System for Cable TV", "comments": "in 3rd Workshop on Recommendation Systems for Television and online\n  Video (RecSysTV), At Boston, MA, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are being explored by Cable TV operators to improve\nuser satisfaction with services, such as Live TV and Video on Demand (VOD)\nservices. More recently, Catch-up TV has been introduced, allowing users to\nwatch recent broadcast content whenever they want to. These services give users\na large set of options from which they can choose from, creating an information\noverflow problem. Thus, recommendation systems arise as essential tools to\nsolve this problem by helping users in their selection, which increases not\nonly user satisfaction but also user engagement and content consumption. In\nthis paper we present a learning to rank approach that uses contextual\ninformation and implicit feedback to improve recommendation systems for a Cable\nTV operator that provides Live and Catch-up TV services. We compare our\napproach with existing state-of-the-art algorithms and show that our approach\nis superior in accuracy, while maintaining high scores of diversity and\nserendipity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 14:57:25 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 15:02:49 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Goncalves", "Diogo", ""], ["Costa", "Miguel", ""], ["Couto", "Francisco M.", ""]]}, {"id": "1609.02453", "submitter": "Francisco  Couto", "authors": "Diogo Goncalves and Miguel Costa and Francisco M. Couto", "title": "A Large-Scale Characterization of User Behaviour in Cable TV", "comments": "in 3rd Workshop on Recommendation Systems for Television and online\n  Video (RecSysTV), At Boston, MA, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Cable TV operators provide their users multiple ways to watch TV\ncontent, such as Live TV and Video on Demand (VOD) services. In the last years,\nCatch-up TV has been introduced, allowing users to watch recent broadcast\ncontent whenever they want to. Understanding how the users interact with such\nservices is important to develop solutions that may increase user satisfaction\n, user engagement and user consumption. In this paper, we characterize, for the\nfirst time, how users interact with a large European Cable TV operator that\nprovides Live TV, Catch-up TV and VOD services. We analyzed many\ncharacteristics, such as the service usage, user engagement, program type,\nprogram genres and time periods. This characterization will help us to have a\ndeeper understanding on how users interact with these different services, that\nmay be used to enhance the recommendation systems of Cable TV providers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 14:59:49 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 10:16:10 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Goncalves", "Diogo", ""], ["Costa", "Miguel", ""], ["Couto", "Francisco M.", ""]]}, {"id": "1609.02489", "submitter": "Christian Bracher", "authors": "Christian Bracher, Sebastian Heinz and Roland Vollgraf", "title": "Fashion DNA: Merging Content and Sales Data for Recommendation and\n  Article Mapping", "comments": "10 pages, 13 figures. Paper presented at the workshop \"Machine\n  Learning Meets Fashion,\" KDD 2016 Conference, San Francisco, USA, March 14,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to determine Fashion DNA, coordinate vectors locating\nfashion items in an abstract space. Our approach is based on a deep neural\nnetwork architecture that ingests curated article information such as tags and\nimages, and is trained to predict sales for a large set of frequent customers.\nIn the process, a dual space of customer style preferences naturally arises.\nInterpretation of the metric of these spaces is straightforward: The product of\nFashion DNA and customer style vectors yields the forecast purchase likelihood\nfor the customer-item pair, while the angle between Fashion DNA vectors is a\nmeasure of item similarity. Importantly, our models are able to generate\nunbiased purchase probabilities for fashion items based solely on article\ninformation, even in absence of sales data, thus circumventing the \"cold-start\nproblem\" of collaborative recommendation approaches. Likewise, it generalizes\neasily and reliably to customers outside the training set. We experiment with\nFashion DNA models based on visual and/or tag item data, evaluate their\nrecommendation power, and discuss the resulting article similarities.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 16:48:20 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Bracher", "Christian", ""], ["Heinz", "Sebastian", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1609.02687", "submitter": "Anukriti Bansal", "authors": "Anukriti Bansal, Sumantra Dutta Roy and Gaurav Harit", "title": "Extraction of Layout Entities and Sub-layout Query-based Retrieval of\n  Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layouts and sub-layouts constitute an important clue while searching a\ndocument on the basis of its structure, or when textual content is\nunknown/irrelevant. A sub-layout specifies the arrangement of document entities\nwithin a smaller portion of the document. We propose an efficient graph-based\nmatching algorithm, integrated with hash-based indexing, to prune a possibly\nlarge search space. A user can specify a combination of sub-layouts of interest\nusing sketch-based queries. The system supports partial matching for\nunspecified layout entities. We handle cases of segmentation pre-processing\nerrors (for text/non-text blocks) with a symmetry maximization-based strategy,\nand accounting for multiple domain-specific plausible segmentation hypotheses.\nWe show promising results of our system on a database of unstructured entities,\ncontaining 4776 newspaper images.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 08:21:13 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Bansal", "Anukriti", ""], ["Roy", "Sumantra Dutta", ""], ["Harit", "Gaurav", ""]]}, {"id": "1609.02839", "submitter": "Richard Oentaryo", "authors": "Jovian Lin, Richard Oentaryo, Ee-Peng Lim, Casey Vu, Adrian Vu, Agus\n  Kwee", "title": "Where is the Goldmine? Finding Promising Business Locations through\n  Facebook Data Analytics", "comments": null, "journal-ref": "Proceedings of the ACM Conference on Hypertext and Social Media,\n  Halifax, Canada, 2016, pp. 93-102", "doi": "10.1145/2914586.2914588", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If you were to open your own cafe, would you not want to effortlessly\nidentify the most suitable location to set up your shop? Choosing an optimal\nphysical location is a critical decision for numerous businesses, as many\nfactors contribute to the final choice of the location. In this paper, we seek\nto address the issue by investigating the use of publicly available Facebook\nPages data---which include user check-ins, types of business, and business\nlocations---to evaluate a user-selected physical location with respect to a\ntype of business. Using a dataset of 20,877 food businesses in Singapore, we\nconduct analysis of several key factors including business categories,\nlocations, and neighboring businesses. From these factors, we extract a set of\nrelevant features and develop a robust predictive model to estimate the\npopularity of a business location. Our experiments have shown that the\npopularity of neighboring business contributes the key features to perform\naccurate prediction. We finally illustrate the practical usage of our proposed\napproach via an interactive web application system.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 15:48:50 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Lin", "Jovian", ""], ["Oentaryo", "Richard", ""], ["Lim", "Ee-Peng", ""], ["Vu", "Casey", ""], ["Vu", "Adrian", ""], ["Kwee", "Agus", ""]]}, {"id": "1609.03067", "submitter": "Nasser Ghadiri", "authors": "Milad Moradi, Nasser Ghadiri", "title": "Quantifying the informativeness for biomedical literature summarization:\n  An itemset mining method", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.02948", "journal-ref": null, "doi": "10.1016/j.cmpb.2017.05.011", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Automatic text summarization tools can help users in the\nbiomedical domain to access information efficiently from a large volume of\nscientific literature and other sources of text documents. In this paper, we\npropose a summarization method that combines itemset mining and domain\nknowledge to construct a concept-based model and to extract the main subtopics\nfrom an input document. Our summarizer quantifies the informativeness of each\nsentence using the support values of itemsets appearing in the sentence.\nMethods: To address the concept-level analysis of text, our method initially\nmaps the original document to biomedical concepts using the UMLS. Then, it\ndiscovers the essential subtopics of the text using a data mining technique,\nnamely itemset mining, and constructs the summarization model. The employed\nitemset mining algorithm extracts a set of frequent itemsets containing\ncorrelated and recurrent concepts of the input document. The summarizer selects\nthe most related and informative sentences and generates the final summary.\nResults: We evaluate the performance of our itemset-based summarizer using the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics, performing a\nset of experiments. The results show that the itemset-based summarizer performs\nbetter than the compared methods. The itemset-based summarizer achieves the\nbest scores for all the assessed ROUGE metrics . Conclusion: Compared to the\nstatistical, similarity, and word frequency methods, the proposed method\ndemonstrates that the summarization model obtained from the concept extraction\nand itemset mining provides the summarizer with an effective metric for\nmeasuring the informative content of sentences. This can lead to an improvement\nin the performance of biomedical literature summarization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 16:09:28 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 04:30:39 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Moradi", "Milad", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1609.03176", "submitter": "Nikita Jain", "authors": "Nikita Jain, Swati Gupta, Dhaval Patel", "title": "E3 : Keyphrase based News Event Exploration Engine", "comments": null, "journal-ref": null, "doi": "10.1145/2914586.2914611", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a novel system E3 for extracting keyphrases from news\ncontent for the purpose of offering the news audience a broad overview of news\nevents, with especially high content volume. Given an input query, E3 extracts\nkeyphrases and enrich them by tagging, ranking and finding role for frequently\nassociated keyphrases. Also, E3 finds the novelty and activeness of keyphrases\nusing news publication date, to identify the most interesting and informative\nkeyphrases.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2016 15:59:35 GMT"}], "update_date": "2016-10-02", "authors_parsed": [["Jain", "Nikita", ""], ["Gupta", "Swati", ""], ["Patel", "Dhaval", ""]]}, {"id": "1609.03457", "submitter": "Deanna Pineau", "authors": "Deanna C. Pineau (University of Waterloo)", "title": "Math-Aware Search Engines: Physics Applications and Overview", "comments": "Full abstract in PDF; 66 pages, 4 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.MS math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines for equations now exist, which return results matching the\nquery's mathematical meaning or structural presentation. Operating over\nscientific papers, online encyclopedias, and math discussion forums, their\ncontent includes physics, math, and other sciences. They enable physicists to\navoid jargon and more easily target mathematical content within and across\ndisciplines. As a natural extension of keyword-based search, they open up a new\nworld for discovering both exact and approximate mathematical solutions;\nphysical systems' analogues and alternative models; and physics' patterns.\n  This review presents the existing math-aware search engines, discusses\nmethods for maximizing their search success, and overviews their math-matching\ncapabilities. Proposed applications to physics are also given, to contribute\ntowards developers' and physicists' exploration of the newly available search\nhorizons.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 06:07:23 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pineau", "Deanna C.", "", "University of Waterloo"]]}, {"id": "1609.03675", "submitter": "Hanjun Dai", "authors": "Hanjun Dai, Yichen Wang, Rakshit Trivedi, Le Song", "title": "Deep Coevolutionary Network: Embedding User and Item Features for\n  Recommendation", "comments": "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS\n  '16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often use latent features to explain the behaviors of\nusers and capture the properties of items. As users interact with different\nitems over time, user and item features can influence each other, evolve and\nco-evolve over time. The compatibility of user and item's feature further\ninfluence the future interaction between users and items. Recently, point\nprocess based models have been proposed in the literature aiming to capture the\ntemporally evolving nature of these latent features. However, these models\noften make strong parametric assumptions about the evolution process of the\nuser and item latent features, which may not reflect the reality, and has\nlimited power in expressing the complex and nonlinear dynamics underlying these\nprocesses. To address these limitations, we propose a novel deep coevolutionary\nnetwork model (DeepCoevolve), for learning user and item features based on\ntheir interaction graph. DeepCoevolve use recurrent neural network (RNN) over\nevolving networks to define the intensity function in point processes, which\nallows the model to capture complex mutual influence between users and items,\nand the feature evolution over time. We also develop an efficient procedure for\ntraining the model parameters, and show that the learned models lead to\nsignificant improvements in recommendation and activity prediction compared to\nprevious state-of-the-arts parametric models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 04:39:33 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 00:25:39 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 04:12:13 GMT"}, {"version": "v4", "created": "Tue, 28 Feb 2017 05:37:37 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dai", "Hanjun", ""], ["Wang", "Yichen", ""], ["Trivedi", "Rakshit", ""], ["Song", "Le", ""]]}, {"id": "1609.04281", "submitter": "Ridho Reinanda", "authors": "Ridho Reinanda, Edgar Meij, Maarten de Rijke", "title": "Document Filtering for Long-tail Entities", "comments": "CIKM2016, Proceedings of the 25th ACM International Conference on\n  Information and Knowledge Management. 2016", "journal-ref": null, "doi": "10.1145/2983323.2983728", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering relevant documents with respect to entities is an essential task in\nthe context of knowledge base construction and maintenance. It entails\nprocessing a time-ordered stream of documents that might be relevant to an\nentity in order to select only those that contain vital information.\nState-of-the-art approaches to document filtering for popular entities are\nentity-dependent: they rely on and are also trained on the specifics of\ndifferentiating features for each specific entity. Moreover, these approaches\ntend to use so-called extrinsic information such as Wikipedia page views and\nrelated entities which is typically only available only for popular head\nentities. Entity-dependent approaches based on such signals are therefore\nill-suited as filtering methods for long-tail entities. In this paper we\npropose a document filtering method for long-tail entities that is\nentity-independent and thus also generalizes to unseen or rarely seen entities.\nIt is based on intrinsic features, i.e., features that are derived from the\ndocuments in which the entities are mentioned. We propose a set of features\nthat capture informativeness, entity-saliency, and timeliness. In particular,\nwe introduce features based on entity aspect similarities, relation patterns,\nand temporal expressions and combine these with standard features for document\nfiltering. Experiments following the TREC KBA 2014 setup on a publicly\navailable dataset show that our model is able to improve the filtering\nperformance for long-tail entities over several baselines. Results of applying\nthe model to unseen entities are promising, indicating that the model is able\nto learn the general characteristics of a vital document. The overall\nperformance across all entities---i.e., not just long-tail entities---improves\nupon the state-of-the-art without depending on any entity-specific training\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 14:09:20 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Reinanda", "Ridho", ""], ["Meij", "Edgar", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1609.04556", "submitter": "Djoerd Hiemstra", "authors": "Dong Nguyen, Thomas Demeester, Dolf Trieschnigg, Djoerd Hiemstra", "title": "Resource Selection for Federated Search on the Web", "comments": "CTIT Technical Report, University of Twente", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A publicly available dataset for federated search reflecting a real web\nenvironment has long been absent, making it difficult for researchers to test\nthe validity of their federated search algorithms for the web setting. We\npresent several experiments and analyses on resource selection on the web using\na recently released test collection containing the results from more than a\nhundred real search engines, ranging from large general web search engines such\nas Google, Bing and Yahoo to small domain-specific engines. First, we\nexperiment with estimating the size of uncooperative search engines on the web\nusing query based sampling and propose a new method using the ClueWeb09\ndataset. We find the size estimates to be highly effective in resource\nselection. Second, we show that an optimized federated search system based on\nsmaller web search engines can be an alternative to a system using large web\nsearch engines. Third, we provide an empirical comparison of several popular\nresource selection methods and find that these methods are not readily suitable\nfor resource selection on the web. Challenges include the sparse resource\ndescriptions and extremely skewed sizes of collections.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 09:49:27 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Nguyen", "Dong", ""], ["Demeester", "Thomas", ""], ["Trieschnigg", "Dolf", ""], ["Hiemstra", "Djoerd", ""]]}, {"id": "1609.05234", "submitter": "Tzu Hsiang Lin", "authors": "Yen-Chen Wu, Tzu-Hsiang Lin, Yang-De Chen, Hung-Yi Lee, Lin-Shan Lee", "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning", "comments": "Accepted conference paper: \"The Annual Conference of the\n  International Speech Communication Association (Interspeech), 2016\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-machine interaction is important for spoken content retrieval. For text\ncontent retrieval, the user can easily scan through and select on a list of\nretrieved item. This is impossible for spoken content retrieval, because the\nretrieved items are difficult to show on screen. Besides, due to the high\ndegree of uncertainty for speech recognition, the retrieval results can be very\nnoisy. One way to counter such difficulties is through user-machine\ninteraction. The machine can take different actions to interact with the user\nto obtain better retrieval results before showing to the user. The suitable\nactions depend on the retrieval status, for example requesting for extra\ninformation from the user, returning a list of topics for user to select, etc.\nIn our previous work, some hand-crafted states estimated from the present\nretrieval results are used to determine the proper actions. In this paper, we\npropose to use Deep-Q-Learning techniques instead to determine the machine\nactions for interactive spoken content retrieval. Deep-Q-Learning bypasses the\nneed for estimation of the hand-crafted states, and directly determine the best\naction base on the present retrieval status even without any human knowledge.\nIt is shown to achieve significantly better performance compared with the\nprevious hand-crafted states.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 20:56:22 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wu", "Yen-Chen", ""], ["Lin", "Tzu-Hsiang", ""], ["Chen", "Yang-De", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1609.05244", "submitter": "Haohan Wang", "authors": "Haohan Wang, Aaksha Meghawat, Louis-Philippe Morency and Eric P. Xing", "title": "Select-Additive Learning: Improving Generalization in Multimodal\n  Sentiment Analysis", "comments": "Supplementary files at:\n  http://www.cs.cmu.edu/~haohanw/document/sal_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis is drawing an increasing amount of attention\nthese days. It enables mining of opinions in video reviews which are now\navailable aplenty on online platforms. However, multimodal sentiment analysis\nhas only a few high-quality data sets annotated for training machine learning\nalgorithms. These limited resources restrict the generalizability of models,\nwhere, for example, the unique characteristics of a few speakers (e.g., wearing\nglasses) may become a confounding factor for the sentiment classification task.\nIn this paper, we propose a Select-Additive Learning (SAL) procedure that\nimproves the generalizability of trained neural networks for multimodal\nsentiment analysis. In our experiments, we show that our SAL approach improves\nprediction accuracy significantly in all three modalities (verbal, acoustic,\nvisual), as well as in their fusion. Our results show that SAL, even when\ntrained on one dataset, achieves good generalization across two new test\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2016 21:33:42 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 21:38:40 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Wang", "Haohan", ""], ["Meghawat", "Aaksha", ""], ["Morency", "Louis-Philippe", ""], ["Xing", "Eric P.", ""]]}, {"id": "1609.05345", "submitter": "Shicong Liu", "authors": "Shicong Liu, Junru Shao, Hongtao Lu", "title": "Generalized residual vector quantization for large scale data", "comments": "published on International Conference on Multimedia and Expo 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector quantization is an essential tool for tasks involving large scale\ndata, for example, large scale similarity search, which is crucial for\ncontent-based information retrieval and analysis. In this paper, we propose a\nnovel vector quantization framework that iteratively minimizes quantization\nerror. First, we provide a detailed review on a relevant vector quantization\nmethod named \\textit{residual vector quantization} (RVQ). Next, we propose\n\\textit{generalized residual vector quantization} (GRVQ) to further improve\nover RVQ. Many vector quantization methods can be viewed as the special cases\nof our proposed framework. We evaluate GRVQ on several large scale benchmark\ndatasets for large scale search, classification and object retrieval. We\ncompared GRVQ with existing methods in detail. Extensive experiments\ndemonstrate our GRVQ framework substantially outperforms existing methods in\nterm of quantization accuracy and computation efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:50:06 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Liu", "Shicong", ""], ["Shao", "Junru", ""], ["Lu", "Hongtao", ""]]}, {"id": "1609.05610", "submitter": "Michal Ferov", "authors": "Michal Ferov and Marek Modr\\'y", "title": "Enhancing LambdaMART Using Oblivious Trees", "comments": "Accepted for publication in proceedings of RUSSIR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is a machine learning technique broadly used in many areas\nsuch as document retrieval, collaborative filtering or question answering. We\npresent experimental results which suggest that the performance of the current\nstate-of-the-art learning to rank algorithm LambdaMART, when used for document\nretrieval for search engines, can be improved if standard regression trees are\nreplaced by oblivious trees. This paper provides a comparison of both variants\nand our results demonstrate that the use of oblivious trees can improve the\nperformance by more than $2.2\\%$. Additional experimental analysis of the\ninfluence of a number of features and of a size of the training set is also\nprovided and confirms the desirability of properties of oblivious decision\ntrees.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 07:03:29 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Ferov", "Michal", ""], ["Modr\u00fd", "Marek", ""]]}, {"id": "1609.05774", "submitter": "Ashlynn Daughton", "authors": "A.R. Daughton, R. Priedhorsky, G. Fairchild, N. Generous, A.\n  Hengartner, E. Abeyta, N. Velappan, A. Lillo, K. Stark, A. Deshpande", "title": "A globally-applicable disease ontology for biosurveillance; Anthology of\n  Biosurveillance Diseases (ABD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biosurveillance, a relatively young field, has recently increased in\nimportance because of its relevance to national security and global health.\nDatabases and tools describing particular subsets of disease are becoming\nincreasingly common in the field. However, a common method to describe those\ndiseases is lacking. Here, we present the Anthology of Biosurveillance Diseases\n(ABD), an ontology of infectious diseases of biosurveillance relevance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 19:37:47 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Daughton", "A. R.", ""], ["Priedhorsky", "R.", ""], ["Fairchild", "G.", ""], ["Generous", "N.", ""], ["Hengartner", "A.", ""], ["Abeyta", "E.", ""], ["Velappan", "N.", ""], ["Lillo", "A.", ""], ["Stark", "K.", ""], ["Deshpande", "A.", ""]]}, {"id": "1609.05787", "submitter": "Qiang Liu", "authors": "Qiang Liu, Shu Wu, Diyi Wang, Zhaokang Li, Liang Wang", "title": "Context-aware Sequential Recommendation", "comments": "IEEE International Conference on Data Mining (ICDM) 2016, to apear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since sequential information plays an important role in modeling user\nbehaviors, various sequential recommendation methods have been proposed.\nMethods based on Markov assumption are widely-used, but independently combine\nseveral most recent components. Recently, Recurrent Neural Networks (RNN) based\nmethods have been successfully applied in several sequential modeling tasks.\nHowever, for real-world applications, these methods have difficulty in modeling\nthe contextual information, which has been proved to be very important for\nbehavior modeling. In this paper, we propose a novel model, named Context-Aware\nRecurrent Neural Networks (CA-RNN). Instead of using the constant input matrix\nand transition matrix in conventional RNN models, CA-RNN employs adaptive\ncontext-specific input matrices and adaptive context-specific transition\nmatrices. The adaptive context-specific input matrices capture external\nsituations where user behaviors happen, such as time, location, weather and so\non. And the adaptive context-specific transition matrices capture how lengths\nof time intervals between adjacent behaviors in historical sequences affect the\ntransition of global sequential features. Experimental results show that the\nproposed CA-RNN model yields significant improvements over state-of-the-art\nsequential recommendation methods and context-aware recommendation methods on\ntwo public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 15:33:46 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Diyi", ""], ["Li", "Zhaokang", ""], ["Wang", "Liang", ""]]}, {"id": "1609.05866", "submitter": "Alexandre de Br\\'ebisson", "authors": "Alexandre de Br\\'ebisson, Pascal Vincent", "title": "A Cheap Linear Attention Mechanism with Fast Lookups and Fixed-Size\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The softmax content-based attention mechanism has proven to be very\nbeneficial in many applications of recurrent neural networks. Nevertheless it\nsuffers from two major computational limitations. First, its computations for\nan attention lookup scale linearly in the size of the attended sequence.\nSecond, it does not encode the sequence into a fixed-size representation but\ninstead requires to memorize all the hidden states. These two limitations\nrestrict the use of the softmax attention mechanism to relatively small-scale\napplications with short sequences and few lookups per sequence. In this work we\nintroduce a family of linear attention mechanisms designed to overcome the two\nlimitations listed above. We show that removing the softmax non-linearity from\nthe traditional attention formulation yields constant-time attention lookups\nand fixed-size representations of the attended sequences. These properties make\nthese linear attention mechanisms particularly suitable for large-scale\napplications with extreme query loads, real-time requirements and memory\nconstraints. Early experiments on a question answering task show that these\nlinear mechanisms yield significantly better accuracy results than no\nattention, but obviously worse than their softmax alternative.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 18:55:18 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["de Br\u00e9bisson", "Alexandre", ""], ["Vincent", "Pascal", ""]]}, {"id": "1609.05989", "submitter": "Mohammed Korayem", "authors": "Mohammed Korayem, Khalifeh Aljadda, and Trey Grainger", "title": "Macro-optimization of email recommendation response rates harnessing\n  individual activity levels and group affinity trends", "comments": "This version is accepted as regular paper in The 15th IEEE\n  International Conference on Machine Learning and Applications (IEEE ICMLA'16)\n  . pre-camera ready version", "journal-ref": "The 15th IEEE International Conference on Machine Learning and\n  Applications (IEEE ICMLA'16) , 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation emails are among the best ways to re-engage with customers\nafter they have left a website. While on-site recommendation systems focus on\nfinding the most relevant items for a user at the moment (right item), email\nrecommendations add two critical additional dimensions: who to send\nrecommendations to (right person) and when to send them (right time). It is\ncritical that a recommendation email system not send too many emails to too\nmany users in too short of a time-window, as users may unsubscribe from future\nemails or become desensitized and ignore future emails if they receive too\nmany. Also, email service providers may mark such emails as spam if too many of\ntheir users are contacted in a short time-window. Optimizing email\nrecommendation systems such that they can yield a maximum response rate for a\nminimum number of email sends is thus critical for the long-term performance of\nsuch a system. In this paper, we present a novel recommendation email system\nthat not only generates recommendations, but which also leverages a combination\nof individual user activity data, as well as the behavior of the group to which\nthey belong, in order to determine each user's likelihood to respond to any\ngiven set of recommendations within a given time period. In doing this, we have\neffectively created a meta-recommendation system which recommends sets of\nrecommendations in order to optimize the aggregate response rate of the entire\nsystem. The proposed technique has been applied successfully within\nCareerBuilder's job recommendation email system to generate a 50\\% increase in\ntotal conversions while also decreasing sent emails by 72%\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2016 01:49:00 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Korayem", "Mohammed", ""], ["Aljadda", "Khalifeh", ""], ["Grainger", "Trey", ""]]}, {"id": "1609.06423", "submitter": "Mayank Singh", "authors": "Mayank Singh, Barnopriyo Barua, Priyank Palod, Manvi Garg, Sidhartha\n  Satapathy, Samuel Bushi, Kumar Ayush, Krishna Sai Rohith, Tulasi Gamidi,\n  Pawan Goyal and Animesh Mukherjee", "title": "OCR++: A Robust Framework For Information Extraction from Scholarly\n  Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes OCR++, an open-source framework designed for a variety of\ninformation extraction tasks from scholarly articles including metadata (title,\nauthor names, affiliation and e-mail), structure (section headings and body\ntext, table and figure headings, URLs and footnotes) and bibliography (citation\ninstances and references). We analyze a diverse set of scientific articles\nwritten in English language to understand generic writing patterns and\nformulate rules to develop this hybrid framework. Extensive evaluations show\nthat the proposed framework outperforms the existing state-of-the-art tools\nwith huge margin in structural information extraction along with improved\nperformance in metadata and bibliography extraction tasks, both in terms of\naccuracy (around 50% improvement) and processing time (around 52% improvement).\nA user experience study conducted with the help of 30 researchers reveals that\nthe researchers found this system to be very helpful. As an additional\nobjective, we discuss two novel use cases including automatically extracting\nlinks to public datasets from the proceedings, which would further accelerate\nthe advancement in digital libraries. The result of the framework can be\nexported as a whole into structured TEI-encoded documents. Our framework is\naccessible online at http://cnergres.iitkgp.ac.in/OCR++/home/.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 06:12:52 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 10:54:57 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2016 13:05:27 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Singh", "Mayank", ""], ["Barua", "Barnopriyo", ""], ["Palod", "Priyank", ""], ["Garg", "Manvi", ""], ["Satapathy", "Sidhartha", ""], ["Bushi", "Samuel", ""], ["Ayush", "Kumar", ""], ["Rohith", "Krishna Sai", ""], ["Gamidi", "Tulasi", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1609.06568", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Scott Sanner, Shengbo Guo", "title": "On the Mathematical Relationship between Expected n-call@k and the\n  Relevance vs. Diversity Trade-off", "comments": "SIGIR short paper", "journal-ref": "Proceedings of the 35th Annual ACM SIG Information Retrieval\n  Conference (SIGIR), pp. 1117-1118. ACM. 2012", "doi": "10.1145/2348283.2348497", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been previously noted that optimization of the n-call@k relevance\nobjective (i.e., a set-based objective that is 1 if at least n documents in a\nset of k are relevant, otherwise 0) encourages more result set diversification\nfor smaller n, but this statement has never been formally quantified. In this\nwork, we explicitly derive the mathematical relationship between expected\nn-call@k and the relevance vs. diversity trade-off --- through fortuitous\ncancellations in the resulting combinatorial optimization, we show the\ntrade-off is a simple and intuitive function of n (notably independent of the\nresult set size k e n), where diversification increases as n approaches 1.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:11:27 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lim", "Kar Wai", ""], ["Sanner", "Scott", ""], ["Guo", "Shengbo", ""]]}, {"id": "1609.06578", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Wray Buntine", "title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by\n  Leveraging Hashtags and Sentiment Lexicon", "comments": "CIKM paper", "journal-ref": "Proceedings of the 23rd ACM International Conference on\n  Information and Knowledge Management (CIKM), pp. 1319-1328. ACM. 2014", "doi": "10.1145/2661829.2662005", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based opinion mining is widely applied to review data to aggregate or\nsummarize opinions of a product, and the current state-of-the-art is achieved\nwith Latent Dirichlet Allocation (LDA)-based model. Although social media data\nlike tweets are laden with opinions, their \"dirty\" nature (as natural language)\nhas discouraged researchers from applying LDA-based opinion model for product\nreview mining. Tweets are often informal, unstructured and lacking labeled data\nsuch as categories and ratings, making it challenging for product opinion\nmining. In this paper, we propose an LDA-based opinion model named Twitter\nOpinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM\nleverages hashtags, mentions, emoticons and strong sentiment words that are\npresent in tweets in its discovery process. It improves opinion prediction by\nmodeling the target-opinion interaction directly, thus discovering target\nspecific opinion words, neglected in existing approaches. Moreover, we propose\na new formulation of incorporating sentiment prior information into a topic\nmodel, by utilizing an existing public sentiment lexicon. This is novel in that\nit learns and updates with the data. We conduct experiments on 9 million tweets\non electronic products, and demonstrate the improved performance of TOTM in\nboth quantitative evaluations and qualitative analysis. We show that\naspect-based opinion analysis on massive volume of tweets provides useful\nopinions on products.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 14:25:23 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Lim", "Kar Wai", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.06616", "submitter": "John J Nay", "authors": "John J. Nay", "title": "Gov2Vec: Learning Distributed Representations of Institutions and Their\n  Legal Text", "comments": "Forthcoming paper in the 2016 Proceedings of the Conference on\n  Empirical Methods in Natural Language Processing Workshop on Natural Language\n  Processing and Computational Social Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare policy differences across institutions by embedding\nrepresentations of the entire legal corpus of each institution and the\nvocabulary shared across all corpora into a continuous vector space. We apply\nour method, Gov2Vec, to Supreme Court opinions, Presidential actions, and\nofficial summaries of Congressional bills. The model discerns meaningful\ndifferences between government branches. We also learn representations for more\nfine-grained word sources: individual Presidents and (2-year) Congresses. The\nsimilarities between learned representations of Congresses over time and\nsitting Presidents are negatively correlated with the bill veto rate, and the\ntemporal ordering of Presidents and Congresses was implicitly learned from only\ntext. With the resulting vectors we answer questions such as: how does Obama\nand the 113th House differ in addressing climate change and how does this vary\nfrom environmental or economic perspectives? Our work illustrates\nvector-arithmetic-based investigations of complex relationships between word\nsources based on their texts. We are extending this to create a more\ncomprehensive legal semantic map.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 16:09:12 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 22:20:12 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Nay", "John J.", ""]]}, {"id": "1609.06791", "submitter": "Kar Wai Lim", "authors": "Kar Wai Lim, Changyou Chen, Wray Buntine", "title": "Twitter-Network Topic Model: A Full Bayesian Treatment for Social\n  Network and Text Modeling", "comments": "NIPS workshop paper", "journal-ref": "NIPS 2013 Topic Models: Computation, Application, and Evaluation,\n  pp. 1-5. Google Sites. 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter data is extremely noisy -- each tweet is short, unstructured and with\ninformal language, a challenge for current topic modeling. On the other hand,\ntweets are accompanied by extra information such as authorship, hashtags and\nthe user-follower network. Exploiting this additional information, we propose\nthe Twitter-Network (TN) topic model to jointly model the text and the social\nnetwork in a full Bayesian nonparametric way. The TN topic model employs the\nhierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian\nprocess random function model for social network modeling. We show that the TN\ntopic model significantly outperforms several existing nonparametric models due\nto its flexibility. Moreover, the TN topic model enables additional informative\ninference such as authors' interests, hashtag analysis, as well as leading to\nfurther applications such as author recommendation, automatic topic labeling\nand hashtag suggestion. Note our general inference framework can readily be\napplied to other topic models with embedded PDP nodes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 01:08:31 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Lim", "Kar Wai", ""], ["Chen", "Changyou", ""], ["Buntine", "Wray", ""]]}, {"id": "1609.08201", "submitter": "Shahriar Shariat", "authors": "Shahriar Shariat and Vladimir Pavlovic", "title": "Robust Time-Series Retrieval Using Probabilistic Adaptive Segmental\n  Alignment", "comments": null, "journal-ref": "Knowl Inf Syst (2016) 49: 91. doi:10.1007/s10115-015-0898-4", "doi": "10.1007/s10115-015-0898-4", "report-no": null, "categories": "cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional pairwise sequence alignment is based on matching individual\nsamples from two sequences, under time monotonicity constraints. However, in\nmany application settings matching subsequences (segments) instead of\nindividual samples may bring in additional robustness to noise or local\nnon-causal perturbations. This paper presents an approach to segmental sequence\nalignment that jointly segments and aligns two sequences, generalizing the\ntraditional per-sample alignment. To accomplish this task, we introduce a\ndistance metric between segments based on average pairwise distances and then\npresent a modified pair-HMM (PHMM) that incorporates the proposed distance\nmetric to solve the joint segmentation and alignment task. We also propose a\nrelaxation to our model that improves the computational efficiency of the\ngeneric segmental PHMM. Our results demonstrate that this new measure of\nsequence similarity can lead to improved classification performance, while\nbeing resilient to noise, on a variety of sequence retrieval problems, from EEG\nto motion sequence classification.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 21:53:42 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shariat", "Shahriar", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1609.08264", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Ming Yang, Qiang Cheng", "title": "Top-N Recommendation on Graphs", "comments": "CIKM 2016", "journal-ref": null, "doi": "10.1145/2983323.2983649", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play an increasingly important role in online\napplications to help users find what they need or prefer. Collaborative\nfiltering algorithms that generate predictions by analyzing the user-item\nrating matrix perform poorly when the matrix is sparse. To alleviate this\nproblem, this paper proposes a simple recommendation algorithm that fully\nexploits the similarity information among users and items and intrinsic\nstructural information of the user-item matrix. The proposed method constructs\na new representation which preserves affinity and structure information in the\nuser-item rating matrix and then performs recommendation task. To capture\nproximity information about users and items, two graphs are constructed.\nManifold learning idea is used to constrain the new representation to be smooth\non these graphs, so as to enforce users and item proximities. Our model is\nformulated as a convex optimization problem, for which we need to solve the\nwell-known Sylvester equation only. We carry out extensive empirical\nevaluations on six benchmark datasets to show the effectiveness of this\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 05:45:03 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Yang", "Ming", ""], ["Cheng", "Qiang", ""]]}, {"id": "1609.08492", "submitter": "Miguel Rodrigues", "authors": "Miguel J. Rodrigues, Miguel Fal\\'e, Andre Lamurias, and Francisco M.\n  Couto", "title": "WS4A: a Biomedical Question and Answering System based on public Web\n  Services and Ontologies", "comments": "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our system, dubbed WS4A (Web Services for All), that\nparticipated in the fourth edition of the BioASQ challenge (2016). We used WS4A\nto perform the Question and Answering (QA) task 4b, which consisted on the\nretrieval of relevant concepts, documents, snippets, RDF triples, exact answers\nand ideal answers for each given question. The novelty in our approach consists\non the maximum exploitation of existing web services in each step of WS4A, such\nas the annotation of text, and the retrieval of metadata for each annotation.\nThe information retrieved included concept identifiers, ontologies, ancestors,\nand most importantly, PubMed identifiers. The paper describes the WS4A pipeline\nand also presents the precision, recall and f-measure values obtained in task\n4b. Our system achieved two second places in two subtasks on one of the five\nbatches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 15:14:04 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 12:12:15 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Rodrigues", "Miguel J.", ""], ["Fal\u00e9", "Miguel", ""], ["Lamurias", "Andre", ""], ["Couto", "Francisco M.", ""]]}, {"id": "1609.08496", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang, Ping Chen, Tong Wang, Xindong Wu", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring topics from the overwhelming amount of short texts becomes a\ncritical but challenging task for many content analysis tasks, such as content\ncharactering, user interest profiling, and emerging topic detecting. Existing\nmethods such as probabilistic latent semantic analysis (PLSA) and latent\nDirichlet allocation (LDA) cannot solve this prob- lem very well since only\nvery limited word co-occurrence information is available in short texts. This\npaper studies how to incorporate the external word correlation knowledge into\nshort texts to improve the coherence of topic modeling. Based on recent results\nin word embeddings that learn se- mantically representations for words from a\nlarge corpus, we introduce a novel method, Embedding-based Topic Model (ETM),\nto learn latent topics from short texts. ETM not only solves the problem of\nvery limited word co-occurrence information by aggregating short texts into\nlong pseudo- texts, but also utilizes a Markov Random Field regularized model\nthat gives correlated words a better chance to be put into the same topic. The\nexperiments on real-world datasets validate the effectiveness of our model\ncomparing with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 15:26:07 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Qiang", "Jipeng", ""], ["Chen", "Ping", ""], ["Wang", "Tong", ""], ["Wu", "Xindong", ""]]}, {"id": "1609.08843", "submitter": "Jiaming Xu", "authors": "Jiaming Xu, Jing Shi, Yiqun Yao, Suncong Zheng, Bo Xu, Bo Xu", "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "comments": "10 pages, to appear in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, end-to-end memory networks have shown promising results on Question\nAnswering task, which encode the past facts into an explicit memory and perform\nreasoning ability by making multiple computational steps on the memory.\nHowever, memory networks conduct the reasoning on sentence-level memory to\noutput coarse semantic vectors and do not further take any attention mechanism\nto focus on words, which may lead to the model lose some detail information,\nespecially when the answers are rare or unknown words. In this paper, we\npropose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the\npast facts into sentence-level memory and word-level memory respectively. Then,\n(k)-max pooling is exploited following reasoning module on the sentence-level\nmemory to sample the (k) most relevant sentences to a question and feed these\nsentences into attention mechanism on the word-level memory to focus the words\nin the selected sentences. Finally, the prediction is jointly learned over the\noutputs of the sentence-level reasoning module and the word-level attention\nmechanism. The experimental results demonstrate that our approach successfully\nconducts answer selection on unknown words and achieves a better performance\nthan memory networks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 10:03:05 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Xu", "Jiaming", ""], ["Shi", "Jing", ""], ["Yao", "Yiqun", ""], ["Zheng", "Suncong", ""], ["Xu", "Bo", ""], ["Xu", "Bo", ""]]}, {"id": "1609.09152", "submitter": "Ruining He", "authors": "Ruining He and Julian McAuley", "title": "Fusing Similarity Models with Markov Chains for Sparse Sequential\n  Recommendation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting personalized sequential behavior is a key task for recommender\nsystems. In order to predict user actions such as the next product to purchase,\nmovie to watch, or place to visit, it is essential to take into account both\nlong-term user preferences and sequential patterns (i.e., short-term dynamics).\nMatrix Factorization and Markov Chain methods have emerged as two separate but\npowerful paradigms for modeling the two respectively. Combining these ideas has\nled to unified methods that accommodate long- and short-term dynamics\nsimultaneously by modeling pairwise user-item and item-item interactions.\n  In spite of the success of such methods for tackling dense data, they are\nchallenged by sparsity issues, which are prevalent in real-world datasets. In\nrecent years, similarity-based methods have been proposed for\n(sequentially-unaware) item recommendation with promising results on sparse\ndatasets. In this paper, we propose to fuse such methods with Markov Chains to\nmake personalized sequential recommendations. We evaluate our method, Fossil,\non a variety of large, real-world datasets. We show quantitatively that Fossil\noutperforms alternative algorithms, especially on sparse datasets, and\nqualitatively that it captures personalized dynamics and is able to make\nmeaningful recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 23:09:24 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["He", "Ruining", ""], ["McAuley", "Julian", ""]]}, {"id": "1609.09188", "submitter": "Leonard K.M. Poon", "authors": "Leonard K.M. Poon and Nevin L. Zhang", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic researchers often need to face with a large collection of research\npapers in the literature. This problem may be even worse for postgraduate\nstudents who are new to a field and may not know where to start. To address\nthis problem, we have developed an online catalog of research papers where the\npapers have been automatically categorized by a topic model. The catalog\ncontains 7719 papers from the proceedings of two artificial intelligence\nconferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet\nAllocation, we use a recently proposed method called hierarchical latent tree\nanalysis for topic modeling. The resulting topic model contains a hierarchy of\ntopics so that users can browse the topics from the top level to the bottom\nlevel. The topic model contains a manageable number of general topics at the\ntop level and allows thousands of fine-grained topics at the bottom level. It\nalso can detect topics that have emerged recently.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 03:22:01 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Zhang", "Nevin L.", ""]]}]